story_url,bodyText
https://medium.com/@benbob/the-death-and-re-birth-of-infrastructure-the-rise-of-public-cloud-c3f4c6119443?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Top highlight
Ben Fathi
Jan 20, 2018·10 min read
“To really understand something is to be liberated from it.” — Dominic Frisby. The Four Horsemen.
I received a lot of feedback on my recent blog regarding “Public vs. Private Cloud”, in which I argued that private clouds, shrink-wrapped software, and — in general — on-prem infrastructure, are going the way of the Dodo.
There is nothing new or earth shattering in what I wrote. Many experts have been saying the same thing for years. We all seem to agree that we’re moving towards the cloud. Yet, for some reason, enterprise companies continue to invest in, and perpetuate, the old model for infrastructure deployment. With all the hype around cloud adoption, it’s easy to forget that over 90% of all IT spend still goes to traditional on-prem deployments. Inertia continues to be a big factor in Enterprise IT organizations just as incrementalism reigns supreme in the R&D organizations of “old school” system and software providers.
I spent most of my career building operating systems and distributed system software delivered as shrink wrapped software and meant to be deployed on-prem. I’m proud of what we all accomplished as an industry. But we’ve come a long way. Those battles are pretty much over and the industry has moved “up the stack” so they can continue to innovate in new and uncharted territories.
Very few companies are starting new processor architectures or building operating systems from scratch. The world standardized on one of two processor types (x86 or ARM), one of two operating systems (Linux or Windows), one of two relational databases (Oracle or SQL), and so on. There is no longer any point in arguing that MIPS was a better/cleaner processor architecture than Intel. I personally spent a huge chunk of my career on that processor and am proud of the work we did but there are no longer any companies out there building systems based on the MIPS architecture. More importantly, there are no companies offering applications compiled for that instruction set. It’s time to move on.
The same logic should be applied to on-prem infrastructure hardware and software. We need to agree, as an industry, that the Enterprise-IT-owned-and-operated data center will also soon go the way of the Dodo.
“One of the insights from our research about commoditization is that whenever it is at work somewhere in a value chain, a reciprocal process of de-commoditization is at work somewhere else in the value chain. … The reciprocality of these processes means that the locus of the ability to differentiate shifts continuously in a value chain as new waves of disruption wash over an industry. As this happens companies that position themselves at a spot in the value chain where performance is not yet good enough will capture the profit.” — Clayton Christensen. The Innovator’s Solution.
By the time you take all the variables in the equation into account, the total cost of ownership of any such solution far surpasses any cloud based solution. Here, I’m including all the hidden costs of installing, managing, patching, upgrading, securing, and testing infrastructure hardware and software in support of enterprise application delivery. Note that I’m not comparing the cost of moving your hardware to the cloud but rather your applications. There’s a big difference. You must abstract away the hardware in order to gain the advantages of cloud, not simply replicate it off-site.
Perhaps the only factors in favor of on-prem infrastructure are compatibility and familiarity. But at the rate this industry is moving, you will be rethinking that particular application or service in five years anyway, so why worry about compatibility with what you were running five years ago? Continuing to invest in on-prem infrastructure is the equivalent of throwing good money after bad down a bottomless well.
The typical smart Enterprise IT person usually spends a large percentage of his or her time and efforts getting close to purveyors of on-prem hardware and software. As the organization grows, he is constantly pressured into buying more servers, improving security, increasing storage, adding better email archival and compliance tools, adding load balancer appliances and firewalls as he is asked to offer better service availability for employees as well as customers. He is also constrained by his budget and the strategic decisions of upper management.
The easiest answer is to ask for the same budget as last year and keep buying more “stuff”. It’s the path of least resistance. And because most enterprise applications run on-prem today, it’s often easiest to just add to the existing infrastructure rather than completely overhaul the application.
These same smart guys often end up creating a symbiotic relationship not just with the sales teams at those hardware and software vendors but also with their respective R&D organizations. And they pressure these well-meaning R&D organizations into signing up for huge enterprise license agreements “if only” they can get a specific feature added to the product. The PM in charge shakes hands, promises to “look into it” for the next release of the OS or the appliance or the firewall, and creates the appropriate specs to get it into the next release. This is how incremental improvements that only solve the problems of a single organization end up as “requirements” that shape major releases of all platform software.
“You know, I have one simple request. And that is to have sharks with frickin’ laser beams attached to their heads! Now evidently, my cycloptic colleague informs me that that can’t be done. Can you remind me what I pay you people for? Honestly, throw me a bone here. What do we have?” — Mike Meyers. Austin Powers: International Man of Mystery.
Meanwhile, back at the ranch, that IT organization has gone through three re-orgs and leadership changes, has changed direction four times, and has laid off or churned most of its staff.
Finally, a year or two later, the fateful moment arrives and we deploy the new version of software on all our servers. And, of course, they all promptly crash. The engineers spend all weekend debugging the problem in the customer’s environment and come back with their verdict: “We ran into a specific bug that only manifests itself when you run version x.y of that firmware on the network controller as well as version a.b of the network driver from the vendor and you have to add more than 5000 firewall rules through this API that the customer requested. We had accounted for two out of three variables but had to cut the testing for that particular combination of variables in order to meet our schedules.”
So, basically, your data center is the first time all these pieces of hardware and software have come together — and I’ve only described the simplest of scenarios.
Every enterprise deployment is bespoke.
By this point. the smart Enterprise IT Guy is polishing up his resume and quietly moves across the street to a competitor. The developers who worked on the software are long gone as well so some poor engineer in a maintenance team gets to “fix” the problem — which usually means introducing a hack because he or she doesn’t really understand the intent of the original author.
Multiply this by two dozen hardware and software vendors and you see why the private cloud/local data center story/on-prem enterprise application deployment model is doomed to failure. The costs associated with the “old” model of computing are often not included in the math when opting for on-prem solutions.
Playing System Integrator to dozens of disparate pieces of hardware and software, owning and operating every level of the stack by people who don’t have access to the code, no longer make sense.
The war is over. Just like we gave up and standardized on one processor architecture and moved up the stack, it’s time to admit that there is much better hygiene in the public cloud world than there is in the spaghetti world of shrink wrapped on-perm software. Reducing the combinatorics increases reliability by reducing complexity.
Go up the stack, young man! Stop fretting about infrastructure, outsource it all to cloud based services, and move up the stack if you really want to add value to the business. And, in doing so, think at the application level, at the service level, not at the infrastructure level.
The only investment I would make in on-prem software at this point would be to improve utilization of existing infrastructure and applications. If it helps squeeze more out of the existing hardware and software, go for it. Otherwise, stop. Stop buying hardware, stop buying software, stop upgrading (except for security fixes), just stop!
Instead, go spend the time to understand the real requirements from the organization on the specific enterprise application. Take the top five requirements, find the cloud vendor that offers them most effectively, and start using it as-a-Service. Don’t worry about every random and esoteric feature that your employees currently use. They’ll figure out how to do their job some new way. Worry only about really nailing the top five requirements. If the rest of your requirements are really important, the software-as-service provider will sooner or later offer it — after the rest of the community-at-large has thoroughly tested it — and not as some one-off feature that you get to be the guinea pig for.
The sooner we all abandon the “old” model and move up the stack, the better off we all are.
And, if you’re in the on-prem infrastructure hardware or software business: Stop listening to your enterprise customers when they ask for bespoke features. You’re not helping . Chances are, you will build a feature that doesn’t actually do what the customer really had in mind, will divert crucial development and test efforts that are doomed because they are guaranteed not to mimic the customer’s kaleidoscopic and unique environment, and will end up disappointing everyone in the end.
IT personnel will correctly point out that they are often powerless when it comes to making such major architectural changes. The purse strings are held by lines of business within the corporation, they only get to implement what the various Business Units want. Having spent millions of dollars on data centers and related infrastructure, those decision makers are reluctant to abandon the status quo for the promise of the cloud.
The good news here is that developers in those same BUs are already moving to the public cloud in droves. They don’t want to be bothered with infrastructure details or delays in hardware procurement, storage allocation, and network reconfiguration required to shoehorn a new application into an existing data center. It’s so much easier to pull out your credit card and buy some capacity on a public cloud. Why fill out forms and wait weeks when you can start coding in minutes? It’s only when the application is ready for deployment that the IT team is consulted.
Given this trend, it will only be a matter of a few years before all new applications are cloud native and the on-prem infrastructure is relegated to the dust bin of history or at best begrudgingly maintained for legacy application support.
My recommendation to IT personnel, in this case, is to avoid adding more load, more users, more applications to the existing on-prem infrastructure. Cap the investment and aggressively move new applications and users to the cloud. Buying more hardware will only increase your depreciation budget over the next few years, thereby further reducing your ability to cut the overall Capex expenditure. If you need additional capacity during the transition, try renting bare metal servers from public clouds instead of building new data centers or upgrading existing ones with new hardware. This at least gets rid of part of the Capex problem and gives you a chance to validate the cloud provider’s reliability and availability while you wait for the next hardware refresh cycle. This, by the way, is the only way in which a “hybrid cloud” strategy makes sense: the outsourcing of hardware to public cloud providers for bursting or failover instead of amassing vast quantities of hardware on-prem just in case you need it.
I fully recognize that the enterprise application market has a very long tail. There are still many companies out there benefiting from the IBM mainframe based market. Many others will continue to flourish for the next decade or two (at least) on the on-prem infrastructure hardware and software market.
But its days are numbered and we all need to step up and rethink our Enterprise applications in the process. We might as well start with a platform (the cloud) that is twenty years newer and fresher in terms of architecture. As opposed to continuing to spend 80–90% of our budgets on perpetuating the legacy enterprise stacks that were designed and implemented in the eighties and nineties.
Here’s the rub, though: To do so will require really sitting down and understanding the top requirements on a per-application level. As opposed to assuming that 100% backwards compatibility trumps all others.
So much has changed over the past two decades. We have learned so much about availability, about reliability, about distributed systems architecture, about telemetry, about analytics and about security. Trying to shoehorn all of those learnings into a dated deployment architecture and a monolithic code base is like wearing a straight jacket and then picking a fight with Mike Tyson. You know it’s not going to end well.
A new generation of startups are disrupting every industry on the planet: not just consumer brands but also enterprise brands like Workday and Salesforce and Atlassian are becoming the standards. I can’t think of a single new startup that concentrates on on-prem software alone. They may offer an on-prem version of their product but all of their development and testing efforts are geared towards cloud based solutions. Carry that trend forward a couple of years and you will see the end of the traditional model.
The startup community and the VC community have spoken clearly. The former are now running world-wide operations and delivering services to millions of customers while the latter have bet heavily on their eventual success. Some small subset of these startups will become the IBMs, Microsofts, and Oracles of tomorrow — and they will get there with 100% born in the cloud software stacks. In fact, they are already delivering enterprise-class software to thousands of enterprise companies and millions of end users.
Who do you think will be more agile five years from now? The enterprise companies who amassed their own data centers and spent their time being System Integrators for the old guard or the ones who bet on the next generation computing platform — the cloud?
Other related blogs:
medium.com
medium.com
blogs.air-watch.com
Former {CTO at VMware, VP at Microsoft, Head of Engineering & Cloud Ops at Cloudflare}. Recovering long distance runner, avid cyclist, newly minted grandpa.
231 
2

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
231 claps
231 
2
Former {CTO at VMware, VP at Microsoft, Head of Engineering & Cloud Ops at Cloudflare}. Recovering long distance runner, avid cyclist, newly minted grandpa.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/starbugs/public-cloud-network-%E5%85%A9%E4%B8%89%E4%BA%8B-%E6%B2%92%E6%9C%89%E6%9C%80%E5%AE%89%E5%85%A8%E5%8F%AA%E6%9C%89%E6%9B%B4%E5%AE%89%E5%85%A8-a6b3eee059cc?source=search_post---------1,"There are currently no responses for this story.
Be the first to respond.
最近花了一些時間在整理網路相關的工作任務，趁著這個機會把自己透過工作碰到學到的 Public Cloud 網路議題給整理成文章；而不同的網路架構規劃其實都有他存在的原因，雖然大家都會想要朝向最佳實踐去實作，但必須要考量在可以使用的人力物力下，怎麼做才是最適合當前的狀況，畢竟只有最適合的解決方案，而沒有最好的，適時的欠下技術債也是在這個領域必須要學習的課題與必要之惡，所以底下的文章內容透過情境設計對應到在不同的公司規模或是需求之下，可以選擇什麼樣的解決方案與替代做法
老闆: 我們先用 Public Cloud 比較快，機器點一下就有了，下週把網站弄出來，我要跟外面的人 Demo
通常在新創公司，比較不會去考慮到網路安全這一塊 (除非是比較特別的產業)，因為先思考怎麼活下去，Time to Market 才是最重要的，所以會以先把 MVP (Most Valuable Product) 做出來為最優先考量，要做的就是立即把老闆的信用卡綁到公有雲端網頁管理介面裡，機器開起來後，把開發好的程式部署上去
公有雲一定會考量到要讓開發者馬上可以使用的需求，所以都會先幫忙預設建立好網路架構的部分，不可能讓使用者還要先去研究 Internet, Intranet，IP 分配…等網路相關議題後，才能夠把機器給開起來，這樣應該就已經兩三天過去了，老闆正氣噗噗地站在後面看著你
好處就是快！因為不需要具備任何的網路知識跟花時間去建置，只要開發人員的網路會通就沒問題了，但其實缺點很明顯就在於開起來的機器會配置 Public IP，也就是說…你連的到，別人就連的到！
雖然 Public IP 就是配置在那邊，但假如行有餘力的話，還是有一些做法可以選擇:
老闆: 為什麼我們的資料庫跟伺服器可以被外部隨意存取到？要是客戶資料被偷怎麼辦？
當公司規模慢慢成長之後，在 Cloud Provider 預設提供的網路設定裡部署了越來越多的服務，然後使用上面提到的 Allowlist 方式在 Network OSI Layer 3~4 佈下了層層關卡，但問題是 Public IP 就還是在，人為疏失就一定會發生，一不小心就是有人開新的機器沒有設定妥當，或是修改既有的設定出了包，這時候就要開始把 Intranet 的觀念帶進來，然後開始花人力物力去做網路規劃與佈置
好處顯而易見，一般使用者不需要存取到的雲端資源，不再配發 Public IP，所以理論上只有公司員工可以存取的到；那缺點就是要有人負責前期的規劃設置與後期的維護管理，因為要走 Site2Site VPN 所以要做好 IP 區段分配，避免 IP 位址衝突，並且要隨時確保 Site2Site VPN 和 VPN Client 服務保持健康運行，不然不只惡意人士連不到，連自己內部員工也都連不到 (這樣最安全了XD)，再來是 Site2Site VPN 透過 Public Internet 建立而成，在穩定和安全性上或許有些人並不買單
上面提到有關 VPN 的缺點有沒有什麼做法可以去克服呢？其實還是有的，底下會列舉一些自己知道的方式
老闆: 現在有辦法檢查所有的對外流量嗎？確保沒有連線到不該連的地方去？下週外部稽核可能會問到這塊喔！
把網路切割成 Public 和 Private 讓外人存取不到內部資源，並且透過 AWS Security Group, Azure Network Security Group 或是 GCP Firewall Rule 去控制網路存取之下，還是不夠嗎?! (搔頭並且眼球佈滿血絲)，就像標題說的，沒有最安全，只有更安全XD 在開始提到解決方案之前，首先要檢視目前所有的對外連線，然後思考兩個問題，1) 這個請求不連到外面的話，在 Intranet 內有辦法被滿足嗎？2) 假如無法被滿足一定要連到 Internet 又該如何確保連線的安全性？接著來看看會有哪一些做法：
優點顯而易見，就是在 Security 的路上又往前邁進了一步，當惡意人士想要幹壞事時，他可以做的事情來得更為限縮並且困難，從攻擊目標偷到東西的 CP 值變得更低；而且可以發現要從 0 分做到 60 分 (Phase 1 -> Phase 2)，跟從 60 分做到 80 分 (Phase 2 -> Phase 3) 其實要花的力氣跟成本是不一樣的；缺點就是要怎麼維護這些多出來的服務跟架構，後續的維運跟管理成本都不小
Infrastructure as Code: 當有一堆的設定需要做管理時，人為操作一定會有疏失產生，所以要怎麼最有效地降低發生的機率?! 自己覺得 IaC 是一定要的，因為它可以明顯地增加變更的能見度與正確性 (Code Review, Change Drift Review)，對於變更管理有很正面的幫助；而且可以再更近一步接著導入 PaC (Policy as Code)，讓問題發生之前就被發現，不再需要亡羊補牢，而是能夠達成未雨綢繆的效果
Buy Enterprise Solution: 正往這個階段邁進的話，當需要在免費開源軟體跟付費商用軟體中間做抉擇時，假如彼此的 Gap 不小且售價又可以接受時，不妨就直接花錢解決問題，因為購買商用軟體除了可以立即解決當下的棘手問題之外，通常有時候想要買的是商用軟體後續的維護跟支援 ；當想要選擇免費開源軟體自己來時，不妨先思考自己公司的核心價值跟競爭力在哪邊，然後估算投入的人力資源跟時間是否合理
上面 Phase 3 Solution 有提到把具有 Network OSI Layer 7 過濾功能的防火牆搬到 Public Cloud 上面去，之前自己曾經跟同事做過這件事情，心得就是…累死我了XD 因為除了整個網路架構要翻上一輪之外，再加上自架 Firewall Service 的 Upgrade，High Availability, Scalability…等，真的會讓人厭世 & 眼神死，好加在近期 Cloud Provider 都有提供類似的服務出來了 (撒花)，例如 AWS Network Firewall 和 Azure Firewall，底下以自己近來使用 AWS Network Firewall 來做心得分享
雖然對外是一個服務，但其實裡面由兩個重要的元件組成，分別是 AWS Gateway Load Balancer 和 Firewall Rule Engine
所以兩個元件合起來之後，使用者需要做的就是更改 AWS VPC Route Table 把對外流量通通導流過去，然後設定好防火牆規則就行了，網路架構不用大改，也不用去煩惱防火牆的維運問題，不過假如使用者覺得 AWS 提供的 Firewall Rule Engine 不符合公司需求，這時候就可以自己架設 Firewall Service 然後一樣透過 Gateway Load Balancer 來完成 VPC 內對外網路流量的過濾與分析
透過 AWS 提供的 Network Firewall 解決了大部分棘手的問題，那 Firewall Rule 要從哪邊來呢？
相信還有不少主題是此篇文章沒有提及的，歡迎大家一起提出來討論，畢竟 Public Cloud Network Security 沒有最安全只有更安全，因為沒有絕對安全的架構或是機制，但攻擊方會去衡量為了入侵所需投入的成本是否划算，例如花一千萬去奪取一百萬的資料就不太合理；而在解決方案的抉擇上只有最適合而沒有最好的，因為大家都只能透過當下所擁有的資訊和資源盡量去做出相對合理的選擇
一群技術人想要寫出一些好文章所建立的技術專欄。每週二一篇原創文章、一封電子報，歡迎大家訂閱！主網站： https://weekly.starbugs.dev/。
343 
343 claps
343 
Written by
原來只是一介草 QA，但開始研究自動化維運雲端服務後，便一頭栽進 DevOps 的世界裏，熱愛鑽研各種可以提升雲端服務品質及增進團隊開發效率的開源技術
一群技術人想要寫出一些好文章所建立的技術專欄。每週二一篇原創文章、一封電子報，歡迎大家訂閱！主網站： https://weekly.starbugs.dev/。
Written by
原來只是一介草 QA，但開始研究自動化維運雲端服務後，便一頭栽進 DevOps 的世界裏，熱愛鑽研各種可以提升雲端服務品質及增進團隊開發效率的開源技術
一群技術人想要寫出一些好文章所建立的技術專欄。每週二一篇原創文章、一封電子報，歡迎大家訂閱！主網站： https://weekly.starbugs.dev/。
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nabtechblog/infrastructure-in-the-new-age-nabs-journey-to-the-public-cloud-793b7c7884fc?source=search_post---------2,"Sign in
There are currently no responses for this story.
Be the first to respond.
National Australia Bank
Nov 23, 2018·4 min read
Rather than sinking resources into keeping the lights in a private cloud environment, we are fully embracing the public cloud — letting the cloud giants do what they do best, while we invest in the future and focus on meeting our customers’ needs.
There’s really no such thing as a ‘private cloud’; there’s the public cloud and there’s managed hosting. Private cloud really just means managed internal hosting, where you lose many of the benefits of the public cloud.
What people refer to as the ‘private cloud’ is not nearly as scalable, elastic or customisable as the public cloud. It leaves the operations teams securing the perimeter and maintaining the infrastructure — when they could be focused instead on innovating and delivering more for customers, while leveraging the world-class infrastructure and security public cloud providers have.
What our customers do care about is secure and reliable services.
The best way to achieve this is with the public cloud. By leveraging the scale of the world’s cloud leaders, NAB can provide far better security, resilience and performance than through on-premise systems. Adopting this approach enables us to move much faster, to be more agile and more resilient, all without having to make large upfront capital purchases.
Going public
Today, banks are understandably slow and cumbersome when it comes to IT. We’re a 160-year-old organisation; it’s extremely hard to be agile with an infrastructure that’s physical in nature and has a lot of processes around systems built up over many years of technology and regulatory change. Once we start to move to ‘infrastructure as code’, we can be far more agile in our approach.
We initially plan to move 35% of our infrastructure into the public cloud, and much more in the long term. We will consult with our regulators at a speed they’re comfortable with, it’s important to us — we’ve invested heavily to work with our regulators on the journey so far and committed to this lock-step approach in the future.
In return, the public cloud will enable us to develop products and services more quickly, products that are far more in tune with new technology and what customers are asking for.
NAB’s program to test out the possibility of using machine learning and facial recognition for ATMs recently made headlines. We built a demo (at the Sibos global finance exhibition of technology) that lets customers withdraw cash without a card or their phone. While it was just a proof of concept, we were able to build that entire environment in a very short space of time — effectively days. We didn’t need to create an entire facial recognition platform — instead, we leveraged the machine learning capabilities already within Microsoft Azure.
The same goes for NAB’s data lake built on AWS, letting us make the most of the bank’s structured and unstructured data. While you could deploy this kind of technology on other styles of cloud infrastructure, the public cloud lets us take advantage of the impressive data analytics AWS has rather than trying to reinvent the wheel ourselves.
Greater efficiency
Some people might argue that the ‘hybrid cloud’ offers the best of both worlds, by combining privately managed and public. I would argue that you’re watering down the right solution by bringing in the private element.
Some might argue for storing specific data internally but, with the right strategy, keeping your data secure with a major cloud provider is far more effective, particularly if you invoke a multi-cloud solution that takes advantage of several key cloud players.
A multi-cloud solution doesn’t need to add complexity, not if you architect your solutions appropriately. Once infrastructure is actually code, much of that infrastructure can be replicated easily from one cloud service to another.
In terms of data jurisdiction, all the major cloud providers now have Australian operations and allow you to choose where your data is held. This provides choices and control over data location to meet obligations and business requirements.
Embracing change
We continue to work closely with our regulators on our approach — a crucial approach as cloud technology’s advanced.
As our industry and the environment constantly changes, business leaders are eager to learn more about the benefits cloud technology can bring. The NAB board is leaning in to the technology. We took the entire board to Silicon Valley earlier this year to meet with Google, Microsoft and Amazon, where they were able to gain a real appreciation for where the benefits lie.
The public cloud is not an experiment, nor is it a flash in the pan. Instead, it’s an evolution of how businesses consume IT moving forward. While NAB doesn’t necessarily want to be on the bleeding edge, we certainly want to be at the forefront if we’re to compete within the Australian market. We have a big program of work coming up and we’re genuinely excited about what the future is looking like.
If you’re interested in joining us on the ride, you can learn more about working in technology at NAB here.
About the author: Steve Day is NAB Executive General Manager for Infrastructure, Cloud and Workplace. Before joining NAB, Steve held senior positions at Telstra and Amazon Web Services.
Official account of NAB’s technology team. Inspiring readers to rethink bank technology and bringing to life the technology enabling change for customers.
77 
77 
77 
Official account of NAB’s technology team. Inspiring readers to rethink bank technology and bringing to life the technology enabling change for customers.
"
https://blog.heptio.com/perspective-on-multi-cloud-part-2-of-3-on-premises-plus-vs-public-cloud-282818574643?source=search_post---------3,"This is the second in my series on multi-cloud. For this post I am going to dig into the characteristics of on-premises deployments vs public cloud deployments. You can find Part 1 here.
Let’s tease the public cloud value proposition apart into its pieces:
Don’t underestimate the economies of scale and the resulting purchasing power that cloud providers enjoy. The most simplistic view of cloud is as outsourcing your infrastructure management problem. Why not let the experts with buying power and scale deliver your basic infrastructure as a service?
The truth is that while cloud is making good progress in terms of cost effectiveness, the economics are still developing. For sophisticated users with relatively predictable workloads there are still cost advantages to running your own infrastructure. However, this will likely change.
Friendly piece of advice 1: You may be able to achieve transient improvements in efficiency vs the public cloud today, but it will become increasingly difficult to maintain those advantages as the cloud providers continue to grow and compete.
Instead of filing a ticket, you call an API or use a self-service console. Public cloud is also great for driving infrastructure agility. Amazon EC2 is short for ‘Elastic Cloud Compute’ for a reason. You can right size your infrastructure footprint to your workload in real time. The bigger thing to think about here is that cloud delivers infrastructure available on demand with an almost instant turn around time. The impact on organizational agility can often more than offset the incremental cost. Many organizations worry that this represents a loss of control and oversight (and there is some truth to that), but the cloud providers will continue to bolster their governance offerings, policy driven provisioning and auditing practices.
Finally, what about managed services? Clouds provide access a broad set of expertly managed services delivered through a provisioning API. Some of these can be hard to replicate without expert support. You probably can’t afford to buy atomic clocks for your data centers and without them you are simply not going to be able to build something like Google Spanner.
Friendly piece of advice 2: It isn’t truly a private ‘cloud’ if (1) your developers can’t provision infrastructure on demand, (2) your developers can’t provision a robust and useful set of standard services on demand.
For many, security is a primary driver for private infrastructure. However, the hard truth is that you will not be able to match the security posture of the major cloud providers. I can say this having had experience with the excellent security folks inside Google. Unless you are working for one of the wealthiest and most paranoid companies you simply will not be able to staff a security organization that comes close to the caliber, talent, or attentiveness of these teams. This isn’t to say that there aren’t great security teams out there in the wild, it is just a complex, multi-dimensional field. The depth of these security teams is another example of economies of scale benefiting the the major public clouds. They can average out the cost of a huge, world class security team over lots of customers.
Regulation may still be a reason for private infrastructure. While I believe eventually regulation will follow the pragmatic realities of economics, it is a slow moving space and will take a long time. For the immediate future, some highly regulated workloads are going to have to remain in private data centers. This may justifies a phased approach to public cloud adoption. Use public build new, unregulated systems. Or start by moving simpler elements over time.
Highly connected devices are driving demand for edge based computing. With many more, and more highly connected devices it becomes a question of balancing the cost of connectivity vs the availability of edge resources. As IoT scenarios become mainstream, we can expect to see a lot more compute showing up at the edge. It behoves organizations to have a strategy to deploy and manage applications there. Some of this compute may be on your premises, some will be in carrier managed facilities, and no doubt the cloud providers themselves will get in the game.
The final and perhaps most obvious point in favor of a hybrid strategy is that your current data centers aren’t just going to go away. An ideal solution would allow developers to use the same set of tools, practices and workflows to operate in both worlds. Perhaps leveraging the public cloud for testing, then deploying your production workload to the current environment.
We really haven’t seen offerings that bring together the world of on-premises deployment topologies at the IaaS level. The AWS/VMware partnership looks interesting, but it isn’t clear to me how that will work in the long run. It feels like there is a decent amount of overlap between what each providers that needs to be worked out. Even if it ends up making sense, it doesn’t bring all the attributes of cloud back to your local datacenter per my earlier point; you don’t get cloud like services on your infrastructure. OpenStack, while promising for on-premises use, really doesn’t have a public cloud analog.
Friendly piece of advice 3: Think about technologies like Kubernetes, Cloud Foundry, or Mesosphere’s DCOS. These were developed from the ground up as a way to decouple workloads from the underlying infrastructure and are a start to a true hybrid offering. The story should include not just basic infrastructure primitives, but a set of services that someone (you, or a provider) can provide to your team that are provisioned by API (vs ticket). Obviously, at Heptio, we believe that Kubernetes does this particularly well.
In many cases it makes sense to look at public cloud as a supplemental source of compute and for new workloads. For many companies, cloud starts with dev and test before deploying on private hardware. Unfortunately this is not as simple as preparing a bootable image that runs in different environments.
The operating system itself naturally lends some level of consistency from environment to environment, and we are seeing ‘vm image bakeries’ emerge as a way to address this problem. CI/CD systems can now produce a bootable image that will run in multiple environments. The harder nut to crack is how to connect the parts of an application together. The code itself must have some way to connect to and consume services that it depends on.
Unless you live in a perfectly monolithic world there is value in thinking about environment agnostic ways to tie the pieces of complex systems together. Each new environment today includes not only new operations tools that force your operators to learn new schools, but introduces eccentricities that are difficult to account for in the application architecture.
Friendly piece of advice 4: Look at an abstraction layer as a way to become ‘cloud ready’ even if you aren’t ready to move all your applications to the cloud today. Micro-service friendly frameworks like Kubernetes help to ensure portability of both applications and skills. For the best results look beyond a simple portable image to a broader definition of application. That grants you an option on moving a workload later if needed. The platform should include the ability to find and bind to services easily. It should also support the decoupling of application pieces so that they may be moved over time.
I hope you found this perspective useful. Obviously, it is a complex space and there are a tremendous number of moving parts.
For the next post in this series I will dig into questions about availability, failure zones, federation and the complexity of operating services across multiple clouds and locations. It will be a little more technical than the series thus far.
Heptio
87 
Thanks to Joe Beda. 
87 claps
87 
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
"
https://towardsdatascience.com/battle-for-the-king-of-public-cloud-aws-vs-azure-9c5505dabb1e?source=search_post---------4,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
Ryan Gleason
Jun 17, 2020·6 min read
The public cloud is taking over whether you like it or not. And what’s not to like about it?
Cloud computing has made it incredibly easy to manage all of our computer system resources. It has provided a layer of abstraction that seemed impossible before 2006.
No longer do we have to manage our underlying infrastructure physically. These developments mean we no longer have to:
Which enables us to:
The speed at which our business can now scale is unprecedented. Never before have we had the ability to go from planning to production so quickly.
But like everything in the world of technology, once there is something new and cutting edge, everybody has to make their version of it.
We are going to look at two of the leaders in the cloud computing space: Amazon Web Services and Microsoft Azure.
Answer the question: Who should I choose to host my business’s software development needs for the next ten years?
AWS and Microsoft Azure have emerged as two of the leaders of this space. So today, we are going to take an in-depth look at these two and determine which is better.
Cloud Computing traces back to the ’60s when “virtualizing” different aspects of the computer was first achieved. So it’s not correct to say Amazon started cloud computing.
But as far as Infrastructure as a Service (IaaS) and the way we know it today, Amazon played a massive role in pushing this to the masses.
What is IaaS? The ability to offer raw computing capacity as a service and being able to access and provision new servers and storage over the internet.
If it weren’t for Amazon, where would we be with cloud computing? It’s hard to imagine we would be this far along.
Starting back in 2006, Amazon launched Amazon Web Services. Their first services being S3 (Simple Storage Service), EC2 (Elastic Cloud Computing), and SQS (Simple Queue Service).
These services allow you to store your data in online storage through an online web service interface (S3), communicate from web app to web app (SQS), and spin up new servers on demand (EC2).
Since then, as of 2020, AWS has more than 212 services. These services cover just about everything you could ever need to grow your IT business. Services span databases, networking, mobile, developer tools, Internet of Things (IoT), machine learning, AR & VR, Blockchain, Quantum Technologies. The list goes on and on.
With the development of these services, the cloud computing game would be forever changed.
Getting a bit later of a start, Microsoft Azure originally started as “Project Red Dog” in 2008 and officially became “Windows Azure” in 2010 when it was delivered commercially. This was when it began to gain momentum.
Azure sought out to offer many of the same services AWS did, including virtual machines, object storage, and content delivery networks. However, the services were designed specifically for Microsoft products.
Meaning if your business were built using the Windows stack (Microsoft SQL Services, Microsoft .NET Services, Live Services, Sharepoint, etc.), Azure would be instrumental for you.
Realizing that they needed to expand past the Windows stack, Azure started to offer support for more programming languages, frameworks, and Operating Systems.
Because it was no longer a tool for just Windows, in 2014, they rebranded as “Microsoft Azure.”
Similar to AWS, Azure provides Infrastructure as a Service (IaaS), software as a service (SaaS) and platform as a service (PaaS).
How can you get started?
Becoming certified in either one of these cloud technologies can prove to be incredibly valuable for you and your career, and be a great starting place for anybody looking to learn more about the cloud.
Trying to decide between Solutions Architect and Developer certifications? Click here for my comparison of the two.
If you already know which certification you would like to pursue, I’ve written articles on how to get started with the Solutions Architect and Developer certs.
Check out the complete list of certifications for AWS and Azure.
I wanted to talk about pricing and do a feature breakdown; however, each of these cloud providers is continuously rethinking their price models and releasing new features. What is true today regarding pricing and feature availability may not be true tomorrow.
But back to our question: Who should I choose to host my business’s software development needs for the next ten years?
Of course, it depends. If you are dealing with an already existing Microsoft stack, I would recommend Azure.
But if I was building my business from scratch and I had to choose just one, I would choose AWS. Why? Because of AWS’s maturity level and training opportunities. I feel safer moving forward with AWS as my cloud provider.
If I’m onboarding a new engineer and they know nothing about the cloud, they can quickly learn by pursuing AWS certifications. The top certifications out there right now are AWS Solutions Architect and AWS Developer, and they are high in demand.
That isn’t to say Microsoft isn’t offering certifications. They are. They just haven’t reached the level of maturity AWS has yet.
The lack of maturity extends to documentation and technical support as well. Learning how to use a new feature or troubleshooting in AWS has become simplified due to the overwhelming support from AWS themselves and the community.
Of course, you could always use a multi-cloud strategy and use both. You do not have to choose just one.
But for the sake of this discussion, if I had to choose just one, I would select AWS because of where they are at the moment in terms of maturity.
medium.com
medium.com
towardsdatascience.com
Giving back to the dev community one article at a time!
See all (22)
177 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
177 claps
177 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/adobetech/cost-efficiency-in-adobe-experience-platform-pipelines-public-cloud-spend-ec708fdadd91?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
This is a follow-up to our previous blog about public cloud cost management for Adobe Experience Platform Pipeline. The previous blog covered an overview of Adobe Experience Platform Pipeline, a breakdown of expenses, a plan of action, and the need for a cost governance program.
We recognize that both technology vendors and our enterprise customers (CIOs and IT organizations) have been faced with public cloud cost management. Building upon that, we focused on defining a cost optimization framework that establishes an iterable approach with sustainable long-term impact. In this blog, we will share our progress on how we reduced annual public cloud COGS spend by 40% without incurring any downtime.
We began by defining the following goals which would serve as our guiding principles as well, to the cost savings approach.
Once we had established clear goals, our next step was to outline a cost optimization framework with a three-phased approach.
The initial focus for Adobe Experience Platform Pipeline was on providing functionality and performance, with costs as an afterthought. Since it migrated from bare metal servers with long procurement cycles and 3 times the capacity needed for optimal utilization, the same philosophy of overprovisioning to buffer poorly defined traffic patterns was taken into the public cloud realm.
This led to production environments running at less than 10% utilization. It simply wasn’t designed to take advantage of the ease of scalability on the public cloud.
Another major contributing factor was excessive provisioning for anticipated use-cases. Pipeline was scaled up well in advance to handle traffic volumes that were 10 times in size as existing use-cases. There were multiple production-scale environments built-in AWS for such use cases with dozens of brokers and PBs of storage.
Application-level self-servicing was also enabled for client provisioning and administration of Kafka topics.
Having a clear understanding of the spend patterns led to identifying the biggest cost drivers, which of course were the massively overprovisioned sites for overly optimistic traffic projections resulting in underutilized Kafka clusters.
From a resource perspective, most of the spend was covered under:
With the biggest cost drivers outlined, we enumerated a list of actionable tasks as our next steps:
While executing on this cost optimization plan, we ran into certain challenges. Here is an overview of issues that we faced and how they were managed.
The first challenge was to identify the correct resource types (Disks, VMs, and Container sizes). The solution for this came from within our Cost Governance Program. As part of a dedicated task force, an efficiency engineering toolkit was developed for AWS and AZURE sites. It analyzed utilization over time and recommended rightsizing for resources based on metrics like CPU utilization, server load, network load (Bytes IN/OUT), disk capacity, and I/O operations along with utilization spikes. In addition to right-sizing recommendations, we also purchased Reserved Instances to cover 90% of the usage and these were adjusted monthly based on changing requirements.
The second challenge we encountered was to change a live system with minimal customer impact and dealing with any client issues while doing so. We handled this issue using the following approach:
The third challenge we had was changing infrastructure provisioning code in place for a live environment and to keep it in sync for ad-hoc changes. Terraform was used to provision both AWS and AZURE infrastructure. To overcome the problem, we added new Terraform modules for managing disk attachments and used them to add extra disks; and then cleaned up the old disks and attachment code after the changes were complete. It was only feasible for smaller Kafka clusters as it led to the next problem.
In the end, we had to deal with cost spikes while making changes to large clusters. With our flat infrastructure provisioning code, the changes like new disk attachments were applied to the whole cluster at once. This caused a spike in our daily run-rate as we could only safely work on a small subset of brokers in parallel. We resolved this problem by using custom Python/Bash scripts to do ad-hoc changes to 3 brokers at a time and removing old disks before working on the next set of brokers. This helped stabilize our daily spend run-rate in a downward trend. Further, ad-hoc changes were imported into Terraform to keep it in sync.
Having dealt with the challenges while implementing the infrastructure changes, Pipeline’s engineering team continued to look into other avenues like application design to further reduce our public cloud cost footprint.
The team revisited application design and architecture to be cost-effective by optimizing certain aspects from a few years ago like the removal of unused shadow topics used for replication. This helped optimize our broker usage by bringing down CPU utilization of VMs as each Kafka broker now needed to handle a smaller number of partitions. We also standardized application deployments on Kubernetes and rightsized these deployments to increase container utilization.
The Pipeline team also addressed a major network expense incurred during repeated consumption of the same data, by creating a virtual view to optimize network bandwidth. More in a later blog!
The Pipeline team was able to bring down its public cloud cost footprint by 40% while growing 30% YoY, focusing not only on established fundamentals of operational optimizations but taking it a step further by revisiting its application needs.
We plan to continually improve processes with a cost-first mentality, starting with onboarding every use case in alignment with cost efficiency practices via Pipeline’s self-service portal.
We will also be reducing the default data retention to one day, and three days with exceptions. Another major undertaking is to make clients cost-aware by providing them access to topic-level costs. We are massively invested in automation and infrastructure code optimizations while completely migrating to container-based deployments in Kubernetes for all the Pipeline services including Kafka.
Finally, we are working towards having robust 2-way autoscaling based on custom usage metrics with right-sized Kubernetes environments for automated cost-effectiveness.
As highlighted in the previous blog post, the fundamental seismic shift for cost management is happening now with the successful establishment of our Cost Governance Program.
We anticipate continued success for this program as teams are embracing the culture of cost first, across the board in Adobe.
Follow the Adobe Tech Blog for more developer stories and resources, and check out Adobe Developers on Twitter for the latest news and developer products. Sign up here for future Adobe Experience Platform Meetup.
News, updates, and thoughts related to Adobe, developers…
11 
11 claps
11 
News, updates, and thoughts related to Adobe, developers, and technology.
Written by
Experience Technologist. Currently Director of Developer Content Engine for DataStax. Former Developer Advocate for Adobe Experience Platform. Twitter: @jaeness
News, updates, and thoughts related to Adobe, developers, and technology.
"
https://medium.com/@benbob/cloudy-with-a-chance-of-bicycles-a-conversation-with-enterprise-it-professionals-80ef40e2209d?source=search_post---------6,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Fathi
Aug 1, 2018·13 min read
“I consider the bicycle to be the most dangerous thing to life and property ever invented. The gentlest of horses are afraid of it.” — Samuel G. Hough, General Manager of the Monarch Line of steam ships. July 14, 1881.
"
https://medium.com/@kyleake/cloud-comparer-a-public-cloud-comparison-ilyes-68d1d9772a7a?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
May 29, 2019·2 min read
Disclaimer: This is my personal Medium blog, therefore anything I post, share, and comment don’t reflect my employer.
Hello all, this post is nothing but to promote “Cloud Comparer by Ilyes”, which compares the various managed cloud services offered by the major public cloud providers in the market — AWS, Azure, Google, IBM, Oracle, and Alibaba. “The…
"
https://engineering.salesforce.com/how-we-operate-kubernetes-multitenant-clusters-in-public-cloud-at-scale-548d9ca48d36?source=search_post---------8,"Salesforce took a very early bet on Kubernetes (K8s) in 2015 to help us begin the journey from monolith to microservices, and we’re happily using it today across product lines and business units. Over the last five years, we gave teams the freedom to adopt K8s as they saw fit. So, teams across the company spun up clusters and created customized configurations, which…became costly and difficult to manage. Teams also had varying levels of K8s knowledge and expertise, and they weren’t all able to dedicate staff time to the operational overhead required to run a cluster. We have many stories we could share about things that we learned the hard way through long debugging processes. Imagine spending hours digging into an intermittent connectivity failure issue only to discover the problem had been caused by a sysctl flag that had been set to 0 in a naive attempt at optimization, when it should have been set to 1!
This incident and others helped us realize we needed uniform practices, tooling, and investments. From automation to visibility to security and network monitoring, we needed solutions that applied across all of the large-scale, multi-tenant clusters running across the many regions within Salesforce. Enter the central Salesforce Kubernetes Platform team.
Read the full post by Prabh Simran Singh, Lead Software Engineer, on VMBlog
Salesforce Engineering Blog: Go behind the cloud with…
13 claps
13 
Written by
Find out what's current with the engineering groups at @salesforce.
Salesforce Engineering Blog: Go behind the cloud with Salesforce Engineers
Written by
Find out what's current with the engineering groups at @salesforce.
Salesforce Engineering Blog: Go behind the cloud with Salesforce Engineers
"
https://medium.com/@jaychapel/the-latest-public-cloud-market-share-and-beyond-16d686c0fc25?source=search_post---------9,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Nov 18, 2019·3 min read
With another recent round of earnings reports from Amazon, Microsoft and Google out of the way it’s always enjoyable to stand back and see what we can discern about the public cloud market share.
According to Synergy Research Group who closely monitor such trends, they saw 37% overall growth year-over-year in public cloud. They reported that it has taken just two years for the public IaaS and PaaS markets to double in size and their forecast shows them doubling within the next three years. Within the overall market it is possible to discern some interesting trends amongst the top three providers, which we discuss below.
Amazon Web Services (AWS)
Last Thursday, Amazon reported that its cloud division revenue increased 35% in the third quarter, which was down from 37% in the previous quarter, and its slowest growth rate in five years. AWS finished its third-quarter with $9 billion in revenue. Each of the three previous quarters also showed a decline in growth which can be seen below.
Microsoft Azure
Microsoft followed AWS’s report with Azure reporting a revenue growth rate of 59%. In a similar vein to AWS, growth was reported as slowing and was down on the previous quarter which was 64% and down from 76% from a year ago. While Microsoft doesn’t break out specific revenue amounts for Azure (unlike AWS) Microsoft did report that its “Intelligent Cloud” business revenue increased 27% to $10.8 billion, with revenue from server products and cloud services increasing 30%
Azure also hit the headlines around the same time as their earnings report with the announcement of their securing the lucrative, high profile and highly contested $10B Pentagon’s JEDI Cloud contract. This was viewed as a key strategic win for the company and a game changer in the face-off with AWS.
Google Cloud Platform (GCP)
Last to report were Google’s parent company Alphabet. During their analysts call a few references were made to overall performance which included the Alphabet CEO calling out Thomas Kurian, who leads the GCP business, in saying “Obviously, ever since Thomas has come in, he has continued to invest across the board. He’s definitely focused a lot on scaling up our sales, partner and operational teams, and it’s playing out well”. Furthermore, it was reported that GCP had hired more sales, engineering and product managers, and that GCP, analytics and compute would continue to be a focus of the company’s investments going forward.
GCP falls into Alphabets “other” revenue bucket, which includes Google Play and hardware. Of businesses, GCP had the highest revenue. Other revenue was $6.43 billion in Q3, which was a 39% increase over $4.64 billion a year ago. There is no doubt that the cloud business is the largest of the three but Alphabet didn’t break out more specific numbers for cloud.
Other Providers
Some of the companies outside the big three, including Alicloud, IBM, Oracle, etc. are all growing, but they continue to lose ground to these three dominant market leaders. To compete, hyper-scale really matters, and these three bring that in spades.
Cloud in 2020 and beyond
As we enter the next decade a number of market watchers are speculating about what the reported slow down in growth means for the public cloud market share. As has been widely observed in other markets it’s a truism that as hyper-scale is achieved growth rates will decline. However, even with the overall reduction in growth they still exceed almost every other area within the broader technology market. As of Q3 2019, the overall quarterly run rate size was $25 billion, implying the annual run rate is now over $100 billion and still growing fast. It’s unlikely that there are too many other markets with better prospects going into next year.
Originally published at www.parkmycloud.com on November 12, 2019.
CEO of ParkMyCloud
See all (317)
7 
7 claps
7 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@bossjones/baking-rackspace-public-cloud-and-private-openstack-images-c57a1616d136?source=search_post---------10,"Sign in
There are currently no responses for this story.
Be the first to respond.
Malcolm Jones
Aug 5, 2015·3 min read
Today’s blog post has to do with every sysadmin’s favorite task, baking images (insert sarcasm). In particular we’re going to talk about how to use Packer to bake images for Rackspace Public Cloud, and Private Openstack Cloud.
NOTE: Assume we are using Packer version v0.7.5
The common issue I discovered with baking openstack images is that it is super easy to set MORE environment variables than needed. Setting too many can cause packer builds to fail with strange error messages. For example, as noted in this packer git issue:
If you look closely at the packer documentation, you’ll notice that both the OS_* and SDK_* env variables have different “weights” associated with them, meaning if you set too many at the same time, they can override each other. Rule of thumb is SDK_* has more precedence than OS_*.
I put the following into a small script full of environment variable named local-public.sh with contents:
NOTE: Notice for Rackspace Public Cloud I used only SDK_* environment variables.
Then in a new bash shell I ran source ./local-public.sh
Following that, I created a template that looks something like this:
NOTE: Notice I did not set any password/etc in the actual template. Packer reads in directly from your environment and populates everything accordingly.
Now packer build -only=rackspace-public openstack-centos66.json And boom! Good to go!
The steps for creating a private Openstack cloud image are very similar, but might require a little bit more information on your Openstack setup. Make sure you know how to query it for what you need!
I put the following into a small script named local-private.sh with contents:
NOTE: Notice for private openstack I used only OS_* environment variables.
Then in a new bash shell I ran source ./local-private.sh, make sure you don’t have any SDK_* variables set!!!!
Following that, I created a template that looks something like this:
Now packer build -only=rackspace-private openstack-centos66.json and boom, private openstack image! Hope this helps someone! Feel free to tweet me if you have any questions / suggestions!
Originally published at Medium.com/BehanceTech.
TONY DARK. Black Tony Stark. DevOps Engineer @Behance (acq’d by @Adobe). NUPE, WAHOO. Commonly referred to as #heron, SON OF BABYLON, Le #Hyena. I build shit.
See all (799)
6 
6 claps
6 
TONY DARK. Black Tony Stark. DevOps Engineer @Behance (acq’d by @Adobe). NUPE, WAHOO. Commonly referred to as #heron, SON OF BABYLON, Le #Hyena. I build shit.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/structure-series/how-ge-is-closing-datacenters-and-moving-to-the-public-cloud-e4a6cd6fd2fa?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
GE’s Chris Drumgoole caused quite a stir in cloud computing circles last year when he vowed that the industrial giant was going to move everything in its arsenal to the public cloud. While it’s definitely hard to turn a 123-year-old company with over 300,000 employees on a dime, GE is making more progress than you might expect.
“One hundred percent public cloud is still our goal,” Drumgoole said in an interview last week ahead of his upcoming appearance at Structure 2015 in November. “We’re on or slightly ahead of schedule with that march. It hasn’t been without pain, but almost none of the pain has been technical.”
As we discussed with Pinterest’s Raj Patel a few weeks back, even cloud-native companies have to build a culture around the use of cloud computing services. At GE, the engineers got on board (after a little grumbling), but getting legal to buy into the plan was much harder, especially given GE’s multinational customer base.
So how is GE pulling this off? Drumgoole said the company is basically triaging its apps: new apps are being developed on and for cloud services exclusively. Old and outdated apps running on servers are simply being shut down, and older but still useful apps that run on servers are being evaluated for migration to cloud services.
“We really don’t see the value in reinvesting in infrastructure for applications that can be run in public multitenant clouds,” he said. The move away from servers has allowed GE to shut down around a dozen datacenters over the past two years, and while the company will still have to operate several datacenters for the foreseeable future, the trend is clear.
So where is Drumgoole running those workloads? He mentioned Amazon Web Services, Azure, and Verizon, but said that there are several other cloud providers in the mix. And he had an interesting point about the international market for cloud services, getting back to the global business (“Brazil is an established market for us”) that GE operates.
“The market is yearning for a big non-US based cloud provider of that scale,” Drumgoole said, referring to AWS and Azure. Apparently the unease over the U.S. surveillance programs brought to light by Edward Snowden in 2013 has not lifted, but no one has stepped up to fill that demand. That’s something to keep on the radar for 2016.
GE will be an interesting company to watch over the next few years, as it might force the multiple cloud providers it works with to get better at working together, something several cloud customers we’ve talked to over the past few months have mentioned on their wish lists. There are the obvious competitive reasons why companies shy away from interoperability, but there are technical reasons, too, Drumgoole said.
A lot of older apps weren’t designed to run across multiple clouds and in many cases, it doesn’t make a lot of sense to re-build those older, functioning apps just to split the workloads between AWS and Azure. However, the new apps GE is developing are being built with the public cloud — and the possibility of running across multiple public clouds — in mind.
Catch Drumgoole’s appearance at Structure 2015 on November 18th at the Julia Morgan Ballroom in San Francisco. You can find more information here, and you can buy your tickets here.
Bringing you the events that shape the tech industry
4 
4 claps
4 
Written by
Executive Editor, Structure. Tech industry observer. Opposed to the designated hitter.
Bringing you the events that shape the tech industry
Written by
Executive Editor, Structure. Tech industry observer. Opposed to the designated hitter.
Bringing you the events that shape the tech industry
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/overprovisioning-always-on-resources-lead-to-26-6-billion-in-public-cloud-waste-expected-in-2021-da888ea68f74?source=search_post---------12,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 9, 2021·3 min read
Overprovisioning and leaving cloud resources on are two enormous sources of wasted spend.
Wasted spend drags down IT budgets — of particular importance as we enter 2021. The Flexera 2021 State of Tech Spend report found that the biggest change in key IT initiatives from 2020 to 2021 was in cost savings, with the percent of respondents ranking cost savings as a top initiative tripling year-over-year.
It’s important that this is being recognized. Based on data collected by Gartner, we estimate that wasted spend will exceed $26.6 billion this year.
Gartner estimates a total market spend of $304 billion on public cloud services end-user spending in 2021, as broken out in the table below. Their estimate for the proportion of that spent on Infrastructure as a Service (IaaS) is $65.3 billion. While wasted spend can be found in any area of cloud spend, customers tend to see the largest amount in these two areas, as well as finding it easiest to identify.
Cloud resources can be considered “idle” when they are running while not being used. For example, when development servers are left running overnight and on weekends when they’re not needed. Since compute resources are paid for by the minute or second, that’s a large portion of the week they’re being paid for but not used (and yes, this applies even if you have reservations.)
Our data shows that about 44% of compute spend is on non-production resources. If we estimate that non-production resources are only needed during a 40-hour work week, the other 128 hours (76%), the resources are sitting idle.
Applying that to the Gartner IaaS number, we estimate that up to $14.5 billion will be wasted on idle resources this year.
Overprovisioning occurs when a larger resource size is selected than is actually needed. There is a mindset of safety behind this, as of course, no one wants their applications to be under-resourced.
But the overprovisioning occurring is far beyond what is necessary, given the elasticity of the cloud. About 40% of instances are sized at least one size larger than needed for their workloads. The cost can be cut in half by reducing an instance by one size, while downsizing by two sizes saves 75%.
Many of our customers show a large percentage of their resources are oversized, but bringing this to a conservative estimate of 40% of resources oversized by one size, giving us a savings per resource of 50%, we estimate that up to $8.7 billion is wasted due to overprovisioning.
Another significant source of waste is orphaned volumes and snapshots. These are resources that have been detached from the infrastructure they were created to support, such as a volume detached from an instance or a snapshot with no volume attachment.
Our customers spend approximately 15% of their bills on storage, and we found that about 35% of that spend is on unattached volumes and snapshots. Applying that to the Gartner spending numbers, we estimate that up to $3.4 billion could be wasted this year on orphaned volumes and snapshots.
Altogether, this gives us an estimate of $26.6 billion to be wasted on unused cloud resources in 2021. This waste estimate is just based on the three prominent sources of cloud waste. It does not include wasted spend on Platform as a Service (PaaS), which makes up $55 billion in cloud spend according to Gartner’s estimates, nor from SaaS, unused reservation commitments, inefficient containerization, and other areas of the bill.
Attacking the three problem areas above is a great area to start for nearly all public cloud users. Here at ParkMyCloud, we’re on a mission to do just that. See how and try it out today, to do your part in reducing wasted cloud spend.
Originally published at www.parkmycloud.com on January 21, 2021.
CEO of ParkMyCloud
16 
16 
16 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/how-much-should-enterprises-worry-about-vendor-lock-in-in-public-cloud-5029bf40fffa?source=search_post---------13,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 16, 2019·5 min read
One of the key drivers to a multi-cloud strategy is the fear of vendor lock-in. “Vendor lock-in” means that a customer is dependent on a particular vendor for products and services, and unable to use another vendor without substantial switching costs or operational impact. The vendor lock-in problem in cloud computing is the situation where customers are dependent (i.e. locked-in) on a single cloud service provider (CSP) technology implementation and cannot easily move to a different vendor without substantial costs or technical incompatibilities.
Before the cloud, IT was running in dedicated on-premises environments, requiring long-term capital investments and an array of software license commitments and never ending hardware refresh contracts. Based on that experience, it is understandable that a customer would be concerned about lock-in. Many large IT vendors like Oracle, IBM, HP, and Cisco would “lock” customers into 3–5–10 year Enterprise License Agreements (ELAs) or All You Can Eat (AYCE) hardware and software license agreements, promising huge discounts and greater buying power — but only for their products, of course. I used to sell these multi-year contracts. There is a common ground for sure, as the customer was locked-in to the vendor for years. But that was then and this is now. Is vendor lock-in really a concern for public cloud users?
Isn’t the point of cloud to provide organizations the agility to speed innovation and save costs by quickly scaling their infrastructure up and down? I mean, we get it — your servers, data, networking, user management, and much more are in the hands of one company, so the dependence on your CSP is huge. And if something goes wrong, it can be very detrimental to your business — your IT is in the cloud, and if you’re like us, your entire business is developed, built and run in the cloud. Most likely, some or all of your organization’s IT infrastructure where you are developing, building and running applications to power your business and generate revenue, is now off-premise, in the cloud. But although “lock-in” sounds scary, you are not stuck in the same way that you were with traditional hardware and software purchases.
Let’s talk about the realities of today’s public cloud-based world. Here are a couple of reasons why vendor lock-in isn’t as widespread a problem as you might think:
Now the cloud is not without risk, and when we talk to customers the primary vendor lock-in concerns we hear are related to moving to another cloud service provider IF something goes awry. You hope that this never has to happen, but it’s a possibility. The general risks include:
To minimize the risk of vendor lock-in, your applications should be built or migrated to be as flexible and loosely coupled as possible. Cloud application components should be loosely linked with the application components that interact with them. And, adopt a multi-cloud strategy.
Many companies are familiar with vendor lock-in from dealing with traditional enterprise companies mentioned above — you begin to use a service only to realize too late what the terms of that relationship brings with it in regards to cost and additional features and services. The same is not entirely true with selecting a cloud service provider like AWS, Azure, or Google Cloud. It’s difficult to avoid some form of dependence as you use the new capabilities and native tools of a given cloud platform. In our own case, we use AWS and we can’t just wake up tomorrow and use Azure or Google Cloud. However, it’s quite possible to set your business up to maintain enough freedom and mitigate risk, so you can feel good about your flexibility.
So how much should enterprises worry about vendor lock-in in public cloud? IMHO: they shouldn’t.
Originally published at www.parkmycloud.com on September 3, 2019.
CEO of ParkMyCloud
11 
11 
11 
CEO of ParkMyCloud
"
https://medium.com/cloud-simplified/a-hybrid-cloud-approach-to-securing-public-cloud-3beef2dc951c?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
“Data breaches can have a clear impact on enterprises’ bottom line, and security teams are desperate to prevent them. However, it’s not the underlying cloud technology that has exacerbated the data breach problem — it’s the immature security practices, overtaxed IT staff and risky end-user behavior surrounding cloud adoption.”Info Security Magazine — 08/07/2019
Capital One is a poster child for public cloud — it’s AWS implementation is a featured case study on the web site. But the recent security breach that exposed over 100 million of its credit card applications could cost the company up to $500 million in U.S. fines.
Capital One is hardly alone. In April of this year researchers found third parties exposed over 540 million Facebook user records on the public cloud. Similarly, last February an “authorized third-party” exposed 2.4 million Dow Jones customer records. According to the Vulnerability and Threat Trends Report, public cloud vulnerabilities increased by 240% in the first half of 2019 from the same period in 2017.
“Misconfigured server infrastructure is often considered one of the most significant causes of data breaches within the IT industry. This human error phenomenon is usually unintentional, but it can have catastrophic consequences regarding the exposure of sensitive personal information as well as potentially damaging the reputation of your business.”Cyber Security Magazine, CS Media 12/26/2018
There was a great deal of press about the Capital One breach including a few articles in the Wall Street Journal alone. One of them, How the Accused Capital One Hacker Stole Reams of Data From the Cloud put the blame for the exposure on a misconfigured server. The article quotes a security researcher who said this is quite common and found over 800 accounts across the two leading cloud providers where misconfigured servers enable outsider access.
IT leaders often look to public cloud as the remedy for on-premises security struggles such as firewall configuration and keeping patching up to date. But public cloud is hardly a panacea. Not only does public cloud not eliminate misconfigured servers and firewalls, it exacerbates the problem.
Moving a security risk from on-premises to public cloud means it is now much more easily targeted by hackers in the highly exposed public cloud arena. Security expert and Citadel Chief Information Security Officer, Christofer Hoff, tweeted, “If your security sucks now, you’ll be pleasantly surprised by the lack of change when you move to Cloud.”
A July 2018 IDC report, Cloud Repatriation Accelerates in a Multi-Cloud World, says that half of all public cloud applications are expected to move back on-premises over the next two years. The number one driver cited by the survey of 400 companies was security.
Not only does a security mistake in public cloud pose much greater risk than on-premises, but effective security is far more challenging. Each public cloud requires specialized security knowledge around areas such as multi-tenancy (shared resources), permissions, network traffic flows, storage buckets, load-balancers, databases, identity access management, and so on.
Consider, for example, backup. A 08/09/2019 TechCrunch article, Hundreds of exposed Amazon cloud backups found online, says that cloud administrators often fail to choose the correct security settings. This leaves EBS snapshots (backups), “inadvertently public and unencrypted.”
Deloitte Consulting Chief Cloud Strategy Officer, David Linthicum, made it clear in a 05/08/2018 InfoWorld article, that even deep IT on-premises security skills don’t translate fully to the specific requirements of a public cloud provider:
“The fact is that enterprises have done a poor job in prepping the talent pool for the cloud…The breaches that I see are caused by people doing dumb things, not by the lack of technology. Things are misconfigured, updates are not applied, or the wrong technologies are chosen. Indeed, you can trace most breaches over the last five years to that root cause of poor talent.”
Security and compliance in the cloud is a shared responsibility between the cloud service providers (CSP) and their customers. Under the Shared Responsibility Model, the CSP is responsible for “security of the cloud” which includes the hardware, software, networking, and facilities that run the cloud services. Organizations (the CSP’s customers), on the other hand, are responsible for “security in the cloud” which includes how they configure and use the resources provided by the CSP.Diem Shin, Fugue 01/23/2019
Public cloud providers could, if they wished, simply implement airtight server/firewall security. As an example, AWS could make its storage bucket (AWS E3) unavailable to anyone on the Internet. But locking down E3 would thwart an organization’s ability to easily share certain information for test/dev purposes or even production.
Public cloud providers built their architectures for rapid integration and scalability. Cloud customers have the freedom to configure and deploy servers as they best see fit with a click of a button, but the trade-off is a requirement to assume much of the resulting security responsibility. The key is recognizing their level of risk appetite and then ensuring best practice conformity against an appropriate security baseline.
It is helpful to look at cloud security as a four-layer spectrum. On one end is the cloud platform layer which is clearly the public cloud provider’s responsibility to secure. At the opposite end of the spectrum is the application layer which is in the customer’s purview to secure. In between these two layers lies a gradient where the cloud provider responsibility gradually declines while the customer responsibility increases.
Cloud customers tend to have a poor track record when it comes to fulfilling their responsibility for security.
Securing both on-premises and multiple clouds is more complex than securing a single cloud. Organizations require a different set of controls for private cloud, public cloud, and then for different public clouds. And there seems little doubt that we’re headed in a hybrid/multi-cloud world. An April 2019 IDC study, Surviving and Thriving in a Multicloud World, states, “Multi-cloud environments are now the norm for enterprise organizations.” Forrester Research confirms that multi-cloud/hybrid cloud environments now comprise 74% of enterprise computing strategies.
Even good security policies do not guarantee security if leadership has no way to ensure compliance. For example, a Nutanix customer had a policy of no cloud-based load balancers with unencrypted data, but upon deploying Xi Beam found 67 such instances.
Effective security mandates an ability to monitor thousands of variables. Is the storage bucket secure? Is the data encrypted? Was the Log Access key discarded after 30 days? This is not possible to do manually. Automated tooling must poll each public and private cloud environment to check configurations against a known list of cloud security best practices. Cloud security management tools must provide:
One such tool is Nutanix Xi Beam which supports both AWS and Azure. Beam also supports security compliance audits for on-premises Nutanix environments that use the Nutanix native hypervisor, AHV. Beam can help detect, and even remedy server and firewall misconfigurations, organizations identify and fix their security issues across multiple cloud accounts by providing cloud security visibility, optimization and control:
VisibilityWhat does the multi-cloud environment look like? How are resources deployed? Are there any exposed databases? Beam answers these types of questions, and many others, by providing a security heatmap and global visibility into the security posture of a multi-cloud environment. It identifies cloud infrastructure security vulnerabilities using 500+ automated audit checks based on industry best practices across public and private clouds.
OptimizationBeam includes a one-click feature that can easily fix security issues and improve a customer’s security posture. It provides out-of-the-box security policies to automate the checks for common regulatory compliance policies such as HIPAA, PCI-DSS, NIST, etc.
ControlBeam brings automation to securing the environment with policy-driven automated workflows that continuously detects security vulnerabilities in real-time. Beam then implements the actions needed to fix them. Customers can create their own custom audit checks to meet their business specific security compliance needs.
For Nutanix customers we are now launching the security compliance audits through Xi Beam. If you’d like to try it out, click here.
The original author of this article is @roidude and a special thanks to Beam product marketing manager, Sahil Bansal, who provided invaluable information, guidance, and editing.
Originally published at https://www.nutanix.com.
An inclusive approach to cloud management.
9 
9 claps
9 
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.fabric8.io/the-easiest-way-to-get-started-with-fabric8-on-the-public-cloud-is-now-stackpointcloud-2f2083d91bf4?source=search_post---------15,"One of the most challenging aspects of kubernetes is getting your kubernetes cluster installed correctly. There are so many things that can go wrong (particularly with persistence, networking, load balancing, firewalls and DNS). Then once thats done you need to install fabric8 on top to get all the awesomeness of an integrated development platform. This can be quite a daunting task to a new user!
Thankfully minikube has really helped make it easy to install fabric8 inside a single node kubernetes cluster on your laptop for Windows, OS X and Linux. Though there’s still the issue of how to install a real kubernetes cluster with fabric8 on a public cloud like AWS, Azure, Google Cloud or Digital Ocean.
Thats all now changed thanks to the nice folks at Stack Point Cloud.
Literally in a few clicks you can select the public cloud of your choice (AWS, Azure, Google Cloud or Digital Ocean), enter your credentials, pick your kubernetes cluster and hardware size, then select fabric8 as a solution and you’re done!
Here’s a video showing how super simple this is!!
Huge thanks to the Stack Point Cloud folks for doing this! Its really gonna help folks who want to use fabric8 on the public cloud get started super quickly and easily! Plus you can install a vanilla kubernetes cluster with Stack Point Cloud too or install other off the shelf solutions.
So what are you waiting for? Click here to spin up your own kubernetes cluster with fabric8 now!
Open Source Developer Platform
4 
2
4 claps
4 
2
Written by
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
Open Source Developer Platform
Written by
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
Open Source Developer Platform
"
https://medium.com/@cloud-opinion/short-post-on-public-cloud-e4eb1389b11f?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
.Cloud Opinion
Jun 6, 2016·1 min read
This is a short post, but a thought keeps coming to me and I can’t get to an answer I find satisfactory. I am hoping that some of you would have thoughts on this.
It feels to me that AWS is now overshooting(*) some customers ( startups, SMB etc ) and has become a premium ( read expensive) option. This is a signal of change in the market and should lead to further competitive battles. This ideally should lead to low-end disruptive offerings.
I do not believe Google is this, because Google has made it very clear that they want to fight AWS at the full feature competition.
We hadn’t yet seen a low end disrupter emerge. Why is this? Comment below or tweet.
Parody + Tech commentary.
See all (536)
3 
1
3 claps
3 
1
Parody + Tech commentary.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jrodthoughts/the-enterprise-public-cloud-wars-are-over-let-s-talk-about-what-s-next-eda2df0c4f04?source=search_post---------17,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jesus Rodriguez
Nov 11, 2015·4 min read
The Enterprise Public Cloud Wars Are Over. Let’s Talk About What’s Next
Last week there were some very interesting developments in the public cloud space. First, Microsoft, Amazon beat Wall Street expectations and reported monster numbers associated with their public cloud offerings.
Also, HP announced that it will be stopping the efforts about the Helion public cloud to focus on the private and hybrid cloud offerings. These news come after two years of sustained efforts and large investments in their public cloud platform.
These new developments has made many questioned whether the enterprise public cloud race is essentially over. While the public cloud space remains really competitive within a few platforms, it’s pretty obvious that we can already identify winners in the enterprise public cloud space.
Amazon and Microsoft by a mile with Google, Salesforce and IBM being relevant second places. Amazon’s AWS and Microsoft’s Azure have managed to develop the most complete offerings in the enterprise public cloud space. Today, AWS and Azure not only provide a larger of number of capabilities than their competitors but also more complete offerings enterprise-ready areas such as security, compliance etc.
If we apply Clayton M. Christensen’s innovator dilemma thesis to the enterprise public cloud space we need to assume that, at some point, a new generation of startups will challenge the well-established enterprise public cloud platforms. However, looking at the near future, it’s hard to imagine what factors will be able to disrupt the lead established by the enterprise public cloud incumbents. In principle, a new entrant in the enterprise public cloud space can try to establish traction by disrupting some of the following areas:
However, after analyzing the current state of the enterprise public cloud platform market, its hard to imagine how any of those factors can be relevant enough for a new entrant to cause disruption in the space. Let’s explore that analysis.
Technology Innovation
From the enterprise capability standpoint, the Azure and AWS cloud are far ahead of every other platform in the space. Ranging from basic infrastructure to sophisticated capabilities such as machine learning or integration, both Azure and AWS are providing native services for every imaginable capability that can be relevant in the enterprise. Salesforce, IBM and Google are also rapidly growing the feature set of their public cloud offerings. In that sense, it’s hard to conceive a new enterprise public cloud platform that will be able to compete with the incumbents in the short term.
Pricing
The enterprise public cloud offering is a race down to 0. Amazon, Microsoft and Google keep lowering the price of their offering in order to not allow any other vendor to gain competitive advantage. In that sense, it will be incredibly difficult for a new enterprise public cloud offering to disrupt the space with a more attractive pricing model.
Developer Community
AWS, Azure, Heroku and Google Cloud are some of the examples of enterprise public cloud platforms that are enjoying growing developer communities. By leveraging and embracing open source technologies, incumbents have been able to attract millions of developers actively building applications in their platforms. As a result, new platforms in the space will have to build similar developer communities to even be competitive with the incumbents.
Enterprise Software Startups
Another element that seems to be a non-factor. Platforms like AWS, Azure and Google Cloud have not only become dominant offers for large enterprises but they have captured the hearts of new enterprise software startups that keep relying on those platforms to power their offerings. Those startups represent a new and vibrant distribution channels for the adoption of the incumbent platforms and will make it extremely hard for a new offering in the space to leverage that channel.
Geographical Presence
Platforms like AWS and Azure have developed an incredibly impressive global footprint with locations that offer viable options in almost every country in the world. Additionally, the services provided by those platforms are available in a large number of languages. This factor makes it almost impossible for new entrants to develop a relevant presence in specific geographies.
New Distribution Channels
The dominant enterprise public cloud platforms have played a masterful game developing a large number of distribution channels from partner networks to whitelabel offerings. The network effects of these channels will make it extremely hard to new offerings to disrupt the enterprise public cloud space by developing new distribution channels.
With the competition in the enterprise public cloud space almost over, the attention has shifted to private and hybrid cloud platforms. Currently, that space remains overly competitive with platforms like Pivotal’s Cloudfoundry, HP Helion, Aprenda and even public cloud incumbents like Azure and AWS trying to create comprehensive offers for enterprises. That will be the subject of a future post.
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
See all (604)
5 
5 claps
5 
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/iot-backend-the-evolution-of-public-cloud-providers-in-the-internet-of-things-iot-e05422a18ce?source=search_post---------18,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·6 min read
The Internet of Things (IoT) has jumbled the agenda of CIOs and CTOs faster than expected and with a breathtaking velocity. As of shortly cloud, big data and social topics occupied center stage. However, in the meantime we are talking more and more about the interconnection of physical objects like human beings, sensors, household items, cars, industrial facilities etc. Who might think that the “Big 4” now disappear from the radar is wrong. Quite the contrary is the case. Cloud infrastructure and platforms belong to the central drivers behind IoT services since they are offering the perfect preconditions to serve as vital enabler and backend services.
The demand for public cloud services shows an increasing momentum. On the one hand it is due to the requirement of CIOs to run their applications more agile and flexible. On the other hand most of the public cloud providers are addressing the needs of their potential customers. Among the varying workload categories that are running on public IaaS platforms standard web applications (42 percent) still represent the major part. By far mobile applications (22 percent), media streaming (17 percent) and analytics services (12 percent) follow. Enterprise applications (4 percent) and IoT services are still playing a minor part.
The reason for the current segmentation: websites, backend services as well as content streaming (music, videos, etc.) are perfect for the public cloud. On the other hand enterprises are still sticking in the middle of their digital transformation and evaluate providers as well as technologies for the successful change. IoT projects are still in the beginning or among the idea generation. Thus in 2015, IoT workloads are only a small proportion on public cloud environments.
Until 2020 this ratio will significantly change. Along with the increasing cloud knowledge within the enterprises IT and the ever-expanding market maturity of public cloud environments for enterprise applications the proportion of this category will increase worldwide from 4 percent to 12 percent. Accordingly, the proportion of web and mobile applications as well as content streaming will decrease. Instead worldwide IoT workloads will almost represent a quarter (23 percent) on public IaaS platforms like AWS, Azure and Co.
The Internet of Things will quickly become a key factor for the future competitiveness of enterprises. Thus, CIOs have to deal with the necessary technologies to support their enterprise business technology strategy. Public cloud environments — infrastructure (IaaS) as well as platforms (PaaS) offer perfect preconditions to serve as supporting backend environments for IoT services and devices. The leading public cloud providers already have prepared their environments with the key features to develop into an IoT backend. The central elements of a holistic IoT backend are characterized as follows (excerpt):
Public cloud based infrastructure-as-a-service (IaaS) will mainly be used to provide compute and storage capacities for IoT deployments. IaaS provides enterprises and developers inexpensive and almost infinite resources to run IoT workloads and store the generated data. Platform-as-a-service (PaaS) offerings will benefit from the IoT market as they provide enterprises faster access to software development tools, frameworks and APIs. PaaS platforms could be used to develop control systems to manage IoT applications, IoT backend services and IoT frontends as well as to integrate with third party solutions to build a complete “IoT value chain”. Even the software-a-as-service (SaaS) market will benefit from the IoT market growth. User-friendly SaaS solutions will facilitate users, executives, managers as well as end customers and partners to analyze and share the data generated by interconnected devices, sensors etc.
digitalSTROM + Microsoft Azure digitalSTROM is one of the pioneers in the IoT market. As a provider of smart home technologies the vendor from Switzerland has developed an intelligent solution for connecting homes to communicate with several devices over the power supply line via smartphone apps. Lego kind bricks form the foundation. Each connected device can be addressed over a single brick, which holds the intelligence of the device. digitalSTROM early evaluated the potentials of a public cloud environment for its IoT offering. Microsoft Azure provides the technological foundation.
General Electric (GE) + Amazon Web Services General Electric (GE) has created an own IoT factory (platform) within the AWS GovCloud (US) region to interconnect humans, simulator, products, sensors etc. with each other. The goal is to improve collaboration, prototyping and product development. GE’s decision for the AWS GovCloud was to fulfill legal and compliance regulations. One customer who already profits from the IoT factory is E.ON. When the demand for energy increases in the past GE typically tried to sell E.ON more turbines. In the course of the digital transformation GE early started to change its business model. GE is using operational data of turbines to optimize the energy efficiency by performing comprehensive analyzes and simulation. E.ON gets real-time access to the interconnected turbines to control the energy management on demand.
ThyssenKrupp + Microsoft Azure Together with CGI ThyssenKrupp has developed a solution to interconnect thousands of sensors and systems within its elevators over the Microsoft Azure cloud. For this purpose they are using Azure IoT services. The solution provides ThyssenKrupp several information from the elevators to monitor the engine temperature, the lift hole calibration, the cabin velocity, the door functionality and more. ThyssenKrupp records the data, transfers it to the cloud and combines it in a single dashboard based on two data types. Alarm signals that indicate urgent problems and events that are only stored for administrative reasons. Engineers get real-time access to the elevators data to immediately make their diagnostics.
All use cases above show three key developments that determine the next five years and will significantly influence the IaaS market:
Thus, several public cloud providers should question themselves whether they have the potential respectively the preconditions to develop their offering further to become an IoT backend. Only the ones who provide services and have development capacities (tools, SDKs, frameworks) in the portfolio will be able to play a central role in the profitable IoT market and being considered as the infrastructure base for novel enterprise and mobile workloads. Note: more and more public cloud infrastructure is used as an enabler and backend infrastructure for IoT offerings.
Various enablement services are available in the public cloud market that can be used to develop an IoT backend infrastructure.
Amazon AWS services for the Internet of Things:
Microsoft Azure IoT-Services:
Amazon AWS didn’t start any noteworthy marketing for the Internet of Things so far. Only a sub website explains the idea of IoT and what kind of existing AWS cloud services should be considered. Even with Amazon Kinesis — predestinated for IoT applications — AWS is taking it easy. However, taking a look under the hood of IoT solutions one realize that many cloud based IoT solutions are delivered via the Amazon cloud.
Microsoft considers the Internet of Things as a strategic growth market and has created Microsoft Azure IoT Services, a specific area within the Azure portfolio. However, so far this is only a best off of existing Azure cloud services that are encapsulating a specific functionality for the Internet of Things.
From a strategy perspective IoT use cases are following the top-down cloud strategy approach. In this case the potentials of the cloud are considered and based on that a new use case is created. This will significantly change the ratio from bottom-up to more top-down use cases in the next years. (Today’s ratio is about 10 percent (top-down) to 90 percent (bottom-up)) More and more enterprises will start to identify and evaluate IoT use cases to enhance their products with sensors and machine-2-machine communication. The market behaviors we see for fitness wearable’s (wristbands and devices people are using to quantify themselves) today will exponentially escalate to other industries.
So, the majorities of the cloud providers are under pressure and can’t rest on their existing portfolio. Instead they need to increase their attractiveness by serving their existing customer base as well as potential new customers with IoT enablement services in terms of microservices and cloud modules. Because the growth of the cloud and the progress of the Internet of Things are closely bound together.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
3 
3 claps
3 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/public-cloud-adoption-statistics-market-shares-through-the-pmc-lens-57ef130c2fba?source=search_post---------19,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 20, 2020·4 min read
While we monitor the market as a whole, cloud adoption statistics are indicators of market share among the large providers. The ParkMyCloud platform sees a very large volume of data flow through it each day, month, and quarter, and therefore affords an interesting and helpful perspective on our users. When this in turn allows you to examine their usage preferences of other third-party services it can be downright enlightening.
For this post, rather than examining the granular detail of specific user preferences for certain products and services, I thought it might be interesting to roll the numbers up and see what this says about the world of public cloud as a whole. In particular, given we are now at the end of earnings season (see our recent post here ) we thought it would be interesting to compare what we see in our customer base. While obviously we only have a tiny percentage of the overall user base of public cloud, in recent years it has increasingly been my belief that it is fairly representative of the overall market. In fact, I would go as far as to say that what we have observed in our data often appears in the public announcements some months later. The big trends such as increased usage of very short lived instances (especially for data analytics workloads) or increased use of custom instances have caught our eye only to be affirmed more broadly by the market.
Some of the things we look at each quarter include:
Over the last year or so it has been interesting to see the shifts occurring in the first of these measures — customers making exclusive use of a single cloud. Putting to one side an obvious caveat which is that customers could have other cloud accounts not brought into the PMC platform, we have observed that some 95% of users are exclusively using a single cloud. There have been some shifts in the relative proportions using either AWS and Azure, AWS and GCP and Azure and GCP but the numbers here are so small compared to those using a provider exclusively, that it is hard to draw any strong conclusions.
Figure 1: Changes in Customers Making Exclusive Use of Cloud Provider (Source: PMC).
However, once again putting the overall representativeness of the PMC user base to one side, we can without doubt see some meaningful changes over the last six quarters in which clouds are being exclusively used by our customers. The chart shown in Figure 1 above, shows the relative changes over the last 18 months, with Q6 being March-May 2020. It is therefore likely that we picked up some of the COVID-19 related shifts, but will see more in the coming quarters.
To show these changes I have rebased the data (Q0) and then looked at the relative changes over the period. So for example, you can see that exclusive Azure users grew their footprint amongst our customer base by some 9.2% by Q5 and ended the period up 6.2%. There is a clear upward trendline for Azure during these last six quarters, versus AWS and GCP which are showing a flat to a slight downward trajectory. As mentioned above the proportion using multiple clouds has stayed fairly static.
It will be interesting to see how these cloud adoption statistic trends play out. Based upon what we are hearing anecdotally from our customers, there has been a lot of growth in the market for Virtual desktop infrastructure (VDI) in the move to remote working, and that Azure have been the largest beneficiary of the shift. With employers increasingly alerting staff to the possible realities of home working throughout the winter months, we think it likely that the trend continues.
Figure 2: Cloud Revenue Growth: AWS, Azure and GCP (Source: Venturebeat)
What we do know from the earnings numbers is that the growth in cloud revenue numbers is slowing down for all three providers with the steepest declines being reported by AWS (although actual earned revenue is still increasing for all three). In such an environment, competition for market share is likely to get even more intense and so monitoring these shifts is likely to become even more important to track.
Originally published at www.parkmycloud.com on August 13, 2020.
CEO of ParkMyCloud
See all (317)
10 
10 claps
10 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/ai-becomes-the-game-changer-in-the-public-cloud-172b3c31994?source=search_post---------20,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Jan 26, 2018·6 min read
After more than 10 years, cloud computing has evolved into a fertile business for providers such as Amazon Web Services or Microsoft. However, competition is getting stronger from laggards like Google and Alibaba. And with the massive and ongoing introduction of AI-related cloud services, providers have increased the competitive pressure themselves, in order to raise attractiveness among their customers.
To build and operate powerful and highly-scalable AI systems is an expensive matter for companies of any size. Eventually, training algorithms and operating the corresponding analytics systems afterwards need oodles of computing power. Providing the necessary computing power in an accurate amount and on time via the own basement, server room or data center is impossible. Computing power that afterwards is not required anymore.
Looking into the spheres of Amazon, Microsoft or Google, all three providers built up an enormous amount of computing power in recent years and equally own a big stake of the 40 billion USD cloud computing industry. For all of them, expanding their portfolios with AI services is the next logical step in the cloud. On one side, developing AI applications respectively the intelligent enhancement of existing applications requires easy access to computing power, data, connectivity and additive platform services. Otherwise, it is necessary to obtain attractiveness among existing customers and to win new customers. Both are looking for accessible solutions to integrate AI into their applications and business models.
Amazon Web Services (AWS) is not only the cloud pioneer and innovation leader, but still by far market leader of the worldwide public cloud market. Right now, AWS is the leading cloud environment for developing as well as deploying cloud and AI application, due to its scalability and comprehensive set of platform services. Among other announcements, AWS presented Amazon Cloud 9 (acquisition of Cloud9 IDE Inc. in July 2016) at the recent re:Invent summit. A cloud-based development environment that is directly integrated into AWS cloud platform to develop cloud-native applications. Moreover, AWS announced six machine learning as a service (MLaaS) services, including a video analysis service as well as a NLP service and a translation service. In addition, AWS offers MXNet, Lex, Rekognition and SageMaker, powerful services for the development of AI applications. SageMaker, in particular, attracts attention, since it helps to control the entire lifecycle of machine learning applications.
However, as with all cloud services, AWS pursues the lock-in approach with AI-related services as well. All AI services are tightly meshed with AWS’ environment to make sure that AWS remains the operating platform after the development of an AI solution.
Amazon also sticks to its yet successful strategy. After Amazon made the technologies behind its massive scalable ecommerce platform publicly available as a service via AWS, technologies behind Alexa, for example, has followed to help customers integrate own chatbots or voice assistants into their applications.
Microsoft has access to a broad customer base in the business environment. This along with a broad portfolio of cloud and AI services offer basically good preconditions to also establish oneself as a leading AI market player. Particularly because of the comprehensive offering of productivity and business process solutions, Microsoft could be high on the agenda of enterprise customer.
Microsoft sticks deep in the middle of digital ecosystems of companies worldwide with products like Windows, Office 365 or Dynamics 365. And that is exactly the point where the data exist respectively the dataflows happen that could be used to train machine learning algorithms and build neural networks. Microsoft Azure is the central hub where everything runs together and provides the necessary cloud-based AI services to execute a company’s AI strategy.
In the cloud, Google is still behind AWS and Microsoft. However, AI could become the game changer. Comparing today’s Google AI services portfolio with AWS and Microsoft you can see that Google is the clear laggard among the innovative provider of public cloud and AI services. This is astounding if you consider that Google invested USD 3.9 billion in AI so far. Compared to the competition, Amazon has invested USD 871 million and Microsoft only USD 690 million. Google simply lacks in consistent execution.
But! Google already has over 1 million AI user (mainly through the acquisition of data science community „Kaggle“) and owns a lot of AI know-how (among others due to the acquisition of “DeepMind”). Moreover, among developers Google is considered as the most powerful AI platform with the most advanced AI tools. Furthermore, TensorFlow is the leading AI engine and for developers the most important AI platform, which serves as the foundation of numerous AI projects. In addition, Google has developed its own Tensor Processing Units (TPUs) that are specifically adapted for the use with TensorFlow. Recently, Google announced Cloud AutoML, a MLaaS that addresses unexperienced machine learning developer, to help creating deep learning models.
And if you keep in mind where Google via Android OS has its fingers in the pie (e.g. Smartphones, home appliances, smart home or cars) the potential of AI services running on the Google Cloud Platform is clearly visible. The only downer is that Google is still only able to serve developers. The tie-breaking access to enterprise customers, something that Microsoft owns, is still missing.
The AI platform and services market is still at an early stage. But in line with the increasing demand to serve their customers with intelligent products and services, companies are going to proceed to search for the necessary technologies and support. And it’s a fact that only the easy access to cloud-based AI services as well as the necessary and fast accessible computing power is imperative for developing novel “intelligent” products, services and business models. Hence, for enterprises it doesn’t make any sense to build in-house AI systems since it is nearly impossible to operate them in a performant and scalable way. Moreover, it is important not to underestimate the access to globally distributed devices and data that has to be analyzed. Only globally scalable and well-connected cloud platforms are able to achieve this.
For providers, AI could become the game changer in the public cloud. After AWS and Microsoft started leading the pack, Google wasn’t able to significantly play catch-up. However, Google’s AI portfolio could make a difference. TensorFlow, particularly and its popularity among developers could play into Google’s hands. But AWS and Microsoft are beware of it and act together against this. “Gluon” is an open source deep learning library both companies have developed together, which looks quite similar to TensorFlow. In addition, AWS and Microsoft provide a broad range of AI engines (frameworks) rather than just TensorFlow.
It is doubtful that AI services are enough for Google to catch up with AWS. But Microsoft could quickly feel the competition. For Microsoft it is crucial, how fast the provider is able to convince its enterprise customer of its AI services portfolio. And at the same time to convey how important other Microsoft products (e.g. Azure IoT) are and to consider them for the AI strategy. AWS is going to stick to its dual strategy and focus on developers as well as enterprise customers and will still lead the public cloud market. AWS will be the home for all those who solely do not want to harness TensorFlow — in particular cloud-native AI users. And not to forget the large customer base that is innovation oriented and is aware of the benefits of AI services.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
11 
11 claps
11 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@billatnapier/what-youll-keep-my-contact-data-for-20-years-and-store-it-in-the-amazon-public-cloud-51313eea55d6?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Prof Bill Buchanan OBE
May 30, 2020·2 min read
The NHSX App in the UK has not had a good time, and has been criticized for its centralisation nature. But it’s really what happens on the back-end that really matters, especially how the data will be used, and how long it will be kept for. And so, the details are finally being released about the NHSX App, and where Public Health England…
"
https://medium.com/foundations/local-government-and-the-public-cloud-c550cbd5b8f6?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
For the last 4 or 5 years, Eduserv has delivered managed services on our own public and private cloud platforms hosted in our Swindon data centre. We have targeted central and local government and the third sector in the UK with our services, predominantly through the UK government’s G-Cloud procurement framework, and we were initially reasonably successful in doing that.
But things have changed…
Recently, we have taken a major strategic decision to move towards providing managed services on the public cloud, particularly focusing on the services available from Amazon and Microsoft. This decision recognises that in future we will not be able to deliver as much value using our own infrastructure as using those large-scale public cloud providers and that we can do more for our customers in partnership with them.
As far as our cloud offers are concerned, we now exist to help our customers get the best out of AWS, Azure and Office 365. That position will clearly evolve over time. Of course, the reality is that many of our customers will need time to migrate their services to the public cloud and that may well lead to hybrid approaches being adopted in the short term. A few may see themselves as having longer term hybrid requirements but we see it as part of our mission to encourage them to move to the public cloud and to understand the rationale for that approach. The long term direction is clear.
In reality, we should have taken this decision some years ago — the ‘public cloud’ writing has been on the wall for some time — but for various reasons we were unable to do so. I won’t go into the reasons here. Suffice to say that we are a bit late to the party.
It’s a big decision for us, and the impact is pretty huge in terms of the AWS and Azure up-skilling that now needs to take place across the business and the associated cultural changes in the way we work. We do not want to become one of the managed service providers (MSPs) that Stephen Orban describes as “holding our customers’ strategies back” and we are working hard to make sure that doesn’t happen. But it’s a big change for us. If nothing else, the changes are significant enough that it makes Eduserv a pretty exciting place to be working right now. I’ll return to how we are dealing with some of these changes in future posts.
We work mainly with organisations that spend other people’s money — either in the form of tax revenue (where we are dealing with central or local government) or in the form of donations (where we are working with charities).
As a tax payer and/or as someone who donates to charity, I want the organisations that are taking my money to spend it as wisely as they can. I don’t want to cycle from Land’s End to John O’ Groats or run a half-marathon only to find that the money I’ve raised is wasted by a charity’s inefficient use of IT. And I don’t want the council-, income- or other tax that I pay to go to organisations that subsequently waste it on poor IT decision-making.
So what does spending money wisely on IT look like?
Well, some obvious things spring to mind. For example, I don’t want those organisations to be spending money on building or maintaining data centres of their own. Why would they do that? It’s not core business for them and there will be other players who can do it more efficiently than they can. And where they recognise that, which increasingly most of them do, I similarly don’t want them to simply lift-and-shift a bunch of on-premise servers (physical or VMs) and move them into someone else’s data centre. Nor do I want them to simply lift-and-shift them to the cloud.
There has to be more intelligence to it than that.
I want them to migrate to the cloud in the best way possible. That means I want them to make sensible decisions about which cloud provider(s) they choose and I want them to get the best out of the way they undertake the migration. Stephen Orban lists 6 strategies for migrating applications to the cloud and that feels like a pretty good place to start.
But why AWS and Azure?
Partly it is down to the breadth of services on offer. And not just breadth… but pace of change as well. Cloud is not just somewhere different to plonk your existing servers — it’s a whole new way of thinking about service delivery. The clearest current example of that is the changes that are brought about by the advent of serverless, which I’ll come back to in future posts, but there are other examples as well — and there will be more examples in the future based on technologies that the Amazons, Microsofts and Googles of this world haven’t even thought of yet.
Remember that ‘best’ doesn’t necessarily mean ‘cheapest’. I mean, yes, cost is clearly an important factor in any decision around IT spend — especially when we are talking about the UK public sector in its current form. But decisions based solely on cost concerns now, will likely have a long term detrimental impact on how well services are able to be delivered in the future.
As both a funder and a consumer, I want organisations in the public- and third- sectors to also be positioning themselves to be able to offer me the best possible services. The best possible social care, or health, or transport choices, or education, or refuse collection, or whatever. In the main — not always, but in the main — that means transforming themselves to consume and deliver services digitally. That requires a huge cultural shift on their part. It means a significant shift around understanding user needs, UX, service design and so on. And it almost certainly requires a shift in culture towards more agile and lean ways of working. It means that they have to be able to attract the kind of people who can do that — and it means transforming current staff so that they can understand the changes happening around them.
Organisations that simply see a move to the cloud as being a lift-and-shift of servers probably aren’t going to be transforming themselves culturally. The mindsets aren’t really going to change. The kind of people they will attract won’t change. They might manage to save some money in the process, though not necessarily, but there isn’t going to be a wider impact for the public good. Choosing a cloud provider that doesn’t offer much in functional terms beyond what an organisation can already do in-house doesn’t encourage a change in thinking — it simply represents the status quo in a new setting.
I see the choice of AWS or Azure against other possible non-public cloud choices in the market as being at least as much a cultural decision as it is a technical one. For me, it represents a belief that things are going to be done differently.
And that belief is what will change organisations and the services they are capable of delivering for the better.
A blog by andypowe11
2 
2 claps
2 
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/alert-logic/public-cloud-adoption-is-forcing-companies-to-rethink-their-overall-security-operations-strategy-d5558970d230?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
By: Allison Armstrong
This week, Alert Logic released the results of a commissioned study conducted by Forrester Consulting (June 2016). The study was based upon a survey of 100 U.S. and U.K. IT professionals responsible for cloud security infrastructure. The purpose of the study was to reflect the priorities and challenges security teams face due to increasing business demand for scalable, on-demand, cloud-based IT service delivery.
You can download the full study and infographic at alertlogic.com/forrester.
Study participants reported increasing investment in security operations (51%) and instituting new policies and controls (49%), both as a direct result of cloud adoption. These data points underscore some of the basic IT security considerations associated with cloud adoption, namely, managing a quickly expanding IT footprint, and interpreting and implementing the shared responsibility model set forth by public cloud providers.
Perhaps a less expected data point is this one: 46% are re-evaluating their entire security operations and controls, across all environments, as a result of cloud adoption.
What are the drivers that are forcing broader changes in security operations strategy? What are the ways that cloud adoption is indirectly influencing change?
These conditions, combined with the potential loss of money and reputation as a result of a security breach, are leading many companies to re-evaluate their overall security operations strategy.
For most companies, achieving security maturity exclusively through in-house security operations is impractical, and ultimately raises new questions.
For these reasons, few companies choose to provide security services and capabilities exclusively through in-house security operations centers (SOCs). More commonly, companies seek to augment their in-house capabilities with managed security services, and managed detection and response services built for cloud, like those provided by Alert Logic.
What’s driving this choice — as ranked by study participants — is aligned to team’s top challenges for building cloud security capabilities within their SOC. Among the challenges they report are: managing security content (44%); identification of multi-vector attacks; threat intelligence (40%); and, correlating threat data (33%).
Alert Logic provides the people, processes and technology to solve these challenges across cloud, on-premises, hybrid and managed/hosted infrastructure. Alert Logic combines deep expertise in advanced threat detection technologies, SOC infrastructure and threat intelligence, as well as the security research and content development needed to deliver cloud security and compliance outcomes. Learn more about our cloud security services.
Originally published at www.alertlogic.com.
AAllison Armstrong has worked in the Cloud and Data Security, IAM, BPM, ITOM and ITFM markets for over 20 years, working with global customers across every vertical from small office through to multi-national corporates, Fed and SLED organizations. In her role as an expert product and market strategist for Alert Logic solutions, she is responsible for developing and implementing the Technical, Product, and GTM strategies for the global business.
Alert Logic has more than a decade of experience pioneering…
2 
2 claps
2 
Written by
The official Medium account of Alert Logic.
Alert Logic has more than a decade of experience pioneering and refining cloud solutions that are secure, flexible and designed to work with hosting and cloud service providers. We deliver a complete solution that lives in the cloud, but is rooted in real expertise.
Written by
The official Medium account of Alert Logic.
Alert Logic has more than a decade of experience pioneering and refining cloud solutions that are secure, flexible and designed to work with hosting and cloud service providers. We deliver a complete solution that lives in the cloud, but is rooted in real expertise.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yogeshmalik/public-cloud-choosing-between-aws-gcp-azure-cloud-war-125a89c64777?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yogesh Malik
Jan 24, 2019·4 min read
◼️ Interconnect◼️ Network Performance & Latency◼️ Price◼️ SD-WAN◼️ Free-tier ◼️ Developer’s view◼️ IoT Development ◼️ API Management◼️ Network-as-a-Service◼️ Backup◼️ Disaster Recovery◼️ Managing Kubernetes ◼️ Container Registry
www.datamation.com
www.networkcomputing.com
www.networkworld.com
www.infoworld.com
www.datamation.com
www.techrepublic.com
www.crn.com
searchcloudcomputing.techtarget.com
www.channele2e.com
www.computerworlduk.com
www.sitepoint.com
www.infoworld.com
searchcloudcomputing.techtarget.com
www.business2community.com
www.networkworld.com
www.cbronline.com
www.techrepublic.com
www.crn.com
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
See all (778)
1 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ghaff/public-cloud-spending-is-up-private-cloud-spending-too-45ac5569269?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gordon Haff
Aug 16, 2018·3 min read
Wind the clock back to the early days of cloud computing and it wasn’t hard to find predictions that public clouds were the singular future of computing. Sure, some of the people saying this had a vested interest in their prediction coming to pass. But others just looked at the scale and economics of public clouds, duly noted the analogies to the electric grid and centralized power generation, and figured that running your own computing infrastructure must be on its way out sooner rather than later.
Fast-forward a decade or so and what’s happened? The broad outlines of the developing hybrid cloud story have been taking shape for a while now. All public or all private strategies are rare, especially among large organizations. But taking hybrid cloud as a given, it’s important to better understand how public cloud is being adopted in a world where private clouds usually have a role to play as well.
To this end, last year Red Hat commissioned Forrester Consulting to evaluate cloud migration strategies by conducting an online survey with 272 respondents working at enterprises that had already migrated, or were in the process of migrating, some existing workloads to the public cloud. So these results represent on-the-ground experience rather than theoretical plans. Forrester summarized this research in two papers: “Cloud Migration Is Actively Embraced, But Not For Everything” and “Hybrid Cloud: An Obvious Reality Or A Conservative Strategy?”
The research looks at a number of different aspects of workload migration, but considering spending patterns is one particularly useful lens. “Follow the money,” as the saying goes.
At first glance, it looked as if these organizations were migrating a lot of applications to public clouds; they’d moved or were planning to move 100 applications on average.
But that translates to only 21% of their workloads today, a number they project to go up to only 25% over the next few years. That’s significant public cloud use but it’s not a wholesale shift.
Forrester also found that private cloud plans are being developed alongside public cloud migrations. They note that: “Even those who are actively migrating their existing workloads to the public cloud currently still report investment in private cloud as a central part of their digital transformation strategies” with the majority citing digital transformation as their primary use case.
As a result, even among these companies (who, remember, were chosen on the basis that they were migrating at least some applications to public clouds) private cloud spending is on the rise. A whopping 87% plan to increase or maintain their level of investment in private cloud over the next four years. This nearly equals the number planning to increase their public cloud spend (88%). It’s also consistent with previous Forrester research, which found that: “Even among enterprises not specifically focused on cloud, or migration, investment interest in private cloud is still high. Sixty percent of North American and European enterprise-hardware decision makers report that building a private cloud is a critical or high priority for their business.”*
Interviews that Forrester conducted as part of this current survey tell the same story in words. For example, the head of infrastructure, architecture, and design at an American financial services firms said: “I think we’ll always be in a hybrid mode. We’ll always need internally hosted apps; I don’t think we’ll ever get everything into public cloud. We might move those apps to a managed service somewhere, but they can’t move to public.”
These results might be surprising to some who are accustomed to hearing about the inevitability of a near-universal shift to public clouds. Indeed, Forrester notes that, as shown by this and other research, “The reality for cloud migrators differs greatly from the current migration dialog.”
Am I surprised? Nope. It’s what I hear from customers all the time. They’re essentially all hybrid in some form. They’re adopting many of the principles often inspired by public clouds: self-service, automation, speed, flexibility. But they’re often doing so on premise while using public clouds where appropriate. One size doesn’t fit all; the history of IT has shown time and time again that if you bet against any single approach conquering all, you’ll likely win.
As Forrester puts it: “The preponderance of both public and private cloud use indicates that no single cloud deployment will be the future for enterprises. Rather, the future of cloud is likely to be a hybrid story.”
*Private Cloud In 2020: Defining The Future Of The Enterprise Data Center,” Forrester Research, Inc., August 24, 2017.
Red Hat cloud guy, photographer, traveler, writer. Opinions are mine alone.
1 
1 
1 
Red Hat cloud guy, photographer, traveler, writer. Opinions are mine alone.
"
https://medium.com/@alibabatech/alibaba-cloud-launches-cloud-box-extending-public-cloud-services-to-local-devices-8e9fbf929a2b?source=search_post---------26,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Oct 5, 2020·3 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Alibaba Cloud ECS
On September 18, at the Apsara Conference 2020 held in Hangzhou, Alibaba Cloud announced the launch of Cloud Box. Based on the proprietary X-Dragon architecture, Cloud Box integrates the cutting-edge technologies of computing, storage, and networking to provide users with a fully managed cloud service that combines software with hardware for local deployment. For users who need to deploy their business in local data centers, Cloud Box provides the same experience as Alibaba Cloud’s public cloud.
Migration to the cloud has become a trend in various industries, especially with the rapid development of big data, artificial intelligence (AI), Internet of Things (IoT), 5G, and other technologies. Most companies regard digital transformation as a priority strategy. However, due to the requirements of compliance, bandwidth, latency, and cost, some enterprises are deploying some of their business in local data centers, but they also want to obtain the same benefits of low costs, elasticity, and agility as provided by the public cloud.
Cloud Box was created to solve this problem. Based on the X-Dragon architecture, Cloud Box provides users with the same cloud product experience as provided by the public cloud. At the same time, through the Virtual Private Cloud (VPC), Cloud Box facilitates the integration of applications deployed on the premises with other local applications or cloud services. Compared with the on-premises data centers, features of Cloud Box, such as on-demand ordering and pay-as-you-go billing, are provided to help users save the one-time investment of offline user-owned IDCs and avoid the cost of idle capacity caused by low device utilization.
The X-Dragon architecture is a hardware-software integrated computing architecture developed by Alibaba Cloud. This architecture helps users obtain computing capabilities superior to those of traditional physical machines. Based on the “X-Dragon architecture”, Cloud Box provides users with an elastic scaling service in on-premises data centers, high-performance computing capabilities, and excellent I/O performance. Meanwhile, by using Alibaba Cloud’s proprietary network devices, users can connect cloud box to local IT facilities to meet ultra-low latency network demands.
Cloud Box provides all types of elastic computing instances, databases, security, container services, and other products. Cloud Box ensures that instance version upgrades, security vulnerabilities, and patch upgrades are synchronized with the public cloud. Users can immediately have access to the latest products and features available on the public cloud.
Security compliance is one of the important reasons for enterprises to deploy their business in local data centers. Users can deploy Cloud Box in local data centers and store their data locally. The enterprise-level network isolation is achieved through the seamless interconnection between the VPC and the public cloud. Therefore, applications can be accessed over an internal network, which meets the regulatory requirements of customers from special industries.
Based on the capabilities of Alibaba Cloud’s public cloud, Cloud Box provides a fully managed cloud service that integrates software and hardware deployment locally. Users can enjoy the same stable experience and SLA services provided by the public cloud without the need for their own maintenance.
Enterprises have changed their computing requirements from “full scenario coverage” to “ubiquitous location”. Alibaba Cloud offers full-scenario coverage to cover “cloud, edge, and end” computing and has launched a series of new product deployment and operation forms. In the future, Alibaba Cloud will extend the public cloud to a broader space and work with its customers to get ready for the upcoming edge computing and 5G era.
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
8 
8 
8 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@renebuest/the-big-misunderstanding-shared-responsibility-in-the-public-cloud-40eb5743c3f1?source=search_post---------27,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 2, 2016·6 min read
Responsibility in the public cloud is a story of several misunderstandings. Advisory sessions and conversations with different companies interested in public cloud unveil the certainty that the classical outsourcing concept is still widely spread among IT decision makers. Public cloud providers are being seen as full service providers. That complicates negotiations at eye level and blocks the quick adoption of public cloud services. „Shared Responsibility“ is the keyword that needs to be internalized. This research note clarifies the wrong assumptions and describes the concept.
In the past 10 years, for the sake of convenience cloud computing was often defined as “Outsourcing 2.0”. What should have led to a better understanding on the user side, however, did public cloud providers a disservice. With the understanding in mind — an external service provider takes over responsibility for (partly all) IT operations — IT decision makers developed the expectations that public cloud providers are full service providers. The IT department just coordinates and controls the external service provider.
What is true for a software-as-a-service (SaaS) provider as a vendor of low-hanging fruits is completely different at platform-as-a-service (PaaS) and in particular at infrastructure-as-a-services (IaaS) level. SaaS providers are delivering ready developed and ready-to-use applications. The complexity, for example with solutions from Salesforce and SAP, comes with the configuration, customization and, if necessary, the integration with other SaaS providers. So, the SaaS provider is responsible for the deployment and the entire operations of the software, and the necessary infrastructure/ platform. The customer is consuming the application. PaaS providers are deploying environments for the development and operations of applications. Via APIs, the customer gets access to the platform and can develop and operate its own applications and provide those to his own customers. Thus, the provider is responsible for the deployment and the operations of the infrastructure and the platform. The customer is 100 percent responsible for his application but doesn’t have any influence on the platform or the infrastructure. IaaS providers only take responsibility at infrastructure level. Everything that is happening at higher levels is in the customer’s area of responsibility.
Thus, it is wrong to see public cloud providers such as Amazon Web Services, Microsoft Azure or VMware (vCloud Air) as full service providers who take whole responsibility for the entire stack — from infrastructure up to application level. Self responsibility is required instead!
A decisive public cloud detail that contrasts this deployment model clearly from outsourcing is the self -service. Depending on their DNA, the providers are only taking responsibility for specific areas. The customer is responsible for the rest.
In the public cloud, furthermore, it is about sharing responsibilities — referred to as Shared Responsibility. The provider and its customer divide the field of duties among themselves. In doing so, the customer’s self-responsibility plays a major role. In the context of IaaS utilization, the provider is responsible for the operations and security of the physical environment. He is taking care of:
The customer is responsible for the operations and security of the logical environment. This includes:
A very important part is security. The customer is 100 percent responsible for securing his own environment. This includes:
Thus, the customer is responsible for the operations and security of his own infrastructure environment and the systems, applications, services, as well as stored data on top of it. However, providers like Amazon Web Services, Microsoft Azure or VMware vCloud Air provide comprehensive tools and services customers can use e.g. to encrypt their data as well as ensure identity and access controls. In addition, enablement services (microservices) exist that customers can adopt to develop own applications more quickly and easily.
By doing this, the customer is all alone in its area of responsibility and thus has to take self-responsibility. However, constantly growing partner networks are helping customers to set up virtual infrastructures in a secure way and run applications and workloads on top of public clouds.
In addition to requiring an understanding of the shared responsibility concept, using public cloud infrastructure also makes imperative the rethinking of the infrastructure design as well as the architecture of the corresponding applications and services.
During the way to public cloud infrastructure, the self-service initially looks simple. However, the devil is in the detail and hides in the complexity that is not obvious at first. That is why CIOs should focus on the following topics from the start:
The growing number of cloud migration projects at big medium-size companies and enterprises indicate that public cloud infrastructure platforms are becoming the new norm, while old architecture, design and security concepts are being replaced. After public clouds have been ignored over several years, this deployment model now also makes its way on the digital infrastructure agenda of IT decision makers. However, only CIOs with a changing mindset taking the shared responsibility concept for granted will successfully make use of the public cloud.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
9 
9 
9 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@renebuest/service-management-in-the-public-cloud-e5a7354e9565?source=search_post---------28,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 5, 2016·2 min read
In the coming years the Public Cloud will inevitably continue to take hold. From a technical point of view, the use of dynamic infrastructure is the only means to respond to ever changing market situations and to address them in a proactive fashion.
The black box Public Cloud makes it harder for IT organizations to keep sight of the big picture and to live up to their supervisory obligations. This becomes evident mainly through the lack of close ties to the actual IT operations of the cloud infrastructure.
The use of Public Cloud infrastructure is based on the shared responsibility model in which the responsibilities are clearly separated between the provider (physical environment) and his clients (logical environment).
In addition to the full responsibility for the logical environment, the customer does not only need to find an answer to the question of how to handle the black box — the physical environment — but also how to measure the services of the cloud provider at this level in order to maintain control.
With ITIL, CIOs have a powerful framework at their disposal which enables them to monitor the public cloud provider at all levels. Through established ITIL procedures, they can provide the business side with the facts that are required for reporting.
The strategy paper can be downloaded free of charge under “Service Management in the Public Cloud — Sourcing within the Digital Transformation“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
1 
1 
1 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/foundations/security-in-public-cloud-how-times-have-changed-3790ec115ca7?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
One of the most significant changes around public sector ICT in recent years has been the attitude to public cloud from a security perspective. One only has to go back a few years for the prevailing mindset to have been that public cloud could never be considered secure enough for public sector workloads. Considerations around UK data sovereignty took precedence and the generally accepted wisdom was that if you wanted security, then you had to do it yourself in your own data centre.
How times have changed.
Now, there seems to be a generally held view that the investments that the hyperscale public cloud providers are able to make around security makes it almost impossible for other providers, including those in-house, to keep up.
This article will outline the benefits of public cloud from a security viewpoint.
The most significant of these is scale, which applies in three ways.
Firstly, the scale of AWS, Microsoft and Google allows them to invest in security in a way that smaller companies are not able to, spending vast sums of money on securing their own data centres, platforms and connectivity.
Secondly, the breadth of the customer base of the hyperscale public cloud providers also means that they all have customers who are highly demanding in security terms — banks and other fin tech companies, the military, content owners such as media companies, healthcare providers and so on. These customers place significant demands on the public cloud providers, forcing them to behave in certain ways with respect to security. But all customers, large and small and irrespective of their particular security requirements, gain the benefits of what the providers have to do to meet the requirements of the most demanding.
Thirdly, having a large number of customers in one space, albeit a geographically distributed space, allows the public cloud providers to use machine learning and other artificial intelligence techniques to spot and predict attacks, including detecting new types of attacks as they emerge, much more quickly than smaller providers would be able to. Because of this, public cloud providers are able to take what they learn about an emerging attack in one geographic region and apply it to other regions in advance of the attack actually happening there.
It is worth remembering that both AWS and Microsoft provide what they call a shared responsibility model. That means that they accept responsibility for the security ‘of the cloud’. Their customers, and managed service providers by extension, are responsible for security ‘in the cloud’.
The three scale-related security considerations above apply mainly to the former, i.e. they enhance the ability of the hyperscale public cloud providers to ensure that they are able to deliver the security ‘of the cloud’.
On the latter, security ‘in the cloud’, it is up to customers and managed service providers, to ensure that whatever services are deployed to that secure cloud adopt best security practice.
However, even here scale comes into play. Because of the large number of customers making use of the cloud providers, two things happen.
Firstly, a significant body of shared ‘good practice’ builds up, meaning that not every customer has to learn everything for themselves.
Secondly, both public cloud providers and the marketplace of third-party vendors that grow up around them, are able to develop and offer a highly-featured set of security-related tooling.
If we think about good information security practice, which is captured in the three facets of the ‘CIA’ model — confidentiality (ensuring that only people who are allowed to access information are able to do so), integrity (ensuring that information is not tampered with or otherwise compromised while it is being stored), and availability (ensuring that information is available when it should be) the public cloud providers are able to offer a wide range of tooling to support these different facets.
For confidentiality, the hyperscale public cloud providers offer very rich identity and access management tooling, coupled with role-based access control models and an array of encryption services covering both data at rest and in transit.
For integrity, they offer advanced log-monitoring, and a vast array of native and third-party tooling that can apply machine learning techniques to that data to spot unusual access patters and behaviour. They also increasingly offer a web application firewalling (WAF) capability.
For availability, they provide the inherent resilience of the underlying infrastructure fabric, significant levels of replication built into their data storage and other offers, as well as content distribution networks to ensure that data is delivered to consumers as rapidly as possible.
Taken together, these features provide a wealth of security-related features, one that continues to evolve at an accelerating pace, that other providers struggle to keep up with.
A blog by andypowe11
5 
5 claps
5 
Written by
Cloud CTO, Jisc
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/public-cloud-managed-kubernetes-our-hands-on-experience-9520ce7c823b?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Sep 25, 2018·8 min read
With Kubernetes firmly established as the way to orchestrate container-based applications, all major cloud providers have rolled out their managed Kubernetes products. In a recent post, we covered why this is good, yet unsurprising, news — and how this will accelerate the emergence of the pipeline problem.
On a technical level, we run our own stack on Kubernetes (see case study), and use mostly homegrown container ops tooling around the deployment pipeline on one side, and lifecycle management on the other. But offering some of that tooling to our customers means that we routinely test it — e.g. our container deployment pipeline Cloud 66 Skycap — with various Kubernetes-based services, so we can continue to build our roadmap. This gives us a great vantage point, so in this post, we’ll take a step back and attempt to do a quick review of the major clouds’ managed Kubernetes services. We are going to keep this review updated as new products come about and the existing ones get updated. If we’ve left anything out, please let us know! (Note that for efficiency, we’ve focused on services from what we see as the leading clouds.)
Amazon Web Services is the largest public cloud provider by far, with both native and Kubernetes-based Container-as-a-Service offerings:
Amazon Elastic Container Service, or ECS for short, was the first container-based runtime product AWS released. ECS acts as a runtime that orchestrates containers on top of EC2 instances in your account. ECS works with Docker containers but is not based on Kubernetes (the ECS scheduler is AWS’s own proprietary one). As an AWS product, ECS has been popular enough for many of those who were running their non-container workloads on AWS and wanted to use containers. However, in the last several months, we’ve seen a shift of mostly net-new workloads towards EKS, AWS’s service built on Kubernetes. There are some technical differences between ECS and EKS, mostly around load balancing (e.g. the lack of a node-based load balancer like kube-proxy in ECS means tighter integration with AWS-specific load balancers like ALB).
Pros
Cons
AWS Elastic Kubernetes Service (officially, Amazon Elastic Container Service for Kubernetes) is their official Kubernetes rollout. EKS works on top of your own EC2 instances and delivers a managed Kubernetes experience, while giving you access to the underlying EC2 instances that power it. EKS can be managed via kubectl (the Kubernetes native CLI client) which is a great benefit compared to ECS. Setting up EKS is not very easy if you are not familiar with IAM, VPC, and other AWS setups, and can be quite challenging if you are starting out with both AWS and Kubernetes. This complexity increases the chances of making mistakes which are harder to fix later on, and might even introduce security vulnerabilities into your infrastructure.
Pros
Cons
AWS Fargate is a fully managed Containers-as-a-Service, offered for both ECS and EKS. It removes the need to launch and maintain the EC2 instances needed to power your cluster, and is managed by AWS when it comes to upgrades and maintenance. At the time of writing, AWS Fargate is not available in all AWS regions.
Pros
Cons
AWS Elastic Container Registry is an AWS Docker registry hosted and managed by AWS. You can use it to store your built Docker images to be used by ECS, EKS, Fargate or your own clusters just like any other hosted Docker image repository. However, while compatible with Docker images, ECR has its own slightly different way of dealing with the Docker client: you will need to regenerate temporary credentials for the repository to use it with the native Ddocker client for a simple docker pull or docker push. While this is a good security measure and compensates for some of Docker’s authentication/authorization shortcomings, it is yet another example of an AWS process which is ever-so-slightly-different from open tools, which might make your life a bit more difficult if you would like to use ECR in conjunction with non-AWS components.
Pros
Cons
Azure Kubernetes Service is Azure’s official Kubernetes offering. This gives you fully managed Kubernetes with a wide range of different versions supported and deep integration with other Azure services, including Azure Active Directory (AzureAD) and Service Principals which could be very useful for controlling access to the cluster for enterprises that use Microsoft Active Directory. As you would expect from Microsoft, AKS is very flexible but can involve a steep learning curve as you’d require to familiarise yourself with other Azure concepts around storage, networking and identity management before setting up a production cluster.
Recently Azure had a major availability incident which affected many of their services including AzureAD and AKS (which they have started debriefing here). While this is not very rare in the cloud space, the length of the downtime and the speed that Azure recovered from it can be a cause for caution when choosing Azure to run high SLA production workloads (though you could also argue that Azure is now less likely to repeat the same mistakes).
Pros
Cons
Azure container registry provides a decent private docker registry to be used with Azure and non-Azure docker workloads. If you are using Microsoft Active Directory, Container Registry is a good candidate with its full integration into Azure AD. Container Registry also integrates natively with the docker client so you can follow your normal development and CI flow when using it.
Pros
Cons
Google Cloud Platform (GCP) is a full service cloud provider alongside AWS and Azure, but unsurprisingly, as the original developer of the core Kubernetes technology (Kubernetes was open sourced by Google in 2014), has the most mature product in this space.
Google Kubernetes Engine is Google’s official managed Kubernetes-as-a-Service product. GKE is by far the most complete and mature product in this field, with support for the latest stable Kubernetes releases being available within a very short time of their release, and constant updates and patching being applied to the platform. GKE offers a vanilla upstream Kubernetes which makes it very easy to get started with if you are familiar with Kubernetes: you will get an endpoint and your cluster credentials and can start using kubectl.
Pros
Cons
Container Registry is GCP’s hosted and managed Docker container registry. It is a standard implementation of a Docker registry with integration into GCP authentication and authorization. This integration runs across the entire GCP product set which allows your team to use their G-Suite credentials to access GCP infrastructure.
GCP’s Container Registry works very well with the native Docker client, without the need for much customization to Docker’s native authentication flow.
Pros
Cons
GCP Cloud Build is a service by GCP that builds Docker images automatically and can be integrated with your git repository to build them upon every commit. Cloud Build was recently “re-announced” at the Next ’18 conference with some improvements, but at its core it is a simple Docker build system hosted by GCP.
Pros
Cons
DigitalOcean is a popular cloud provider among developers and has been improving its service quality and product range for a while now, making them increasingly worthy of being considered a 1st-tier cloud provider used for production workloads.
DigitalOcean has a primary focus on simplicity around all of their products, and in that context, their managed Kubernetes offering doesn’t disappoint. While still in beta, DOK (not the official name for DigitalOcean Kubernetes, but what I’m going to call it here!), is very simple to setup and get started with. Once you have a cluster, you will get an endpoint (managed by DO and not on your own Droplets — i.e. servers) and a cluster certificate and can get started. DOK provides native Kubernetes as a managed service, where you pay for the worker nodes and possibly something for the cluster itself (it’s in beta and free at the moment).
Like other cloud providers that have integrated cloud components like load balancers or persistent block storage, DO has integrated DigitalOcean Load Balancers and Block Storage into their managed Kubernetes service. The result is a good user experience which hopefully will be matched by good availability and service level.
Pros
Cons
Note: Out of beta with the limited availability from 1st October.
The prominence and ubiquity of managed Kubernetes is great news for developers and operators everywhere: worrying about upgrading Kubernetes or its components should be the last thing you need to think about when you are focusing on adding value to your business. At the other side of this equation is a need for better tooling to get you from git to kubectl in a way that is automate-able, reliable, repeatable, and easily maintainable—as well as tools to manage the lifecycle of applications atop of those Kubernetes clusters.
Originally published at blog.cloud66.com on September 25, 2018.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
37 
37 
37 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://medium.com/@nutanix/top-7-blind-spots-to-watch-out-for-in-your-public-cloud-strategy-9979570b1e5e?source=search_post---------31,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Jan 17, 2017·5 min read
A May 2016 survey cites that 51% of surveyed organizations took over a year to plan their public cloud strategy. Few may take up to three years too! It’s completely comprehensible why it takes so long and that a lot of detailing goes into it — understanding the precise costs and challenges that the cloud will introduce, knowing how to make the public cloud approach work for the organization, what tools & technology choices to make that will supplement the cloud adoption, etc.
Despite a detailed, pragmatic approach towards building the public cloud strategy, a majority of organizations still fail at some point. And our cloud geeks attribute it to ‘blind spots’ that get overlooked either due to complexity or lack of awareness. Soon enough, in some cases, these blind spots might take the team back to the boardroom.
To usher in the right approach towards building a seamless and successful public cloud strategy, we’ve collated the top seven blind spots that smart companies watch out for during their cloud-first and cloud-ready journey.
Many companies have realized that the real benefit of cloud computing is not the cost savings it can bring. But it is the agility and time-to-market. And the prominent factor that plays a vital role in bringing such a nimbleness are the TCO models. However, many companies don’t define the actual TCO. They just go by the cost data alone, which may save some operational expenses in the short term but not in the long term. Hence, they end up missing the market when it comes to IT’s ability to deliver the real value of the business.
The way forward is to consider TCO models that also identify gray areas, and take them into account during calculations. Mainly, these models must understand the actual value of cloud-based technology. Plus, they should & must take critical factors into account too, like existing infrastructure in place, existing skills & workforce involved, the cost of all the cloud services when in operations, value of agility & time-to-market, future capital expenditures, and cost of risk around compliance issues.
Understanding the terms of a cloud service is paramount. Agreed. But it is more critical to know who owns the data in the system. The decisiveness lies in carefully checking the terms and conditions of the contract and ensure the data policy includes all the fine lines that ensure the actual owner owns the data.
By doing so, you, as a user, can own and recover the data on-demand. Above all, your service provider cannot access, use, or share your data in any shape or form without your written permission.
While you focus on putting data policy and terms of cloud service in place, you should not change the spotlight on SLAs. A strong SLA goes a long way in monitoring, measuring, and managing how the cloud service provider’s services are performing. The essence lies in working closely with lawyers who can help define strong contracts. And also help you get what you want from the service, and whether this can be expressed in the contract.
If you still find this less important, then consider this scenario: You have SLAs with AWS but have no idea how its SaaS offering is performing. That’s because AWS gives them figures for the performance of the infrastructure, not the software.
Many enterprises fail to develop a cloud strategy that are linked to business outcomes, because they miss out leveraging the real benefits of elasticity feature that a cloud offers. They purchase instances in bulk to handle peak demands, like how they did with on-premise IT infra, and then turn a blind eye towards idle resources that could be optimized easily. They also overlook the fact that ‘anything and everything’ on the cloud can be codified. And APIs can be used to automate the tasks on the cloud completely.
Even if APIs are used, weak APIs and mismanagement of APIs can take a toll on the elasticity feature of the cloud. Essentially, going NoOps — with efficient APIs and APIs management — is the way forward.
While Continuous Development, Continuous Testing, Continuous Integration, and Continuous Deployment play a significant role in bringing agility into the business process, workforce working on each of these Continuous Delivery stages ( which is the end goal of DevOps) contribute equally to the success of the cloud. Organizations need to identify the right talent and “ PEOPLE PROOF” their DevOps team to make it strong. Essentially, to ensure that there are no roadblocks in achieving any of the milestones due to the skills shortage.
The best way forward is to go the NoOps way, so that more Ops teams can work on innovating on the cloud, rather than operating.
To date, vendor lock-in remains one of the major roadblocks in achieving success in the cloud. To this end, a majority of IT leaders consciously have been choosing not to invest in cloud fully. For the reason that they value long-term vendor flexibility over long-term cloud success, say experts.
One of the best approaches decreed by cloud experts is to avoid assigning business processes and data to the cloud service provider. Another solution, say, experts, not to keep one foot out of the cloud into on-premise, but to completely embrace it in a new way. Here’s how: By managing IT with governance models, taking cost control measures and the processes, etc.
With the choice of public cloud, which features a shared responsibility model, its users are responsible for their data security and access management of all the cloud resources. While building a cloud strategy, one should respect the fact that the freedom of elasticity that the cloud offers is accompanied by greater responsibility. And this responsibility can be administered only by bridging the cloud security and compliance gaps correctly. How? By adopting ‘Continuous Security’ and making a habit of regular audits and backups, preferably automated.
The Final Word
Today’s public cloud is all about driving business innovation, agility, and enabling new processes and insights that were previously impossible. And for this to happen, a practical public cloud strategy is the cornerstone. A strategy that is based on your own unique landscape and requirements while also taking all the critical blind spots into account, This is our take. Tell us what’s your public cloud strategy for 2017 is? Share your learning and stories in the cloud with us on Twitter, Facebook, and LinkedIn.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@alibaba-cloud/why-public-cloud-is-not-a-security-concern-1924521e26b9?source=search_post---------32,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 28, 2018·4 min read
By Chris Riley
Security is still cited as one of the primary reasons to avoid the public cloud. Once upon a time, these concerns made a certain amount of sense. The security tools available from public cloud providers in earlier times were less advanced.
Today, however, we have moved past that age. Even though some people still think the public cloud is insecure, and organizations may still be hesitant to adopt the public cloud for that reason, this mindset will stifle efforts to innovate via the cloud. It will prevent organizations from gaining the cost benefits, application deployment advantages and agility that are products of a cloud-first strategy.
This article explains why the public cloud, when used properly, is as secure as any other type of deployment environment — and may even be more secure in most situations than other deployment options.
The phrase “IT Security” is just a way to start the conversation, because “security” is a blanket term for a class of technologies and practices. The technologies are:
1.) Network Security2.) Application security
And the practices are:
1.) Prevention2.) Detection3.) Remediation
Each dimension has a set of processes, best practices and tools that accompany them. And organizations can specialize in one area over another (for example, most large enterprises have a lot of experience in IT and network security prevention, but little in detection, remediation and application security).
For cloud providers, security is part of the service they provide. Customers expect their public cloud provider to put deliberate effort into preventing attacks, responding quickly if one happens, and staying ahead of modern types of hacks.
Because it is part of their business, they invest a disproportionately higher amount in security than the typical organization. And the types of experience they have are rooted in datacenter-level security, instead of what most organizations have, which is experience in traditional IT security.
Enterprises are not in the business of security. They will generally not staff up enough and procure the best technology to address it — nor do they typically have a 24/7/365 network operations center, and teams dedicated to understanding the trends in hacking. They’re also not as likely to invest in penetration testing, which helps organizations discover vulnerabilities in their software and infrastructure before they are exploited by attackers.
This does not reflect on the quality of the organization’s IT department. It just means that it’s not core to their business. Where IT organizations in enterprises excel is in IT security for the corporate end user, and many organizations have well-established and hardened solutions to protect their end users. But they cannot say the same for applications.
This is the first place where public cloud security is a huge benefit. It’s a separation of concerns. By offloading compute workloads that are not in the domain expertise of enterprise IT departments, people can focus on the systems and security considerations that are.
There are two areas where security concerns can garner some attention.
1.) The public cloud’s ubiquity invites more attacks: This argument is built on the premise that because it is more popular and has more users, it attracts more hackers (like hunting fish in a barrel), and a private cloud is less known, and draws less attention. This argument has weight; however, hackers generally know that private clouds have more back doors, and targeted enterprise attacks have greater ROI. High-stakes hackers target specific enterprises with specific goals in mind. Their attacks are more sophisticated than those scanning ports on a public cloud IP range, which contradicts the primary concern. (By being secluded, you actually might be attacked more.)
2.) Cloud providers’ employees have access: Most public cloud providers can give plenty of proof of internal processes that prevent internal employees from accessing your services, compute and data. Regardless, you are still in control, and consequently, you should do what it takes to have proper access controls and security.
When you move workloads to the public cloud, you maintain control. There are a number of steps you can take to enhance security, such as the following:
Password-protect your VMs. This is the first most obvious example. Identifying VMs is not difficult. If your VM is easily accessible, that is a security mistake you have unfortunately made.
When you purchase compute and services from the public cloud, you are still in control. This means you get to enforce your policies, and implement all the same security practices that you have for your private cloud. Not only do you have the same level of control, which immediately diminishes the vast majority of key concerns, but these services have also developed additional tools. Cloud providers have created tools such as monitoring, access controls, and network isolation for greater security — often with newer technology than most organizations have available on-premises.
IT organizations gain a lot by offloading a large portion of their security concerns to public cloud providers. They get to focus more on strategy and policy creation, which are not only higher- value activities, but are also more fun for most. In addition, organizations today may gain better overall security by relying on the advanced security tools that cloud providers offer instead of in-house technologies that are difficult to manage and may not be ideally suited for the deployment environments that they support.
The complete pros and cons list of reasons to offload security to the public cloud is beyond the scope of this post, but suffice it to say that it contains far more pros than cons.
If you’d like to experiment with the public cloud for application deployment, you might consider the current offer of $300 in free credits to new users on Alibaba Cloud, the first public cloud provider to obtain C5 compliance certification.
Reference:
https://www.alibabacloud.com/blog/Why-Public-Cloud-is-Not-a-Security-Concern_p485663?spm=a2c41.11240300.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://itnext.io/organizations-feel-threatened-by-public-cloud-resources-5e1492aa7913?source=search_post---------33,"Recently, Dell released its 2016 Data Security Survey which aims to obtain a comprehensive look at IT decision makers and C-level executives viewpoint on data security trends and its impact on their businesses. More than 1,300 business and IT decision makers across the United States, Europe, and the Asia Pacific region, were involved.
The survey indicated that organizations see their data at risk in public cloud platforms. It found that, while more employees are using public cloud services in the work environment, businesses feel less confident in their ability to control risks posed by these applications. Therefore companies are restricting access to public cloud resources. “Nearly four in five respondents were concerned with uploading critical data to the cloud, and 58% were actually more concerned than they had been a year ago.”
Interestingly, the survey revealed companies limiting approach in dealing with this problem, despite different solutions that exist, such as intelligent data encryption. The encrypted data is not readable even if hackers gain access to the cloud’s contents. Due to this concern, 38% of decision makers have limited access to public cloud resources within their organization. Furthermore, the majority of the respondents using public cloud are thoroughly dependent on their cloud vendor’s security solutions.
Currently, just one third of companies consider enhancing secure access to public cloud environments as a key focus for their security infrastructure. That is despite 83% acknowledging that employees are using public cloud environments for storing and sharing valuable data.
The report concludes that security programs must provide sufficient security for employees, while enabling, not restricting technology that helps them to be productive. Organizations can restrict or prohibit public cloud usage, however it is more proactive and effective to protect corporate’s data regardless of its location.
Other highlights of Dell Data Security Survey include:
ITNEXT is a platform for IT developers & software engineers…
1 
1 clap
1 
Written by
https://itnext.io/ Common commuter between the realms of sanity and insanity, reasoning and emotions, reality and fantasy.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
https://itnext.io/ Common commuter between the realms of sanity and insanity, reasoning and emotions, reality and fantasy.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/@RadwareBlog/making-public-cloud-migration-a-turnkey-process-4b95e71f18b?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Apr 5, 2017·1 min read
by Frank Yue
Public clouds are great for managing applications and data without the frustration and expense of supporting the underlying infrastructure. When I lease a car, I am able to use it for the standard tasks that I perform. Functionally, the car is able to do the same things as a vehicle that I could purchase. I can run errands, drive to work, or even take trips.
One of the main advantages of leasing the car is that when there is a problem or maintenance needs to be performed, I am not responsible. The automobile dealer where I leased the car from handles all of those tasks. Oil changes, filter replacements, and all significant work to keep the car running well is taken care of without my intervention beyond bringing the car in when requested.
Public cloud environments like Azure give businesses the IT infrastructure to host their applications and data. They create and support the infrastructure so the business only needs to be concerned about the application and data. The IT organization can focus on managing the aspects important to the business and offload the more common and generalized support to the public cloud provider.
Read more: http://ow.ly/RVGJ30aALer
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
1 
1 
1 
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/hybrid-and-multi-cloud-the-real-value-of-public-cloud-infrastructure-f354c4df67df?source=search_post---------36,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·7 min read
Since the beginning of cloud computing the hybrid cloud is on everyone’s lips. Praised as the universal remedy by vendors, consultants as well as analysts the combination of various cloud deployment models is permanently in the focus during discussions, at panels and conversations with CIOs and IT infrastructure manager. The core questions that needs to be clarified: What are the benefits and do credible hybrid use cases indeed exist, which can be used as best practice guidance notes. This analysis is giving answers to these questions and also describes the ideas behind multi cloud scenarios.
Many developers and startups bless the public cloud to escape from high and incalculable upfront costs into infrastructure resources (server, storage, software). Examples like Pinterest or Netflix are showing real use cases and confirm the true benefit. Without the public cloud Pinterest would have never experienced such growth in a short time. Also Netflix benefits from the scalable access to public cloud infrastructure. In the 4th quarter 2014 Netflix has delivered 7.8 billion hours of videos. This is a data traffic of 24,021,900 terabytes of data.
However, what these prime examples are hiding: All of them are green field approaches — like almost every workload that is developed as a native web application on public cloud infrastructure and just represent the tip of the iceberg. However, the reality in the corporate world unveils a completely different truth. Inside the iceberg you find aplenty of legacy applications that are not ready to be operate in the public cloud at the present stage. Furthermore, requirements and scenarios exist for which the use of the public cloud is ineligible. In addition, most of the infrastructure manager and architects know their workloads and its demand very good. Provider should finally accept this and admit that the public cloud in most cases is too expensive for static workloads and other deployment models are more attractive.
By definition, the hybrid cloud sphere of activity is limited to connect a private cloud with the resources of a public cloud. In this case, a company is running an own cloud infrastructure and uses the scalability of a public cloud provider to get further resources like compute, storage or other services on demand. With the rise of further cloud deployment models other hybrid cloud scenarios have been developed that include hosted private and managed private cloud. In particular, for most static workloads — these where the requirements of the infrastructure on average are known — an external static hosted infrastructure fits very well. Variations because of marketing campaigns or the Christmas season — that are occurring periodically — can be compensated by dynamically add further resources from a public cloud.
This approach can be mapped to many other scenarios. In this case, not only pure infrastructure resources like virtual machines, storage or databases must be in the foreground. Even the hybrid use of value added services from the public cloud providers within self-developed applications should be considered, to use a ready function instead of developing it on the own again or benefit from external innovations immediately. With this approach the public cloud offers companies a real value without outsourcing the whole IT environment.
Real hybrid cloud use cases can be find at Microsoft, Rackspace, VMware and Pironet NDH:
In the course of the continuously propagation of the hybrid cloud also multi cloud scenarios are moving in the focus. For a better understanding of the multi cloud, it helps to consider the supply chain model of the automotive industry as an example. The automaker sets on various (sometimes redundant) suppliers, which provide him with single components, assemblies or ready systems. In the end the automaker assembles the just in time delivered parts within the own assembly factory.
The multi cloud respectively the hybrid cloud are adopting the idea from the automotive industry by working together with more than one cloud provider (cloud supplier) and integrating everything with the own cloud application respectively the own cloud infrastructure in the end.
As part of the cloud supply chain three delivery tiers exist that can be used to develop an own cloud application or to build an own cloud infrastructure:
In a multi cloud model an enterprise cloud infrastructure respectively a cloud application can fall back on more than one cloud supplier and thus integrate various Micro Services, Modules and Complete Systems of different providers. For this model a company develops most of the infrastructure/ application on its own and extends the architecture with additional external services whose effort would be much too big to redevelop it on its own.
However, this leads to higher costs at cloud management level (supplier management) as well as at integration level. Solutions like SixSq Slipstream or Flexiant Concerto are specialized on multi cloud management and support during the usage and management of cloud infrastructure across providers. On the contrary Elastic.io works on several cloud layers, across various providers and supports as a central connector to make cloud integration easier.
The cloud supply chain is an important part of the Digital Infrastructure Fabric (DIF) and should be considered in any case to benefit from the variety of different cloud infrastructure, platforms and applications. The only disadvantage is that the value added services (Micro Services, Modules) named above are still only available in the portfolios of Amazon Web Services and Microsoft Azure. In the course of the rapid development of use cases for the Internet of Things (IoT), IoT platforms and mobile backend infrastructure are taking an ever-growing significance. Ready solutions (Cloud Modules) are helping potential customers to reduce the development effort and giving impulses for new ideas.
Infrastructure providers whose portfolios still focus on pure infrastructure resources like servers (virtual machines, bare metal), storage and some databases will disappear from the screen in the midterm. Only the ones who enhance their infrastructure with enablement services for web applications, mobile and IoT applications will remain competitive.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-ranked-as-leader-in-forrester-wave-public-cloud-development-and-infrastructure-28a9ca98afb0?source=search_post---------37,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 1, 2020·7 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Developer
Forrester, an international authoritative research institute, recently released the report, “The Forrester Wave: Public Cloud Development and Infrastructure Platform in China, Q4 2020”. The report evaluated major providers of public cloud services in China from the three dimensions of strategies, products, and market landscape, focusing on developer experience, development services, and operations at the infrastructure layer. Alibaba Cloud received full marks in the evaluation of container services, serverless computing, and microservices. Moreover, Alibaba Cloud got the highest comprehensive score in nine evaluation items of application development services, ranking at the top in the cloud-native field.
According to Forrester’s report, Alibaba Cloud is a leading platform for enterprises’ software development and operations. As the first company that enters the public cloud market in China, Alibaba Cloud has extensive experience in helping Chinese enterprises meet their development and operation requirements. Alibaba Cloud provides a wide range of services and is widely used to develop container services, microservices, and serverless computing. Alibaba Cloud also provides multiple vertical solutions for various industries such as retail, finance, manufacturing, and government agencies. For enterprises that require digital transformation, Alibaba Cloud is an ideal choice.
In the report, Forrester said that as more and more Chinese developers use public cloud to develop applications, they have helped to transform an old application architecture to a modern one. Leading cloud service providers are committed to creating comprehensive development experiences and innovative application practices. These factors reflect the core competitiveness of cloud service providers. In the evaluation of application development capability, Alibaba Cloud got the highest score in container services, microservices, and serverless computing. In addition, Alibaba Cloud got the highest comprehensive score of nine evaluation items among all cloud service providers in China.
Over the past few years, container services have become increasingly accepted by enterprises. Alibaba Cloud provides the most various container products in the industry. Its container services have been growing by more than 400% in consecutive years. During the Double 11 in 2019, Alibaba’s core systems were migrated to the cloud for the first time. During the Double 11 in 2020, Alibaba’s core systems were fully cloud-native, with 80% of its core business being deployed on Container Service for Kubernetes (ACK). ACK can scale up millions of containers within an hour, which helps deliver 58.3 million transactions per second at business peak.
Currently, ACK is available in 21 public cloud zones in China and overseas. ACK also allows customers to deploy Kubernetes in their own data centers and edge side. Based on ECS bare metal instances, ACK provides excellent performance, efficient scheduling, and comprehensive security. Alibaba Cloud also provides a variety of differentiated products, such as Alibaba Cloud Service Mesh (ASM) compatible with Istio, Serverless Kubernetes (ASK) based on Elastic Container Instance (ECI), exclusive Container Registry (ACR) that provides image scanning, secure sandbox container runtime based on lightweight virtual machine technology, edge computing container service (ACK@Edge), and Alibaba Cloud Genomics Service (AGS). At the end of September, Alibaba Cloud took the lead in passing the container performance test of the China Academy for Information and Communications Technology (CAICT) and obtained the highest-level certification: “excellence.” Alibaba Cloud can provide 1 million pods and 10,000 nodes in a single cluster and has built the largest container cluster in China. In Forrester’s evaluation of public cloud container services, Alibaba Cloud ranked first in China as a strong performer.
In addition to continuous optimization for the cloud-native experience, Alibaba’s cloud-native team is also developing solutions for a wide range of scenarios, helping enterprises reduce costs by leveraging new technologies. With Apsara DevOps, the cloud-native team built an all-in-one cloud-native DevOps solution for general Kubernetes scenarios, microservices based on Spring Cloud or Dubbo, and lightweight function computing. This helps more enterprises to start cloud-based R&D.
In the Forrester report, Alibaba Cloud received the highest score in the container evaluation and showed eye-catching performance in serverless computing and microservices. Alibaba Cloud is the only cloud service provider in China with the highest score. Alibaba Cloud provides the most complete serverless product matrix among all cloud service providers, including function compute, serverless application engine (SAE), ASK for container orchestration, and ECI for container instances. At the same time, Alibaba Cloud is continuously optimizing the user experience of serverless products, including the full integration of function compute into the container ecosystem and the addition of container image triggers. Alibaba Cloud open sources Serverless Devs, China’s first serverless developer platform, to provide the toolchain for developers. SAE provides flexible policy configuration in the QPS/RT dimension, adds enterprise-level features such as current limiting and downgrading, and strengthens application lifecycle management. EventBridge, a serverless event bus, is released. The standardized CloudEvent 1.0 protocol helps users easily build a loosely coupled and distributed event-driven architecture.
On October 23, Alibaba officially open sourced its first Serverless developer platform, Serverless Devs, which is also the first platform in the industry that supports the full lifecycle management of cloud-native products based on mainstream serverless services and frameworks. Serverless Devs includes Serverless Devs Tool and Serverless Devs App Store. This helps developers to experience multi-cloud products with ease and rapid deployment of serverless projects.
Alibaba Cloud has made efforts to develop serverless computing and accumulated most serverless computing users in China. In a recent research report on serverless computing released by Cloud Native Computing Foundation (CNCF), Alibaba Cloud’s function compute topped the list with 46% of serverless computing users. According to the report, many Chinese developers are migrating from traditional architectures to serverless architectures. This figure is also recorded in the first survey report on China’s cloud-native users released by CAICT. The report shows that, in China, the user scale of Alibaba Cloud’s serverless computing accounts for 66% of all users, more than other cloud service providers combined.
As an R&D base, serverless computing is accepted by more enterprises and applied in business practices. In addition to the early “taste” of internet enterprises, traditional enterprises are also exploring the large-scale use of serverless computing. Take Century Lianhua for example. During the Double 11 in 2019, function compute helped Century Lianhua survive the shopping festival. This year, Century Lianhua migrated its entire business to function compute 2.0, dealing with the transaction that is 230% more than that of last year. In addition, function compute improved the delivery efficiency of R&D by more than 30% and reduced the cost of elastic resources by more than 40%. Today, the entire Alibaba economy is practicing serverless computing, including Taobao, Tmall, Alipay, DingTalk, Fliggy, Idle Fish, and Yuque. The application scenarios of serverless computing are extended to frontend full-stack, mini programs, microservices, new retail, and games.
Alibaba has deep accumulation in microservice systems and has performed a lot of work in open source projects. Through some open source projects such as Dubbo and Nacos, and commercial products such as Enterprise Distributed Application Service (EDAS) and Microservice Engine (MSE), the experience and practices in Alibaba microservice system are exported to external users. In addition, Alibaba is also integrating microservice systems and open source technologies with the cloud so that customers who use cloud can easily and directly use open source products. In addition, Alibaba Cloud is also promoting the evolution of microservice systems to the next generation, especially in the cloud-native field. Alibaba Cloud is vigorously promoting the integration of microservice systems and Service Mesh to achieve better compatibility and interoperability, which improves the activity and maturity of the entire Service Mesh ecosystem.
In this year’s Double 11, Alibaba implemented the world’s most extensive cloud-native practice, which once again confirms the great value of cloud-native for enterprises and society. Cloud-native technologies based on containers, microservices, and serverless computing will transform core enterprise architectures into Internet-based, help enterprise IT infrastructure be more flexible and autonomous, and improve business agility. This will help enterprises flexibly deal with uncertainties in the business world. Cloud-native technologies will promote the collaboration of applications on the cloud, edge, and devices, and build dynamic, large-scale, and borderless cloud application architectures.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/interview-innovation-and-scalability-in-the-public-cloud-c01a95e065b3?source=search_post---------38,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 25, 2016·3 min read
In this interview with Cloud Era Institute, I discuss the growing trend of companies opting into the public cloud to leverage scalable infrastructure and global outreach. I also share how vendor lock-in contributes to innovation. I provide an important distinction between data privacy and data security, and explain why the public cloud is a shared responsibility.
What trends are you seeing in the public cloud right now?
The first trend is that the public cloud is growing because enterprises need innovation and global scalability. Previously enterprises talked about building private cloud environments, but soon realized the financial impact of building a massive, scalable infrastructure. Last year, Amazon Web Services (AWS) released over seven hundred new services and functionalities, something that would not be possible for a private cloud or a web hosting company. Amazon and Microsoft Azure are investing heavily in innovations at the infrastructure level, in data center operating, and new services.
Another big trend we are seeing are containers like Docker, which has gained momentum because of the importance of portability. With Docker, you can move workloads to different cloud providers. You can capitulate your application and its dependencies in a container, then move everything from one system to another.
A third trend is microservices such as Azure machine learning or AWS Short Notification Service. You can use the microservice approach to create your own powerful application.
Netflix is an on-demand video streaming platform with massive scalability and a highly available application on top of AWS, created based on microservice architecture. Their application is always running because when one microservice has a problem, the others remain unaffected. It is a single application running on top of AWS and connected via a public API.
CIOs and developers typically don’t like vendor lock-in, but I believe it helps with innovation. If you are using an iPhone, then you are totally locked in the Apple environment, and you love it. Apple is able to innovate because they have a closed ecosystem. It’s the same with AWS and Azure, since they also have service lock-in. This is how companies are able to innovate.
What has been the biggest challenge for businesses in the public cloud?
For Germany, Europe, and the U.S., it is data privacy and security. It is important to separate data privacy and data security. Data privacy is about legality issues and ensuring that you are fulfilling the law. Data security means that data is stored securely so nobody can access it without authorization.
Germany thinks its data centers are more secure than the U.S., which is not true. Data centers in Germany, the U.S., or Australia have the same physical security. When it comes to data security, it is no big deal to store data in the U.S.
Another big issue is a lack of cloud knowledge. The cloud has been around more than ten years, yet there is still a global lag. Many people do not understand how to create cloud applications that can be used on cloud platforms; from the design, to the architecture, microservices, and containers.
Public clouds are shared-security environments. There is a lack of knowledge about this as well. A public cloud provider is only responsible for the physical infrastructure and ensuring that the virtual infrastructure can be deployed.
Everything on top of the virtual infrastructure belongs to the customer. In the public cloud, the customer has to create their own virtual infrastructure, for example on top of AWS, and then has to run systems and applications on top of it. To fire up a virtual machine is not cloud. The application and virtual infrastructure must also be scalable.
What have observed about marketing as it relates to the cloud?
It is not only a cloud issue; it is that many companies do not focus on content marketing. It is better to market your products with good content, not just advertisements. Unique content is just as important as having an expert voice contributing to it. It is better to let the people write who are experts, not the marketing people.
– — –
– — – The interview with Cloud Era Institute has been published under “Global technology expert, Rene Buest, on innovation and scalability in the public cloud“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jstrachan/to-be-clear-im-advocating-using-managed-kubernetes-offerings-on-the-public-cloud-which-run-on-bare-c7474c3f333f?source=search_post---------39,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Strachan
Oct 9, 2018·1 min read
API Expert
to be clear I’m advocating using managed Kubernetes offerings on the public cloud which run on bare metal (or a single VM per host) and every process run inside kubernetes is a linux container. So at most 1 VM per host; often 1 docker daemon and then all processes running natively as a linux container; so no real additional overhead for each process you run on a host- other than the cost of the kubelet/docker daemon per host which is pretty small
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
1
1
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
"
https://medium.com/@alibaba-cloud/is-it-possible-to-deploy-a-private-cloud-in-a-public-cloud-f1d34c2edb94?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 18, 2020·8 min read
Many people might find this question paradoxical. How can a cloud be both private and public? If you are one of the people asking this, keep the question in mind and read on.
Alibaba Cloud users are accustomed to going directly to ECS instances without a second thought. However, there is a hidden gem among Alibaba Cloud resources, called the Dedicated Host (DDH). As the name implies, a dedicated host is a physical host that belongs to a single tenant, which is different from the conventional concept of multi-tenant public clouds. Some users say that a dedicated host is like a private cloud in the public cloud.
Why they say that DDH is like having a private cloud in the public cloud? Let’s start with on-premise deployment.
The traditional method of IT deployment is to purchase some servers and deploy them to build a fixed-capacity resource pool for IT needs. I think we are all aware of the shortcomings of on-premise deployments.
So even with these shortcomings, why do many enterprises still stick with private clouds? This is because the advantages of on-premise deployment cannot be completely replaced by multi-tenant public clouds. These advantages all boil down to one thing: control, over everything.
Each enterprise is unique and therefore has different use cases and IT requirements. This is like buying clothes off-the-rack. You might get a wide range of choices and sizes, but the garments are never going to fit you like a bespoke suit. To extend the metaphor, for enterprises, “fitting” means satisfying specific business requirements. A perfect fit requires that the enterprise has almost complete control over every aspect of their IT environment, such as control over budget, control over cost, and control over deployment.
In its RightScale 2019 State of the Cloud Report, Flexera pointed out that a large proportion of large enterprises (84%) and small to medium enterprises (69%) consider managing cloud expenditures a big challenge. This does not mean that cloud computing is more expensive than traditional deployment, but rather that with the new deployment form comes new budget management challenges. Resource groups, subscription instances, and reserved instances are all cloud computing concepts that can help enterprises better manage their budgets. However, the challenges are still there. Can we then purchase some servers in a public cloud as if we were doing this for an on-premise deployment, and deliver them to business departments or project teams as a resource pool?
I think we can. The DDH is perfect for this. Enterprises can create clusters of physical hosts and then deliver them to business departments for use. Since the resources of the host are fixed, you only need to allocate fixed resource pools to different business departments, and forbid the creation of resources outside the hosts to restrict IT resource expenditures to within the budget. At the same time, DDH supports resource groups and tags, allowing flexible resource allocation to different departments.
Another financial concern of enterprises is the assessment of resource utilization. DDH allows you to view the available and free resources of hosts, and trace CPU usage. By using these numbers, you can easily determine the resource usage of each host at present and over a specified period of time. This is also good news for the O&M team. They can use this information to monitor resource usage and workload in real-time and allocate resources to the development team more effectively.
But hang on. Does this mean you lose the elasticity of the public cloud? The answer is “No.” Alibaba Cloud ensures the elasticity of the hosts. If you run out of resources, scaling can be achieved in just 20 seconds. Creating ECS instances is just like doing so in a multi-tenant environment. You can choose among a variety of instance types and subscription or pay-as-you-go billing methods (you do not pay extra for instances). What you lose in elasticity is just the granularity of scaling. Switching from ECS instances to dedicated hosts should not have any meaningful impact on a stable service.
Just like you need different types of clothing for different weather, Alibaba Cloud provides users with a wide range of instance types for different use cases. For each article of clothing, you need different sizes from XS, S, M, L, up to XXXL to suit different body types. For Alibaba Cloud ECS instances, we have different instance families for different services, and different instance types for different workloads:
For enterprises that need flexibility and elasticity, such as technology companies that are growing rapidly, this is the ideal purchasing experience. Just pick a style and size that fit you, place an order and it’s ready. Then what about enterprises that have mostly stable businesses?
The following figure shows the CPU load of an enterprise over a 24-hour period. The overall average load is only 1% to 2% (see the preceding figure). However, there are still some virtual machines with loads of more than 60%, some even reaching close to 100% (see the following figure).
Above: Average CPU load over 24 hoursBelow: Top 20 virtual machines with the most CPU loads over 24 hours
Is there a more cost-effective deployment method for enterprises like this, where CPU loads are low most of the time, accompanied by the occasional heavy load? In this case, you can use CPU overprovisioning to increase the resource utilization rate of low-load applications, and pin CPUs to high-load applications to ensure performance stability. For applications with special loads, you can create ECS instances with a non-standard CPU-to-memory ratio. All of these steps involve customization, which brings us back to where we started: control, over everything.
The key is that CPU overprovisioning is controlled by the enterprise. The goal is to avoid multi-tenant contention caused by CPU overprovisioning and find the balance between cost and performance. The other key to control lies in the ability for enterprises to obtain host loads in real time, so they can migrate ECS instances across different hosts according to the actual situation. All of these are possible using the resource pool created by the dedicated hosts.
With the same resources, you can create N times the nodes, at 1/N the cost.
CPU overprovisioning is great at helping users reduce application deployment costs. You can check the CPU usage of the servers you manage and see what optimizations can be made. This is a convenient and simple way to increase deployment density and does not involve application changes.
Cloud computing uses virtualization technology to present underlying physical hardware as standardized computing resources, thus freeing users from the headache of maintenance of physical hardware. This standardization is now gradually moving toward Serverless. Perhaps one day, we can really stop caring about the server, and simply use interfaces and events to complete the delivery of computing capabilities.
But let’s come back to earth for a minute. Most enterprises are still in the era of physical servers or virtual machines. Industry regulations or security audits may require exclusive physical servers, or the commercial license you bought may be bound to the physical machine, or your core business system may need anti-affinity deployment to reduce the impact of single points of downtime. Whatever the reason, there are cases where we still need that tangible physical deployment, and the concept of the physical machine has not reached the end of its days yet.
However, in the real world there are countless deployment requirements. Especially for enterprises that have on-premise deployments, because such enterprises have always had complete control over physical resources, their deployment requirements are not only the result of the businesses they run, but are also influenced by their O&M culture. When asked about an enterprise’s ideal cloud deployment scenario, different users may have different answers. The simplest and most direct way is to give users full deployment control.
A dedicated host is an exclusive physical server, which can meet strict industry regulatory requirements. It also supports Bring Your Own License (BYOL) which means you can use your commercial licenses in the cloud. You can freely move your ECS instances across hosts and still be able to use them without changing their network configurations. Dedicated hosts reside in an exclusive resource pool, but they are not isolated from the public cloud. You can still use VPC network interconnections to transfer data.
Alibaba Cloud provides an elastic resource pool that requires no maintenance or deployment. DDH builds on top of the advantages of the public cloud by giving enterprises more control over their IT infrastructure. This allows enterprises to host their private clouds in the public cloud, to get the best of both worlds.
Differences between on-premises deployment and dedicated host deployment
To learn more about Alibaba Cloud Dedicated Host and its capabilities, visit the official documentation
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-the-public-cloud-increases-devops-success-4e3f9207b595?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 14, 2018·5 min read
By Chris Riley
The best companion to DevOps is the public cloud. While the principles and practices of DevOps can be applied to and can benefit private clouds and internal applications, organizations that have applications in the public cloud find that DevOps practitioners can provide even greater benefit.
First, we need to be very clear about what DevOps means. DevOps comes in two primary flavors: DevOps as a practice, and DevOps principles. The practice involves very specific tactics from DevOps engineers, IT managers, site reliability engineers (SREs), quality engineering, and developers, with a focus on scripted infrastructure, ChatOps, incident management, monitoring, and all other flavors of automation.
DevOps principles guide the tactics. Generally, when you hear the term DevOps, it’s meant to encompass all elements of what makes a sustainable and efficient development environment. It includes organizational structure, DevOps practice, and philosophy. DevOps is not something that is accomplished. It is a means of operating so that the development environment is not at some point beyond its prime, and locked into a specific pipeline toolset or processes.
When we talk about the public cloud enhancing DevOps, we are talking about what can be done at the intersections of practice and principles.
Considering the principle definition of DevOps, you cannot say that one company is or is not doing it. As long as you are focused on building better applications and more frequent and more efficient releases, then you are of the DevOps mindset; therefore, anyone who is building applications can be of the DevOps mindset. But in working to reach some more advanced DevOps practices, the technology stack can be a limiting factor. This includes:
1.) automated testing and continuous testing2.) continuous integration3.) continuous delivery and canary releases
These are hard to implement without the flexibility and services you can find in the public cloud. The public cloud removes a collection of barriers to adopting these practices by removing all hardware obstacles, and making the relationship between source repository and production seamless.
“Public” cloud does not presuppose the application is public. Internal line of business applications can also run in production in a public cloud, which removes barriers for some organizations that might claim public cloud is not for them because they are not building a commercial application.
All the principles of modern development practices can be implemented in private clouds. But the rate of adoption and flexibility cannot match that of applications deployed to the public cloud.
Public cloud boosts DevOps in two dimensions: The first is organizational and process- oriented, and the second is technical. The time from infrastructure request to access to that infrastructure is generally considerably shorter with public cloud than with private. This availability unshackles teams. IT is able to service developers without feeling the high pressure of the requests they receive, and developers do not shy away from features and functionality because of the potential hassle of trying to get the resources they need.
From a technology standpoint, public cloud service providers like Alibaba Cloud are more current and advanced in infrastructure. This is what they do and get paid for. Alibaba Cloud offers DevOps-friendly features such as integrated performance monitoring and the ability to set up continuous delivery chains in an easy fashion. Most companies are not in the business of data centers, and because of that, they are not operating at the same levels that a cloud provider like Alibaba Cloud can.
More specifically, this is what the public cloud does for DevOps:
● On-demand resources: Organizations worry about the pool of resources they want to offer their developers. But with the public cloud, developers can provision resources on-demand. When it’s required, resource negotiations can happen. So IT is free to consider compliance and policy, rather than spending time making sure developers have what they need.
● Better security: Most organizations do not have 24/7 maintenance staff that is always on top of the latest exploits. Because public cloud providers have to make sure their infrastructure is as secure as possible, they are usually far more advanced in security then the typical company. If your private cloud is only accessible in your office building, and not connected to the public Web, it may be more secure than the public cloud. But in most cases private clouds are not this isolated.
● Increased accessibility: The accessibility of public clouds in multiple geographies offers many benefits. It allows developers to build applications from anywhere. Accessibility affords more options for development teams to get their jobs done, with less reliance on static development environments.
● Greater expertise: One major thing public cloud providers do is actively find new services to make developers’ abilities to create, deploy, and run applications even easier: things like SSD instances, Big Data PaaS offerings, release automation tools, and integrations into toolchains and pipelines, along with pre-built application services like messaging, logging, and monitoring. All of these things mean that developers can create more functionality faster, and create more seamless processes for deploying and monitoring applications. For a typical company to give their developers the same offering, they would need to have a dedicated cloud services development team, which is expensive, and likely not as competitive.
The public cloud is not the only path to DevOps, and there are instances (especially in highly secure environments) where it makes sense to stick with a private cloud. In other cases, the hybrid cloud can provide the best of both worlds.
Still, even companies building non-commercial applications that are not accessible to the general public can benefit from leveraging the public cloud to accelerate their DevOps adoption and success.
If you’d like to get started using the public cloud to advance your DevOps strategy, you can take advantage of Alibaba Cloud’s current offer of $300 in free credit.
Reference:
https://www.alibabacloud.com/blog/How-the-Public-Cloud-Increases-DevOps-Success_p438388?spm=a2c41.11225206.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/interview-public-cloud-services-in-search-of-the-white-knight-e8401462b0ab?source=search_post---------42,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 19, 2016·3 min read
Hybrid and multi-cloud strategies are near the top of the agenda for IT decision-makers. They understand that a modern, cloud-based IT world shouldn’t just be drawn in black and white. Diversity is needed to purchase services and innovations from a larger number of cloud providers. Private clouds quickly meet their limits here and don’t offer the benefits of a public cloud.
What is the optimal strategy for using public cloud services in enterprise IT? Find the answers in an interview with T-Systems in “Public cloud services: in search of the white knight”.
Mr. Buest, public, private, hybrid: when does which cloud offering become relevant for a company? There’s no catch-all answer here. We are now seeing an increasing number of companies that are intensely engaging with the public cloud, following an “all in” approach. This means, they do not manage a local IT infrastructure or internal data centers anymore, instead they are migrating everything to public cloud infrastructures or platforms, or purchasing what they need under a SaaS (Software-as-a-Service) model. However, these companies are still a minority.
…and that means? At the moment, most companies prefer to use private cloud environments. It’s a logical consequence of the legacy solutions that companies still maintain in their IT. However, we believe that in the future, a majority of German companies will move to hybrid or multi-cloud architectures, enabling them to cover all the facets they need for their digital transformation.
And how can companies coordinate these different solutions in combination? By using cloud management solutions that have interfaces to the most commonplace public cloud offers, as well as to private cloud solutions. They provide powerful tools for managing workloads in different environments and shifting virtual machines, data and applications around. Another option for seamless management is iPaaS: integration Platform as a Service (iPaaS) provides cloud-based integration solutions. In the pre-cloud era, such solutions were also called “middleware”. They provide support for the interaction between different cloud services.
What do companies have to watch out for principally when using these cloud services? They should not underestimate the lack of understanding of the public cloud, nor the challenges associated with setting up and operating multi-cloud environments. The benefits gained from using multi-cloud infrastructures, platforms and services often come at a heavy price: namely, the costs that result from the complexity, integration, management and necessary operations. Multi-cloud management and a general lack of cloud experience are currently the key challenges many companies are facing.
What is the solution? Managed public cloud providers (MPCPs) are positioning themselves as “white knights” or “friends in need”. They develop and operate the systems, applications and virtual environments for their customers — in both the public cloud infrastructures and multi-cloud environments — in a managed cloud service model.
– — –
– — – The interview with T-Systems has been published under “Public cloud services: What really matters“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@renebuest/public-cloud-the-key-to-a-successful-digital-transformation-9f8e1b0c4d2e?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 5, 2016·2 min read
Within the framework of the digital agenda, IT infrastructure is of central importance. More than two thirds (68 percent) of companies regard digital infrastructure as the most important building block and the key to the successful digitization of their business models and processes. The Public Cloud is one of the most important vehicles of the digital evolution. Only by means of dynamically acting and globally scalable infrastructure are companies able to adapt their IT strategies to continuously changing market situations. Hence, they can strongly support the technical aspects of their company strategy. With a Digital Infrastructure Fabric, companies are mapping the technological image of their “Digital Enterprise“, defining all necessary players and drivers within their digital evolution.
Public Cloud infrastructure services represent a solid base, to support the digitization strategies of companies regardless of their size, predominantly however, companies with very scalable IT workloads. For example, startups are allowed to grow slowly without having to invest massively in IT resources from the very beginning. In this way, companies get a hold on one of the most important features, to have a say in the digital evolution: speed. Today for IT departments, there is more at stake than just preserving the status quo. IT must position itself as a strategic partner and business enabler and be capable of satisfying the individual needs of specialized departments. They need to pursue the goal of creating a competitive edge for the company on the basis of digital technologies. In this context, public cloud infrastructure support the proactive measures of the IT departments.
The strategy paper can be downloaded free of charge under “Public Cloud — The Key to a Successful Digital Transformation“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@auth0/japans-digital-transformation-driving-public-cloud-spend-21c208b0229b?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Auth0
Apr 23, 2018·3 min read
Shedding legacy systems through digital transformation is increasing the global spend on public cloud services, according to a recent IDC forecast. By the end of 2018, worldwide spending on public cloud is expected to reach $160 billion — a 23.2% increase over 2017. While the United States is still spending the bulk of those billions, Japan’s rapidly growing market is expected to contribute at least $5.8 billion to that spend.
With increased reliance on cloud services, comes an increased awareness of the potential for cyberattacks, especially since Japan’s government cloud faces consistent attacks. On April 3, Japan’s National Center of Incident Readiness and Strategy for Cybersecurity warned that official email addresses for about 2,000 government officials had been leaked, reports the Japan News, noting that each year, government agencies face more than one million attacks.
“Without the right protections,” says Auth0 CISO Joan Pepin, “government cloud can be at least as vulnerable as public cloud. Authentication — making sure that users are who they claim to be — is a foundational security control.”
Certain industries, like the discrete manufacturing, manufacturing, and professional services industries that will take up to 43% of Japan’s spend, are turning to IoT to increase their capabilities, according to IDC. While these devices offer rapid, non-touch action as well as the collection of massive amounts of data, they also open organizations to various vulnerabilities, since the IoT industry lacks a consistent set of standards and protocols.
More than 20 billion connected devices are expected to be online globally by 2021, according to Gartner, bringing with them the potential for geometrically increasing risk. These vulnerabilities are easily exploited by hackers — botnet controllers increased by 140% in 2017.
““Without the right protections,” says Auth0’s @CloudCISO_Joan, “government cloud can be at least as vulnerable as public cloud. Authentication — making sure that users are who they claim to be — is a foundational security control.””
TWEET THIS
Given all the cybersecurity risks, Japan, like several other governments around the globe, took significant steps to protect personal data when the Act on the Protection of Personal Information (APPI) came into force last May. Although focused on data protection and privacy like the EU’s GDPR, APPI focuses on protecting the rights of individuals and extends protections to include personal identifier codes, and aggregated information in the “business operator’s” database. The EU and Japan are currently in discussions regarding mutual “whitelisting.”
Based on these regulations and stricter policies around data management in the region, the need for enterprises in Japan to prioritize identity management is undeniable. We addressed this demand by establishing a formal Auth0 presence in Tokyo last year, and launching our website in Japanese: https://auth0.com/jp, and are continuing to dedicate significant resources to this region. Auth0 is uniquely suited to address the challenges of Japan-based and global companies looking to expand in this growing market. If you’re based in Japan and would like to learn more about how Auth0 can create a first line of seamless and secure defense, please reach out to yuichiro.kuwano@auth0.com.
“Auth0’s authentication can help protect industries like discrete manufacturing, manufacturing, and professional services expected to take up to 43% of Japan’s $5.8B public cloud spend spend in 2018.”
TWEET THIS
Originally published at auth0.com.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
"
https://medium.com/@hullsean/should-we-right-size-instances-in-the-public-cloud-9410af4f2694?source=search_post---------45,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sean Hull
Jan 30, 2020·3 min read
Recently he wrote a piece titled Right Sizing your instances is nonesense
Not to be outdone, a blogger Joe at Sunshower.io wrote a counterpoint piece Why Right Sizing instances is not nonsense.
Join 35,000 others and follow Sean Hull on twitter @hullsean.
So what’s the verdict here? Is Corey wrong and Joe right? Or is Joe right and Corey wrong?
I would argue it depends. Corey’s piece emphasizes the big picture, essentially that technical changes can buy more trouble than the money they save. While
Corey’s article uses a broad brush, and in doing so some of his specifics are incorrect. For example the point about older versions of Operating Systems and hypervisor. In most cases your OS won’t know it’s running on a hypbervisor at all, so this seems a very rare edge case indeed.
That said his big picture conclusion seems spot on. Changing instance sizes can be a huge risk if you don’t do so regularly. With legacy apps, who knows how they might behave. In this you carry the same burden as changing instances at all. Your AMI may not be ready, or you may have had some manual steps to build the box again.
Related: How can we keep cloud architectures simple
The first thing about Joe’s post that caught my attention was to use the console to change the instance size. You shouldn’t be manually changing instances through the console to start with. What, everything is code, all the way down? Hehe…
Also his points about cost savings seemed cherry picked. If you can save that much money by changing instance sizes, it is well worth the cost to do regression, integration & disaster recovery tests to make sure it will all work. Get on it!
Also: What hidden things does a deposit reveal?
IAC is really the way to go. But that still doesn’t mean you’re out of the woods. Be cautious when changing instance sizes!
With code, there is testing. So change your Terraform code and then verify it works. Now just because you have a variable indicating the instance size, doesn’t mean changing it won’t break something.
Read: Can communication mixups sour an engagement?
Changing an instance size and redeploying could break all manner of things. It’s possible you used a variable for the instance size in some places and hard coded it in others. Or made some weird reference in an autoscaling group.
It may be the AMI you’ve built works on one type of instance but not another. Or that your AMI is deployed in one region but not another. Or that your old instance size is available in us-east-2, but your new instance size is not yet available there. Yes the console wouldn’t have offered it, but your Terraform code didn’t know.
Check out: How I use 5 daily habits to stay on track
As you’ll see further down in the comments, Joe suggests to put limits around instance size changes. That makes sense. After you’ve done testing, you’ll have an idea of what these limits should be. No instance size less than 30GB memory? No instance size greater than X. None in region Y. Etc.
It’ll require tweaking your terraform infrastructure code, so it’s not a free change. But it’ll pay dividends if your costs savings are in the thousands.
Also: Can daily notes help you work better with clients?
Originally published at https://www.iheavy.com.
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
"
https://medium.com/@hullsean/how-do-we-migrate-our-business-to-the-public-cloud-18e865cf6990?source=search_post---------46,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sean Hull
Aug 10, 2018·3 min read
via GIPHY
The public cloud is no longer a bleeding edge technology for the trailblazers. It’s mainstream now. As you think about it, you consider your customers and the SLAs they’ve come to expect.
Join 38,000 others and follow Sean Hull on twitter @hullsean.
It’s not if, but when to move to the cloud, how to get there, and how fast will be the transition?
Here are my thoughts on what to start thinking about.
Teams with experience in traditional datacenters have certain ways of architecting solutions, and thinking about problems. For example they may choose NFS servers to host objects, where in the cloud you will use object storage such as S3.
S3 has all sorts of new features, like lifecycle policies, and super super redundant eleven 9’s of durability. But your applications may need to be retrofitted to work with it, and your devs may need to learn about new features and functionality.
What about networking? This changes a lot in the cloud, with VPCs, and virtual appliances like NATs and Gateways. And what about security groups?
Interacting with this new world of cloud resources, requires new skillsets and new ways of thinking. So priority one will be getting your engineering teams learning, and upgrading skills. I wrote a piece about this how do I migrate my skills to the cloud?
Related: When you have to take the fall
With the old style datacenter, you typically have a firewall, and everything gets blocked & controlled. The new world of cloud computing uses security groups. These can be applied at the network level, across your VPC, or at the server level. And of course you can have many security groups with overlapping jurisdictions. Here’s how you setup a VPC with Terraform
So understanding how things work in the public cloud is quite new and challenging. There are ingress and egress rules, ways to audit with network flow logs, and more.
However again, it’s one thing to have the features available, it’s quite another to put them to proper use.
Related: When clients don’t pay
While the public cloud collectively is extremely resilient, the individual components such as EC2 instances are decidedly not reliable. It’s expected that they can and will die frequently. It’s your job as the customer to build things in a self-healing way.
That means VPCs with multiple subnets, across availability zones (multi-az). And that means redundant instances for everything. What’s more you front your servers with load balancers (classic or application). These themselves are redundant.
Whether you are building a containerized application and deploying on ECS or a traditional auto-scaling webserver with database backend, you’ll need to plan for failure. And that means code that detects, and reacts to such failures without downtime to the end user.
Related: Why i ask for a deposit
You’ve heard about devops, now it’s time to put it into practice. Building your complete stack in code, is very possible with tools like Terraform. But you may have trouble along the way. I wrote I tried to write infra as code with Terraform and AWS and it didn’t go as expected
So there’s a learning curve. Both for your operations teams who have previously called Rackspace to get a new server provisioned. And also for your business, learning what incurs an outage, and the tricky finicky sides to managing your public cloud through code.
Related: Why i ask for a deposit
As you automate more and more pieces, you may have less confidence in the overall scope of your deployments. How many servers am I using right now? How many S3 buckets? What about elastic IPs?
As your automation can itself spinup new temporary environments, those resource counts will change from moment to moment. Even a spike in user engagement or a sudden flash sale, can change your cloud footprint in an instant.
That’s where heavy use of logging such as ELK (elasticsearch, logstash and kibana) can really help. Sure AWS offers CloudWatch and CloudTrail, but again you must put it all to good use.
Related: Why i ask for a deposit
Originally published at Scalable Startups.
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
"
https://medium.com/@hullsean/are-you-as-good-as-the-public-cloud-fc60b9d60d41?source=search_post---------47,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sean Hull
Feb 2, 2020·3 min read
Did I hear that right?
Join 38,000 others and follow Sean Hull on twitter @hullsean.
Perhaps that is their estimate, or the maximum amount they want to budget for. Regardless that’s a lot of money any way you slice it. A lot of folks are commenting about how crazy that is, and how much datacenter you could build yourself with that much money.
What do you think? Is it foolhardy? Or is there a hidden wisdom here?
Here’s my take.
If you’re comparing the cost of the cloud to the raw numbers of running your own datacenter, the hardware costs are not enough. You’ll need to include the ops teams & other engineers. Right, you probably guessed that.
But did you factor in the costs of a legion of testers. This is the hidden cost that commercial software carries, even while open source software gets this benefit for free.
With a public cloud like AWS you have millions of customers testing the product everyday, and running into edge cases long before you do. So you get a better service, that’s more reliable, all invisibly for free.
Related: How can we keep cloud architectures simple
Anybody who was building web applications in the year 2000 will remember how websites didn’t load the same for different customers. Depending on where in the world they were located, they could experience a very different user experience.
These days we assume that we can be global from day one. But how exactly do we achieve this? Remember with a public cloud, you’re getting tons of things for free, without knowing it. Moving data between AZs or regions? That’s all going across a private interconnect.
And that’s not even including the 180 nodes inside cloudfront that give you a global CDN footprint too!
Also: What hidden things does a deposit reveal?
I remember the days of DBA job role, do you? Probably not. I specialized in this for years, and there were tons of companies hiring me to help them with it. First Oracle, then MySQL, then Postgres.
Then along came Amazon RDS. Guess what, companies don’t really hire for that role anymore. They do need help with it from time to time, but not as a primary specialization.
What do I mean? Well by hosting your application on AWS, you’re benefiting from the work of teams of engineers in different departments, all expanding on APIs and automating things that those one million customers are asking for.
You’re not going to be able to innovate that well and that quickly in your own datacenter. So you’ll pay more!
Read: Can communication mixups sour an engagement?
A quick peek at Terraform’s community modules on Github and you’ll probably blush. From VPCs to bastion boxes, key management to load balancers, lots of code has been written and open sourced.
By deploying on a platform that a lot of other devs are using, you’ll benefit from all this open source code. That means you won’t have to write that stuff yourself.
Sure you’ll have integration work to do, but the hidden benefit of being on a popular platform saves you money.
Check out: How I use 5 daily habits to stay on track
If you build your own datacenter, you have to buy all your capacity. So there are no spare servers sitting around waiting for your use. In the public cloud there is always spare capacity.
What that means is you can write automation code to spinup copies of your application stack in alternate regions, at the push of a button. Thus you effectively get disaster recovery for free!
Also: Can daily notes help you work better with clients?
Originally published at https://www.iheavy.com.
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
"
https://medium.com/@davidmytton/fighting-over-who-has-the-greenest-public-cloud-148fea2027d6?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Sep 25, 2020·4 min read
Earlier in the decade, public cloud was a pricing battleground. AWS, Google and Azure announced regular price decreases. Prices for compute and storage were the usual targets, but innovative pricing mechanisms such as Google’s sustained…
"
https://medium.com/@tcv/is-the-cloud-safe-reducing-business-risk-as-enterprises-aggressively-move-to-the-public-cloud-c422d7da3716?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
TCV
Nov 11, 2019·11 min read
Digital transformation is driving enterprises to rapidly enter the next chapter of cloud adoption. Nearly half of current infrastructure-as-a-service Enterprise users are running production applications on public cloud infrastructure. As such, organizations are acutely focused on dynamic scaling, 24×7 availability, streamlined management and development tools to make the migration seamless…yet, security seems to be an afterthought or maybe just assumed to be “locked down” given that the bulk of workloads are at Amazon Web Services, Microsoft Azure or Google Cloud. Given the brands and heft of these mega tech companies, how can these clouds possibly not be secure?
Recent high-profile breaches demonstrate that there are inherent risks in the public cloud. In fact, just moving workloads to these branded cloud providers does NOT make them more secure at all. It’s clear that enterprises must ensure their security stack is properly architected for the cloud. The recent Capital One breach was a shock to the system.
In the case of Capital One, a combination of a tech savvy team and AWS were breached by vulnerabilities that were known and could have been avoided. Does that mean it’s inherently risky to migrate to the cloud? Probably not, but it is clear we need better tools and processes to make this migration secure, scalable and cost-effective.
In this podcast, TCV’s Tim McAdam and Vectra CEO, Hitesh Sheth, talk about what it takes to reduce business risk in the cloud — and keeping enterprises, consumers and their transactions/interactions secure — while capitalizing on the tremendous opportunities the cloud offers.
For these insights and more, settle back and press play.
***
Tim McAdam: Welcome to Growth Journeys, a podcast series from TCV, focused on lessons from the field from entrepreneurs in the TCV ecosystem. I’m Tim McAdam, General Partner at TCV, and I’m here with Hitesh Sheth, CEO of Vectra, a leader in applying artificial intelligence to detect and respond in real time to cyberattacks in the cloud, data center, and enterprise infrastructures. Hitesh brings a wealth of experience from senior roles at Aruba, Juniper, and Cisco, that affords him important lessons about how enterprises can assess and address security as they migrate workloads to the cloud. These lessons include views on encryption, 5G, and commingled log data, to name a few. We’re covering all these topics today, but first, thanks for joining me, Hitesh, and welcome to Growth Journeys.
Hitesh Sheth: Great to be here, Tim. Thank you for having me.
Tim McAdam: So, let’s start with a relatively simple one, but probably complicated in its scope. What’s the general state of cloud security today?
Hitesh Sheth: Cloud security today is, in my view, where Windows used to be circa 1990s. If you go back in time a couple of decades when Windows started to proliferate, security was really not the first thing that Microsoft thought about. And at that time, it looked like a pretty complex setup with multiple operating system versions, different devices on which Windows was getting deployed, and it felt like it was an endless opportunity for attackers to leverage.
Now, fast forward to today, and if you look at the cloud environment, whether you’re dealing with serverless computing, whether you’re looking at Kubernetes, none of the technologies that are being built for the cloud have had security at the front end, and by comparison we have a thousand-fold more complex scenario than we had when Windows started prevailing from a security point of view.
So, I think the scenario we have right now is that while cloud is taking off exponentially, the security holes that we are facing are indeed very profound.
Tim McAdam: And how do you think enterprises should approach assessing their security vulnerabilities as they migrate these workloads to the cloud?
Hitesh Sheth: One of the most important things that they should think about very carefully is that whatever strategy they had in place in their traditional on-prem networks is not the strategy they should deploy into the cloud. And a good example would be — you think of perimeters when you think of on-prem networks. So traditional firewalls tend to be the way you think about security. That already is disappearing in traditional networks, and that certainly cannot apply when you’re looking at cloud infrastructure.
Now, I think Gartner has come out with a very good synthesis of how to think about building visibility for next-generation SOCs and they’ve got this thing called the Triad, and the Triad has three components to it. There is a SIEM in it. There is NDR, which is network detect and response. And there is endpoint detect and response, EDR. And logically, if you have those three technologies in place, then you have the best shot at delivering comprehensive visibility for the SOC. And the good news there, is that it is independent of whether you’re in the cloud or on on-prem networks as well.
Tim McAdam: Right. And just for the audience, could you define what a SIEM is?
Hitesh Sheth: Absolutely. SIEM is security information and event management systems. A vendor example here would be Splunk. When you’re looking at EDR, a vendor example would be CrowdStrike. And then certainly when it comes to NDR, Vectra would be the example in mind.
Tim McAdam: Perfect. So, talk about encryption for a second and what role encryption will play in securing workloads. And I think there are probably some schools of thought that say, “Why do you need any of this stuff if our data’s encrypted?”
Hitesh Sheth: Correct. So, I think there’s good news and bad news in encryption. Let me start with the good news. The good news is that you can indeed encrypt the traffic from say, the endpoint to the edge of the infrastructure, or to the SaaS application. And so, in theory, you are reducing the opportunities for a hacker to break into that workload or into the payload and initiate a cyberattack. So that’s the good news.
However, the reality is that whether you’re dealing with data centers or you’re dealing with cloud infrastructure, the number of times where the traffic’s going to get encrypted post the edge of the cloud or the data center tends to be very, very limited. And therefore, you have the need to still continuously monitor the inside of the data center or the inside of the cloud for tracking advance attacks. That’s number one.
But number two what is also probably not fully appreciated is that encryption is actually a friend for attackers. So, if your device is compromised, Tim, and then your traffic is encrypted from your device to the SaaS application, then if I’m the hacker, the chances that somebody’s going to pick me up really get diminished. Therefore, you know, logically the only way you can really find those attacks is by looking at the behavior of your device and how you’re interacting with the application. Therefore, behavioral approaches become really essential in this scenario.
Tim McAdam: Right. And that begs the question — that might be a device-specific viewpoint. But how about the data itself? Obviously, multi-tenant cloud applications have effectively commingled log data or log data from multiple customers. Is that a limitation or security risk as enterprises move their workloads to the cloud, and how do enterprises gain comfort that the integrity of their data will remain intact as they move workloads to the cloud?
Hitesh Sheth: The reason logs get commingled in the cloud environment — I’ll come back to the point I made earlier. Security is an afterthought in the scenario. The primary objective of doing that is to add efficiency to IT ops. That is the reason why they do that. For a customer, who is adopting cloud services, you have to reconsider the Triad that I described earlier. You have to have a SIEM. You can take this commingled log data and you can have this centralized in one place for analysis purposes.
But, what is really crucial is that you don’t rely on that by itself. You have to use network detect and response. You have to use endpoint detect and response. And so, the whole point of that Triad is to give you coverage in scenarios like the one you just described.
Tim McAdam: Got it. That makes sense. How about trends around next-gen communications like 5G, for example, and then this whole mindset of zero trust? How do you see these newer trends enhancing, or frankly, causing security issues?
Hitesh Sheth: The benefit of 5G is that we, as users, can bypass traditional networks, and with our devices — whether it’s a phone or a tablet — you can go straight to the cloud and order the SaaS application. You don’t have to worry about your traditional network and the security therein. Which is great.
Now, the challenge with that is that you have just now opened up a direct path into the data without any intermediary layers. So, this is where zero trust is supposed to come in.
Zero trust is supposed to introduce the notion that unless every device is authenticated, it should not be trusted. But frankly, it’s a very simplistic view of security because it essentially says, if Tim on Tim’s phone is authenticated, then Tim and Tim’s device are now automatically safe. But what if somebody stole your credentials? And that happens on a daily basis, as we know. And, therefore, it is not enough to rely on something like zero trust.
You have got to have the right monitoring principles in place in the cloud itself to ensure that if your credentials are stolen on one end, you’ve got the right mechanisms to watch for the behavior of the privileged user in the cloud.
Tim McAdam: Got it. So, let’s talk about responsibility for a second. I recently read a Gartner report that was talking about degrees of hand-off points from infrastructure as a service providers, to platform as a service providers, to SaaS providers. How do you think about this shared responsibility continuum, and do you see this security responsibility changing over time?
Hitesh Sheth: First of all, I think a lot of companies make the mistake of thinking that the security responsibility is solely the cloud provider’s responsibility. And I think that mistake originates from consumers of SaaS applications.
If you are consuming Salesforce, as an example, I think it’s very reasonable to expect that Salesforce has taken care of your security requirements. In theory, that’s generally true. However, if you are the entity that is actually deploying your applications into the cloud environment, having that expectation that AWS, Microsoft, Google, have done the same thing is fundamentally not true.
At the end of the day, the company that’s utilizing cloud resources is responsible for the security of the network layer, the data on top of that, the applications, and how people are interacting with those applications. That responsibility solely resides with the entity that is using those services. And I think even as cloud providers evolve their security offerings, it would be a mistake for consumers of those offerings to relinquish their responsibility back to the cloud provider.
Tim McAdam: So, Hitesh, you can’t pick up the paper today without reading headlines about the shortage of qualified cybersecurity talent relative to the size of the problem. This is a massive issue. Why haven’t more cybersecurity companies adopted an AI/ML framework like Vectra’s given the obvious dearth of humans in the sector?
Hitesh Sheth: I actually think, Tim, that a lot of security vendors are talking about AI today. It’s become one of the pain points for customers, where AI has evolved into a buzzword from vendors, and they talk about it all the time.
The issue fundamentally is that the vendors are approaching this completely wrong, in my view. Even for investors, as they think about investing in companies that are touting AI, the principle of generalized AI simply does not work. Generalized AI equals a human being. And AI is not advanced enough, from a software point of view, to repeat what a human being would do in technology. So, the notion of applied AI is really key here. Applied AI does work as evidenced from the work that we do at Vectra.
And I think the key there is you cannot just take AI by itself. If it’s application-specific, then domain becomes very critical. And one of the early epiphanies that we had in our journey here is that as we experimented with generalized AI, and frankly we made mistakes with that. And what struck us very quickly was that, “Hey, you need security domain, you’ve got to have security domain paired up with AI for this to work.” If I’m a customer, I would be testing for that every single day before accepting a vendor’s word that their tech is actually going to work in my environment. Otherwise, it’s the person behind the curtain actually doing the work, not the software.
Tim McAdam: Right. Well, thank you for making all those generalized AI mistakes before we invested, Hitesh.
Hitesh Sheth: And, yes, we did that in the first few years, Tim, as you know well, but if you don’t make mistakes, you don’t learn. And we are much better off as a result.
Tim McAdam: So lastly, at a recent offsite, one of my partners floated the concept of via negativa, or addition by subtraction, as it related to our business model as investors. That is to say, focus on fewer, more high-impact investment themes or investment types by not focusing on others. Hitesh, should via negativa apply to streamlining the security posture of enterprises as they think about moving to the cloud?
Hitesh Sheth: I think it’s an absolutely fantastic principle for how you think about where you invest in infrastructure broadly and certainly in security, because as we all know, security is rife with a plethora of technologies and vendors pitching the next-greatest tool to customers every single day. Yet, paradigms have evolved very, very rapidly.
So for example, if I am building something from ground up, a customer should ask themselves, why do they really need a firewall? For what purpose? If I have EDR on my endpoint, if I have the right setup for monitoring my workloads in the cloud, what role does a firewall really play? What role does a perimeter play? If you want to save your dollars, OpEx or CapEx, I’ll put something bold out there and say, eliminate the firewall. I would challenge somebody to do that. And then provided they are actually following the SOC Triad — be religious about implementing the SOC Triad.
Do that first and then question the need for spend on anything else next. That’s the approach — that’s how via negativa can apply to security spend.
Tim McAdam: That is bold. I like it. Hitesh, thanks for joining us today.
Hitesh Sheth: Thanks very much, Tim, really appreciate it.
***
The views and opinions expressed are those of the speakers and do not necessarily reflect those of TCMI, Inc. or its affiliates (“TCV”). TCV has not verified the accuracy of any statements by the speakers and disclaims any responsibility therefor. This blog post is not an offer to sell or the solicitation of an offer to purchase an interest in any private fund managed or sponsored by TCV or any of the securities of any company discussed. The TCV portfolio companies identified above, if any, are not necessarily representative of all TCV investments, and no assumption should be made that the investments identified were or will be profitable. For a complete list of TCV investments, please visit www.tcv.com/all-companies/. For additional important disclaimers regarding this document, please see “Informational Purposes Only” in the Terms of Use for TCV’s website, available at http://www.tcv.com/terms-of-use/.
Originally published at https://www.tcv.com on November 11, 2019.
TCV backs growth-stage private & public tech companies. For a complete list of TCV investments, visit: www.tcv.com/portfolio-list.
TCV backs growth-stage private & public tech companies. For a complete list of TCV investments, visit: www.tcv.com/portfolio-list.
"
https://medium.com/security-thinking-cap/public-cloud-security-primer-1895facac0a5?source=search_post---------50,"There are currently no responses for this story.
Be the first to respond.
Public clouds have been greatly promoted as an approach for organizations to reduce information technology (IT) costs and increase technology flexibility and scalability. Cloud computing allows smaller organizations to employ IT services that would previously have been too expensive to implement due to high up-front infrastructure costs. Companies can implement IT solutions faster in a public cloud because they do not have to spend time creating and configuring the technology environment. Larger organizations, already familiar with remote computing operations, gain flexibility and scalability by utilizing cloud services or implementing private clouds to consolidate IT resources.
A public cloud offers different levels of access where more or less services are performed by the provider.
Software as a Service (SaaS) is software that runs on cloud services and is typically accessed over a web browser or through some form of application virtualization. Cloud providers perform all maintenance on the software. All clients need to do is utilize it.
Platform as a service (PaaS) provides computing resources such as processing power, memory, and storage to clients in the form of a virtual machine. The details on the infrastructure hosting this virtual machine may be a “black box” to the customer similar to the Internet. When you sign up for Internet access, you are provided with a line and bandwidth but you do not know how that service is provided to you, what route your data may take, and so forth. Similarly, when renting public cloud space, you are provided with a virtual machine but you do not know the specifics of what is involved in providing it to you.
Infrastructure as a Service (IaaS) provides customers with access to provision virtual machines in addition to configuring their networking and storage. IaaS customers can still perform all the functions of PaaS.
It may be difficult and somewhat unsettling to provide one organization with control over data and systems that are critical to another organization’s success. Nonetheless, there is constant pressure to reduce IT costs by moving to public cloud services while still exercising due diligence in selecting a secure and reliable cloud provider. With the emergence of large companies like Microsoft and Amazon entering the public cloud marketplace, many major companies have felt more comfortable moving to the cloud.
However, the security of the public cloud is still passionately debated. Recently, concerns about public cloud security arose with the release of findings from an investigation into my cloud service providers, Amazon, Gigenet, Rackspace, and VPS. Revelations of the above findings have focused on the following issues.
Cloud computing offers customers computing resources generally in the form of virtual machines for rent at generally lower costs than the organization would incur by hosting the servers in-house. Companies can achieve considerable savings through economies of scale. The rented computing resources are just a portion of the available resources hosted by the provider as much of the infrastructure is shared among clients of the provider. This model presents potential security risks to cloud computing clients if the rented space is not adequately separated from other customers. Inadequate separation could give an attacker, who has compromised one client in the cloud, access to other clients. Attackers could also rent space in the cloud and then use that space as a base of attack on neighboring clients.
Another risk of sharing cloud space is that the actions of shared clients on a public cloud could indirectly impact fellow users if servers that host multiple clients are blacklisted, thus, causing unavailability to multiple clients due to the actions of one in the cloud. In addition to this potential problem are the concerns about where the servers are actually located geographically. The laws in one country may differ greatly and the cloud network may be subject to international laws. There may be limitations on whether data can or should cross international boundaries and contract terms may be less enforceable in another country.
Backup protocols may also present challenges to businesses moving their IT structure to a public cloud. Backup sets, rotations and off-site storage are all managed by the cloud provider. Thus it becomes important to understand how the backups work, how reliable the service is, and how long restores are expected to take. Recovery time is extremely important when essential data is missing from a production system. It is also important to understand whether backup sets can be moved to another provider or to in-house operations if the contract with the cloud provider is terminated. Backup operations are often conducted across many clients at once so it may not be possible to extract historical backup data for a specific client from the cloud.
The report found intra-server vulnerabilities — that data on other clients’ storage was accessible through shared disks and networks. The study was able to access other clients’ virtual disk drives which should have been inaccessible as well as access data from other client systems on the network. These providers did not adequately secure the storage of data and networking resources offered to their clients, thus, leaving them open to a data breach or attack. The virtual machines were housed on systems running outdated hypervisor software that was vulnerable to attack.
When evaluating a public cloud provider, consideration of the following security concerns may be utilized to determine if a potential vendor has the essential cloud security measures in place.
In addition to the above questions, consider running a security audit on the virtual node prior to using it to verify that the above questions are sufficiently answered. The selection of a cloud provider should be based on the security parameters that are provided and the implementation of necessary security controls. The recent study demonstrated that security cannot be assumed even when large, reputable companies are involved. Therefore, it is important to ensure that a cloud provider has these security controls in place by asking questions such as the ones in this article.
Cybersecurity and Privacy thoughts by Eric Vanderburg
Written by
Security and Technology Leader, Author, Speaker, Private Investigator and Expert Witness. Vice President of Cybersecurity at TCDI. www.tcdi.com
Cybersecurity and Privacy thoughts by Eric Vanderburg
Written by
Security and Technology Leader, Author, Speaker, Private Investigator and Expert Witness. Vice President of Cybersecurity at TCDI. www.tcdi.com
Cybersecurity and Privacy thoughts by Eric Vanderburg
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/saving-money-on-batch-workloads-in-public-cloud-361aad8ced64?source=search_post---------51,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 23, 2017·5 min read
Large companies have traditionally had an impressive list of batch workloads, which run at night, when people have gone home for the day. These include such things as application and database backup jobs; extraction, transform, and load (ETL) jobs; disaster recovery (DR) environment checks and updates; online analytical processing (OLAP) jobs; and monthly/ quarterly billing updates or financial “close”, to name a few.
Traditionally, with on-premise data centers, these workloads have run at night to allow the same hardware infrastructure that supports daytime interactive workloads to be repurposed, if you will, to run these batch workloads at night. This served a couple of purposes:
Things Are Different with Public Cloud
As companies move to the public cloud, they are no longer constrained by having to repurpose the same infrastructure. In fact, they can spin up and spin down new resources on demand in AWS, Azure or Google Cloud Platform (GCP), running both interactive and batch workloads whenever they want.
Network contention is also less of concern, since the public cloud providers typically have plenty of bandwidth. The exception of course is where batch workloads use the same application interfaces or APIs to read/write data.
So, moving to public cloud offers a spectrum of possibilities, and you can use one or any combination of them:
The advantage of this approach is potentially faster time to implement and (maybe) less expensive monthly cloud costs, because the compute services run only at the times you specify. The disadvantages of this approach may be the degree of operational/configuration control you have; the fact, that these services may be totally foreign to your existing DevOps folks/processes (i.e., there is a steep learning curve); and it may tie you to that specific cloud provider.
A Simple Alternative
If you are looking to minimize impact to your DevOps processes (that is, the first two approaches mentioned above), but still save money, then ParkMyCloud can help.
Normally, with the first two options, there are cron jobs scheduled to kick-off batch jobs at the appropriate times throughout the day, but the underlying instances must be running for cron to do its thing. You could use ParkMyCloud to put parking schedules on these resources, such they are turned OFF for most of the day, but are turned ON just-in-time to still allow the cron jobs to execute.
We have been successfully using this approach in our own infrastructure for some time now, to control a batch server used to do database backups. This would, in fact, provide more savings than AWS reserved instances.  Let’s look at specific example in AWS. Suppose you have an m4.large server you use run batch jobs. Assuming Linux pricing in us-east-1, this server costs $0.10 per hour, or about $73 per month. Suppose you have configured cron to start batch jobs at midnight UTC and that they normally complete 1 to 1–½ hours later.  You could purchase a Reserved Instance for that server, where you either pay nothing upfront or all upfront and your savings would be 38%-42%.
Or, you could put a ParkMyCloud schedule where the instance is only ON from 11 pm-1 am UTC, allowing enough time for the cron jobs to start and run. The savings in that case would be 87.6% (including the cost of ParkMyCloud) without the need for a one year commitment. Depending on how many batch servers you run in your environment and their sizes, that could be some hefty savings.
Conclusion
Public cloud will offer you a lot of freedom and some potentially attractive cost savings as you move batch workloads from on premise. You are no longer constrained by having the same infrastructure serve two vastly different types of workloads — interactive and batch. The savings you can achieve by moving to public cloud can vary, depending on the approach you take and the provider/service you use.
The approach you take, depends on the amount of process change you’re willing to absorb in your DevOps processes. If you are willing to throw caution to the wind, the cloud provider PaaS offerings for batch can be quite compelling.   If you wish to take a more cautious approach, then we engineered ParkMyCloud to park servers without the need for scripting, or the need for you to be a DevOps expert. This approach allows you to achieve decent savings, with minimal change to your DevOps batch processes and without the need for Reserved Instances.
Originally published at www.parkmycloud.com on August 23, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@RadwareBlog/have-crypto-miners-infiltrated-your-public-cloud-2ce2ea6678d6?source=search_post---------52,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Jul 17, 2019·2 min read
How do you know if bad actors are siphoning off your power and racking up huge bills for your organization? These silent malware scripts could be infecting your public cloud infrastructure right now, but would you even know it?
The concept of crypto-jacking is fairly simple. A script (or malware) infects a host computer and silently steals CPU power for the purpose of mining crypto-currency.
It first gained popularity in 2017 with ThePirateBay website, which infected its visitors with the malware. Its popularity surged in 2018 with companies like Coinhive, that promised ad-free web browsing in exchange for CPU power. Coinhive, and others like it, lowered the barrier to entry for criminals and was quickly exploited by ‘crypto-gangs’ that leveraged the scripts to infect multitudes of popular websites across the globe and mine for Monero.
Most individuals could not detect the malware unless it over-taxed the device. Common symptoms of crypto-jacking malware include device performance degradation, batteries overheating, device malfunction, and increases in power consumption. During its peak in December, 2017, Symantec claimed to have blocked more than 8 million cryptojacking events across its customer base.
Then, market conditions changed. Many end-point security solutions learned to identify and blacklist crypto-mining malwares running on individual endpoints. Coinhive (and many copycat companies) have been shut down. The price of crypto-currency crashed, which made smaller mining operations unprofitable. So, hackers looking for the larger payoff continued to develop more sophisticated crypto-jacking malwaresand went hunting for the bigger fish.
Read the full article: http://bit.ly/2O97VgC
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
1
1
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
"
https://medium.com/@alibaba-cloud/private-vs-public-cloud-which-one-is-right-for-your-business-68fc5a12d8f4?source=search_post---------53,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 2, 2018·2 min read
As more and more companies migrate their data and operations to a cloud-based infrastructure, experts predict that widespread adoption is just on the horizon. In Southeast Asia alone, the International Data Corporation (IDC) forecasted that 70% of enterprises in the region will be moving towards a hybrid cloud strategy1 . Such an approach addresses the limitations of using solely a private cloud or public cloud structure, and combines the best of both worlds. (See “The growing significance of the Hybrid Cloud” for more about its advantages.)
That said, this doesn’t imply that a purely public or private cloud solution should be disregarded. If you’re on the fence as to whether or not a purely public or private cloud solution is better-suited to your business, here are a few points to consider:
Go for private cloud when…
● Your type of business requires a high and stable performance, allowing low latency and speedy access to its database. ● Security is a top concern for your business, in which large amounts of sensitive information may be possessed, stored or transferred. There could also be regulation standards required to comply.● The cloud infrastructure and system need to be customized for a unique internal structure and needs, especially if the applications are not compatible with existing public cloud systems.
This approach is more commonly adopted among the financial services, government and healthcare sectors.
Go for public cloud when…
● Usage pattern or demand for capacity is relatively unpredictable, for instance, when a new application is first launched. Public cloud solutions provide flexible options to rapidly scale up or down after implementation.● A limited budget is allocated, making it unfeasible to deploy any private or hybrid cloud systems, and hence sharing a cloud infrastructure with other users can be more economical.● A lack of in-house expertise or resources for management and implementation of the cloud infrastructure, while professional vendors are able to provide readily-made, easy-to-use hardware, software and support that suit the business demands.
This approach is more commonly adopted among SMEs and companies in the retailing and manufacturing sectors.
In cloud, there is no one-size-fits-all approach. Even though the majority of companies are looking at hybrid cloud nowadays, and this trend seems to be accelerating, it may just not be right for your business. A purely public or private cloud strategy still provides unique values to many companies regardless of industry or size. The key is to evaluate the options realistically based on actual user demands and usage situations, as well as the company’s IT infrastructure, support, objectives and plan.
1https://www.idc.com/getdoc.jsp?containerId=prAP41538916
Reference:
https://www.alibabacloud.com/blog/Private-vs-Public-Cloud-Which-one-is-right-for-your-business_p71871
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-launches-cloud-box-extending-public-cloud-services-to-local-devices-7e5fbc899949?source=search_post---------54,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 22, 2020·3 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Alibaba Cloud ECS
On September 18, at the Apsara Conference 2020 held in Hangzhou, Alibaba Cloud announced the launch of Cloud Box. Based on the proprietary X-Dragon architecture, Cloud Box integrates the cutting-edge technologies of computing, storage, and networking to provide users with a fully managed cloud service that combines software with hardware for local deployment. For users who need to deploy their business in local data centers, Cloud Box provides the same experience as Alibaba Cloud’s public cloud.
Migration to the cloud has become a trend in various industries, especially with the rapid development of big data, artificial intelligence (AI), Internet of Things (IoT), 5G, and other technologies. Most companies regard digital transformation as a priority strategy. However, due to the requirements of compliance, bandwidth, latency, and cost, some enterprises are deploying some of their business in local data centers, but they also want to obtain the same benefits of low costs, elasticity, and agility as provided by the public cloud.
Cloud Box was created to solve this problem. Based on the X-Dragon architecture, Cloud Box provides users with the same cloud product experience as provided by the public cloud. At the same time, through the Virtual Private Cloud (VPC), Cloud Box facilitates the integration of applications deployed on the premises with other local applications or cloud services. Compared with the on-premises data centers, features of Cloud Box, such as on-demand ordering and pay-as-you-go billing, are provided to help users save the one-time investment of offline user-owned IDCs and avoid the cost of idle capacity caused by low device utilization.
The X-Dragon architecture is a hardware-software integrated computing architecture developed by Alibaba Cloud. This architecture helps users obtain computing capabilities superior to those of traditional physical machines. Based on the “X-Dragon architecture”, Cloud Box provides users with an elastic scaling service in on-premises data centers, high-performance computing capabilities, and excellent I/O performance. Meanwhile, by using Alibaba Cloud’s proprietary network devices, users can connect cloud box to local IT facilities to meet ultra-low latency network demands.
Cloud Box provides all types of elastic computing instances, databases, security, container services, and other products. Cloud Box ensures that instance version upgrades, security vulnerabilities, and patch upgrades are synchronized with the public cloud. Users can immediately have access to the latest products and features available on the public cloud.
Security compliance is one of the important reasons for enterprises to deploy their business in local data centers. Users can deploy Cloud Box in local data centers and store their data locally. The enterprise-level network isolation is achieved through the seamless interconnection between the VPC and the public cloud. Therefore, applications can be accessed over an internal network, which meets the regulatory requirements of customers from special industries.
Based on the capabilities of Alibaba Cloud’s public cloud, Cloud Box provides a fully managed cloud service that integrates software and hardware deployment locally. Users can enjoy the same stable experience and SLA services provided by the public cloud without the need for their own maintenance.
Enterprises have changed their computing requirements from “full scenario coverage” to “ubiquitous location”. Alibaba Cloud offers full-scenario coverage to cover “cloud, edge, and end” computing and has launched a series of new product deployment and operation forms. In the future, Alibaba Cloud will extend the public cloud to a broader space and work with its customers to get ready for the upcoming edge computing and 5G era.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/foundations/flessr-public-cloud-infrastructure-update-cdba06d1572b?source=search_post---------55,"There are currently no responses for this story.
Be the first to respond.
I wrote a brief update for the FleSSR project blog yesterday, covering some work we did last week at our (relatively new) Swindon Data Centre to build the initial infrastructure for the project’s public cloud. I won’t repeat any of that here but would just like to note that the FAS 3140 SAN cluster (Storage Area Network) that we are being loaned by NetApp via Q Associates for the duration of the project, of which we’ll use about 10 Tbytes for FleSRR, will be up and running over the next couple of days meaning that this infrastructure will be substantial enough for some real testing.
As an aside, when Eduserv’s new Swindon Data Centre originally opened all staff we’re encouraged to go over from Bath to have a look round. I didn’t bother because “what’s the point of looking round a shed?” — it wasn’t one of my more popular in-house comments :-)
As it happens, I was quite wrong… the Data Centre is actually quite impressive, not just because of the available space (which is much bigger than I was expecting) but also the quality of the one ‘vault’ that has been built so far and the associated infrastructure. It looks (to my eyes) like a great resource… now we’ve just got to get it used by our primary communities — education, government and health. I’m hopeful that FleSSR represents a small step towards what will eventually become a well-valued community resource.
Originally published at efoundations.typepad.com on November 2, 2010.
A blog by andypowe11
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/@coralineltd/public-cloud-%E0%B8%AD%E0%B8%B1%E0%B8%99%E0%B8%95%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%88%E0%B8%A3%E0%B8%B4%E0%B8%87%E0%B9%80%E0%B8%AB%E0%B8%A3%E0%B8%AD-f85d43214c25?source=search_post---------56,"Sign in
There are currently no responses for this story.
Be the first to respond.
CORALINE CO. LTD
Apr 22, 2018·3 min read
ช่วงนี้ข่าวเรื่องข้อมูลรั่วไหลออกมากันเยอะไม่ว่าจะเป็น Facebook หรือแม้แต่บริษัทสื่อสารยักษ์ใหญ่ของไทยก็มีปัญหาเรื่องนี้เช่นกัน ในฐานะบริษัทเอกชนที่ทำงานเป็นที่ปรึกษาด้าน Big Data, Data Science และ Digital Transformation เราจึงอยากขออธิบายเรื่องเกี่ยวกับการทำงานของ ให้เข้าใจกัน … ดังนี้
Cloud เริ่มเข้ามามีบทบาทมากขึ้นตั้งแต่ John McCarthy นักวิทยาศาสตร์คอมพิวเตอร์ชาวอเมริกา ผู้ซึ่งเป็นคนเสนอแนวคิดเรื่อง Artificial Intelligence (AI) ได้เสนอแนวคิดเรื่องการแชร์สาธารณูปโภค (Public Utility) ในปี 1961 หลังจากนั้นแนวคิดเรื่อง Cloud ก็ได้พัฒนาต่อไปเรื่อยๆโดยเฉพาะในวงการ Software จนกระทั่งในปี 1999 เริ่มมีบริษัท Software ได้ให้กำเนิด Software as a Service (SaaS) ขึ้นมา เพื่อการทำ Customer Relationship Management (CRM) หรือการบริหารลูกค้าสัมพันธ์ขึ้นเป็นครั้งแรกและหลังจากนั้นการพัฒนา Cloud ก็เป็นที่นิยมมากขึ้นเพราะหลายๆองค์กรเริ่มเห็นด้วยกับการบริหารจัดการทรัพยากรด้าน IT ในรูปแบบนี้และเมื่อมีบริษัทอื่นที่มี Core Business เป็นที่เข้มแข็งกว่ามานำเสนอสินค้าและบริการด้าน IT เหล่านี้โดยเฉพาะในราคาที่ประหยัดกว่าการทำเองจึงทำให้ Cloud ได้รับความนิยมมากขึ้นเรื่อยๆ
Cloud Computing เป็นที่นิยมมากขึ้นเมื่อ Amazon ได้นำเสนอ Elastic Compute Cloud ในปี 2006 Google นำเสนอ Google App Engine ในปี 2008 ตามมาด้วย Microsoft นำเสนอ Microsoft Azure ในปี 2010 และ IBM นำเสนอ IBM SmartCloud ในปี 2011 จะเห็นได้ว่าบริษัท IT ยักษ์ใหญ่ต่างเปรียนกลยุทธจากการขาย Software สำเร็จรูปมาเป็นในรูปของ Cloud Computing ทั้งสิ้นและการมาของ Cloud นี้เองที่ทำให้การสร้างข้อมูล เก็บข้อมูล และประมวลผลข้อมูลทำได้สะดวกขึ้น เพราะเทคโนโลยีที่ได้รับการพัฒนาให้ดีขึ้นแล้ว ราคาของ Cloud ในรูปแบบต่างๆ ก็ค่อนข้างถูกเนื่องจากเป็นรูปแบบของการแชร์สาธารณูปโภค หรือ Public Utility นั้นเอง
Cloud Computing ไม่ได้หมายถึงที่เก็บข้อมูล หรือ Data Storage เท่านั้น แต่เรากำลังพูดถึง Computer หรือ Server ที่วางอยู่ที่อื่น อยู่ที่ว่าเราจะเลือกใช้บริการ Cloud ประเภทใด ความน่าตื่นเต้นของ Cloud คือเราสามารถเลือกบริการได้หลากหลายไม่ว่าจะเป็น ไม่ว่าจะเป็น Infrastructure as a service (IaaS), Platform as a service (PaaS) หรือ Software as a service (SaaS) โดยที่การจ่ายเงินจะเป็นรูปแบบ Pay as you go หมายถึงเราอาจจะเลือกใช้แค่ Server ก็ได้ Storage ก็ได้ หรือใช้ Application อื่นๆ รวมไปถึงเป็น Machine Learning ก็ได้ทำให้การจัดการข้อมูล และประมวลผลข้อมูลเกิดขึ้นได้ง่ายขึ้น
ประเภทการใช้บริการของ Cloud แบ่งออกเป็นส่วนต่างๆ ดังภาพด้านล่างนี้
จาก 2 ภาพข้างบน จะเห็นว่า การใช้ Cloud ไม่ใช่แค่เรื่อง Server หรือ Storage แต่ยังมี Application มากมายที่ส่งเสริมสนับสนุนกันอยู่อีกด้วย จึงทำให้ระบบต่างๆ สามารถส่งผ่านถึงกันได้ง่ายขึ้น
รูปแบบการลงทุนใน Cloud แบ่งออกเป็น 2 ประเภทหลักๆ ได้แก่
ข้อดี คือ องค์กรสามารถควบคุมระบบทั้งหมดได้ด้วยตัวเอง
ข้อเสีย คือ ราคาแพง และต้องดูแลรักษาระบบทั้งหมดด้วยตัวเอง
ข้อดี คือราคาถูก จ่ายเท่าที่เลือกใช้งาน และไม่ต้องเสียเงินกับค่าดูแลรักษาระบบ
ข้อเสีย คือไม่สามารถควบคุมหรือแก้ไขระบบ Cloud นี้ได้ตามใจ
อย่างไรก็ตาม องค์กรสามารถผสมผสานรูปแบบของ Cloud เรียกว่าเป็น Hybrid Cloud นั้นเอง
การมี Cloud ทำให้ระบบการจัดเก็บข้อมูลมีราคาถูกลงอย่างมีนัยสำคัญ ทำให้ข้อจำกัดเรื่องปริมาณข้อมูลที่เก็บได้มีน้อยลง โดยเฉพาะอย่างยิ่ง เมื่อเก็บข้อมูลกับ Public Cloud ที่มีระบบการใช้จ่ายแบบ On-Demand จ่ายเท่าที่ใช้ และเมื่อต้องการเพิ่มความจุก็แค่คลิ๊กจ่ายเงินทางอินเตอร์เน็ตในไม่กี่นาที ก็สามารถเพิ่มความจุได้อย่างจุใจ จึงเป็นที่มาว่า ทำให้การเก็บข้อมูลทุกวันนี้ถึงสะดวกขึ้น จึงทำให้การทำงานในเชิง Big Data มีประสิทธิภาพมากขึ้นด้วย แต่ที่น่ากลัวก็คือ การเข้ามาของ Cloud อาจทำให้ …
ตำแหน่งงานของ IT ในอนาคตกำลังเปลี่ยนแปลงไป
คราวนี้มาถึงสิ่งที่หลายๆ คนกลัว นั้นคือ Public Cloud ปลอดภัยจริงหรือ?? โดยเราขอแบ่งออกเป็น 2 ประเด็น
สำหรับทั้งเคสของ Facebook และของบริษัทสื่อสารสัญชาติไทยบริษัทหนึ่ง ทั้ง 2 เคส เกิดขึ้นจากความผิดพลาดของ “ผู้ใช้งาน” ไม่ใช่ของ “ผู้ให้บริการ Cloud” ส่วนเรื่อง Facebook Account โดน Hack นั้น ก็เป็นเรื่องของ Application ของ Facebook ไม่เกี่ยวกับ Cloud
ข้อสรุปของเราคือ ทุกสิ่งทุกอย่าง มีช่องโหว่ ไม่ว่าจะเป็น Physical Data Center เอง ก็ไม่ปลอดภัย 100% ทุกอย่างอยู่ที่เราจะเลือก ตัวอย่างเช่น หากเราต้องการเดินทางจากหาดใหญ่ไปเชียงใหม่ เราอาจจะเลือกเดินทางโดยเครื่องบิน เร็วกว่า ประหยัดกว่า ไม่ต้องจ่ายจ้างนักบินเอง ไม่ต้องบินเอง ไม่ต้องซ่อมบำรุงเครื่องบิน หรือเทรนวิธีการขับเครื่องบิน เพียงแค่ต้องแชร์เครื่องบินกับผู้โดยสารท่านอื่นๆ และหากมีเหตุการณ์อันตรายใดๆ เกิดขึ้น ก็อาจไม่สามารถควบคุมอะไรใดๆได้มากนัก อีกทางเลือกหนึ่งคือเลือกที่จะขับรถไปเอง ช้ากว่า ลงทุนมากกว่าเพราะเราต้องลงทุนซื้อรถ ต้องไปฝึกขับรถ ต้องซ่อมบำรุงรถ ต้องซื้อประกัน จ่ายค่าน้ำมันและอื่นๆ อีกมากมาย แต่เราไม่จำเป็นต้องแชร์ที่นั่งกับใครและเราเป็นคนขับเอง สามารถควบคุมการเดินทางได้เองทั้งหมด
ดังนั้นมันจึงอยู่ที่ว่าแต่ละองค์กรจะเลือกทางไหนและต้องมีนโยบาย “เทรน” พนักงานให้เป็นในทิศทางนั้นๆตามที่ต้องการให้ได้ เพื่อเดินหน้าต่อไปและต้องไม่ลืมว่า เทคโนโลยีเปลี่ยนไปเรื่อยๆ หากเราไม่อยากโดน Disrupt เราก็ควร Disrupt ความล้าหลังของตัวเอง ให้ได้เสียก่อน
ปล.การใช้ Public Cloud ไม่ได้น่ากลัวอย่างที่คิด เพราะหลายๆ องค์กรยักษ์ใหญ่ รวมไปถึงรัฐบาลของประเทศต่างๆ เขาก็ใช้กันจนเป็นเรื่องปกติ เพราะปัญหามันไม่ได้อยู่ที่ ผู้ให้บริการ Cloud แต่มันคือ จะทำอย่างไรให้ใช้งาน Cloud ให้ถูกต้อง และถูกทางเสียมากกว่า ซึ่งหากโจทย์ไหนเป็นข้อมูลที่ละเอียดอ่อนมากๆ จะเลือกใช้เป็น Private Cloud ก็ได้ อยู่ที่การตัดสินใจของแต่ละหน่วยงาน
“Once your mindset changes, everything on the outside will change along with it.”
— — Steve Maraboli — -
— — — — — — — — — — — — — — — — — — — — —
We turn your DATA into your KEY of success.
ให้คำปรึกษาการทำโครงการ Big Data, Data Model, Artificial Intelligence และ Digital Transformation เพื่อเพิ่มศักยภาพของธุรกิจ
ติดต่อ inquiry@coraline.co.th, Tel: 099–425–5398
— — — — — — — — — — — — — — — — — — — — —
#BigData
#DataScience
#Optimization
#ProductivityImprovement
#Coraline
Originally published at https://www.coraline.co.th .
We seek to be the acknowledged leader in Data Science & Operations Research in searching for new solutions and bringing customer’s big data into real action.
See all (11)
We seek to be the acknowledged leader in Data Science & Operations Research in searching for new solutions and bringing customer’s big data into real action.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Anders Christiansen
Oct 15, 2018·6 min read
One of the hardest concepts to grasp when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes. It is also one of the most important parameters you can tune for improved performance on your dataset. In fact, if anchor boxes are not tuned correctly, your neural network will never even know that certain small, large or irregular objects exist and will never have a chance to detect them. Luckily, there are some simple steps you can take to make sure you do not fall into this trap.
When you use a neural network like YOLO or SDD to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object. The multiple predictions are output with the following format:
Prediction 1: (X, Y, Height, Width), Class….Prediction ~80,000: (X, Y, Height, Width), Class
Where the(X, Y, Height, Width) is called the “bounding box”, or box surrounding the objects. This box and the object class are labelled manually by human annotators.
In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:
We need to tell our network if each of its predictions is correct or not in order for it to be able to learn. But what do we tell the neural network it prediction should be? Should the predicted class be:
Prediction 1: PearPrediction 2: Apple
Or should it be:
Prediction 1: ApplePrediction 2: Pear
What if the network predicts:
Prediction 1: ApplePrediction 2: Apple
We need our network’s two predictors to be able to tell whether it is their job to predict the pear or the apple. To do this there are a several tools. Predictors can specialize in certain size objects, objects with a certain aspect ratio (tall vs. wide), or objects in different parts of the image. Most networks use all three criteria. In our example of the pear/apple image, we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image. Then we would have our answer for what the network should be predicting:
Prediction 1: PearPrediction 2: Apple
State of the art object detection systems currently do the following:
1. Create thousands of “anchor boxes” or “prior boxes” for each predictor that represent the ideal location, shape and size of the object it specializes in predicting.
2. For each anchor box, calculate which object’s bounding box has the highest overlap divided by non-overlap. This is called Intersection Over Union or IOU.
3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU.
4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example.
5. If the highest IOU is less than 40%, then the anchor box should predict that there is no object.
This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object appears in an image. Taking a look at an open source implementation of RetinaNet, a state-of-the-art object detector, we can visualize the anchor boxes. There are too many to visualize all at once, however here are just 1% of them:
Using the default anchor box configuration can create predictors that are too specialized and objects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes. In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak our anchor boxes to be much smaller, such as this 1% sample:
In the RetinaNet configuration, the smallest anchor box size is 32x32. This means that many objects smaller than this will go undetected. Here is an example from the WiderFace dataset (Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou) where we match bounding boxes to their respective anchor boxes, but some fall through the cracks:
In this case, only four of the ground truth bounding boxes overlap with any of the anchor boxes. The neural network will never learn to predict the other faces. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the faces line up with at least one of our anchor boxes and our neural network can learn to detect them!
As a general rule, you should ask yourself the following questions about your dataset before diving into training your model:
You can get a rough estimate of these by actually calculating the most extreme sizes and aspect ratios in the dataset. YOLO v3, another object detector, uses K-means to estimate the ideal bounding boxes. Another option is to learn the anchor box configuration.
Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes and then decoding them as though they were predictions from your model. You should be able to recover the ground truth bounding boxes.
Also, remember that if the center of the bounding box and anchor box differ, this will reduce the IOU. Even if you have small anchor boxes, you may miss some ground truth boxes if the stride between anchor boxes is wide. One way to ameliorate this is to lower the IOU threshold from 50% to 40%.
A recent article by David Pacassi Torrico comparing current API implementations of face detection highlights the importance of correctly specifying anchor boxes. You can see that the algorithms do well except for small faces. Below are some pictures where an API failed to detect any faces at all, but many were detected with our new model:
If you enjoy this article, you might like reading about object detection without anchor boxes.
For a more in-depth explanation of anchor boxes you can refer to Andrew Ng’s Deep Learning Specialization or Jeremy Howards’s fast.ai
VP of Data Science @ Kavak
See all (27)
1.6K 
5
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
1.6K claps
1.6K 
5
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/descarteslabs-team/thunder-from-the-cloud-40-000-cores-running-in-concert-on-aws-bf1610679978?source=search_post---------58,"There are currently no responses for this story.
Be the first to respond.
Back in 1998, one year after winning his second Gordon Bell prize in high-performance computing, Mike Warren and his colleagues at Los Alamos National Lab assembled the first Linux cluster to appear on the TOP500 list of the world’s fastest supercomputers. Now, leading our tech team 21 years later at Descartes Labs, he’s completed another trend-setting accomplishment by setting a new performance benchmark using virtualized resources in the public cloud, obtaining more petaflops than the fastest computer in the world could in 2010.
Mike’s use of Linux in 1998 was an outlier as it rejected the fragmented and proprietary operating systems that powered each of the other 499 machines on the list. He had grown tired of porting code between IBM, Sun, and a half-dozen flavors of UNIX, so the decoupling of hardware and software that Linux offered was positively embraced. Among other benefits like scalability, interoperability and a worldwide community, Linux allowed Mike and other HPC innovators to change the game by assembling commodity PCs into custom supercomputers of their own.
Today, what was once just a single instance, is now a de facto standard, as every single one of the TOP500 computers on the list run Linux. The adoption of the Linux operating system represented a huge leap in simplifying the design and deployment of HPC applications. It used to be that practitioners would buy a special IBM or Cray system, then it became easy to purchase mail-order PC’s and install Linux, now Amazon and other cloud providers have basically taken the hardware part out of the equation. This latest advancement severely disrupted HPC vendors starting around 2002 as usage split between “tightly-coupled” applications running on dedicated machines and “loosely-coupled” applications running in the public cloud, even though both were using essentially the same hardware underneath.
There are many well-known examples of loosely-coupled applications running successfully on the public cloud with tens or even hundreds of thousands of cores. Applications abound across drug discovery, materials science, particle physics, and the cleaning and calibration of petabytes of satellite imagery that we run here at Descartes Labs. These “massively parallel” applications are amazing in their own right but lack the “interconnect”, or core-to-core low latency network communication that’s needed to power big physics simulations like seismic processing, thermodynamics, cosmology, weather forecasting and more. These highly interconnected applications were previously thought of as only suitable for massive on-premise systems like the Summit supercomputer at Oakridge National Laboratory, or China’s Sunway TaihuLight, the latter being used to simulate the birth of the universe with a technique known as “N-body simulation.”
The two supercomputing paths may be starting to merge back together again as our team recently demonstrated on Easter weekend in April 2019. Using publicly available spot resources on AWS, we attained performance of 1.926 petaflops running the standard of HPC achievement, a giant matrix inversion called the Linpack Benchmark. Our engineering team’s goal that day was to use the Linpack Benchmark to see how far the cloud would scale. If it was capable of placing #136 on the TOP500 list then it would be capable of running global-scale customer models for Descartes Labs, including the simulations we developed for Cargill and DARPA.
One of the more interesting aspects of the story is that we didn’t ask Amazon to give our engineers any special dispensation, discount, or custom planning or setup. We wanted to see if we could do this on our own, which if completed successfully, would also be a testament to the self-service model of AWS. Our team merely followed the standard steps to request a “placement group”, or high-network throughput instance block, which is sort of like reserving a mini-Oakridge inside the AWS infrastructure. We were granted access to a group of nodes in the AWS US-East 1 region for approximately $5,000 charged to the company credit card. The potential for democratization of HPC was palpable since the cost to run custom hardware at that speed is probably closer to $20 to $30 million. Not to mention a 6–12 month wait time.
Mike believes this is the first time “virtualized” processors have been used on the TOP500 list, although AWS has been on before with a four times smaller 0.484 petaflop setup that was widely believed to have been run on bare hardware. Our system leveraged unique technical details like a fine-tuned hypervisor between the Descartes Labs code and the virtualized AWS Intel Skylake processors, as well as advanced use of MPI messaging, and Packer, a tool for creating identical machine images that manage the software configuration on each node. All of these taken together may mean the Descartes Labs system is deserving of its own software category on the TOP500 list.
Mike Warren’s vision today is a continuation of the story that started with Linux in the early days. He’s aware of the common refrain that “everyone knows the cloud is useless for tightly coupled HPC.” But he believes that just isn’t true. The cloud can definitely be used. It’s not magic, just a mix of experience, skill, and a mind for innovation. For some reason, others just haven’t really tried it yet. It’s kind of like how no one knew you could use mail order PC’s to roll your own supercomputer, or no one knew you could use Linux instead of dealing with the latest flavor of UNIX shipped on an IBM or Cray.
Back at our headquarters in Santa Fe, our team is constantly tuning the ideal architecture needed to serve our global-scale earth science projects. These include large-scale weather simulations, giant 3D Fourier transforms in seismic modeling, and greenhouse gas mixing dynamics in the atmosphere. We believe that true HPC applications will eventually migrate over to the cloud en masse. The advantages versus traditional supercomputers are hard to ignore. HPC professionals can buy their own machine at a massive cost or rent time on a highly specialized cluster somewhere that uses an old version of Linux from two years ago that needs to be brought up-to-date. But in the cloud, it’s all under your control. There can be seven different versions running different Linux kernels tuned for specific applications and it’s all easy to manage.
In summary, Big machines have historically been used for a very specific purpose, but the cloud is generalizable across purposes. The democratization of HPC is bringing the price point down to something that’s available to business, and we are well positioned to help our customers take advantage of it. We’ve built a Data Refinery that processes location-related data across the world and makes predictions about future states. These predictions are made possible only by scaling up a supercomputer in the cloud that can handle petabyte or exabyte datasets. This scaling enables our customer models to become truly global in their size and scope.
If you have a complex systems problem that you previously thought was too expensive or complicated to model, consider contacting us to learn more. Until then, keep an eye out for our tightly-coupled supercomputer in the cloud on the latest TOP500 list.
Explore posts from the Descartes Labs team
812 
4
812 claps
812 
4
Explore posts from the Descartes Labs team
Written by
Descartes Labs is building a data refinery for geospatial data.
Explore posts from the Descartes Labs team
"
https://betterprogramming.pub/architecting-for-the-cloud-ad3c4505e053?source=search_post---------59,"Sign in
Bob Roebling
Feb 7, 2020·16 min read
The image above might be rocket science, but changing how you develop programs for the public cloud isn’t!
There are many ways to successfully write code that works in Microsoft Azure, Amazon Web Services, Google Cloud Platform, or IBM Cloud. I’m not here to promote one platform over the other —so…
"
https://blog.palantir.com/palantir-apollo-powering-saas-where-no-saas-has-gone-before-7be3e565c379?source=search_post---------60,"At Palantir, our approach to software has undergone a radical transformation in the past few years. When we first launched Gotham in 2008, cloud computing was in its infancy, and most enterprise software was still installed on customer-owned hardware. This was especially true in the large governmental organizations that used Gotham, where our software ran on-premises and upgrades were relatively infrequent and manual.
Government and Enterprise use Foundry on public cloud for mission critical operations.
When we built Foundry in 2016, the software world looked a lot different, so instead of replicating what we did with Gotham, we took a new approach: we built Foundry as a cloud-native SaaS with a microservice architecture. Our customers embraced the SaaS model much faster than we expected, and it has since come to dominate our business. In the past two years, every new Commercial customer has opted for our SaaS platform. And the same can be said for the US Government, where since receiving FedRAMP Moderate and DoD IL-5 authorization — the highest SaaS authorization, used for Mission-Critical and National Security Systems — our public-cloud SaaS has been used by nearly all new customers for their unclassified work.
However, what makes our two user-facing SaaS platforms — Gotham and Foundry — truly unique is what is under the hood. Both of these platforms stand on the shoulders of a continuous delivery system we call Palantir Apollo. Apollo has been so critical to our success, we consider it to be our third platform. It not only unlocked our transformation to SaaS platforms, but has allowed us to go beyond just SaaS. With Apollo, our SaaS platforms run seamlessly in places where no SaaS has gone before.
Most enterprise SaaS companies only run in the public cloud, and often only use a single cloud provider. But this wasn’t enough for our customers. Some of our government customers require their software to run in purpose-built government or classified clouds that live separately from standard public cloud infrastructure. Others need to use our software in extreme disconnected environments, from submarines to drones. We asked ourselves, how do we build a SaaS that works for all of our customers? That’s where Apollo comes in.
In the past, we’ve written about the value of Apollo for our developers. In this post, we’ll discuss the value of Apollo for our customers.
Palantir Apollo is a continuous delivery system that powers our software platforms. Apollo was conceived alongside Foundry, initially built as the automation and delivery infrastructure for our public-cloud SaaS. However, given our roots in classified and on-prem environments, we knew that a traditional SaaS based on a single public cloud provider wouldn’t work everywhere. We needed unified tooling to bring the same SaaS platform to all of our customers, regardless of environment constraints. So we built Apollo to run as its own standalone platform — independent, decoupled from Foundry, running as a layer that sits between our applications and the underlying infrastructure.
Today, Apollo brings the same SaaS-style management to all of our platforms. It runs nearly everywhere that our software is deployed — cloud, on-prem, and classified networks — and has transformed the way that our software scales. Apollo orchestrates updates across heterogeneous deployment targets, unlocking fleet-wide autonomous management that has been a step-change in the efficiency of our operations.
Apollo manages and safely deploys our Gotham and Foundry platforms. Each platform is made up of hundreds of individual services, each owned by a development team that writes and releases product features independently. This approach allows us to roll out updates across services asynchronously, increasing feature velocity. However, this type of microservice architecture traditionally makes the deployment of the software more complex. Apollo is the brain that automates this complexity away. It is configured to decide what to upgrade, when to do it, and how. It monitors developer releases, resolves dependencies, and deploys new versions.
Apollo does this automatically in a way that obviates the need for user downtime or human intervention. It performs safe deploys with staged blue-green upgrades, and observes the roll-out process. If it notices an emergent issue, it begins a roll-back process, notifies the relevant development team, and avoids an outage. If successful, it moves on to the next service, and repeats — continuously delivering incremental, automatic updates for the fleet over 41,000 times a week.
Apollo performs this same automation for not just our SaaS, but also connected on-premises appliances, removing operating environment as a constraint. In fact, that’s one of the main reasons we built it. From Day 1, we’ve worked with customers in the most secure and highly regulated industries, operated in the world’s most logistically challenging environments, and built software capable of handling our customer’s most sensitive data. The experience of solving for the most technically challenging environment first, rather than adding it on later, is what led us to build Apollo as a foundation for software that can be written once and run anywhere.
Apollo has allowed us to extend our platforms across cloud regions to meet our customers’ data residency requirements, and across cloud providers to run a multi-cloud SaaS. It has allowed us to extend our platforms on-premises to connected hardware appliances and edge devices. And it has done so without adding significant overhead or requiring specialized workflows: our software developers write code, Apollo deploys it, and our centralized operations team can monitor the whole fleet from a single pane of glass. In other words, Apollo has enabled Palantir to bring our SaaS — and the SaaS economics that come with it — to environments where no SaaS has gone before.
At Palantir, we don’t take any software deployment scenario for granted. Our software engineers don’t know if their products will be deployed as part of our standard SaaS platform, serving web-scale traffic from the public cloud, or in Afghanistan, running on a ruggedized server rack in the back of a Humvee. Because Apollo removes the environment as a constraint, engineers can focus on velocity and application code. They write code once that works for all customers. This is in stark contrast to other software companies, where engineers write code that targets a specific environment and must engineer specialized products for each new deployment scenario they face. Ultimately, this approach brings more capabilities to our customers, faster.
This technology brings our SaaS and all its benefits to organizations who have never used SaaS before. Not only can we fully embrace modern development practices like devops, continuous delivery, containerization, auto-scaling, and automated upgrades — the core components of a SaaS solution — but we do so without traditional constraints of only supporting a single public cloud provider. We run our software in a homogenous way across heterogeneous environments, powered by Apollo. Apollo is the platform that makes it possible to bring a unified SaaS to our entire customer base.
Akshay Krishnaswamy leads Enterprise Architecture for Palantir’s Commercial business.Clark Minor leads Palantir’s platform infrastructure and cloud strategy.Rob Fink is the founding engineer of the Palantir Foundry data platform.
Palantir Blog
404 
2
404 claps
404 
2
Written by

Palantir Blog
Written by

Palantir Blog
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/micro/m3o-an-open-source-aws-alternative-61e3699b3d2a?source=search_post---------61,"There are currently no responses for this story.
Be the first to respond.
M3O is an open source public cloud platform.We are building an AWS alternative for the next generation of developers.
AWS was a first generation public cloud provider started in 2006. It’s infrastructure services and pay as go pricing model made it an incredibly compelling choice for a previous generation of developers. But what about the future?
M3O is an attempt to build a new public cloud platform with higher level building blocks for the next generation of developers. M3O is powered by the open source Micro platform and programmable real world Micro Services.
AWS is a fairly complex beast which makes it hard for new developers to get started. In the past we needed VMs and file storage, but today with the Jamstack and other modern development models, the building blocks we’re looking for are changing. They’re mostly now third party APIs. M3O is looking to aggregate all those third party public APIs into a single uniform offering with a slick new dev UX.
So far there are over 35+ services. Here are some of the highlights:
See the full list at m3o.com/explore or the source at github.com/micro/services.
M3O is built on existing public cloud infrastructure using managed kubernetes along with our own infrastructure automation and abstraction layer for existing third party public APIs. We host the open source Micro project as our base OS and use it to power all the Micro Services, which provide simpler building blocks for existing cloud primitives.
We host our own custom dev UX (m3o/cloud) on top of the infrastructure stack and a backend which acts as the management control plane.
Developers build and contribute to services in github.com/micro/services, a vendor neutral home. We then automate the building and publishing of those services and client libraries. This creates a shared and fully managed platform for everyone to leverage.
We primarily use existing open source software, fully managed services and third party public APIs as the backing infrastructure then layer a standard interface on top. With all the services on one platform, accessible with one API token, we drastically improve the Dev UX.
This project is a combination of open source projects and a platform managed by the Micro team. Our goal is to enable any developer to contribute to the open source while benefiting from the platform as a shared resource.
The cloud hosted providers of Micro services:
The core cloud OS and services exists in a vendor neutral org
The hosting of Micro services on m3o.com is powered by the following:
If you’d like to publish your own APIs on the M3O platform fill in this form and we’ll get back to you.
Hopefully you buy into what we’re talking about and the need for something new in the public cloud space. If you like what you’re hearing, Signup for Free or send us some feedback. Reach out on slack or twitter if you have any questions.
Programming the real world
161 
3
161 claps
161 
3
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
Written by
Working on Micro
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
"
https://medium.com/@eyevinntechnology/using-ffmpeg-and-srt-to-transport-video-signal-to-the-cloud-7160960f846a?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eyevinn Technology
Mar 28, 2019·6 min read
Moving the live video transcoding for online distribution to public cloud requires a way to transport the video signal to the cloud. There a couple of ways this can be achieved and in this article we will mainly look at how to use SRT (Secure Reliable Transport) and ffmpeg to accomplish this.
Before we look into how to use SRT let us look at what other options exists. The simplest option available today is to use RTMP as the transport protocol. It is a protocol developed by Adobe and has been around for a while. Many production tools such as Wirecast, Teradek and OBS have support for RTMP today and this is the protocol that YouTube, Facebook and Twitch supports.
RTMP is a TCP-based protocol where every packet is acknowledged and guaranteed to be delivered meaning that the underlying protocol ensures that all bits in the bitstream are received by the receiving end. This handshake process for every packet adds overhead and emposes some limitations.
Another drawback with RTMP is that not all Live Transcoding software supports RTMP out-of-the-box. This limits some of the options you have on choosing the right transcoding software. You would in many cases need a “bridge” from RTMP to MPEG-TS over UDP/Multicast. This can also be accomplished using ffmpeg and we will describe how in a separate article.
And equally it is not always you on the production side (sender) have the option to use RTMP as the transportation protocol. For those cases there are a couple of commercial options available, such as Zixi, to transfer a MPEG-TS stream over the Internet.
An option to the commercial and proprietary protocols is to use SRT (Secure Reliable Transport) to transfer the signal and how this can be done with ffmpeg is what we will describe here.
SRT is an open source video transport protocol and technology stack backed by the SRT Alliance. Today the SRT Alliance consists of over 40 vendors including Haivision, Wowza, Teradek, and many more. The standard is open and available to anyone who wants to implement it and SRT is also open source which means that the software (technology stack) required to implement SRT is available from a public repository and there are no fees to incorporate the technology into any device or service.
Havision, one of the founders of SRT, provides an open source SDK to enable SRT support in your software. This is the SDK that we will compile and include in our ffmpeg build.
The SDK is depending on that you have libssl and tcl installed, e.g. on Ubuntu:
When compiling ffmpeg enable SRT by adding the option below when running the configure script.
To setup an SRT Transmitter that takes an MPEG-TS video stream and transmit it to an SRT Receiver over the Internet you start ffmpeg with the following command.
This setup the SRT Transmitter as a server that the SRT Receiver can connect to. If you instead have an SRT Receiver acting as a server you run the following instead (assuming that 192.168.1.42 is the IP of the SRT Receiver).
If you want to quickly try this out you can use the SRTTx Docker container in our toolbox.
To setup an SRT Receiver with ffmpeg that outputs the incoming stream over a multicast network you start ffmpeg with this command.
In thise case the SRT Receiver acts as a server which the SRT Transmitter will connect to. If you instead wants to act as a client and assuming the IP of the SRT Transmitter is 192.168.1.19 you run the following command.
To quickly try this out you can use the SRTRx Docker container from our toolbox and use the loopts Docker container to generate a test stream to transmit.
To emulate the encoder we instead use VLC to verify that the stream is transported. On the sender side.
And on the receiver side.
Then open VLC with the following address to join the multicast stream:
The SRT Transmitter in the toolbox can also take an RTMP stream and transport using SRT to an SRT Receiver. Spin up the SRTTx Docker container on the computer running for example OBS or Wirecast and you can use SRT instead of RTMP to transfer the signal to the cloud.l
Start the SRTTx Docker container and given that the SRT Receiver is available on IP 192.168.1.6 use the following arguments.
And on the receiver side run the following
And again, to try this out we can use VLC to view the MPEG-TS stream that is received.
In this article I have shown how you with ffmpeg can transport an MPEG-TS stream over the Internet using SRT as the protocol. I personally believe that we in a not so distant future will see more support for SRT on both the Sender and Receiver side as the SDK is available as open source and can easily (without a fee) be incorporated in the software. I would assume that Wirecast or OBS will soon have support for SRT given the number of companies now joining the SRT Alliance.
If you have any further questions and comments on this blog drop a comment below or tweet me on Twitter (@JonasBirme).
Eyevinn Technology is the leading independent consultant firm specializing in video technology and media distribution, and proud organizer of the yearly nordic conference Streaming Tech Sweden.
We are consultants sharing the passion for the technology for a media consumer of the future.
286 
1
286 claps
286 
1
We are consultants sharing the passion for the technology for a media consumer of the future.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/micro/why-developers-need-an-aws-alternative-7a0123cebd8?source=search_post---------63,"There are currently no responses for this story.
Be the first to respond.
Author: Asim Aslam, founder of Micro Services, Inc. (Micro). Micro is building M3O, an open source public cloud platform. An AWS alternative for the next generation of developers. Consume popular public APIs as higher level building blocks, all on one platform, for a 10x developer experience.
AWS was launched over 15 years ago, imagined as an operating system for the internet at a time before the cloud even existed as a concept. It was built to provide on-demand access to compute, storage and infrastructure services. What previously might have taken 6 weeks to provision was now being done in minutes. It largely unlocked productivity in a way that had not yet been imagined and enabled developers to quickly scale web services as users were coming online.
Yet for all it’s worth, AWS has largely maintained this experience and failed to keep up with modern day needs of developers. In 2006, we expected a level of server and database management as developers. What we classified as sysadmins (not yet devops) was a skill we’d gladly learn if it meant being able to ship the next shiny Rails app.
Today we’re looking for more. We’re looking for not just fully managed (which AWS attempts to convince us they are), but an entirely serverless experience. We don’t want to have to deploy that next database or spin up containers. We don’t want to deal with the issues that arise when dealing with the complexities of this new fangled Kubernetes. As developers, all we want to focus on is building that next product and leveraging the APIs that let us do that.
In 2021, AWS is slowly being eaten by third party API providers like Algolia, Elastic, CockroachDB, Twilio, Stripe, Sendgrid, Segment and so many many more. We’re looking for entirely API first experiences in the cloud that don’t require us to think about the infrastructure. We’re looking for platforms that compliment our modern day Jamstack architectures powered by the Netlify’s and Vercel’s of the world.
AWS now leaves a lot to be desired for the next generation of devs. Can they do anything to fix that?
We personally don’t believe so.
Then if AWS isn’t built for developers, who’s it for? AWS was never built for developers in the first place, let’s be clear about that. AWS was about provisioning infrastructure services on which we could then run our software which was still automated by the sysadmins in our companies. How do we know that? Because we were heavily invested in AWS in our prior companies.
We contended with the complexity of the bare metal era before AWS and then what came after managed largely by a disarray of hand crafted bash scripts, python libraries and eventually configuration management tools like chef and puppet. We escaped just as the DevOps movement took off but continued to witness the extraordinary pains of building systems for the cloud as a software engineer.
Yet in all that time, we never once saw developers personally touch CloudFormation, or swim the sea of endless complexity unless they truly had to. No, those developers would gladly choose a Heroku long before an AWS, but if you worked at a startup that was scaling, at some point in the lifetime of the company you could expect an infrastructure engineer to join and quickly replatform you to AWS.
The truth of the matter is. AWS was built for operators, not the developers.
There’s enough people now shouting at the screen pointing to services like AWS Lambda or Fargate talking about it’s serverless nature or how it was built for developers but I’d argue, this is just AWS pandering to an existing Enterprise audience and checking off boxes. AWS is about building the 80% solution to keep existing customers happy, that doesn’t mean the actual users in those companies are happy using them.
AWS was the “just good enough” solution for a time in which we had nothing else. The book store that started a cloud computing business even admits to being shocked that they had a 7 year head start on everyone else. Had Google gotten their act together and shipped their superior internal tools as public services long ago, we’d be having an entirely different conversation.
The fact of the matter is, AWS doesn’t understand developers and the harder they try the more complex their offering becomes. As a developer AWS is an overwhelming and anxiety inducing experience.
What we need is a clean slate approach. As developers we need a new experience for cloud services. We need a new public cloud platform. One that focuses entirely on the developer experience. Higher level building blocks for existing public APIs.
Replicating an AWS isn’t the answer. VMs (EC2) and file storage (S3) are not the primitives developers need today. We need to start with next level building blocks for the next generation of devs. Today we are all about the Jamstack and leveraging third party APIs as the backend.
We need a public API platform that aggregates the existing market and provides a new clean abstraction layer on top, all through a single unified offering. One that simplifies the pricing model rather than requiring a pricing calculator to know what you’re spending.
For what it’s worth, we thank AWS for getting us to this point, but now it’s time to hand the torch to someone else. Someone who understands what developers need and provide the next level building blocks for new types of software we’ll come to use.
The world is no longer talking about building mobile apps or web services but instead, crypto networks and the metaverse. Your grandparents can barely use a mobile phone, are we really expecting AWS and others to help us build the metaverse?
It’s up to developers to build the future and with it decide the kinds of platforms we want to build on. We’re now more than ever interested in open platforms. Not just in the case of Web3 but more so in regards to “open source eating everything”. It’s not enough that just the services you run are open source, the entire system also needs to be so.
AWS built in an era before GitHub, and the explosive nature of open source, is not. AWS is a silo, and a ship filled with containers of teams all building APIs in isolation. Their control plane is not open source, their platform is not open source, their system is not open source. AWS is not open source.
We are a generation of developers who are looking for a new platform, one that aligns with our goals, beliefs and mantras and one that is entirely based on open source software.
I’m Asim Aslam, the founder of Micro, and we’re building M3O, a new open source public cloud platform, an AWS alternative for the next generation of developers. Come join me in deciding how, where and what we’re going to build the future on.
See the source on GitHub.
Programming the real world
210 
12
210 claps
210 
12
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
Written by
Working on Micro
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
"
https://digitalculturist.com/the-geopolitics-behind-the-cloud-data-centers-b1c424d874b6?source=search_post---------64,"A year ago I got interested in the reasons behind the choice of locations of the public cloud data centers, and mainly those outside the US. Microsoft, Amazon, Google (and IBM up to a point) have built, or acquired, data centers throughout the world. I wanted to know why they chose these locations, but I dropped the issue.
Only after the switch in the US presidency did my interest in the subject rekindle. As it happened most of the CEO’s of these mega-companies publicly shared their view on politics and globalization, as the new executive branch started sharing their executive orders and worldview. To understand how it connects to the data centers a very short background is needed.
What happened in the US represents only one example, but a major one, of a return of nation-states to a more aggressive nationalism and patriotism. All around the world we can see nations and various ethnic groups trying to regain, or reconstitute, their independence. The latest examples were Brexit and the Scottish referendum. Today we have 200 countries (nation-states) in the world, but there are more than 5,000 different ethnic groups. If a large number of ethnic groups fought for their independence, can the destabilized world and its operating system (the UN) survive the upheaval?! The answer is most probably not. These numbers may present a risk to countries where they have a large ethnic minority which can be perceived as a potential threat. In most cases the largest or strongest ethnic group established their modern country.
Halford Mackinder is the “father” of Geopolitics and the creator of a very famous map — “The Mackinder System” and his architecture of power. To put it very simply — according to Mackinder (and the second half of the 20th century’s history) the world is doing everything it can to contain the core, or the pivot area, which is in fact Russia (Cold War, Georgia 2008, Ukraine 2014, Syria 2016, Eurasian Union). This was achieved, among other ways, through military and economic alliances. Trump and his unknown relations with Putin, and apparently favoring Russia, may undermine this world order as many fear.
Although these two issues (the rise of nationalism, and the containment of the Pivot Area) seem disconnected, they are in fact the two avoidance factors that are apparent from the choice of the locations:
1. Avoiding the “Pivot Area” — The one major aspect that is visible from the locations of the cloud data centers in that none of them reside in the Pivot Area, or more specifically in Russia. All of them reside in the inter-crescent and outer-crescent, creating a “cloud-containment” region. None of the host countries share even a border with Russia (Except Google in Finland, and in China but the data centers are the furthest possible from the border)
2. Avoiding an ethnic mix — There is a very clear trend when analyzing the demographics of these cloud-states, and more specifically the ethnic mix of them. The countries in which the three giants (Amazon, Microsoft and Google) choose to build their data centers are countries in which there is a very clear majority for one ethnic group. Google has the only exception with their data center in Belgium, where the country is split approx. 50/50. IBM received their data centers through an acquisition (SoftLayer), so the locations were not their choice.
Presumably financial considerations are very important for these companies and the leading factor as they choose and negotiate their next location. But, as they invest many billions of dollars in each data center the stability of the host country may be of higher importance. This stability (or vulnerability) of a country is not only measured through its demographics, but includes a number of additional factors that need to be considered i.e. — the political system and its structure, the natural and environmental conditions, scale of development, and good infrastructure. The vast majority of the host countries answer most of these parameters with flying colors.
Another title that companies like Microsoft, Amazon, and Google go by is “multi-national”. Nation states need to operate within a system, which today is globalization. These multi-nationals are the operators of globalization and are de-facto “states”, but without elections and without most state responsibilities.
It seems that these multi-nationals choose “uni-national” states in which they can operate their data centers securely, so that the globalization and liberal values promoted through them can be accelerated, but without the risk of damaging their amenable host.
With the new U.S. administration apparently reversing course and thereby threatening many factors which allowed these multinationals to survive & thrive, will the CEOs manage to combine concern for refugee and expatriates rights, while at the same time securing their business interests, status, and future recruitment?
Observing the digital age through the eyes of those who…
48 
48 claps
48 
Written by
Futurist @ Amdocs. Technology & innovation evangelist. Emergency management expert. Risk & crisis comm researcher and lecturer.
Observing the digital age through the eyes of those who created it.
Written by
Futurist @ Amdocs. Technology & innovation evangelist. Emergency management expert. Risk & crisis comm researcher and lecturer.
Observing the digital age through the eyes of those who created it.
"
https://medium.com/@benbob/bring-out-your-dead-f71f67b5c960?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Fathi
May 18, 2018·8 min read
Enterprise companies represent a huge and lucrative market for the computing industry as they need to run many business critical applications and are willing to pay large sums of money to do so reliably and at scale. For the past few decades, vendors have been selling enterprise companies hardware and software solutions for the implementation of…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/platformer-blog/on-prem-solutions-load-balancing-with-haproxy-4e06a118653a?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
Recently, in a project that I’m involved in, there was a requirement to load balance their workloads. Take public cloud, Google, AWS, Azure etc, setting up a load balancer is fairly easy. It’s just a matter of couple of clicks in the console. This is not the case when you are going on-prem. You should be able to configure a load balancer on your own. In this tutorial, I’m going to guide you through on configuring HAProxy to get you going with load balancing.
First, let’s set up a couple of Virtual Machines with Apache web servers. I’m going to assume the base image would be Ubuntu 18.04 LTS for the rest of this guide.
In both VMs let’s quickly set up Apache by running the following commands.
Now let us change the default Apache welcome page to reflect the name of the server.In VM 1
In VM 2
Okay. Now we set up another VM with Ubuntu being the base image. Do a curl on the internal IP addresses of VM1 and VM2 and check whether you get the hello world response we set up on the previous step.
Now we install HAProxy in our bastion host. To do that, enter the following in command line.
This should get installed in a couple of minutes.
Now, you have to edit the haproxy config which is situated in /etc/haproxy/haproxy.cfg
This will display a config file as following.
At the bottom of the file, let’s insert our load balancing configurations.
The above snippet tells the haproxy to bind all the incoming requests attached to port 80 on any ip range and forward them to a backend service called web servers, which we will be defining below.
Here you should replace the text in bold with the relevant ip addresses of your web servers. Note here that the load balancing option we have given is round robin. Round robin selects servers in turns. This is the default algorithm. You can learn more about other options from here.
Now let’s save the config file and verify whether configs are correct by running the following command.
Alright! If it passes the verification, You can go ahead and apply the configuration by restarting the haproxy service.
Now if you curl the ip address of your bastion host from your local machine, you should be able to see the response alternating from VM 1 and VM 2.
There you have it! This was a pretty basic tutorial and let me know if you have any queries on this.
HAProxy isn’t the only kid in town. If you feel like HAProxy might be too complex for your needs, the following solutions may be a better fit:
There is a weird issue with centOS SELinux where haproxy has an issue with the forwarding port. To fix this run the following commands.
Clap for support. Cheers!
Platformer.com
119 
119 claps
119 
Platformer.com Blog | Technical stories written by our team, partners and invited authors on Cloud, Containers, Kubernetes, Serverless, etc
Written by
Software Architect
Platformer.com Blog | Technical stories written by our team, partners and invited authors on Cloud, Containers, Kubernetes, Serverless, etc
"
https://medium.com/@davidmytton/3-hybrid-cloud-myths-98b7be239070?source=search_post---------67,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Oct 20, 2015·7 min read
The end game for the big public cloud providers (AWS, Azure, Google) is 100% pure public cloud. Their goal is to completely take over all on-premise software and infrastructure deployments.
So why do we continue to hear “the future is hybrid cloud”, combining the best of on-premise with the best of public cloud? Let’s examine some of the arguments.
"
https://medium.com/@Bdeeter/introducing-the-bvp-nasdaq-emerging-cloud-index-514c977e2198?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Byron Deeter
Oct 2, 2018·5 min read
Introducing the BVP Nasdaq Emerging Cloud Index
Five years ago, we launched the BVP Cloud Index, which became the benchmark for the rapidly expanding universe of public cloud computing companies. Today, in partnership with Nasdaq, we’re excited to unveil the next generation of our cloud index: the BVP Nasdaq Emerging Cloud Index, an industry index designed to track the performance of emerging public companies primarily involved in providing cloud software to their customers. The index, listed as EMCLOUD, brings together the complementary expertise of Nasdaq, which has a long-established history of creating indexes, and Bessemer Venture Partners, which has been at the forefront of investing in cloud computing companies over the past two decades.
Because we’ve long believed in the power of the cloud, in addition to launching the BVP Cloud Index we have also worked to plant our flag in the ground (or rather, the cloud) in other ways: publishing the 10 Laws of Cloud Computing and our annual State of the Cloud Industry report; co-producing The Cloud 100 to celebrate the leading private cloud companies; and partnering with 150 cloud companies that have gone on to produce 11 IPOs, dozens of sizable M&A exits, and many hyper-growth private cloud companies.
The cloud industry’s growth potential is tremendous. In fact, when the BVP Index launched in 2013 it’s market cap was under $100Bn. Today the market cap is close to $700Bn, and there are five times as many public cloud companies as there were 10 years ago. Now with the BVP Nasdaq Emerging Cloud Index, cloud has matured to the point that it is primed to have a stock market index to call its own!
How We Got Here
While cloud computing has become the most important trend in the software industry, a decade ago the dominance of the cloud was far from obvious. Eventually, the acknowledgement of certain product and business model advantages gradually cemented cloud as being the status quo for computing.
On the product side, deploying via cloud allowed companies to deploy software updates quickly, efficiently, and at scale; to more easily expand to foreign markets; and to automate service delivery such that customer acquisition could happen 24/7. And on the business side, companies took the benefits of predictable, annuity-like revenue, lower R&D costs without the need to support multiple stacks, and higher customer stickiness given embedment into workflow.
Customers, on the other hand, were happy to move their costs from CapEx to OpEx, to enjoy a lower barrier to trying services without contract lock-in, and to pick solutions feature by feature. There are drawbacks to cloud, too, including high working capital requirements that require a lot of up-front cash, but over time investors understood the beneficial cash flow payoff implications of steepening a cloud company’s growth curve up-front. Today, the question is not, “Why should you be a cloud company?” but, “Will your growth continue?”.
And, as we look back, it’s important to remember that the growth of cloud is far from over. In the next few years we will see new cloud trends materialize from the continued rise of the developer to the propagation of verticalization, software-defined infrastructure models, machine learning enablement, and payments-as-a-service. As companies leveraging these trends continue to grow, including those on the Forbes Cloud 100, we will see more waves of cloud IPOs. Cloud computing continues to be a dominant force worth market time and attention.
A New Index to Track the Growth of the Cloud Industry
While past performance is not indicative of future results, the cloud computing industry is well positioned to continue on its growth trajectory. We hope the BVP Nasdaq Emerging Cloud Index will offer industry participants a benchmark for the performance of the emerging public cloud computing market.
We leveraged our deep expertise in Cloud to develop an objective definition that categorizes companies in the industry and we partnered with Nasdaq, given their long-established history of creating indexes and ability to deliver the most accurate real-time data. There are a number of companies in the BVP Nasdaq Emerging Cloud Index that were not part of the BVP Cloud Index due to the new index’s refined eligibility requirements. For example, Adobe is included in the initial index given that it has masterfully transitioned to a cloud company, and 2018 cloud IPOs DocuSign, Dropbox, Zuora, and Zscaler have been included as well. Additionally, given the partnership with Nasdaq, we believe the new benchmark will be even more reflective of the sector’s performance as the index will be calculated on an equal-weighted basis and employ Nasdaq’s index maintenance and rebalancing standards.
The BVP Nasdaq Emerging Cloud Index (EMCLOUD) at a Glance
In addition, EMCLOUD has increased ~370% since 2013, versus the Nasdaq Composite, S&P, and Dow that increased ~120%, ~75%, and ~75%, respectively. LTM EMCLOUD increased ~85%, while Nasdaq increased ~25%, S&P ~20%, and Dow ~20%.
The Cloud Computing Revolution Marches On
We appreciate the opportunity to be a part of this ever-expanding cloud community, and we are extremely excited to continue to provide the benchmark for the growth cloud industry, now in partnership with Nasdaq. We look forward to welcoming more companies added to this index over time.
Find the BVP Nasdaq Emerging Cloud Index here: https://indexes.nasdaqomx.com/Index/Overview/EMCLOUD
-Byron Deeter, Kristina Shen, Mary D’Onofrio
Partner @bessemervp w/ @twilio @boxhq @doubledutch @simplymeasured @clear_care @sendgrid @clearslide @adaptiveinsight @instructure @intercom @docusign @vidyard
151 
151 
151 
Partner @bessemervp w/ @twilio @boxhq @doubledutch @simplymeasured @clear_care @sendgrid @clearslide @adaptiveinsight @instructure @intercom @docusign @vidyard
"
https://blog.heptio.com/perspective-on-multi-cloud-part-3-of-3-availability-and-multi-cloud-5018762d2702?source=search_post---------69,"In the first post in this series I talked about the cloud providers and investing in multi-public cloud strategies. The second post explored topics around on-premises and public cloud. In this post I dig into the topic of achieving availability in the cloud.
By way of background, my perspective on this topic is colored by years spent working at Google. Google has developed a pretty nuanced view on service availability. The dominant concern of the Google SRE (site reliability engineer) is achieving a negotiated SLA (service level agreement). They balance the practicalities of building and delivering an evolving technology with achieving an acceptable level of availability.
As companies large and small explore the topic of multi-cloud, the impact of single provider failures and the need to be resistant to provider outages comes up. I hope to provide a useful framework to think about this.
Obviously there is a lot more to this topic than I can cover in one post, so we will focus here on the availability of the compute resources that make up a service. Storage systems availability considerations are something for a future post.
Given a top line availability goal for a service, there are two things that dominate the equation: MTTF (mean time to failure) and MTTR (mean time to recovery). The overall availability as observed by users of a service is approximated by the formula A = MTTF / (MTTF+MTTR).
If you are down for ten seconds a day for ten years there is a chance no one will notice. A customer refreshes their website page 10 seconds after receiving an HTTP 404 or 503 (or whatever). The app picks up where they left off and likely think nothing of it (assuming you don’t loose a bunch of state in the process: there is no forgiveness for service outage impacting data underlying state). If however you are down for a whole business day during office hours, chances are people are going to notice and talk about it. You may well lose business. Interestingly over a 10 year timeframe both are examples of the same fundamental SLA: 99.99% availability (“four 9's”).
It worries me that people building real world systems often fail to separate the idea of regularity of service interruption with the impact of a single service interruption event. The relentless focus on driving down MTTF introduces complexities that can ironically increase MTTR considerably.
Friendly piece of advice 1: Don’t conflate MTTF and MTTR. In most real world environments you don’t have enough real world experience with your service to truly understand your MTTF. Unless you are particularly unlucky, you probably have not yet seen a five standard deviation outage event yet. MTTR is something that you can perfectly understand (because you can test it). Managing it down is smart.
For a lot of folks running non-business critical workloads, a dead simple (even non-HA) clustering technology that can quickly be reconstituted from first principles is sensible vs introducing the operations complexity of more available systems. It is smart to step back and figure out what you want to accomplish in terms of top line availability, and what you are willing to pay to go beyond that level. Reflexively focusing on the most theoretically available solution may introduce more operational complexity than you should be taking on.
One of the lovely things about modern orchestration frameworks (like Kubernetes) is that they significantly reduce the MTTR for most application level outages. MTTF looks pretty much the same as it would running on a VM, but MTTR goes down from potentially hours (if operators are involved in recovery operations) to seconds in most cases. You can of course accomplish this in a traditional VM based flow, but even then the difference is pretty stark since VM provisioning and configuration can take minutes vs the second it takes to bounce a container to a new node.
Many customers first experiencing Kubernetes are giddy because what would traditionally be a pageable application component down event just goes away (the cluster detects failure and restarts the container). You can see a significant availability boost for a traditional application by containerizing, making sure the health checking model is correct and deploying it in a clustering environment.
Friendly piece of advice 2: for most common deployments, application level outages will dominate the MTTF equation. Think about using modern orchestration technologies to smooth over application outages, and isolate your deployment from individual infrastructure (node) failures.
You are however still subject to the realities of zone based outages, and have to develop a plan accordingly. There is a temptation to create dynamic systems that spread work across multiple zones and autonomously adapt to outages. For sophisticated users this represents a lot of promise, but adds risk.
Almost every significant outage event I have seen has been as a result of operator error. Often as not an operator has fat fingered a command and pushed broken configuration to a large number of nodes. While federation technologies that stitch deployments across multiple failure domains solve for one problem it is a very sharp tool indeed. When a single control plane stretches across zones, the “blast radius” of such a mistake can now also stretch across zones.
Friendly piece of advice 3: A modest substitution of human toil for fancy automatic federation will often insulate you from operator driven correlated outages. Simpler systems in the real world will often achieve better overall availability, particularly when you factor in the MTTR part of the availability equation.
In the case of Kubernetes, the pattern I favor is to run two (or more) independent clusters in different failure domains (zones). A professionally (cloud provider) run load balancer in front of them spreads load between them. Within each zone you implement an isolated scaling mechanism that allows each cluster to grow should all the load be delivered to a single zone. This introduces toil (yes, you are doubling the number of clusters you have to run and operate), but you also have the flexibility to run green/blue deployments and the work that is being done in the community to manage down the operations complexity of a cluster will continue to make it easier to live with. More importantly if an operator breaks one zone, a simple configuration update you can recover with a very low MTTR.
Friendly piece of advice 4: When you use a technology that federates or spreads load across failure domains to achieve very low MTTF’s, consider using something built and run by the pros (cloud providers). Build in controls (technical or procedural) to avoid pushing configurations that impact multiple failure domains simultaneously.
At the end of the day, for the vast majority of normal users, running in normal situations, cloud provider outages are not going to dominate the availability equation for applications. While multi-failure domain deployment (within a single cloud provider) is essential in many real world applications, the cost and complexity of running a single application across multiple public clouds won’t benefit the average user today. Some technically sophisticated organizations could achieve better general availability by running across clouds, but in reality the systems necessary to do this will introduce more problems than they solve.
Friendly piece of advice 5: Unless you are on the absolute bleeding edge of distributed systems design, and have deep operations DNA, you are not going to get better top line availability for a service by actively running it across multiple public cloud providers.
The cloud providers do a very good job managing security, availability, establishing disaster recovery protocols, etc than any but the most wealthy, paranoid and operationally mature organizations. There are two things that I do think about a fair bit though (but then I tend to be a bit paranoid) :
(1) Most of the outages we see are simple operator error. Services go down because someone makes a mistake. The providers are very good about understanding this and creating better controls each time it happens. Every service outage improves the cloud service. But what would happen if a smart bad actor managed to infiltrate the control plane of a cloud provider? To be clear this is far less likely than the same happening to someone running their own on-premises software but with systems this large and intricate it isn’t clear how quickly they could be restarted (the TTR could be very high indeed).
(2) We haven’t seen the final state of the cloud giants. Microsoft and Google are charging hard and each have unique advantages over the incumbent, if for no other reason that they started later with public cloud and have the advantage of building from a newer technology base.
While I am hardly unbiased, I will close this series with one last piece of advice.
Last piece of friendly advice: The landscape is still evolving and it seems likely that we haven’t seen the worst of all outage events, or even settled into which cloud provider yields the best overall availability. I don’t advocate actively running apps across multiple clouds today since the technology needed isn’t built and battle tested. It likely creates more problems than it solves. It would however not hurt to have a playbook that would allows you to your critical services up and running in another cloud if you really, really needed to.
Consider: (1) betting on a framework that allows you to relatively quickly turn up a critical service somewhere else to create a business survivable MTTR, (2) think about technology that supports the propagation of critical data to another cloud for safe keeping, (3) actually try it from time to time.
Hope this was helpful. Do reach out to us at Heptio (inquiries@heptio.com) if you want to talk with us about this. Also follow @heptio on Twitter.
Heptio
51 
51 claps
51 
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
"
https://blog.realkinetic.com/gcp-and-aws-whats-the-difference-3b1329f0ffb3?source=search_post---------70,"AWS has long been leading the charge when it comes to public cloud providers. I believe this is largely attributed to Bezos’ mandate of “APIs everywhere” in the early days of Amazon, which in turn allowed them to be one of the first major players in the space. Google, on the other hand, has a very different DNA. In contrast to Amazon’s laser-focused product mindset, their approach to cloud has broadly been to spin out services based on internal systems backing Google’s core business. When put in the context of the very different leadership styles and cultures of the two companies, this actually starts to make a lot of sense. But which approach is better, and what does this mean for those trying to settle on a cloud provider?
I think GCP gets a bad rap for three reasons: historically, their support has been pretty terrible, there’s the massive gap in offerings between GCP and AWS, and Google tends to be very opaque with its product roadmaps and commitments. It is nearly impossible now to keep track of all the services AWS offers (which seems to continue to grow at a staggering rate), while GCP’s list of services remains fairly modest in comparison. Naively, it would seem AWS is the obvious “better” choice purely due to the number of services. Of course, there’s much more to the story. This article is less of a comparison of the two cloud providers (for that, there is a plethora of analyses) and more of a look at their differing philosophies and legacies.
AWS and GCP are working toward the same goal from completely opposite ends. AWS is the ops engineer’s cloud. It provides all of the low-level primitives ops folks love like network management, granular identity and access management (IAM), load balancers, placement groups for controlling how instances are placed on underlying hardware, and so forth. You need an ops team just to manage all of these things. It’s not entirely different from a traditional on-prem build-out, just in someone else’s data center. This is why ops folks tend to gravitate toward AWS — it’s familiar and provides the control and flexibility they like.
GCP is approaching it from the angle of providing the best managed services of any cloud. It is the software engineer’s cloud. In many cases, you don’t need a traditional ops team, or at least very minimal staffing in that area. The trade-off is it’s more opinionated. This is apparent when you consider GCP was launched in 2008 with the release of Google App Engine. Other key GCP offerings (and acquisitions) bear this out further, such as Google Kubernetes Engine (GKE), Cloud Spanner, Firebase, and Stackdriver.
A client recently asked me why more companies aren’t using Heroku. I have nothing personal against Heroku, but the reality is I have not personally run into a company of any size using it. I’m sure they exist, but looking at the customer list on their website, it’s mostly small startups. For greenfield initiatives, larger enterprises are simply apprehensive to use it (and PaaS offerings in general). But I think GCP has a pretty compelling story for managed services with a nice spectrum of control from fully managed “NoOps” type services to straight VMs:
Firebase, Cloud Functions → App Engine → App Engine Flex → GKE → GCE
With a typical PaaS like Heroku, you start to lose that ability to “drop down” a level. Even if a company can get by with a fully managed PaaS, they feel more comfortable having the escape hatch, whether it’s justified or not. App Engine Flexible Environment helps with this by providing a container as a service solution, making it much easier to jump to GKE.
I read an article recently on the good, bad, and ugly of GCP. It does a nice job of telling the same story in a slightly different way. It shows the byzantine nature of the IAM model in AWS and GCP’s much simpler permissioning system. It describes the dozens of compute-instance types AWS has and the four GCP has (micro, standard, highmem, and highcpu — with the ability to combine whatever combination of CPU and memory that makes sense for your workload). It also touches on the differences in product philosophy. In particular, when GCP releases new services or features into general availability (GA), they are usually very high quality. In contrast, when AWS releases something, the quality and production-readiness varies greatly. The common saying is “Google’s Beta is like AWS’s GA.” The flipside is GCP’s services often stay in Beta for a very long time.
GCP also does a better job of integrating their different services together, providing a much smaller set of core primitives that are global and work well for many use cases. The article points out Cloud Pub/Sub as a good example. In AWS, you have SQS, SNS, Amazon MQ, Kinesis Data Streams, Kinesis Data Firehose, DynamoDB Streams, and the list seems to only grow over time. GCP has Pub/Sub. It’s flexible enough to fit many (but not all) of the same use cases. The downside of this is Google engineers tend to be pretty opinionated about how problems should be solved.
This difference in philosophy usually means AWS is shipping more services, faster. I think a big part of this is because there isn’t much of a cohesive “platform” story. AWS has lots of disparate pieces — building blocks — many of which are low-level components or more or less hosted versions of existing tech at varying degrees of ready come GA. This becomes apparent when you have to trudge through their hodgepodge of clunky service dashboards which often have a wildly different look and feel than the others. That’s not to say there aren’t integrations between products, it just feels less consistent than GCP. The other reason for this, I suspect, is Amazon’s pervasive service-oriented culture.
For example, AWS took ActiveMQ and stood it up as a managed service called Amazon MQ. This is something Google is unlikely to do. It’s just not in their DNA. It’s also one reason why they are so far behind. GCP tends to be more on the side of shipping homegrown services, but the tech is usually good and ready for primetime when it’s released. Often they spin out internal services by rewriting them for public consumption. This has made them much slower than AWS.
Part of Amazon’s problem, too, is that they are — in a sense — victims of their own success. They got a much earlier head start. The AWS platform launched in 2002 and made its public debut in 2004 with SQS, shortly followed by S3 and EC2. As a result, there’s more legacy and cruft that has built up over time. Google just started a lot later.
More recently, Google has become much more strategic about embracing open APIs. The obvious case is what it has done with Kubernetes — first by open sourcing it, then rallying the community around it, and finally making a massive strategic investment in GKE and the surrounding ecosystem with pieces like Istio. And it has paid off. GKE is, by far and away, the best managed Kubernetes experience currently available. Amazon, who historically has shied away from open APIs (Google has too), had their hand forced, finally making Elastic Container Service for Kubernetes (EKS) generally available last month — probably a bit prematurely. For a long time, Amazon held firm on ECS as the way to run container workloads in AWS. The community spoke, however, and Amazon reluctantly gave in. Other lower-profile cases of Google embracing open APIs include Cloud Dataflow (Apache Beam) and Cloud ML (TensorFlow). As an aside, machine learning and data is another area GCP is leading the charge with its ML and other services like BigQuery, which is arguably a better product than Amazon Redshift.
There are some other implications with the respective approaches of GCP and AWS, one of which is compliance. AWS usually hits certifications faster, but it’s typically on a region-by-region basis. There’s also GovCloud for FedRAMP, which is an entirely separate region. GCP usually takes longer on compliance, but when it happens, it certifies everything. On the same note, services and features in AWS are usually rolled out by region, which often precludes organizations from taking advantage of them immediately. In GCP, resources are usually global, and the console shows things for the entire cloud project. In AWS, the console UIs are usually regional or zonal.
For a long time, billing has been a rough spot for GCP. They basically gave you a monthly toy spreadsheet with your spend, which was nearly useless for larger operations. There also was not a good way to forecast spend and track it throughout the month. You could only alert on actual spend and not estimated usage. The situation has improved a bit more recently with better reporting, integration with Data Studio, and the recently announced forecasting feature, but it’s still not on par with AWS’s built-in dashboarding. That said, AWS’s billing is so complicated and difficult to manage, there is a small cottage industry just around managing your AWS bill.
Related to billing, GCP has a simpler pricing model. With AWS, you can purchase Reserved Instances to reduce compute spend, which effectively allows you to rent VMs upfront at a considerable discount. This can be really nice if you have stable and predictable workloads. GCP offers sustained use discounts, which are automatic discounts that get applied when running GCE instances for a significant portion of the billing month. If you run a standard instance for more than 25% of a month, Google automatically discounts your bill. The discount increases when you run for a larger portion of the month. They also do what they call inferred instances, which is bin-packing partial instance usage into a single instance to prevent you from losing your discount if you replace instances. Still, GCP has a direct answer to Amazon’s Reserved Instances called committed use discounts. This allows you to purchase a specific amount of vCPUs and memory for a discount in return for committing to a usage term of one or three years. Committed use discounts are automatically applied to the instances you run, and sustained use discounts are applied to anything on top of that.
Support has still been a touchy point for GCP, though they are working to improve it. In my experience, Google has become more committed to helping customers of all sizes be successful on GCP, primarily because AWS has eaten their lunch for a long time. They are much more willing to assign named account reps to customers regardless of size, while AWS won’t give you the time of day if you’re a smaller shop. Their Customer Reliability Engineering program is also one example of how they are trying to differentiate in the support area.
Something interesting that was pointed out to me by a friend and former AWS engineer was that, while GCP and AWS are converging on the same point from opposite ends, they also have completely opposite organizational structures and practices.
Google relies heavily on SREs and service error budgets for operations and support. SREs will manage the operations of a service, but if it exceeds its error budget too frequently, the pager gets handed back to the engineering team. Amazon support falls more on the engineers. This org structure likely influences the way Google and Amazon approach their services, i.e. Conway’s Law. AWS does less to separate development from operations and, as a result, the systems reflect that.
Suffice to say, there are compelling reasons to go with both AWS and GCP. Sufficiently large organizations will likely end up building out on both. You can use either provider to build the same thing, but how you get there depends heavily on the kinds of teams and skill sets your organization has, what your goals are operationally, and other nuances like compliance and workload shapes. If you have significant ops investment, AWS might be a better fit. If you have lots of software engineers, GCP might be. Pricing is often a point of discussion as well, but the truth is you will end up spending more in some areas and less in others. Moreover, all providers are essentially in a race to the bottom anyway as they commoditize more and more. Where it becomes interesting is how they differentiate with value-added services. This is where “multi-cloud” becomes truly meaningful.
Real Kinetic has extensive experience leveraging both AWS and GCP. Learn more about working with us.
Our thoughts, opinions, and insights into technology and…
115 
2
115 claps
115 
2
Written by
Managing Partner at Real Kinetic. Interested in distributed systems, messaging infrastructure, and resilience engineering.
Our thoughts, opinions, and insights into technology and leadership. We blog about scalability, devops, and organizational issues.
Written by
Managing Partner at Real Kinetic. Interested in distributed systems, messaging infrastructure, and resilience engineering.
Our thoughts, opinions, and insights into technology and leadership. We blog about scalability, devops, and organizational issues.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/levvel-consulting/saml2-vs-jwt-apigee-azure-active-directory-integration-a-jwt-story-a3eb00769a1f?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
In our next SAML2 vs JWT post, we are going to use a JWT with a very simple API that is proxied through Apigee Edge Public Cloud. The JWT token will be an OAuth2 access token generated by Azure Active Directory. In the last post in this series, we explored what JSON Web Tokens (JWTs) are and the information it contains. This post builds on the capabilities presented by Dino Chiesa at the Apigee I Love APIs conference in October, 2015. The example presented in this post is available in this github repository. This post briefly describes how to adapt the solution to working with other Azure Active Directory tenants, but primarily focuses on the details of making work with the example tenant setup here — keeping a functioning AAD tenant up and available for this example costs money and presents some security issues.
The figure below shows the system actors that will be involved in this story.
An API Consumer will obtain an access token (a JWT) by authentication against Azure Active Directory using an OAuth2 Resource Owner Password Credentials Authorization Grant. The JWT is then placed into the Authorization header of an API request and sent to the Apigee Edge API Gateway that we’ve constructed. The API Proxy pulls down and caches the federation metadata that is published by Azure Active Directory. The API Proxy attempts to validate the JWT token included in the request; the token is determined to either be valid or not. If it is valid, then the JWT is removed from the request and passed back to the API Provider. If the token is not valid, then an error is returned to the API Consumer.
First, we need an API. Let’s use https://timeapi.org. This will create a nice example that is harmless and mostly pointless (there are probably easier ways to find the time). Without the API Gateway layer, this API call will look something like:
Request:
Response:
The full documentation of this API is available at https://www.timeapi.org.
There are numerous API interface documentation paradigms out there. I like to use Swagger 2.0 or OpenAPI v2.0. Apigee is built around this interface language, I work for an Apigee partner, and it is simple to setup for most APIs. I created a simple Swagger definition that describes the functionality of this API that we are using. It can be found here. Apigee has the ability to define an API Proxy based upon the structure of an existing Swagger interface definition. This can definitely eliminate some of the busy work associated with creating a new API Proxy on Apigee Edge, but it will lead to a much more complex example for what we are trying to show.
Now, our goal is to offload the authentication and authorization functions of this API onto the API Gateway layer. To do this securely, we would normally establish a trust relationship between the API Gateway and the API Provider. Unfortunately, timeapi.org is a public API that doesn’t take any steps to provide such a mechanism. So, we won’t be able to do that for this example. If we did control the API Provider, that trust relationship could be accomplished through a variety of mechanisms including, but not limited to:
The authentication step will be the validation of a JWT token (per the spec) including:
The Authorization step could be quite elaborate, but for our purposes, we will rely upon the OAuth2/JWT authorization mechanisms that are provided to us in the form of the audience (“aud” parameter). As long as the this field contains the expected string, it is accepted — a very coarse grained authorization policy. The audience parameter in the JWT token, multiple roles described by its values, and a non-trivial mapping of API endpoints to roles provides the basis of a rich Role-Based Access Control (RBAC) policy. But, again, for our purposes, as long as the JWT token included with the request contains the expected audience value, the request will be considered authorized.
First, let’s construct a simple API Proxy that can proxy requests back to https://www.timeapi.org. This can be done with this tutorial.
The resulting API Proxy will look something close to the API Proxy in the image above. To access this API endpoint, one must call:
If your organization name is rcbjBlueMars (see upper right-hand corner), environment is called test (see upper right-hand corner), and the base path is “/time”, then this would be:
Since Apigee automatically does a translation of the front-end path and backend path,
will be translated to:
To have this solution look a little more professional and have the DNS names and OAuth2 scope information match it, we will setup a DNS CNAME entry called rcbj0001-api.rcbj.net that points at rcbjbluemars-test.apigee.net. This particular domain is registered at godaddy.com. So, that is done through standard configuration mechanisms in its domain management application. Apigee Support would need to add this name to the Virtual Host configuration in Apigee Edge Public Cloud. I’m doing this in the community edition; so, this isn’t available, but in any of the paid-for versions of the platform, this can be done quite easily. Then, the API request could be sent to:
The response looks just like the original API call response above. The Apigee Trace (think of it as an API processing policy debugger) session for this request looks like the following:
Obviously, there isn’t much going on here at this point. We are going to change that.
Now, consider this presentation from the Apigee I Love APIs conference in October, 2015, describing the use of JWT with Apigee API Proxies. This presentation and accompanying code were created by Dino Chiesa who I did two webcasts with in 2016. The code that accompanies this presentation can be found here. Apigee doesn’t have out of the box support for JWT token generation or validation. But, this project contains custom Java Code that can be used to validate JWT tokens from a variety of sources (Google, Azure Active Directory, SalesForce, etc). Instructions for how to deploy the sample API Proxy are included on the GitHub repository; I’m not going to cover the build and deploy process for the sample proxy.
I extracted the conditional rule that can validate JWTs generated by Azure Active Directory. We are going to use AAD as the Identity Provider that generates JWT tokens. Azure Active Directory uses JWT as the OAuth2 access token, which works out well for our goals. The details of how an Azure AD tenant was configured to work with this tutorial can be found here.
If we add the “parse + validate alg=RS256-ms” conditional rule to our sample proxy from above, we have something that looks similar too:
I added additional actions that extract the JWT token from Authorization header and, if valid, remove the Authorization header before forwarding the API request to the API Provider endpoint.
CORS has also been added to this project following the instuctions outlined here.
This API Proxy can be downloaded here.
The original example from the Apigee conference requires the APigee Edge enterprise pricing plan. To get this to run in a non-Enterprise Apigee organization, a couple of tweaks had to be made to the Java code. The source code and instructions for building the modified libraries are available in the github repository.
To run this in your own environment a couple of configuration changes must be made to the API Proxy including:
For an API Consumer, we will use the following script that runs a series of curl commands. The first curl command is the call to AAD to obtain an access token (JWT); the second curl command is the call to the API.
This script does not require any arguments. If it is called test-client.sh (as it is in the github repository), then one would simply run:
If you are trying to get this to work in your own environment and your own AAD tenant, then the CLIENT_ID, USERNAME_, PASSWORD_, RESOURCE_URL, and TENANT_ID variables must be updated to match the details for your tenant.
The first curl command (OAuth2 call) response looks something like:
The access_token property is a JWT token. This can be copied directly into the Authorization HTTP Request Header (per RFC 6750) as “Bearer JWT…”.
The payload of this JWT token looks like:
The details of what is in this token are explored in our last post.
An application such as a web application, SPA app, or Mobile App could cache the access token and refresh token. The access token can be used across API calls until it expires. The refresh token can be used to obtain a new access token — more on this later. Eventually, the user would have to log into the app again; this would be a configurable set of parameters. Applications such as these would likely use an interactive login with the OAuth2 Authorization Code Authorization Grant or Implicit Authorization Grant.
The trace session associated with running the test script will look similar to the following:
The first policy (colored box) in the request is a Cache Lookup Policy. This looks in an Apigee Cache called signer-cert for the cached copy of the Azure Active Directory signer certificate included in the Federation Metadata for the AAD tenant that this API Proxy trusts. Since this isn’t the first time this API Proxy has been run in the past hour, the certificate is found and assigned to a flow variable called “cached.ms.cert”. Since there was an entry in the cache, the next four policies are skipped (as indicated by the arced arrow in the upper, left-hand corner of each. Had the cached metadata not been found, the second policy (a Service Callout Policy) would have made a call out to:
The metadata document would be written to a flow variable called “msCert”.
The third policy (which also didn’t run because the metadata was found in the cache) is a Javascript callout that removes the XML Declaration from the XML document stored in the “msCert” flow variable. The fourth policy (also didn’t run this time) is an Extract Variable policy that reads the signer certificate value from the retrieved XML document via an XPath expression and would assign the certificate value to the “ms.certificate” flow variable. The fifth policy (ran this time) is a Populate Cache Policy that writes the “ms.certificate” flow variable to the signer-cert Apigee Cache. The sixth policy (also didn’t run this time) writes the value stored in the “ms.certificate” flow variable to the signer-cert cache so that it can be efficiently retrieved later. The seventh policy, which always runs, extracts the JWT token from the Authorization header and places it in “authn.jwt” flow variable. The eighth policy is a Java Callout that runs the code from the original GitHub project and validates the JWT token. The next policy is a Raise Fault Policy that only runs if an error condition flow variable was set in the Java Callout Policy that just ran — any failure of JWT validation returns a 401. The final policy is a Javascript Callout that removes the Authorization header and the JWT token it contains.
The response processing only runs one Policy, which sets several CORS-related HTTP Response Headers.
Another way to test this API is to use Swagger Editor. Load the Time API Swagger document into the Swagger Editor by choosing File->Import File from the menu. Then, click the “Choose File”. Load the $REPOSITORY_HOME/swagger/swagger.json. Finally, click the Import button. You will see a screen similar to the following.
Click the “Authentication” button.
Run the test-client.sh script again. Use the ASSERTION variable value as the access token value that is put in this pop up’s field. Then, click the Authenticate button.
Now, scroll down to the “Try this operation” button for the GET on /{timezone}/{when}.json. The screen expands to show fields to put values in for all varibles.
Make sure the check box next to “oauth2ResourcePasswordGrant” is checked. In the “timezone” field, add a mainland US timezone (as defined in the swagger interface definition) such as “pst”. In the “when” field, add a natural language time description as defined by the chronic documentation (such as “now”). The request should look similar to the following at this point.
Click the “Send Request” button. The response will look something like the following.
With that, we have covered this JWT-based authentication and authorization example end-to-end. As always, please leave comments and questions. For more from Robert and the Levvel team, visit our website.
Ask us how we can help transform your business.
We help big companies innovate like startups, & small…
69 
1
69 claps
69 
1
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
Written by
My focus within Information Technology is API Management, Integration, and Identity–especially where these three intersect.
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
"
https://itnext.io/migrate-for-anthos-application-migration-and-modernization-78113264b07a?source=search_post---------72,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Migration involves lifting and shifting virtual machines/legacy applications running on physical servers into public cloud platforms which offer many benefits, including enhanced agility, significantly reduced overhead compared to a data center, and a standard usage and management environment. The first step of modernizing an application is to break an application into a set of container images that include everything needed to run a portion of the application: code, runtime, system tools, system libraries, and settings. With Kubernetes all these containers can be hosted on a “managed” cluster where all the underlying Hardware, OS/Kernel and Cluster SRE (Site Reliability Engineering) become the responsibility of the cloud provider.
Migrate for Anthos addresses both migration and modernization providing an almost real-time solution to take an existing VM and make it available as a Kubernetes hosted pod with all the values associated with executing the applications in a Kubernetes cluster. This framework minimizes the manual effort required to move and convert existing applications into containers on Anthos GKE and Google Kubernetes Engine. Users can target workloads from virtual machines or physical servers running on-premises, in Compute Engine, or in other clouds.
For containerizing the virtual machines Migrate for Anthos creates a wrapper container image from the source VM, the OS is replaced with the one supported by GKE, the filesystem is analyzed by parsing fstab and then a Persistent Volume is mounted using Migrate for Anthos Container Storage Interface (CSI) driver which streams the data from the source VM.
Migrate for Anthos converts the source VMs to system containers running in GKE. System containers when compared to application containers run multiple processes and applications in a single container. Although, the application is not converted to true microservice architecture, there are some basic advantages with containerizing the monolith application which includes portability, better packing and containerization virtualize applications at the operating system level, with multiple containers running atop the OS kernel directly, making them far more lightweight and agile than VMs, providing developers with a sandboxed view of the OS logically isolated from other applications.
The tool provides an easy-to-use interface to plan, execute and manage the migration using declarative specification. All controllers run on a processing-cluster (GKE Cluster), users can manage all aspects of migration using CRD’s.
Migrate for Anthos works alongside Migrate for Compute Engine (in cloud-to-cloud migration scenario, not needed when the SourceProvider is Google Compute Engine) which uses streaming technology, where the complete VM storage need not be migrated to run the container in GKE, this speeds up the entire migration process. Formerly Velostrata — this cloud migration technology was acquired by Google last year to augment Kubernetes, and it does so by delivering on two significant capabilities — converting existing VMs into Pods (Kubernetes applications) and streaming on-premise virtual and physical machines to generate clones in GCE instances.
A sample workflow below shows sequence of steps and explanation of components involved in migrating a sample Apache Tomcat application running as virtual machine on GCE (Google Compute Engine) to a Kubernetes Deployment running on GKE (Google Kubernetes Engine) using Migrate for Anthos.
The migration source here is GCE and there is no need of Migrate for Compute Engine (formerly Velostrata).
The migration architecture involves a GKE cluster to host various controllers required for migration and is referred as ‘processing cluster’. Once the migration is complete all the artifacts are stored in GCS (Google Cloud Storage) and the generated container image is pushed to GCR (Google Container Registry).
Migration (setting up and managing a Migrate) can be performed using Migrate for Anthos CLI (migctl) available in cloud shell or using the Anthos portal (Migrate to containers).
The sequence of steps in migrating a VM to a container, followed by its deployment on GKE is as shown below:
A sample Tomcat application deployed from marketplace is used for tryout, the application is deployed to a single Compute Engine instance.
As shown below the application is fully functional displaying default webpage when accessed using the External_IP.
A GKE cluster called ‘processing cluster’ is created, this will host all required Migrate for Anthos controllers, CRD’s and other components required to perform the migration.
Once the GKE cluster is available, users can use ‘migctl’ (Migrate for Anthos CLI) to bootstrap required infra.
As shown below the ‘migctl setup install’ creates an admission-validator, csi-vlsdisk-controller, csi-vlsdisk-node, v2k-controller-manager, v2k-generic-csi-controller, v2k-generic-csi-node. Apart from these, a cert-deploy job is created which creates required certificates for webhook authentication and a controllers-upgrade job which upgrades the controllers to the latest version.
As part of bootstrap, multiple CRD’s are installed on the cluster. Every migration unit/step/configuration persists in the cluster as a custom resource.
As part of bootstrap a bucket is created in GCS (Google Cloud Storage) which stores the artifacts once the migration is complete.
The migration requires users to define a source where the instance is migrated from. Supported sources include VMware, AWS, Azure, or Compute Engine. If the migration is from CE (Google Compute Engine) there is no need for Migrate for Compute Engine, Migrate for Compute Engine is required when migrating virtual machines (VMs) running on other clouds including on-premises (VMware).
The SourceProvider is used in migration configuration. Migration configuration can be customized: Selection of areas excluded from containerization, Selection of area to be extracted as Persistent Volume, Artifacts storage location information, storage settings, etc. , shown below is the SourceProvider (for CE) and Migration CRD:
Users can use ‘migctl’ to configure the SourceProvider and the cluster can have multiple SourceProvider(s) objects which can be used in different Migration configurations.
As shown above, the controller manager watches for Anthos Migrate related objects and realize the same on the cluster.
A CRD SourceProvider holds the information provided above, here the SourceProvider is Compute Engine.
Migration of a VM is initiated by starting migration from portal or using the CLI. This results in creation of a migration plan file. This is implemented as a Kubernetes Custom Resource Definition (CRD) and is contained along with additional resources such as a Kubernetes PersistentVolumeClaim in the migration plan file.
The migration plan is created as a custom resource and the same can be edited using Anthos portal or editing the Migration object from the cluster.
A migration plan can be customized based on the nature of the workload using an intent flag. The flag’s value determines the contents of your migration plan, which in turn guides the migration.
There are three intents supported in the migration schema. Image: Used with stateless workloads, ImageAndData: Used with stateful workloads where the application and user mode system is extracted to a container image, and data is extracted to a persistent volume, Data: Used with stateful workloads where only the data portion of the migration plan will be extracted into a persistent volume.
By default, Migrate for Anthos will exclude typical VM content that isn’t relevant in the context of GKE. The filters field value lists paths that should be excluded from migration and will not be part of either the container image or the data volume.
The dataVolumes field specifies a list of folders to include in the data volume as part of the migration. This must be edited for a migration with an intent of Data or ImageAndData.
Using the metadata field, users can specify the name and namespace for the migration. By default, Migrate for Anthos uses the values specified in the migration spec.
Name of the image that will be created from the VM files and directories copied from the source platform. This image will not be runnable on GKE because it hasn’t been adapted for deployment there.
Migrate for Anthos uses Cloud Storage to store files related to migration, bucket and folder names can be customized.
This step also creates initial disk snapshots/copies and stores the same in a persistent volume and seamlessly creates PV’s and PVC’s required for the same.
This stage executes the migration plan and the following tasks are executed:
The controller creates a job for generating the artifacts and uploading the image to GCR:
Once the migration is complete the migration details — artifacts section will list all the resource URL’s. Generate artifacts creates a Dockerfile, Container image for deployment (ready-to-use image), Container image base layer (non-runnable image layers), Kubernetes deployment spec, Migration plan and a file with all artifact links.
All the artifacts generated above are stored in GCS (Google Cloud Storage):
Separate buckets are created for artifacts and image layers:
The buckets are created by PVC’s requesting storage (StorageClass: <GCS>) and the corresponding persistent volumes are created in the cluster.
Generated image is pushed to GCR (Google Container Registry) and the same is configured in deployment spec.
Generated Dockerfile:
File with all artifact links:
Generated Deployment spec:
The generated deployment file can be used to deploy the migrated application as a Kubernetes deployment.
The deployment spec will be configured with the image pushed to GCR:
As shown below the application is fully functional and running as a Kubernetes pod.
Users can use all the features of GKE and other supporting operations, monitoring and networking features available on GCE with the migrated application.
In this scenario, Migrate for Anthos uses Migrate for Compute Manager which streams cloud or on-premise virtual and physical machines to generate clones in GCE instances.
As the other scenario discussed above is within GCP there is no need for this entity, here VM’s from external cloud providers are migrated (cloned) to GCE and follows the same steps to migrate the VM to a container once the VM is available on GCE. Communication between Migrate for Compute Engine and the source cloud will be via VPN.
Apart from setting up VPN and configuring a Migrate for Compute manager, the VM -> Container migration involves the same steps as CE — SourceProvider.
Migrate for Compute Engine (formerly Velostrata) is available in marketplace and is deployed as a managed instance.
Deploying Migration Manager requires users to enable site-to-site VPN and required firewalls to enable communication between the VPC in which the manager is positioned and the VPC of the source VM on the other cloud.
As shown in the scenario below, a site-to-site VPN is created between GCP and AWS. The same implies to Azure:
As shown below GCP uses Cloud Router that works over Cloud VPN or Cloud Interconnect connections to provide dynamic routing by using the Border Gateway Protocol (BGP) for your VPC networks.
AWS site-to-site VPN:
The manager requires specific permissions to handle the instances like switch-off, switch-on etc., this can be enabled using policies attached to a user and providing the user keys while configuring the manager. An example on AWS is as below:
Once the migration manager is deployed users can configure the same using the portal/UI. Users should provide authentication to both source and destination clouds. For GCP it is using the service-account-json file and for AWS it is AcessKey and SecurityKey.
A Cloud Extension is configured which creates required VM’s on GCP (destination). A Cloud Extension uses a dual-node active/passive configuration for high availability; each node serves its own workloads while providing backup for the other.
Runbooks are configured which define the VMs to be migrated, their order, and additional configuration parameters.
Migrate for Anthos ServiceProvider spec should be configured with the manager information to enable migration, in this case there are two steps where the first migrates (technically creates a clone as GCE instances) the VM from an other cloud provider and secondly containerizing the same. Spec for migrating to compute engine from other clouds and on-prem is as shown below:
*******************************************************************
Accelerating the migration and adoption of the modern platform enables the business to operate more efficiently with unified policy, management, and skill sets across existing and newly developed apps.
When workloads are upgraded to containers running on managed Kubernetes clusters like GKE, IT departments can eliminate OS-level maintenance and security patching for VMs and automate at scale. Legacy apps that are migrated into GKE can be easily upgraded with modern functionality and a large set of available/maintained add-ons can be used for integration without having to rewrite application code.
ITNEXT is a platform for IT developers & software engineers…
34 
2
34 claps
34 
2
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/gitlab-magazine/there-are-3-public-clouds-left-and-well-use-all-of-them-214858d793e9?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
IBM was the 4th public cloud if you leave the Chinese market dominated by Alibaba out of the picture. IBM Cloud was losing ground to the top 3 public clouds. They decided to acquire RedHat for 31% of their market cap. This was a big bet on a hybrid and multi-cloud future.
That means that there will be three public clouds left that each spend $10b+ per year on new data centers.
The question is if organizations will standardize on one cloud or go multi-cloud. Moving an application to another cloud after you used cloud specific services like Lambda, DynamoDB, and BigQuery is a lot of work for little payoff.
Despite that it is likely that most organizations will be multi-cloud for the following reasons:
GitLab is a single application for the entire DevOps…
50 
50 claps
50 
Written by
Legal first name is Sytse. Co-founder and CEO of GitLab. I like innovative projects, accessible education, all remote work, charter cities, and macro economics.
GitLab is a single application for the entire DevOps lifecycle.
Written by
Legal first name is Sytse. Co-founder and CEO of GitLab. I like innovative projects, accessible education, all remote work, charter cities, and macro economics.
GitLab is a single application for the entire DevOps lifecycle.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/asecuritysite-when-bob-met-alice/data-security-and-resilience-using-secret-shares-and-elliptic-curve-methods-6c6ce6976a5c?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
Our data structures are often not secure, and these are especially at risk when we use public cloud systems. Along with this, we become highly dependent on specific cloud systems. Over the past few years, all the cloud infrastructures have had significant outages, and thus we need to build-in resilience into your data storage, and where we can cope with an…
"
https://medium.com/@odsc/an-introduction-to-aws-networking-virtual-private-cloud-1639a91c67c1?source=search_post---------75,"Sign in
There are currently no responses for this story.
Be the first to respond.
ODSC - Open Data Science
Jul 13, 2020·5 min read
Cloud computing is one of the major trends in computing today and has been for many years. Public cloud providers have transformed the start-up industry and what it means to launch a service from scratch. We no longer need to build our own infrastructure; we can pay public cloud providers to rent a portion of their resources for our infrastructure needs.
Amazon Web Services (AWS — : https://aws.amazon.com/) was the first company to offer IaaS public cloud services and was the clear leader in the space by market share in 2019. If we define the term Software–Defined Networking (SDN) as a group of software services working together to create network constructs, we can make the argument that AWS is the world‘s largest implementer of SDN. They utilize the massive scale of their global network, data centers, and servers to offer an amazing array of networking services. In this article, we will take a look at one of the most important concepts in AWS networking: Virtual Private Cloud.
This article is an excerpt from the book Mastering Python Networking, Third Edition by Eric Chou, a completely updated and revised edition of the bestselling guide to Python Networking, updated to Python 3 and including the latest on network data analysis, Cloud Networking, Ansible 2.8, and new libraries.
Amazon Virtual Private Cloud
Amazon virtual private cloud (Amazon VPC) enables customers to launch AWS resources in a virtual network dedicated to the customer‘s account. It is truly a customizable network that allows you to define your own IP address range, add and delete subnets, create routes, add VPN gateways, associate security policies, connect EC2 instances to your own data center, and much more.
In the early days when VPC was not available, all EC2 instances in the AZ were on a single, flat network that was shared among all customers. How comfortable would the customer be with putting their information in the cloud? Not very, I‘d imagine. Between the launch of EC2 in 2007 and the launch of VPC in 2009, VPC functions were some of the most requested features of AWS.
Since December 2013, all EC2 instances are VPC-only; you can no longer create an EC2 instance that is non-VPC (EC2-Classic), nor would you want to. If we use a launch wizard to create our EC2 instance, it will automatically be put into a default VPC with a virtual internet gateway for public access. In my opinion, only the most basic use cases should use the default VPC. In most cases, we should define our own non-default, customized VPC.
Creating VPC using AWS console
Let‘s create the following VPC using the AWS console in us-east-1:
VPC is AWS region-bound, and the subnets are AZ-based. Our first VPC will be based in us-east-1; the three subnets will be allocated to two different AZs in 1a and 1b.
Using the AWS console to create the VPC and subnets is pretty straightforward, and AWS provides a number of good tutorials online. I have listed the steps with the associated locations of each on the VPC dashboard:
The first two steps are point-and-click processes that most network engineers can work through, even without prior experience. By default, the VPC only contains the local route, 10.0.0.0/16. Now, we will create an internet gateway and associate it with the VPC:
We can then create a custom route table with a default route pointing to the internet gateway, which will allow for internet access. We will associate this route table with our subnet in us-east-1a, 10.0.0.0/24, thus allowing it to be public-facing:
Let‘s use the Boto3 Python SDK to see what we have created; I used the tag mastering_python_networking_demo as the tag for the VPC, which we can use as the filter:
This script will allow us to programmatically query the region for the VPC we created:
If we created EC2 instances and put them in different subnets as is, the hosts would be able to reach each other across subnets. You may be wondering how the subnets can reach one another within the VPC since we only created an internet gateway in subnet 1a? In a physical network, the network needs to connect to a router to reach beyond its own local network.
It is not so different in VPC, except it is an implicit router with a default routing table of the local network, which in our example is 10.0.0.0/16. This implicit router was created when we created our VPC. Any subnet that is not associated with a custom routing table is associated with the main table.
In this article, we covered AWS Virtual Private Cloud overview. Explore the power of Python libraries to tackle difficult network problems efficiently and effectively, including pyATS, Nornir, and Ansible 2.8 with Mastering Python Networking, Third Edition by Eric Chou.
About the Author
Eric Chou is a seasoned technologist with over 20 years of experience. He has worked on some of the largest networks in the industry while working at Amazon, Azure, and other Fortune 500 companies. Eric is passionate about network automation, Python, and helping companies build better security postures. Eric is also the primary inventor for two U.S. patents in IP telephony. He shares his deep interest in technology through his books, classes, and blog, and contributes to some of the popular Python open-source projects.
Original post here.
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
52 
52 
52 
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-free-tier-comparison-19b68578e7f?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 20, 2020·6 min read
Whether you’re new to public cloud altogether or already use one provider and are interested in trying another, you may be interested in a comparison of the AWS vs Azure vs Google free tier. The big three cloud providers — AWS, Azure and Google Cloud — each have a free tier available that’s designed to give users the cloud experience without all the costs. They include free trial versions of numerous services so users can test out different products and learn how they work before they make a huge commitment. While they may only cover a small environment, it’s a good way to learn more about each cloud provider. For all of the cloud providers, the 12-month free trials are available to only new users.
AWS free tier includes more than 60 products. There are two different types of free options that are available depending on the product used: always free and 12 months free. To help customers get started on AWS, the services that fall under the free 12-months are for new trial customers and give customers the ability to use the products for free (up to a specific level of usage) for one year from the date the account was created. Keep in mind that once the free 12 months are up, your services will start to be charged at the normal rate. Be prepared and review this checklist of things to do when you outgrow the AWS free tier.
The Azure equivalent of a free tier is referred to as a free account. As a new user in Azure, you’re given a $200 credit that has to be used in the first 30 days after activating your account. When you’ve used up the credit or 30 days have expired, you’ll have to upgrade to a paid account if you wish to continue using certain products. Ensure that you have a plan to reduce Azure costs in place. If you don’t need the paid products, there’s also the always free option.
Some of the ways people choose to use their free account are to gain insights from their data, test and deploy enterprise apps, create custom mobile experiences and more.
The Google Cloud Free Tier is essentially an extended free trial that gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts — a 12-month free trial with a $300 credit to use with any Google Cloud services and always free, which provides limited access to many common Google Cloud resources, free of charge. Google Cloud gives you a little more time with your credit than Azure, you get the full 12 months of the free trial to use your credit. Unlike free trials from the other cloud providers, Google does not automatically charge you once the trial ends — this way you’re guaranteed that the free tier is actually 100% free. Keep in mind that your trial ends after 12 months or once you’ve exhausted the $300 credit. Any usage beyond the free monthly usage limits are covered by the $300 free credit — you must upgrade to a paid account to continue using Google Cloud.
It’s important to note that the always-free services vary widely between the cloud providers and there are usage limitations. Keep in mind the cloud providers’ motivations: they want you to get attached to the services so you start paying for them. So, be aware of the limits before you spin up any resources, and don’t be surprised by any charges.
In AWS, when your free tier expires or if your application use exceeds the free tier limits, you pay standard, pay-as-you-go service rates. Azure and Google both offer credits for new users that start a free trial, which are a handy way to set a spending limit. However, costs can get a little tricky if you aren’t paying attention. Once the credits have been used you’ll have to upgrade your account if you wish to continue using the products. Essentially, the credit that was acting as a spending limit is automatically removed so whatever you use beyond the free amounts, you will now have to pay for. In Google Cloud, there is a cap on the number of virtual CPUs you can use at once — and you can’t add GPUs or use Windows Server instances.
For 12 months after you upgrade your account, certain amounts of popular products are free. After 12 months, unless decommissioned, any products you may be using will continue to run, and you’ll be billed at the standard pay-as-you-go rates.
Another limitation is that commercial software and operating system licenses typically aren’t available under the free tiers.
These offerings are “use it or lose it” — if you don’t use all your credits or utilize all your usage, there will be no rollover into future months.
AWS has 33 products that fall under the one-year free tier — here are some of the most popular:
For the always-free option, you’ll find a number of products as well, some of these include:
Azure has 19 products that are free each month for 12 months — here are some of the most popular:
For their always free offerings, you’ll find even more popular products — here are a few:
Unlike AWS and Azure, Google Cloud does not have a 12 months free offerings. However, Google Cloud does still have a free tier with a wide range of always free services — some of the most popular ones include:
Check out these blog posts on free credits for each cloud provider to see how you can start saving:
Originally published at www.parkmycloud.com on July 15, 2020
CEO of ParkMyCloud
41 
41 
41 
CEO of ParkMyCloud
"
https://medium.com/javarevisited/7-best-courses-to-learn-spring-boot-with-aws-and-azure-cloud-platform-9f953d12bb93?source=search_post---------77,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn how to deploy Spring Boot apps and Microservices on public cloud platforms like AWS, Azure, and Google Cloud Platform and looking for the best resources like online courses then you have come to the right place.
Disclosure — Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
Earlier, I have shared my favorite Spring Boot Course and best Microservices courses and in this course, I will share some advanced level courses to learn about containerization and deployment of Spring Boot Microservices on AWS, Azure, Google Cloud Platform, and even on legacy Cound Foundry.
If you have been reading technical blogs and articles then you might be hearing about widespread cloud adoption among all sizes of companies. In the last few years, many companies, both big and small have shifted their infrastructure to the cloud or in the process of doing it.
I have no doubt that the next generation of Java applications will be written for and run in the Cloud and that’s why it’s important for Java developers to learn about Cloud platforms like AWS, Azure, GCP, Cloud Foundry, and others.
Thankfully Java frameworks like Spring Framework is taking this cloud move seriously and new frameworks like Spring cloud is getting popular which makes developing cloud-based application easy.
While there will be some challenges to shift the focus from writing in premises to cloud-native applications, adopting Microservices architecture and cloud-native Java can help Java developers stay ahead of the curve.
The microservices architecture perfectly suits the public cloud, with its focus on elastic scaling with on-demand resources. Since most of the web application and Microservice development is happening on Spring Boot, the main thing you can learn as of now is how to deploy your Spring Boot application on different cloud platforms like AWS and Microsoft’s Azure Cloud Platform While I have shared about cloud-computing resources like AWS, Azure, and GCP in past my readers asked me something which is focused on Java and Spring Boot and that’s why I am going to share the 5 best cloud courses that are focused on Java and Spring Boot. These are the practical and hands-on courses that will teach you things like how to deploy your Spring Boot application to AWS with Elastic Beanstalk, ECS, and Fargat, or deploy Java Microservices to AWS and other cloud platforms. These courses are equally useful for both beginner and experienced Java developers how are working with Spring Boot and cloud platforms as well as DevOps Engineers who are responsible for managing Java applications on Cloud.
Without wasting any more of your time, here is a list of the best hands-on cloud courses for Java and Spring Boot developers. Currently, these online training course covers cloud platforms like Amazon Web Service, Microsoft Azure, Google Cloud Platform and Pivotal’s Cloud Foundry environment but I will keep adding new training courses which are focused on Java and Spring boot but teach you how to deploy a Spring Boot application and Microservices in Google Cloud Platform and others.
This is one of the first courses you should take if you want to deploy your Spring Boot applications to AWS. This course will teach you step by step to deploy a Java Spring Boot REST APIs and Full Stack application to AWS using Elastic Beanstalk service. Created by Ranga Rao Karnam, a fellow Java developer and best selling Udemy instructor this course will not only teach you to core AWS services like EC2, S3, AWS CodePipeLine, AWS CodeBuild, SQS, IAM, CloudWatch but also teach you things like how to deploy a RESTful web service into the cloud. You will learn how to containerize your Java and Spring Boot application using Docker and then deploy it into Cloud. You will also learn how to automatically scale your Java applications based on load as well as deploy multiple instances behind a load balancer using Elastic Beanstalk service in AWS.
You will also learn how to create a continuous delivery pipeline with AWS Code Pipeline which is quite important from a DevOps perspective.
Here is the link to join this course — Deploy Java Spring Boot Apps to AWS with Elastic Beanstalk
Overall, a very practical and useful course for experienced Java developer who wants to learn how to deploy, scale, and manager Java and Spring boot application on AWS.
This is another great course for Java developers who wants to learn how to deploy Spring Boot Applications to the Cloud on AWS and how to implement Continuous Integration and Continuous Delivery in AWS (CI/CD) for DevOps.  Created by John Thompson from Spring Framework Guru, one of my favorite Java instructors on Udemy, this course is focused on DevOps for Spring application on the AWS cloud platform.
In this course, you will learn how to deploy Spring Applications to multiple environments including AWS. You will start with basics like creating a server in AWS using the Amazon EC2 service. This is a very hands-on course and to get the most out of this course, you will need an AWS account. Don’t worry, you don’t need to spend any additional money as you should be able to use the AWS free tier to complete the course assignments.
In this course, you will learn how to install Jenkins on a Linux server. A server that you will provide in the AWS cloud. You will also learn how to use Docker and MySQL databases in the AWS environment.
Here is the link to join this course — Spring Framework DevOps on AWS
The course also teaches you best practices used in enterprise software development like using a continuous integration server for continuous delivery.
This is another advanced Spring Boot Microservice course for Java developers who wants to deploy into public cloud computing platforms like AWS and Azure.
One of the key steps before deploying into the cloud is containerizing or Dockerizing your Spring Boot applications and this course will teach you how to extend, refine, harden, test, and “dockerize” your Spring Boot microservices, and turn them into production-ready applications.
You will also learn about how to link to external databases, build secure APIs, use unit and integration testing to uncover application flaws during development and configure scalable deployment options with Docker containers.
Overall an advanced course to extend, refine, harden, test, and “dockerize” your Spring Boot microservices, and turn them into production-ready applications.
Here is the link to join this course — Extending, Securing, and Dockerizing Spring Boot Microservices
By the way, you would need a LinkedIn Learning membership to watch this course which costs around $19.99 per month but you can also watch this course for FREE by taking their 1-month-free-trail which is a great way to explore their 16000+ online courses on the latest technology.
This is a free Spring Boto Microservice course from Coursera where you will learn how to develop Java Microservice with Spring Boot and Spring Cloud Microservices on Google Cloud Platform
This course is created by Google Cloud Training, so you will be learning from the source. In his course, you will use Cloud Runtime Configuration and Spring Cloud Config to manage your application’s configuration.
You’ll send and receive messages with Cloud Pub/Sub and Spring Integration. You’ll also use Cloud SQL as a managed relational database for your Java applications, and learn how to migrate to Cloud Spanner, which is Google Cloud’s globally distributed strongly consistent database service.
It will also teach you about tracing and debugging your Spring applications with Stackdriver.
Here is the link to join this course — Building Scalable Java Microservices with Spring Boot and Spring Cloud
By the way, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
Apart from major cloud platforms like AWS, Azure, and GCP, there also exist specialized cloud platforms like Pivotal’s Cloud Foundry, also known as PFC. If you remember, Pivotal is the company behind Spring Framework and they are also pioneering cloud-native Java development.
By the way, PCF is now called Tanzi. PWS is no longer available. You would need to install PCF Dev on Your Local Machine to play with PCF. If you are looking for a course to learn how to deploy a Java or Spring Boot application, a RESTful API, Full Stack Applications, and Microservices to Pivotal Cloud Foundry then this is the perfect course for you. In this course, you will not only learn Pivotal Cloud Foundry ( PCF ) fundamentals but also things like how to deploy Spring Boot REST API to the Pivotal Cloud Foundry environment.
This course covers a number of PCF Services like Databases, Spring Cloud Services including Service Registry and Config Server which is important for Java developers.
You will not only learn to deploy REST APIS and Microservices but also Full Stack Applications are written in Java and Spring Boot.
Here is the link to join this Spring Boot occurs —Master Pivotal Cloud Foundry (PCF) with Spring Microservices
You will also learn how to Auto Scale applications based on load as well as deploy multiple instances behind a load balancer using Pivotal Cloud Foundry. In short, a good, hands-on course to learn about the Pivotal Cloud Foundry platform from Java and Spring boot developer’s perspective.
This is another advanced course on AWS for Java and Spring Boot developers. It contains over 8-hours of online training material that will teach you everything you need to know about AWS from a Java developer’s perspective. Created by In28Minutes, this course starts with explaining AWS fundamentals and then covers a number of AWS Services like ECS — Elastic Container Services, AWS Fargate, EC2 — Elastic Compute Cloud, S3, AWS CodePipeLine, AWS CodeBuild, IAM, CloudWatch, ELB, Target Groups, X-Ray, AWS Parameter Store, AWS App Mesh and Route 53. You will not only learn how to build Docker images for your Java Spring Boot Microservice Projects but also the basics of implementing Container Orchestration with ECS (Elastic Container Service) — Cluster, Task Definitions, Tasks, Containers, and Services.
You will also learn practical stuff like creating a continuous delivery pipeline with AWS Code Pipeline and how to debug problems with deploying containers using Service events and AWS CloudWatch logs etc. It also covers implementing Centralized Configuration Management for your Java Spring Boot Microservices with AWS Parameter Store.
Here is the link to join this course — Deploy Spring Boot Microservices to AWS
I highly recommend this course to experienced Java developers and DevOps engineers who are responsible for managing Java-based Microservices and Spring boot applications. Overall, an advanced AWS course for Java and Spring Boot developers. You will learn a lot of practical stuff for deployment, scaling, monitoring, troubleshooting, and tracing Java and Spring boot application on AWS.
This is another Spring Boot course by Ranga Karnam and in this course, he will teach you how to deploy Java Spring Boot REST API, Full Stack, Docker, and Web Apps with Azure App Service and Azure Web Apps into the Microsoft Azure platform. It’s not very different from the first course which talks about deploying Spring Boot application on AWS and if you have gone through that course then deploying on Azure will be much easier as both AWS and Azure.
Even though both AWS and Azure have different services for computing, storage, and network but concepts and processes remain the same. Things like deploying a containerized version are applicable for both AWS and Azure. The good thing is that you will learn how to deploy your Java Spring Boot application online for live Internet access, which is what many Java developers always ask. It gives you a lot of satisfaction to see your app live on the web and you can also share the links with your friends and colleagues.
Here is the link to join this course — Take Java Spring Boot Apps to Azure
If you have a startup idea then you can also use the techniques learned in this course to deploy a proof of concept app and share it with your clients and beta tester. Overall a practical and hands-on course to deploy Java and Spring Boot applications on the Microsoft Azure platform. That’s all about some of the best courses to learn how to deploy Spring Boot applications on various cloud platforms like AWS, Microsoft Azure, and Pivotal’s CloudFoundary.
The list not just include basic courses that teach you AWS and Azure basics along with Java deployment but also some advanced courses which will teach you how to deploy your spring boot on the internet and access it via the web and automatically scale up and down based upon load by using sophisticated services provided by AWS.   Other Java and Spring articles you may like to explore
Thanks for reading this article so far. If you like these best Spring Boot and Cloud Computing courses then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note. P. S. — If you are looking for a free course to learn Spring Boot and Cloud then you can also check out this Spring Boot and AWS S3 free course on Udemy. This course is created by Nelson Djaolo and it will teach you how to upload images and files to Amazon S3. The course is completely free and all you need is to create a free Udemy account to enroll in this course.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
142 
142 claps
142 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://blog.amis.com/aws-irsa-for-self-hosted-kubernetes-e045564494af?source=search_post---------78,"When Kubernetes comes to public cloud AWS, there is a issue that each K8S Pod needs specific permission to access AWS cloud resource, but AWS only can grant the permission under EC2 instance level, instead of K8S Pod level, hence, there are two workaround methods emerging in the community:
Finally, AWS made changes in the AWS identity APIs to recognize Kubernetes pods, so each K8S Pod can have specific IAM Role to acquire proper permission to access AWS cloud resource (This feature called IRSA). For the AWS Hosted K8S (A.K.A. EKS), official provided detail document and blog post to demonstrate how to achieve it, but there is still not many online resource to talk about how to enable it for Self-Hosted K8S in AWS, hence, I write this post to go through how to enable IRSA in Self-Hosted K8S.
There are many steps need to be done when enabling AWS IRSA in Self-Hosted K8S, So allow me to simply summarize what I will do in this post first.
In order to let the demonstration more smoothly in this post, I refer to the AWS official GitHub “amazon-eks-pod-identity-webhook” to create a simple repository, the major flow and steps are the same, but I made some customization to fulfill my requirement.
There is something needing to be install in your environment
2. Make sure the AWS cli is installed, and you have the AWS IAM and S3 permission
3. Of cause, this demo need the kubectl
4. Finally, let us start by git clone the demo repository!
The self-signed certificate will be generated and under the folder certs, there is also a K8S secret created for storing the certificate
After execute the gen-oidc-endpoint.sh, the key pair for OIDC service account is created under folder keys, and it create AWS S3 Bucket as a OIDC discovery endpoint, and use the endpoint to create OIDC provider in the AWS IAM, the key pair and service-account-issuer will be used by Kubernetes API Server later
In order to integrate API service with the OIDC provider, there are 4 flags need to be added when start it
Finally, we complete the integration between Kubernetes API Server and OIDC, the last step is to deploy Pod Identity Webhook
The Pod Identity Webhook is running in the K8S cluster now, and starting to monitoring the creation of Pod, once there is Pod created, mutating webhook will be triggered, and inject environment AWS_IAM_ROLE_ARN and AWS_WEB_IDENTITY_TOKEN_FILE into Pod
Here I use the official example to verify whether the IRSA works or not, there is a K8S job which will put one file into the S3 bucket, that approve the Pod can assume role to get the S3 write permission
Although enable IRSA in Self-Hosted K8S is complicated, but I feel it’s worthy, hoping the official and community can make the flow more and more easy and smooth, and due to the aws-iam-token mount in a stranger path /var/run/secrets/eks.amazonaws.com/serviceaccount, hence, don’t forget to use the latest AWS SDK to implement the application, then application can assume role successfully
Using breakthrough blockchain technology, Amis has created…
121 
1
121 claps
121 
1
Written by
原來只是一介草 QA，但開始研究自動化維運雲端服務後，便一頭栽進 DevOps 的世界裏，熱愛鑽研各種可以提升雲端服務品質及增進團隊開發效率的開源技術
Using breakthrough blockchain technology, AMIS (https//am.is) has created a standardized platform to let business create information exchange systems and make transaction data open and shareable to improve the quality of life for everyone.
Written by
原來只是一介草 QA，但開始研究自動化維運雲端服務後，便一頭栽進 DevOps 的世界裏，熱愛鑽研各種可以提升雲端服務品質及增進團隊開發效率的開源技術
Using breakthrough blockchain technology, AMIS (https//am.is) has created a standardized platform to let business create information exchange systems and make transaction data open and shareable to improve the quality of life for everyone.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.devgenius.io/scalability-and-cost-analysis-for-cloud-based-software-systems-part-1-472012435b26?source=search_post---------79,"The transition of major organizational IT systems from on-premise to public cloud platforms deployments seems inexorable. Here are some numbers on cloud adoption from a recent survey:
Individual market surveys and analyses should mostly (IMHO) be treated as fantasy, but when all indicators point in the same direction, some credence must be paid to their findings.
Taken on a global scale, these numbers are staggering. No wonder cloud providers make so much money — a forecast $266 billion in 2020 in fact. Organizations from startups to Government agencies to multinationals see clouds as digital transformation platforms and a foundational technology to improve business continuity.
Two of the great attractions of cloud platforms is their pay-as-you-go billing and ability to rapidly scale up (and down) to meet fluctuating workloads and data volumes. This ability to scale of course doesn’t come for free. Your applications need to be architected to leverage the scalable services provided by cloud platforms. And of course, costs and scale are indelibly connected — the more resources you utilize for extended periods, the larger your cloud bills will be at the end of the month.
And those monthly clouds bills can be big. Really big. Even worse, unexpectedly big! Cases of ‘sticker shock’ for significant cloud overspend are rife — in one survey 69% of respondents regularly overspent on their cloud budget by more than 25%. One well-known case spent $500K on an Azure task before it was noticed. Reasons attributed to overspending are many, including the lack of deployment of auto-scaling solutions, poor long-term capacity planning, and inadequate exploitation of cloud architectures leading to sub-optimal system footprints.
As I’ve discussed in previous articles, scaling a system on a cloud platform involves increased costs. It’s inevitable. However exactly ‘how much’ is something you as an architect can control through a range of architectural decisions.
The term ‘software architecture’ has far too many definitions and hence tends to be used much more broadly than it was originally intended. In general, a good rule of thumb is that design decisions that are cemented into the system, and hence difficult (and expensive) to change, are architectural. These design decisions have ramifications on both the way a system is constructed, and its non-functional characteristics once deployed. Consequently, design decisions that affect performance and scalability are, to use Michael Nygaard’s terminology, architecturally significant.
On a cloud platform, architects are confronted with a myriad of architectural decisions. These are both broad, in terms of the overall architectural pattern or style the system adopts — e.g. microservices, n-tier, event driven — and narrow, specific to individual components and cloud services that the system is built upon. In this sense, architectural decisions pervade all aspects of the system design and deployment on the cloud. And the consequences of these decisions are highly apparent when you receive your monthly cloud bill.
As I describe in this article, cost-effective scaling requires:
On a cloud platform, precisely how you achieve the above depends on the details of the cloud services you utilize. For example, if you deploy your web services on an Infrastructure-as-a-Service (IaaS) platform, you will spin up new virtual machines (VMs) to increase capacity using the cloud-provided load balancing service. Your costs are essentially proportional to the type of VMs you choose and the duration they are deployed for.
Alternatively, if you choose a platform-as-a-Service (PaaS) deployment, your friendly cloud-based serverless engine such as AWS Lambda or Google App Engine (GAE) will basically manage autoscaling (up and down) for you. These platforms provide various policies that you can influence through configuration settings to attempt to optimize scaling.
In general, your costs are proportional to the type of server instance you choose to deploy, the number of requests and processing duration for each request, and/or how long each server instance remains resident and in use on the serverless infrastructure.
Every platform is different.
You also have other decisions to make. If you deploy on VMs, you can basically select any server framework technology and associated programming languages that you want. Python with Flask. Node.js and Express. Spring with Java. The technology world is your veritable oyster.
If you choose serverless, the platform will constrain your language and framework options, or provide its own API you need to utilize. As an example, if you want to deploy a Java/Spring server, the AWS Lambda framework provides some glue to invoke Spring from a lambda function. GCP can deploy Java servlets and Spring Boot easily, but Spring or Jersey — it’s not so clear? If PHP is your gig, then GCP is probably the go as it’s not natively supported on Lambda.
Similar architectural decisions need to be made at all stages of a cloud deployment. For example, in the data tier, do you deploy and manage your own database servers on VMs, or use a cloud-managed database service. If the former, what configuration VMs do you choose, and how are you going to scale (up or out) your database as request load grows? If the latter, how do you design a data model that balances read and write performance and cost, as well as minimizes storage costs? And there are often other limits to design around. For example, Google Firestore limits concurrent writes to 10K per second.
Welcome to the world of cloud deployments. Every cloud service has many architecturally significant decisions that need to be addressed. Great cloud services have a low barrier to entry, and enough facilities and capabilities to enable you to maximize your spending with ease. A little cynical I know, but there’s a grain of truth in there.
While it’s pretty obvious that architecturally significant cloud platform decisions affect costs, especially as services scale, the magnitude of this effect is difficult to gauge in the abstract. Will experimenting with platforms and deployment options save say 20% of base costs, or just an insignificant amount that doesn’t justify the effort of expensive engineers and architects? Without some controlled experiments to investigate the magnitude of these effects, we’re all just guessing.
Luckily, thanks to some funding from Google Cloud Platform Research, we were able to design such a set of experiments. With a vast range of possible services and options to explore, and limited money and time, we targeted just two design decisions that face architects using a serverless platform — in our case Google App Engine (GAE) — and designed experiments around them. The questions we wanted to answer were as follows:
In the remainder of this article, we describe the results of our investigations into the programming language diversity. A followup will describe the results from the platform configuration experiments.
Briefly, the experimental approach was as follows:
This approach follows the well-understood microbenchmarks approach. Microbenchmarks are effective at differentiating platforms and component level behaviors in terms of performance and scalability. Why?
On cloud platforms, much of the execution time in handling an HTTP service request is consumed by platform and framework code. In effect, our service (business) logic typically executes for a small fraction of the overall request time. In a microbenchmark approach, this fraction is probably of the order of 1% or so — in real applications it's often not much more. The rest of the time is consumed by platform and language runtime code, which is relatively constant across different application behaviors.
In our first experiment, described in this article, we executed the same client request loads against servers performing the same business logic implemented in Java, Go, Python and Node.js. With an identical data model and the same database API calls implemented in each version, the only variable is the programming language runtime and how this is managed by GAE as the request load scales. This means the results should be broadly representative of the performance differences any application will see in a similar usage/platform scenario.
We ran a series of tests for each server running on the US-West2 data center. We varied the peak client request load from 128 to 1024 maximum concurrent requests during the peak test phase. Figure 1 shows an example test run with 1024 client threads, with warmup and cooldown phases to enable the test to exercise GAE scale up and scale down capabilities.
The latency for all requests is measured and logged on the client. Post-test processing calculates various latency statistics, along with throughput in terms of requests per second processed from the start of the test to the end.
We also use the GCP console to get additional test metrics, including costs, which we measured by ‘instance hours’ in a GAE application. On US-West2, an instance hour costs 6 cents for the default B1 instance that we utilized.
Replicating test behavior reliably on a cloud platform is a known tricky exercise, as there are so many potential variables involved and the environment is dynamic. This potentially leads to results variability. Luckily studies have shown this variability is low in practice. Still, to minimize any effects from variability, we put experimental controls in place.
Specifically:
TL;DR — we did the best we could with time and budget.
We ran identical test loads comprising 128, 256, 512, and 1024 concurrent clients against each server implementation. The results in terms of throughput are summarized in Figure 2. For each test, we show the throughput for the complete test, including warmup and cooldown phases, in the left column and the throughput for the peak phase alone in the right column. Probably the biggest takeaway from these charts is that each language implementation scales nicely as the load grows. GAE autoscaling policies are doing their job rather well.
The absolute throughput for each language varies somewhat — note the different ranges on the y-axes. To make the results more directly comparable, Figure 3 compares the throughput with the same 1024 client load for each language. The Go implementation clearly provides the highest throughput, peaking at around 13K requests/second, while Java and Node.js peak around 10K requests/second and Python around 9K. Put another way, the Python server delivers about 70% of the throughput of the Go server at peak load, and the Java/Node.js servers deliver about 77%.
These results show we have appreciable peak throughput differences. In reality, Python is delivering strong performance and there’s no apparent barrier for this not being able to scale further. That’s a good thing if you are a Python shop 😉
So next, let’s examine the costs, as depicted in Figure 4. This shows how many instance hours the GAE console reports for executing each test with 1024 clients. Instances are the language runtimes that GAE uses to execute and automatically scale an application. At any given time, an application can be running on one or more instances depending on load, with GAE load-balancing requests across the instances. You are billed in GAE for each instance hour, hence it basically represents the cost to serve a workload.
Figure 4 reveals stark differences. The Go implementation requires 1/3rd of the instances that Python requires, and delivers considerably higher throughput. Java and Node.js fall in-between, but as these two deliver similar performance, Java is somewhat more cost-effective.
Finally, let’s examine a direct comparison that combines the mean throughput for the overall test with the cost for each test run. Figure 5 shows the throughput per instance hour, calculated by dividing the mean throughput for the complete test by the number of instance hours consumed to run the test. This is effectively the price-performance ratio.
Basically, Go is delivering around 4x more ‘bang for your buck’ than Python, 3x more than Node.js, and 2x more than Java. Or put another way, the Go server would deliver the same performance as the Python server at around ¼ of the cost.
These results seem somewhat important. For any application, your mileage may vary depending on your workload, business logic, data model, and so on. But there’s good reason to believe these results have broad relevance. Why?
GAE automatic scaling creates runtime instances based on a number of factors, including request arrival rate and response latencies. As the number of request arrivals increases, new instances are spawned to process these requests and keep latencies low. There are a number of autoscaling parameters that influence how this behaves. In the tests reported above, default settings were used for all tests for all languages.
The GCP console enables you to monitor various metrics during a test. We monitored two of these closely:
We noticed considerable variability between languages in these two metrics. For example, the peak number of instances deployed for Python in a 1024 client test was of the order of 550. For the same test, the peak instances for the Go server was of the order of 100. The peak instances for Node.js were similar to Python, with Java about 30% lower.
The load time for new instances was also highly variable. Go instances were consistently loading in a second or less, whereas the Java average load time was around 4 seconds and the p95 about 11 seconds. Instance load time is an influential factor on scaling up performance and is affected by several factors. There’s a good description of this complexity here. A see used default autoscaling settings, we made no attempt to avoid the ‘cold start’ problem by using resident instances. This would probably help with Java performance at least. Of course, resident instances cost you money.
These metrics give us some insights into the measured cost, performance, and scalability variability we have observed in our tests. It’d be interesting to explore these issues more deeply to see if there are methodical ways to reduce the instance numbers and load time values for Python, Java and Node.js. But there are only so many hours in a day. And dollars in research credits available.
One of the major attractions of clouds is the ease of scaling a system. You literally get scalability at your fingertips if you architect your system with a view for growth. This all of course does not come for free. As your workload grows, so does your bill at the end of the month. Sometimes by a lot more than your clients or manager would like.
This article has explored how scalability and cost differ with programming language on Google App Engine. The results strongly indicate the effect of programming language choice on performance and cost. GAE does a great job of scaling your services regardless of language. But the language you choose can lead to a four times cost reduction for a given workload in the best case. Under heavy workloads, that’s a lot of money.
Of course, these results are contextual and specific to how the GAE serverless platform handles processing instances. The benchmarks were run in Summer 2019 and so language runtimes and no doubt GAE will have changed since then. How similar implementations with these workloads would perform on AWS Lambda, or Azure Serverless, we don’t know. But we would love to investigate. I’d put money on language choice being a major cost determinant at scale on any platform.
I’d like to acknowledge the efforts of Yixing Shao, Yijin Lee, Yiwen Zhang, Cong Li and Stephen Martin from Northeastern University’s Align MSCS Program in Seattle. They are excellent examples of the high-quality students we have in the MSCS program across all Northeastern locations.
Coding, Tutorials, News, UX, UI and much more related to development
38 
38 claps
38 
Written by
Software architect, author, professor, teacher. Fascinated by the principles and technologies for building scalable distributed systems.
Coding, Tutorials, News, UX, UI and much more related to development
Written by
Software architect, author, professor, teacher. Fascinated by the principles and technologies for building scalable distributed systems.
Coding, Tutorials, News, UX, UI and much more related to development
"
https://m.subbu.org/there-is-no-pendulum-5ea4d7c92abc?source=search_post---------80,"As public cloud spend goes up, will the pendulum swing back to enterprise data centers? Sorry, but there is no pendulum to swing back.
A pendulum exists only when you treat a public cloud as a compute center and not as a platform.
The top three clouds offer 60–100 services each today. These are well integrated to help get a lot of work done. Each of these have ecosystem of services built around them, and are growing strong as platforms. The fact that each of these platforms run in compute centers is just a necessary detail. Once you start building businesses on top of these platforms and are generating value, there is no going back.
Here is an analogy. Smart phones are certainly more expensive than feature phones. But feature phones are not coming back. Today’s smart phones are build around platforms.
A blog on tech and leadership
11 
2
11 claps
11 
2
Written by
See https://www.Subbu.org
A blog on tech and leadership
Written by
See https://www.Subbu.org
A blog on tech and leadership
"
https://medium.com/@opsview/a-devops-story-how-to-monitor-linux-hosts-across-aws-azure-and-gcp-32960c05d105?source=search_post---------81,"Sign in
There are currently no responses for this story.
Be the first to respond.
Opsview
Jun 30, 2017·3 min read
Whether you prefer Ubuntu, Centos or RHEL, monitoring the performance of your Linux hosts in the public cloud is not as straightforward as it might seem. Each cloud provider has its own monitoring system that varies considerably, and many leave out the more detailed kernel-level statistics that you need. Additionally, each cloud provider offers a different API layer and available metrics, making homogenous data access problematic at best. I find that host-level, cloud agnostic monitoring solution is essential to ensure that I’m getting the most out of my virtual machines and to meet service level agreements.
Amazon’s CloudWatch will tell you the CPU load on your machines and alert you when one fails a status check, but it provides no insight as to RAM usage, individual process resource consumption, or disk space usage. The performance reporting tools with Microsoft Azure are great for their Windows instances, but the metrics available for Linux hosts are considerably less detailed. A cloud agnostic monitoring solution like Opsview picks up where these solutions end, giving you the more detailed process and kernel level information that you need to ensure you’re using your servers efficiently.
If a server is being overwhelmed with traffic, the simple solution is to spin up another copy of it behind the load balancer. In a pinch, it’s a good solution at 2 AM, and, like most DevOps specialists, I’ve been in this situation before. Multiple sites were hot-linking a dynamically-created graphic on a customer’s site, costing them tens of thousands of dollars in extra bandwidth and server costs. By analyzing historical network traffic patterns, web server requests, and process tracing, I was able to pinpoint the cost. You can’t conduct this kind of analysis with the tools that are provided with cloud solutions, and performing these by hand on multiple web servers is tedious at best.
Adding more capacity in the short term seems like a decent solution, but spending money on extra instances and unneeded hardware can quickly get expensive. Wouldn’t it be nice to know which processes are consuming the most CPU and RAM, or what network hosts are making the most connections? Blocking abusive hosts and fixing misbehaving scripts is a lot less costly than blindly throwing hardware at the problem.
Many organizations use a mix of cloud and on-premises IT infrastructure, sometimes known as ‘Hybrid Cloud’. You could create your own monitoring system with thousands of hours of custom programming, both up front and throughout the years maintenance as vendor APIs change, but this solution is expensive and pulls valuable development staff away from revenue-generating operations. In smaller start-up shops the development hours are often not available for custom solutions, yet your customers demand constant availability and performance.
This is why a cloud-agnostic, on-premise monitoring solution like Opsview is an ideal solution. You get one standard interface giving deep, unified insight into a wide variety of data sets that you control without having to constantly monitor and adapt to vendor API changelogs. You can take advantage of the strengths of each cloud provider, knowing that your monitoring solution will adapt to meet your technical and business needs.
Get started monitoring your Linux hosts using our free trial.
Written by Robert W. Oliver II
Specialists in cloud and infrastructure monitoring.
See all (1,125)
18 
18 claps
18 
Specialists in cloud and infrastructure monitoring.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@etherealmind/its-hard-to-build-cloud-netflix-4528a1c7a343?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
EtherealMind
Feb 12, 2016·3 min read
Over at the Netflix blog, they were proudly announcing that they have fully migrated to AWS as a public cloud platform.
Given the obvious benefits of the cloud, why did it take us a full seven years to complete the migration? The truth is, moving to the cloud was a lot of hard work, and we had to make a number of difficult choices along the way.
For Enterprise IT, people should pause and think very carefully about that.
Arguably, the easiest way to move to the cloud is to forklift all of the systems, unchanged, out of the data center and drop them in AWS. But in doing so, you end up moving all the problems and limitations of the data center along with it. Instead, we chose the cloud-native approach, rebuilding virtually all of our technology and fundamentally changing the way we operate the company.
This paragraph highlights just how complex and difficult it is to consume public cloud. Netflix took the correct approach of adopting “cloud-native” and re-architecting all their apps.
Re-architecting apps requires vast amounts of money, time and resources. The article highlights just how much time, seven years, it takes.
Architecturally, we migrated from a monolithic app to hundreds of micro-services, and denormalized and our data model, using NoSQL databases. Budget approvals, centralized release coordination and multi-week hardware provisioning cycles made way to continuous delivery, engineering teams making independent decisions using self service tools in a loosely coupled DevOps environment, helping accelerate innovation. Many new systems had to be built, and new skills learned. It took time and effort to transform Netflix into a cloud-native company, but it put us in a much better position to continue to grow and become a global TV network.
Moving to the public cloud isn’t easy.
Netflix is a digital-native, technology-only company that has deep experience of operating on AWS. It is high profile in speaking at conferences, releases open-source software on a regular basis and blogs about its technology so that it can attract and retain high quality technical staff.
And its still took them seven years to migrate to the public cloud.
By comparison to this, private cloud for Enterprise IT is a heck of a lot easier. You can sustain your legacy applications while you commence the decades long process of updating your applications, overhauling processes and adapting. And don’t forget that adding and training staff is going to be critical to your success.
Also, Netflix didn’t outsource their transition to the cloud because everything has to be integrated in a DevOps enviroment to consistently apply and reuse the cloud platform. You have to do it yourself.
Originally published at etherealmind.com on February 12, 2016.
Human Infrastructure for Networks. Blogs at EtherealMind.com. Podcasts at PacketPushers.net. Personal at GregFerro.com
See all (65)
12 
1
12 claps
12 
1
Human Infrastructure for Networks. Blogs at EtherealMind.com. Podcasts at PacketPushers.net. Personal at GregFerro.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-3beb1ab820a?source=search_post---------83,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Feb 12, 2020·5 min read
PART FIVE — Implementing scalable workflows on the public cloud with Terra
In part one, I presented the general rationale for building quick start examples (or ‘Hello World’) for bioinformatics tools.
In part two, I reviewed one such example for the CSIRO Bioinformatics VariantSpark library, runnable via smart templates in the AWS Marketplace.
In part three, I covered how to convert a locally runnable example to a reusable cloud example, by working with Google Cloud Platform custom Virtual Machine images…
"
https://medium.com/@nutanix/5-reasons-why-you-should-question-your-old-aws-cloud-security-practices-d70c3399f66d?source=search_post---------84,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Apr 21, 2017·4 min read
Agile deployments and scalability seem to be the most dominant trend in public cloud, today; especially on AWS. While you scale your business on cloud, AWS too keeps scaling its services as well as upgrading its technology from time to time, to keep up with the technology disruptions happening across the globe. To that end, your cloud engineers have to constantly adapt to architectural changes as and when updates are announced. While all these architectural changes are made, AWS Cloud Security best practices and audits need to be relooked too from time to time.
As a CISO, have you ever questioned your old practices and relooked at them whether it’s relevant in the present day.
Here are few excerpts from our AWS Cloud Security Think Tank: A collation of deliberations we had recently at Botmetric HQ with our security experts on why anyone on cloud should question their old AWS cloud security best practices.
“Securing the server end is just one part of enterprise cloud security. If there is a leakage at the endpoints, the net result is adverse impact on your cloud infrastructure. Newer approaches to assert the legitimacy of the endpoint is more important than ever.” — Upaang Saxena, Botmetric LLC.
As most cloud apps provide APIs, the client authentication mechanisms have to be redesigned. Moreover, as the endpoints are now mobile devices, IOT devices, and laptops that might be anywhere in the world, increasingly the endpoint security is moving away from perimeter based security model giving way to Identity based endpoint security model. Hence, newer approaches to assert the legitimacy of the endpoint is more important than ever.
“Use managed policies, because with managed policies it easier to manage access across users. ” — Jaiprakash Dave, Minjar Cloud Solutions
Earlier, only Identity-based (IAM) inline policies were available. Managed policies came later. So not all old AWS cloud best practices that existed during inline policies era might hold good in the present day. So, it is recommended to use managed policies that is available now. With managed policies you can manage permissions from a central place rather than having it attached directly to users. It also enables to properly categorize policies and reuse them. Updating permissions also becomes easier when a single managed policy is attached to multiple users. Plus, in managed policies you can add up to 10 managed policies to a user, role, or group. The size of each managed policy, however, cannot exceed 5,120 characters.
“We encourage our clients to make multiple account switch roles for access controls as per their security needs.” — Anoop Khandelwal, Botmetric LLC.
Earlier, it was not recommended to switch roles for access controls while using VPC. However, now it is recommended to make multiple account switch roles for access controls as per their security needs. Plus, earlier VPCs came with de facto defaults, which was inherently less than ideal from a security perspective. Now, Amazon VPC provides features that you can use to increase and monitor the security for your Virtual Private Cloud (VPC).
DDOS attacks through compromised IOT devices such as Mirai Bot attacks caught the security professionals by surprise. The possibility of the scale of the attack was not predicted by any security analyst. Such new attack vectors will be designed by hackers to penetrate popular and highly sensitive websites and it would be difficult to anticipate all potential attack vectors. So cloud professionals have to revisit their architecture and be ready with better contingency measures in case of such unanticipated attack vectors.
“You (cloud security engineer) need to relook into your architecture now and then and come up with better contingency measures for new age attack vectors like massively distributed denial of service(DDOS). ” — Abhinay Dronavally, Botmetric LLC.
Today, most enterprise applications consume data from external web services and also expose their data. The authentication mechanisms for the APIs cannot be the same as human user authentication, like earlier days. APIs must fit into machine to machine interactions. Focus more on integration API security mechanisms with specialized API security solution.
“As data breaches can happen through API, integration of API security mechanisms are a must.” — Shivanarayana Rayapati, Minjar Cloud Solutions.
As the sophistication of the attacks keep increasing, the security solutions too would have to improve their detection methods. Today’s security solutions leverage Artificial Intelligence (AI) algorithms like Random Forest Classification, Deep Learning techniques, etc. to study, organize, and identify the underlying access patterns of various users. A well thought-through approach is pivotal in securing your AWS cloud. For that matter, any cloud.
What are your thoughts? Share with us on Twitter, facebook, LinkedIn. We would love to understand.
Meanwhile, if you need to understand how Botmetric is helping customers stay secure and 100% compliant on cloud with over 200+ cloud health checks per day, signup for a 14-day trial to run an audit and view your cloud security posture. Sign Up Here.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
6 
6 
6 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@sraney/as-amazon-wrapped-up-its-4th-annual-aws-re-invent-conference-last-week-it-became-clear-to-me-that-92415ef77abd?source=search_post---------85,"Sign in
There are currently no responses for this story.
Be the first to respond.
Scott Raney
Nov 4, 2015·4 min read
As Amazon wrapped up its 4th annual AWS re:Invent Conference last week, it became clear to me that the company is no longer just a developer platform; but rather a full-fledged enterprise company serving the needs of small businesses to Fortune 500 companies.
AWS is dominating the public cloud market, and it is increasingly clear that this market represents the largest disruption of enterprise computing since the introduction of the personal computer.
To wit:
AWS is on a $10B run rate growing at 81% year over year. They have thriving businesses in compute, storage and databases with emerging businesses in a range of areas across the enterprise software spectrum.
According to Baird, AWS will represent just 5% of data center spend this year and less than 1% of overall enterprise IT spend. In other words, there is still significant room for growth as more enterprises embrace cloud technologies. AWS’s scale advantages coupled with an impressive track record of innovation and huge aspirations have the company extremely well positioned for the next decade.
As a the conference wrapped up, rumors of Dell’s interest in buying EMC and their controlling interest in VMware hit the newswire. EMC, the stalwart of on-premise enterprise storage, and VMware, the creator of virtualization and champion of server and data center consolidation — two of the most successful companies in data center infrastructure over the past decade — are on the block with their best days clearly behind them.
In contrast, AWS is ascending and dictating the corporate strategies of nearly all major enterprise IT vendors. While Microsoft, IBM, et al have great assets to challenge in this market, it is clear that Amazon is driving the agenda for enterprise computing and is the most important enterprise IT company in the world today.
Redpoint Ventures
6 
6 
6 
Redpoint Ventures
"
https://medium.com/@nutanix/top-11-hard-won-lessons-learned-about-aws-auto-scaling-5bfe56da755f?source=search_post---------86,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Jan 17, 2017·5 min read
Auto scaling, as we know today, is one of the most powerful tools leveraging the elasticity feature of public cloud — Amazon Web Services (AWS). Its ability to improve the availability of an application or a service, while still keeping cloud infrastructure costs under check, has been applauded by many enterprises across verticals, be it fleet management services or NASA’s research base.
However, at times, AWS Auto Scaling can be a double-edged sword. For the reason that, it introduces higher level complexity in the technology architecture and daily operations management. Without the proper configuration and testing, it might do more harm than good. Even so, all these challenges can be nullified with few precautions. To this end, we’ve collated few lessons we learned over a period — to help you make the most of Auto Scaling capabilities on AWS.
There is a myth among many AWS users that AWS Auto Scaling is hard to use and not so useful with stateful applications. However, the fact is that it is not hard to use. You can get started in minutes, with few precautionary measures like using sticky sessions, keeping provisioning time to minimum, etc. Plus, AWS Auto Scaling helps monitor the instances and heals them if they become unhealthy.
Here’s how: Once the Auto Scaling is activated, it automatically creates an Auto Scaling Group, and provisions the instances accordingly behind the load balancer. This maintains the performance of the application. In addition, Auto Scaling’s Rebalance feature ensures that your capacity is automatically distributed among several availability zone to maximize the resilience of the application. So, whether your application is stateful or dynamic, AWS Auto Scaling helps maintain its performance irrespective of compute capacity demands.
Identify the metrics for the constraining resources, like CPU utilization, memory utilization, of an application. By doing so, it will help track how the resources are impacting the performance of the application. And the result of this analysis will provide the threshold values that will help scale up and scale down the resources perfectly.
The best way forward is to configure Auto Scaling with AWS CloudWatch so that you can fetch these metrics, as and when needed. Using CloudWatch, you can track the metrics in real-time. CloudWatch can be configured to launch the provisioning of an auto scaling group based on the state of a particular metric.
The resource configurations have to be specified in Auto Scaling groups feature provided by AWS. Auto scaling groups would also include rules defining circumstances under which the resources will be launched dynamically. AWS allows assigning the of autoscale groups to the Elastic Load Balancers (ELBs) so that the requests coming to the load balancers are routed to the newly deployed resources whenever they are commissioned.
A practical auto-scaling policy must include multiple metrics, instead of just one allowed by CloudWatch. The best approach to circumvent this restriction is to code a custom metric as a Boolean function using Python and the Boto framework. You can use application specific metric as well along with default metrics like memory utilization or CPU, network, etc.
As an alternative to writing complex code for the custom metric, you can also architect your applications to take requests from a Simple Queuing Service and enable CloudWatch to monitor the length of the queues to decide the scale of the computing environment based on the amount of items in the queue at a given time.
To reduce the time taken to provision instances that contain many custom software (not included in the standard AMIs), you can create a custom AMI that contains the software components and libraries required to create the server instance.
Along with AWS EC2, other resources such as AWS DynamoDB, can also be scaled up and scaled down using Auto Scaling. However, the implementation of the policies are different. Since storage is the second most important service other than computing service, efforts to optimize storage will yield good performance as well as cost benefits.
Setting up thresholds as described above is reactive. Hence, you can leverage time-series prediction analytics to identify patterns within the traffic logs and ensure that the resources are scaled up at pre-defined time, before events take place.
Auto scaling policies must be defined based on the capacity needs as per Availability Zone (AZ) to save on cost spikes. Because pricing of the resources are based on different regions that encompass these AZs. This is critical especially for Auto Scaling groups configured to leverage multiple AZs along with a percent-based scaling policy.
By using Reactive Scaling policies on top of schedule scaling feature will give you the ability to really respond to the dynamic changing conditions in your application.
Conclusion:
Embrace an intelligent cloud management platform.
Here’s why: Despite configuring CloudWatch and other features of Auto Scaling, you cannot always get everything you need. Further automating various Auto Scaling features using key data-driven insights is the way forward. So, sign-up for an intelligent cloud management platform like Botmetric, which throws key insights to manage AWS Auto Scaling, provides detailed predictive analytics and helps you leapfrog your business towards digital disruption.
Also, do listen to Andre Dufour’s recent keynote* on AWS Auto Scaling during the recent 2016 re:Invent, where he reveals that Auto Scaling feature will be available to Amazon EMR (Elastic Map Reduce) service as well along with AWS ECS Container service, and Spot Fleet in regards to dynamic scheduling policies.
It is evident. Automation in every field is upon us. There will soon be a time when we will reach the NoOps state. If you have any questions in regards to AWS Auto Scaling or how you can minimize Ops work with scheduled Auto Scaling or anything about cloud management, just comment below or give us a shout out on Twitter, Facebook, or LinkedIn. We’re all ears!
Botmetric Cloud Geeks are here to help.
*References: https://www.youtube.com/watch?v=gCJkR8Bs31Q
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
22 
2
22 
22 
2
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-cloud-governance-models-c664a97e2489?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 4, 2020·4 min read
The deliverability of cloud governance models has improved as public cloud usage continues to grow and mature. These models allow large enterprises to tier and scale their AWS Accounts, Azure Subscriptions and Google Projects across hundreds and thousands of cloud users and services. When we first started talking to customers 5+ years ago, mostly AWS users at the time, they often had a single AWS account for their entire organization and required third-party tools to manage usage and costs by project, line of business or application owner. But now, the “Big 3” cloud providers offer an array of ways for even the largest Fortune 500 enterprises to set up, run and manage their use of the dizzying volume of cloud services.
The main way cloud providers allow cloud administrators to manage and grant access to their services is by leveraging Identity and Access Management (IAM) and providing options for roles and policies that govern both access and usage. IAM lets you grant granular access to specific AWS, Azure and/or Google Cloud resources and helps prevent access to other resources. IAM lets you adopt the security principle of least privilege, where you grant only necessary permissions to access specific resources like VM’s, Databases, Storage, Containers, etc.. With IAM, you manage access control by defining who (identity) has what access (role) for which resource.
In ParkMyCloud, we apply this with Teams and Roles. Admins can create Teams (equivalent to Projects, Applications, or Lines of Business) and can invite a Team Lead to manage that PMC Team, and they can in turn grant users access and set permissions for them, which can then by automated based on policies, usually by leveraging tags but you can use other metadata as well.
What if you want more flexibility with the cloud providers to both manage user access and to more tightly align your cloud services and usage to your organizational structure, projects and applications? Each of the major providers has designed ways for large enterprises to implement a hierarchical usage of cloud users and services that probably can look very similar to that enterprises organization chart. (If you can understand their jargon.)
We dug into AWS, Azure and Google and this is what we found:
Amazon Web Services (AWS)
Microsoft Azure
Google Cloud
Tips for implementing Cloud Governance Models:
The cloud providers have done a pretty good job of documenting their roles, policies and hierarchies and creating a graphical representation of their current hierarchical structures cloud governance models. Of course, none of them use the same terminology — I mean, why would you, too easy, right? (And why does Google rank a ‘Folder’ above a ‘Project’? )
With these options available to you, your cloud operations team can make sure to use this to your advantage when planning new resources, accounts, and use cases within your organization. Let us know your thoughts and if you use any of these models to improve your cloud usage.
Originally published at www.parkmycloud.com on September 1, 2020.
CEO of ParkMyCloud
15 
15 
15 
CEO of ParkMyCloud
"
https://medium.com/@cloud-opinion/rough-notes-on-cloud-and-enterprises-a79da06a332b?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
.Cloud Opinion
May 26, 2016·2 min read
Thoughts?
Parody + Tech commentary.
7 
3
7 
7 
3
Parody + Tech commentary.
"
https://saasholic.com/saas-sales-strategy-5052a49b8a?source=search_post---------89,"Software as a Service (SaaS for short) currently makes up the largest segment of the public cloud market. The SaaS market itself is worth $94.8 billion and is expected to grow quite a bit more in the coming years.
Are you looking to get into the SaaS game? Have you already started but need help maximizing your sales?
Either way, we’ve got some helpful tips for you. Read on for some advice that will help you create the perfect SaaS sales strategy for your business.
From marketing and sales and demos and follow-up calls, you have to be persistent if you want people to take an interest and invest in your software. Make sure that you have regular follow-ups included in your sales plan.
Remember, it often takes several connections with a potential customer before you close the deal. Don’t be discouraged if they turn you down the first time. Reach out again and see if you can change their mind.
It’s best to keep your free trials short (a few days to a week is great).
People tend to take shorter trials more seriously and will be more inclined to commit to trying out your software right away if they only have a limited amount of time to do so. 
When you’re demonstrating your software to someone, be sure to keep your presentation short as well.
People have short attention spans and full schedules these days. They don’t have time for a drawn-out demonstration that takes up their whole afternoon.
You’ll be more likely to close the deal if you keep things short and sweet.
A strong email marketing campaign will help you sell more programs and get more people interested in what you have to offer. Some easy ways to strengthen your campaign include:
These simple steps can make a big difference in the number of responses you get to your emails.
It’s anticipated that 73 percent of enterprises will run almost entirely off of SaaS by the year 2020.
Clearly, there’s a high demand for SaaS. In order to help meet that demand and help people save some money in the process, it’s a good idea to offer annual plans.
This allows you to receive larger amounts of money upfront, and it gives your buyers a chance to get your software at a lower price (while also committing to a year’s worth of service). 
As you can see, there are lots of things you can do to improve your SaaS sales strategy. Whether you’ve been in the business for a while or are just getting started, these tips will help you increase your sales and boost your bottom line.
Do you want to learn more about SaaS? If so, check out some of our other posts today. This one on measuring and optimizing performance is a great starting point.
Don’t forget to subscribe, too, so you can get the latest SaaS news delivered straight to your inbox.
© 2022 SaaSholic. All rights reserved.
"
https://medium.com/@jaychapel/even-if-youre-not-yet-multi-cloud-you-should-use-cloud-agnostic-tools-d95fc8cb7353?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 3, 2019·4 min read
There’s a simple fact for public cloud users today: you need to use cloud agnostic tools. Yes — even if you only use one public cloud. Why? This recommendation comes down to a few drivers that we see time and time again.
There is an enterprise IT trend to multi-cloud and hybrid cloud — such a prevalent trend that even if you are currently single-cloud, you should plan for the eventuality of using more than one cloud, as the multi-cloud future has arrived. Dave Bartoletti, VP and Principal Analyst at Forrester Research, who broke down multi-cloud and hybrid cloud by the numbers:
In addition, standardizing on cloud agnostic tools also can alleviate costs associated with policy design, deployment, and enforcement across different cloud environments. Management and monitoring using the same service platform greatly reduces the issue of mismatched security policies and uncertainty in enforcement. Cloud agnostic tools that also operate in the context of the data center — whether in a cloud, virtualized, container, or traditional infrastructure — are a boon for organizations who need to be agile and move quickly. Being able to reuse policies and services across the entire multi-cloud spectrum reduces friction in the deployment process and offers assurances in consistency of performance and security.
We talk to different size enterprises using the cloud on a daily basis, and always ask if they are using cloud native tools, or if they are using third party tools that are cloud agnostic. The answer — it’s a mix to be sure, often it’s a mix between cloud-native and third-party tools within the same enterprise.
What we hear is that managing the cloud infrastructure is quite a complex job, especially when you have different clouds, technologies, and a diverse and opinionated user community to support. So a common theme with many of the third-party tools we see used tend to include freemium models, a technology someone used at a previous company, tools recommended by the cloud services provider (CSP) themselves, and open-API-driven solutions that allow for maximum automation in their cloud operations. It also serves the tools vendors well if deploying the tool includes minimum effort — in other words, SaaS tools that do not require a bunch of services and integration work. Plug and play is a must.
For context, here at ParkMyCloud support AWS, Azure, Google and Alibaba clouds, and usually talk to DevOps and IT Ops folks responsible for their cloud infrastructure. And those folks are usually after cloud cost control and governance when speaking with us. So our conversations tend to focus on the tools they use and need for cloud infrastructure management like CI/CD, monitoring, cost control, cost visibility and optimization, and user governance. For user governance and internal communication, Single-sign On and ChatOps are must have.
So we decided to compile a list of the most common clouds and tools we run across here at ParkMyCloud, in order of popularity:
Our suggestion is to use cloud agnostic tools wherever possible. Our experience tells us that a majority of the enterprises lean this way anyways. The upfront cost in terms of license fee and/or set up could be more, but we think it comes down to (1) most people will end up hybrid/multi-cloud in the future, even if they aren’t now, and (2) cloud agnostic tools are more likely to meet your needs as a user, as the companies building those tools will stay laser-focused on supporting and improving said functionality across the big CSPs.
Originally published at www.parkmycloud.com on October 4, 2018.
CEO of ParkMyCloud
16 
16 claps
16 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/the-cloud-managed-services-market-is-growing-and-thats-good-for-msps-660bfe8cfd12?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 15, 2019·3 min read
Lately, we have been talking to quite a few providers of cloud managed services that play in both the private and public cloud spaces. These conversations have centered around how cloud management needs are evolving as enterprises’ hybrid and multi-cloud needs have accelerated.
Most refer to this market as cloud managed services (for once, no acronym associated), and many of these managed service providers (MSPs) also sell migration services to bring customers from private to public cloud, and cloud services between Amazon Web Services (AWS), Microsoft Azure, and Google Compute Platform (GCP). So these MSPs can help you move your applications to the cloud, sell you the cloud services you’re using, and manage and optimize your cloud services. It’s a rapidly growing market with a lot of M&A activity as MSPs race to provide differentiated cloud managed services that enable them to help enterprises get to market faster, better, and cheaper.
The global cloud managed services market size is expected to reach USD 82.51 billion by 2025, according to a study conducted by Grand View Research, Inc. Enterprises are focusing on their primary business operations, which results in higher cloud managed services adoption. Business services, security services, network services, data center services, and mobility services are major categories in the cloud managed services market. Implementation of these services will help enterprises reduce IT and operations costs and will also enhance productivity of those enterprises.
Taking a step back, I had a look at Wikipedia to make sure we were all aligned on what managed services provider are and cloud management is (cloud managed services):
Cloud managed services enable organizations to augment competencies that they lack, or to replace functions or processes that incurred huge recurring costs. These services optimize recurring in-house IT costs, transform IT systems and automate business processes allowing enterprises to achieve their business objectives.
The “net net” is that MSPs providing managed cloud services enable enterprises to adopt and manage their cloud services more efficiently.
In March 2018 Gartner published a Magic Quadrant for Public Cloud Infrastructure Managed Service Providers if you're interested to see who they rank as the best of the best in when implementing and operating solutions on AWS, Azure, and GCP (note this includes multi-cloud but not hybrid cloud). Several large SI’s are on the list like Accenture, Capgemini, and Deloitte, along with newer born in the cloud pure play MSPs like 2ndWatch, Cloudreach, and REANcloud.
What’s interesting to us about this list is the recent M&A activity we have seen with many of these companies, here’s a few we were able to remember over a beer (shout out to Crooked Run Brewery in Sterling, VA):
As you can see, there is a clear bias towards buying “born in the cloud”, public cloud focused MSPs, as that’s where the lack of enterprise expertise lies, and of course the hyper growth is occurring as companies migrate from private to public cloud. Many of these providers started off supporting just AWS, and now need to or have begun supporting Azure and Google as well to support The “big 3” cloud service providers in this new, and emerging multi-cloud world.
MSPs that want to get into the cloud managed services game need to realize the pains are different in the public cloud, and that their focus needs to be on helping enterprises with security and governance, managing cloud spending, the lack of resources/expertise, and the ability to manage multi-cloud.
Originally published at www.parkmycloud.com on September 13, 2018.
CEO of ParkMyCloud
8 
8 
8 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/the-multi-cloud-environment-in-2020-advantages-and-disadvantages-5a0ed2f2fec8?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 8, 2020·5 min read
Now more than ever, organizations have been implementing multi-cloud environments for their public cloud infrastructure.
We not only see this in our customers’ environments: a growing proportion use multiple cloud providers. Additionally, industry experts and analysts report the same. In early June, IDG released its 8th Cloud Computing Survey results where they broke down IT environments, multi-cloud and IT budgets by the numbers. This report also goes into both the upsides and downsides using multiple public clouds. Here’s what they found:
Other goals include:
Interestingly, within multi-cloud customers of ParkMyCloud, the majority are users of AWS and Google Cloud, or AWS and Azure; very few are users of Azure and Google Cloud. About 1% of customers have a presence in all three.
The study found that the likelihood of an organization using a multi-cloud environment depends on its size and industry. For instance, government, financial services and manufacturing organizations are less likely to stick to one cloud due to possible security concerns that come with using multiple clouds. IDG concluded that enterprises are more concerned with avoiding vendor lock-in while SMBs are more likely to make cost savings/optimization a priority (makes sense, the smaller the company, the more worried they are about finances).
Since multi-cloud has been a growing trend over the last few years, we thought it’d be interesting to take a look at why businesses are heading this direction with their infrastructure. More often than not, public cloud users and enterprises have adopted multi-cloud to meet their cloud computing needs. The following are a few advantages and typically the most common reasons users adopt multi-cloud.
While taking advantage of features and capabilities from different cloud providers can be a great way to get the most out of the benefits that cloud services can offer, if not used optimally, these strategies can also result in wasted time, money, and computing capacity. The reality is that these are sometimes only perceived advantages that never come to fruition.
As companies implement their multi-cloud environments, they are finding downsides. A staggering 94% of respondents — regardless of the number of clouds they use or size of their organization — find it hard to fully take advantage of their public cloud resources. The survey cited the biggest challenge is controlling cloud costs — users think they’ll be saving money but end up spending more. When organizations migrate to multi-cloud they think they will be cutting costs, but what they typically fail to account for is the growing cloud services and data as well as lack of visibility. For many organizations we talk to, multiple clouds are being used because different groups within the organization use different cloud providers, which makes for challenges in centralized control and management. Controlling these issues brings about another issue of increased costs due to the need of cloud management tools.
Some other challenges companies using multiple public clouds run into are:
Configuring and managing different CSPs requires deep expertise which makes it more of a pressing need to find employees that have the experience and capabilities to manage multiple clouds. This means that more staff are needed to manage multi-cloud environments confidentiality so it can be done in a way that is secure and highly available. The lack of skills and expertise for managing multiple clouds can become a major issue for organizations as their cloud environments won’t be managed efficiently. In order to try fix this issue, organizations are allocating a decent amount of their IT budget to cloud-specific roles with the hope that adding more specialization in this area can help improve efficiency.
The statistics on cloud computing show that companies not only use multiple clouds today, but they have plans to expand multi-cloud investments:
In a survey of nearly 551 IT people who are involved in the purchasing process for cloud computing, 55% of organizations currently use multiple public clouds.
Organizations using multiple cloud platforms say they will allocate more (35%) of their IT budget to cloud computing.
SMBs plan to include slightly more for cloud computing in their budgets (33%) compared to enterprises
As cloud costs remain a primary concern, especially for SMBs, it’s important organizations keep up with the latest cloud usage trends to manage spend and prevent waste. To keep costs in check for a multi-cloud, you can make things easier for your IT department and implement an optimization tool that can track usage and spend across different cloud providers.
For more insight on the rise of multi-cloud and hybrid cloud strategies, and to demonstrate the impact on cloud spend, check out the drain of wasted spend on IT budgets here.
Originally published at www.parkmycloud.com on July 2, 2020
CEO of ParkMyCloud
13 
13 
13 
CEO of ParkMyCloud
"
https://medium.com/aelfblockchain/microsoft-now-supports-a-one-click-deployment-of-aelf-first-interoperable-blockchain-is-8b1dafba6abd?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
On July 31, aelf’s Enterprise 0.7.0 beta version was officially made available on the world-leading public cloud computing platform, Microsoft Azure. Developers can now build, manage, and extend blockchain applications through aelf’s Enterprise 0.7.0 Beta version on Azure Marketplace. In addition, aelf provides supporting tutorials, smart contracts, and Dapp development examples such that users can quickly begin working on business logic and application development.
On top of the turn-key blockchain infrastructure available now both on Azure and Amazon Web Services (AWS), aelf also provides a library of sample codes and app development guides according to the individual needs. Aelf specializes in helping partner enterprises to establish the basics of their blockchain networks, deploy transaction nodes on cloud services such as Azure, and ultimately create decentralized applications. They have been assisting companies to unlock new business models through blockchain and accelerates the innovation of multi-institutional business connectivity.
Aelf aims to become the bedrock of global blockchain ecology and will do so by creating comprehensive blockchain packages for developers and enterprises to deploy blockchain based applications rapidly and securely. Aelf will continue to drive blockchain innovation and bring more unique business advantages to users on Microsoft Azure.
Create and manage aelf enterprise blockchain network:
Https://azuremarketplace.microsoft.com/en-us/marketplace/apps/aelf.aelf-enterprise
· aelf Telegram community: English, Türkçe, Español, 한국, 日本語, 中文，русский, العربية, Deutsch, Italiano, Français, हिन्दी, and Tiếng Việt,
· aelf Twitter
· aelf Facebook
· aelf YouTube
· aelf Instagram
· aelf Reddit
· aelf Medium (for the latest update and articles)
· aelf Github (complete aelf project codes)
For more information, visit aelf.io
Tomorrow is running on ælf.
131 
131 claps
131 
ælf, the next breakthrough in Blockchain.
Written by
ælf, the next breakthrough in Blockchain.
ælf, the next breakthrough in Blockchain.
"
https://medium.com/@nutanix/devsecops-the-next-wave-of-cloud-security-1196adc7c5f3?source=search_post---------94,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Dec 26, 2016·2 min read
The adoption of DevOps, agile and public cloud services among businesses worldwide is increasing by the day. These are seen as the major shift in enterprise IT, and as the next wave after Internet. Thanks to digital democratization, due to which businesses have to be nimble to remain competitive. That said, security threats and cybercrime continue to outsmart this business despite having cutting-edge security wall around them. To this end, DevSecOps was born to bridge the security gap into DevOps, just as DevOps bridged the development and operations divide.
Plugging in the right chord: Security into DevOps, on the cloud
Business leaders now understand that moving to the cloud is not just any tech adaptation, but it is more about speed of service delivery and dynamic scalability. One of the most significant paybacks of the DevOps has been better software quality delivered faster, even on the cloud.
Cloud technology dissolves enterprise perimeter, the key construct around which security solutions have been developed. Earlier,security concerns were holding back many businesses from jumping on to the cloud bandwagon. And when the idea of perimeter and boundary was once again threatened by new security requirements such as those warranted by Bring Your Own Device (BYOD) policies, the IT industry slowly started to embrace the cloud. Security professionals are now leveraging real-time analytics and have also adopted “Continuous Security” in clear parallel to the “Continuous Integration” and “Continuous deployment” approach of the DevOps movement.
Image Source: RSAConference, 2016, DevSecOps In Baby Steps
DevSecOps Tools: Filling in the Security Gap
Many enterprises have started to explore ways of making application quality and security testing more scripted, continuous, and automated. With DevSecOps, they are taking an automation approach for security tests throughout development, even on the cloud. They are even integrating security-feature design and implementation into the development lifecycle in ways that wasn’t possible before.
With DevSecOps on the cloud, security becomes an essential part of the development process itself instead of being an afterthought.
Read the full article in our blog post to learn more about DevSecOps - the next wave of cloud security, here.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
7 
7 
7 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://faun.pub/the-rise-of-the-enterprise-cloud-manager-d9a29bc863c7?source=search_post---------95,"There is a growing job function among companies using public cloud: the Enterprise Cloud Manager. We did a study on ParkMyCloud users which showed that a growing proportion of them have “cloud” or the name of their cloud provider such as “AWS” in their job title. This indicates a growing degree of specialization for individuals who manage cloud infrastructure as demonstrated by their cloud computing job titles. And, in some companies, there is a dedicated role for cloud management — such as an Enterprise Cloud Manager.
The world of cloud management is constantly changing and becoming increasingly complex even for the best cloud manager. Recently, the increased adoption of hybrid and multi-cloud environments by organizations to take advantage of best-of-breed solutions, make it more confusing, expensive, and even harder to control. If someone is not fully versed in this field, they may not always know how to handle problems related to governance, security, and cost control. It is important to dedicate resources in your organization to cloud management and related cloud job roles. This chart from Gartner gives us a look at all the things that are involved in cloud management so we can better understand how many parts need to come together for it to run smoothly.
Having a role in your organization that is dedicated to cloud management allows others, who are not specialized in that field, to focus on their jobs, while also centralizing responsibility. With the help of an Enterprise Cloud Manager, responsibilities are delegated appropriately to ensure cloud environments are handled according to best practices in governance, security, and cost control.
After all, just because you adopt public cloud infrastructure does not mean you have addressed any governance or cost issues — which seems rather obvious when you consider that there are sub-industries created around addressing these problems, but you’d be surprised how often eager adopters assume the technology will do the work and forget that cloud management is not a technological but a human behavior problem.
And someone has to be there to bring the motivational bagels to the “you really need to turn your instances off” meeting.
Cohesively, businesses with a presence in the cloud, regardless of their size, should also consider adopting the functionalities of a Cloud Center of Excellence (CCoE) — which, if the resources are available, can be like an entire department of Enterprise Cloud Managers. Essentially, a CCoE brings together cross-functional teams to manage cloud strategy, governance, and best practices, and serve as cloud leaders for the entire organization.
The role of an Enterprise Cloud Manager or cloud center of excellence (or cloud operations center or cloud enablement team, whatever you want to call it) is to oversee cloud operations. They know all the ins and outs of cloud management so they are able to create processes for resource provisioning and services. Their focus is on optimizing their infrastructure which will help streamline all their cloud operations, improve productivity, and optimize cloud costs.
Moreover, the Enterprise Cloud Manager can systematize the foundation that creates a CCoE with some key guiding principles like the ones outlined by AWS Cloud Center of Excellence here.
With the Enterprise Cloud Manager leadership, DevOps, CloudOps, Infrastructure, and Finance teams within the CCoE can ensure that the organization’s diverse set of business units are using a common set of best practices to spearhead their cloud efforts while keeping balanced working relationships, operational efficiency, and innovative thinking needed to achieve organizational goals.
It’s worth noting that while descriptive, the “Enterprise Cloud Manager” title isn’t necessarily something widely adopted. We’ve run across folks with titles in Cloud Manager, Cloud Operations Manager, Cloud Project Manager, Cloud Infrastructure Manager, Cloud Delivery Manager, etc.
If you’re on the job hunt, we have a few other ideas for cloud and AWS jobs for you to check out.
With so much going on in this space, it isn’t possible to expect just one person or a team to manage all of this by hand — you need automation tools. The great thing is that these tools deliver tangible results that make automation a key component for successful enterprise cloud operations and work for companies of any size. Primary users can be people dedicated to this full time, such as an Enterprise Cloud Manager, as well as people managing cloud infrastructure on top of other responsibilities.
Why are these tools important? They provide two main things: visibility and action to act on those recommendations. (That is, unless you’re willing to let go of the steering wheel and let the platform make the decisions — but most folks aren’t, yet.) Customers that were once managing resources manually are now saving time and money by implementing an automation tool. Take a look at the automation tools that are set up through your cloud vendor, as well as third-party tools that are available for cost optimization and beyond. Setting up these tools for automation will lessen the need for routine check-ins and maintenance while ensuring your infrastructure is optimized.
To put it simply, if you have more than a handful of cloud instances: yes. If you’re small, it may be part of someone’s job description. If you’re large, it may be a center of excellence.
But if you want your organization to be well informed and up to date, then it is important that you have the organizational roles in place to oversee your cloud operations — an Enterprise Cloud Manager, CCoE and automation tools.
Originally published at www.parkmycloud.com on February 27, 2020.
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬.
To join our community Slack team chat 🗣️ read our weekly Faun topics 🗞️, and connect with the community 📣 click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
13 
13 claps
13 
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/noteb-x/dear-developer-de65bb1ee767?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
Wiza Jalakasi
I would like to thank you for submitting your viewpoint into the noteb∅x, I can definitely relate to the challenges portrayed during your entrepreneurial growth cycles.
“A luta continua.”
The struggle is real, not just for the enterprise, nor the SMB, neither the casual startup — but for all the future thinkers, dreamers, and architects of this beatuiful continent.
Let me first let you in on what my vision is for cloud infrastructure and managed solutions in the EMEA region. Rather, I’d prefer it IMEA — as I feel many young professionals can learn bigconcepts from our supporting team in India.
Best. Mentorship. Ever.
Today, I was having trouble deploying Dynamics CRM, AX, NAV, and SL onto my 365 tenant. I loved the fact that I had the opportunity to connect with a support rockstar. I always enjoy this part: connecting with India. And surely with every escalation, I am always at peace — because I wholly trust their ‘frame of work’ — not just the ITIL foundation, but even more so the Microsoft Operations Framework.
ie. In the leagues of the Medium writer, Florian Hoeppner
noteb0x.com
I aspire that this new breed of designers will spark a massive local interest and intrigue with how and what the cloud can offer — irrespective the user’s realm or scope in whichever industry they are leading/pioneering. In five years time, I would like to see a hyper-scale public cloud infrastructure available in Africa — I would also like to see an ubiquitous design model emerge with one soul purpose:
(see RedPixie on The API Economy)
It really is quite mid-evil how competition still plays a pivotal role in the corporate banter — especially telecoms, and how they fight a Wit vs. Wit to an easily influenced audience of under-developed professionals LinkedIn to their network. I would like to converge the best practises and models of all the leading IaaS providers (as listed in the Gartner MQ report) and spark a new generation of young problem-solvers. Ultimately crafting and engineering a dynamic and fully scalable “Managed Solutions as a Service” core offer. This ideally is my forte in IT — How to manage the service, and promising continuity of the service and also, an alien idea, offering training with the end-users on how to easily tap into perceivably complex services such as Business Intelligence and Analytics — which as the paradigm states, was only privy to the C-Levels on the on top floor.
Power is shifting and as you are aware this is evidently a new era/error, and this surely must compel a new solution.
There are many obstacles and summits that I identify critical to my own personal development as a human — outside of my Office 365. I am captivated by Machine Learning and Bot services, even cognitive awareness and how this allows us to have more empathy over tough mediums such as email or VoIP, or even, the dreaded dial-in conferences.
Still, haven’t figured that one out (I thought I was safely ‘conferenced’ with my Lync solution — It seems what applies for me does not apply for my peers)
PS. I’m taking a liking to meet.jit.si
Ultimately I aspire to be a broad-spectrum solutions architect from the smaller scale independent inventors, to the nationwide service providers, to even the global corporations such as the likes of Microsoft or even, Alphabet Inc. Personally, I have my sights set on Jelly industries, the makers of a new kind of search engine. Truly a start-up of this breed and calibre is matched perfectly for the curious dynamic of a knowledge-starved youth.
Who, are in most cases, misled by their social media timelines.
Something has to shift.
…that’s the goal, for hyper-scale public infrastructure to empower a sky-driven vista of a tomorrow continuously starting-up the future and then — with enough ‘climate’ change — even capitalise the A in EMEA.
Aspiring Solutions Architect
A dynamic zero-to-cloud story: spiritually uncapped.
5 
5 claps
5 
A dynamic zero-to-cloud story: spiritually uncapped. Featuring indefinite articles sans authoritative capital letters, pitch-forked venture caps, or slackly ground startup hats. Working on: Baby’s First Bootstrap 💫
Written by
atypical nefelibata “cloud-walker” (lit sic.) liberal, pantheistic, and insecure. nubivagantly dreaming of a scintilla in abditory. engineer by design. ✌️
A dynamic zero-to-cloud story: spiritually uncapped. Featuring indefinite articles sans authoritative capital letters, pitch-forked venture caps, or slackly ground startup hats. Working on: Baby’s First Bootstrap 💫
"
https://medium.com/@jaychapel/microsofts-start-stop-vm-solution-vs-parkmycloud-5db83fad99a9?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 28, 2019·6 min read
Users looking to save money on public cloud may be in the market for a start/stop VM solution. While it sounds simple, there is huge savings potential available simply by stopping VMs, typically on a schedule. The basic idea is that non-production instances don’t need to run 24×7, so by turning VMs off when they’re not needed, you can save money.
If you use Microsoft Azure, perhaps you’ve seen the Start/Stop VM solution in the Azure Marketplace. You may want this tool if you want to configure Azure to start/stop VMs for the weekend or on weekday nights. It may also serve as a way to avoid creating a stop VM powershell.
Users of Azure have taken advantage of this option to start/stop VMs during off-hours, but have found that it is lacking some key functionality that they require for their business. Let’s take a look at what this Start/Stop tool offers and what it lacks, then compare it to ParkMyCloud’s comprehensive offering.
Let’s take a look at Azure’s start/stop VM solution. The crux of this solution is the use of a few Azure services, specifically Automation and Log Analytics to schedule the VMs and Azure Monitor emails to let you know when a system was shut down or started. Both scheduling and keeping track of said schedules are important.
As far as the backbone of Azure services, the use of native tools within Azure can be useful if you’re already baked into the Azure ecosystem, but can be prohibitive to exploring other cloud options. You may only use Azure at the moment, but having the flexibility to use other public clouds in the future is a strong reason to use cloud-agnostic tools today.
Next, this solution costs money, but it’s not very easy to estimate the cost (but does that surprise you?). The total cost is based on the underlying services (Automation, Log Analytics, and Azure Monitor), which means it could be very cheap or very expensive depending on what else you use and how often you’re scheduling resources.
The schedules themselves can be based on time, but only for a single start and stop time — which is not practical for typical applications. The page claims it can be based on utilization, but in the initial setup there is no place to configure that. It also needs to be set up for 4 hours before it can show you any log or monitoring information.
The interface for setting up schedules and automation is not very user-friendly. It requires creating automation scripts that are either for stopping or starting only, and only have one time attached. This is tedious, and the single-time configuration makes it difficult to maximize off time and therefore savings.
To create new schedules, you have to create new scripts, which makes the interface confusing for those who aren’t used to the Azure portal. At the end of the setup, you’ll have at least a dozen new objects in your Azure subscription, which only grows if you have any significant number of VMs.
Users have noted numerous complaints in the solution’s reviews:
Luckily, there’s an easier option.
So if the Start/Stop VM Solution from Microsoft can start and stop Azure VMs, what more do you need? Well, we at ParkMyCloud have heard from customers (ranging from day-1 startups to Fortune 100 companies) that there are features necessary for a cloud cost optimization tool if it is going to get widespread adoption.
That’s why we created ParkMyCloud: to provide simple, straightforward cost optimization that provides rapid ROI while being easy to use. You can use ParkMyCloud to save money through Azure start/stop VM schedules for non-production resources that are not needed evenings and weekends, as well as RightSizing overprovisioned resources.
Here are some of the features ParkMyCloud has that are missing from the Microsoft tool:
As you can tell, the Start/Stop VM solution from Microsoft can be useful for very specific cases, but most customers will find it lacking the features they really need to make cloud cost savings a priority. ParkMyCloud offers these features at a low cost, so try out the free trial now to see how quickly you can cut your Azure cloud bill.
Related Reading:
Originally published at www.parkmycloud.com on August 5, 2019.
CEO of ParkMyCloud
24 
24 
24 
CEO of ParkMyCloud
"
https://medium.com/@nutanix/61st-aws-price-reduction-ec2-reserved-instances-m4-prices-slashed-d84c075295d?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
May 7, 2017·2 min read
Amazon Web Services (AWS) never fails to amaze customers with its public cloud offerings. As of today AWS offers a plethora of services that cater to various business needs and workload types. With its revolutionary pay-as-you-go model, AWS is empowering continuous agility in businesses. The principle is quite simple and straight- “Pay less by using more.”
AWS has come back with the announcement of its 61st round of price cuts. You heard it right! EC2 RIs & M4 prices have been slashed by a fifth of their original price ( an icing on the cake for businesses running on EC2 RIs and M4 instances).
In the words of Jeff Barr in one of his recent blogs, “Our customers use multiple strategies to purchase and manage their Reserved Instances. Some prefer to make an upfront payment and earn a bigger discount; some prefer to pay nothing upfront and get a smaller (yet still substantial) discount. In the middle, others are happiest with a partial upfront payment and a discount that falls in between the two other options. In order to meet this wide range of preferences we are adding 3 Year No Upfront Standard Reserved Instances for most of the current generation instance types. We are also reducing prices for No Upfront Reserved Instances, Convertible Reserved Instances, and General Purpose M4 instances (both On-Demand and Reserved Instances). This is our 61st AWS Price Reduction.”
So what are the new changes in the price reductions of EC2 RIs and M4 instances?
In a nutshell, here are the 5 key things you need to know about the new price reduction of EC2 RIs and M4:
•No Upfront Option For 3 Year Standard RIs
•~17% Price Reductions For No Upfront Reserved Instances
•~21% Reduced Prices For Convertible Reserved Instances
The discounts depend on how much the customer would pay upfront (more upfront means more discount).
To understand each of these price reductions and multiple reservation options for both EC2 and M4 instances in detail, read the full article HERE.
Cost modelling, budget reduction and cost optimization are some of the top most considerations for businesses irrespective of size. Whether it is an enterprise with 100+ foot print or a small start-up with less than 10 employees, this price reduction is a great news.
If you are still struggling with saving costs on AWS EC2, let us know. Botmetric Cost & Governance will help you holistically reduce your AWS bill. Just drop in a line below in the comment section or give us a shout out at @BotmetricHQ. Meanwhile, you can also get started with a Botmetric 14-day free trial HERE, to explore Botmetric for unified AWS cloud management.
Happy Clouding!
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
4 
4 
4 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@jaychapel/wasted-cloud-spend-to-exceed-17-6-billion-in-2020-fueled-by-cloud-computing-growth-7c8f81d5c616?source=search_post---------99,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2020·4 min read
More than 90% of organizations will use public cloud services this year, fueled by record cloud computing growth. In fact, public cloud customers will spend more than $50 billion on Infrastructure as a Service (IaaS) from providers like AWS, Azure, and Google. While this growth is due in large part to wider adoption of public cloud services, much of it is also due to growth of infrastructure within existing customers’ accounts. Unfortunately, the growth in spending often exceeds the growth in business. That’s because a huge portion of what companies are spending on cloud is wasted.
Before we get to the waste, let’s look a little closer at that growth in the cloud market. Gartner recently predicted that cloud services spending will grow 17% in 2020, to reach $266.4 billion.
While Software as a Service (SaaS) makes up the largest market segment at $116 billion, the fastest growing portion of cloud spend will continue to be Infrastructure as a Service (IaaS), growing 24% year-over-year to reach $50 billion in 2020.
Typically, we find that about ⅔ of enterprise’s average public cloud bill is spent on compute, which means about $33.3 billion this year will be spent on compute resources.
Unfortunately, this portion of a cloud bill is particularly vulnerable to wasted spend.
As cloud computing growth continues and cloud users mature, you might hope that this $50 billion is being put to optimal use. While we do find that cloud customers are more aware of the potential for wasted spending than they were just a few years ago, this does not seem to be correlated with cost optimized infrastructure from the beginning — it’s simply not a default human behavior. We frequently run potential savings reports for companies interested in using ParkMyCloud, to find out whether or not they will benefit from using the product. Invariably, we find wasted spend in these customers’ accounts. For example, one healthcare IT provider was found to be wasting up to $5.24 million annually on their cloud spend, an average of more than $1,000 per resource per year.
Here’s where the total waste is coming from:
Idle resources are VMs and instances being paid for by the hour, minute, or second, that are not actually being used 24×7. Typically, these are non-production resources being used for development, staging, testing, and QA. Based on data collected from our users, about 44% of their compute spend is on non-production resources. Most non-production resources are only used during a 40-hour work week, and do not need to run 24/7. That means that for the other 128 hours of the week (76%), the resources sit idle, but are still paid for.
So, we find the following wasted spend from idle resources:
$33.3 billion in compute spend * 0.44 non-production * 0.76 of week idle = $11 billion wasted on idle cloud resources in 2020.
Another source of wasted cloud spend is overprovisioned infrastructure — that is, paying for resources are larger in capacity than needed. That means you’re paying for resource capacity you’re rarely, or never, using.
About 40% of instances are sized at least one size larger than needed for their workloads. Just by reducing an instance by one size, the cost is reduced by 50%. Downsizing by two sizes saves 75%.
The data we see in ParkMyCloud’s users’ infrastructure confirms this, and in the problem may be even larger. Infrastructure managed in our platform has an average CPU utilization of 4.9%. Of course, this could be skewed by the fact that resources managed in ParkMyCloud are more commonly for non-production resources. However, it still paints a picture of gross underutilization, ripe for rightsizing and optimization.
If we take a conservative estimate of 40% of resources oversized by just one size, we find the following:
$33 billion in compute spend * 0.4 oversized * 0.5 overspend per oversized resource = $6.6 billion wasted on oversized resources in 2020.
Between idle and overprovisioned resources alone, that’s $17.6 billion in cloud spend that will be completely wasted this year. And the potential is even higher. Other sources of waste include orphaned volumes, inefficient containerization, underutilized databases, instances running on legacy resource types, unused reserved instances, and more. Some of these result in significant one-off savings (such as deleting unattached volumes and old snapshots) whereas others can deliver regular monthly savings.
That’s a minimum of about $5 million wasted per day, every day this year, that could be reallocated toward other areas of the business.
It’s time to end wasted cloud spend. Join ParkMyCloud in taking a stand against it today.
Originally published at www.parkmycloud.com on March 10, 2020.
CEO of ParkMyCloud
See all (317)
26 
26 claps
26 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-lambda-pricing-low-but-unpredictable-1945b86be058?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 22, 2019·3 min read
Today’s entry into our exploration of public cloud prices focuses on AWS Lambda pricing.
Low costs are often cited as a benefit of using serverless. A recent survey showed that companies saved an average of 4 developer workdays per month by adopting serverless, and 21% of companies reported cost reduction as a main benefit. But why aren’t 100% of companies reporting cost savings?
In this article, we’ll take a look at the Lambda pricing model, and some things you need to keep in mind when estimating costs for serverless infrastructure.
AWS Lambda pricing is based on what you use. There are two major factors that contribute to the calculation of “what you use”:
There is a free tier available to all Lambda users — and note that this is unrelated to your regular AWS free tier usage. Every user gets 1 million requests per month and 400,000 GB-Seconds per month, for free.
In addition to requests and duration, you will also be charged for additional AWS services used or data transfers — regardless of whether you’re using Lambda’s free tier. For many applications, API requests and data transfers will cost significantly more than the AWS Lambda core pricing.
Ultimately, Lambda pricing is confusing and hard to predict. Here’s why:
Of course, there are several AWS Lambda pricing calculators out there to help estimate costs — ranging from the simpler that include only the number of executions, memory allocation, and average duration (examples from Dashbirdand A Cloud Guru) to those incorporating language, activity patterns, and EC2 comparisons from the cheekily named servers.lol.
There are plenty of benefits to serverless, from low latency to scalability to simple deployment. However, alongside vendor lock-in, applications with long or variable execution times, and control over application performance, cost is another reason why serverless may not replace traditional servers for all situations.
Originally published at www.parkmycloud.com on May 20, 2019.
CEO of ParkMyCloud
26 
26 
26 
CEO of ParkMyCloud
"
https://medium.com/@OpenAtMicrosoft/azure-at-a-glance-b407640d6274?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
Microsoft + Open Source
Mar 4, 2016·1 min read
From open source to data centers, here’s a rundown on why Azure is a great choice for your private, hybrid and public cloud needs.
Curious to learn more about our open source features? Visit our new open source hub on the Azure website at: www.azure.com/opensource!
All things open at Microsoft. www.microsoft.com/opensource
25 
25 
25 
All things open at Microsoft. www.microsoft.com/opensource
"
https://medium.com/@jaychapel/is-azure-surging-azure-growth-in-2020-dd891bd4ca5a?source=search_post---------102,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 26, 2020·3 min read
Microsoft Azure growth has long held the silver medal in public cloud. As of Q1 2020, Azure held 17% of the public cloud market, behind AWS’s 32%. But much of the adaptation to COVID-19 has happened after the Q1 period, which means they’re missing some dramatic activity: the drop in usage for businesses with lower demand, the massive increase in usage for those with high demand, and the infrastructure changes to support the at-home workforce.
Market reporting comparing Azure to its competitors in the IaaS market has shown steady growth and gain in market share. Microsoft reported that Azure grew 59% year-over-year last quarter, and has been growing at similar rates for the past year.
While these Azure growth rates are reported, the actual revenue numbers are reported as part of the “Intelligent Cloud” business, which includes Azure, other private and hybrid server products, GitHub, and enterprise services.
Something to keep in mind is that it’s easy to equate growth with net new customers Azure has gained — however, much of the growth comes from the increase in resources and usage within each customer. As just one example, among ParkMyCloud users, the average number of resources per Azure account increased 30-fold over a six-month period ending in February this year.
Back in March, Microsoft shared that, given any capacity constraints within a region, it would be giving resource priority to certain types of customers: first responders, health and emergency management services, critical government infrastructure, and Microsoft Teams to enable remote work. Even as they shared that, some customers were already running up against capacity constraints in certain regions and unable to create or restart VMs.
Whether customers experienced these shortages themselves or not, we’ve heard anecdotally that the possibility of capacity constraints has instilled enough fear in some that they’ve chosen to leave resources running when not being used as an (expensive) guarantee of availability for the next time they’re needed.
Microsoft Teams and Windows Virtual Desktops (VDI) are also seeing rapid adoption. As of last month, Teams daily active users were up to 75 million, up from 32 million in early March. Teams is part of the Productivity and Business Processes segment and does not impact the Intelligent Cloud revenue. However, it is integrated with Office 365 products, making it the platform of choice for many new users right now almost by default, similarly to the many enterprise users that adopt Azure as part of larger Microsoft agreements.
So — is Azure experiencing growth? Certainly, yes. But is it growing faster than competitors? Right now, there’s no evidence that it is.
Are you among the newest batch of Azure users? There’s a lot to learn. Here are a few resources other new users have found helpful.
And use this checklist to find other ways you might be wasting money.
Originally published at www.parkmycloud.com on May 20, 2020.
CEO of ParkMyCloud
See all (317)
13 
13 claps
13 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-much-do-the-differences-between-cloud-providers-actually-matter-7a7554e4234b?source=search_post---------103,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 23, 2019·7 min read
When evaluating public cloud providers, whether to choose the best for your environment or simply to try to decipher the disparity in market share, it can be easy to get hung up on the differences. AWS, Microsoft Azure, and Google Cloud each have their own service catalog, pricing and purchasing variations, and flavor. But do these differences actually matter?
Let’s take a look at some of the differences between the various cloud providers.
First up is terminology. At first glance, it may seem like the cloud providers each have a unique spread of offerings, but many of these products and services are quite similar once you get the names aligned. Here are a few examples:
Obviously, this is not a sign of substantive differences in offerings — and just goes to show that the providers are often more similar than it might appear at first glance.
Though we are able to align comparable products across AWS, Azure, and Google Cloud, there are of course differences between these offerings. In fact, with the number of products and services available today (we’ve counted 176 from AWS alone), comparing each is beyond the scope of this single blog post.
For our purposes, we can compare what is still the core product for cloud service providers: compute. Compute products make up about ⅔ of most companies’ cloud bills, so the similarities and differences here will account for the core of most users’ cloud experiences.
Here’s a brief comparison of the compute option features across cloud providers:
Of course, if you plan to make heavy use of a particular service, such as Function-as-a-Service/serverless, you’ll want to do a detailed comparison of those offerings on their own.
That covers functionality. How do the prices compare? One way to do this is by selecting a particular resource type, finding comparable versions across the cloud providers, and comparing prices. Here’s an example of a few instances’ costs as of this writing (all are Linux OS):
For more accurate results, pull up each cloud provider’s price list. Of course, not all instance types will be as easy to compare across providers — especially once you get outside the core compute offerings into options that are more variable, more configurable, and perhaps even charged differently (in fact, AWS and Google actually charge per second).
Note that AWS and Azure list distinct prices for instance types with the Windows OS, while Google Cloud adds a per-core license charge, on top of the base instance cost.
The table above represents the default On Demand pricing options. However, each provider offers a variety of methods to reduce these base costs, which we’ll look at in the Purchasing Options section below.
Comparisons of the myriad purchasing options are worth several blog posts on their own, so we’ll keep it high level here. These are the most commonly used — and discussed — options to lower costs from the listed On Demand prices for AWS, Microsoft Azure, and Google Cloud.
Each of the major cloud providers offers a way for customers to purchase compute capacity in advance in exchange for a discount: AWS Reserved Instances, Azure Reserved Virtual Machine Instances, and Google Committed Use discounts. There are a few interesting variations, for example, AWS offers an option to purchase “Convertible Reserved Instances”, which allow reservations to be exchanged across families, operating systems, and instance sizes. On the other hand, Azure offers similar flexibility in their core Reserved VM option. Google Cloud’s program is somewhat more flexible regarding resources, as customers must only select a number of vCPUs and memory, rather than a specific instance size and type.
What about if you change your mind? AWS users have the option to resell their reservations on a marketplace if they decide they’re no longer needed, while Azure users will pay a penalty to cancel, and Google users cannot cancel.
Another discounting mechanism is the idea of spot instances in AWS, low-priority VMs in Azure, and preemptible VMs, as they’re called on Google. These options allow users to purchase unused capacity for a steep discount. The cost of this discount is that these instances can be interrupted (or perhaps Azure puts it best with their “evicted” term) in favor of higher priority demand — i.e. someone who paid more. For this reason, this pricing structure is best used for fault-tolerant applications and short-lived processes, such as financial modeling, rendering, testing, etc. While there are variations in the exact mechanisms for purchasing and using these instance types across clouds, they have similar discount amounts and use cases.
Google Cloud Platform offers another cost-saving option that doesn’t have a direct equivalent in AWS or Azure: Sustained Use Discounts. This is an automatic, built-in discount for compute capacity, giving you a larger percentage off the more you run the instance. Be aware that the GCP prices listed can be somewhat misleading, as a sustained use discount is already built in, assuming full-month usage — but it is nice to see the cloud provider looking after its customers and requiring no extra cost or work for this discount.
A last sort of “purchasing option” is related to contract agreements. With all three major cloud providers, enterprise contracts are available. Typically, these are aimed at enterprise customers, and encourage large companies to commit to specific levels of usage and spend in exchange for an across-the-board discount — for example, AWS EDPs, Azure Enterprise Agreements. As these are not published options and will depend on the size of your infrastructure, your relationship with the cloud provider, etc., it’s hard to say what impact this will have on your bill and how it will compare between clouds.
As of earlier this year, AWS still dominates public cloud market share at 47%, while Azure and Google trail at 22% and 7% respectively. AWS quarterly sales are at least $7.7 billion — while Microsoft and Google avoid reporting specific numbers for public cloud but instead lump it in with other services, making revenue impossible to compare (other than an assumption that, yes, AWS is beating them.) Both report growth and their market share seems to be on the rise as well.
Does this matter? In some ways, yes. There’s a positive feedback loop in which larger enterprises will be more inclined to go with the solution that is serving the most large enterprises. And, AWS’s many-year head start and market advantage have let it develop more, and more innovative, offerings at a rapid pace. But ultimately we do not find the market picture to have any impact on functionality or availability.
It’s worth mentioning that you may also need to consider whether or not your organization competes with the cloud provider’s other lines of business. For example, many retailers specifically choose to avoid using AWS because they compete directly with Amazon. However, for most organizations, this is not an important factor to consider.
There’s also just the pure perception of the differences between cloud providers. Many folks perceive Azure as a bit stodgy, while Google Cloud seems slick but perhaps less performant than AWS.
Some appreciate AWS and Azure’s enterprise support and find Google Cloud lacking here, but this is changing as Google onboards more large customers and focuses on enterprise compatibility.
There are also perceptions regarding ease of use, but actually, we find these to be most affected by the platform you’re used to using. Ultimately, whatever you’re most familiar with is going to be the easiest — and any can be learned.
On some of the factors we went through above, the cloud providers do have variations. But on many variables, the providers and their offerings are so similar as to be equivalent. If there’s a particular area that’s especially important to your business (such as serverless, or integration with Microsoft applications), you may find that it becomes the deciding factor.
And… that’s okay! The fact of the matter is, you’re likely to be using multiple clouds soon, if you’re not already — so you will have access to the advantages of each provider. Additionally, applications and data are now more portable than ever due to containers.
So, prepare yourself and your environment for a multi-cloud reality. Build your applications to avoid vendor lock-in. Use cloud-agnostic tools where possible to take advantage of the benefits of abstraction layers.
Even if you’re only considering one cloud at the moment, these choices will benefit you in the long run. And remember: if your company is telling you to use a specific cloud provider, or an obscure requirement drives you to one in particular — don’t worry. The differences don’t matter that much.
Originally published at www.parkmycloud.com on September 11, 2019.
CEO of ParkMyCloud
See all (317)
12 
12 claps
12 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/live-digital-event-cloud-learn-87776cedde4d?source=search_post---------104,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Dec 7, 2021·1 min read
Did you know that worldwide spending on public cloud will have grown by 23% at the end of 2021 compared to 2020z? (Based on the latest Gartner forecast.) Adding cloud skills to your resume seems like an easy way to get ahead. But things change fast in the cloud, with a constant stream of new features and evolution. To get ahead and stay ahead, you need to be in constant interaction with the latest news and trends to improve your skills and stay current.
Luckily, there’s a lab for that! And this week only, get free access to labs at the biggest free learning event of the year, Google Cloud Learn!
Get unique opportunities to earn in-demand (and top paying) cloud credentials, participate in workshops and live Q&A with experts, and more. Starting December 8th, 2021.
Who: Devs, IT professionals, and data practitioners at all career levels
What: Google Cloud Learn.
Where: Online
When: Dec. 8–9 (NorthAm, LatAm, Japan); Dec. 9–10 (EMEA)
Why: Increase your cloud knowledge.
How: With free live demos, career development workshops & more
Register now!
7 
7 
7 
"
https://medium.com/@alibaba-cloud/building-a-high-performance-container-solution-with-super-computing-cluster-and-singularity-4115a88679be?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 20, 2019·6 min read
Alibaba Cloud Elastic High Performance Computing (E-HPC) service provides users with one-stop HPC services over the public cloud based on the Alibaba Cloud infrastructure. E-HPC automatically integrates the IaaS-layer hardware resources to provide users with on-cloud HPC clusters. In addition, E-HPC further supports the high availability of on-cloud HPC services and provides new features, such as CloudMetrics multidimensional performance monitoring and low-cost resumable computing, to allow users to use on-cloud HPC services in a better and more cost-effective manner.
This article introduces the elastic high performance container solution launched by Alibaba Cloud Super Computing Cluster and its applications in the Molecular Dynamics (MD) and AI fields.
Singularity is a container technology developed by the Lawrence Berkeley National Lab specifically for large-scale and cross-node HPC and DL workloads. Singularity features lightweight, fast deployment, and convenient migration. It supports conversion from Docker images to Singularity images. Singularity differs from Docker in the following aspects:
Singularity can be started by both root and non-root users. Before and after the startup of the container, the user context remains unchanged. Therefore, user permissions are the same both inside and outside the container.
Singularity emphasizes the convenience, portability, and scalability of the container service, and weakens the high isolation of the container process. Therefore, Singularity is more lightweight, has a smaller kernel namespace, and results in less performance loss.
The following figure shows the HPL performance data measured when different containers are used on a single Shenlong bare metal server (ecs.ebmg5.24xlarge, Intel Xeon (Skylake) Platinum 8163, 2.5 GHz, 96 vCPUs, and 384 GB). As shown in the following figure, the HPL performance measured when the Singularity container is used is slightly better than that when the Docker container is used, and is equivalent to that of the host.
Singularity is highly suitable for scenarios where HPC is used. It allows full utilization of host software and hardware resources, including the HPC scheduler (PBS and Slurm), cross-node communication library (IntelMPI and OpenMPI), network interconnection (Ethernet and InfiniBand), file systems, and accelerators (GPU). Users can use Singularity without having to perform extra adaptation to HPC.
Alibaba Cloud E-HPC integrates the open source Singularity container technology. While supporting the rapid deployment and flexible migration of user software environments, E-HPC also ensures the high availability of on-cloud HPC services and compatibility with existing E-HPC components, delivering an efficient and easy-to-use elastic and high performance container solution to users.
Users only need to pack and upload their local software environments to the Docker Hub to complete the entire process on the E-HPC console: Create a cluster > Pull an image > Deploy a container application > Submit a job > Monitor the performance and query monitoring results. In this way, users can reduce the HPC costs and improve the scientific research efficiency and production efficiency.
NAMD is a type of mainstream MD simulation software featuring good scalability and high parallel efficiency. It is often used to process large-scale molecular systems. In the following description, we assume that a Singularity image containing Intel MPI, NAMD, and inputfile is created based on the image docker.io/centos:7.2.1511. The PBS scheduler is used to submit the NAMD container job and a local job sequentially to four SCC nodes (ecs.scch5.16xlarge, Intel Xeon (Skylake) Gold 6149, 3.1 GHz, 32 physical cores, and 192 GB). The PBS job script is as follows:
To differences in CPU usage, RoCE network bandwidth, and software execution efficiency between running NAMD in the container and running NAMD directly on the host, the performance monitoring tool CloudMetrics that comes with E-HPC is used to monitor the usage of SCC cluster resources. The functions of CloudMetrics have been introduced in previous articles, particularly GROMACS and WRF. The following figure shows the resource usage information for each node.
As shown in this figure, the cluster resource usage is basically the same no matter whether NAMD is executed in the container or directly on the host. That is, the CPU is fully loaded and the RoCE network bandwidth remains at about 1.3 Gbit/s on each of the four nodes. The job execution times are 1324s and 1308s, respectively. This result indicates that Singularity is not only highly adapted to the host scheduler, MPI parallel library, and RoCE network, but also can ensure efficient execution of container jobs. The performance loss is less than 2% if container jobs are executed in Singularity, instead of on the host.
CIFAR-10 is a classic dataset in the image recognition field. In the following description, it is assumed that a Singularity and a Docker container that contain the image classification model are created based on the image docker.io/tensorflow/tensorflow: latest-devel-gpu-py3. Based on these two containers, training is carried out on a single EGS node (ecs.gn5-c8g1.4xlarge, Intel Xeon E5–2682v4, 2.5 GHz, 16 vCPUs, 120 GB, and 2 P100s). The command lines are as follows:
CloudMetrics is used to monitor the jobs. The following figure shows the resource usage information of the node.
As shown in the figure, no significant difference is found in the resource usage when training of the TensorFlow image classification model is conducted in the Singularity or Docker container. The CPU usage remains at 75%, and the usage of a single GPU ranges from 30% to 40%. Regarding the training efficiency, the training durations for 100,000 steps are 1432s and 1506s, respectively. This indicates that the Singularity container is not only highly adapted to the host GPU and CUDA, and but also has slightly higher job execution efficiency than the Docker container.
Alibaba Cloud Super Computing Cluster integrates the open source Singularity container technology to deliver an efficient and easy-to-use on-cloud elastic high performance container solution. This solution greatly reduces users’ cloud migration costs and improves their scientific research efficiency.
Reference:https://www.alibabacloud.com/blog/building-a-high-performance-container-solution-with-super-computing-cluster-and-singularity_594569?spm=a2c41.12663314.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
8 
1
8 claps
8 
1
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@krishnan/serverless-kubernetes-extreme-positions-hurts-adoption-88d18cf15de7?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krish
Aug 21, 2018·3 min read
Serverless is driving passionate debate in social media just like how public cloud ramped up the discussions circa 2008. We are once again seeing patterns in discussions that mirror what we saw in the early days of cloud. The increasing pitch of some of these messages (by the respective advocates or critics) are not helping anyone. In this post, we want to debunk some of these claims and help set the tone.
One of the underlying tones in the serverless advocacy is that it is the magic pill for all the IT problems. In fact, I saw one slide from the recently concluded ServerlessConf where the speaker was making fun of Kubernetes users. Yes, serverless is the highest level of developer platform abstraction that does all the heavy lifting needed to deploy the functions. But it is still severely opinionated and not suitable for all kinds of workloads. In fact, it is even limited for some of the stateless workloads. Dismissing any technology other than serverless as unnecessary heavy lifting is simplistic. Yes, if you invest heavily in running your own data centers, it is an unnecessary heavy lifting. Even here, it is not a binary characterization. What developers will be using, in the foreseeable future, are a continuum of services starting with managed Kubernetes services to serverless containers to serverless functions. Any claims about serverless as the only service that matters is either simplistic thinking or just pure hallucination.
On the other side of the debate, we have people making claims that Kubernetes is the future and serverless is just a crazy talk by a bunch of developers. In fact, the “Kubernetes is the only future” camp was involved in a conversation with someone advocating serverless on Twitter. The counter-argument against serverless, especially Functions as a Service offering like AWS Lambda was as follows:
If people should use serverless to avoid the heavy lifting, then everyone should be using public transport than driving their own car
Even though I personally believe that using public transport is the right thing to do to avoid climate change, I find this comparison too simplistic for a rebuttal. Public transport, at least in the USA, is not widespread enough to be an alternative agile mode of transportation for driving your own car. But, in the case of public cloud services, there are no such bottlenecks. The consumption of a service like AWS Lambda is as simple as opening the web browser and signing up with one’s credit card.
Yeah, this post is more of a rant than a typical StackSense post but it is time we do not take a binary position on any technology. That is an easy way to make any curious enterprise user drag their feet on any new technology. Instead, let us engage in discussions that actually helps enterprise users understand the landscape better and embrace modern technologies.
Originally posted at StackSense.io
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
13 
13 
13 
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
"
https://medium.com/@jaychapel/should-you-use-the-cloud-native-instance-scheduler-tools-ab3d00b7b160?source=search_post---------107,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 15, 2019·4 min read
When adopting or optimizing your public cloud use, it’s important to eliminate wasted spend from idle resources — which is why you need to include an instance scheduler in your plan. An instance scheduler ensures that non-production resources — those used for development, staging, testing, and QA — are stopped when they’re not being used, so you aren’t charged for compute time you’re not actually using.
AWS, Azure, and Google Cloud each offer an instance scheduler option. Will these fit your needs — or will you need something more robust? Let’s take a look at the offerings and see the benefits and drawbacks of each.
AWS has a solution called the AWS Instance Scheduler. AWS provides a CloudFormation template that deploys all the infrastructure needed to schedule EC2 and RDS instances. This infrastructure includes DynamoDB tables, Lambda functions, and CloudWatch alarms and metrics, and relies on tagging of instances to shut down and turn on the resources.
The AWS Instance scheduler is fairly robust in that it allows you to have multiple schedules, override those schedules, connect to other AWS accounts, temporarily resize instances, and manage both EC2 instances and RDS databases. However, that management is done exclusively through editing DynamoDB table entries, which is not the most user-friendly experience. All of those settings in DynamoDB are applied via instance tags, which is good if your organization is tag-savvy, but can be a problem if not all users have access to change tags.
If you will have multiple users adding and updating schedules, the Instance Scheduler does not provide good auditing or multi-user capabilities. You’ll want to strongly consider an alternative.
Microsoft has a feature called Azure Automation, which includes multiple solutions for VM management. One of those solutions is “Start/Stop VMs during off-hours”, which deploys runbooks, schedules, and log analytics in your Azure subscription for managing instances. Configuration is done in the runbook parameters and variables, and email notifications can be sent for each schedule.
This solution steps you through the setup for timing of start and stop, along with email configuration and the target VMs. However, multiple schedules require multiple deployments of the solution, and connecting to additional Azure subscriptions requires even more deployments. They do include the ability to order or sequence your start/stop, which can be very helpful for multi-component applications, but there’s no option for temporary overrides and no UI for self-service management. One really nice feature is the ability to recognize when instances are idle, and automatically stop them after a set time period, which the other tools don’t provide.
Google also has packaged some of their Cloud components together into a Google Cloud Scheduler. This includes usage of Google Cloud Functions for running the scripts, Google Cloud Pub/Sub messages for driving the actions, and Google Cloud Scheduler Jobs to actually kick-off the start and stop for the VMs. Unlike AWS and Azure, this requires individual setup (instead of being packaged into a deployment), but the documentation takes you step-by-step through the process.
Google Cloud Scheduler relies on instance names instead of tags by default, though the functions are all made available for you to modify as you need. The settings are all built into those functions, which makes updating or modifying much more complicated than the other services. There’s also no real UI available, and the out-of-the-box experience is fairly limited in scope.
Each of the instance scheduler tools provided by the cloud providers has a few limitations. One possible dealbreaker is that none of these tools are multi-cloud capable, so if your organization uses multiple public clouds then you may need to go for a third-party tool. They also don’t provide a self-service UI, built-in RBAC capabilities, Single Sign-On, or reporting capabilities. When it comes to cost, all of these tools are “free”, but you end up paying for the deployed infrastructure and services that are used, so the cost can be very hard to pin down.
We built ParkMyCloud to solve the instance scheduler problem (now with rightsizing too). Here’s how the functionality stacks up against the cloud-native options:
Overall, the cloud-native instance scheduler tools can help you get started on your cost-saving journey, but may not fulfill your longer-term requirements due to their limitations.
Try ParkMyCloud with a free trial — we think you’ll find that it meets your needs in the long run.
Originally published at www.parkmycloud.com on January 10, 2019.
CEO of ParkMyCloud
9 
9 
9 
CEO of ParkMyCloud
"
https://medium.com/@IBMDeveloper/orchestrate-multi-tier-application-deployments-on-kubernetes-using-ansible-part-1-a-comparison-f6c01bb43aba?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Oct 14, 2019·1 min read
In the era of microservices, containers, Kubernetes, and DevOps, deployment of complex multi-tier systems in the public cloud is achieved with continuous integration and delivery (CI/CD) pipelines. However, things become complicated when attempting to deliver applications onto private cloud or on-premise systems, where CI/CD isn’t owned by development, or doesn’t even exist at all.
In this article, I’ll discuss existing approaches to an on-premise delivery of containerized software, identify their strengths, and share common pitfalls.
Deploying complex application systems is not a new problem. Over the past few decades, the need for automated configuration and management, or orchestration, of software has been identified many times. In the operating systems space, configuration management tools like Chef, Puppet, Salt, and finally, Ansible, orchestrate configuration of OS-native applications. Tools like AWS CloudFormation, OpenStack Heat, and Terraform, orchestrate the deployment of Infrastructure-as-a-Service (IaaS) virtual resources including machines, block storage devices, or software defined networks (SDN). The order in which these tools were created, illustrates how complex, server-oriented solutions have faded, creating space for specialized serverless tools that can perform one kind of task particularly well (and fast).
You can read the full article on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
14 
14 
14 
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/@a2d2/building-the-death-star-26849be656f0?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Andy Manoske
Nov 13, 2015·8 min read
Why fear of change and the public cloud is motivating the largest tech merger in history
In the first Star Wars, the Death Star is created as a final stroke in a plan to consolidate rule and suppress competition in the Galactic Empire. After sweeping away the last semblances of competition, the Empire builds the Death Star as an instrument to stave off competition so the Empire can focus on expansion.
“The regional governors now have direct control over their territories,” Imperial Moff Tarkin tells protagonist Princess Leia. “ Fear will keep the local systems in line. Fear of this battle station.”
For Michael Dell, whose aspirations of consolidation are grand not unlike Emperor Palpatine’s, that death star is EMC.
With a $67B price tag, the proposed merger of Dell and EMC will be the largest consolidation of tech in the history of business. If it passes regulatory approval, the resulting company’s rise will be a major sea change for the competitive landscape: one with serious implications for Dell/EMC’s traditional competition, and a one that also hints at Dell’s real aspirations and concerns in the face of radical change coming in the next decade of infrastructure technology.
Dissolving the Senate
Dell’s focus has vacillated between the consumer and enterprise over the last decade. That said, throughout their life they have done well to invest strongly in their Enterprise Solutions Group (ESG), a business unit that in 2013 drove the company’s growth and 10% of the company’s net revenue the year prior.
In 2013 the company also took themselves private, with CEO/Founder Michael Dell vowing to double down on their investments in ESG and make it the heart of Dell. In a short amount of time, Dell’s focus has helped them climb to become the second largest IT server vendor in the world. A year later in 2014 Dell controlled 17.3% of the world’s server hardware market according to Gartner.
EMC on the other hand has charted a different path than Dell. Rather than double down on focus, EMC has spent much of the past two decades diversifying their holdings across a wide swath of IT infrastructure.
After taking a commanding lead as the largest enterprise storage vendor in the world in the 90’s, the Massachusetts-based EMC focused their efforts on capturing strongholds in other aspects of IT infrastructure in the hopes of owning the entire stack for how companies host applications and data. By the 2010’s, EMC would own the largest enterprise security company in the world (RSA) and over 80% of the largest server virtualization technology company in the world (VMware).
Their “acquire and control” methodology would also give them a big head start in the still-early world of enterprise public cloud. EMC’s 2013 spin-out of Pivotal Software (a company comprised of EMC business units Pivotal Labs, Greenplum, CloudFoundry, and SpringSource) would become the first of its kind to focus on tailoring what at the time considered a radical notion: the enterprise would start mirroring startup technology development best practices in relying on the cloud to build and host mission-critical applications.
But while EMC has been incredibly strategic in its acquisitions, the company’s core business of array-based storage servers has struggled to make effective use of those acquisitions’ technologies. EMC’s languishing performance in storage has come at the cost of faster-moving companies such as Dell and upstart storage companies such as Pure Storage and Nimble Storage.
This has led activist investors who own EMC’s public stock to push for either a change in leadership or a consolidation with another vendor better suited to make use of the company’s technology portfolio — something Michael Dell has been more than happy to do.
“A Fully Operational Battle Station”
At roughly the size of a small moon, the Death Star in Star Wars is an fearsome pinion of the Galactic Empire’s might.
Dell/EMC is no different. With Dell’s command of the existing enterprise server market and EMC’s command of enterprise storage, security, and virtualization, Dell/EMC will own nearly all of the top tier stack for hosting large scale applications and data on premise within the data center.
In the private cloud world, Dell/EMC will also take control over EMC’s new cloud technologies spin out. This new private cloud unit incorporates recent acquisition Virtustream to use EMC’s hardware and VMware’s server virtualization technology to allow enterprise companies to host their own private clouds off heterogeneous server and storage technologies.
Simply put, this would allow large enterprises to basically run their own “version” of Amazon AWS, supposedly allowing for the flexibility of dynamically spinning up/down servers while maintaining direct control over those servers within one’s own infrastructure. With Dell’s servers hosting Virtustream technology, the company would own nearly all points in the stack, bleeding little if any margin and maximizing their revenue with each sale to the increasingly cloud-minded IT enterprise.
“Our Most Desperate Hour”
The Death Star is an imposing weapon. But it has flaws.
It’s slow, taking so long to transit from blowing up Alderaan to Yavin V that the rebels in Star Wars can amass their forces for a final decisive strike. It’s built to repel fleets of capital ships, and is vulnerable to small squadrons of starfighters who could blow the entire thing up with one well-placed shot.
And the mere existence of the Death Star is a damning admission by the Galactic Empire of the Rebel Alliance’s power. The fact that the Empire needed to spend a bulk of their resources amassing a giant superweapon admits that there is an existential risk to the Empire’s rule, one driven by an insurgent cause that they understand very little and control even less.
All of these are apparent in Dell/EMC’s merger. The most critical flaw here is in the very need to merge in the first place. Just like the Death Star represents the Galactic Empire’s existential fear of the Rebel Alliance, the Dell/EMC merger reflects both companies’ manifested fear of how enterprise IT is shifting away from expensive on-premise (and even private cloud) infrastructures towards a more public cloud-friendly future.
A major reason why both Dell and EMC are concerned about the state of enterprise IT has to do with generational changes in the culture of software developers.
Whereas developers of previous generations accepted the technological reality of having to worry about where and how to host their applications and data, the rise of public cloud vendors like AWS in the last decade have created a new, insurgent generation of software engineers who frankly could not care less about whether their MySQL database is hosted on an EMC VCE or Dell PowerVault array.
Software engineers who grew up writing code on Heroku or AWS only want to focus on developing software. They have little, to absolutely zero, loyalty to IT vendors like EMC or NetApp. They are frugal because the aggressively low pricing of public cloud vendors in the past has baked within them an innate perception of what their infrastructure should cost. And they aren’t scared of the public nature of the cloud, as the public cloud is what they have relied upon for hosting their applications since they were in college.
The consequences of this generational shift in software engineering culture is real. According to IDC, “cloud IT infrastructure spending will grow at a compound annual growth rate (CAGR) of 15.6% and will reach $54.6 billion by 2019 accounting for 46.5% of the total spending on IT infrastructure.”
Of this spend, IDC notes that public cloud will dominate both in growth and total market size: the global IT infrastructure market in 2019 will be $35.3 billion with a healthy growth rate of 16.5% year over year, versus private cloud IT infrastructure being only half of that at $19.2 billion with a slower CAGR of 14%.
This growth will all come at the cost of traditional infrastructure. IDC ominously notes that the 15.6% annual growth of cloud infrastructure over the next five years is in stark contrast to that of traditional IT infrastructure like Dell and EMC, who will see their markets shrink by -1.4% every year over the next five years.
These numbers paint a very disturbing picture for the world of traditional enterprise IT. The growing insurgency of millennial and post-millennial software engineers with public cloud experience poses a serious existential threat to the Dells and EMCs of the world.
This threat absolutely merits the desperate creation of a $64 billion dollar superweapon to silence incumbent enterprise IT vendors, giving time for a resource-flush Dell/EMC to tailor technology like Virtustream and Pivotal to building a real competitor against increasingly enterprise-friendly Amazon AWS.
Shooting the Exhaust Port
The Death Star in Star Wars is an impressive and mighty weapon. It effortlessly destroys an entire planet, erasing one of the galaxy’s most prominent civilizations and sending horrifying ripples across the galaxy via the force.
But it is ponderously vulnerable and ultimately is destroyed — not by some equally gigantic superweapon or innumerably large fleet of capital ships, but by a small team of rebel pilots who nimbly duck and weave through the station’s seemingly endless defenses and guide a single torpedo into the heart of the Death Star’s reactor via a once in a lifetime shot on a two meter wide exhaust port.
Here at Amplify Partners we are focused on investing in the next generation of infrastructure technology. Essentially, it’s our job to find and arm that insurgency — that team of rebel pilots who can take the whole thing out with one good shot.
The rise of a new generation of developers who have a completely different approach to IT infrastructure is changing how storage, compute, and security are all seen, used, and purchased. New IT infrastructure companies will be built by former software engineers that are building technology and business models tailored to this new approach, and some like Docker and Cask* are already in their early stages of gathering traction within the developer community.
Change isn’t coming. Change is here, and it’s clear through desperate maneuverings like Dell’s incredible merger with EMC that the rebels are starting to win.
* Our firm Amplify Partners holds an equity position in Cask.
Principal PM for Cryptography and Security Products @HashiCorp. Formerly Defense/NatSec & Crypto @NetApp, VC @GGVCapital + @AmplifyPartners
See all (448)
4 
4 claps
4 
Principal PM for Cryptography and Security Products @HashiCorp. Formerly Defense/NatSec & Crypto @NetApp, VC @GGVCapital + @AmplifyPartners
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/8-ways-to-improve-cloud-automation-through-tagging-9f6d1717ff91?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 4, 2019·3 min read
Since the beginning of public cloud, users have been attempting to improve cloud automation. This can be driven by laziness, scale, organizational mandate, or some combination of those. Since the rise of DevOps practices and principles, this “automate everything” approach has become even more popular, as it’s one of the main pillars of DevOps. One of the ways you can help sort, filter, and automate your cloud environment is to utilize tags on your cloud resources.
In the cloud infrastructure world, tags are labels or identifiers that are attached to your instances. This is a way for you to provide custom metadata to accompany the existing metadata, such as instance family and size, region, VPC, IP information, and more. Tags are created as key/value pairs, although the value is optional if you just want to use the key. For instance, your key could be “Department” with a value of “Finance”, or you could have a key of just “Finance”.
There are 4 general tag categories, as laid out in the best practices from AWS:
In general, more tags are better, even if you aren’t actively using those tags just yet. Planning ahead for ways you might search through or group instances and resources can help save headaches down the line. You should also ensure that you standardize your tags by being consistent with the capitalization/spelling and limiting the scope of both the keys and the values for those keys. Using management and provisioning tools like Terraform or Ansible can automate and maintain your tagging standards.
Once you’ve got your tagging system implemented and your resources labeled properly, you can really dive into your cloud automation strategy. Many different automation tools can read these tags and utilize them, but here are a few ideas to help make your life better:
As your cloud use grows, implementing cloud automation will be a crucial piece of your infrastructure management. Utilizing tags not only helps with human sorting and searching, but also with automated tasks and scripts. If you’re not already tagging your systems, having a strategy on the tagging and the automation can save you both time and money.
Originally published at www.parkmycloud.com on August 14, 2018.
CEO of ParkMyCloud
9 
9 
9 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/future-trends-in-cloud-computing-all-point-to-optimization-6fc3bfbe6db?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 5, 2020·5 min read
Given our focus on public cloud cost control, we here at ParkMyCloud are always trying to understand more about the future trends in cloud computing, specifically the public cloud infrastructure (IaaS) and platform (PaaS) market. Now that public cloud has become ubiquitous, there’s a common theme. While new services and products continue to develop, more and more of them are focusing on not just creating capabilities that were previously lacking — they’re focused on optimizing what already exists.
Before we dive into optimization, let’s take a look at how the cloud market continues to grow in 2020 and beyond. Gartner estimates that $257.9B will be spent on public cloud services in 2020, up 6.3 from 2019 as outlined in the table below:
And according to IDC, almost half of IT spending is cloud-based, “reaching 60% of all IT infrastructure and 60–70% of all software, services and technology spending in 2020.” These projections come mid-2020, showing that even given the disruption this year, between Gartner and IDC, no one expects cloud adoption and spending to slow down any time soon. So what’s driving this growth and what are the future trends in cloud computing we should be on the lookout for in 2020 and beyond?
There is definitely a lot of hype around Blockchain, Quantum Computing, Machine Learning, and AI, as there should be. But at a more basic level, cloud computing is changing businesses in many ways. Whether it is the way they store their data, improvements to agility and go-to-market for faster release of new products and services, or how they develop and operate services remotely in today’s “locked-down world”, cloud computing is benefitting all businesses in every sector. Smart businesses are always looking for the most innovative ways to improve and accomplish their business objectives, i.e., make money.
When it comes to cloud technology, more and more businesses are realizing the benefits that cloud can provide them and are beginning to seek more cloud solutions to conduct their business activities. And obviously, Amazon, Microsoft, Google, Alibaba, IBM, Cisco, VMWare and Oracle plan to capture this spend by providing a dizzying array of IaaS, PaaS, and DaaS offerings to help enterprises build and run their services.
Cloud Automation Tools: as modern IT environments continue to become more diverse and distributed in the pursuit of key business goals, they also bear new challenges for the operation teams responsible for keeping everything running smoothly. The go-to strategy for taming the associated complexity can be summed up in one word — automation.
Automation tools, including some that incorporate AI, are on the rise in 2020. These new automation capabilities, along with comprehensive dashboards that provide a holistic view into multi-cloud operations, will become increasingly important for cloud and IT operations to support the lines of business regardless of where they place their workloads. These tools can help put the right workloads in the right place, manage costs, improve security and governance, and ensure application performance.
Desktop as a service (DaaS): DaaS is expected to have the most significant growth in 2020, increasing 95.4% to $1.2 billion. DaaS offers an inexpensive option for enterprises that are supporting the surge of remote workers due to the global pandemic and their need to securely access enterprise applications from multiple devices and locations.
Multi-Cloud and Hybrid Cloud: Once predicted as the future, the multi- and hybrid cloud world has arrived and will continue to grow. Most enterprises (93 percent) described their strategy as multi-cloud in 2020 according to a Flexera report (up 21% from 2018) and 87% have a hybrid cloud strategy. In addition, 71 percent of public cloud adopters are using 2+ unique cloud environments/platforms. These numbers will only go up in 2021. While this offers plenty of advantages to organizations looking to benefit from different cloud capabilities, using more than one CSP complicates governance, cost optimization, and cloud management further as native CSP tools are not multi-cloud. As cloud computing costs remain a primary concern, it’s crucial for organizations to stay ahead with insight into cloud usage trends to manage spend (and prevent waste) and optimize application performance.
It’s a complex problem, but we do see many organizations adopting a multi-cloud strategy with cost control and governance in mind, as it avoids vendor lock-in and allows flexibility for deploying workloads in the most cost-efficient manner (and at a high level, keeps the cloud providers competitive against each other to continually lower prices).
Growth of Managed Services: The global cloud managed services market is growing rapidly and is expected to reach $116B billion by 2025, growing from $62.4B in 2020 according to a study conducted by Markets and Markets. Enterprises are focusing on their primary business operations, which results in higher cloud managed services adoption. Business services, security services, network services, data center services, and mobility services are major categories in the cloud managed services market. Implementation of these services will help enterprises reduce IT and operations costs and will also enhance productivity of those enterprises.
Managed service providers — the good ones, anyway — are experts in their field and some of the most informed consumers of public cloud. By handing cloud operations off to an outside provider, companies are not only optimizing their own time and human resources — they’re also pushing MSPs to become efficient cloud managers so they can remain competitive and keep costs down for themselves and their customers.
While today, it sometimes seems like we’ve seen the main components of cloud operations and all that’s left to do is optimize them, history tells us that’s not the case. Cloud has been and will continue to be a disruptive force in enterprise IT for years to come as has the Global Pandemic of 2020, and future technology trends in cloud computing will continue to shape the way enterprises leverage public, private and hybrid cloud. Remember: AWS was founded in 2006, the cloud infrastructure revolution is still in early days, and there is plenty more XaaS to be built.
Originally published at www.parkmycloud.com on October 1, 2020.
CEO of ParkMyCloud
See all (317)
11 
11 claps
11 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GGVCapital/microsoft-and-google-have-fallen-behind-amazon-but-multi-cloud-changes-the-game-18e7d4aad9f1?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
GGV Capital
Nov 6, 2017·9 min read
Public cloud computing has surged into tech’s limelight, and many initial doubters are now adapting to the cloud computing wave. Larry Ellison, CEO of Oracle, famously remarked in 2008: “What the hell is cloud computing?… I have no idea what anyone’s talking about. I mean, it’s really just complete gibberish.” Fast forward to October 2017, when Oracle held its OpenWorld conference this year, the theme was “ The Ultimate Cloud Experience.”
We have entered an age where no tech giant can afford to ignore the inexorable force of cloud computing. Major tech companies such as Microsoft, Google, IBM, Alibaba and, yes, Oracle have woken up to the reality that cloud is here, it’s real, and they are already behind Amazon.
When Amazon first launched Amazon Web Services (AWS), many thought it would just appeal to startups. But the convenience, speed, and flexibility provided by the public cloud have proven so attractive that even larger enterprises have been moving a growing percentage of their compute workloads to the cloud.
The explosive growth of AWS caught the market by surprise. In April 2015, Amazon broke out its cloud computing revenues generated by AWS for 2014 for the first time. It turned out that its cloud computing unit was nearly a $5 billion business that year and, to everyone’s surprise, Amazon’s most profitable sector. Since then, AWS revenue has nearly tripled in a two-year span. So did Amazon’s market capitalization.
The 2015 AWS disclosure was a watershed moment. Like Oracle, all other major tech companies realized they needed to get in the cloud game fast, and started redoubling their investments into cloud. These companies have deep pockets and they’re playing for keeps — the cloud opportunity is too big to ignore.
Every company is a software company that needs the cloud
Why has cloud become so indispensable to so many companies? Because pretty much every company has become a software company, and they all need to deliver their software faster and to more people than ever before.
Even in traditional industries, a company’s competitiveness increasingly depends on the quality of its software. Carmakers such as Tesla release new software frequently to power the next generation of smart automobiles. Airlines need fast, easy-to-use software to compete with aggregators in order to drive more bookings back to their own sites. Restaurants need well-designed software to ensure a seamless food delivery experience ( More than half of Domino Pizza’s orders now come from online).
Software companies have better things to do than negotiate data center leases, purchase and install hardware, and manage and secure their infrastructure. With public cloud, they can not only easily spin up a virtual machine with an API call, but also access a myriad of related services and building blocks (such as RDS, Lambda, Redshift, GKE, etc.) that are basically impossible to replicate in-house.
No time to waste
Driven by intense competition, the requirements for software products are constantly changing, so much so that companies need to release updates daily or even multiple times per day. The rise of CI (continuous integration) and CD (continuous deployment) is a direct consequence of today’s rapidly changing software requirements. Traditionally, those who wrote code and those who deployed code worked separately in silos, forming a bottleneck when deploying new software. Now, in order to realize a CI/CD workflow, many companies have integrated their Development and Operations groups through the “DevOps” role. DevOps helps companies seamlessly launch new features on a continuous basis and avail themselves of the flexibility and global nature of public cloud.
Additionally, the advent of containerization and micro-services software architectures is enabling companies to release updates quickly, burst availability when demand spikes, and make efficient use of public clouds. Containers bring an unprecedented level of portability, and micro-services architectures are inherently distributed, enabling the break-out of workloads across physical locations. Portability has shown initial value across development, staging and production. Hybrid environments — spanning public cloud for some portions of an application’s systems and physical, on-prem or co-located infrastructure for other parts — have become the norm, facilitating segregation for security purposes, for example.
Together, these trends have led to the use of multiple clouds, or “multi-cloud.” In particular, as the public clouds adopt native support for Kubernetes, multi-cloud momentum should continue to build.
Enter the multi-cloud era
Given the many growing public cloud options and intensity of the competition between the tech giants, we’re seeing a rapid move to multi-cloud amongst many large companies. As Armon Dadgar, co-founder and co-CTO of HashiCorp attests, “in almost every Fortune 500 account we are engaging, customers are either planning or using multiple cloud providers.”
There are several factors driving this phenomenon:
Startups can thrive by capitalizing on multi-cloud trend
The multi-cloud world can be a scary and confusing one, especially for companies in traditional industries that are just adopting software as a differentiator. However, the benefits are hard to ignore. A recent survey by RightScale found that 85% of enterprises now have a multi-cloud strategy, up from 82% in 2016. This creates immense opportunities for startups that can help companies work seamlessly across various different cloud providers. Startups that promise cloud neutrality — not being locked into one particular vendor — will have significant advantage in this new battlefield.
For startups riding the multi-cloud wave, some areas of opportunity include:
At GGV, we are excited to back companies that can make sense of and drive value from this new, complex ecosystem. Entrepreneurs that understand, embrace, and specialize in navigating this multi-cloud world will have a shot at being the winners in the multi-cloud wars. Please reach out to us with your multi-cloud startups and to share ideas on this dynamic opportunity!
Special thanks to Armon Dadgar, Edith Harbaugh, Kris Beevers, Bassam Tabbara, Crystal Huang, Daniel Lopez and Semil Shah for their advice, thoughtful reviews and edits.
Glenn Solomon is a Managing Partner at GGV Capital. He’s led GGV’s investments in Zendesk, Square, Successfactors, Nimble Storage, Isilon,Pandora, HashiCorp, Slack, NS1, AlienVault, Bitsight, Domo, AirBnB, Synackand others. He blogs regularly. Follow on Twitter @glennsolomon.
Originally published at http://goinglongblog.com/multi-cloud-next-big-thing-technology/ on November 6, 2017.
GGV Capital is a global venture capital firm that invests in local founders. For more @jrichlive, @glennsolomon, and @hanstung
See all (108)
63 
63 claps
63 
GGV Capital is a global venture capital firm that invests in local founders. For more @jrichlive, @glennsolomon, and @hanstung
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@BMCSoftware/how-does-cloud-vendor-performance-impact-your-user-experience-76c23d61b92f?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
BMC Software
Mar 2, 2017·3 min read
With public cloud revenue expected to soar from $80 billion spent in 2015 to $167 billion by 2020, according to Forbes, businesses are rapidly moving to the cloud. This move has been accompanied by a shift in visibility in system and application performance. You have access to a significant amount of data that can help you understand the value and performance of your cloud vendors.
However, there is a difference between data and metrics. Metrics help you determine what data is important and transform it into something meaningful. To ensure your cloud vendors are performing to your expectations and providing your end users with a positive experience, here are some metrics your business should measure.
Focus on the data that matters most to your company. The following metrics point to the performance of your cloud vendors.
Now that you know what metrics you need to keep track of to be successful, you need to know where to access this data. Consider the following four types of data to measure in your break down:
Metrics give you clarity into the performance of your cloud vendors. They help you take large amounts of data and transform it into proactive actions you can take to make your business run more efficiently.
Northwestern University is a great example of how metrics helped their team collaborate and resolve problems. Before they implemented BMC TrueSight they manually had to track down and troubleshoot problems for nearly 25,000 students. Because the process was time-consuming and inefficient, their team couldn’t collaborate on the data to come up with a viable solution. With the help of BMC TrueSight App Visibility Manager, a web-based capacity and availability service monitoring dashboard, they were able to use data to accelerate problem detection and resolution. This provided their students with an improved user experience because they could demonstrate their quality of service based on data.
BMC is a global leader in software solutions that help IT transform traditional businesses into digital enterprises for the ultimate competitive advantage.
53 
1
53 
53 
1
BMC is a global leader in software solutions that help IT transform traditional businesses into digital enterprises for the ultimate competitive advantage.
"
https://medium.com/@cloud66/deploy-your-ruby-on-rails-application-to-microsoft-azure-with-cloud-66-225d11535248?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Oct 3, 2017·3 min read
With Cloud 66 you can easily deploy and scale your Rails app on Microsoft Azure public cloud. We already had native integration with Azure a couple years back using Azure Service Management deployment model.
Listen to our customers we decided to upgrade our integration to support the preferred Azure Resource Management. This gives you a lot of new cool features on our platform and your Rails application can benefit from this.
Moving forward we are going to deprecate the old deploy model because new features of the Azure platform will be supported only in the new Azure Resource Management deployment model.
When we supported Azure back in the days the Azure portal looked very different and was pretty well organized around the old deploy model.
Fast-forward to the recent Portal, if you already deployed with the old model, the resources for your Rails application are all over the place and not organized around a resource group.
Moving forward with the new deployment model all resources are grouped inside one resource group. This gives you more overview of resources, better cost management and more!
With the new integration, we got a lot of new features.
Application Gateway
When you are ready to ramp up your service and you need more capacity a Load Balancer is what you need in front or your Rails app cluster. Adding a Load Balancer to your Rails application, we will take care of the heavy lifting and install and configure a new Application Gateway.
SSL termination
If you secure your web traffic with an SSL certificate we got that covered too. With the new integration, we support SSL termination. Adding an SSL certificate will automagically configure your Application Gateway!
Security groups
When you deploy your Rails app we provision your infrastructure and will create all the resources you need; public ip-addresses, disks, virtual machines, virtual networks and security groups.
At Cloud 66 we make sure your application is secure. We configure your security group but also configure UFW on every provisioned virtual machine.
Cloud 66 need some credentials to authenticate our agent to provision your infrastructure through Cloud 66. You need to specify the following credentials:
You can find those by following the step by step guide created by Microsoft.
Make sure you give your created Active Directory App the right access. Check if your Active Directory Application is added to your Azure subscription and has the role Contributor.
Now you are ready to deploy your Rails app to Microsoft Azure!
Our integration is still in Beta but is production ready!
Maestro is a full container management service we offer and can be used if you want to deploy a polyglot app using Rails and other cool tech. Your containers can be deployed to Azure using the new integration and the cool features.
At the moment our Container Service V2 backed by Kubernetes is not supported on Azure due to network restrictions on Azure.
Originally published at blog.cloud66.com on October 3, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
29 
29 claps
29 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/where-are-you-on-the-cloud-spend-optimization-maturity-curve-f7d9b90f21ce?source=search_post---------115,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 4, 2020·4 min read
Cloud spend optimization is always top of mind for public cloud users. It’s usually up there with Security, Governance, and Compliance — and now in 2020, 73% of respondents to Flexera’s State of the Cloud report said that ‘Optimize existing use of cloud (cost savings)’ was their #1 initiative this year.
So — what the heck does that mean? There are many ways to spin it, and while “cost optimization” is broadly applicable, the strategies and tactics to get there will vary widely based on your organization and the maturity of your cloud use.
Having this discussion within enterprises can be challenging, and perspectives change depending on who you talk to within an organization — FinOps? CloudOps? ITOps? DevOps? And outside of operations, what about the Line of Business (LoB) or the Application owners? Maybe they don’t care about optimization in terms of cost but in terms of performance, so in reality optimization can mean something different to cloud owners and users based on your role and responsibility.
Ultimately though, there are a number of steps that are common no matter who you are. In order to facilitate this discussion and understand where enterprises are in their cloud cost optimization journey, we created a framework called the Cloud Cost Optimization Maturity Curve to identify these common steps.
While cloud users could be doing any combination of these actions, this is a representation of actions you can take to control cloud spend in order of complexity. For example, Visibility in and of itself does not necessarily save you money but can help identify areas ripe for optimization based on data. And taking scaling actions on IaaS may or may not save you money, but may help you improve application performance through better resource allocation, scaling either up (more $$$) or down (less $$$).
Let’s dig into each in a little more detail:
For more ways to reduce costs, check out the cloud waste checklist for 26 steps to take to eliminate wasted spend at a more granular level.
Originally published at www.parkmycloud.com on July 27, 2020
CEO of ParkMyCloud
See all (317)
12 
12 claps
12 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/threat-intelligence-for-complete-aws-cloud-security-and-compliance-5762c376f4fe?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Oct 23, 2017·1 min read
With organizations of all sizes evaluating a move to public cloud, it is important to assess the security implications to ensure a secure cloud infrastructure. Intelligent cloud management platforms can quickly assess and mitigate vulnerabilities in real time and adopt a comprehensive security management for your cloud.
Follow these audit policies to assuage critical vulnerabilities:
To know more about how you can bulletproof your AWS infrastructure resolve the most critical audit vulnerabilities quickly read here.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
23 
23 
23 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@jaychapel/spot-instances-can-save-money-but-are-cloud-customers-too-scared-to-use-them-828c654633a6?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 17, 2020·5 min read
Spot instances and similar “spare capacity” models are frequently cited as one of the top ways to save money on public cloud. However, we’ve noticed that fewer cloud customers are taking advantage of this discounted capacity than you might expect.
We say “spot instances” in this article for simplicity, but each cloud provider has their own name for the sale of discounted spare capacity — AWS’s spot instances, Azure’s spot VMs and Google Cloud’s preemptible VMs.
Spot instances are a type of purchasing option that allows users to take advantage of spare capacity at a low price, with the possibility that it could be reclaimed for other workloads with just brief notice.
In the past, AWS’s model required users to bid on Spot capacity. However, the model has since been simplified so users don’t actually have to bid for Spot Instances anymore. Instead, they pay the Spot price that’s in effect for the current hour for the instances that they launch. The prices are now more predictable with much less volatility. Customers still have the option to control costs by providing a maximum price that they’re willing to pay in the console when they request Spot Instances.
Variations of spot instances are offered across different cloud providers. AWS has Spot Instances while Google Cloud offers preemptible VMs and as of March of this year, Microsoft Azure announced an even more direct equivalent to Spot Instances, called Azure Spot Virtual Machines.
Spot VMs have replaced the preview of Azure’s low-priority VMs on scale sets — all eligible low-priority VMs on scale sets have automatically been transitioned to Spot VMs. Azure Spot VMs provide access to unused Azure compute capacity at deep discounts. Spot VMs can be evicted at any time if Azure needs capacity.
AWS spot instances have variable pricing. Azure Spot VMs offer the same characteristics as a pay-as-you-go virtual machine, the differences being pricing and evictions. Google Preemptible VMs offer a fixed discounting structure. Google’s offering is a bit more flexible, with no limitations on the instance types. Preemptible VMs are designed to be a low-cost, short-duration option for batch jobs and fault-tolerant workloads.
Our research indicates that less than 20% of cloud users use spot instances on a regular basis, despite spot being on nearly every list of ways to reduce costs (including our own).
While applications can be built to withstand interruption, specific concerns remain, such as loss of log data, exhausting capacity and fluctuation in the spot market price.
In AWS, it’s important to note that while spot prices can reach the on-demand price, since they are driven by long-term supply and demand, they don’t normally reach on-demand price.
A Spot Fleet, in which you specify a certain capacity of instances you want to maintain, is a collection of Spot Instances and can also include On-Demand Instances. AWS attempts to meet the target capacity specified by using a Spot Fleet to launch the number of Spot Instances and On-Demand Instances specified in the Spot Fleet request.
To help reduce the impact of interruptions, you can set up Spot Fleets to respond to interruption notices by hibernating or stopping instances instead of terminating when capacity is no longer available. Spot Fleets will not launch on-demand capacity if Spot capacity is not available on all the capacity pools specified.
AWS also has a capability that allows you to use Amazon EC2 Auto Scaling to scale Spot Instances — this feature also combines different EC2 instance types and pricing models. You are in control of the instance types used to build your group — groups are always looking for the lowest cost while meeting other requirements you’ve set. This option may be a popular choice for some as ASGs are more familiar to customers compared to Fleet, and more suitable for many different workload types. If you switch part or all of your ASGs over to Spot Instances, you may be able to save up to 90% when compared to On-Demand Instances.
Another interesting feature worth noting is Amazon’s capacity-optimized spot instance allocation strategy. When customers diversify their Fleet or Auto Scaling group, the system will launch capacity from the most available capacity pools, effectively decreasing interruptions. In fact, by switching to capacity-optimized allocation users are able to reduce their overall interruption rate by about 75%.
There is one main caveat when it comes to spot instances — they are interruptible. All three major cloud providers have mechanisms in place for these spare capacity resources to be interrupted, related to changes in capacity availability and/or changes in pricing.
This means workloads can be “evicted” from a spot instance or VM. Essentially, this means that if a cloud provider needs the resource at any given time, your workloads can be kicked off. You are notified when an AWS spot instance is going to be evicted: AWS emits an event two minutes prior to the actual interruption. In Azure, you can opt to receive notifications that tell you when your VM is going to be evicted. However, you will have only 30 seconds to finish any jobs and perform shutdown tasks prior to the eviction making it almost impossible to manage. Google Cloud also gives you 30 seconds to shut down your instances when you’re preempted so you can save your work for later. Google also always terminates preemptible instances after 24 hours of running. All of this means your application must be designed to be interruptible, and should expect it to happen regularly — difficult for some applications, but not so much for others that are rather stateless, or normally process work in small chunks.
Companies such as Spot — recently acquired by NetApp (congrats!) — help in this regard by safely moving the workload to another available spot instance automatically.
Our research has indicated that fewer than one-quarter of users agree that their spot eviction rate was too low to be a concern — which means for most, eviction rate is a concern. Of course, it’s certainly possible to build applications to be resilient to eviction. For instance, applications can make use of many instance types in order to tolerate market fluctuations and make appropriate bids for each type.
AWS also offers an automatic scaling feature that has the ability to increase or decrease the target capacity of your Spot Fleet automatically based on demand. The goal of this is to allow users to scale in conservatively in order to protect your application’s availability.
People who are hesitant to build for spot more likely use regular VMs, perhaps with Reserved Instances for savings. It’s likely that people open to the idea of spot instances are the same who would be early adopters for other tech, like serverless, and no longer have a need for Spot.
For the right architecture, spot instances can provide significant savings. It’s a matter of whether you want to bother.
Originally published at www.parkmycloud.com on June 9, 2020.
CEO of ParkMyCloud
10 
10 
10 
CEO of ParkMyCloud
"
https://architecht.io/iaas-edge-kubernetes-why-cloud-adoption-is-still-early-and-still-evolving-34be60080303?source=search_post---------118,NA
https://netflixtechblog.com/nosql-netflix-talk-part-1-d3c5faa94d6a?source=search_post---------119,"A few weeks ago, I gave the first in a series of planned talks on the topic of NoSQL @ Netflix. By now, it is widely known that Netflix has achieved something remarkable over the past 2 years — accelerated subscriber growth with an ever-improving streaming experience. In addition to streaming more titles to more devices in both the US and Canada, Netflix has moved its infrastructure, data, and applications to the AWS cloud.
In the spirit of helping others with similar needs, we are sharing our experiences with AWS and NoSQL technologies via this tech blog and several speaking appearances at conferences. Via these efforts, we hope to foster both improvements in cloud and NoSQL offerings and collaboration with open-source communities.
The NoSQL @ Netflix series specifically aims to share our recommendations on the best use of NoSQL technologies in high-traffic websites. What makes our experience unique is that we are using publicly available NoSQL and cloud technology to serve high-traffic customer-driven read-write workloads. Once again:
Netflix’s NoSQL Use-cases = public NoSQL + public cloud + customer traffic + R/W workload + high traffic conditions
The slideshow below was loosely based on the following whitepaper. Some of the key questions addressed by the slides and whitepaper are as follows:
Driven by a culture that prizes curiosity and continuous improvement, Netflix is already pushing NoSQL technology and adoption further. If you would like to work with us on these technologies, have a look at our open positions
The slides are available here:
— Siddharth “Sid” Anand, Cloud Systems
docs.google.com
www.meetup.com
Originally published at techblog.netflix.com on March 31, 2017.
Learn about Netflix’s world class engineering efforts…
2 
2 claps
2 
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/@kenhuiny/data-encryption-in-the-cloud-part-4-comparing-aws-azure-and-google-cloud-1cda50ef1606?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kenneth Hui
Mar 9, 2018·21 min read
Due to the length of this blog post (20 pages), I’ve decided to make it available as a downloaded PDF which you can grab here. But I suggest reading the first section of this page before switching to the PDF if you plan to do so.
I’ve written previously about the role of data encryption as a critical component of any company’s security posture and the potential pitfalls of not using encryption properly. This is magnified when you are talking about storing data outside of customer data centers such as is the case when archiving data to public cloud storage repositories such as Amazon S3, Azure Blob Storage and Google Cloud Storage. It is important to understand that while public cloud providers are responsible for securing the infrastructure and provide tools for protecting the data stored in their infrastructures, the user is ultimately responsible for using those tools to secure their data.
I want to continue this blog series by giving an overview of how encryption at rest is implemented among the big three public clouds — Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP). In the interest of keeping this post to a manageable scope, I will focus specifically on the following:
I assume, in this blog post, that readers are familiar with the basics of data encryption and encryption key management. If you you want to learn more about encryption generally or need a refresher on terms such as envelope encryption and key encryption key, I strongly suggest reading my encryption primer and key management blog posts. They will provide a foundation for understand the concepts discussed in this post.
I will be talking about server-side vs. client side encryption throughout the post so it might be helpful here to review the differences.
With server-side encryption, data is not encrypted until it is transferred to the target, in this case the object storage service. All three providers offer server-side encryption with some differences in implementation details, particularly in regards to key management.
With client-side encryption, data is encrypted at the source and prior to it being transferred to the target, in this case the object storage service. All three public cloud providers allow for client-side encryption with some offering varying levels of integration.
So now let’s take a look at each of the public cloud providers.
Amazon S3 is the AWS object storage service and is by far, the most widely used in the world. As the public cloud provider with the most longevity, AWS has the encryption services and key management offerings with the most options. Details on the AWS services and options are also the most clearly explained and the most accessible of the three providers.
Amazon S3 supports both server-side and client-side encryption with a number of options for each. Customers have the option of enabling server-side encryption by default for all uploaded objects to S3. For both server-side and client-side encryption, AWS utilizes AES-256 with Galois Counter Mode (GCM) for any symmetric key encryption operations. Without getting into detail, GCM provides authenticated encryption by adding a unique tag to the ciphertext which verifies that the encrypted data has not been tampered with in any way. Envelope encryption is used for all client-side options and for all server-side options except when the customer provides the encryption key.
For server-side encryption, Amazon S3 supports three options:
With SSE-S3, both the KEKs and the DEKs are stored and managed by the S3 service. All key management functions, including the periodic rotation of keys, are performed by the service without input required from the user. S3 does this by using an AWS-managed Key Management Service (KMS). The encryption workflow for SSE-S3 is as follows:
The decryption workflow is as follows:
Before diving into the SSE-KMS option, it is important to note that KMS uses the term Customer Master key (CMK) to describe what would be typically called the Key Encryption Key (KEK). Similarly, AWS uses the term Data Key to describe what would be typically called the Data Encryption Key (DEK). These are not merely a change in terminology but a CMK and a data key are logical representations of a KEK and a DEK respectively. I will provide more details when we talk specifically about key management.
With SSE-KMS, the Data Key is encrypted either by a default CMK that is automatically created when a user chooses to encrypt an S3 object for the first time in an AWS Region or by a pre-existing CMK created by the user. Using a CMK created explicitly by the user provides more flexibility and control over the CMK. The encryption workflow for SSE-KMS is as follows:
The decryption workflow is as follows:
The SSE-C option places the burden of key management completely on the user. Amazon S3 still handles the encryption and decryption process but the customer provides the encryption keys which must be a AES-256 symmetric key. The customer provides these keys for every encryption and decryption operation. AWS doesn’t store the actual keys but performs a salted Hash-based Message Authentication (HMAC) operation against the keys and stores the resultant hash. Without getting into details, an HMAC hash is essentially a digital signature that can be used to verify the authenticity of the keys for future operations without having to store customer-provided keys in AWS. The hash itself cannot be used to decrypt data or to recover a lost key.
The encryption workflow for SSE-C is as follows:
The decryption workflow is as follows:
Moving on to client-side encryption, Amazon S3 supports two options:
To assist customers who choose the client-side encryption option, AWS provides an Amazon S3 encryption client which is embedded into the AWS SDK for a number of languages including Java, Go and others. The encryption client handles all data encryption and decryption operations using AES-256 GCM symmetric encryption with a master key (AWS equivalent of a Key Encryption Key) generated in KMS or provided by the user.
With client-side encryption that leverages AWS KMS, the customer creates a CMK in KMS and receives a CMK ID, which is a logical representation of the actual CMK. Users provide the CMK ID when they request an object, ensuring that the actual CMK never leaves KMS. The encryption workflow is as follows:
The decryption workflow is as follows:
Using this option, customers are able to encrypt data before it leave their data center and uploaded to S3. However, they can do without bearing the responsibility for maintaining a cryptographic system or their own Key Management Infrastructure (KMI).
The final client-side encryption option requires the customer to provide the master key (KEK) that is used to encrypt any Data Keys. This master key can be a symmetric key or an asymmetric public key. The client side master key is provided to the AWS encryption client which handles all data encryption and decryption operations. The customer has full responsibility for storing and managing the master key. The encryption workflow is as follows:
The decryption workflow is as follows:
Users can also choose to encrypt data prior to being uploaded to S3 using their own cryptographic and key management infrastructure without the AWS encryption client. The encryption process is transparent to S3 and the encrypted data is stored as it would be with unencrypted data.
From a key management perspective, AWS primarily offers three options to help customer with encryption of their S3 data:
We’ve already spent time on the first option and explained that the S3 service can store and manage encryption keys on behalf of the customer, includes periodic rotation of keys. With this option, users are offloading all key management responsibilities to AWS.
CloudHSM offers key storage for customers with their own key management software who don’t want to manage their own key storage system. CloudHSM give customers a dedicated hardware appliance that is tamper-proof running in an Amazon Data Center. The CloudHSM appliance integrates with key management software that users can run on-premises or in AWS. User are responsible for the full lifecycle of the keys that are stored in CloudHSM.
AWS Key Management Service (KMS) is a fully managed service that is essentially provided to users as key management software as a service running on a fleet of hosted HSM appliances. Users can interact with KMS via a web interface that gives them the option of managing the full lifecycle of encryption keys stored in KMS. User can also choose to leverage KMS for key management through SSE-KMS or Client-Side Encryption with KMS-managed customer keys.
Historically Amazon has called their Hardware Security Module (HSM) appliances Hardened Security Appliances (HSA) but they are essentially the same thing.
Internally, KMS has multiple layers of abstraction, all designed to secure the encryption keys and to simplify key management. This begins with the CMK which represents the top a customer’s key hierarchy within KMS. A CMK is not an actual instance of a master key but a logical container for a set of historical keys.
When a user requests a new CMK, the following workflow occurs:
When a CMK is rotated, the following workflow occurs:
Because of the use of these abstraction, the CMK maintains the same CMK ID and ARN even if the CMK is rotated multiple times. Only the backing key changes so uses can continue using the “same” CMK while avoiding encryption key over-reuse.
Note that during the data encryption process, KMS uses an HSA to generate a DEK which is also known as a Customer Data Key (CDK). The CDK is also a logical key container that holds the actual DEK and other relevant cryptographic materials.
Beyond the creating and rotating of keys, KMS enables management of the full lifecycle of encryption keys.
In terms of KMS integration with S3, SSE-S3, KMS is fully managed by AWS and offers the following key management capabilities:
With SSE-KMS and Client-Side Encryption with KMS-managed customer keys,, customer can managed their CMKs and owns the following key management capabilities:
Azure Blob Storage is the Microsoft Azure object storage services offering. For the purposes of this post, we will be focusing on Block Blob Storage which is most similar to Amazon S3 and Google Cloud Storage. While Azure has almost as many options as AWS for data encryption and key management, the details are not always easy to find and details are often unclear or missing.
Azure supports both server-side and client-side encryption with users having the option of enabling server-side encryption by default for all uploaded objects. In Azure, server-side encryption is called Storage Service Encryption when it pertains to blob storage. Azure leverages envelope encryption using AES-256 symmetric keys for data or content encryption (Microsoft uses the term Content Encryption Key in place of Data Encryption Key) and supports using either a symmetric or an asymmetric keys for the Key Encryption Key (KEK), depending on who is generating and managing the keys.
Storage Service Encryption supports using a KEK that is either:
The encryption workflow for Storage Service Encryption is as follows:
The decryption workflow is as follows:
For client-side encryption, Azure supplies a storage client library, written for Java, .NET and Python, that integrates with Azure encryption. With this option, users have the option of storing and managing their own KEKs or using Azure Key Vault. The workflow is as follows:
The decryption workflow is as follows:
Users can also choose to encrypt data prior to being uploaded to Azure using their own cryptographic and key management infrastructure without the storage client library. The encryption process is transparent to Azure Blob Storage and the encrypted data is stored as it would be with unencrypted data.
For server-side encryption, keys are managed via one of two options:
For client-side encryption, keys are managed via one of three options::
For both the service-managed and Azure Key Vault options, keys are stored in a set of Hardware Security Modules (HSM) that are managed by Microsoft.
With the service-managed option, all keys are generated by the Azure Blob Storage Service and managed by Microsoft. This includes storing keys, rotating keys and also managing the lifecycle of older historical keys which are needed for decryption of older data.
Server-Side encryption using service-managed keys can be enabled by default for Azure Blob storage and is a good option for customers who don’t have their own key management infrastructure on-premises or in Azure and don’t want to assume the responsibility for key management.
Azure Key Vault is a service that exposes an HSM-backed key management infrastructure to Azure customers and can be integrated with both server-side and client-side encryption. Users can generate keys via Azure Key Vault or import them to the Key Vault. Responsibility for managing the lifecycle of the keys fall on the user using Azure Key Vault tools.
Azure Key Vault is a great option for customers who have a requirement for managing their own encryption keys but don’t have their own key management infrastructure installed. Unlike service-managed keys, customers can choose when they want to rotate, revoke or delete keys while relying on Azure Key Vault and the HSM infrastructure the service runs on to store and to protect all sets of historical keys.
Google Cloud Storage (not the most original name in the world) Is the Google Cloud Platform (GCP) object storage service. Probably due to the newness of the service, Google Cloud Storage has the fewest options for encrypting data and managing keys. Those options, however, are clearly explained.
Google Cloud Storage performs server-side encryption by default on all uploaded objects. All data is broken into chunks which can be up to several GB in size. Using envelope encryption, each chunk of data is encrypted with a unique Data Encryption Key (DEK) that is also encrypted with a Key Encryption Key (KEK) and the encrypted version of the DEK is then stored alongside the encrypted data. The encrypted chunks of data are then distributed across Google’s storage systems. Both the KEK and the DEK use symmetric AES-256 with Galois Counter Mode (GCM) cipher.
Google Cloud Storage supports server-side encryption with two options:
With server-side encryption using KEKs that are stored and managed by Google’s internal KMS (not to be confused with Google’s Cloud KMS service) or supplied by the customer. The workflow using keys managed by KMS is as follows:
The decryption workflow is as follows:
With the Customer-Supplied Encryption Key (CSEK) option, users have to generate their own AES-256 symmetric key and provide it to Google Cloud Storage for encryption/decryption operations. The CSEK is only stored in storage system memory and never persisted to any Google Cloud device. The encryption workflow is as follows:
The decryption workflow is as follows:
Since Google’s KMS is not involved, users are responsible for not only key generation but their own key management.
Google Cloud allows for client-side encryption but does not currently offer any specific integrations such as a client-side library for generating DEKs. The user is responsible for generating the encryption keys and encrypting the data prior to uploading it to Google Cloud Storage. The encryption process is transparent to Google Cloud Storage and the encrypted data is stored as it would be with unencrypted data, which means the client-side encrypted data will actually be encrypted again by the service.
The Google Cloud Storage Encryption by Default option leverages Google’s internal Key Management Service (KMS) and should not be confused with GCP’s Cloud KMS offering which is currently NOT supported with Google Cloud Storage. Customer KEKs are generated by and centrally stored in Google’s internal KMS. This KMS is protected using a hierarchy of encryption keys.
The root KMS Master Key distributor is a distributed system where each instance will periodically compare its keys with the keys stored in a random peer system and reconcile any differences that are found. This ensures that there is no single point of failure and that KMS maintains high availability.
Users can offload a host of key management functions to Google’s KMS, including key generation and key rotation. KEKs are generated using a random number generator built by Google and seeded from various entropy sources. Key rotation is performed every 90 days and up to 20 versions of a KEK can be stored at a time. This means all the DEKs that are encrypted by a given KEK would need to be re-encrypted at least once every 5 years.
Below is a side-by-side comparison of encryption-at-rest across the three providers’ object storage services. As you may expect, the robustness of the service and the diversity of options is strongly correlated with the age of the cloud provider.
It should be clear that security in general and data encryption in particular are key features for the big three public cloud providers. Every user who is considering if they should use the public cloud or which public cloud to choose should factor data encryption into their decision. Hopefully, this blog series will help shed some light and assist readers in their evaluations.
Originally published at cloudarchitectmusings.com on March 9, 2018.
Ken is a Solutions Architect for Amazon Web Services (AWS), focused on helping Public Sector customers. He is passionate about Cloud Computing and great food.
21 
21 
21 
Ken is a Solutions Architect for Amazon Web Services (AWS), focused on helping Public Sector customers. He is passionate about Cloud Computing and great food.
"
https://medium.com/@sawaba/you-re-right-listening-to-customers-is-holding-them-back-99a6628248b6?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adrian Sanabria
Apr 23, 2016·1 min read
.Cloud Opinion
You’re right — listening to customers is holding them back. Tough decision for a company, because these are mostly existing customers (though on the legacy side, considering cloud). The irony is that AWS owes a lot of its success and current product portfolio to listening to customers. The difference is that AWS has customers hell-bent on innovating, whereas Microsoft has customers hell-bent on dragging their feet. The average “we’re interested in Azure” query is probably focused on savings, and doesn’t really understand the whole automation/agility aspect of the cloud. They’re going to forklift everything into Azure — servers, vendors, existing licenses, bloat and even crappy legacy processes.
A year later, they’ll leave “the cloud” for a private cloud option and rave about what a big mistake using public cloud was.
I can’t speak to how well Microsoft does or doesn’t educate its customers, but they’ve told me that customer often isn’t even willing to talk without some legacy IT concessions. The idea is to at least get the customers dipping their toes in, and then perhaps gradually get them educated and willing to adopt the ideas and concepts that scare them with cloud. I think this is a more painful route in the longterm, but Microsoft has different customers, so I think choosing a different path is hard to avoid.
Information security veteran blogging primarily about how technology can hinder or help productivity and progress here. Co-founder of Savage Security.
2 
1
2 
2 
1
Information security veteran blogging primarily about how technology can hinder or help productivity and progress here. Co-founder of Savage Security.
"
https://medium.com/@jstrachan/that-video-walks-you-through-how-to-install-jenkins-x-and-how-to-then-use-it-to-create-a-new-f2b58b7cbd7d?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Strachan
May 9, 2018·1 min read
Aaron Schlesinger
that video walks you through how to install Jenkins X and how to then use it to create a new project etc.
Here’s how to install it (either on the public cloud, creating a k8s cluster or installing on an existing k8s cluster):
jenkins-x.io
Then once you’ve got things installed here’s how to create a new spring boot app, quickstart or import existing apps:
jenkins-x.io
There’s also a few demos:
https://jenkins-x.io/demos/
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
2 
2 
2 
I created Groovy and Apache Camel. I’m currently working at CloudBees on Jenkins X: automated CI/D for Kubernetes: https://jenkins-x.io/
"
https://medium.com/@Technology_Adv/box-vs-dropbox-which-is-best-for-your-business-f20309b204e0?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechnologyAdvice
Mar 30, 2017·8 min read
Public cloud storage gives your business the freedom to share, edit, and collaborate on work assets in real-time, from any location. And if you lose a corporate device or your office is submerged by the flood of the century, all your data is safe and secure.
It’s no wonder businesses across the globe are making cloud storage part of their virtual IT environment.
The public/private cloud storage market will surpass $56 billion by 2019, according to MarketsandMarkets, and two of the biggest players on the field are Box and Dropbox. The 2016 Intel report on cloud adoption and security “Building Trust in a Cloudy Sky” stated that 93 percent of those surveyed used cloud software, with 59 percent stating their belief that public cloud can reduce cost of ownership. If you’re considering a solution for your business, you’ve likely wondered about the difference between Box v. Dropbox. On the surface, they seem to offer a similar solution for business customers: secure cloud storage with collaborative and administrative tools.
While that’s true, there are some clear distinctions in packaging, functionality, user experience that set them apart.
Before we get into specific features, let’s take a look at Box vs Dropbox from a 10,000-foot view.
Box, Inc. released its IPO (initial public offering) early this year and enjoyed a period of significant growth, now valued by Hoovers at $216 million. Their user base includes 37 million people and 47,000 organizations.
Dropbox hasn’t gone public yet, so it’s hard to put a number on their revenues. According to recent reports, they now have over 500 million users, which dwarfs Box’s user base. But then 200 million of those users are paying customers who purchase Dropbox Pro or Dropbox for Business.
Both vendors are about 10 years old, and both offer multiple plans for business and personal accounts. But it’s important to note how Box and Dropbox uniquely position themselves in the market. It wouldn’t make much sense if they offered the same product, targeted toward the same customer, for different prices — which is why they don’t.
While Box does offer two personal storage plans, their product is first and foremost an enterprise solution. Consequently, its features and user experience are tailored to the needs of the CIOs and IT departments:
“We’re a 100 percent enterprise-focused company. All the technology we’re building goes toward asking how do we make it easier, or more scalable, or simpler, and just a better way for businesses to share and manage and access this data.” — Aaron Levie, Box CEO
Dropbox, on the other hand, began as a consumer-facing product and gradually made its way into the workplace as a form of “consumerized IT.” In other words: people liked Dropbox; people used Dropbox at work; workplaces saw the benefits of Dropbox; workplaces adopted Dropbox as a business solution.
“We’re solving really important problems for a big chunk of the world, not just Silicon Valley. Our users are trapeze artists, high school football coaches . . . physicists who collaborate across the world.” — Drew Houston, Dropbox CEO
These different approaches to file management pose unique challenges to each vendor.
For Box, that might mean improving the user experience on an individual level, making their platform easier to use and easier to administrate. For Dropbox, it might mean providing deeper administrative control.
Let’s take a closer look.
The first thing you’ll need to consider is the core product: how much does it cost, and what are you getting for the money?
Box offers three pricing tiers for businesses. Their “Starter” package, aimed at 3–10 person teams, provides 100 GB of storage. The “Business” tier — for medium-sized teams with more advanced needs — provides unlimited storage; and the “Enterprise” tier adds some advanced features like help desk, workflow automation, and metadata management.
Dropbox offers three plans for businesses with 5 users and up: Dropbox for Business at the Standard, Advanced and Enterprise. The Standard plan includes most business capabilities and 2TB of storage. Users who want more granular administration, IT, and domain controls should upgrade to a higher plan. This is a departure from the former one-plan business approach, but gives companies of different sizes and needs options without paying enterprise-grade prices.
Box provides a full suite of advanced security features, from file encryption to custom data retention rules and enterprise mobility management (EMM). Administrators can decide which users are granted access to files and folders and create user groups for easier assignment. There are seven levels of access control, which address access, preview, editing, and sharing.
Files themselves are protected by TLS and multi-layered encryption, file versioning, and expiration controls, as well as custom content security policies. Users can also apply passwords to confidential files as needed.
Features:
Dropbox includes standard security for a public cloud server — more than enough for the average business to protect their sensitive data. With Dropbox for Business, you’ll get 256-bit AES encryption (same as Box), group management and sharing restrictions, and the ability to remotely wipe data in the case of a compromised account or hostile termination.
Administrators can use the admin dashboard to track user and team access stats, including logins, devices, and sharing, as well as add and remove team members. Dropbox for Business does not offer data retention policies, which could be a drawback if you’re in a highly-regulated industry, but probably won’t affect most users.
Features:
Collaboration is one of the biggest selling points of cloud storage for a business, so it’s important to consider how a given solution will help your team work together. What tools do vendors offer, beyond basic file sharing?
Both vendors perform pretty well in this category. First, they both offer flexible sharing options including link sharing, invite-only sharing, and external sharing. Second, they both enable teams to collaborate in real time using team or group folders. Users can comment on files, work directly from Outlook, and access their storage account through native mobile apps for most devices. Not only that, but both vendors have built-in file recovery and versioning to keep users from losing their work.
Dropbox unique feature: Request files from internal or external personnel through the “File Request” tool; shared files automatically show up in your folder
Box unique feature: Create new documents inside of the application using Box Notes (the editing tools are fairly basic, so most users will still need a third party application)
Both platforms integrate with Office 365, DocuSign, and other content apps.
Obviously, neither of these products are designed to be a full-fledged project management solution.
But they can support existing project strategies by helping your teams manage work in an orderly, repeatable fashion — especially files and documents directly related to the project, such as RFPs, budgets, gantt charts, media assets, and so forth.
With Dropbox, you can use team folders and groups to collaborate on shared assets in real time. Each user can customize their notification preferences to stay up-to-date without being inundated. Dropbox also has a built-in photo organizer (Carousel), which makes it easy for teams to share multimedia files associated with a project.
With Box, you can create workflow automation rules to manage files based on certain conditions (e.g. if a file in Folder X is edited by a certain user or team, send the file to another specified user for review and approval.)
This level of task management admittedly primitive, but it can still be helpful for teams that manage a cyclical review process, such as editorial or design teams.
Almost every business has a system or set of systems that support their core business operations. In sales, this might be a CRM database. In the medical field, it might be an EHR system. Whatever the case, it’s crucial that your file management tools integrate seamlessly with your enterprise software. That means you need to be able to upload, download, and sync files from directly within the application.
How do Box and Dropbox line up with this standard?
For starters, they both integrate with Office 365 and Outlook. This is important since Outlook, in many ways, is still the gold standard for work email and calendaring (and since almost 80 percent of Fortune 500 companies have used or are using SharePoint).
Beyond that, each platform uses APIs to integrate with a variety of third-party applications.
Dropbox claims a list of 300,000 connected apps; Box, a mere 1,000.
Dropbox’s universality, in large part, can be attributed to its role as a consumer and business app. Box, on the other hand, mainly focuses on business-critical apps such as CRMs and marketing platforms. Here are some out-of-the-box highlights (not a comprehensive list):
The verdict?
Box and Dropbox both offer solid solutions for public cloud storage — including versatile file sharing, mobile access, and reliable security. The “better” product will depend on your unique priorities and budget. If you’re still having trouble deciding, give us a call. We’d love to help.
And remember, Box and Dropbox aren’t the only public cloud storage solutions. Other options for private cloud storage run within your company’s firewall, but these bring along with them the problems of build, manage, and upkeep throughout the cloud’s life cycle. Public cloud storage provides the ease of SaaS with most of the security. To see a list of options sorted by industry, features, and operating system, check out our product selection tool.
This article was updated March 30, 2017 to include Dropbox Business updates and more recent information regarding cloud usage.
Originally published at technologyadvice.com on March 30, 2017.
We know a few things about technology, but our real speciality is helping people. Unbiased research. Crowd sourced reviews. Expert advice. technologyadvice.com
1 
1 
1 
We know a few things about technology, but our real speciality is helping people. Unbiased research. Crowd sourced reviews. Expert advice. technologyadvice.com
"
https://medium.com/@OpenAtMicrosoft/harness-the-power-of-open-source-a7fc862db4af?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Microsoft + Open Source
Mar 7, 2016·1 min read
Want to learn more about Microsoft Azure’s public cloud offering?
Join speaker Jose Miguel Parrella, Open Source Product Manager at Microsoft, this Wednesday, March 9 at 10 am PST, to learn how different organizations around the globe are harnessing the power of open source in Azure.
From compute fabric with Linux to open data solutions, see how you can make the most of open source tools on the cloud.
Sign up here:https://event.on24.com/eventRegistration/EventLobbyServlet?target=lobby20.jsp&eventid=1134651&sessionid=1&key=9D64A7B13C5E8457116518496C332D83&eventuserid=135598003
All things open at Microsoft. www.microsoft.com/opensource
51 
51 
51 
All things open at Microsoft. www.microsoft.com/opensource
"
https://medium.com/@jaychapel/tips-for-purchasing-software-through-cloud-marketplaces-4c4ac0b97f2d?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 2, 2020·5 min read
One of the activities we are engaged in is the cloud marketplaces run by the large public cloud providers. These marketplaces provide an alternative channel for software and services companies apart from the more typical direct sales or reseller/distributor models. Many customers ask about options to buy our product via one of these marketplaces — which has given us some tips for others interested in purchasing this way.
Given the “app store model” that has been so widely embraced by consumers (App Store, Google Play Store etc) it’s not surprising that the cloud providers see an opportunity to leverage their customer footprint and derive revenue share when customers choose to purchase via their marketplaces. For customers, it can be a way to consolidate bills, get discounts, and simplify administration.
The business model is simple. The Cloud Service Providers (CSPs) charge a percentage of revenue based on the value of the purchase price being paid by the customer. Companies list their products to reach buyers who they would not otherwise reach, or provide a purchasing method which better suits the customers needs since they can add the cost of the purchased product onto their monthly cloud bill thus avoiding complex new procurement/purchasing arrangements. The CSPs obviously hope that the value proposition is strong enough to warrant the sellers giving up some margin in exchange for net additional sales or sales which would be otherwise overly complex to close and bureaucratically burdensome.
Currently, we only participate in the AWS Marketplace, but there are similar options available in the Azure Marketplace and the Google Cloud Marketplace. The largest and seemingly most well stocked is that of AWS where there are close to some 10,000 listings from a broad range of software vendors. In fact, Gartner estimates that some 25% of global 1,000 organizations are using such online marketplaces. Forrester reports that as much as 40% of all B2B transactions are through digital sales channels. And a 2112 Group Survey reported that 11% of channel executives believe marketplaces will drive the majority of their indirect revenue as soon as 2023.
These organizations claim the benefits as being: a lower sales/buying cycle time; ease of use, increased buyer choice; and ease of provisioning and deployment. Additionally the promise of leveraging the CSPs own account managers to support co-selling on specific opportunities and the potential for them to act as lead sources, albeit we imagine these need to be larger deals and part of a broader relationship between the CSPs and their most valuable ISV customers. Still finding and aligning with CSP sales reps who get to retire quota by selling your product via the marketplace especially if it means those same reps get to sell more of their core cloud services.
Opportunities to offer alternate sales models can also be made available through the marketplace. For example, charging on a metered basis where the customer only pays for what is used and has this cost added to the bill (rather than a fixed monthly fee) or via longer term contracts secured over two or three years at discounted rates.
Those companies that have managed to optimize their offerings in partnership with CSPs and have developed co-developed / co-branded products have the potential for a lot of upside. Databricks partnership with Azure and Snowflake and Datadog with AWS have driven enormous growth and helped them build unicorn sized businesses within a few years.
One area which has been somewhat frustrating is the ability for customers to discover appropriate software products to meet their needs within the marketplaces. In part this is a similar challenge as faced in consumer facing app marketplaces where there is an overabundance of products and the categorization and search algorithms are often weak. This leaves the sellers (particularly the lesser known ones) frustrated and customers unable to determine what software is best to meet their needs. In our own cost optimization space this has many different dimensions and lots of offerings often
So what do buyers need to know about these marketplaces and making them work to your advantage? To help answer this we have included a short checklist of tips and considerations.
Depending on your situation, there may be other considerations but what is for sure is that managing multiple marketplaces requires time and resources. If you have not already investigated these, either as a buyer or a seller, now might be the time to have a look.
Originally published at www.parkmycloud.com on September 29, 2020.
CEO of ParkMyCloud
9 
9 
9 
CEO of ParkMyCloud
"
https://medium.com/@Apiumhub/cloud-computing-a-growing-trend-in-2017-a91ec066c43d?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
May 25, 2017·8 min read
In the past few years, cloud computing has become a common word. In fact, According to the Worldwide Semiannual Public Cloud Services Spending Guide, worldwide spending on public cloud services will grow at a 19.4% CAGR, from almost $70 billion in 2015 to more than $141 billion in 2019 with companies investing in cloud services for new competitive advantages. In the following overview about cloud computing you will get more information about what is cloud computing, the different types of service models, the benefits of cloud computing and most importantly, the cloud computing trends in 2017.
To summarise it, it’s more or less when you store data and programs on a cloud instead of storing it on the hard drive of a computer. What do you need to access the data? An internet connection. So yes, it’s about sharing, storing, processing and managing resources delivered over a network of remote servers hosted on the Internet.
Here, we talk about offering services that are related to hardware. It’s basically using a third party provider for hosting a hardware. It can be said that you are paying to access hardwares over the net, this can mean server or storage services. An example would be if you have a subscription with a hosting company to store files on their servers with a pay per use model.
Leading vendors:
Here, you are getting a development platform on the cloud. What does that mean? You get access to a platform that allows you to develop and build applications that run on systems software or hardware that belong to other companies. For example, developing an ecommerce web that is running on a vendor’s server.
Leading players:
Ss its name says it, here, you get a complete software on the cloud. So you are running a complete application on the system of a third party, hosted on the cloud. A very common example would be a CRM system, here at Apiumhub we use ZOHO that enables us to get a sort of overview of our sales cycles.
Leading players:
A private cloud is when an infrastructure is either hosted on-site on a company’s intranet, or hosted in the data center of a service provider and the resources are not shared with other organizations. We use it to increase storage capacity and the power of the processor. It is often used by big companies with big amounts of data or that have strict regulations about their data and certain types of regulations. The main advantage of going for a private cloud is that it offers a higher level of control and security. Also, it’s more customisable and therefore adapter to the specific IT requirements of companies. In general, the disadvantage would be that the maintenance and management is part of the responsibility of the company.
On the public cloud, the data of a company is stored in the data center of a third-party provider, on a shared hardware, and the storage and processor capacity are not owned by that company. Usually, small to medium sized companies use public cloud computing due to various advantages it provides them; as it’s not their hardware, it implies that there are no maintenance costs to the client and that they are not responsible for the management. Another advantage is that the time it takes for testing & deploying is decreased. Basically, it delivers agility, scalability and efficiency. Although security breaches are kind of rare, some businesses get scared off by that when it comes to public cloud.
The name is quite clear, hybrid cloud is a mix of private & public cloud services, it’s almost as if you get the best of both! What is does is that is allows to move between both clouds and enables you to leverage the beast of what each one offers. The main advantage is that you get huge flexibility and much more options, you can for example put the most critical operations on the private cloud and the rest that on the public, increasing your agility.
Now that we all kind of got what is cloud computing and the different types we’ve got, here’s the part that keeps on evolving and that is really great to know about; cloud computing trends in 2017!
The development of native apps keeps on increasing and cloud providers are more and more focusing on how to provide services for those applications that are more complex and that need things like time-based analytics, omni-channel support, and microservice support.
You probably all know what IoT is and how much it grew in the past year. Well, it’s still the beginning. In 2017, we might reach millions of sensors and other devices coming online! All of is mostly focused around smart cities, Connected Buildings, Predictive Maintenance and autonomous traffic
Don’t be shocked to see more and more image recognition and voice interfaces. Between the release of TensorFlow by google and the three new machine learning services announced by amazon, you can be sure that in 2017 it will continue on growing, specially because it is becoming easier for developers to use and integrate into applications.
Since 2015, the use of hybrid cloud has continued to grow and is expected to grow even more in 2017. Making hybrid cloud work implies having an audit function to verify that the service still fits for purpose. This is getting us to a new position “a cloud service broker” that is responsible for defining services and choose the best way to manage and secure them.
When talking about next-generation cloud applications, containers are very important. In the last two years, many companies have started using container technologies like Docker, to help them standardize the way they package and deploy code. It enables developers to really manage code.
This technology is crucial to agile development and to microservice architectures. As we will get more applications directed towards microservices, container platforms that run microservices will be more on demand. Other than being one of the most exiting cloud computing trends for developers (at least it’s the case here at Apiumhub), this obviously implies that there will be new challenges, for example companies will have to be more careful with security, monitoring, storage and networking issues.
As the demand for the cloud services is that high and will continue on growing, scalability and maintenance costs are therefore more and more important. Hyperconverged infrastructure solutions is great because it offers to help by proposing pre-integrated compute and storage resources that will help companies when it comes to getting their cloud implementations running faster. Hyper converged platforms will be more used for cloud architecture in 2017 and will become the default infrastructure platform when it come to building the private part of a hybrid cloud.
Cloud is not seen as if it’s only for small companies anymore. Huge players have started to notice the advantages and facility of it, the fact that it cuts costs and risks. So yes, we will continue on seeing the transitions of infrastructures into the cloud.
But more specifically, the public cloud. In fact, C levels are getting more and more comfortable when it comes to hosting software in the public cloud and we expect this to be one of the cloud computing trends that will grow the most.
Last year we saw many self service solutions and data has become more and more easy to push on the cloud without having a technical background. Hopefully we will soon say goodbye to the complexity of data integration & transformation, it might be more of a copy/paste action.
With all those growing cloud computing trends, there are much more companies using the cloud for their business, there will be more security standards that will try to get better migration to the cloud and therefore encouraging more businesses to integrate and adopt the cloud. This increases the demand for cloud expertise. Cloud-focused training are more frequent and training programs are focusing on cloud security, hosted databases, and infrastructure as a service.
Enough with cloud computing trends! By now you must have understood that with cloud computing you get many benefits. In a way you are enabling access to data from almost anywhere and with the growth of digital devices around us, we are just making it all more efficient and available. Here are the five main benefits of cloud computing:
With a purchased software you usually get yearly releases but when you’re using cloud computing services, it’s easy, you can get an upgraded system immediately. So you get the latest versions when they are released, including new features and functionalities and that on a regular base and with the latest technology. This can mean that you get up-to-date versions of software and upgrades to servers and processing power.
Yes, you reduce the costs. Different costs. First of all, as companies have smaller (or none) data centers by using cloud computing, it implies that you reduce costs because you don’t need to buy equipment, hardware, facilities, utilities, etc. you reduce the number of servers and software costs. Other than that, you are also lowering the staff costs and system maintenance costs.
Granting access to your employees means that you are boosting their flexibility. In fact, you can access, edit and share data from anywhere you are, because you only need a device and an internet connection (some apps even work offline). Most of employees are quite happy when they know that they can take their work anywhere. Happy employees means higher productivity.
In fact, it’s much more flexible. You can play with your capacity, scale up or down your storage following your specific needs, may they be changing or not. If your needs increase or decrease, you really don’t need to worry about it.
Obviously all businesses want to protect their data. There are many situations, which might rarely happen, but when they do, represent a tremendous problem as for example, natural disasters or power failures. Well with the cloud, everything is backed up in a secure place. This means that you can always access your data unless your device is broken or that you have no connection. But that’s not as big of a problem than losing the whole data, you just need to get another device. Your data is in the cloud, you can access it no matter what happens to your machine.
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
2 
2 
2 
Software architecture, web & mobile app development www.apiumhub.com
"
https://medium.com/idgtechtalk/when-it-comes-to-cloud-dont-fall-for-buzzwords-e9abe33f5c0?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
Hybrid cloud, multi cloud, public cloud, private cloud, and more. The buzzwords surrounding cloud computing are important to master — or are they?
At our #IDGTECHtalk Twitter chat yesterday we discussed hybrid cloud. Does it exist? Is it necessary? Our experts weighed in to determine the role hybrid cloud has in modern tech. Here’s what they said:
Hybrid cloud is a combination of both private and public cloud. That is an important distinction as we continue this recap.
And hybrid cloud should not be confused with multi cloud, which is explained further by Ed Featherston below:
As we saw above, cloud can get a little complicated. So it is essential that a clear strategy is outlined before you deploy cloud. In the best case scenario cloud can assist with analytics, application development, and security. But not having a cloud strategy can lead to murkiness in your overall business strategy, cloud sprawl, and gaps in security.
While cloud is increasingly common in most organization’s tech strategy, it should not be taken lightly. In addition to having a solid strategy, your implementation should be well thought out. This includes hiring the right people to handle your cloud implementation and maintain your strategy in the long-term.
Whether you are trying to properly define which cloud offering is right for you or you’re revising your cloud strategy, our experts agree that the first thing you should consider is consulting with the tech experts within your organizations to determine what cloud strategy will work best.
Are you interested in being part of the next #IDGTECHtalk? Please join us every Thursday at 12pm ET for an exciting chat.
The #1 tech publisher.
3 
3 claps
3 
The #1 tech publisher. We report the hottest tech trends from leading experts & the biggest brands. Join us every Thursday, 12pm ET, for the #idgtechtalk chat.
Written by

The #1 tech publisher. We report the hottest tech trends from leading experts & the biggest brands. Join us every Thursday, 12pm ET, for the #idgtechtalk chat.
"
https://medium.com/@nutanix/botmetric-cloud-report-the-future-of-cloud-f5bc80735d9b?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Dec 12, 2017·3 min read
The future of cloud is very exciting. Get insights into this exciting tomorrow through Botmetric’s Annual Public Cloud Usage Report 2017: An in depth analysis of public cloud adoption and usage trends. This year, we are thrilled to tell you that we were able to track over $1 billion public cloud consumption of our customers. A lot of hard work has been the foundation of this specially curated report. We wanted to share our unique insights with you, as you were a crucial pillar of making this report possible. This report is your ticket to the journey into the past (consumption trends) and future of cloud computing. The Botmetric 2017 Public Cloud Report gives you crucial information across a wide scope of cloud statistics such as popular cloud services, cost saving actions, cloud maturity index, spend patterns, management challenges, and multi cloud trends.
Most Popular Cloud Services
Ever wondered what the most popular AWS and Azure services are? EC2 and VMs are the big players in AWS and Azure respectively. This is no surprise as compute capacity is the rudimentary basis for running any and every application. EC2 (Elastic Compute Cloud) is a secure and resizable compute capacity service. Especially loved by developers, the service makes web-scale cloud computing a lot more easier and faster. And of course, you can use AWS’s pay as you go feature to use AWS’s most popular service!
With Azure Virtual Machines, you are no longer confined in the realms of virtualization. Computing services in virtualization has never been more flexible. You can virtualize services for development and testing, running applications, as well as for extending your datacenter. VM’s are Azure’s most popular service for a reason: You can deploy application in literally seconds.
This is just a brief insight into the most popular cloud services. Want to know the exact figures and get specific data into all AWS and Azure popular services? All you have to do is download the report!
Top Cost Saving Actions
Be a smart cloud user. Don’t neglect paramount cost saving opportunities. Use the Botmetric Cloud Report to know exactly what other users are doing to save their cloud costs.
Today, the maximum number of AWS users are using Reserved Instance Planning to save big on their cloud costs. Planning is the KEY here. Proper planning and performance testing is crucial in understanding your application’s compute needs. Once you know your computing needs in and out, RIs is the best way to cut down costs.
Virtually all Azure customers start and stop their non production VMs to save cloud costs during weekends and non working hours. On an average, this saves more than 7% of the total cloud spend.
This post is just a preview of the wealth of cloud knowledge awaiting you. To become a cloud master, download Botmetric’s complete Annual Public Cloud Usage Report 2017 now!
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
12 
12 
12 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@cloud66/openshift-commons-briefing-bringing-policy-as-code-into-the-container-deployment-pipeline-2f5024ad9691?source=search_post---------130,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Aug 22, 2018·2 min read
Kubernetes and containers are fast gaining ground as infrastructure building blocks, and as managed (public cloud) Kubernetes becomes a commodity in the market, more and more production workloads migrate to containers.
As a consequence, an increasing number of infrastructure architects, DevOps specialists and engineering managers are coming to terms with their next problem: managing multi-environment deployment, configuration templates, policy and security in the pipeline. A great example of this is the case of Red Hat OpenShift, where the sophistication, quality, and robustness of the platform itself exposes the immaturity and fragmented nature of the CI/CD space.
Cloud 66, a proud member of OpenShift Commons, has tackled these exact same issues when building and running our Kubernetes-based stack, which serves over 4,000 customer workloads. Our very own Khash Sajadi recently presented a briefing on the Commons’ channel: Bringing Policy-As-Code into the Container Delivery Pipeline.
Here’s the link to the briefing (description follows). We hope you enjoy the briefing and do let us know if you have any questions or comments at hello@cloud66.com.
Containers bring Devs and Ops closer together, and at the pace of commits on a microservices app, that can be daunting to some IT Ops teams. A delicate balance is required between operational governance and developer freedom — and that balance needs to be automated. Now that they’ve put in place cutting-edge containerized infrastructure on the likes of OpenShift, how do IT Ops teams and dev managers ensure infrastructure and security policies are embedded into the deployment pipeline, in an easily-maintainable way, and without slowing down code? How do they avoid building custom technology for deployment, in a rapidly commoditizing world?
This session will walk through tools (open source and otherwise) developed by Cloud 66, which runs 4,000 customer workloads on Kubernetes, supported by over 2,000 lines of configuration. These tools enable teams to:
- secure images and manage secrets and IP in the build; — bring in configuration validation into the pipeline; — ensure fine-grained access control; and — complement CI tools with powerful multi-environment, multi-team deployment capabilities.
Originally published at blog.cloud66.com on August 22, 2018.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
31 
31 
31 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://medium.com/@Terren_in_VA/hybrid-cloud-storms-874498309bab?source=search_post---------131,"Sign in
There are currently no responses for this story.
Be the first to respond.
Terren Peterson
Dec 14, 2015·4 min read
Cloud computing was supposed to have multiple types — you remember, Public Cloud, Private Cloud, and Hybrid Cloud? Large-complex systems would likely fall in the Hybrid category, given that we would want to architect to take advantage of the best of both worlds. I can still remember how clear it looked…
Do you see it all together in the Cloud? Front-end, middleware, storage, all soaring to new heights in a brilliant scene. They all fit together into one, well integrated architecture, taking advantage of all available options.
Reality sets in…
Here’s what a Hybrid Cloud now looks like to me.
Two different weather conditions, with a harsh boundary between them. It’s usually being along the front where the turbulence kicks in, with lightning, hail, and wind sheer along it. You’d rather be on either side, just not in the middle, but sometimes the road takes you right into it.
How did we get here?
Every large enterprise has a unique set of infrastructure and vendors to work with, but I’m guessing that there are some common themes for how the Hybrid model is growing in many companies. For starters, the “Private Cloud” side of the equation evolved from a unique set of challenges, and was heavily influenced by past investments in infrastructure as well as licensing agreements. Over time, to optimize on-premise software license costs, databases and other back-end systems were consolidated onto shared infrastructure, with virtualization and partitioning technologies used to engineer a common utility. Recent investments may have incorporated self-service portals and automation that enable a database instance to be provisioned in hours or minutes, and the commonality of the infrastructure yields significant operating cost savings, providing a return on the investment. With automation and standardization, patching and version management becomes easier than it was before, and commercial licensing investments can continue to be reused and charged back to projects by PMO’s. Poof — a “Private Cloud” was born!
The “Public Cloud” side came up a different road, starting out with digital renovation and replacement projects where infrastructure was needed fast, and the case for leveraging ready made services was too compelling. Some new platforms were soon built on these new providers, but ecosystems needed to find boundaries between what already existed and what already worked, so interfaces to systems in existing data centers were built, and boundaries were cast. Using any Public Cloud provider requires adopting a model versus bringing existing processes along, and some engineering talent that are leading these efforts came from companies that never owned a datacenter, or had worked in these new eco-systems long enough that they forgot the old ways of doing things. Gradually Public Cloud implementations grew, and teams harnessed the automation and capabilities inherent with the vendor selected. Business units appreciated the transparency of costs, and enabled localization of choices to levels unheard of in traditional hosting models. As usage moved from skunk-works applications to high-volume applications, engineering skills deepened, and auto-scaling, multi-zone/region deployments became the norm. Of course there were always limits on scope for what a new project was willing to build, but rather than bring that that old database over, teams rationalized that they could just connect back to it.
Two models, two infrastructures, two skill-sets required, too complex.
It’s hard to see how agility can be sustained in this model as there aren’t many software engineers that can (or want to) span these two environments, so it assumes baking into platforms and organizations two silos that then need to work together. After all, isn’t the expectation of any new technology investment to deliver better agility than what was being replaced? Over time these infrastructure paradigms will continue to evolve down their own paths, and become very different looking organisms, and be constantly at odds. Managing the network connectivity seems to be the most difficult challenge, and potentially building a choke point where delivery schedules will crash into. What’s the value of a software-defined network in a Public Cloud if you can’t bind to the back-end services? How does high availability get built into these infrastructures if the fault tolerance, scaling, and other characteristics across the two are fundamentally different?
So what’s the Future of the Hybrid Cloud?
Potentially the Hybrid Cloud is really just a transitory state, one that allows renovation and investment cycles to eventually push through to an all-in approach with the Public Cloud. It’s not to say that there will not be Private Cloud enterprises, but the level of complexity organizations will be taking on in a year or two with this model will create incentives for finishing the move to external hosting, or pulling back on premise — not staying in a state requiring the two models to be integrated, but time will tell…
Technology enthusiast, Amazon Alexa Champion, EV enthusiast. Always learning how to make new things with the latest tech.
See all (378)
1 
1 clap
1 
Technology enthusiast, Amazon Alexa Champion, EV enthusiast. Always learning how to make new things with the latest tech.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/cloud-compliance-transparency-and-support-top-cloud-customer-s-concerns-7ee425b75dd9?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Oct 30, 2015·4 min read
Good news — the cloud spending and adoption are both on the rise. Global research firm Forrester forecasts the global public cloud market will reach $191 billion by 2020, up from $53 billion in 2013. However, according to a recent cloud study conducted by Forrester, 60% of cloud customers say that challenges with cloud compliance, transparency and support are preventing them from expanding their cloud use. HOSTING takes a look at what is causing the disconnect.
Forrester surveyed 275 infrastructure and operations (I&O) professionals in the U.S., the U.K. and Singapore who are responsible for selecting and maintaining relationships with their organizations public cloud infrastructure findings. Over 70% of respondents have been using cloud services for more than a year, but aren’t exactly “feeling the love.”
As we emphasized in the HOSTING webinar covering the Alert Logic 2015 Cloud Security Report, many cloud customers mistakenly assume that the security and compliance of their data in the cloud rests solely on the cloud service provider (CSP). In reality, it’s a shared responsibility between the customer and the CSP. However, compliance challenges restrict 63% of companies from expanding their cloud usage.
The Forrester survey also found that technology managers aren’t getting clear insights regarding cloud costs and performance metrics. Survey respondents reported that they receive incomplete metadata about their cloud workloads including compliance status, performance data, historical information, security data, and billing and cost metrics. What’s even more troubling is that thirty-nine percent paid for cloud resources that weren’t used.
Our webinar, Navigating the Cloud Migration Minefield, lists numerous speed bumps that companies can experience when moving data and applications to the cloud, without the expert guidance of an experienced CSP. While many of the survey respondents engaged with a CSP, fifty-one percent are not satisfied with their onboarding process because it took too long (21%) or it lacked human support (19%). Additionally, once their cloud migration was complete, the respondents experienced lingering support issues (19%) and higher-than-expected cloud support costs (20%). These issues are enough to keep 60% of respondents from expanding their cloud footprint.
Whether you’re new to the cloud, or an early adopter of it, you can take steps to ensure a successful cloud experience.
While compliance is a shared responsibility, make sure you understand exactly what resources and services the CSP provides in order to keep your data secure and in compliance.
Engage with a CSP that offers superior visibility in to metadata on workload performance, compliance and cloud costs. Evaluate the CSP’s native management tools to ensure they are user-friendly and provide you with the analytics and alerting capabilities your business requires.
Your organization should offer a designated support team that is well-versed in your business needs and cloud environment. Schedule a time to visit with your support team in order to ascertain their cloud knowledge and experience supporting your type of cloud environment. Find out how their performance is measured and if they use the Net Promoter Score (NPS). Finally, ask to see sample run books from their other customers.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
1 
1 
1 
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/20ms/the-cloud-lock-in-conundrum-26d808571cae?source=search_post---------133,"There are currently no responses for this story.
Be the first to respond.
One of the biggest barriers to public cloud adoption has always been the danger of vendor lock-in. Seasoned IT professionals who’ve witnessed IT industry practices across successive generations of technology are all too aware of the trap. A vendor reels you in with bold promises of increased efficiency, agility and convenience, but once you’ve made considerable investments in its platform, technology stack or ecosystem you may well wake up one day and realize you’re locked in. What can be done to prevent this?
With the cloud, though, many businesses are determined not to get caught in the same trap. As we note in our white paper on Reducing The Risk Of Cloud Supplier Lock-In, 78% of IT professionals have been put off from taking full advantage of the public cloud because of wariness about being locked into a particular cloud supplier.
Typically, those fearful of supplier lock-in have tried to avoid providers’ platform-as-a-service (PaaS) offerings. Instead, they have tended to use just basic infrastructure (IaaS) services, while maintaining an independent cloud platform — often using open source databases, platforms and tools such as Apache Hadoop, OpenStack, MongoDB, etc.
In the paper, we give a series of helpful tips about how to minimize the risks of lock-in. These include:
But is lock-in always a bad thing? No — it very much depends on what your business is trying to achieve and the level of in-house technical expertise you have. Sometimes, adopting a cloud supplier’s proprietary platform and services wholesale is the most sensible strategic choice.
Given that digital transformation is all about gaining the agility to become ever more responsive to market needs, for some customers the fully-integrated, increasingly easy-to-use platforms, tools and services offered by leading public cloud providers such as Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP) make perfect sense. For what should be a fair premium, they allow a business to build and scale the applications and services it needs quickly and easily, using the latest technologies, without the need for capital expenditure or additional in-house technical expertise and resources.
The ongoing competition among the leading players is likely to mean that they will all continue to invest in, improve and add value to their cloud platforms, refining features and services for the foreseeable future. And since customers are increasingly demanding open APIs and ever clearer pricing structures, many believe that’s the market-leaders most likely direction.
So what types of businesses might suit a lock in? Or, at least, what business will opt for a proprietary cloud platform or ecosystem) Is it ever logical?
By carrying out the kind of planning and due diligence we outline in our aforementioned white paper, you can still mitigate the most potentially damaging risks of lock-in even when you choose to throw your lot in with a single supplier.
But if fear of lock-in is still holding back your cloud ambitions, it may be worth asking what’s the greater risk — dependence on a single supplier, or falling behind your increasingly agile, cloud-enabled competitors? That’s quite a conundrum.
Originally published at blog.100tb.com.
Analysis, predictions and views on technology and business
1 
1 clap
1 
Written by
100TB is an innovative hosting provider supplying cutting edge infrastructure, high speed bandwidth & services to our clients globally. Support: @100TBHelp
Analysis, predictions and views on technology and business
Written by
100TB is an innovative hosting provider supplying cutting edge infrastructure, high speed bandwidth & services to our clients globally. Support: @100TBHelp
Analysis, predictions and views on technology and business
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/free-courses-on-cloud-computing-7e3c61a4375a?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 22, 2019·3 min read
As a global cloud provider and NO.1 public cloud in Asia, Alibaba Cloud will provide you with the right tools and resources to build your future career.
You can get Alibaba Cloud Elastic Compute Service (ECS) for only $0.99 for an entire year. That’s up to $687.93 in savings. In addition, you can claim 10 training courses and receive 50GB/month data transfer voucher for 12 months.
You can click here to get more details. Here are some introductions below.
Why is this event worth attending?
ü Get a head start on your career in the cloud. With access to our select Elastic Compute Service (ECS) for only $0.99 and ten free training courses, Alibaba Cloud is here to make your journey to cloud computing more accessible.
ü Upon successful verification, college educators will receive access to discounted Alibaba Cloud Elastic Compute Service (ECS) along with 15 Apsara Clouder training courses. More coming soon.
ü The Alibaba Cloud University Cooperation Program (AUCP) is a global education program aiming to support schools and students around the world to gain access to Alibaba Cloud resources and nurture future talents in the cloud computing, cloud security, and big data industries Learn more.
What are the rules?
1. For now, this offer is exclusive to university students from selected countries & regions after successful verification of enrollment. Supported countries and regions are USA, Canada, Mexico, Brazil, Germany, Poland, France, Australia, New Zealand, Hong Kong, Taiwan, Japan, Philippines, UK, Turkey, and Thailand. We are working on extending the program to more countries and regions in the coming months.
2. Identity verification is required before verifying one’s education educational status if 1) The names on the credit card associated with your account do not match your enrollment record or 2) PayPal is your preferred method of payment.
3. Student Verification Tips:
• Each account can submit the verification request twice.
• Please use the same name in identity verification (if you are asked to do so as above-mentioned) and student verification.
• If you are not able to request verification, you may not be eligible for this program.
• If you already uploaded supporting documents and are not approved, you can also send an email for details of the disapproval or request for further review.
• If you have an enterprise account, you are not eligible to request for student verification.
4. The offer is limited to 1 server per student. The offer includes:
• $0.99 for select prepaid ECS servers on a yearly subscription. The supported instance types might be changed time to time.
• 10 free clouder certification courses for students and 15 free clouder certification courses for educators. Regular, non-discounted prices apply for subsequent course.
• 50GB data package for eligible students for 12 consecutive months from the date of activation
How to join the education program?
Step 1: Create an account
Register an Alibaba Cloud account and add a method of payment. It is recommended to add your credit card for faster processing time. No upfront cost required.
Step 2: Verify you’re a student
Submit your application to verify your student enrollment status. You may have to take an extra step to verify your identity if your credit card information does not match your enrollment record.
Step 3: Get student benefits
Purchase ECS cloud server for $0.99 and enjoy the free training course. You will also receive 50GB data transfer per month for 12 months after purchasing your ECS instance.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@nutanix/real-time-cloud-security-compliance-7e051c66882a?source=search_post---------135,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Aug 14, 2019·2 min read
by Sahil Bansal
In recent months several data breaches have highlighted the increased security risk in public cloud and multi-cloud deployments. However, these security risks do not always arise because of anything inherently insecure about the cloud, rather because of using the cloud insecurely. Gartner predicts that through 2022, at least 95% of cloud security issues will arise because of user errors leading to misconfigured resources causing security vulnerabilities.
Earlier this year, the Dow Jones revealed that more than 2.4 million financial records were leaked because one of their AWS Elasticsearch instances was not password protected and also allowed public access. In another incident, more than half a billion people’s personal Facebook data was found on S3 storage buckets with global read/write permissions owned by a third party called Cultura Collectiva.
Organizations that are embracing public cloud and multi-cloud architectures really do need to re-evaluate their cloud security strategy and focus on building automated processes that not only detect security risks in real-time but also help to fix them. Security admins also need to ensure their cloud environments meet regulatory compliance standards such as PCI-DSS, HIPAA, GDPR, etc. This is where Xi Beam can help you significantly improve your overall cloud security posture.
Xi Beam helps you gain real-time visibility and control over your multi-cloud security health by automating 250+ security audits based on industry best practices and Beam’s recommendations to improve your cloud infrastructure security. You can also create your own custom audits to meet your specific security compliance needs.
Beam also lists out the exact steps needed to remediate any security issues that may have been detected. Also, for several security audits, Beam helps you to immediately improve your cloud security by remediating security vulnerabilities with just one-click!
Finish the blog by clicking here: https://www.nutanix.com
© 2019 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned herein are registered trademarks or trademarks of Nutanix, Inc. in the United States and other countries. All other brand names mentioned herein are for identification purposes only and may be the trademarks of their respective holder(s).
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
See all (807)
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-object-storage-vs-ibm-cloud-and-oracle-cloud-equivalent-ef62198f2dfa?source=search_post---------137,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 2, 2018·5 min read
Most public cloud providers provide object storage for general-purpose data storage. Object storage is an ideal storage solution for use in the cloud, because it is designed for massive scalability, high reliability, and low cost. Object storage works by placing data into one or more storage buckets, which can then be accessed programmatically.
Nearly all public cloud providers offer object storage to their subscribers, including IBM, Oracle, and Alibaba Cloud. In this article, we compare the object storage offerings of each of these cloud providers.
The IBM cloud defines four different classes of object storage. The first of these classes is simply called Standard. This is IBM’s basic, run-of-the-mill object storage, and is intended to be used for general- purpose, active workloads.
The second storage class defined by IBM is called Vault. As its name implies, Vault is intended for use with data that is accessed infrequently. Unlike Standard storage, IBM imposes a charge for accessing data residing within Vault storage.
IBM’s third class of object storage is Cold Vault. Cold Vault is similar to Vault storage, in that it is intended for use with data that is not frequently accessed, but the assumption is that data within Cold Vault storage exists primarily in an archival state and will rarely, if ever be accessed. The cost of retrieving data from Cold Vault storage is greater than that of retrieving data from Vault storage.
The fourth and final type of object storage defined by IBM is Flex storage. Flex storage is designed for use with dynamic workloads whose data access patterns are difficult to predict.
Regardless of which class of object storage is being used, IBM uses an Information Dispersal Algorithm (IDA) that is similar to erasure coding as a mechanism for determining data placement. This algorithm is designed to ensure both the security of the data and the reliability of data retrieval. The algorithm is based on spanning data across a number of nodes. For the sake of security, none of the nodes contain a complete copy of the data. At the same time, however, redundancy is implemented in a way that ensures that not all nodes have to be online in order to retrieve the data. The individual nodes communicate with one another using TLS, and an All or Nothing Transform algorithm prevents data from being disclosed if any of the nodes are found to be compromised. Clients communicate with object storage using HTTPS communications.
Like IBM, Oracle has designed its object storage to achieve reliability through redundancy. The service, which is designed for 99.95% availability, spans data across multiple storage servers, and across availability domains. Although Oracle does not seem to provide a reliability metric, Oracle uses checksums to monitor data integrity, and offers a remediation mechanism that can automatically correct any data that is found to be corrupt.
Access to Oracle’s object storage is based on Oracle Cloud’s Infrastructure Identity and Access Management, which ensures that data is only accessible to authorized users who have been properly authenticated. Connectivity to Oracle’s object storage is based on the HTTPS protocol, which ensures that data access is SSL-encrypted. Additionally, Oracle encrypts data at rest using the AES-256 encryption algorithm.
Alibaba Cloud’s Object Storage Service (OSS) is designed to provide the best possible availability, security, and performance, while also being easy to integrate with existing workloads and third-party applications. The service guarantees 99.9% availability, and 99.99999999% reliability. In fact, Alibaba Cloud stores three complete copies of each object.
Clients access OSS using HTTPS-encrypted connectivity, and Alibaba Cloud also provides encrypted cloud storage to protect data at rest. Furthermore, the company has implemented defensive in-depth strategies to guard against things like Distributed Denial of Service (DDoS) attacks and unauthorized logging access.
Perhaps the most compelling reason to use Alibaba Cloud OSS is its raw performance. The service is rated to handle more than 50,000 requests per second, and supports multiple, simultaneous read/write requests. Within individual regions, Alibaba Cloud uses a Border Gateway Protocol multi-line network access system to achieve the best possible performance. In fact, the service is specifically designed to allow new content to be appended to an existing object, which makes it possible to playback a video even if the file has not yet been completely written.
Alibaba Cloud provides multiple integration methods for its cloud-based object storage. As you would probably expect, the object storage integrates seamlessly with other Alibaba Cloud services such as the Content Delivery Network (CDN) and the Elastic Compute Service (ECS). It is also possible to link web applications (including mobile web applications) to the service by using RESTful APIs, SDKs, and client tools. Alibaba also provides a dedicated console for accessing and monitoring object storage.
It is also worth noting that Alibaba Cloud offers automated data lifecycle management capabilities for object storage. Policies can be set that will automatically purge aging data, or transfer that data to lower-cost storage.
Alibaba Cloud is currently offering a free trial of their cloud services. You can register for a free trial subscription at: https://www.alibabacloud.com/campaign/free-trial#free-products. The trial includes a $300 credit that can be used to explore Alibaba’s various cloud offerings.
Although almost all cloud providers offer an object storage solution, not all object storage platforms are created equally. When evaluating your options, it is important to consider performance, reliability, security, and any extra features such as automated data lifecycle management.
Brien Posey is a Fixate IO contributor, and a 16-time Microsoft MVP with over two decades of IT experience. Prior to going freelance, Brien was CIO for a national chain of hospitals and healthcare facilities. He also served as lead network engineer for the United States Department of Defense at Fort Knox. Brien has also worked as a network administrator for some of the largest insurance companies in America. In addition to his continued work in IT, Brien has spent the last three years training as a Commercial Scientist-Astronaut Candidate for a mission to study polar mesospheric clouds from space. You can follow Posey’s spaceflight training at www.brienposey.com/space
Reference:
https://www.alibabacloud.com/blog/Alibaba-Cloud-Object-Storage-vs-IBM-Cloud-and-Oracle-Cloud-Equivalent_p293464?spm=a2c41.11204207.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/gartner-blockchain-cloud-service-report-alibaba-cloud-ranks-2nd-globally-d15456f9b813?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 22, 2019·3 min read
Recently, Gartner, a famous research institution, published the blockchain cloud service report for the world leading public cloud service providers. Alibaba Cloud, as the only Chinese technology company on the list, won the highest score in 6 judging standards and ranked 2nd place overall. Alibaba Cloud was placed only after Microsoft Azure, and is ahead of other prominent cloud providers such as AWS and Google.
The judging criteria of Gartner cover many aspects, including the fundamental blockchain technologies, infrastructure, availability, scalability, interoperability, security, cloud service integration, and application development. Alibaba cloud got the highest score in 6 indexes.
According to the Gartner analyst, the blockchain service of Alibaba Cloud can smoothly integrate with its cloud services, and provide value-added services to its customers at the same time.
Alibaba Cloud Blockchain-as-a-Service (BaaS) is positioned “to focus only on the infrastructure and not to develop upper-layer applications.” Alibaba Cloud BaaS is focusing on improving its ability to be integrated by other enterprises. Currently, Alibaba Cloud BaaS covers lots of products and services, including the enterprise-level BaaS service, the agile BaaS platform that supports private deployment, and blockchain solutions for container services. Alibaba Cloud BaaS has been widely used in many scenarios such as traceability of commodities, blockchain finance, and data asset transaction management.
The Gartner report further pointed out that Alibaba Cloud BaaS supports three mainstream blockchain technologies: the Ant Blockchain, Hyperledger Fabric, and Quorum, which offers flexible choices to enterprises. BaaS is the first product that applies the Intel SGX security technology into Hyperledger Fabric. This allows Alibaba Cloud BaaS to meet financial-grade security standards in various aspects, such as secure and compliant data centers, high-level security protection, and secure and flexible Internet connection.
Alibaba Cloud has invested a lot in blockchain technology development. In 2017 and 2018, Alibaba Cloud ranked 1st place in terms of the number of blockchain patents. In Aug 2018, Alibaba Cloud, together with Alipay, announced their investment in the construction of the blockchain infrastructure. Currently, the two parties have jointly served customers across many industry sectors, including government, retail, manufacturing, finance, Internet, media, and health care. For example, Alibaba Cloud and Alipay jointly developed a medical care blockchain solution, which has been successfully applied to the electronic prescription management of Central Hospital of Wuhan. This solution allows the hospital to implement automatic management of the entire prescription lifecycle, including prescription issuance, prescription review by pharmacist, drug delivery, drug payment, and process supervision.
Reference:https://www.alibabacloud.com/blog/gartner-blockchain-cloud-service-report-alibaba-cloud-ranks-2nd-globally_594806?spm=a2c41.12883559.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@renebuest/cloud-data-fabric-enterprise-storage-services-in-the-cloud-2d3b9e3d94bf?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 5, 2016·2 min read
The maturing public cloud infrastructure gains increasing importance as an attractive alternative to on-premise enterprise IT infrastructure. Public cloud infrastructure offer an abundance of possibilities to CIOs when it comes to operating existing infrastructure and application environments more flexibly and at a lower cost. However, the existing public cloud storage services have been developed with a focus on the new generation of applications. This is why they are less well prepared to run existing enterprise applications. At present, the requisite storage concepts, standards and technologies for use of legacy enterprise applications on public cloud infrastructure are still not available. In order to ensure the continued existence as well as the proper operations of conventional application architectures on public cloud infrastructures, it is necessary to transfer the established and widespread standards into the public cloud. As legacy applications still constitute the lion‘s share of potential cloud migration candidates for new types of cloud native applications, well-known storage concepts are required to ensure the migration of existing applications without modifications to a public cloud infrastructure, where they continue to be operated.
In the strategy paper “Cloud Data Fabric”, Crisp Research analyses and explains the different enterprise storage options in the cloud and illustrates how to establish storage management with public cloud infrastructure.
The strategy paper can be downloaded free of charge under “Cloud Data Fabric — Enterprise Storage Services in the Cloud“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@alibaba-cloud/a-new-milestone-for-alibaba-clouds-container-and-serverless-services-903c59d3d11e?source=search_post---------140,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 28, 2021·7 min read
By Aliware
Recently, Gartner released the “Competitive Landscape: Public Cloud Container Service Market 2021” report. Through this report, Alibaba Cloud became the only Chinese enterprise to be listed for three consecutive years.
According to Gartner, container services have become mature and entered the mainstream market. More diversified container implementation scenarios in enterprises have emerged, and many container vendors have begun mergers and acquisitions. In order to provide container services to more industries, Gartner suggests that vendors should enrich various enterprise-level application image markets. Vendors should also increase support for Application Platform as a Service (aPaaS) and implement various cloud deployment and management. In this report, with the exception of traditional dimensions such as Kubernetes support, container images, Serverless containers, and service mesh, the cluster deployment and management are added for assessment.
Alibaba Cloud is one of the first cloud service vendors in China to provide container services. Alibaba Cloud provides ACK Pro and ACR EE for enterprise-level large-scale production to meet the requirements for reliability and security. Alibaba Cloud also provides services for Serverless containers, machine learning, edge computing, and distributed clouds, which have been applied to enterprises in Internet, new retail, education, healthcare, and other industries.
Serverless technology has become a hot topic in recent years. The main aim of serverless is to help developers reduce workload by minimizing routine code development tasks, such as for server O&M, for cloud applications. Through serverless, developers can focus more on creating more value for businesses. At the same time, serverless technology can also improve the startup speed of applications significantly.
Apart from Function Compute (FC) for event-driven applications and Serverless Application Engine (SAE) for microservice applications, Alibaba Cloud also provides Serverless container products called Alibaba Cloud Serverless Kubernetes (ASK). ASK provides a standard Kubernetes interface and extreme elasticity for users without O&M requirements. Users can conveniently create Serverless clusters without the need to manage Kubernetes nodes and servers. Users do not need to carefully plan the capacity or care about the underlying servers.
Pods in ASK clusters run in secure and isolated container environments based on Elastic Container Instance (ECI). The underlying compute resources of different ECIs run independently as they are completely isolated by lightweight virtual sandboxes. In addition, based on the Serverless container architecture deeply optimized by Alibaba Cloud, ASK clusters are provided with great elasticity without being limited by the computing capacity of cluster nodes. Currently, ASK clusters can start 500 pods in 30 seconds, and a single cluster can provide a capacity of 20 KB for each pod.
Prudencemed, a mobile medical device developer, uses Serverless containers to improve the prediction accuracy of AI models to 99.99%. Weibo uses Serverless containers to implement auto scaling of business, reducing the end-to-end scaling time by 70%.
Gartner believes that container technology and its surrounding ecosystems are maturing and can be used in new application scenarios, such as machine learning and edge computing.
Alibaba Cloud container services have been implemented in cloud native AI scenarios for the past few years. Alibaba Cloud has provided many tools, services, and solutions for heterogeneous resource management, AI application lifecycle management, AI task scheduling and elasticity, AI data management and acceleration, and unified AI-big data orchestration. It has helped many customers improve the production efficiency of AI engineering, optimize GPU resource utilization, and accelerate AI platform construction. In addition, Alibaba Cloud promotes the development of the Kubernetes community towards large-scale data computing fields, such as AI and big data.
TAL, an online education enterprise, deploys a large number of important AI+ education businesses through Alibaba Cloud container services. By doing so, TAL has accumulated over 100 AI capabilities and over 10 AI solutions for education scenarios in terms of images, audios, and natural language processing, covering all teaching stages. With these capabilities and solutions, TAL has improved the overall AI development and deployment efficiency and realized efficient MLOps. In addition, TAL has solved the problem of low resource reuse rate in AI service scenarios requiring high concurrency, high performance, and high resources. The stability of the platform reached as high as 99.99%.
At the Alibaba Cloud Computing Summit 2021 in Shenzhen, Alibaba Cloud announced the cloud-native AI solution. This solution provides several features such as underlying resource O&M and optimization, multi-tenant quota management, AI task management and intelligent scheduling, big data service integration, and support for AI engine acceleration and algorithm application at upper layers. At the same time, it provides a visual interface, command line tools, and SDKs, which are simple and easy for extension and integration. AI service providers, including data scientists, AI algorithm engineers, and AI platform builders and operators, can freely use these features to customize their own AI platforms based on Kubernetes.
Traditional cloud computing centers adopt a centralized storage and computing model, which cannot meet the needs of edge devices for timeliness, capacity, and computing power. Therefore, building edge infrastructures has become a new topic.
The cloud-native architecture of cloud-edge-device collaboration has been developed. On the cloud, the cloud-native management and product capabilities are retained. A large number of edge nodes and edge businesses have become the workloads of the cloud-native system. To achieve the integration of the business, O&M, and ecosystem, edge cloud native technology provides better service governance and traffic control capabilities, unified application management and O&M, and higher isolation, security and efficiency.
In 2019, Alibaba Cloud released ACK@Edge, the first container product for edge computing in the industry. ACK@Edge features cloud standard management and edge moderate autonomy. On the cloud, it offers powerful cloud-native management capabilities, which can achieve business integration such as CDNPaaS and IoTPaaS. Cloud-edge O&M and management can also be implemented through multi-specification and multi-link solutions. On the edge, based on native Kubernetes capabilities, ACK@Edge provides capabilities such as edge autonomy, unitized management, traffic topology, and fine-grained detection of edge computing power.
SUNAC group builds a digital and intelligent customer service platform for community customers based on ACK@Edge. On this platform, SUNAC uniformly manages intelligent devices in community scenarios such as gateway systems and information screens, and in home scenarios such as TV, smart air conditioners, and Tmall Genie. For example, SUNAC achieves the all-in-one management of community parking lots and enables intelligent collaboration based on cloud-edge-device unification. Youku uses ACK@Edge to uniformly manage a dozen regions and numerous edge nodes on the public cloud. Through elastic scaling, Youku saves machine costs by more than 50% and reduces device-to-device network latency by 75%.
After the significant changes in the competitive landscape for container management in 2020, mainstream cloud service providers have released container management solutions of distributed clouds. Gartner predicts that 81% of enterprises will adopt the multiple/hybrid cloud strategy. “Most enterprises adopt the multi-cloud strategy to avoid supplier locking or using the best solution.” This strategy ensures the purchase agility of enterprises, and balances availability, performance, data sovereignty, regulatory, and labor costs.
Alibaba Cloud has provided hybrid cloud support for ACK since September 2019. In September 2020, ACK upgraded the management capability of distributed cloud applications. In addition to Kubernetes clusters on Alibaba Cloud, users can also manage Kubernetes clusters in their own IDC and on other clouds. Thus, centralized cluster management, IT governance, security protection, application management, and backup management can be realized. This year, Alibaba Cloud also released unified delivery capability of application centers for ACK. Developers can use GitOps to publish applications to different cloud environments in a secure, consistent, and stable manner. This ensures that the whole process is secure and controllable, and improves the efficiency and stability of application delivery.
ACK can be used in combination with Alibaba Cloud Service Mesh (ASM). As the first Istio-compatible hosted service mesh platform in the industry, ASM provides unified capabilities for services running on different computing infrastructures. Through traffic control, mesh observation, and inter-service communication security, ASM enables features such as service nearby access, failover, and gray release. ASM can simplify service management and be applied to Kubernetes clusters, Serverless Kubernetes clusters, Elastic Compute Service (ECS) instances, and clusters created by users.
ASM insists on developing “hosted, standard, secure, stable, simple, and scalable” products. Currently, it has covered 12 regions and maintains the leading position in product integrity and market share. ASM has been used in the largest mesh cluster in the industry and core businesses such as e-commerce and DingTalk of Alibaba. In addition, it also continues to rank first in terms of number of customers for public cloud, covering online education and vocational training, e-commerce, logistics, supply chain, gaming, Internet of vehicles, and IoT.
ASM decouples service governance from business and submerges to the infrastructure layer, so that business development focuses more on the business. By doing so, in a standardized way, ASM solves the challenges of multi-language multi-programming framework and service governance in the microservice software architecture. In ultra-large scale application scenarios, ASM ensures system stability, improves observability and diagnosability, and improves service governance capabilities and performance.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@datapath_io/5-things-you-need-to-know-about-multi-cloud-1b3f5510b0ad?source=search_post---------141,"Sign in
There are currently no responses for this story.
Be the first to respond.
Datapath.io
Mar 12, 2018·3 min read
Global Cloud adoption has been growing at a mind-bending pace. Forrestor predicts the global public cloud market to hit $ 178 Billion in 2018.
However, companies moving to the cloud still tend to keep a part of their infrastructure rooted in their on-premise data centers, as seen in the 3 times increase in hybrid cloud adoption.
This increase in hybrid cloud adoption can be seen as a middle ground for companies looking to leverage the cost benefits and scalability of the cloud, but not yet willing to put all their eggs in the cloud basket.
As companies grow more confident in the security procedures of public cloud providers and realize the huge cost benefits of leveraging cloud technologies, multi-cloud configurations are expected to overcome hybrid cloud architectures.
A multi-cloud approach is one which leverages more than one cloud service providers. These cloud providers can be either private or public.
Public cloud providers refer to any of the hyperscalers like AWS, GCP or Microsoft Azure. Customers of public providers share the underlying resources. Private cloud is differentiated by the fact that underlying resources are dedicated to a single customer.
Hybrid cloud approach ties in a public or private cloud deployment with an already existing on-premise infrastructure. Hybrid cloud has emerged as the go-to strategy for companies embarking on their cloud journey.
Multi-cloud architectures are the logical conclusion to a company’s cloud journey. As companies leverage cloud technologies, distributing cloud resources over multiple cloud providers, allows them to realize the benefits of increased cost saving, scalability and disaster recovery.
Multi-cloud approach caters to a number of use-cases including disaster recovery, cost savings and performance.
Below are some of the use-cases that a multi-cloud approach covers:
Distributing cloud workloads across different cloud providers allows companies to build resilient and robust applications. This ensures continued operations during outages and failures across cloud providers.
Not all cloud providers are born equal. The same applies to the individual services they provide. Cloud services can be priced differently by several cloud providers. Companies looking to optimize cloud usage costs can choose services across cloud providers based on their individual price points by implementing a multi-cloud approach.
In the same way that cloud providers have differing costs, individual services also differ in how well they suit a company’s needs. Deploying multi-cloud workloads enables companies to pick and choose services based on their unique requirements.
Multi-cloud architectures also allow companies to reduce their dependence on any one single cloud provider.
Multi-cloud approach also adds an additional layer of scalability on top of public cloud service providers and allows companies to provision resources as and when application and traffic requirements demand.
Multi-cloud approach also enables companies to meet compliance requirements. Not all cloud providers have a presence in markets where companies are required to keep a local presence. Leveraging multiple data centres from several cloud providers enable them to meet these requirements.
By leveraging regional presence of multiple cloud providers companies are able to push their applications nearer to end-users allowing them to provide a more reliable and optimized level of performance.
A multi-cloud approach while benefiting companies does introduce multiple connectivity challenges.
Ensuring secure and reliable connectivity between several cloud data centers is mostly a manual process and requires considerable up-front costs. These solutions also tend to be unreliable in terms of performance and high availability.
Datapath.io’s cloud to cloud network provides companies, a super easy way of deplying secure and dedicated connectivity between multi-cloud workloads. It includes an enterprise grade IPsec VPN network deployed over a global MPLS backbone.
The cloud to cloud network leverages native cloud tools for automation, connectivity and encryption to provide a friction less multi-cloud connectivity solution.
Originally published at datapath.io on March 12, 2018.
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
"
https://medium.com/cloud-simplified/examining-azure-reserved-instances-one-piece-of-the-cloud-expense-puzzle-928bdc176379?source=search_post---------142,"There are currently no responses for this story.
Be the first to respond.
While Amazon Web Services (AWS) continues to dominate the public cloud market, later entrants have been gaining ground with Microsoft Azure doubling its market share last year to 13%. Azure’s increasing momentum is due, in part, to new services and consumption models including Reserved Instances (RIs) that were announced at Microsoft Ignite last year. Let’s do a quick rundown of the highlights from the RI announcement.
Similar to AWS RIs, Azure RIs allow users to buy reserved Virtual Machine (VM) instances for one or three year periods. The advanced purchase of the entire time period results in a much lower hourly rate — up to 72% cheaper than on-demand pricing, depending on the region and instance series. You can also save up to a whopping 80% If you qualify for Azure Hybrid Benefit.
For now, Azure requires users to pay the full amount up-front for RIs. This makes them a great option for predictable and consistent workload needs. It makes sense to use RIs for workloads that run 24/7, rather than those that run intermittently, because you’ve already paid for the entire time period. RIs are strictly about pricing and do not affect the runtime state of your VMs. For customers with an Enterprise Agreement, Microsoft deducts the reservation cost from existing monetary commitments.
RIs are available for either Windows or Linux operating systems, and you can use RIs for VMs, Azure Batch service (for large-scale parallel high-performance computing batch jobs) and VM scale sets.
However, there are some restrictions. Azure RI is not available for every type of VM or in every country.
The following VM types are not available for RI, as of August 2018:
As for geographical restrictions, the reserved instance for the pay-as-you-go subscription is not available in Brazil, India, China, Taiwan, Russia, Korea, Argentina, Hong Kong, Indonesia, Liechtenstein, Malaysia, Mexico, Saudi Arabia, South Africa and Turkey.
RIs provide predictable pricing, prioritized compute capacity and some flexibility for exchanging or canceling reservations, should your business needs change.
Managing Azure Reservations
Planning what to reserve is a complex process particularly when you have multiple subscriptions across regions. To further complicate things, It is also necessary to perpetually monitor and manage RIs for optimal usage. Many conditions can change such as your workload size, VM types, new VMs made available that may be more cost effective, etc. You may have RIs that are unused or underutilized, and you may have to keep an eye on RI renewals. A lot of similar factors determine the profitability of your RI investment.
Azure provides a range of management options to assist you in this process.
Selecting reservation scope
You have two options when it comes to the scope of RIs — shared or single subscription. The shared option makes the RI available to all subscriptions in your enterprise agreement which increases the likelihood that the RI will get used. Azure also allows you to change the scope of the reservation.
Splitting a single reservation
Once you have more than one resource instance in a reservation, it may be necessary to assign instances within that reservation to a different subscription to ensure optimal utilization. For example, if you purchased 7 RIs of D2 VMs and specified the scope for subscription A but noticed over time that 4 of those RIs are underutilized, you can split your reservation and assign some of those RIs to subscription B for better utilization.
Instance-size flexibility
Azure Reserved VM Instances have instance size flexibility which automatically applies RI savings to other VMs within the same region and the same RI group. This feature can apply your RI purchases to VMs that are not currently being used.
For example, if you purchase a D8s_v3 RI in the East US region but are not running the VM, instance size flexibility applies the 8-cores RI benefit to other Ds_v3 VMs running in the same region. The RI benefit could take the form of two 4-core VMs (D4s_v3) or half the cost of a single 16-core VM (D16s_v3).
However, the reservation discount doesn’t apply to VMs in different size series groups. Continuing with the above example, reservation discounts from the DSv3 series VMs do not apply to VMs in the DSv2-series. Moreover, within the size series group, the number of VMs to which the reservation discount applies depends on factors such as the VM size chosen at the time of RI purchase and the VM sizes that are running.
Exchanging an RI
Azure allows users to exchange RIs at any time. All RI features can be changed including instance family, size, geo-region and term. The remaining time is credited to the next new RI purchase. The new RI purchase must be of equal or greater value than the prorated amount and for a full 1-year or 3-year term.
Cancelling an RI
You can cancel at any point in time during the reservation period with a 12% early termination fee along with a $50,000 refund cap per calendar year.
Under the right conditions with careful planning and ongoing management, Azure RIs offer users a way to significantly reduce their public cloud TCO.
Reserve Instances are, of course, just one piece of an ever-growing cloud expense puzzle. A puzzle that frequently involves multiple public and private clouds that are part of a hybrid enterprise cloud strategy designed to accommodate all workloads at the right time and place.
Nutanix Beam removes the guesswork from this process with machine intelligence driven recommendation algorithms that analyze workload patterns and continuously suggest optimal purchasing decisions. Beam supports both AWS and Azure cloud providing seamless cloud services expense management.
Beam also provides security and compliance for multi-cloud environments, with one-click remediation for cloud vulnerabilities, generating insights into cloud compliance and security vulnerabilities in real time so that you can resolve potential threats before they become business problems. Head here to give Beam a try today!
Disclaimer: This blog may contain links to external websites that are not part of Nutanix.com. Nutanix does not control these sites and disclaims all responsibility for the content or accuracy of any external site. Our decision to link to an external site should not be considered an endorsement of any content on such site.
© 2018 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned herein are trademarks of Nutanix, Inc., registered in the United States and other countries.
Certify and maintain cloud security compliance for HIPAA, ISO, PCI-DSS, CIS, NiST and SOC-2 by enforcing standard or custom policies
Originally published at www.nutanix.com on September 21, 2018.
An inclusive approach to cloud management.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
"
https://medium.com/@nutanix/full-recap-of-the-surfeit-of-announcements-at-aws-re-invent-2016-a106f60950b6?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Dec 4, 2016·5 min read
AWS, yet again, reigns the public cloud arena from all dimensions with a plethora of new announcements at AWS re:Invent 2016. With digital disruption treading in every vertical across the globe, AWS has now set a stage for itself to be a key part of this evolution. And the congregation, which had over 35,000 attendees, witnessed 25+ new announcements and 10+ updates by Amazon vice president and chief technology officer Werner Vogels. And to you, we present here a cool zippy cheat sheet of all the new & updated epoch-making-announcements at 2016 AWS re:Invent:
For Compute
Amazon EC2 C5: A new powerful compute optimized instance featuring the highest performing processors and the lowest price/compute performance in EC2. Read More »
Amazon EC2 Elastic GPUs: The debutant attachable Elastic GPUs, which are cost effective and provide a flexible way to add graphics acceleration to Amazon EC2 Instances. Read More »
Amazon EC2 F1: A compute instance with programmable hardware for application acceleration. Read More »
Amazon EC2 I3: The latest generation of storage optimized high I/O instances, featuring NVMe based SSDs for the most demanding I/O intensive relational, NoSQL, transactional, and analytics workloads. Read More »
Amazon EC2 R4:The latest generation of memory optimized instances that are 20% more price performant over R3 instances.Read More »
Amazon EC2 T2: Available in t2.xlarge and t2.2xlarge are the newest Amazon EC2 burstable-performance instances. Read More »
Amazon Lightsail: Helps launch and manage a virtual private server with AWS starting at $5/month. Read More »
AWS Batch: Enables developers, scientists, and engineers to run hundreds of thousands of batch computing jobs on AWS. Read More »
Amazon EC2 Systems Manager: Helps automate key management tasks like collecting system inventory, applying OS patches, automating image creation, and configuring OS and applications at scale. Additionally, users can record and govern instance’s software configuration with AWS Config.
C# support on Lambda: Enables use of C# with AWS Lambda. Read More »
Lambda@Edge: Allows developers to deliver a low latency user experience for customized web applications by enabling them to run code at CloudFront edge locations without provisioning or managing servers. Read More »
For Management
AWS Personal Health Dashboard: Provides a personalized view of AWS service health.
AWS OpsWorks for Chef Automate: Provides a fully managed Chef server and a suite of automation tools for continuous deployment, automated testing for compliance, and a user interface for visibility into nodes. Read More »
AWS Organizations: Helps IT teams to manage multiple AWS accounts. Read More »
For Security
AWS Shield: A managed DDoS protection service that safeguards web applications using Elastic Load Balancing (ELB), Amazon CloudFront, and Amazon Route 53. Read More »
For Migration
AWS Snowmobile: An exabyte-scale data transfer service used to move extremely large amounts of data. Read More »
For Containers
Blox: A collection of open source software that enables customers to build custom schedulers and integrate third-party schedulers on top of ECS. Read More »
For Database
Amazon Aurora with PostgreSQL Compatibility: Get up to twice the performance of the typical PostgreSQL database and the features you love in Amazon Aurora. Read More »
For Developers looking for Tools
AWS CodeBuild: Helps build and test code in the cloud as you scale. It can be used with other AWS services; and it is also integrated with AWS Elastic Beanstalk to enable testing of Elastic Beanstalk apps. Read More »
AWS X-Ray: Helps developers analyze and debug production, distributed applications, such as those built using a micro services architecture. It also shows how the underlying services are performing, so that you can identify and troubleshoot the root cause of performance issues and errors. Read More »
For Hybrid Environment
AWS Greengrass: Helps run IoT applications seamlessly across the AWS cloud and local devices using AWS Lambda. Read More »
AWS IoT Button: For improved developer experience. Read More »
AWS Snowball Edge: A petabyte-scale data transfer service with on-board storage and compute. Read More »
VMware on AWS Cloud: Helps run VMware workloads on the AWS Cloud. Read More »
For Mobile
Amazon Pinpoint: Helps run targeted push notification campaigns to improve user engagement in mobile apps. Read More »
AWS Mobile Hub integration with Amazon Lex: Helps build mobile apps that use speech and text, in addition to touch to your mobile app. Read More »
For Analytics
Amazon Athena: an interactive query service that helps analyze data in Amazon S3 using SQL.
AWS Glue: A fully managed ETL service that simplifies and automates data discovery, transformation, and job scheduling tasks. Read More »
For Application Services
AWS Step Functions: Helps coordinate the components of distributed applications and microservices using visual workflows. Read More »
For Artificial Intelligence (AI)
Amazon Lex: A deep learning engine (which powers Alexa) for building conversational interfaces using voice and text. Read More »
Amazon Polly: Built to revolutionize speech-enabled products, it turns text into lifelike speech, currently offering support for 24 languages and 47 lifelike voices. Read More »
Amazon Rekognition: Built to revolutionize the face recognition and object-recognition solutions, it helps to add image analysis to your application, help detect objects, scenes, and faces in images, and also search and compare faces. Read More »
Give us a shout out on Twitter, Facebook, and LinkedIn if you think we have missed out any announcement, or if you want to reach out to any of our AWS experts who can clarify your doubts on these new AWS announcements/updates. We will continue to work on how we can help improve AWS cloud management for you. Do stay tuned with us for other interesting news too!
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/foundations/microsoft-azure-and-office-365-resourcing-issues-4df5b8efcc18?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
As must be clear to everyone by now, there has been a massive spike in demand for public cloud services since the coronavirus outbreak first hit us. Microsoft report that whole countries have gone from zero use of cloud to deliver teaching to 100% coverage of cloud-based remote learning in a matter of weeks. MIcrosoft Teams has probably borne the brunt of that demand.
It is therefore not surprising that we are beginning to see the first signs of resourcing problems. Yesterday, the Register reported that ‘Azure appears to be full’: UK punters complain of capacity issues on Microsoft’s cloud and I’ve seen similar reports elsewhere.
If you get errors when trying to provision VMs on Azure, the advice from Microsoft appears to be:
Obviously the last of these needs to be treated with some caution. Although all Azure regions are built to the same level of compliance there are obviously factors to consider with relocating data: for example, although the EU Model Clauses and Privacy Shield are recognised as GDPR-compliant, they are likely to require ongoing monitoring by the data controller.
That said, a recent blog post on the Information Commissioner’s Coronavirus blog says:
“We are a reasonable and pragmatic regulator, one that does not operate in isolation from matters of serious public concern. Regarding compliance with information rights work when assessing a complaint brought to us during this period, we will take into account the compelling public interest in the current health emergency.”
Remember that the cloud providers are prioritising government and emergency services use of the cloud.
Freeing up capacity for emergency health service use by temporarily moving less critical stuff to less-stressed regions seems to chime very well with that “compelling public interest”. Though, in the current emergency, are there any regions that are “less-stressed”?
Note that I do not believe that Microsoft will move data out of the region into which customers have put it — however, what customers choose to do is their decision and we may well be tempted to created resources outside of our normally prefered regions in response to the current crisis.
To the above three pieces of advice, I would add two more:
Sorry, I appreciate that these are both very obvious. But the main point is that we all need to provision resources carefully and being mindful of the wider impact on others. Otherwise we just become part of the problem.
Clearly, capacity issues in Azure will affect, and be affected by, capacity issues in Office 365. Microsoft have scaled their Office 365 capacity vastly, particularly in response to increased demand for Teams. Therefore, many of the same considerations will apply. That said, I’m unclear around best practice here, specifically in terms of how to provision Office 365 resources in order to minimise the impact on underlying services. As Office 365 is delivered as SaaS we, as customers, have relatively little control over the underlying resource allocation. But Microsoft are making adjustments to various features to maximise utilisation.
In all cases I would advise strongly against panic buying. We all know where everybody buying too much toilet roll gets us.
(Thanks to my colleague Andrew Cormack for advising on the GDPR-related aspects of this post).
Originally published at https://cloud.jiscinvolve.org on March 25, 2020.
A blog by andypowe11
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/@RadwareBlog/eliminating-excessive-permissions-c659e4f7a91e?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Jun 11, 2019·1 min read
Excessive permissions are the #1 threat to workloads hosted on the public cloud. As organizations migrate their computing resources to public cloud environments, they lose visibility and control over their assets.
In order to accelerate the speed of business, extensive permissions are frequently granted to users who shouldn’t have them, which creates a major security risk should any of these users ever become compromised by hackers.
Read the full article: http://bit.ly/2Zhr00T
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
See all (1,432)
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/cloud-cost-management-tool-comparison-fe4cce2d975b?source=search_post---------146,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 29, 2017·4 min read
Not only has it become apparent that public cloud is here to stay, it’s also growing faster as time goes on (by 2020, it is estimated that more than 40% of enterprise workloads will be in the cloud). IT infrastructure has changed permanently, and enterprise organizations are coming to terms with some of the side effects of this shift. One of those side effects is the need for tools and processes (and even teams in larger organizations) dedicated to cloud cost management and cost control. Executives from all teams within an organization want to see costs, projections, usage, savings, and quantifiable efforts to save the company money while maximizing IT throughput as enterprises shift to resources to the cloud.
There’s a variety of tools to solve some of these problems, so let’s take a look at a few of the major ones. All of the tools mentioned below support Amazon AWS, Microsoft Azure, and Google Cloud Platform.
CloudHealth provides detailed analytics and reporting on your overall cloud spend, with the ability to slice-and-dice that data in a variety of ways. Recommendations about your instances are made based on a score driven by instance utilization and cloud provider best practices. This data is collected from agents that are installed on the instances, along with cloud-level information. Analysis and business intelligence tools for cloud spend and infrastructure utilization are featured prominently in the dashboard, with governance provided through policies driven by teams for alerts and thresholds. Some actions can be scripted, such as deleting elastic IPs/snapshots and managing EC2 instances, but reporting and dashboards are the main focus.
Overall, the platform seems to be a popular choice for large enterprises wanting cost and governance visibility across their cloud infrastructure. Pricing is based on a percentage of your monthly cloud spend.
Cloudcheckr provides visibility into governance, security, compliance, and cost problems based on doing analytics and checks against logic built into their platform. It relies on non-native tools and integrations to take action on the recommendations, such as Spotinst, Ansible, or Chef. CloudCheckr’s reports cover a wide range of topics, including inventory, utilization, security, costs, and overall best-practices. The UI is simple and is likely equally well regarded by technical and non-technical users.
The platform seems to be a popular choice with small and medium sized enterprises looking for greater overall visibility and recommendations to help optimize their use of cloud. Given their SMB focus customers are often provided this service through MSPs. Pricing is based on your cloud spend, but a free tier is also available.
Cloudyn (recently acquired by Microsoft) is focused on providing advice and recommendations along with chargeback and showback capabilities for enterprise organizations. Cloud resources and costs can be managed through their hierarchical team structure. Visibility, alerting, and recommendations are made in real time to assist in right-sizing instances and identifying outlying resources. Like CloudCheckr, it relies on external tools or people to act upon recommendations and lacks automation
Their platform options include supporting MSPs in the management of their end customer’s cloud environments as well as an interesting cloud benchmarking service called Cloudyndex. Pricing for Cloudyn is also based on your monthly cloud spend. Much of the focus seems to be on current Microsoft Azure customers and users.
Unlike the other tools mentioned, ParkMyCloud focuses on actions and automated scheduling of resources to provide optimization and immediate ROI. Reports and dashboards are available to show the cost savings provided by these schedules and recommendations on which instances to park. The schedules can be manually attached to instances, or automatically assigned based on tags or naming schemes through its Policy Engine. It pairs well with the other previously mentioned recommendation-based tools in this space to provide total cost control through both actions and reporting.
ParkMyCloud is widely used by DevOps and IT Ops in organizations from small startups to global multinationals, all who are keen to automate cost control by leveraging ParkMyCloud’s native API and pre-built integration with tools like Slack, Atlassian, and Jenkins. Pricing is based on a cost per-instance, with a free tier available.
Cloud cost management isn’t just a “should think about” item, it’s a “must have in place” item, regardless of the size of a company’s cloud bill. Specialized tools can help you view, manage, and project your cloud costs no matter which provider you choose. The right toolkit can supercharge your IT infrastructure, so consider a combination of some of the tools above to really get the most out of your AWS, Azure, or Google environment.
Originally published at www.parkmycloud.com on August 29, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@jystewart/getting-past-off-shoring-132b74665498?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Stewart
Jul 20, 2017·7 min read
One of the factors many organisations (including governments) agonise over when deciding whether to use public cloud services is whether or not services and data can be stored “off shore”. It’s not a topic we tend to discuss very well.
“Off shore” usually means stored in data centres in other countries but can sometimes mean in facilities within the originating country but operated by foreign-owned companies.
For UK organisations looking at infrastructure as a service that conversation is dissipating now that the three biggest players all have UK data centres, but switching to UK data centres is really just dodging the issue rather than looking at how and why decisions are made.
It was great to hear Ian McCormack from NCSC addressing offshoring in his spot in the keynote at the AWS Public Sector Summit in DC recently.
“It’s often said to us that due to the global nature of a service it’s somehow inherently less secure than if exactly the same service was hosted on a datacentre in the UK. But actually that just doesn’t stand up to technical security outside of particular national security type applications.” — Ian McCormack, NCSC
For the vast majority of applications from the vast majority of organisations, the physical location is not a factor in confidentiality or integrity. There may be compliance requirements that force decisions on you, or there may be performance reasons to choose particular geographies, but not security.
That said, the topic comes up so often that it seems worth breaking down some thoughts on how you might approach the issue if you want to really consider the risks. Which hosting companies you use, who they’re owned by, and where their various assets are hosted should be considered within your overall risk assessment.
A risk model is only as good as your understanding of the service it’s protecting. Before starting on anything you should make sure you have a solid grasp of the service expectations, what its impact on other services is, and so on. That will help you understand any trade-offs that need to be made, and also help understand whether any offshoring concerns might be coming from.
The following thoughts are based on a set of conversations over the past couple of years. They’re far from comprehensive, but I’m regularly in situations where I find people who don’t know where to start with breaking down these issues and it seemed worth sharing even some sketchy thoughts.
Before getting into detail on the particular risks, it’s worth first considering the scenarios where the location of data might be important.
Are you solely concerned about data security, or are your concerns about making sure your services keep working in the unlikely event that all network connections out of the UK fail?
If that unlikely event is a real consideration for you would you need to get your services back up and running immediately, or will backups that let you rebuild locally be sufficient? Most of the time it’s going to be more important that you’re running in multiple locations than whether one of those locations is in your home country.
Once you understand your context, you can go into the next level of detail. Roughly speaking there are three areas of risk that people are concerned about when they trust their services to a third party, regardless of the classification of that data.
There is also a further risk that we don’t often discuss, which is that the availability of services will be disrupted due to the complexity of international network routing.
It’s worth noting that I’m assuming you are using a robust cloud provider and are applying good practices to your cloud usage so that the chances of other customers affecting your services are very small.
The risks relating to staff at cloud companies accessing your data are similar whether your data is entirely contained within the UK or is stored elsewhere.
Before worrying about where the data is, you should be thinking about what impact comes with disclosure of the data. For much of what we do simply being careful about how we use a tool will minimise that impact. For example, if we’re using a project management tool we shouldn’t include personal data or credentials in what we store.
When you do have data that needs to be restricted then many infrastructure as a service providers will share information about the measures they take to make it very difficult for their staff to access customers’ data. Increasingly details of those measures are available publicly. Those measures apply whether the data is stored in the UK or outside it.
Non-UK ownership of companies, or non-UK residence of data centres is a reality of most modern internet services. There have been a number of legal cases around the world over the past few years beginning to test to what extent governments can compel companies to provide their customers’ data to law enforcement agencies or litigants in certain cases.
The full ramifications of those cases are still unclear and the legal situation will continue to vary significantly from jurisdiction to jurisdiction, but there are other things we should consider before getting into the detailed legal situation.
Once again, we need to understand the risk associated with a court granting access to the data we have in a service. That will largely depend on the way any data that is disclosed will be handled and what guarantees we have offered to our users. Access to a very specific record granted via a warrant and committing the accessing parties to hold the information carefully, is very different from a court allowing various parties to hold full copies of a database without protection.
We then need to consider the likelihood of such an order and the practicalities of fulfilling it. These cases are extremely rare and likely to remain so.
For governments, in the very rare circumstance where such situations did arise, most foreign governments are likely to use diplomatic channels to address requests of this sort that touch on government-owned data. Not to do so would risk a diplomatic incident and that is rarely worthwhile. Those diplomatic channels give us an opportunity to find other ways to address the situation.
Regardless of whether you’re a government, where your data is stored on third-parties’ servers that doesn’t mean it’s entirely out of your control. Just because your data rests on someone else’s server doesn’t mean you can’t encrypt it and store the keys elsewhere, or take other similar steps.
There are always risks that hostile actors will want to interfere with your service, and that’s something that should be considered as part of the general threat modelling and risk assessment for a system regardless of where it’s hosted.
Infrastructure security is incredibly important but far too often people focus on that at the cost of application security, which is where the easiest to exploit vulnerabilities are usually found. Regardless of where an application is hosted you should be managing the application security appropriately. With that, you should be taking appropriate and proportionate steps to maintain the integrity of the data in your services. For example, when using large-scale public cloud services you should implement industry standard encryption of your network traffic, and thinking about how you’d detect tampering with your data at rest.
It is possible that certain types of attacks will be easier if data is in another country, particularly disruption to the availability of a service. If your service is genuinely critical, you should already have plans to make it resilient against network outages, for example by deploying software you run to multiple “regions” or by ensuring that your software-as-a-service providers do similarly.
It’s also worth noting that there are limits to what you can reasonably prepare for or protect against.
One example of where an organisation has thought ahead about that and set out to be realistic, is the threat model for UK-OFFICIAL. The UK government accepted the possibility that determined and highly capable foreign governments may be able to access some data and services:
“This model does not imply that information within the OFFICAL tier will not be targeted by some sophisticated and determined threat actors (including Foreign Intelligence Services) who may deploy advanced capabilities. It may be. Rather, a risk based decision has been taken not to invest in controls to assure protection against those threats, i.e. proportionate not guaranteed protection.”
For the vast majority of what any of us do, considerations about where to host your data and services come down to good architectural practices: are our services designed to be resilient, fault-tolerant and responsive enough to meet users’ expectations?
In some cases being comfortable hosting services off-shore brings very real advantages, not just because it gives us access to a wider market of suppliers but also because it allows for geographical resilience, or for better services to those based outside the UK.
The main thing that’s important whatever we’re doing is to maintain awareness of what we’re using and how. In a cloud-centric world that no longer means understanding every server, but having a good sense of where companies you’re dependent on are owned and operated and for high-availability services how their network connectivity is provided.
Originally published at jystewart.net on July 20, 2017.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
"
https://medium.com/@alibaba-cloud/assessing-alibaba-cloud-6d138f794cf3?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 5, 2018·6 min read
When talking to CIOs about their public cloud plans, the conversation often features a discussion about the merits of using Amazon Web Services (AWS) over Microsoft Azure, or vice versa.
The breadth of services AWS is able to offer, along with the rate at which it rolls out new services, is part of its appeal, but the fact that so many enterprises now rely on its infrastructure to host their workloads and applications instils confidence in CIOs that the Amazon cloud is the way to go.
The growth of the Microsoft public cloud, meanwhile, is being driven by long-term users of the Redmond giant’s on-premises tech upgrading and migrating workloads to Azure, so they can either go all-in on the platform or adopt a hybrid cloud strategy.
It is not uncommon for the Google Cloud Platform to get a mention too, particularly in enterprises that may be looking to expand their use of the search giant’s off-premises offerings beyond its G-Suite of online business productivity services.
On the face of it, the public cloud market appears to be something of a three-horse race, but a fourth challenger is fast emerging, in the form of Chinese service provider Alibaba Cloud.
According to Gartner’s September 2017 global infrastructure-as-a-service (IaaS) market tracker, Alibaba Cloud outperformed AWS, Microsoft, and Google in revenue growth terms over the course of 2016, having increased by 126.5% from $298m to $675m.
The report also pegs Alibaba Cloud as being the third largest public cloud provider, with about 3% market share. Microsoft, incidentally, is in second place with 7.1%, AWS is the global market leader with 44.2%, leaving Google in fourth place.
In the company’s native China, Alibaba Cloud is the public cloud market leader, with 47.6% market share, and — over the past couple of years — it has set its sights on replicating this success overseas by going head-to-head with AWS, Microsoft, and Google.
Convincing CIOs outside of China, who may be unfamiliar with the brand, to choose Alibaba Cloud over its competitors’ offerings is one area Gartner flagged in June 2017 as being a potential sticking point for its plans for world domination.
As the Gartner Magic Quadrant analysis of global IaaS market states, Alibaba Cloud has “very little” to differentiate it from the other runners and riders in the public cloud space.
The document states: “Alibaba’s [international offering] launched in mid-2016. It has a limited track record, and does not have the full capabilities or performance of the China offering.
“It has rapidly expanded its offerings to markets outside of China in the past 18 months, but the firm does not have substantial mind share with buyers in those markets, as it is still building the required local talent, industry expertise and go-to-market capabilities.”
In the light of all this, the Gartner report concludes that Alibaba Cloud faces “substantial challenges that it must overcome before it can translate its success in China to markets outside of its home territory”.
One way it has sought to do this is by tapping into the demand for locally hosted cloud services, having outlined its commitment in March 2016 to build its first datacenter outside of China.
This paved the way for the company to open a European facility in Germany during November 2016. It has since expanded the number of datacenter regions it operates to incorporate 12 locations across the globe, including Australia, Hong Kong, Indonesia, Japan, Malaysia and the United Arab Emirates.
Alibaba Cloud Europe general manager Yeming Wang said that a second European datacenter could be in the offing for the company some time in 2018.
“We are quite aggressive in certain ways to deploy our datacenters globally, and last year [alone] we launched several datacenters across Asia-Pacific, Indonesia and Malaysia. This year we are considering a datacenter in Europe, but it may be too early to say.”
As well as trying to court overseas enterprises, the company is also looking to give its Chinese customers a helping hand with their international expansion plans with the promise of local access to its cloud products in whatever territory they want to operate in.
“That is one of our biggest group of clients: Chinese customers who [want to] come to the UK. The biggest advantage they have is they are able to maintain a single user interface [wherever they are in the world], and we can serve up services to them from China, and from Europe too.”
Incidentally, this is a concept the British Medical Journal (BMJ) is putting to use in reverse. The healthcare publisher is using Alibaba’s public cloud services to aid the delivery of its online learning resources to healthcare professionals in China.
BMJ chief digital officer Sharon Cooper said teaming up with Alibaba Cloud means these resources can be served up directly to users from mainland China, in keeping with the country’s internet regulations.
“The Chinese market is hugely attractive to almost any organization, but specifically for us is the massive increase in healthcare provision that is being seen in the number of hospitals, the number of doctors that need to be trained and the changes to the way healthcare is being delivered.”
“You also have to follow a whole other set of regulations and guidelines to do business in China, and for us it was about trying to work out how we can maximize and utilize what we can deliver there without having to invest massively in building our own infrastructure.”
In terms of its technology offering, Alibaba Cloud’s services portfolio has some similarities with what AWS has to offer, with its own takes on cloud-based object and block storage, as well as elastic compute services, container-based offerings and its own content delivery network, to name a few.
These services have been keenly adopted by government organizations, as well as enterprises operating in the retail, financial services and logistics industries, which Alibaba Cloud claims has given it a deep understanding of the cloud needs of these sectors.
As such, it plans to draw on the experience gained from working with customers in these verticals to target overseas customers operating in similar areas, using its burgeoning portfolio of big data analytics and artificial intelligence (AI) services as an attraction, said Wang.
To this end, Alibaba Cloud used the Mobile World Congress in February 2018 to introduce new additions to its AI portfolio, including image search tools, its business-focused Intelligent Services Robot chatbot technology and big data application management offering, Dataphin.
It also debuted its Bare Metal Elastic Compute Service instances that can be grouped together to create super-computing clusters, as part of its efforts to make deeper inroads into the high-performance computing and research market.
This has already seen Alibaba Cloud partner with the Met Office in the UK and the two firms embark on a big data-related hackathon.
During the event, participants were tasked with using Met Office weather data to create an algorithm that would help plot a safe route for some unmanned balloons across the Earth.
Alibaba Cloud has also decided to pit itself against Microsoft’s hybrid cloud-enabling Azure Stack offering by providing its own take on the technology, with the international launch of its Apsara Stack product.
The technology is already used by more than 120 enterprise customers in the company’s native China, it is claimed, and an international version is in the offing that will allow European users of its tech to access and run Alibaba Cloud services in their own private datacenters.
Derek Wang, Chief Cloud Architect at Alibaba Cloud International, mentioned that the fact that the company is offering users access to artificial intelligence-infused cloud services within their own datacenters is a significant point of competitive difference for the firm.
“We understand that a lot of enterprises in Europe are conservative about adopting public cloud, so we launched a new private cloud [to cater to that], so users can also use AI and datacenter intelligence services in their own [facilities], and that makes us different from Amazon, Microsoft and Google.”
Whether or not that proposition will be enough to win over European CIOs remains to be seen, but it is fair to say that Alibaba Cloud is not giving up without a fight.
Source: https: //www.computerweekly.com/feature/Assessing-Alibaba-Cloud
Reference:
https://www.alibabacloud.com/blog/assessing-alibaba-cloud_593784?spm=a2c41.11731789.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@RadwareBlog/mitigating-cloud-attacks-with-configuration-hardening-d5f71d8ff53?source=search_post---------149,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Feb 26, 2019·2 min read
For attackers, misconfigurations in the public cloud can be exploited for a number of reasons. Typical attack scenarios include several kill chain steps, such as reconnaissance, lateral movement, privilege escalation, data acquisition, persistence and data exfiltration. These steps might be fully or partially utilized by an attacker over dozens of days until the ultimate objective is achieved and the attacker reaches the valuable data.
To prevent attacks, enterprises must harden configurations to address promiscuous permissions by applying continuous hardening checks to limit the attack surface as much as possible. The goals are to avoid public exposure of data from the cloud and reduce overly permissive access to resources by making sure communication between entities within a cloud, as well as access to assets and APIs, are only allowed for valid reasons.
For example, the private data of six million Verizon users was exposed when maintenance work changed a configuration and made an S3 bucket public. Only smart configuration hardening that applies the approach of “least privilege” enables enterprises to meet those goals.
The process requires applying behavior analytics methods over time, including regular reviews of permissions and a continuous analysis of usual behavior of each entity, just to ensure users only have access to what they need, nothing more. By reducing the attack surface, enterprises make it harder for hackers to move laterally in the cloud.
The process is complex and is often best managed with the assistance of an outside security partner with deep expertise and a system that combines a lot of algorithms that measure activity across the network to detect anomalies and determine if malicious intent is probable. Often attackers will perform keychain attacks over several days or months.
Read more: http://bit.ly/2SqJoAy
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
"
https://medium.com/@RadwareBlog/excessive-permissions-are-your-1-cloud-threat-f34ce19a39df?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Feb 20, 2019·1 min read
Migrating workloads to public cloud environment opens up organizations to a slate of new, cloud-native attack vectors which did not exist in the world of premise-based data centers. In this new environment, workload security is defined by which users have access to your cloud environment, and what permissions they have. As a result, protecting against excessive permissions, and quickly responding when those permissions are abused, becomes the #1 priority for security administrators.
Traditionally, computing workloads resided within the organization’s data centers, where they were protected against insider threats. Application protection was focused primarily on perimeter protection, through mechanisms such as firewalls, IPS/IDS, WAF and DDoS protection, secure gateways, etc.
However, moving workloads to the cloud has led to organizations (and IT administrators) to lose direct physical control over their workloads, and relinquish many aspects of security through the Shared Responsibility Model. As a result, the insider of the old, premise-based world is suddenly an outsider in the new world of publicly hosted cloud workloads.
Read more: http://bit.ly/2ImfD4g
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
"
https://medium.com/@techgenyz/huawei-and-sony-team-up-to-showcase-cloud-based-media-solutions-at-mwc-2018-f670b8c990ff?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGenyz
Feb 27, 2018·1 min read
Huawei has teamed up with Sony Professional to showcase cloud-based high-end media solutions based on Open Telekom Cloud, the public cloud platform from T-Systems, of which Huawei is the public cloud partner.
Open Telekom Cloud, “an affordable, scalable and easy to use cloud infrastructure solution”, provides on-demand and secure cloud services with optimized performance for media content and video processing allowing companies enabling solutions like Sony’s Media Backbone Hive cloud-based production system to assess dynamic market conditions.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
"
https://medium.com/@jaychapel/3-things-companies-using-cloud-computing-should-make-sure-their-employees-do-6f9753d7087b?source=search_post---------152,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 24, 2017·2 min read
These days, there’s a huge range of companies using cloud computing, especially public cloud. While your infrastructure size and range of services used may vary, there are a few things every organization should keep in mind. Here are the top 3 we recommend for anyone in your organization who touches your cloud infrastructure.
OK, so this one is obvious, but it bears repeating every time. Keep your cloud access secure.
For one, make sure your cloud provider keys don’t end up on GitHub… it’s happened too many times.
(there are a few open source tools out there that can help search your GitHub for this very problem, check out AWSLabs’s git-secrets).
Organizations should also enforce user governance and use Role-Based Access Control (RBAC) to ensure that only the people who need access to specific resources can access them.
There’s an inherent problem created when you make computing a pay-as-you-go utility, as public cloud has done: it’s easy to waste money.
First of all, the default for computing resources is that they’re “always on” unless you specifically turn them off. That means you’re always paying for it.
Additionally, over-provisioning is prevalent — 55% of all public cloud resources are not correctly sized for their resources. The last is perhaps the most brutal: 15% of spend is on resources which are no longer used. It’s like discovering that you’re still paying for that gym membership you signed up for last year, despite the fact that you haven’t set foot inside. Completely wasted money.
In order to keep costs in check, companies using cloud computing need to ensure they have cost controls in place to eliminate and prevent cloud waste — which, by the way, is the problem we set out to solve when we created ParkMyCloud.
Third, companies should ensure that their IT and development teams continue their professional development on cloud computing topics, whether by taking training courses or attending local Meetup groups to network with and learn from peers. We have a soft spot in our hearts for our local AWS DC Meetup, which we help organize, but there are great meetups in cities across the world on AWS, Azure, Google Cloud, and more.
Best yet, go to the source itself. Microsoft Azure has a huge events calendar, though AWS re:Invent is probably the biggest. It’s an enormous gathering for learning, training, and announcements of new products and services (and it’s pretty fun, too).
Originally published at www.parkmycloud.com on October 24, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/5-ways-to-get-discounts-on-cloud-resources-parkmycloud-fbd6fcc2f27b?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 10, 2018·3 min read
Whether you’re just getting started on public cloud, or you’ve gotten a bill that blew your budget out of the water, it’s a good idea to research ways to get discounts on cloud resources. There’s no reason to pay list price when so many cost-savings measures are available (and your peers are probably taking advantage of them!) Here are our top five ways to get discounts on cloud.
By purchasing your compute power in advance, you can get a discounted rate — the notable examples being AWS Reserved Instances, Azure Reserved Instances, and Google Committed Use Discounts.
So will these save you money? Actually, that’s a great question. There are several factors that weigh into the answer:
This blog post about AWS Reserved Instances digs into these issues further. Bottom line: paying in advance can save you money, but proceed with caution.
The primary example of “spending more to save more” in the cloud computing world is Google Sustained Use Discounts. This is a cool option for automatic savings — as long as you use an instance for at least 25% of the month, GCP will charge you less than list price.
But just like the advanced purchasing options above, there are several factors to account for before assuming this will really save you “up to 60%” of the cost. It may actually be better to just turn off your resources when you’re not using them — more in this post about Google Sustained Use Discounts.
Anyone who’s shopped at Costco isn’t surprised that buying in bulk can get you a discount. Last week, Twitter announced that it will be using Google Cloud Platform for cold data storage and flexible compute Hadoop clusters — at an estimated list price of $10,000,000/month. Of course, it’s unthinkable that they would actually pay that much — as such a high-profile customer, Twitter is likely to have massive discounts on GCP’s list prices. We often hear from our Azure customers that they chose Azure due to pre-existing Microsoft Enterprise Agreements that give them substantial discounts.
If you have or foresee a large volume of infrastructure costs, make sure to look into:
Each of the major cloud providers offers free credit programs to startups to lure them and get locked in on their services — but that’s not a bad thing. We’ve talked to startups focused on anything from education to location services who have gotten their money’s worth out of these credits while they focus on growth.
If you work for a startup, check out:
So far, history tells us that if you wait a few months, your public cloud provider will drop their prices, giving you a built-in discount.
If you stick with the existing resource types, rather than flocking to the newer, shinier models, you should be all set. The same AWS m1.large instance that cost $0.40/hour in 2008 now goes for $0.175. We’ll just say that’s not exactly on pace with inflation.
What if you’re not a startup, you’re not an enterprise, and you just need some regular compute and database infrastructure now? Should you worry if you don’t get discounts on cloud list prices? No sweat. Even by paying list price, it’s still possible to optimize your spend. Make sure you’re combing through your bill every so often to find orphaned or unused resources that need to be deleted.
Additionally, right-size your resources and turn them off when you’re not using them to pay only for what you actually need — you’ll save money, even without a discount.
Originally published at www.parkmycloud.com on May 10, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/how-to-optimize-cloud-spend-with-parkmycloud-408a95343f83?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 19, 2017·2 min read
The focus on how to optimize cloud spend is now as relentless as the initial surge was to migrate workloads from ‘on-prem’ to public cloud. A lot of this focus, and resultant discussions, were in regards to options related to the use of Reserved Instances (RI’s), Spot Instances,or other pre-pay options. The pay-up-front discount plan makes sense when you have some degree of visibility on future needs, and when there is no ‘turn-if-off’ option, which we here at ParkMyCloud call “parking”.
When it comes to the ability to ‘park instances’ we like to divide the world into two halves. There are those Production Systems, which typically need to be running 24/7/365, and then there are Non-Production Systems, which at least in theory have the potential to be parked when not in use. The former are typically your end-customer or enterprise facing systems, which need to be online and available at all times.In this case, RI’s typically make sense. When it comes to those non-production systems, that’s where a tool such as ParkMyCloud comes into play. Here you have an opportunity to review the usage patterns and needs of your organization and how to optimize cloud spend accordingly. For example, you may well discover that your QA team never works on weekends, so you can turn their EC2 instances off on a Friday night and turn them back on first thing on Monday morning. Elsewhere, you might find other workloads that can be turned off in the small hours or even workloads which can be left off for extended periods.
Our customers typically like to view both their production and non-production systems in our simple dashboard. Here they can view all their public cloud infrastructure and simply lock those production systems which cannot be touched. Once within the dashboard the different non-production workloads can then be reviewed and either centrally managed by an admin or have their management delegated to individual business units or teams.
Based on our customer usage we track, we see these non-production systems typically accounting for about 50% of what the companies spend on compute (i.e. instances / VMs). We then see those who aggressively manage these non-production instances saving up to 65% of their cost, which then makes a large dent in their overall cloud bill.
So, when you are thinking about how to optimize cloud spend, there’s a lot more opportunities than just committing to purchase in advance, especially for your non-production workloads.
Originally published at www.parkmycloud.com on September 19, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://stacksense.io/serverless-extreme-positions-false-dichotomies-3fb4063a797d?source=search_post---------155,"Sign in
Krish
Aug 9, 2018·3 min read
Serverless is driving the passionate debate in social media just like how the public cloud ramped up the discussions circa 2008. We are once again seeing patterns in discussions that mirror what we saw in the early days of the cloud. The increasing pitch of some of these messages (by the respective advocates or critics) is not helping anyone. In this post, we want to debunk some of these claims and help set the tone.
One of the underlying tones in serverless advocacy is that it is the magic pill for all the IT problems. In fact, I saw one slide from the recently concluded ServerlessConf where the speaker was making fun of Kubernetes users. Yes, serverless is the highest level of developer platform abstraction that does all the heavy lifting needed to deploy the functions. But it is still severely opinionated and not suitable for all kinds of workloads. In fact, it is even limited for some of the stateless workloads. Dismissing any technology other than serverless as unnecessary heavy lifting is simplistic. Yes, if you invest heavily in running your own data centers, it is unnecessary heavy lifting. Even here, it is not a binary characterization. What developers will be using, in the foreseeable future, are a continuum of services starting with managed Kubernetes services to serverless containers to serverless functions. Any claims about serverless as the only service that matters is either simplistic thinking or just pure hallucination.
On the other side of the debate, we have people making claims that Kubernetes is the future and serverless is just a crazy talk by a bunch of developers. In fact, the “Kubernetes is the only future” camp was involved in a conversation with someone advocating serverless on Twitter. The counter-argument against serverless, especially Functions as a Service offering like AWS Lambda was as follows:
If people should use serverless to avoid the heavy lifting, then everyone should be using public transport than driving their own car
Even though I personally believe that using public transport is the right thing to do to avoid climate change, I find this comparison too simplistic for a rebuttal. Public transport, at least in the USA, is not widespread enough to be an alternative agile mode of transportation for driving your own car. But, in the case of public cloud services, there are no such bottlenecks. The consumption of a service like AWS Lambda is as simple as opening the web browser and signing up with one’s credit card.
Yeah, this post is more of a rant than a typical StackSense post but it is time we do not take a binary position on any technology. That is an easy way to make any curious enterprise user drag their feet on any new technology. Instead, let us engage in discussions that actually help enterprise users understand the landscape better and embrace modern technologies.
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
This blog helps enterprise decision-makers understand the emerging technologies and it is part of Rishidot Research publications
"
https://medium.com/@alibaba-cloud/the-growing-significance-of-the-hybrid-cloud-8d215eb13a89?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 3, 2018·5 min read
A hybrid cloud refers to an integrated cloud platform that combines public cloud servers with private data centers. Hybrid clouds are fast becoming the implementation of choice for businesses that are seeking to embrace cloud computing. A 2016 survey that asked IT professionals from a range of enterprises about their adoption of cloud computing found that 71% of respondents used a hybrid cloud, up from 58% in 2015. What explains this dramatic growth in the adoption of hybrid clouds, which is projected to rise even further in the future?The answer largely lies in the fact that a hybrid cloud is able to overcome some of the limitations that come from relying purely on a public cloud service. While businesses gain significant advantages from adopting public cloud facilities, such as acquiring enormous storage space, immense processing power and scalability, there are also drawbacks involved which a hybrid cloud arrangement can help mitigate. Let’s look at a couple of these drawbacks and how they can be circumvented with a hybrid cloud model.
A public cloud is typically accessed through the Internet, which makes any data that is held on the public cloud vulnerable to being infringed by malicious actors. Competent public cloud providers implement strong security protocols to reduce this risk, but it can never be eliminated entirely. For organizations that have particularly sensitive data or applications, such as client billing information or intellectual property crucial to the business, a hybrid cloud enables them to place this most sensitive data on private servers which cannot be accessed through the Internet. Less confidential data or applications, which are usually more copious in volume, can be placed in the public cloud and benefit from the large storage capacity public servers provide.
Sometimes it’s not possible to move all applications that an enterprise uses over to the public cloud, because they may not be supported by public cloud servers. Very old applications such as a COBOL DB2 database which is used to manage inventory is generally incompatible with the hardware used by public cloud providers. Either the application has to be re-written to work on the public cloud, which could cost millions of dollars rendering the option impractical in most cases, or it has to run on private hardware. A hybrid cloud architecture enables companies to continue using legacy software which is important to their operations in a private environment, while using the public cloud to utilize more up-to-date software which can benefit from the enhanced processing capability provided.
Public cloud data centers inevitably introduce latency, however small, because they are accessed through the Internet as noted earlier. There are certain applications which are highly sensitive to latency and are therefore not appropriate for being hosted on a public cloud. Examples include algorithms which predict natural disasters such as earthquakes and tsunamis, where any delay in the calculations could mean the difference between life and death. Latency can affect even high quality voice and video streaming. A hybrid cloud allows such applications to be run on private hardware to circumvent latency issues.
Given that a public cloud is maintained by a third party, it can provide local IT professionals with less management capabilities than over private hardware. For example, pre-packaged software that is accessible through the public cloud, also called software-as-a-service (SaaS), can be subject to updates or redesigns beyond the control of the local IT team. Such updates may cause conflicts with other software, hardware or data that is being used by the company. Having a hybrid cloud enables the IT team to keep some software, which they feel may cause these kinds of conflicts if modified, on private machines and under their control.That said, in some respects a public cloud can grant increased control, for instance by providing IT teams with additional computational resources to perform tasks that may not be executed locally.
Keeping critical data and applications only on a public cloud gives rise to the risk that they may become inaccessible for a particular period of time if the public cloud experiences technical difficulties. A hybrid cloud allows critical data and applications to be duplicated on private servers, ensuring they are still accessible even if the public cloud goes down. In a similar way, if the private servers experience technical difficulties, the critical data and applications will still be accessible through the public cloud.The best of both worldsA hybrid cloud arrangement enables enterprises to enjoy the best of both worlds. They can reap the myriad of benefits that public cloud services provide, including the ability to purchase additional processing power that can scale up or down based on demand. They can also enjoy the advantages of private data centers, including enhanced security or practically zero latency. It’s therefore no wonder that hybrid clouds are increasing in popularity.
In spite of all this, hybrid clouds do have some shortcomings. A significant drawback is that a hybrid cloud can be a prohibitively expensive option if the IT budget is small, since hybrid requires investment in private hardware (whether on-site or hosted by a vendor) as well as public servers.There may also be cases where companies have existing on-site infrastructure which is serving their purposes well and does not require any integration with the public cloud. These include LAN-based SCADA networks used in industrial automation processes.Another downside is that getting public and private servers to communicate with one another can be a tricky issue, especially if they run on different types of hardware and operating systems. Strong IT expertise, whether locally or from a cloud provider, is needed to resolve this problem in order to get the public and private components of the hybrid cloud running as a seamless system. Investing in a cloud management platform may also be needed, which increases costs further. But in some cases, it may not be possible to integrate private hardware with public cloud architecture at all because the two are totally incompatible.By weighing up the pros and cons of the hybrid cloud, organizations may decide that a hybrid arrangement is beneficial for certain departments (e.g. those sensitive to downtime or using legacy software), whereas the public cloud is more suitable for other departments (e.g. those with lower IT budgets or with no on-site computing infrastructure). The hybrid cloud may also be advantageous for certain industries, but not for others (e.g. the military sector where it is risky or against protocol to put any data in the public cloud). Government regulations, such as requiring customer data to be held locally, will also dictate the kind of cloud configuration a company decides to use.
Reference:
https://www.alibabacloud.com/blog/The-growing-significance-of-the-Hybrid-Cloud_p71046
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@jaychapel/why-serverless-computing-will-be-bigger-than-containers-4205eb8dced3?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jan 18, 2018·5 min read
One of the more popular trends in public cloud adoption is the use of serverless computing in AWS, Microsoft Azure, and Google Cloud. All of the major public cloud vendors offer serverless computing options, including databases, functions/scripts, load balancers, and more. When designing new or updated applications, many developers are looking at serverless components as an option. This new craze is coming at a time when the last big thing, containers, is still around and a topic of conversation. So, when users are starting up new projects or streamlining applications, will they stick with traditional virtual machines or go with a new paradigm? And out of all these buzzy trends, will anything come out on top and endure?
The “traditional” approach to deployment of an application is to use a fleet of virtual machines running software on your favorite operating system. This approach is what most deployments have been like for 20 years, which means that there are countless resources available for installation, management, and upkeep. However, that also means you and your team have to spend the time and energy to install, manage, and keep that fleet going. You also have to plan for things like high availability, load balancing, and upgrades, as well as decide if these VMs are going to be on-prem or in the cloud. I don’t see the use of virtual machines declining anytime soon, but there are better options for some use cases.
Containerization involves isolating an application by making it think it’s the only application on a server, with only the hardware available that you allow. Containers can divide up a virtual machine in a similar way that virtual machines can divide up a physical server. This idea has been around since the early 1980s, but has really started to pick up steam due to the release of Docker in 2013. The main benefits of containerization are the ability to maximize the utilization of physical hardware while deploying pieces of a microservices architecture that can easily run on any OS.
This sounds great in theory, but there are a couple of downsides to this approach. The primary problem is the additional operational complexity, as you still have to manage the physical hardware and the virtual machines, along with the container orchestration without much of a performance boost. The added complexity without removing any current orchestration means that you now have to think about more, not less, You also need to build in redundancy, train your users and developers, and ensure communication between pieces on top of your existing physical and virtual infrastructure.
Speaking of container orchestration, the other main downside is the multitude of options surrounding containers and their management, as there’s no one clear choice of what to use (and it’s hard to tell if any of the existing ones will just go away one day and leave you with a mess). Kubernetes seems to be the front runner in this area, but Apache Mesos and Docker Swarm are big players as well. Which do you choose, and do you force all users and teams to use the same one? What if the company who manages those applications makes a change that you didn’t plan for? There’s a lot of questions and unknowns, along with just having to make the choice that could have ramifications for years to come.
When users or developers are working on a project that involves a database and some python scripts, they just want the database and the scripts, not a server that is running database software and a server that runs scripts. That’s because the main idea behind serverless architecture is the goal of trying to eliminate all the overhead that comes along with these requests for specific software. This is a big benefit to those who just want to get something up and running without installing operating systems, tweaking configuration files, and worrying about redundancy and uptime.
This isn’t all sunshine and rainbows, however. One of the big downsides to serverless comes hand-in-hand with that reduced complexity, in that you also typically have reduced customization. Running an older database version or having a long-running python function might not be possible using serverless services. Another possible downside is that you are typically locked in to a vendor once you start developing your applications around serverless architecture, as the APIs are often going to be vendor-specific.
That being said, it appears that the reduced complexity is a big deal for the users who want things to “just work”. Dealing with less headaches and less management so they can get creative and deploy some cool applications is one of the main goals of folks who are trying to push the boundaries of what’s possible. If Amazon, Microsoft, or Google want to handle database patching and python versioning so you don’t have to, then let them deal with it and move on to the fun stuff!
Here at ParkMyCloud, we’re doing a mix of serverless and traditional virtual machines to maximize the benefits and minimize the overhead for what we do. By using serverless where it makes sense without forcing a square peg into a round hole, we can run virtual machines to handle the code we’ve already written while using serverless architecture for things like databases, load balancing, and email messages. We’re starting to see more customers going with this approach as well, who then use ParkMyCloud to keep the costs of virtual machines low when they aren’t in use. (If you’d like to do the same, check out a trial of ParkMyCloud to get your hybrid infrastructure optimized.)
When it comes to development and operations, there are numerous decisions to make that all have pros and cons. Serverless architecture is the latest deployment option available, and it clearly helps reduce complexity and accounts for things that may give you headaches. The reduced mobility is something that containers can handle really well, but involves more complexity in deployment and ongoing management. Software installed on virtual machines is a tried-and-true method, but does mean you are doing a lot of the work yourself. It’s the fact that serverless computing is so simple to implement that makes it more than a trend: this is a paradigm that will endure, where containers won’t.
Originally published at www.parkmycloud.com on January 18, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/why-the-ncaa-google-cloud-ads-matter-2d4e777cec3a?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 15, 2018·3 min read
NCAA, Google Cloud? What does the cloud have to do with March Madness? Actually, public cloud is increasingly being used and promoted in sports. When you watch the tournament on NCAA, Google Cloud ads will show prominently. Plus, the NCAA has chosen to run its infrastructure on Google Cloud Platform (GCP).
(By the way, have your done your bracket yet? I just did mine — I went chalk and picked Villanova. Couldn’t see my WVU Mountaineers winning it all).
So we will see and hear a lot of Google Cloud in the coming weeks. Google recently announced a multiyear sponsorship deal with the NCAA and will run these ads throughout the upcoming NCAA basketball tournament. Google is hoping to expand its cloud business by taking complex topics such as cloud computing, machine learning and artificial intelligence and making them relatable to a wider audience.
So why does is matter that NCAA and Google Cloud will appear so prominently together this March Madness?
First of all, Google Cloud is always matching wits with the other major cloud providers — and in this case, they’ve had their hooks in various mainstream sporting leagues and events for several years. For example, did you notice the partnership between AWS and the National Football League (NFL)? Both AWS and NFL promote machine-learning capabilities — software that helps recognize patterns and make predictions — to quickly analyze data captured during games. The data could provide new kinds of statistics for fans and insights that could help coaches.
Second, there’s the infrastructure that supports these huge events. I can tell you as a sports fan that me and my mates will all be live streaming football, basketball, golf and soccer (yes the English Premier League) on our phones and tablets wherever we are. We do this while watching the kids play sports, working in the office, and even while we are playing golf — hook it up to cart (a buggy for my UK mates). Many of these content providers are using AWS, Microsoft Azure, GCP, and IBM Cloud to get this content to us in real time, and to analyze it and provide valuable insights for a better user experience.
Or take a look at the Masters golf tournament. Usually IBM and ATT are big sponsors, although the Masters is usually very hush hush about a lot of this. Last year there was a lot of talk of IBM Watson, the Masters and the surreal experience they were able to deliver. This is a really good read on what went on behind the scenes and how Watson and IBM’s cloud delivered that experience. IBM used Machine learning, Visual recognition, Speech-to-text, and cognitive computing to build a phenomenal user experience for Masters viewers and visitors.
The NCAA and Google Cloud are not just ad partners, but the NCAA is also a GCP customer. The NCAA is migrating 80+ years of historical and play-by-play data, from 90 championships and 24 sports to GCP. To start, the NCAA will tap into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. So Google Cloud not only gets advertising prominence for one of the most-watched events of the year, it gets a high-profile customer and one of the coolest use cases out there.
Enjoy the tournament — let’s go Cats!
Originally published at www.parkmycloud.com on March 15, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://stacksense.io/cio-perspective-on-multi-cloud-and-lock-in-19a71e7364e4?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krish
Oct 12, 2017·3 min read
The Multi-Cloud hype is increasing every day with both legacy vendors and public cloud providers (more specifically, Google which is lagging behind both AWS and Azure) talking about Multi-Cloud, there is quite a bit of confusion on whether Multi-Cloud will help avoid lock-in. A recent conversation on Twitter provided arguments on both sides of the debate. In this post, we will tackle the issue of lock-in in the context of multi-cloud so that enterprise decision-makers get some clarity on the right strategy for their organization.
Vendor lock-in was expensive in the traditional software world. However, the economic model of cloud along with the continuous innovation by providers driven by the competition in the space has ensured that enterprise decision-makers are not feeling the economic pinch of vendor lock-in. So, if you make a case for lock-in using the risks associated with getting locked into a single vendor, people are going to make a case that it is not hurting them as the cost of getting started with a new provider is not too high. At this point of enterprise cloud adoption, there is not going to be enough support for the risks associated with vendor lock-in.
Instead, one should frame the question of lock-in from the point of view of having the flexibility to innovate. The flexibility to innovate rapidly is more critical than the cost of getting locked into a single vendor. If your lock-in to the technology of a single provider slows your organization down, you are letting a competitor or an entirely new player from an unrelated vertical out-innovate your organization, potentially disrupting your entire business. The mantra for the Modern Enterprise decision-makers should be
Retain the flexibility to innovate rapidly or get disrupted
The threat of disruption through rapid innovation in technology is what is keeping innovative CIOs awake in the night. Losing your flexibility to change course or build a new feature or service in a short timeframe is critical for any modern enterprise. Multi-Cloud becomes an important part of your strategy if you approach lock-in from this point of view.
Every single cloud provider has a different set of services. This impacts the architecture, dependencies, etc. for the applications. When you are locked into a specific set of services from a single provider, your ability to take advantage of services offered by another provider (say Google ML capabilities) is not going to be easy. Yes, there are efforts like Open Service Broker API gaining momentum but it is a long way to go before ISVs across the industry embrace the API or all the cloud providers treat them as first-class citizens. The market for modern IT stacks is still the wild west. Cloud providers and ISVs are marketing themselves in all possible ways including partnerships with everybody and anybody. Soon, the market is going to consolidate and business reality will come to bite these cloud providers. We will soon realize that all the support for OSS foundations and partnerships with competitors are just pure marketing with very little value to end-users.
This puts the onus on the enterprises to have a strategy that gives them the flexibility to innovate as technology changes rapidly and as they feel the heat from their own business realities. The only way to retain the flexibility to avoid lock-in to specific technologies provided by a single provider, the new central dogma for the modern enterprise.
At StackSense, we are committed to sharing our knowledge about modern stacks to help enterprise decision-makers in their modern enterprise strategy. We will be posting a series of posts related to multi-cloud that will help in the process.
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
This blog helps enterprise decision-makers understand the emerging technologies and it is part of Rishidot Research publications
"
https://medium.com/paypal-tech/400-days-paypals-data-warehouse-migration-to-google-bigquery-8c3b845eb6c9?source=search_post---------160,"There are currently no responses for this story.
Be the first to respond.
The first in a multi-step Google Cloud Platform journey
By Romit Mehta, Vaishali Walia, and Bala Natarajan
“Take the first step in faith. You don’t have to see the whole staircase. Just take the first step.” — Dr. Martin Luther King Jr.
PayPal has experienced record growth since the beginning of the global pandemic. To keep up with the demand from growth, we decided to migrate PayPal Analytics platforms to the public cloud. The first big migration of a warehouse workload to BigQuery in Google Cloud took less than a year. Along the way, the PayPal team built a platform which would support many other use cases as well.
This writeup captures a milestone migration experience. We migrated half of our data and processing from Teradata systems to Google Cloud Platform’s BigQuery.
As organizations and consumers ventured into new ways of doing business during the pandemic, PayPal experienced record-high transaction volumes. This put a lot of pressure on the offline analytics systems used for compliance, risk processing, product and financial analytics, marketing, customer success, and fraud protection. These analytics systems were all in on-premises data centers. The systems were powered by Teradata and Hadoop at the core, with additional software and workflows in place to manage resources across these systems.
Demands for processing data were far outstripping the existing capacity on-premises. Adding capacity quickly during the pandemic had its own share of challenges. Data platform teams managed the crisis with manual intervention to prioritize various workloads that demanded additional processing time. Given the business outlook of continuing growth, PayPal realized that the analytics ecosystem required a change.
Additionally, we recognized the opportunity to modernize our data strategy in accordance with the ideas of better agility, discoverability, shareability and ecosystem integration. BigQuery allows us to centralize our data platform without losing capabilities such as SQL access, Spark integration, and advanced ML training. Also, BigQuery has some advanced features such as ML and real-time analytics that can be leveraged without moving data out to another system.
There were multiple factors that PayPal took into consideration in choosing cloud over on-premises expansion. PayPal’s data team started a blueprint for migration to public cloud, to keep up with data demands for the next five years based on Google Cloud Platform’s capabilities.
PayPal’s analytics infrastructure is based on a constellation of technologies for various use cases. Data analysts and a portion of data scientists predominantly depended on a data warehouse for their data work. The data in the warehouse was semi-structured, making it easier for teams to do analysis and reporting.
A simplified view of the data flow is provided in the following figure. Data from site databases first goes into the data warehouse. A copy of some of the data from the warehouse is made into a data lake which is powered by open source technologies. Data is then embellished with other data sources like tracking, experimentation, and data from PayPal’s adjacencies to get transformed and loaded back into the analytics warehouse for consumption.
PayPal managed two vendor-based data warehouse clusters on-premises, with a total storage of 20+ petabytes serving over 3,000 users. The capacity requirement was constantly growing as data became critical for business decisions. The analytics warehouse was limited by storage and CPU, and the main warehouse was limited by IO and storage.
Warehouse use cases could broadly be classified into interactive and batch workloads. Interactive workloads include ad-hoc queries from users using Jupyter Notebooks, and reports and dashboards using BI tools like Tableau and Qlikview. Batch workloads are scheduled using Airflow and UC4. Workloads are mostly written in SQL and executed using shell or Python scripts.
Due to challenges that arose as traffic grew, many of the transformation jobs and batch loads were running behind schedule. PayPal analysts and data scientists were seeing data way behind their service-level agreements (SLAs), with a degraded experience, and all of that delayed decision-making. Of the two major warehouses, PayPal decided to first migrate the analytics warehouse to BigQuery to experience using the service as a replacement for Teradata, and in the process build a platform around Google Cloud Platform services for PayPal’s data users. The main warehouse would be migrated later based on the learnings and experience from the analytics warehouse.
Technological challenges
Improving data users’ experience at PayPal involves addressing the following technological challenges:
Adoption Challenges
Changes to infrastructure need to overcome the following adoption challenges:
Given the list of challenges PayPal had to address, it was very clear that creating new on-premises solutions would be a problem. The building blocks for robust solutions were focused around the cloud with less support for on-premises infrastructure. Additionally, scaling requires buying hardware and the long lead times were a hindrance to business enablement. PayPal was already moving significant workloads to Google Cloud Platform which made the choice of moving the analytics platform to Google Cloud Platform easier. We evaluated various vendors that offered their services on Google Cloud Platform to see if they could solve some of the technological challenges mentioned earlier, and we narrowed down the choice to BigQuery. We ran a 12-week evaluation of BigQuery to cover different types of use cases. It performed well against the success criteria we targeted. A summary of the evaluation results is provided below.
We will be writing about the evaluation process, success criteria and the results in a separate post.
As part of our blueprint, we decided to tackle the “Analytics Warehouse” shown in Figure 1.
Once we chose which cloud and warehouse to explore, we identified the following tracks and started getting to the next phase.
Customer Contact
We reached out to users of the warehouse based on their usage stats over the past 12 months, as well as the data providers in that cluster. We set up time, walked them over the decision and sought their buy-in for this migration. This stakeholder buy-in was important for us to have a successful migration. We explained the rationale and how we planned to approach the problem. Some users were excited and wanted to be closely involved with the migration effort. We identified one team within one business unit as an early adopter and focused our migration effort on their use cases and data requirements.
Secure Infrastructure Buildout
We built a secure infrastructure to move data to the cloud. We kept data within BigQuery as US, multi-region to be accessed from other regions in the US. We implemented a secure private interconnect between our data center and the region in Google Cloud Platform nearest the analytics warehouse. Since we expected to operate in a hybrid mode (other connected systems remain on-premises for the foreseeable future), a private interconnect with no egress costs was a better option.
We decided to secure our data using PayPal-provided private keys in BigQuery, within a service perimeter offered by Google Cloud Platform. This ensured that data was secure and within a perimeter which cannot be accessed from outside. We deployed automation to prevent accidental creations of data sets that lack encryption keys. This way, we had encryption enabled by default for all data stored in Google Cloud Platform in a way that was compliant with our internal policies and external regulations.
We have used this infrastructure to copy more than 15 petabytes for BigQuery and 80+ petabytes into Google Cloud Services for various use cases. We use the same network infrastructure for users to access BigQuery through Jupyter Notebooks, Tableau, or from their scheduled jobs.
Regulatory Compliance and Pen-Testing
As a fintech organization that deals with PCI and PII data elements in our datasets, we worked with various regulators to file our intent to move data to the cloud. PayPal’s InfoSec, regional business units and legal teams worked overtime with our teams to prepare paperwork for regulators. We followed up with rounds of pen-tests to ensure there were no gaps. This helped us validate what we had designed into the infrastructure to secure the data and access to the data.
DDL (Data Definition Language) and SQL Conversion
Given that we were taking our data users to the cloud with a new technology, we wanted to ease the pain of transitioning from Teradata to BigQuery. To achieve this, we evaluated various options and selected a tool from CompilerWorks. Its transpiler allowed us to create DDLs in BigQuery, and use that schema to convert DMLs and user SQLs from Teradata flavor to BigQuery. PayPal worked to harden the transpiler configurations to generate performant, clean BigQuery-compatible SQLs.
This automated code conversion was a very critical step for us to get right as we wanted to ease the migration for our users. In addition to the code conversion, we also extracted valuable lineage data from CompilerWorks’s tool. We created an automation framework as well as a portal for interactive use and self-service code conversion. The automation kept polling for changes in on-premises infrastructure and create the equivalent in BigQuery as new artifacts got created. We requested users to use the portal to convert their existing or known SQLs to BigQuery-compatible SQLs for their testing and validation. We leveraged the same framework to convert users’ jobs, Tableau dashboards, and Notebooks for testing and validation. This automation helped us convert more than 10K SQLs.
Workloads, Schemas and Tables Identification
To scope the workloads, the team went through all Notebooks in our repository, Tableau dashboards and UC4 logs. Based on the tables we identified, we created a lineage graph to come up with a list of tables and schemas used, active scheduled jobs, notebooks, and dashboards. We validated the scope of work with users and confirmed that it was a true representation of the workloads on the cluster. This helped the team greatly reduce the number of workloads that we needed to migrate. Here is a breakdown of what was deprecated from the overall inventory.
Spending time on automation helped us separate out the used ones from unused ones and gain validation from users at the last step. Having users manually find this out would be tedious and error prone.
Data Movement, Loading and Validation
As we worked through this project, it became clear that data movement is very contextual to our setup, and off-the-shelf tools had limitations in being able to seamlessly copy data to Google Cloud Platform. This was the hardest part of the whole project. It was not the volume but accidental complexities that made it harder. Here is a quick list of problems we encountered:
Dry Runs and Wet Runs
Dry runs, an execution with no data, ensured that there were no syntax errors with the queries that were transformed. If the dry run was successful, we loaded data into the tables and requested users to conduct a wet run. Wet runs were one-off executions to test whether the result sets were all correct. We created test data sets for users to do their wet run, before they qualified their workload for production. All of this were enabled for users using our application lifecycle management portal which our users were accustomed to for deploying applications. We laid lots of emphasis on fitting our testing into an ecosystem that our users were used to.
Visibility into Progress
A number of the above activities were happening in parallel. This required coordination, which is harder for humans or coordinated spreadsheets. We kept track of all the data in BigQuery which was automatically updated as executions happened. We created dashboards to track sequences of activities and to report consistently to our execs and stakeholders on progress. These dashboards tracked the data copy progress for multiple milestones, workload rationalization and progress on readiness of notebooks, scheduled jobs and BI dashboards for dry and wet runs. A sample report looked like this. Users were able to search by DB name and table names for checking status.
Team Work Makes the Dream Work
This was very true in our case since many teams across PayPal came together to get to this landmark. We believe the following makes our story unique, and that helps us succeed:
Our user community at PayPal has transitioned and taken to BigQuery really well. Users needed initial help with project conventions (a new concept for them compared to Teradata) and with some help, they got productive very quickly. Users absolutely love the query performance, faster data load times and full visibility through BigQuery logs.
Taking PayPal through this migration helped us evaluate and observe what BigQuery and Google Cloud Platform can bring to PayPal. Data users now use SQL, as well as Spark through Notebooks and Google’s Dataproc over BigQuery. This helps us maintain a single copy of data along with visibility that Google Data Catalog provides for our data.
Plans are afoot to consolidate multiple datasets from finance, HR, marketing and third party systems like Salesforce along with site activity into BigQuery to enable faster business modeling and decision making. Teams are looking at streaming capabilities to inject site data sets directly to BigQuery for near real time usage to our analysts. In addition to BigQuery, some of our teams are also leveraging Google DataProc and Google Cloud Storage to consolidate many pieces of our open-source based data lake shown in Figure 1.
There are many employees at PayPal who worked directly and indirectly on this effort. Many employees in our India offices spent time on this effort while dealing with the raging pandemic. Our thanks to all of them!
Article credits:
Many thanks to Vaishali Walia who leads this program, and the entire Deloitte team for helping keep the migration on track.
Also thanks to Bala Natarajan who provided input for this article, and Melissa O’Malley, Michael Davis, and Parviz Deyhim who helped make it publication ready.
The PayPal Technology Blog
629 
6
629 claps
629 
6
The PayPal Technology Blog
Written by
Product Manager, Data Platform Products @ PayPal
The PayPal Technology Blog
"
https://itnext.io/architecting-a-successful-saas-eaa24c5ad6d7?source=search_post---------161,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Orginally published on the AWS APN Blog.
You’re a startup with an idea for a revolutionary new software product. You quickly build a beta version and deploy it to the cloud. After a successful social-marketing campaign and concerted sales effort, dozens of customers subscribe to your SaaS-based product. You’re ecstatic…until you realize you never architected your product for this level of success. You were so busy coding, raising capital, marketing, and selling, you never planned how you would scale your Sass product. How you would ensure your customer’s security, as well as your own. How you would meet the product reliability, compliance, and performance you promised. And, how you would monitor and meter your customer’s usage, no matter how fast you or they grew.
I’ve often heard budding entrepreneurs jest, if only success was their biggest problem. Certainly, success won’t be their biggest problem. For many, the problems come afterward, when they disappoint their customers by failing to deliver the quality product they promised. Or worse, damaging their customer’s reputation (and their own) by losing or exposing sensitive data. As the old saying goes, ‘you never get a second chance to make a first impression.’ Customer trust is hard-earned and easily lost. Properly architecting a scalable and secure SaaS-based product is just as important as feature development and sales. No one wants to fail on Day 1 — you worked too hard to get there.
In this series of posts, Architecting a Successful SaaS, we will explore how to properly plan and architect a SaaS product offering, designed for hosting on the cloud. We will start by answering basic questions, like, what is SaaS, what are the alternatives to SaaS for software distribution, and what are the most common SaaS product models. We will then examine different high-level SaaS architectures, review tenant isolation strategies, and explore how SaaS vendors securely interact with their customer’s cloud accounts. Finally, we will discuss how SaaS providers can meet established best practices, like those from AWS SaaS Factory and the AWS Well-Architected Framework.
For this post, I have chosen many examples of cloud services from AWS and vendors from AWS Marketplace. However, the principals discussed may be applied to other leading cloud providers, SaaS products, and cloud-based software marketplaces. All information in this post is publicly available.
According to AWS Marketplace, ‘SaaS [Software as a Service] is a delivery model for software applications whereby the vendor hosts and operates the application over the Internet. Customers pay for using the software without owning the underlying infrastructure.’ Another definition from AWS, ‘SaaS is a licensing and delivery model whereby software is centrally managed and hosted by a provider and available to customers on a subscription basis.’
A SaaS product, like other forms of software, is produced by what is commonly referred to as an Independent Software Vendor (ISV). According to Wikipedia, an Independent Software Vendor ‘is an organization specializing in making and selling software, as opposed to hardware, designed for mass or niche markets. This is in contrast to in-house software, which is developed by the organization that will use it, or custom software, which is designed or adapted for a single, specific third party. Although ISV-provided software is consumed by end-users, it remains the property of the vendor.’
Although estimates vary greatly, according to The Software as a Service (SaaS) Global Market Report 2020, the global SaaS market was valued at about $134.44B in 2018 and is expected to grow to $220.21B at a Compound annual growth rate (CAGR) of 13.1% through 2022. Statista predicts SaaS revenues will grow even faster, forecasting revenues of $266B by 2022, with continued strong positive growth to $346B by 2027.
Let’s start by reviewing the three most common ways that individuals, businesses, academic institutions, the public sector, and government consume services from cloud providers such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud, and IBM Cloud (now includes Red Hat).
Indirect users are customers who consume cloud-based SaaS products. Indirect users are often unlikely to know which cloud provider host’s the SaaS products to which they subscribe. Many SaaS products can import and export data, as well as integrate with other SaaS products. Many successful companies run their entire business in the cloud using a combination of SaaS products from multiple vendors.
Examples
Direct users are customers who use cloud-based Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) products to build and run their software; the DIY (do it yourself) model. The software deployed in the customer’s account may be created by the customer or purchased from a third-party software vendor and deployed within the customer’s cloud account. Direct users may purchase IaaS and PaaS services from multiple cloud providers.
Examples
Hybrid users are customers who use a combination of IaaS, PaaS, and SaaS products. Customers often connect multiple IaaS, PaaS, and SaaS products as part of larger enterprise software application platforms.
Examples
Most cloud-based software is sold in one of two ways, Customer-deployed or SaaS. Below, we see a breakdown by the method of product delivery on AWS Marketplace. All items in the chart, except SaaS, represent Customer-deployed products. Serverless applications are available elsewhere on AWS and are not represented in the AWS Marketplace statistics.
An ISV who sells customer-deployed software products to consumers of cloud-based IaaS and PaaS services. Products are installed by the customer, Systems Integrator (SI), or the ISV into the customer’s cloud account. Customer-deployed products are reminiscent of traditional ‘boxed’ software.
Customers typically pay a reoccurring hourly, monthly, or annual subscription fee for the software product, commonly referred to as pay-as-you-go (PAYG). The subscription fee paid to the vendor is in addition to the fees charged to the customer by the cloud service provider for the underlying compute resources on which the customer-deployed product runs in the customer’s cloud account.
Some customer-deployed products may also require a software license. Software licenses are often purchased separately through other channels. Applying a license you already own to a newly purchased product is commonly referred to as bring your own license (BYOL). BYOL is common in larger enterprise customers, who may have entered into an Enterprise License Agreement (ELA) with the ISV.
Customer-deployed cloud-based software products can take a variety of forms. The most common deliverables include some combination of virtual machines (VMs) such as Amazon Machine Images (AMIs), Docker images, Amazon SageMaker models, or Infrastructure as Code such as AWS CloudFormation, HashiCorp Terraform, or Helm Charts. Customers usually pull these deliverables from a vendor’s AWS account or other public or private source code or binary repositories. Below, we see the breakdown of customer-deployed products, by the method of delivery, on AWS Marketplace.
Although historically, AMIs have been the predominant method of customer-deployed software delivery, newer technologies, such as Docker images, serverless, SageMaker models, and AWS Data Exchange datasets will continue to grow in this segment. The AWS Serverless Application Repository (SAR), currently contains over 500 serverless applications, not reflected in this chart. AWS appears to be moving toward making it easier to sell serverless software applications in AWS Marketplace, according to one recent post.
Customer-deployed cloud-based software products may require a connection between the installed product and the ISV for product support, license verification, product upgrades, or security notifications.
Examples
An ISV who sells SaaS software products to customers. The SaaS product is deployed, managed, and sold by the ISV and hosted by a cloud provider, such as AWS. A SaaS product may or may not interact with a customer’s cloud account. SaaS products are similar to customer-deployed products with respect to their subscription-based fee structure. Subscriptions may be based on a unit of measure, often a period of time. Subscriptions may also be based on the number of users, requests, hosts, or the volume of data.
A significant difference between SaaS products and customer-deployed products is the lack of direct customer costs from the underlying cloud provider. The underlying costs are bundled into the subscription fee for the SaaS product.
Similar to Customer-deployed products, SaaS products target both consumers and businesses. SaaS products span a wide variety of consumer, business, industry-specific, and technical categories. AWS Marketplace offers products from vendors covering eight major categories and over 70 sub-categories.
I regularly work with a wide variety of cloud-based software vendors. In my experience, most cloud-based SaaS products fit into one of four categories, based on the primary way a customer interacts with the SaaS product:
A stand-alone SaaS product has no interaction with a customer’s cloud account. Customers of stand-alone SaaS products interact with the product through an interface provided by the SaaS vendor. Many stand-alone SaaS products can import and export customer data, as well as integrate with other cloud-based SaaS products. Stand-alone SaaS products may target consumers, known as Business-to-Consumer (B2C SaaS). They may also target businesses, known as Business-to-Business (B2B SaaS).
Examples
A SaaS product that connects to a customer’s data sources in their cloud account or on-prem. These SaaS products often fall into the categories of Big Data and Data Analytics, Machine Learning and Artificial Intelligence, and IoT (Internet of Things). Products in these categories work with large quantities of data. Given the sheer quantity of data or real-time nature of the data, importing or manually inputting data directly into the SaaS product, through the SaaS vendor’s user interface is unrealistic. Often, these SaaS products will cache some portion of the customer’s data to reduce customer’s data transfer costs.
Similar to the previous stand-alone SaaS products, customers of these SaaS products interact with the product thought a user interface provided by the SaaS vendor.
Examples
A SaaS product that interacts with, and augments a customer’s application, which is managed by the customer in their own cloud account. These SaaS products often maintain secure, loosely-coupled, unidirectional or bidirectional connections between the vendor’s SaaS product and the customer’s account. Vendors on AWS often use services like Amazon EventBridge, AWS PrivateLink, VPC Peering, Amazon S3, Amazon Kinesis, Amazon SQS, and Amazon SNS to interact with customer’s accounts and exchange data. Often, these SaaS products fall within the categories of Security, Logging and Monitoring, and DevOps.
Customers of these types of SaaS products generally interact with their own software, as well as the SaaS product thought an interface provided by the SaaS vendor.
Examples
Discrete SaaS products are a variation of SaaS augmentation products. Discrete SaaS products provide specific, distinct functionality to a customer’s software application. These products may be an API, data source, or machine learning model, which is often accessed completely through a vendor’s API. The products have a limited or no visual user interface. These SaaS products are sometimes referred to as a ‘Service as a Service’. Discrete SaaS products often fall into the categories of Artificial Intelligence and Machine Learning, Financial Services, Reference Data, and Authentication and Authorization.
Examples
There is a new category of products on AWS Marketplace. Released in November 2019, AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. According to AWS, Data Exchange vendors can publish new data, as well as automatically publish revisions to existing data and notify subscribers. Once subscribed to a data product, customers can use the AWS Data Exchange API to load data into Amazon S3 and then analyze it with a wide variety of AWS analytics and machine learning services.
Data Exchange seems to best fit the description of a customer-deployed product. However, given the nature of the vendor-subscriber relationship, where data may be regularly exchanged — revised and published by the vendor and pulled by the subscriber — I would consider Data Exchange a cloud-based hybrid product.
AWS Data Exchange products are available on AWS Marketplace. The list of qualified data providers is growing and includes Reuters, Foursquare, TransUnion, Pitney Bowes, IMDb, Epsilon, ADP, Dun & Bradstreet, and others. As illustrated below, data sets are available in the categories of financial services, public sector, healthcare, media, telecommunications, and more.
Examples
In this first post, we’ve become familiar with the common ways in which customers consume cloud-based IaaS, PaaS, and SaaS products and services. We also explored the different ways in which ISVs sell their software products to customers. In future posts, we will examine different high-level SaaS architectures, review tenant isolation strategies, and explore how SaaS vendors securely interact with their customer’s cloud accounts. Finally, we will discuss how SaaS providers can meet best-practices, like those from AWS SaaS Factory and the AWS Well-Architected Framework.
Here are some great references to learn more about building and managing SaaS products on AWS.
This blog represents my own view points and not of my employer, Amazon Web Services.
Cloud computing image — copyright melpomen (123rf.com)
ITNEXT is a platform for IT developers & software engineers…
69 
69 claps
69 
Written by
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://m.subbu.org/how-to-think-about-multi-cloud-46ea0dfba8dd?source=search_post---------162,"Public clouds are no longer equivalent. These are platforms with some overlapping basic capabilities and yet different in many respects. Here are a few factors I would consider when thinking of multiple clouds.
The same considerations apply for hybrid-cloud too.
A blog on tech and leadership
20 
3
Some rights reserved

20 claps
20 
3
Written by
See https://www.Subbu.org
A blog on tech and leadership
Written by
See https://www.Subbu.org
A blog on tech and leadership
"
https://medium.com/capital-one-tech/going-public-on-the-cloud-9449528bd4bf?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
I’ve lived and worked in Silicon Valley my entire career, mostly focused on areas on the frontier of technology. A decade ago I learned about cloud computing and immediately recognized what it represented: a new platform that would transform the world of tech for both users and vendors. I threw myself headlong into the world of cloud computing, convinced I wanted to participate in the revolution.
Since then I’ve worked on cloud projects all over the world. I’ve consulted with enterprises and service providers, and spent time at two cloud software startups that ended up being acquired by incumbent technology vendors.
A little less than a year ago I joined Capital One as Vice President of Cloud Strategy. A number of people I know in the cloud community pinged me to ask why I joined an end user organization, because they found it a little surprising — people on the vendor side rarely move to IT users. But to my mind my choice to join Capital One wasn’t just logical, it represents alignment with the future of technology.
Let me explain.
Traditionally, one of the biggest barriers to effective IT has been friction in the system. Provisioning resources takes weeks to months. Configuring and operating them is difficult and error-prone. Changing an installed application to increase resources because of growing user load…well, you’re back at the weeks to months provisioning bottleneck.
Cloud gets rid of all that friction. Computing resources are available in minutes. And the cloud provider takes care of configuring and managing them. Magic, right?
It’s true, but many IT organizations approach this incorrectly. They treat cloud computing like a data center at the end of the wire. So they leave their traditional SDLC practices in place. The impedance mismatch between cloud speed and ITIL speed means they don’t achieve any observable improvement in their IT outcomes.
What you need to really take advantage of cloud computing is a complete rethink of your approach to IT. Yes, you get your infrastructure from a cloud provider. But you also implement agile development practices. You implement DevOps deployment methods. And you use SRE operations processes. The overall approach is often called cloud-native. We all know cloud-native companies, because we use them every day. Netflix. Spotify. Lyft. Airbnb. It’s no accident that cloud-native and disruption go hand-in-hand. Using the power of cloud-native practices, these companies force transformation into industries. And leaves traditional companies with no hopes of catching up.
And that brings me to Capital One. A number of years ago it looked at banking and concluded it was ripe for disruption. After all, banking is inherently a digital business, so it makes sense that using the most advanced IT practices could give the company a competitive advantage.
So it adopted agile. And DevOps. And decided to go all-in on cloud computing. Along with that, Capital One recognized it needs cloud-native talent, so it recruited heavily to bring news skills in-house. When I look around my office, it feels like a cutting-edge software company, and the conversations I have are those that could take place at LinkedIn or Salesforce.
So when Capital One reached out to me to help drive its cloud strategy I responded enthusiastically I joined Capital One because I believe it represents the future of how enterprises will need to respond to the cloud-native world. Every industry and every company will have its Lyft (or Airbnb, or Spotify) moment as customers and partners demand digital-based offerings.
I believe that our economy will see more disruption in the next decade than occurred in the entire twentieth century. Every person in tech will need to decide where he or she wants to experience that disruption: within a company surfing the cloud-native wave or one being inundated by it.
Did I mention we’re hiring at Capital One?
DISCLOSURE STATEMENT: These opinions are those of the author. Unless noted otherwise in this post, Capital One is not affiliated with, nor is it endorsed by, any of the companies mentioned. All trademarks and other intellectual property used or displayed are the ownership of their respective owners. This article is © 2018 Capital One.
The low down on our high tech from the engineering experts…
58 
2
58 claps
58 
2
Written by
Named by Wired.com as one of the ten most influential persons in cloud computing. Learn more at bernardgolden.com
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
Written by
Named by Wired.com as one of the ten most influential persons in cloud computing. Learn more at bernardgolden.com
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
"
https://medium.com/simply-technology/cloud-bubble-e5946aac6e7a?source=search_post---------164,"There are currently no responses for this story.
Be the first to respond.
Is your organization in a ‘cloud bubble’?
In financial circles, an asset bubble exists when:
the price for an asset exceeds its fundamental value by a large margin.
With public cloud services, it can be tricky to nail down things like asset, price, and especially fundamental value. Businesses that operate data centers manage physical assets — racks, servers, switches, and facilities. You…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/multi-cloud-hybrid-cloud-and-cloud-spend-statistics-on-cloud-computing-ba4c194d2e10?source=search_post---------165,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2019·4 min read
The latest statistics on cloud computing all point to multi-cloud and hybrid cloud as the reality for most companies. This is confirmed by what we see in our customers’ environments, as well as by what industry experts and analysts report. At last week’s CloudHealth Connect18 in Boston we heard from Dave Bartoletti, VP and Principal Analyst at Forrester Research, who broke down multi-cloud and hybrid cloud by the numbers:
More often than not, public cloud users and enterprises have adopted a multi-cloud or hybrid cloud strategy to meet their cloud computing needs. Taking advantage of features and capabilities from different cloud providers can be a great way to get the most out of the benefits that cloud services can offer, but if not used optimally, these strategies can also result in wasted time, money, and computing capacity.
The data is telling — but we won’t stop there. For more insight on the rise of multi-cloud and hybrid cloud strategies, and to demonstrate the impact on cloud spend (and waste) — we have compiled a few more statistics on cloud computing.
The statistics on cloud computing show that companies not only use multiple clouds today, but they have plans to expand multi- and hybrid cloud use in the future:
As enterprises’ cloud footprints expand, so too does their spending:
“Cloud is an inexpensive and easily accessible technology. People consume more, thereby spending more, and forget to control or limit their consumption. With ease of access, inevitably some resources get orphaned with no ownership; these continue to incur costs. Some resources are overprovisioned to provide extra capacity as a ‘just in case’ solution. Unexpected line items, such as bandwidth, are consumed. The IT department has limited visibility or control of these items.”
We’ve noticed some interesting patterns in the cloud platforms adopted by ParkMyCloud users as well, which highlight the multi-cloud trends discussed above as well as correlations between the types of companies that are attracted to each of the major public clouds. We observed:
Upon examining these statistics on cloud computing, it’s clear that multi-cloud and hybrid cloud approaches are not just the future, they’re the current state of affairs. While this offers plenty of advantages to organizations looking to benefit from different cloud capabilities, using more than one CSP complicates governance, cost optimization, and cloud management further as native CSP tools are not multi-cloud. As cloud costs remain a primary concern, it’s crucial for organizations to stay ahead with insight into cloud usage trends to manage spend (and prevent waste). To keep costs in check for a multi-cloud or hybrid cloud environment, optimization tools that can track usage and spend across different cloud providers are a CIO’s best friend.
Originally published at www.parkmycloud.com on September 18, 2018
CEO of ParkMyCloud
12 
12 
12 
CEO of ParkMyCloud
"
https://medium.com/google-cloud/twigcp168-aadbc5fa7644?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
If you came here from This Week in Google Cloud’s video series here are the links for the topics covered this week :
Here are the main GCP stories for this past week :
First, we’re just a couple of weeks away from Cloud Next ’19. Make sure to register or make plans to follow Next OnAir ! Everyone on the team is incredibly excited (and busy)!
“Istio / Announcing Istio 1.1” (istio.io). This is a release focused on performance and scalability. The control plane memory consumption was a particular area of focus.
“NVIDIA’s RAPIDS joins our set of Deep Learning VM images for faster data science” (Google blog). Try these one-click setup images if you’re into deep learning and haven’t already. You’ll probably find out that they’re incredibly useful and a true time-saver.
“Now generally available: Plug-in for VMware vRealize Automation” (Google blog). More reasons for VMware customers to manage and consume Google Cloud resources.
From the “This Week in Machine Learning” department :
From still my favorite “Customers and partners talk best about GCP” department :
From the “Solutions for security, privacy, and performance“ department :
From the “This week in GCP on Medium” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
43 
43 claps
43 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/twigcp167-85d30bfe7cff?source=search_post---------167,"There are currently no responses for this story.
Be the first to respond.
Here are the main stories for this past week :
“New GCP region in Zurich: Growing our support for Swiss and European businesses” (Google blog). This new region has three availability zones and it is the 6th region in EMEA and 19th Worldwide.
“Site Reliability Engineering: Measuring and Managing Reliability” (Coursera). SRE is Google’s DevOps implementation and this new Coursera course is *the* place to learn more.
Google Cloud Technology Partners (cloud.google.com). Check out which resources partners have access to, starting with our SLO Guide.
“The service mesh era: Using Istio and Stackdriver to build an SRE service” (Google blog). How to easily surface application metrics with Istio and Stackdriver Monitoring with a step-by-step tutorial to follow.
From the “if you like data and basketball, this is nirvana” department :
From the “Continuous Build|Integration|Deployment|Delivery” department :
From the “I bet your Pi day wasn’t anything like Emma’s Pi day” department :
From the “another busy week across all of GCP” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/cloud-simplified/aws-transit-gateway-what-it-is-benefits-and-limitations-a001f1975e06?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Written by Abhinay Dronavalli
Among the announcements, AWS re:Invent 2018 attendees are buzzing about is the AWS Transit Gateway designed to simplify network management. IT fraternity seems to love the number 1 (one click, one view), and this new tool continues that theme, merging cloud resources and on-prem datacenters into one network topology.
Why is this important? According to Amazon, their Virtual Private Cloud (VCP) is one of the most popular and essential features of Amazon Web Services. Highly configurable and controllable, customers tend to create many — even hundreds– of VCPs. And this can lead to connectivity chaos.
Enter the Transit Gateway which can create connections between VPCs beyond the abilities of the peering solutions previously used. With a single set of controls, you can connect VCPs you already have in play, worldwide offices, and datacenters — even across multiple AWS accounts.
Essentially, Transit Gateways give you a way to simplify network architecture, reduce operational overhead, and centrally manage external connectivity.
You can use the command-line interface (CLI), AWS Management Console, or AWS CloudFormation to create and manage your AWS Transit Gateway. AWS Transit Gateway is integrated with Identity and Access Management (IAM), enabling you to manage access to AWS Transit Gateway securely.
AWS Transit Gateway is available in US East (Virginia), US East (Ohio), US West (Oregon), US West (Northern California), EU (Ireland), and AsiaPacific (Mumbai) AWS Regions. Support for other AWS Regions should be coming soon. Pricing details can be found here.
You can also use Nutanix Beam to centralize cloud governance controls across multiple teams to track entire cloud spend and map consumption to business units. Beam visualizes resources by groups and departments, empowering cloud operators to manage their usage. Try it free
An inclusive approach to cloud management.
4 
4 claps
4 
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@altexsoft/8-cloud-migration-best-practices-to-make-sure-you-do-it-right-70095f1503ec?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
AltexSoft Inc
Feb 13, 2019·7 min read
This is a guest article by Limor Wainstein from Agile SEO.
An increasing number of enterprises and organizations view migrating some of their IT workloads to the public cloud as not just an intelligent choice, but as pivotal to their IT strategies.
There is, however, a need to ensure a smooth cloud migration. Previously, we helped you prepare for the hybrid cloud, and this article describes the motives for moving to the public cloud. Important considerations before migrating include potential cloud migration challenges, and eight best practices to make sure you get your cloud migration right.
Going by industry-projections alone, LogicMonitor’s Cloud Vision 2020: The Future of the Cloud Study predicts that 41 percent of enterprise workloads will be run on public cloud platforms by 2020.
The public cloud is a computing model in which third-party providers make computing resources and services, such as virtual machines, applications, databases, and storage, available and accessible to customers via the public Internet. While it’s unlikely that on-premise workloads will become completely obsolete due to the explosion in cloud adoption, the numbers are hard to ignore.
However, the main arguments for moving to the public cloud are slightly more nuanced than noting that everyone else seems to be doing it and simply following suit. More specifically, some of the more compelling benefits of the public-cloud model are:
Despite the obvious numerous benefits of migrating to the public cloud, there are several important challenges to bear in mind before you make the move. Some of the more pertinent challenges are:
Given that organizations often cite a single hour of downtime as costing over $100,000, it’s crucial to conduct any public cloud migration in such a way that disruption of applications is minimized. If users or customers can’t properly access apps or data, your operations suffer and costs quickly mount.
With new stories emerging all the time highlighting major data breaches, there is a real need for organizations to ensure not only that the data they move to the cloud is secure, but also that they remain compliant with relevant regulations in moving to the cloud.
Given that one of the main draws of the public cloud is the service model’s cost reduction compared to provisioning on-premise infrastructure, it’s worth noting that in some studies, cloud users have given estimates that up to 30 percent of their organization’s cloud spend has been wasted. It’s clear that the public cloud is only as cost-effective as you make it.
There is a skills shortage in terms of personnel capable of securing cloud environments and ensuring that cloud migrations go as smoothly as possible. It’s not enough to have people who understand IT security because public cloud security requires knowledge of the major platforms and their nuances, such as AWS and its identity and access management.
Check NetApp’s article on cloud migration strategy for more on the challenges and steps involved in a public cloud migration.
A best practice you can’t ignore is to begin by mapping out a migration strategy that identifies clear business motives and use cases for moving to the cloud. Perhaps the most advisable strategy is to migrate in phases or conduct a pilot light migration in which you start with the least business-critical workloads that can give you the experience and confidence to move forward with a larger migration.
You could also borrow from Gartner’s five Rs that highlight your options for migrating applications to the cloud. These options are rehost on infrastructure as a service (IaaS), refactor for platform as a service (PaaS), revise for IaaS or PaaS, rebuild on PaaS, or replace with software as a service (SaaS).
Given that compliance and security are among the top concerns for organizations moving to the cloud, it is critical to create a cloud governance framework with clear, policy-based rules that help organizations to prepare for a secure cloud adoption.
Cloud governance is an extension of IT governance that takes into account the inherent risks of trusting data and apps to third-party services. It defines ways of doing things — tools, procedures, skills, and competencies — so that the company migrating to the cloud can do so with minimal risk and maximum value.
Good cloud governance incorporates a wealth of things such as structures, roles, responsibilities, policies, plans, objectives, principles, measures, and a decision framework.
The default network used by public cloud providers is the public Internet. Some organizations, such as large enterprises, might worry that the Internet is too slow and not secure enough to meet their business goals.
Some third-party public cloud providers recognize that an Internet connection might not be the optimal choice. AWS, for example, offers a dedicated network connection to its infrastructure from company offices. Azure has its ExpressRoute service that establishes a connection directly between your network and Azure, bypassing the Internet.
If a dedicated network connection is not necessary, it is still worthwhile to pursue a better, faster service from your Internet Service Provider given that moving to the cloud encompasses users transitioning from accessing data or apps locally via gigabit-speed local network connections to much slower Internet connections.
Because companies often cite a lack of cloud expertise or a cloud skills gap as barriers to migration, it makes sense to train staff in your chosen cloud platforms as early as possible. Due to the level of abstraction the cloud introduces and the inherently different design of public cloud systems, it might be wise to establish a series of training sessions designed to get employees across different teams up to speed in cloud concepts.
By training staff early, you can give them a better chance of adapting to the new ways of doing things in a timely manner.
It is vital to properly manage software licensing in the cloud. A real concern for enterprises is whether their existing licenses for on-premise software extend to the cloud. Some software vendors operate a Bring Your Own Software and License (BYOSL) program that gives enterprises express permission to migrate their applications to the cloud. Other vendors specify usage rights per number of concurrent users.
A solid approach is to document all enterprise applications and closely study their licensing rules in relation to cloud computing. In circumstances where it is unclear, talk to the vendor to see if existing licenses you have purchased can be updated for the application to be used in the cloud. Software Asset Management (SAM) tools can prove useful in reducing risks, costs and complexities associated with extending license management to the cloud.
Downtime or service disruptions are not desirable outcomes for any cloud migration strategy. To minimize disruption and improve the overall efficiency of the migration, it is an important best practice to automate repeated patterns where possible. Automation not only speeds up the process of migration, it also lowers both cost and risk.
There are even tools that aim to help you automate the migration of virtual machines and data. Scripts can also prove useful, such as when you need to change a database from an on-premise one to a cloud version. Automation and the phased cloud migration approach can work in tandem, as you identify repeated patterns over time that you can automate in subsequent migration phases.
You should monitor cloud usage from the outset if you want to avoid adding your company’s funds to the statistic of 35 percent of cloud budgets that are wasted. A centralized dashboard that identifies running instances across different cloud services can really help you out here.
Monitoring for compliance and security is also crucial, and you’ll ideally want to collect logs from apps, systems, databases, and network touchpoints to ensure information security requirements are being met.
If you’ve done your due diligence on researching cloud service providers, you’ll have factored into your decision — or at least you should have factored it in — the level of support you’ll have. A good support team can provide a critical ally during any cloud migration project. Cloud support staff are experts in the particular service they work for, and they should be able to promptly answer technical questions or help you with any issues you have.
It’s becoming increasingly clear that if businesses of all sizes want to remain competitive, they should be looking to migrate some of their workflows to the public cloud. Actually conducting the migration in a smooth and efficient way is no small feat, though. By following the best practices outlined here, your company increases the probability of doing it right.
Limor is a technical writer and editor at Agile SEO, a boutique digital marketing agency focused on technology and SaaS markets. She has over 10 years’ experience writing technical articles and documentation for various audiences, including technical on-site content, software documentation, and dev guides. She specializes in big data analytics, computer/network security, middleware, software development and APIs.
Want to write an article for our blog? Read our requirements and guidelines to become a contributor.
Originally published at AltexSoft Tech Blog “8 Cloud Migration Best Practices to Make Sure You Do It Right”
Being a Technology & Solution Consulting company, AltexSoft co-builds technology products to help companies accelerate growth.
4 
1
4 
4 
1
Being a Technology & Solution Consulting company, AltexSoft co-builds technology products to help companies accelerate growth.
"
https://medium.com/memory-leak/our-investment-in-opsani-continuous-cloud-optimization-b893800fe5e7?source=search_post---------170,"There are currently no responses for this story.
Be the first to respond.
During my time on Cisco’s Corporate Development team I supported the UCS server division and had a firsthand view of the massive shift in computing architecture to the cloud. Companies replatform to the cloud to decrease operational burdens, increase agility, and improve scalability.
While using cloud infrastructure, teams often emphasize accelerated software delivery to remain competitive over performance and cost optimization. Parkmycloud estimates $14.1B of cloud compute spending will be wasted in 2019. I discussed cloud expensive management as a key 2019 DevOps trend here.
Businesses use performance engineers and cloud consultants to manually conduct point in time performance/cost analysis. Unfortunately, code is updated so frequently today, their recommendations are quickly out-of-date and useless. We believe teams can have it all — incredible application user experiences at optimized costs.
Enter Opsani, a continuous cloud optimization solution to fine-tune runtime environments. Using ML models, Opsani continuously assesses the efficient frontier between the performance and the cost of running services in the cloud. Opsani’s models evaluate more than just the infrastructure layer like instance types and sizing, but also additional application parameters like middleware configuration, garbage collection, worker threads, etc.
Moreover, Opsani eliminates previously manual actions through automation, taking the burden off engineering and DevOps teams. Before each deployment Opsani runs a canary test of the new service in the production and applies its AI to evaluate the ideal runtime environment. The application layer visualizes the ML analysis, and Opsani’s explainable AI makes suggestions that are easily understood in plain language. DevOps teams can choose to individually apply the recommendations or automatically push them to production.
We became excited about Opsani for multiple reasons:
Serial entrepreneurs Ross Schibler and Peter Nickolov founded Opsani. Ross is a third-time Redpoint entrepreneur having previously founded Rapid City (acquired by Nortel) and Topspin (acquired by Cisco). Ross served as the CTO for Cisco’s data center servers, storage, and networking BU. Peter founded 3tera (acquired by CA), which was one of the first cloud operating systems and became CA’s fastest-growing product. At CA, he was the technical leader for the AppLogic Cloud Platform leading teams of hundreds of employees globally. The team is well-positioned in the cloud optimization space as they have firsthand experience transitioning companies to the public cloud, building solutions for cloud environments, and managing businesses at scale.
We’re excited to have led the Series A investment in Opsani alongside existing investors Zetta Venture Partners and Bain Capital Ventures. We are thrilled to partner with Ross, Peter, and the entire team on the journey forward. If you are interested in joining the stellar team, please check out their careers page here.
VC Astasia Myers’ perspectives on distributed computing…
8 
8 claps
8 
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
"
https://medium.com/@jaychapel/4-types-of-idle-cloud-resources-that-are-wasting-your-money-435e0f7dd29e?source=search_post---------171,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 16, 2020·4 min read
Every year, an exorbitant amount of money is wasted on idle cloud resources. That is — resources that are provisioned, and being paid for, but not actually being used. This is a huge problem that clogs up cloud environments and drains budgets.
Note: a version of this blog was originally published in 2018. It has been completely updated and rewritten for 2020.
The issue of idle resources is something that is recognized even by the cloud providers themselves. This may sound counterintuitive. Doesn’t AWS just want as much money from you as it can get? Well, maybe, yes: but the best way for them to do this is by providing you with a positive experience and the most value for your money.
Case in point: at the AWS re:Invent keynote this week, Andy Jassy spoke about a few core guidelines for organizations to follow to ensure organizations are on the path for successful technology financial management. “Start early and start small…The key is to start experimenting with what matters the most to your organization” Jassy said. He shared that a great place to start is by deleting or stopping idle resources in your cloud environment. Small changes like this can have huge impacts and benefits can increase as time goes on. Idle resources are eating at your cloud budget causing you to spend money on resources that aren’t even being used.
AWS’s cloud financial management framework mentions this among the myriad ways your organization can improve practices to reduce usage waste and optimize costs.
The typical “idle resources” that come to mind are instances purchased On Demand that are being used for non-production purposes like development, testing, QA, staging, etc. These resources can be “parked” when they’re not being used, such as on nights and weekends, saving 65% or more per resource each month. In order to fully understand the problem of idle cloud resources, we have to expand this scope beyond just your typical virtual machine.
Most non-production resources can be parked about 65% of the time, that is, parked 12 hours per day and all day on weekends (this is confirmed by looking at the resources parked in ParkMyCloud — they’re scheduled to be off just under 65% of the time.) We see that our customers are paying their cloud providers an average list price of $220 per month for their instances. If you’re currently paying $220 per month for an instance and leaving it running all the time, that means you’re wasting $143 per instance per month.
Maybe that doesn’t sound like much. But if that’s the case for 10 instances, you’re wasting $1,430 per month. One hundred instances? You’re up to a bill of $14,300 for time you’re not using. And that’s just a simple micro example. At a macro level that’s literally billions of dollars in wasted cloud spend.
So what kinds of resources are typically left idle, consuming your budget? Let’s dig into that, looking at the big three cloud providers — Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).
Cloud waste is a billion-dollar problem facing businesses today. Make sure you’re turning off idle cloud resources in your environment, by parking those that can be stopped and eliminating those that can’t, to do your part in optimizing cloud spend.
Originally published at www.parkmycloud.com on December 4, 2020
CEO of ParkMyCloud
6 
6 
6 
CEO of ParkMyCloud
"
https://medium.com/@mikethebbop/plan-on-long-sales-cycles-for-software-designed-to-enable-enterprise-migrations-to-the-cloud-d60ad6ab1c1?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ira Michael Blonder
Oct 11, 2018·4 min read
Early stage software vendors marketing solutions meant to help enterprise-class organizations migrate from on-premises data centers to public tenants “in the cloud” should plan on long sales cycles. Examples of these solutions include technical help systems, data translation tools, and cyber security tools.
The approval process for any of these solutions can take a year or longer to complete and usually exhibits predictable stages:
The first stage amounts to a scouting expedition led by people in technical roles (administrators, analysts, even software designers). These people usually collect information from websites, occasionally use website chat/email/telephone call (rarely) to determine cost information, and sit through product demonstrations. Once they have collected the information they need, they then report back to internal stakeholders. The decisions made by the scouting team amount to separating products of interest from the rest.
For products selected for a second stage, the interaction usually includes a presentation for a larger group. Planning a successful presentation should be a paramount concern for the sales team working on the opportunity. Missing steps can actually end up extending the sales cycle, if they are lucky to recover and proceed further with the prospect.
People responsible for budgets, and authorized to purchase the core components required for a successful migration, can and should be included as attendees. Likewise representatives from any business lines, or internal departments sure to contribute to an implementation plan for the migration and, necessarily, the product under scrutiny, should be invited to attend the presentation. For heavily regulated organizations like financial institutions, health care systems/hospitals, energy companies, this means the sales team must identify people from compliance, internal audit, even risk management and get them invited to the presentation.
If the presentation is to be a win/win, the audience should get their presentation, but the sales team should also get a lot of information critically important to designing a sales plan for the opportunity. It is mission critical therefore, for the sales team, to be experienced and happy using the opening moments of the presentation to ask a set of planned open-ended questions and just listen. Jumping into the presentation is usually a big mistake.
The first two stages can take 3–6 months to transpire. Why? Because the magnitude of the impact on an enterprise-class organization migrating daily computing processes from an on-premises data center to a public tenant in the cloud is enormous. There are an extensive set of factors to be considered and planned for. Consideration and planning take time. As well, executives must be convinced to support the migration decision and budgets have to be planned and allocated.
The third stage amounts to testing the solution, internally, by the prospect. I have seen this stage, alone, take as long as 3 months to enfold. Depending on the impact of a product on an organization, and its migration plan, internal testing can even take longer. Usually higher ticket solutions will require more testing.
Once again, the sales team should be careful to work directly with the prospect to plan the internal testing schedule and scope. Too many sales teams simply cede the entire internal testing phase over to the prospect. They take a step back and adopt an entirely reactive position simply “checking in” periodically with people to take the pulse of interest in the solution and the pace of the internal testing.
Not smart. The internal testing phase should be designed to demonstrate for decision-makers the value proposition implicit to a purchase of the product. The software sales team should be comfortable working closely with their counterparts at the prospect on a fair estimate of just how much value the product will deliver to a successful migration effort.
An additional factor amounts to how the prospect is interacting with its targeted IaaS/PaaS/SaaS provider. This interaction is entirely out of the control of the software sales team, but will play a direct role in just what decision the prospect ends up making on the purchase. Such is life for products captive to specific platforms.
Adding in the time required for the prospect to progress to a successful conclusion of our stage three, we are up to approximately nine months of time spent from the original date the software company was contacted.
The fourth stage is procurement. This stage can take the remaining 3 months to round us to a year spent on the opportunity. License agreements will need to be reviewed by the prospect’s legal team. If the prospect goes outside for license agreement review, it might take a month or longer to finalize an agreement acceptable to the prospect and the software company.
At the same time Procurement has been negotiating the purchase price. For some enterprise-class organizations the objective for Procurement’s role is to deliver a discount below pricing already negotiated. Where the prospects business architecture includes a lot of silos, the pricing process will, for sure, include some “u turns”, which the software sales team should have already anticipated and planned for.
Once these four steps have been completed (believe me, there are many “sub steps” throughout a “typical” sales engagement) a decision to purchase can simply end up on an executive’s desk somewhere if the corporate culture rewards moving slowly.
What’s the point of illuminating these four steps? My hope is anyone reading this story will get a sense of why enterprise software is a complex type of sale. The class of enterprise software with prominent market interest, today, is related to cloud migrations. Product marketers at early stage software companies with solutions for this market will do well if they assume long sales cycles and charge accordingly for them.
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
10 
10 
10 
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
"
https://medium.com/@jaychapel/how-to-use-an-aws-edp-for-discounted-cloud-resources-876647b374b3?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 28, 2020·4 min read
Lately, many of our AWS customers (especially those purchasing through the AWS marketplace) have mentioned that they are using an AWS EDP, which stands for Amazon Web Services Enterprise Discount Program. Essentially, this is AWS’s way to provide enterprises a discount off its services based on a volume (consumption) commitment. In the most recent Flexera State of the Cloud Report, 37% of respondents using AWS reported using an EDP.
A simple example of how an AWS EDP or “AWS Enterprise Agreement” might work is as follows: for the next 3 years, you commit to spend $5MM on AWS services, and receive a 13% discount. Even if you don’t spend $5MM you would still owe them $5MM, and of course if you go over you would get billed for the overage. Of course, the terms and amounts are all up to negotiation with AWS.
AWS’s website does not provide a lot of information about these agreements, which is perhaps to be expected considering they will customize the terms for any given customer. Here’s what they say: “Customers also have the option to enroll in an Enterprise Agreement with AWS. Enterprise Agreements give customers the option to tailor agreements that best suit their needs. For additional information on Enterprise Agreements please contact your sales representative.”
There are a few things you should consider about the EDP contract terms you agree upon with AWS. For example, the agreement may be limited to certain accounts, services, and/or regions.
You’ll see big numbers in the news, such as Apple’s $30 million monthly on AWS or Pinterest’s $750 million multi-year deal — but even if you’re not a tech giant or a unicorn startup, an Amazon EDP can still be on the table and a way to get an across-the-board discount.
Going back to my days at IBM, we used to generally refer to discount contracts as Enterprise License Agreements (ELAs). An ELA is a software site license that is sold to large enterprises. It typically allows for unlimited use of single or multiple software products throughout the organization, although there were often some restrictions and limitations. During my time at IBM, these were sold upfront for a set dollar amount and term, generally 3 to 5 years and usually had a cap on usage, so at some point overages could kick in — which would help with the renegotiation, of course.
Other terms used with a similar concept include Site License, Enterprise Agreement (this is a common Microsoft term — EA), Volume Purchase Agreement (VPA) and All You Can Eat (AYCE). What all of these have in common is that the vendor gets a large revenue/spend commit, and the enterprise gets discounting and flexibility.
AWS provides enterprises with multiple ways to consume its services based on their business needs and get volume discounts. Traditional on-demand instances allow you to pay for capacity by the hour without any long-term commitments or upfront payments. AWS Savings Plans are a way to save by committing to use at a micro scale: you commit to a certain amount of spend per hour, and in return get a discount on the VMs you’re already running. The less flexible reserved instances are another option for applications with steady-state or predictable usage and can provide up to a 75% discount compared to on-demand pricing. Especially for smaller organizations, there are a number of ways to get AWS credits to ease the burden. And of course they promote scale groups, spot instances, and other optimization efforts to reduce spend and waste but those are more cost control opportunities then they are discounts. Plus, you can always wait for better pricing.
Whether you participate in this program is somewhat predicated on your existing partner relationship and amount of spend with AWS, but you can always reach out to your AWS representative. Before committing to an AWS EDP, ensure that you are confident your organization will consume the amount of resources you are committing to. Keep in mind that this can also include the AWS Marketplace. The third party solutions you can buy on the AWS Marketplace also count toward your AWS EDP, and leverage that discount structure — so before completing a third-party transaction, make sure you check the Marketplace to see if the cloud solution you buy is listed there.
Originally published at www.parkmycloud.com on October 22, 2020.
CEO of ParkMyCloud
9 
9 
9 
CEO of ParkMyCloud
"
https://lab.wallarm.com/on-prem-vs-cloud-5deaf3e79a86?source=search_post---------174,"The only constant in this world is change, and these days it’s coming quicker and faster than ever before, as is evident in the explosive market for cloud services.
A recent research and analysis from Cisco showed that the global internet traffic to and from different cloud services has been growing at a rate of 30 percent each year. So whether you like it or not, the cloud is how we structure, architect, use, and secure the environments under our control today. The time when servers were kept safely behind perimeter firewalls with intrusion controls and preventive solutions is past; organizations will instead need to depend on outsourced and hosted environments to handle security.
It is interesting to mention the “defense in depth” analogy relating security to a medieval castle (we even use the same analogy in our own Wallarm video! ) with controlled access to different locations inside and a deep moat filled with alligators around the perimeter. This “hard outside” and “soft inside” model was designed to make it as difficult as possible to breach the defenses. However, if an invader were able to get inside the walls, there was full access to the resources inside.
Today, the castle defense analogy is no longer relevant: systems and users move with ease from within the security of a protected corporate perimeter to their local coffee shop or to an entirely different country as part of their normal workday — and don’t pretend you never bring your laptop with you on vacation. To secure today’s generation of environments and platforms requires a totally reworked approach that not many organizations are ready for.
Some firms promote the idea of a “cloud-first strategy” for all technology deployments and business applications. This might not be a bad idea, but it doesn’t mean that your first priority should be to cut and paste your entire architecture into the cloud or a containerized environment — especially if you will be forced to choose between a new architecture and environment that is outside of your own direct control and the traditional security controls that you have been depending upon.
Now, in recent years, technology has evolved even more, to the point that it allows for more seamless security in floating environments that need to span traditional data centers, cross border, virtualization, and cloud environments. This allows organizations to grow their capabilities without having to choose between security and the latest generation of technology stacks.
This is essential for most companies because it’s no longer a question of “if” but “when and how” they will move to the cloud.
It is easy to think that the cloud is a relatively new phenomenon, but if we go back in time a bit, we’ll find that the principle of outsourced or shared computer power is not new at all. Already in the 50s and 60s, when computers still were relatively new, researchers, universities, and militaries shared costs and resources to gain computing power.
Today’s modern technology makes it possible to add new solutions and functions and to have a flexibility that was entirely unthinkable fifty years ago, but there are still many things to consider before making the jump to a cloud environment.
The cloud (OPEX) vs. on-premise (CAPEX) discussion is something that has been going on for decades as firms navigate the advantages and disadvantages of each approach as well as the related risks and dependability of the underlying service and technology.
From the production and operations side, the choice to move into the cloud or maintain on-premise infrastructure has been and remains a very complicated question, especially if there is a demand for real-time connectivity and security.
Medium and large businesses more often look to the on-premise model because it provides better stability and more reliability. Moreover, they generally have larger IT budgets so they can hire dedicated employees, and, if needed, can consult individuals and experts who have the training and skills to handle in-house solutions.
Small companies with only one or few IT persons tasked with handling multiple unique systems are often not able to keep up with the requirements of running the business in a stable and secure way; those companies tend to be a lot more flexible and prepared to start from or move their systems to a cloud environment, which eliminates a lot of the workload and complexity.
In a production environment, using the cloud permits companies to add servers and software much faster — which can be particularly critical for companies that have to frequently update their systems and applications. Additional advantages are the elimination of machine maintenance and software upgrades. All of this also reduces the cost of operations and ownership.
For smaller businesses trying to avert the capital expenditure of their own IT staff and on-premise solutions, it seems a logical choice to take on the ability to move information to and from the cloud without the need to take on additional hardware, hires, or internal personnel, instead focusing on revenue-generating tasks.
The choice to move to a cloud solution also shines a light on another critical area: how to address and handle sensitive data and information and whether it makes sense to keep confidential and sensitive information in an on-premise environment or if it should be hosted by or transferred to a third party.
The ability to move to the cloud can also be limited by a company’s existing in-house developed applications and software that require many updates and real-time access.
However, the current marketplace is now developing toward a stage where cloud solutions are as close to instantaneous as a desktop link and where new vendors provide hybrid design solutions that allow the option to remain in full control of sensitive information and data.
Given these circumstances, the cloud versus. on-premise debate will nonetheless remain an “it depends” scenario in which the dimensions of the business and their particular requirements will determine the ideal infrastructure.
…To be continued!
I agree to Wallarm Privacy Policy.
Webinars
More insights
Subscribe for the latest news
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/foundations/every-journey-starts-with-the-first-step-84e6fe0165d4?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
There’s been a couple of interesting stories in the tech press this week…
Firstly around the latest profit announcements from both AWS and Microsoft. Amazon as a whole reported a “very strong quarter” with AWS driving a significant proportion of that:
AWS revenue for the three months to 30 September 2017 hit $4.58bn, resulting in operating income of $1.71bn — up 8% from the previous quarter.
Meanwhile, Microsoft is reporting that it has “just hit its $20 billion cloud goal, almost a year ahead of schedule”:
The software giant’s commercial cloud business is anchored by Azure, its cloud computing service, and by the Office 365 productivity suite, both of which are growing like crazy. Azure revenue jumped 90% in the company’s fiscal first quarter, compared with the same period a year earlier. And revenue from the business-targeted version of Office 365 grew 42% over the same period.
Secondly, the news that HMRC’s switch to AWS killed a small UK cloud business, DataCentred.
The business was started by TeleCity founder Dr Mike Kelly with £9m funding from venture capitalist John Moulton and local authorities. The firm turned over £1.2m in 2016, but was plunged into the red after losing the HMRC contract and subsequently went into administration in August.
Reg readers will no doubt debate the inherent weaknesses of being overly reliant on one customer, but also the irony of HMRC paying for the services of AWS, a firm sometimes criticised for its tax efficiency.
Commentary on the two stories is somewhat to be expected, generally along the lines of “the UK government is breaking its own commitment to support SMEs by taking business to hyperscale public cloud providers” and “why is the UK government supporting a public cloud provider like AWS who doesn’t pay its correct share of tax”.
I don’t particularly want to get embroiled in the debate around the second of these — even if true, it strikes me as highly likely that AWS are playing within the rules set out by Government. If the rules aren’t working for tax payers (and they may well not be) the fault lies at least as much with Government as it does with anyone else.
As a tax payer, I want the big players (all big players, not just the big cloud players) to do the right thing (i.e. pay their fair share of tax) but that requires that both they and the Government work together to make that happen. Actually… scratch that — I just want all tax payers to do the right thing, irrespective of size!
On the SME issue, I think it is worth remembering that all the major global public cloud providers (I’m thinking particularly of AWS, Amazon and Google here) actively encourage an ecosystem of smaller partners and other players to grow up around them. So whilst some of the SMEs who used to thrive in the old world might suffer, I guarantee you that there will be others that are emerging profitably in the new world. What is the balance between SME winners and SME losers? I don’t know but G-Cloud sales numbers might at least provide some indicative numbers.
This is a particularly pertinent story for Eduserv, as we transition ourselves from the old world to the new world — not an easy journey I can tell you.
We’re doing a lot of talking to our public- and third-sector customers right now — both existing and prospective. On public cloud adoption, we’re seeing some challenges in the public sector which are worth noting. We find that, often, one or other of the following ‘cloud adoption’ anti-patterns comes into play.
Colocation is good enough to take the cloud foot off the gas. Where closing a data centre is one of the key drivers for moving to the cloud, we often see public sector organisations using colocation as a staging post in that journey. To be honest, we often actively encourage this approach. The story goes that we can free up on-premise real-estate faster by lifting-and-shifting infrastructure to a colocation staging area than we can by moving direct to the cloud. And the move carries the promise of onward migration to the cloud. The anti-pattern is that as soon as the lift-and-shift has happened, the pressure to move is removed and the drive to cloud significantly reduces.
In a time of financial pressure, this lack of drive can be so significant that the cloud move never happens.
Short-term financial constraints trump long-term strategy. The other major driver for public cloud adoption, wrongly in my opinion, is cost-savings. In this case, what often happens is that an organisation that has notionally taken a strategic decision to move to the cloud gets stuck because each individual migration that they consider (particularly when taken in isolation) may not stack up financially against the historical investment that they have already made in their own data centre facilities. The anti-pattern here is that organisations never take the first step to the cloud, because the short-term financial constraints take precedence over any long-term strategic gains (many of which will not be directly financial).
In both cases, I would encourage public sector (and other) organisations to ask themselves the following two questions:
If the answer to the first is “yes”, I would say, “Good luck with that — let us know how you get on (but remember that if you change your mind in 2 or 5 years time, you’ll be 2 or 5 years further behind the cloud curve than you otherwise would have been!)”.
On the second, I would argue that to attract the brightest and best, public sector organisations have to position themselves firmly within the mainstream public cloud arena. That is where all the action is. We are no longer in 2003 — VMware really isn’t sexy any more. Get real!
I don’t know how we make that happen — particularly in local government where the financial constraints are so pressing right now. But we have to do something because otherwise we are storing up a reservoir of poor digital service delivery in the local government sector for years to come.
We need to see a change of mentality. We need some imagination. We need to stop seeing ‘cost’ as our only driver. As a citizen, I want to see the same kind of drive for better digital services that I see in GDS and central government to be also coming from my local council.
My gut feeling is that the change of outlook probably has to come from central government in some way because it’s not clear to me that local government, as a whole, can make that change on its own?
In the meantime, if you work in a public- or third-sector organisation that recognises it needs to get to the cloud somehow, pick something. Pick anything! And take your first step. You’ll learn a lot and when you get to the second thing, it’ll be easier, faster and probably cheaper.
But take that first step because until you do, your journey to public cloud hasn’t really started.
A blog by andypowe11
2 
2 claps
2 
Written by
Cloud CTO, Jisc
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/@GiantSwarm/taking-our-roadmap-public-30c3d05d12f7?source=search_post---------176,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Mar 26, 2020·4 min read
At Giant Swarm, everything we do is intertwined with our values. The decision to take our roadmap public is no exception. We began by challenging the status quo. Our roadmap has always resided in our private GitHub repository. Is this a good enough reason for it to always do so in the future?
Then the conversation started.
We see ourselves as citizens of the open-source community and big believers in transparency. It stands to reason that we would be open to publishing our roadmap.
Too much feedback and pressure to deliver
Will customers be wondering why we didn’t implement what they asked for, when they asked for it?
Well, our customers won’t be. Our customers have a dedicated person who is a Solution Engineer. They have easy access to this person (think Slack, not a ticket system). And they speak with this person on a weekly basis — at the very least. We can easily put things into perspective for our customers and explain what drove us to the decisions we’ve made.
Loss of flexibility and agility
Will people be asking why a promised feature was delayed or changed?
Probably, but we are all for having a conversation. It may seem counterintuitive, but it is easy to negotiate priority changes when it is known which deadlines are lost and why.
What if the competition gets wind of our plans?
What if they do?
Our mode of operation is not to look around for feature/function parity with our competitors. Our mode of operation is to keep very close to our customers. Learn their needs and then build them the most robust solution we can offer. We also keep tabs on upstream to help us provide the best long-term solution.
Some would say that imitation is the best form of flattery. For us, getting copied is the best form of product validation. Which is one of the reasons that our code has always been open-source. A motivated competitor could already be up to date and using it. If you are interested, you can have a look at it and use it too.
We are transparent
As I mentioned earlier, we value transparency. If we can be transparent internally and with our customers, why not with the public at large? We hope that making our product roadmap accessible to the general public can turn things like the specs and features into public discussions.
We strive for excellence
A public roadmap is a work product that is, well, publicly displayed. While roadmapping behind closed doors, there were shortcuts and shorthand that we could get away with. We can’t do that anymore. The public roadmap forces us to be diligent about maintaining the roadmap. We must express user stories in an understandable way and add context to maximize clarity. Ultimately, we now have extrinsic incentives to do this job at the highest level.
We encourage feedback
Our customers have always had the opportunity to share their needs, concerns, and priorities. These conversations tended to be siloed with a broader discussion only being held in-house. Now our customers (and you too) can make feature requests on the roadmap itself, rather than through intermediaries. Conversations about the roadmap and feature requests in the public domain could spark meaningful conversations. Conversations held within the communities we belong to and not just within the company.
We are trustworthy
While I wrote a whole post about trust a while ago. When your roadmap is accessible to the general public it showcases reliability and trustworthiness.
Taking our roadmap public was a big project. When we undertook it, we could easily articulate some of the pros as you can see above. It is now live and the main thing that we have gained is a changed perspective that has us continuously improving, which is an all-around win.
Would you like to join the conversation about our product or roadmap? Have a look and leave your comments or tweet us Giant Swarm
Written by Oshrat Nir — Product Marketing Manager @Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
See all (60)
2 
2 claps
2 
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/5-reasons-why-cloud-security-is-important-for-all-businesses-30cf6a4c6477?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 11, 2021·5 min read
With time, technology and various other technology-related factors have evolved concerning how companies function digitally. Most companies now utilize the benefits of digital data storing and sharing, which makes their work extremely convenient and efficient. In the US, almost all major companies use a distinct form of cloud computing!
This technology due being used more commonly now is also greatly trusted. Since its rapid growth in the past few years, the evolution of technology has taken over the world, with which the proliferation of cloud computing has greatly increased.
This type of computing of data tends to help organizations functioning at all scales benefit from it. It can help these organizations decrease their capital overheads and immensely assist in managing IT-related infrastructure. It is also to be understood that with the evolution of technology, most companies have shifted to online forms and are building larger and more efficient digital infrastructure.
It thus becomes important for companies to resort to the practices that other companies are engaged within mediums that are compatible with others. Staying relevant and in business is extremely important for companies, much more relevant to smaller organizations than nigger ones with more capital.
There is a range of cloud security options that different companies can choose from, based on their needs and desire to invest. These are grouped into three large distinct groups:
A private cloud is generally used for computing within a particular organization it is used by; it cannot be used for transactions and work with other organizations. The company’s infrastructure and the resources will be solely used by the company that employed it and not shared with any other companies. These kinds of cloud deployments tend to be more expensive. These have a large setup price. However, instead of the price, they also offer better customization and security to your company.
A public cloud, as opposed to the one previously mentioned, is managed by an external third party. It is usually sourced from a third-party provider and is used by more than one organization. This system is usually, that is, space on this cloud server is typically “rented” to different organizations or groups of organizations. Here, the external organization is responsible for the security and other features the system offers like — maintenance and other general upkeep.
As per the system’s name, the hybrid system is an efficient mix of both the private and public systems. Organizations that require the best kind of service in terms of rapid scalability with the best kind-encryption opt for this system. It may be a little expensive, but it is most efficient for almost all kinds of work required by the companies.
Data security is extremely dangerous for a company’s financial health and may also take a large amount of time to be detected. Data security on the cloud storage systems becomes even more significant, especially if you’re using a public cloud alternative giving a third-party access to it. While it is in your service provider’s best interest to protect your data, a breach can still happen; in most scenarios, the client has to go the extra mile and use the hybrid system and protect your data.
Cloud storage has immensely helped access data from anywhere in the world. This, however, makes it extremely significant that your data is protected and is handled properly. The employees in some scenarios may not adhere to required standards that are recommended by using public internet etc. These practices entail a security risk and make your data susceptible to malware and phishing. Cloud storage systems can prevent these as well.
It is of general understanding that any disaster can strike at any point in time and immensely affect your data and company if you’re not properly secured. It could be a flood, fire, or any other natural disaster that can adversely affect your business. It is thus important you’re safely secured and your data is protected, which could potentially cause you huge losses. Cloud security systems can help to prevent that by providing extra services.
There are certain data protection standards that the companies/businesses must comply with to keep functioning legally without attracting the regulators’ wrath. These protection standards like HIPPA and GDPR are almost universal. They are generally put together to ensure the company’s integrity and maintain the security of the companies opting for cloud security. This is because if the customer’s data is compromised, the cloud security provider will not be blamed. No blame can be passed; the regulators will hold them liable. Large financial organizations, those involving banking, health, insurance, have exacting standards because they have a lot at stake to lose. You will also lose face in case of a breach of data.
Many organizations that use cloud storage have leaked important, potentially sensitive data to the public. This leak, needless to say, has not been intentional but also cannot be prevented much. However, it has greatly affected them and their company’s integrity and general image in the market and affected their business prospects. For preventing situations of this kind, it is important to install cloud security. The cloud security systems enforce access controls on employees and anyone who officially has the privileges to access the data. They do this by limiting access to the data they can access to only those who need it. This makes it much harder for those who wish to leak the data or use it for ill-purposes. This greatly protects the data of the companies.
You must manage and take into consideration different factors that might ultimately affect your company. These are only general guidelines and pointers to understand why it is important why cloud security is significant; however, your reasons for the same may vary.
However, with the evolution of technology and cloud storage adoption, it is highly advised to go one step further and protect your data.
Check out my blog for the more interesting tutorial at Linuxbuz.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1
1 
1 
1
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/we-already-follow-the-sun-alibaba-clouds-chief-warns-competitors-8f1b3dc952bf?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 11, 2018·6 min read
Alibaba Cloud was launched in 2009 in Mainland China and accounts today to more than 47% of the national market share. The giant of the giants has, however, not wanted to miss the international boat and set sail in 2015 into expanding its footprint beyond Chinese borders. Alibaba Cloud, Alibaba Group’s cloud business arm, counts today with 18 international regions and 43 availability zones outside Mainland China.
Speaking to Data Economy’s João Marques Lima, Yeming Wang, GM for Alibaba Cloud EMEA, says the company is “very proud to say that today it already follows the sun, from a data center coverage point of view”.
With the international business growing more than 100% in the last three years alone, Alibaba Cloud promises it’s here to stay. We find out more.
Yeming Wang, GM for Alibaba Cloud EMEA
For Alibaba Cloud we see two sides. First, the Chinese market, where we developed early. Today, regarding our Chinese product and solutions, we can say it is one of the leaders and in our system we have more than 160 products.
When you compare the different providers regarding the products or features, Alibaba is one of the leading companies. For the international market, we are trying to identify several verticals in order to excel in some of the verticals.
It is too early to say we want to compete outside of China with AWS or anybody else, because we have three years [of being active in the market], others have maybe more than ten years. But we want to get excellence in some of the verticals. Especially those verticals where the Alibaba Group is very specialized in, such as retail, fintech or media.
On one side we are developing the business [China Mainland], on the other side we want to learn about the local market. Learn especially from partners and clients about what they need. There are two sides at the same time.
From the business and development point of view, we see a lot of opportunities and good initiatives for Alibaba outside of China. But first, we will be in China, because China today has become an attractive market for a lot of global players, because today the technology makes it possible even for SMEs to go into China.
Today, China with the open policies and especially with Xi Jinping’s government is driving a lot of interest for foreign companies to develop their business in China and at Alibaba, we want to tackle this opportunity, to try to serve those companies or those players in order to help them build their business in China or in Asia.
There are two initiatives from the Chinese government. One is the One Belt, One Road, but this is more about China going outside, and the other side is China wanting to increase the imports. We see a very exciting momentum to help more European companies to do business in China. Secondly, multi clouds.
Multi clouds will be a very big opportunity and initiative. At Alibaba, we are also investing a lot in migration tools, different products. And thirdly, we believe there are a lot of opportunities around digital transformation.
AI or big data are already digital transformation, it is not anymore an IT transformation, it is a Data Technology transformation, instead of IT.
In the past, we talked about digital transformation for a long time, for more than ten years, but in the past it was all down to IT, but IT alone is a call center, it is about connection, about information.
Today is more about big data, this is facilitating your business, not only cost wise, but also increases your revenue. Alibaba is investing a lot on how to match the data, how to provide the tooling or machine for them to implement the big data and AI.
At Alibaba Group we are managing more than 600 million consumers within the ecosystem. This year our revenue will be over $500bn. All these users are mission critical, because they are doing shopping or trading globally, not only in China.
Internally, Alibaba always treats the data in two parts: first is about the technology, you need to have the best technology for security. Every day, Alibaba’s applications or website has the highest amount of attacks.
In the Alibaba Group, we have a very strong security committee, and we leverage this team to develop Alibaba Cloud’s security capabilities. Alibaba Cloud capabilities don’t come from the cloud business arm only, it comes from the Alibaba Group security capabilities.
The other side is about compliance. In China, even internally, we treat this with six categories of different data within the Alibaba Group and different categories have very strict limits regarding which information can be released up to which level.
The customer data is the most critical one, and even Alibaba internally, if Alibaba Cloud wants to get the data from an ecommerce you cannot. Outside of China, with the Cloud business we do it differently.
First, we apply different standards. We already apply global or regional different standards, from Singapore, the US or Europe, and the most recent one was Germany’s C5 extension. And of course, GDPR.
When you design your product, you have to consider all the requirements into the whole process, from R&D to the production to the final roll out and so on. For every company, [GDPR] will bring a lot of effort with it for them to really fulfil the GDPR requirements.
We respect these requirements and we have a dedicated team who are working with our external supplier, also a partner, to deliver this.
No. Two years ago we launched a program called “Two Centre”. For Chinese clients, the data is managed from Hangzhou [where Alibaba’s HQ are located], but outside of China we manage it from Singapore.
This is not there because of GDPR, this is there from the very beginning. This is trying to give more confidence and transparency to the global users.
In EMEA we have three data centers, two in Frankfurt which covers most of the European region, and the other one is in Dubai to cover the Middle East region.
We are planning an additional data center in Europe, but we will only disclose where in a couple of months.
There are two types of clients. The first type of client are Chinese clients that are coming into Europe. We see more and more Chinese companies coming into Europe. In China they have already been Alibaba’s clients, and globally they want to have only one account to manage their global business. Now we are also developing local clients.
Not only local clients going to China or Asia, but also local clients in local markets, with some not even interested in going to China. We are also developing some of the clients, especially in the media, ecommerce and fintech sector.
Since we have the local data center, mostly since last year. Our first data center was opened in November 2016. After that we started to build up on local partners, clients and last year especially we tried to focus also on some of the verticals which are less sensitive. Other sectors, if you talk about government, for example, we believe it is too early.
This needs some time and more education. It is not because it is a Chinese cloud, or an American cloud, those clients, generally, are slower. They even try to think on-premises first and then cloud, maybe private cloud first and then public cloud. They need to evaluate for quite a long time.
We don’t have this kind of goal at this moment, but we will see when the time is right. When they have enough trust on the capability.
From a market size point of view, we see today West Europe, with the UK, Germany and France and the Nordic. These four are the key markets today.
Source: https://data-economy.com/we-already-follow-the-sun-alibaba-clouds-chief-warns-competitors/
Reference:
https://www.alibabacloud.com/blog/we-already-follow-the-sun%3A-alibaba-cloud%27s-chief-warns-competitors_593795?spm=a2c41.11751276.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-techreckoning-dispatch/did-vmware-do-what-it-had-to-do-c0a585ca7b79?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
Hi friends,
Check out those whales. We’ve been seeing a lot of them right off the beaches in Half Moon Bay. It’s either a wonderful gift from nature of a sign of incipient ecosystem collapse. ¯\_(ツ)_/¯
Last week was VMworld and I trekked there for the yearly infrastructure homecoming and bacchanal. I laid off the steak but I did accept some Moscow Mules from vendors of various sizes.
The first question that everybody asks is: Will Dell leave VMware alone? to which the only answer we have so far is the only one that Michael Dell and Pat Gelsinger are willing to give, which is, Yes, Dell will leave its Golden Goose alone. The implication, of course, in all acquisition promises, is the continued existence of Golden Eggs.
But the real question people have is about the Golden Eggs. Does VMware still have them? Kurt Marko, writing for Diginomica, seems to think no. IT diehards seek shelter at VMworld as future rushes past. Kurt has been around long enough not to chase the latest shiny object, but the crux of his argument seems to revolve around two different worldviews, and indeed two different surveys.
VMware’s analysis showed that adoption of public cloud services was still small, amounting to 15% of total workloads this year, doubling to 30% in five years. … 451 Research released survey data from 32,000 senior IT professionals showing that “41% of all enterprise workloads are currently running in some type of public or private cloud. By mid-2018, that number is expected to rise to 60%…”
I was trained as a scientist to look at data. You would choose very different personal and corporate strategies depending on which number captured the world as it currently is. No wonder each side thinks the other are either frivolous hipsters or stodgy old ostrich-heads.
Normally I think that a lot of confusion around cloud comes from not mentioning years — are you talking 2026 or next quarter? And I’ve got a quota to make next quarter so stop inventing crazy stories about ten years from now.
But in this case, both numbers, the 15% and the 41%, both say they are about the present day, 2016. So we have to ask, we have two samples, but of what populations?
I suspect VMware’s number includes the long tail of smaller shops and non-tech businesses, the kind of places that would never be part of a 451 survey. But again, VMware didn’t reveal how it got those keynote numbers either, so ¯\_(ツ)_/¯.
I do think that apps and workloads are the right way to look at this. If you’re making greenfield apps, you are making them with Microsoft, who is pushing you to Azure, or OSS/Linux, which is leading you to AWS. But the legacy apps? That’s VMware.
So when I do my imaginary math on the back of my imaginary envelope, and I know there are still Windows NT apps and Windows 2003 machines running businesses — and neither of those have been supported for a while, and I know how long even health organizations hold on to their apps (as long as possible), so my imaginary math comes out closer to VMware’s than 451’s. I just don’t see how 41% of all the workloads in the world are in a cloud, unless we’re just counting vSphere as a cloud, and I hope we’re not doing that. I think we’re all going to have to transform, but I don’t think we’re all butterflies yet.
So if the event horizon is indeed farther away, is VMware prepping its engines to escape the singularity and make it back to known space? Based on what we saw at the show, I’m guardedly optimistic. (Disclaimer: I’m still long VMW.)
VMware had to do three things:
So overall I think VMware can continue to be relevant if they can continue to be relevant and make friends in the clouds. Somebody has to be the grown-ups here.
Last time, we gave away a 2011 vintage vExpert bag: I am pleased to announce that our random winner is Alex Muetstege of VMguru! This week, I want to give away two prize packs: each containing one deck of 2008 VMworld playing cards with VMUG leader backs, and one folder made from vinyl VMworld signs a few years later. Just drop me a line and we’ll roll the dice.
Last time we also asked if you save boxes, because I was having a box crisis in the garage. Your responses were too disturbing to repeat. Some of you people save every product box that comes into your house, going back to the 386. You are both silly and highly flammable with your attics and basements stuffed with cardboard and styrofoam. I’m saving only the boxes for expensive audio gear and iPhones that we might sell. I am at peace with this middle way.
A few podcasts to check out: TechReckoning Podcast #5: IT that adds value to small business (sponsored by Nutanix), Geek Whisperers #116: Online and offline community at Matt’s Wedding. #118: Do what you liked doing when you were 10 with Sarah Vela And Mayfield Chat with Champions s2n5: Choose your people first with James Currier. I liked that one. Also thanks to theCUBE for having me on as a guest host! Check out our VMworld 2016 final wrap-up.
Let me know how you come down on this saving boxes thing and why on earth you do it! Also let me know what you thought of VMworld and if you’re Team 15% or Team 41%? And finally how do you like this new letter-style newsletter? More on that next time. Until then, take care.
Photo credits: Anette Larding Roman, Steve Maller via the We Love Half Moon Bay Facebook Group
Originally published at techreckoning.com.
A periodic newsletter from John Mark Troyer with links and…
1 
1 clap
1 
Written by
Techie, talker, influencemarketingcouncil.com, Chief Reckoner at techreckoning.com, Geek Whisperers podcast. Enterprise tech is the best tech.
A periodic newsletter from John Mark Troyer with links and opinions about enterprise technology.
Written by
Techie, talker, influencemarketingcouncil.com, Chief Reckoner at techreckoning.com, Geek Whisperers podcast. Enterprise tech is the best tech.
A periodic newsletter from John Mark Troyer with links and opinions about enterprise technology.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-alibaba-cloud-plans-to-disrupt-the-cloud-market-in-emea-95547f2e12d2?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 10, 2018·5 min read
Alibaba Cloud is continuing a push out of China into the EMEA market that started around 18 months ago. Alibaba Cloud EMEA General Manager, Yeming Wang, sat down with Computerworld UK to discuss the firm’s strategy for disrupting a highly competitive cloud computing market.
The cloud computing division of the $500 billion Chinese technology giant is not that different to its western rivals — Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP) — in that it offers a broad range of cloud computing services, from basic infrastructure-as-a-service (IaaS) capabilities like elastic computing, storage and database solutions, to application services and advanced analytics capabilities like machine learning (more on this later).
Alibaba Cloud was launched in 2009 and supports some of the biggest internet companies on the planet — although you probably have never used them directly. Both the ecommerce giant Alibaba and fintech app Ant Financial boast more than 500 million active users.
Alibaba Cloud has seven availability zones in China alone, seven more across Asia Pacific and Hong Kong, two in the US, one in Dubai, and one in Frankfurt for Europe. It also now has local teams in four EMEA locations: the UK, Germany, France and Dubai.
Alibaba Cloud is well poised to disrupt the big three public cloud providers for a few key reasons, EMEA general manager Yeming Wang told Computerworld UK at Alibaba Cloud’s London office opposite the Savoy hotel.
Firstly, the company has access to the enormous Chinese market by providing cloud infrastructure there. But it has also acted as a trusted cloud partner for Chinese companies expanding into Europe through its Frankfurt availability zone.
Complementing all of this is the growing demand for multi-cloud strategies at the enterprise level.
We look at Europe as very strategic and the market is remarkable. We have to be here and at the same time we want to have more innovation here with clients, partners and even research entities and develop this ecosystem.
Wang also pointed to Alibaba’s performance capabilities, and in particular the 360,000 transactions per second Alibaba itself processes during ‘Singles Day’, the equivalent to Black Friday in China.
This is like a stress test and this shows the very strong cloud capability of our elastic [compute], big data and security. Nowhere else can you find such a workload on the cloud. We aren’t going to say we are better, but Alibaba is well proven in extreme stress tests.
Naturally the prime advantage Wang and Alibaba see when it comes to bringing its cloud services to customers outside of China is the ability to offer seamless access to a Chinese availability zone.
When we first came to Europe the main drive was to help those Chinese clients come overseas. So we see this is a big advantage for us when we talk about China or Asia. Alibaba Cloud today is the only one that is a real global cloud because even for AWS in Asia, in China, they are like an island, it’s quite independent from the rest of the world.
AWS runs two availability zones in the China region, one in Beijing run by Chinese company Sinnet, and one in Ningxia, which is operated by Ningxia Western Cloud Data Technology (NWCD). Chinese law does not allow foreign companies to run cloud computing infrastructure there.
Similarly Microsoft runs its Chinese Azure regions through 21Vianet. Google currently has no plans to run a China region.
Wang is bullish on the idea of modern clients that want to be multi-cloud or even cloud agnostic, providing Alibaba with the opportunity to hoover up a piece of what Gartner forecasts will be a $53 billion IaaS market by next year.
We are talking to our local clients here a lot about a multi-cloud approach. We see they have a clear demand to say we want multi cloud.
He also asserted that adoption of Alibaba should be simple for anyone familiar with popular cloud computing modes of working.
If people are familiar with AWS, they can use Alibaba Cloud. A lot of the user behavior, and even the APIs, are very similar. This delivers a lot of simplicity for developers looking to use Alibaba Cloud.
Wang pointed to some verticals where Alibaba has a unique ability to deliver value, namely retail, media and fintech. Wang clearly separates customers between those seeking general cloud computing, so IaaS, to vertical solutions.
From my point of view I see two different markets. One is for the basic infrastructure. On the other hand I think the real competition will be where you can have a business event instead of this commodity.
For example, Ant Financial is Alibaba’s extremely popular fintech app, offering Chinese customers loans, banking, payments, insurance and credit services. So “a full range of fintech technologies, this is also on cloud, so we have this knowhow,” he said.
One challenge for Alibaba Cloud is to bring the European market some more tangible case studies than simply bragging about the scale of Alibaba services.
Wang admits as much, saying:
We are also very focused on flagship clients to bring the market something remarkable and we hope that after half a year, or a year, we will have some good news for you.
When it comes to AI and machine learning, which are increasingly key differentiators for vendors in this market, Alibaba takes a slightly different approach to the big three. Under its AI brand ET Brain, Alibaba Cloud offers more vertical-specific applications of AI, including solutions for industrial IoT insights (ET Industrial Brain), smart cities and medicine.
This is a model that more closely resembles IBM Watson than the broader capabilities the likes of Google, Microsoft and AWS offer. That is except for the fact that Watson “seems to be not so successful from an IBM point of view”, Wang said.
The good thing for us is from the very beginning with ET Industry Brain we worked with a solar panel program and later on we applied that to another company, so making a standard product.
Alibaba Cloud also has its own version of the popular data science competition site and community Kaggle, which Google acquired last year, called Tianchi. The cloud vendors all clearly see value in owning these data science ecosystems and communities and pushing them to run on their cloud platforms, ideally.
Source: https://www.computerworlduk.com/cloud-computing/how-alibaba-cloud-plans-disrupt-aws-microsoft-google-in-emea-3678283/
Reference:
https://www.alibabacloud.com/blog/how-alibaba-cloud-plans-to-disrupt-the-cloud-market-in-emea_593794?spm=a2c41.11747542.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@hullsean/why-is-there-a-company-whose-only-purpose-is-to-help-me-with-my-aws-bill-615e997869d8?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sean Hull
Apr 11, 2020·2 min read
Join 35,000 others and follow Sean Hull on twitter @hullsean.
Of late, I see more and more discussions on CTO forums, and in the news on surprise AWS bills. Did you hear what happened NASA’s cloud deployment fail?
So I thought I’d provide some history, and a few ideas…
Believe it or not this has happened before. In the days when I did a lot of Oracle work, I remember it was terrible how tough their licensing was. And companies were pretty much hamstrung because they needed the technology to run their business.
Given the demand, new firms like Miro Consulting stepped in to help. Their business is specifically built to help customers with out-of-control Oracle bills.
Read: How to hire a developer that doesn’t suck
What’s this got to do with Amazon Web Services you say?
Just today I discovered a firm that promises to help you lower your aws bill by 20–40%.
Promises aside, if there is a business doing this, there must be money to be made in it. So that speaks to a sizeable market opportunity.
Related: When you have to take the fall
Hey if you’re thinking you can manage it yourself, why not look to AWS for assistance on lowering your bill.
Something tells me there’s a conflict of interest there, but I digress.
Point is with this topic becoming of growing interest, I think you’ll see more businesses stepping into this niche.
Related: How do I migrate my skills to the cloud?
Originally published at https://www.iheavy.com.
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
iheavy.com/signup ✦ Devops ✦ Docker ✦ AWS/GCP ✦ ECS/Kubernetes ✦ Terraform ✦ High Availability ✦ Scalability ✦ Boutique consulting
"
https://medium.com/cloud-simplified/5-keys-to-being-compliant-in-the-cloud-42ce703326fa?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
By Harold Bell
If you’ve been employed in a formal engagement of some kind, it’s likely that you’re familiar with the idea of regulatory compliance. Remember when you were signing what seemed to be a thousand documents the day you accepted the job? Well within a few of those pages lie specific instructions on how to safeguard sensitive information and protect the privacy of those individuals in the process. This includes guidance on the proper way to store this information, access and share it, as well as the procedure to report a data breach should one ever occur.
For example, the Health Insurance Portability and Accountability Act, or HIPAA, is regulatory legislation for the healthcare industry that was signed in the 1990’s to protect the privacy of patients. The law is holistic in nature, meaning it enacts protection for patient information regardless if it is stored via hard copy or digitally, and whether or not it is shared. To drive accountability in protecting patient’s medical records and other confidential data, healthcare providers and their partners will be fined heavily for non-compliance. In certain circumstances, criminal charges have also been filed against negligent parties.
Compliance rules can be different if you work in government (FedRAMP), manufacturing (GMP), or real estate (CFPB), and can also be impacted by whether or not you collect payments from cardholders (PCI-DSS). With that said, you can also find yourself on the hook for many compliance regulations if your organization wears multiple hats. And as if the compliance landscape wasn’t already a minefield, technology innovations have added yet one more layer for professionals to account for: the cloud. This means you now have to mitigate risks on devices you own internally, while also addressing the risks of data stored in third party environments. Impossible? No. But the task ahead won’t be easy. You’ll need a lot of discipline and the help of some strategic partners. With that said, we’ve outlined 5 tips to ensuring data compliance in the cloud — just click here to continue reading on the Nutanix Blog.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sriramhere/whats-hp-s-cloud-strategy-part-2-a-quick-swot-analysis-39268bbd1dea?source=search_post---------183,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Sriram Subramanian
Jul 2, 2014·6 min read
Disclaimer: I was one of the invited bloggers attending HP Discover 2014 with T&E taken care of by HP. HP DIDN’T pay for this blog post.Please note that all these are based on public data and no speculations or use of any confidential data here.
In my previous post, we saw a quick intro to HP Helion portfolio. Before I go deeper in to the major constituents (HP Helion OpenStack, HP Helion Development Platform), I would like to provide my take on strengths and weaknesses of HP Helion/ HP’s cloud strategy.
[caption id=”attachment_459"" align=”alignleft” width=”297""]
Credit: http://go.hpcloud.com/forrester_wave_private_cloud_2013[/caption]
According to a Forrester study in Q4 2013, HP leads in Private Cloud Solutions vendors, beating VmWare and Microsoft in both Offerings and Strategy. This is attributed to the massing offering — HP CloudSystem Enterprise Suite, which is a bundle of products like HP CSA, HP Orchestration Automation etc. It is to be noted that this was also one of the first OpenStack based private cloud offerings in the market. Combined with the converged infrastructure, this has a lot of traction and adoption, giving HP a huge success among Private Cloud Solution vendors.
HP has a vast portfolio of proprietary software (some of them listed above). HP has also had success in combining with open source OpenStack (previously know as HP Cloud OS). Continuing this trend, HP Helion has the right mix of proprietary software (management and automation layer, other enabling technologies) along with open source (OpenStack and Cloud Foundry). This is the right path to open and (eventually) inter operable clouds.
HP Helion also attracts both operators and developers together (through OpenStack and Cloud Foundry layered appropriately).
HP Helion offers the complete stack — starting from physical infrastructure, IaaS, all the way to Professional services: hardware, software and services. Consider multiple data centers (with some of them FedRAMP certifications) — you get the complete deal. HP’s customers are used to such one-throat-to-choke offerings from HP and HP Helion extends that to cloud workloads.
HP has a strong enterprise customer base, across multiple segments — retail, financial services, automation and more. HP’s Enterprise Group and Enterprise Services generated revenue as much or more than their Printers & Personal Systems group in the past three years. This strong enterprise customer history combined with successful private cloud solutions adoption puts HP Helion at an advantageous position for enterprise adoption.
HP has been a strong OpenStack contributor, consistently placed among top 3 for overall code contributions to OpenStack (towards various projects and across multiple releases) over the past two years. HP currently also has of 6 OpenStack PTLs, emphasizing their strong OpenStack code contributions. This helps them gain the mind share among OpenStack community and OpenStack customers.
HP is the only OpenStack based vendor to offer indemnification to its qualified customers using HP Helion OpenStack code from third-party patent, copyright and trade-secret infringement claims directed to OpenStack code alone or in combination with Linux.
This is going to be a huge attraction of customers who are in a dilemma adopting open source based offerings.
HP Helion portfolio consists of a mix of open-source and proprietary technologies, but the messaging around it emphasizes OpenStack and Cloud Foundry strongly. While this is the right messaging strategically the value proposition needs to be clearer. Otherwise it’s customers are either not going to see the value in the constituent open-source technologies (see it as fully proprietary), or going to see only these open-source components (see it as fully open-source) and be skeptical.
This was not the case with HP Cloud earlier, because, it only had OpenStack which was wrapped deep inside. It was also not offered as a standalone product (as in the case of HP Helion OpenStack).
HP has its share of missed/ not successful initiatives — Autonomy acquisition, earlier version of HP Public Cloud, etc. While this is bound to happen to any big company, customers might have this concern with HP Helion too. However, with a strong leadership, financial backing and direct support from the CEO, I am positive that earlier misses won’t be repeated with HP Helion.
I found it difficult to wrap my head around the massive set of product offerings outside of HP Helion portfolio. This could be confusing to an existing customer who is used to certain offerings (say HP Financial Services); how are they going to be transitioned to cloud/ HP Helion?
This weakness can easily be turned in to a huge opportunity, given HP’s success with enterprise customers and huge services force. It also helps that the customers already have an existing relationship with HP.
Fellow bloggers kept asking this, along with ‘why a new OpenStack Distro?’. With many a OpenStack distros, and HP’s long involvement in OpenStack, this is a natural question. HP Helion needs to get the messaging stronger on value proposition to answer “why” and “why now?”.
I don’t see being a new OpenStack distro as a weakness; but the need for more experience selling open-source focused solution is. This is also where a talent acquisition could help — engineers, sales, marking, services or support talent experienced in OpenStack based offerings.
My suggestion would be a smaller team (less than 30), so that is easier to integrate without losing the momentum.
Enterprise space is a great advantage for HP. Enterprise customers have a strong relationship with HP, have been using its various offerings for quite a while. They would be much more comfortable taking HP’s help in moving to cloud — HP Helion. I can’t stress enough the advantage of having the entire stack offered by a single vendor here.
HP has demonstrated success in Private Cloud Solution offerings. Combined with its public cloud offering, multiple data centers, HP has a strong path to success in Hybrid cloud. It also had the unique advantage of having OpenStack based both public cloud and private cloud solutions (until eNovance acquisition by Red Hat). HP is well positioned among OpenStack vendors in this hybrid cloud opportunity.
HP has demonstrated strong OpenStack code contributions, stronger commitment to OpenStack and is one of the Platinum members of OpenStack Foundation. HP also has public cloud offering and private cloud solution based on OpenStack. They also have dominant mind share around enterprise customers.
All these put them at strategic advantage to be a dominant OpenStack vendor.
As much as HP is upping their game, competition is too. Consider eNovance acquisition by Red Hat. Consider IBM releasing their OpenStack distro. Consider incumbent OpenStack experts firing in all cylinders (Mirantis expanding to Europe, Metacloud expanding to hosted private cloud solution etc and more) — even though these are much smaller compared to the size of HP, they have a stronger mind share among OpenStack community.
There is also a whole bunch of action happening outside of OpenStack: around Docker and similar eco-systems. While they are at much incipient stages compared to OpenStack, it is something not to ignore.
HP Helion needs to continuously keep upping their contributions to OpenStack and continue courting existing customers to HP Helion.
This is not unique to HP. Any big company is bound to have some slow moving parts and some really fast groups. HP Helion is moving at a very fast pace, which some other groups/ traditional cash-cows may not like. One can only do such much about this; as long as HP Helion is executing on its plans, this threat is irrelevant.
This is also needs to be considered while considering talent acquisitions. Smaller teams with less physical baggage are more suitable than big companies. It is very important to continue the momentum.
HP Helion has right things in place: strong leadership team, a strong engineering team, substantial financial backing, direct support from the CEO, a and existing HP customer base. From what I have seen, HP Helion has the right strategy and vision. They are also executing on their plans so far.
HP Helion needs to continue their execution and HP Helion will be a huge success — I am confident!
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
See all (704)
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/configuring-cdns-for-multiple-regions-on-alibaba-cloud-31c74416cfe?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 23, 2018·4 min read
Alibaba CDN is a service that optimizes delivery of files, images, and assets. It’s used to improve the performance of websites hosted by server applications such as NGINX or Apache.
Alibaba CDN is beneficial in many ways, but it can be particularly useful for boosting performance in situations where your applications are distributed across a wide geographic area. By properly configuring the CDN for multiple regions, you can ensure that data is delivered to users faster and more reliably, no matter how close they are in physical terms to your main datacenter.
This is an important benefit, because although in theory the Internet connects all parts of the world equally, in practice, data transfer speeds are impacted by the physical difference separating servers from endpoints. Given that even a few seconds’ delay is often not acceptable to users, it is very important to optimize the delivery speed of data and the availability of your application no matter how much physical distance separates users from your data center.
Alibaba CDN is one of the services provided by Alibaba Cloud. It has several distribution nodes around the world so that simultaneous updates to different services are facilitated, and consequently reduce downtime.
Alibaba CDN is a network of clusters of edge servers distributed in different regions, and distribution is concentrated on these servers. In this way, the applications work in an accelerated way by being close to the users, with faithful copies of the applications on their servers. Like many of Alibaba Cloud’s services, CDN has a pay-per-use pricing model.
Below is a diagram that illustrates how data replication is performed between CDN nodes. As we can see, the user makes the request to the initial access point, and this request will analyze where the user is located so that the nearest server responds to it. That way, there is no delay in application access for the user.
Source: http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/27105/intl_en/1464580456881/architecture-en-001.png
When we apply the configuration of the CDN in a multi-national way, it is easy to see how we should proceed. You need to create the CDN and define in which countries your application should be available. You can even use monitoring to see where you need to scale up due to localized demand. Updates take place automatically, and transparently to the user.
To create a CDN, we need to access the Alibaba Cloud portal. In the main dashboard, we have to choose the option Alibaba CND, in the field Storage & CDN, as indicated in the image below.
When accessing it, you must select the CDN Domain Name List on the left side and create a domain name by clicking Add Domain on the right side. Within these options, choose according to your needs. An example of this screen is presented below.
Within Business Type, we can choose the following options:
Within Origin Site Type, we have the following options:
And to finish, we need to choose the Accelerating Region:
At this point, you must select Global Acceleration or Overseas Acceleration (non-Chinese mainland) so that your application can be replicated in different places in China. After going through the steps of creating the CDN Domain Name, there is an approval period. After approval, the management console can be accessed.
After CDN creation, we can add our files to the CDN, then get the URL of each file and use them to build websites and applications. The files served by Alibaba CDN will be provided efficiently and quickly to users, in whichever regions the CDN has been configured to serve.
Not all public cloud platforms provide complete built-in CDN services. Before considering which cloud to use, evaluate the importance of having an integrated CDN solution, which, as noted above, can be particularly useful in situations where you need to deploy an application across a wide geographic area.
Brena Monteiro is a Fixate IO Contributor and a software engineer with experience in the analysis and development of systems. She is a free software enthusiast and an apprentice of new technologies.
Reference:
https://www.alibabacloud.com/blog/Configuring-CDNs-for-Multiple-Regions-on-Alibaba-Cloud_p261430?spm=a2c41.11177127.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sriramhere/openstack-marketplace-review-aaac3f06bcc2?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sriram Subramanian
Jun 4, 2014·3 min read
One of the interesting announcements during the Atlanta Summit was the launch of OpenStack Marketplace. OpenStack Marketplace provides necessary details that help you to make an informed decision in choosing the tools/ resources for your OpenStack cloud. I am a firm believer of open, transparent markets and am happy to see such an effort for the OpenStack eco-system. Here are my observations playing with it and some recommendations to improve.
Here are the top 3 things I like about the OpenStack Marketplace. I am particularly interested in the Rating/ Review feature.
Not only the participants are classified appropriately (like Training Service Providers, Consultants etc), they are geo-tagged. If you want a OpenStack Public Cloud in Europe, zoom in to Europe or choose from the dropdown list.
The map also gives me a quick view in to where OpenStack clouds/ services are offered globally.
When I asked for the Ratings/ Reviews feature during my chat with the OpenStack Foundation during the summit, pat came the reply ‘It’s already there!’. I have always felt the need for customer-driven ratings/ reviews for OpenStack services, something like Yelp. Very happy to see that in place!
They of course need to be populated; if you could do your part by rating/ reviewing appropriately, it is much appreciated.
There is good amount of details for each listed participant — data center locations for public cloud, APIs supported, version supported, duration and levels for courses, billing plans etc., appropriate to the type of services they are offering. They also appear to be normalized to follow same patterns across different vendors in a category. Such normalized details help consumer to compare apples to apples.
Here are some features/ improvements I would like to see in the Marketplace.
As I had mentioned in my summit report, every body was hiring. With the dearth of developer/ operator resources, it might help to allow listing of jobs and profiles here. Particularly Profiles of developers seeking for new opportunities would help the situation (OpenStack Job Board already lists jobs, but not as part of OpenStack Marketplace)
This has been asked around a lot. I am still not convinced if the Foundation should be doing such certifications, but it is only a logical next step to provide some kind of vetting around the services. I would like to see some kind of vetting here (and having ideas how to do this while I am composing this :))
Developer in me speaks! It would be cool to send in location/ type parameters as query parameters and get the results. Currently, all search is UI based. I need to choose the location/ type by either clicking in the map, or choosing from the dropdown. Lazy me likes something like
http://www.openstack.org/marketplace/public-clouds/service=object&location=us
Next step would be a cool Mobile App :)
I am almost tempted to include a federation of cloud service providers, but we are far from it. So not here, not now.
I am thrilled to see this initiative and view this as a great case of Foundation playing the role of enabler in the OpenStack eco-system, Kudos!
Do your part by either getting your service listed or make informed decision with the help of the OpenStack Marketplace and finally by providing rating/ reviews for the providers who have helped you!
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
See all (704)
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/challenges-of-choosing-your-first-cloud-d48683e58dce?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 10, 2018·4 min read
By Sabih Saeed, Solutions Architect at Ricoh
In the world of cloud computing where every organization wants to migrate to public cloud, there are challenges along the way. There is no match for flexibility and scalability we get in the public cloud, but we need a well-developed plan to best utilize the endless features available to us in the public cloud. I will discuss the most fundamental challenges and describe how Alibaba Cloud can help you solve these challenges.
When choosing the right cloud product or solution, key decisions that you will have to make includes the following:
While there is no definite winner among all the public cloud providers out there, each of them has their unique strengths and weaknesses. Choosing the right provider is the most critical decision as once you start deploying or migrating, you will realize that it’s not simple to move out of the public cloud service provider because in the cloud, you do lose the control to some extent. For example, you can’t access the host under normal offering, if you want a dedicated host then it’s going to cost you more.
Here are a few guidelines to help you make an informed decision.
Once you have picked your service provider, the first thing will be to look at the pricing estimates and see how much it’s going to cost to run services from the public cloud of your choice.
Here are a few tips for you to better manage your budget to minimize costs.
I hope this guide will come in handy when your enterprise is deciding whether or not to migrate to the cloud.
Reference:https://www.alibabacloud.com/blog/challenges-of-choosing-your-first-cloud_594246?spm=a2c41.12386707.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/security-thinking-cap/cloudsizing-finding-the-right-fit-for-your-cloud-5c452a951521?source=search_post---------187,"There are currently no responses for this story.
Be the first to respond.
The maturation of the cloud is fascinating as it continues to adapt, providing more opportunities for companies and consumers to leverage the vast computing and storage power of computers around the world. Whether those resources are housed in a corporate data center or dedicated hosting facility as part of private cloud services or through third-party public cloud offerings, the cloud is most likely part of your everyday life and it is one of the biggest technology growth areas, offering companies ways to save money and become more adaptable to change.
There are many options for cloud consumers, those utilizing or wishing to utilize cloud services. A large differentiator in cloud types lies in ownership and operation of the cloud infrastructure and three main types of clouds, private, public and hybrid are used to support differing business needs.
Private clouds allow business units to utilize cloud services without needing direct capital investment. The organization makes the investment in the underlying technology resources and support personnel to maintain the equipment and offers cloud resources to business units as a service.
Private cloud resources are not shared with other companies, resulting in predictable performance and optimized workloads. Neither are they restricted by the requirements of other clients. This allows for private cloud services to be customized so that they are tailored to the organization’s needs.
There are disadvantages to utilizing a private cloud. The main disadvantage is the large capital investment required on the part of the organization to implement and expand a private cloud. This makes it less flexible than public cloud offerings and more difficult for organizations to test the waters by deploying pilot or prototype systems or to offer services. Rather, prototypes and pilots must make a business case that results in realistic expectations of long-term revenue to cover capital expenses. However, an organization can set up a private cloud using outside hosted resources. The difference here between a private cloud that is hosted and a public cloud is that the private cloud resources are dedicated to you, not shared among multiple companies.
Public clouds, on the other hand, are what most end users think of when the word “cloud” is mentioned for these clouds are owned and operated by an outside entity and services are provided on a subscription basis, or sometimes for free. Cloud consumers can purchase only the services they need and they can easily increase or decrease their cloud resources by simply purchasing more or less. Public cloud services can also be made available very quickly to consumers because the infrastructure is already there. This is important for companies that need to rapidly respond to demand. In some cases, public cloud services can be provisioned hours or minutes later compared to days or weeks of procurement time in private clouds
Many public cloud services are designed for a specific use case that may or may not fit your own organizational use case. Public cloud providers do this in order to better manage their solution and reduce the complexity of upgrades and maintenance. Public cloud services can be customized but this tends to increase the cost of the service and reduce service portability or the ability of the cloud consumer to migrate from one cloud provider to another.
Since public clouds are operated by a third party, consumers of the cloud do not have the same level of visibility into the underlying technology, processes, and procedures that go into providing those services. This makes it more difficult to ensure that services in the cloud meet organizational compliance requirements. This is especially crucial when a data breach occurs and the organization must investigate and notify its customers. Public cloud contracts may not specify notification and compliance requirements leading to issues such as lack of timely notification of a data breach, inability to identify breach scope or other required data, and fines and sanctions against the cloud consumer.
Both of these cloud models are powerful methods for providing organizational technology services but not all companies neatly fit into one of these two categories. This has led to the rise of the hybrid cloud. The hybrid cloud extends the private cloud to the public cloud. This adds the flexibility private clouds lack but still allows the organization to manage the data, processes, and controls in the way they do with a purely private cloud.
In a hybrid cloud, customizations can be integrated on the private segment while standardized, out-of-the-box, portions of a solution are located on the public segment. This allows the organization to tailor the solution to their needs without limiting their ability to move the standardized elements to another cloud vendor or to spread the workload and service availability risk among multiple cloud vendors.
One significant benefit of the hybrid cloud is the ability to utilize existing infrastructure and to migrate portions of a service to public segments over time. This reduces the disruption a large change would have on system availability and utilization which can increase productivity. The front-end of a system can stay the same for users while back-end components are moved around the hybrid cloud.
The piece that makes this all work is a hybrid cloud service and associated management tools such as Dell Cloud Manager. These tools centralize the administration of the hybrid cloud and interface with the public and private segments to enforce defined rule sets and establish communication and functionality between the components.
The hybrid cloud offers many of the advantages of both public and private clouds. This is not to say that the hybrid cloud is the best solution for all cloud scenarios as many services may still find that a private or public solution meets their needs. The biggest news and key element of the hybrid cloud is its fit for the myriad solutions that have yet to make their way to the cloud due to one objection or another or for those that had to settle for one type that did not truly meet their needs. With hybrid in the mix, cloud services can be more ubiquitously deployed and utilized, resulting in increased agility, closer alignment to operational objectives, and a better match of technology expenses to revenues.
Cybersecurity and Privacy thoughts by Eric Vanderburg
Cybersecurity and Privacy thoughts by Eric Vanderburg
Written by
Security and Technology Leader, Author, Speaker, Private Investigator and Expert Witness. Vice President of Cybersecurity at TCDI. www.tcdi.com
Cybersecurity and Privacy thoughts by Eric Vanderburg
"
https://medium.com/security-thinking-cap/no-compromise-with-the-hybrid-cloud-6e28b110cef?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
This statement may be familiar to many who have considered cloud services and it was both the start and end to many cloud discussions.
What is most important to you, cloud security and service customization or flexibility and cost?
Those who picked security and service customization adopted a private cloud model and those who picked flexibility and cost chose a public cloud model. Those that couldn’t choose continued using traditional IT to solve today’s problems and they had a tough time of it.
The good news is that you don’t have to make that choice anymore. Security, service customization, flexibility, and cost objectives can each be met through a merger of public and private cloud approaches in the hybrid cloud. To understand how this works, let’s briefly explore both prior models and the compare them to the hybrid cloud.
Organizations have more control over data and services when using a private cloud. This control allows for cloud services to be tailored to the company’s security strategy to better protect the data including security controls, and procedures necessary to meet compliance requirements. Along with greater control is increased visibility into the system for easier management and incident response. For example, computer forensic or investigative work can be streamlined as no third party limits access to the data or logs and the organization can collect evidence directly, resulting in a clearer chain of custody. Public clouds offer less visibility and control, making it harder to enforce security requirements, perform investigations, collaborate on incident response and notify customers quickly about data breaches. They have received the most criticism for their ability to securely protect data, especially in regulated businesses that must meet compliance requirements.
Private clouds may be shared among business units but they are not shared between unknown entities as is common in public cloud offerings. This reduces the chance that a successful exploit of a neighboring cloud system will impact organizational systems. However, public clouds are by nature targets because they are visible, well-known repositories of data. Attackers may not know what data resides in a public cloud or whether it is worth their effort to attack but public clouds hold so much data that they make a tempting target for attackers. By placing data in a public cloud, consumers are no longer a target of opportunity, they are a target of intent.
Public clouds offer the best flexibility since they can be expanded or adopted almost at will. Cloud consumers purchase just the services they desire. When they want more storage or additional processing power, they simply increase their cloud plan. Similarly, when they no longer need resources, they can release them back to the cloud.
Private clouds differ greatly in their flexibility. Organizations often purchase the servers, storage, and networking equipment along with the necessary software to set up a private cloud and they must pay IT personnel to maintain it. They also need to make purchases as the environment grows. Unfortunately, if demand for the private cloud shrinks, the investment is already made and the organization must find a different use for the equipment or suffer a poor return on investment when the equipment stands idle or when IT staff are not fully utilized. Hosted options are available for private clouds, but the organization must still have staff who are capable of managing the private cloud.
Cost models differ greatly between cloud offerings. Public cloud pricing is based on service level and utilization. This tends to work well for companies that want to keep service costs aligned to usage. Private clouds often require direct capital expenditure, as mentioned above, or at least additional staff to manage, create and expand them.
The hybrid cloud combines elements of the private and public cloud models. Private cloud elements provide the portal to services but public cloud elements can be used to extend the private cloud as needed. This makes the hybrid cloud flexible. Standardized elements that do not need the enhanced security of the private segment can be moved to the public segment, allowing for growth without as significant investment in capital equipment.
Data flows between public and private segments of the hybrid cloud can be fine-tuned to adhere to organizational security, privacy and compliance rules. For example, sensitive or confidential data, such as trade secrets, financials, and customer information could reside on the private element of the cloud while more operational data and public data are pushed to the public segment as needed. Alternatively, data could be allowed to be pushed to the public segment of the hybrid cloud but would only be able to reside there for a limited time and the data would be encrypted automatically.
I’m happy to say that you don’t have to choose between security and service customization or flexibility and cost. You can get it all in the hybrid cloud. For those who have rejected public or private cloud models, I encourage you to seriously consider the hybrid cloud. Tomorrow’s challenges will come in all shapes and sizes, many of which existing IT cannot handle. Move to a platform engineered for the future and reshape your business with the hybrid cloud.
Cybersecurity and Privacy thoughts by Eric Vanderburg
Cybersecurity and Privacy thoughts by Eric Vanderburg
Written by
Security and Technology Leader, Author, Speaker, Private Investigator and Expert Witness. Vice President of Cybersecurity at TCDI. www.tcdi.com
Cybersecurity and Privacy thoughts by Eric Vanderburg
"
https://medium.com/clouddon/cloud-don-llc-announces-entry-into-the-rackspace-partner-network-e71f57dfe22b?source=search_post---------189,"There are currently no responses for this story.
Be the first to respond.
We are excited to announce that Cloud Don has become one of the Referral Partners for Rackspace. Since many of our clients are in the APAC region, we are currently partnered with Rackspace HK. Here is the press release.
CloudDon - catalyzing modern enterprise IT transformations
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
"
https://medium.com/clouddon/openstack-ecosystem-mapped-c23231ca4d1d?source=search_post---------190,"There are currently no responses for this story.
Be the first to respond.
[Given how dynamic the OpenStack ecosystem is, we knew that we need to update the mapping often. With the acquisition news around Blue Box and Piston Cloud today, we had to do it sooner than expected. Congratulations!]
Download the whitepaper here: Mapping OpenStack EcoSystem
[caption id=”attachment_719"" align=”alignleft” width=”1101""]
OpenStack Eco System Mapped[/caption]
OpenStack ecosystem is a vibrant, diverse, ever evolving one with more than 400 participating companies and more than 18000 individual members. Participants vary from small startups to huge, public companies. Solutions vary from niche offerings to a wide portfolio of offerings. With such a variety and diversity, there is also a possibility of getting overwhelmed.
We have mapped the OpenStack ecosystem and are publishing the results as a free for download position paper. If you are curious about the OpenStack ecosystem or would like to learn more on various options and choices available while adopting OpenStack, this will be of great help.
You may also want to watch the presentation at the OpenStack Summit titled ‘Navigating the OpenStack Eco System’ that was co-presented with Seth Fox of Solinea or download the slide deck here. If you want to check the mind map, it is shared here.
If we had missed any or if you would like us to consider someone, please leave us a comment or reach us @sriramhere
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/clouddon/whats-hp-s-cloud-strategy-part-2-a-quick-swot-analysis-cc5a8b54c389?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
Disclaimer: I was one of the invited bloggers attending HP Discover 2014 with T&E taken care of by HP. HP DIDN’T pay for this blog post.Please note that all these are based on public data and no speculations or use of any confidential data here.
In my previous post, we saw a quick intro to HP Helion portfolio. Before I go deeper in to the major constituents (HP Helion OpenStack, HP Helion Development Platform), I would like to provide my take on strengths and weaknesses of HP Helion/ HP’s cloud strategy.
[caption id=”attachment_459"" align=”alignleft” width=”297""]
Credit: http://go.hpcloud.com/forrester_wave_private_cloud_2013[/caption]
According to a Forrester study in Q4 2013, HP leads in Private Cloud Solutions vendors, beating VmWare and Microsoft in both Offerings and Strategy. This is attributed to the massing offering — HP CloudSystem Enterprise Suite, which is a bundle of products like HP CSA, HP Orchestration Automation etc. It is to be noted that this was also one of the first OpenStack based private cloud offerings in the market. Combined with the converged infrastructure, this has a lot of traction and adoption, giving HP a huge success among Private Cloud Solution vendors.
HP has a vast portfolio of proprietary software (some of them listed above). HP has also had success in combining with open source OpenStack (previously know as HP Cloud OS). Continuing this trend, HP Helion has the right mix of proprietary software (management and automation layer, other enabling technologies) along with open source (OpenStack and Cloud Foundry). This is the right path to open and (eventually) inter operable clouds.
HP Helion also attracts both operators and developers together (through OpenStack and Cloud Foundry layered appropriately).
HP Helion offers the complete stack — starting from physical infrastructure, IaaS, all the way to Professional services: hardware, software and services. Consider multiple data centers (with some of them FedRAMP certifications) — you get the complete deal. HP’s customers are used to such one-throat-to-choke offerings from HP and HP Helion extends that to cloud workloads.
HP has a strong enterprise customer base, across multiple segments — retail, financial services, automation and more. HP’s Enterprise Group and Enterprise Services generated revenue as much or more than their Printers & Personal Systems group in the past three years. This strong enterprise customer history combined with successful private cloud solutions adoption puts HP Helion at an advantageous position for enterprise adoption.
HP has been a strong OpenStack contributor, consistently placed among top 3 for overall code contributions to OpenStack (towards various projects and across multiple releases) over the past two years. HP currently also has of 6 OpenStack PTLs, emphasizing their strong OpenStack code contributions. This helps them gain the mind share among OpenStack community and OpenStack customers.
HP is the only OpenStack based vendor to offer indemnification to its qualified customers using HP Helion OpenStack code from third-party patent, copyright and trade-secret infringement claims directed to OpenStack code alone or in combination with Linux.
This is going to be a huge attraction of customers who are in a dilemma adopting open source based offerings.
HP Helion portfolio consists of a mix of open-source and proprietary technologies, but the messaging around it emphasizes OpenStack and Cloud Foundry strongly. While this is the right messaging strategically the value proposition needs to be clearer. Otherwise it’s customers are either not going to see the value in the constituent open-source technologies (see it as fully proprietary), or going to see only these open-source components (see it as fully open-source) and be skeptical.
This was not the case with HP Cloud earlier, because, it only had OpenStack which was wrapped deep inside. It was also not offered as a standalone product (as in the case of HP Helion OpenStack).
HP has its share of missed/ not successful initiatives — Autonomy acquisition, earlier version of HP Public Cloud, etc. While this is bound to happen to any big company, customers might have this concern with HP Helion too. However, with a strong leadership, financial backing and direct support from the CEO, I am positive that earlier misses won’t be repeated with HP Helion.
I found it difficult to wrap my head around the massive set of product offerings outside of HP Helion portfolio. This could be confusing to an existing customer who is used to certain offerings (say HP Financial Services); how are they going to be transitioned to cloud/ HP Helion?
This weakness can easily be turned in to a huge opportunity, given HP’s success with enterprise customers and huge services force. It also helps that the customers already have an existing relationship with HP.
Fellow bloggers kept asking this, along with ‘why a new OpenStack Distro?’. With many a OpenStack distros, and HP’s long involvement in OpenStack, this is a natural question. HP Helion needs to get the messaging stronger on value proposition to answer “why” and “why now?”.
I don’t see being a new OpenStack distro as a weakness; but the need for more experience selling open-source focused solution is. This is also where a talent acquisition could help — engineers, sales, marking, services or support talent experienced in OpenStack based offerings.
My suggestion would be a smaller team (less than 30), so that is easier to integrate without losing the momentum.
Enterprise space is a great advantage for HP. Enterprise customers have a strong relationship with HP, have been using its various offerings for quite a while. They would be much more comfortable taking HP’s help in moving to cloud — HP Helion. I can’t stress enough the advantage of having the entire stack offered by a single vendor here.
HP has demonstrated success in Private Cloud Solution offerings. Combined with its public cloud offering, multiple data centers, HP has a strong path to success in Hybrid cloud. It also had the unique advantage of having OpenStack based both public cloud and private cloud solutions (until eNovance acquisition by Red Hat). HP is well positioned among OpenStack vendors in this hybrid cloud opportunity.
HP has demonstrated strong OpenStack code contributions, stronger commitment to OpenStack and is one of the Platinum members of OpenStack Foundation. HP also has public cloud offering and private cloud solution based on OpenStack. They also have dominant mind share around enterprise customers.
All these put them at strategic advantage to be a dominant OpenStack vendor.
As much as HP is upping their game, competition is too. Consider eNovance acquisition by Red Hat. Consider IBM releasing their OpenStack distro. Consider incumbent OpenStack experts firing in all cylinders (Mirantis expanding to Europe, Metacloud expanding to hosted private cloud solution etc and more) — even though these are much smaller compared to the size of HP, they have a stronger mind share among OpenStack community.
There is also a whole bunch of action happening outside of OpenStack: around Docker and similar eco-systems. While they are at much incipient stages compared to OpenStack, it is something not to ignore.
HP Helion needs to continuously keep upping their contributions to OpenStack and continue courting existing customers to HP Helion.
This is not unique to HP. Any big company is bound to have some slow moving parts and some really fast groups. HP Helion is moving at a very fast pace, which some other groups/ traditional cash-cows may not like. One can only do such much about this; as long as HP Helion is executing on its plans, this threat is irrelevant.
This is also needs to be considered while considering talent acquisitions. Smaller teams with less physical baggage are more suitable than big companies. It is very important to continue the momentum.
HP Helion has right things in place: strong leadership team, a strong engineering team, substantial financial backing, direct support from the CEO, a and existing HP customer base. From what I have seen, HP Helion has the right strategy and vision. They are also executing on their plans so far.
HP Helion needs to continue their execution and HP Helion will be a huge success — I am confident!
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sriramhere/cloud-don-llc-announces-entry-into-the-rackspace-partner-network-3c6473c7c17e?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sriram Subramanian
Jan 14, 2014·2 min read
We are excited to announce that Cloud Don has become one of the Referral Partners for Rackspace. Since many of our clients are in the APAC region, we are currently partnered with Rackspace HK. Here is the press release.
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
"
https://medium.com/clouddon/openstack-marketplace-review-c16e9adcd3fa?source=search_post---------193,"There are currently no responses for this story.
Be the first to respond.
One of the interesting announcements during the Atlanta Summit was the launch of OpenStack Marketplace. OpenStack Marketplace provides necessary details that help you to make an informed decision in choosing the tools/ resources for your OpenStack cloud. I am a firm believer of open, transparent markets and am happy to see such an effort for the OpenStack eco-system. Here are my observations playing with it and some recommendations to improve.
Here are the top 3 things I like about the OpenStack Marketplace. I am particularly interested in the Rating/ Review feature.
Not only the participants are classified appropriately (like Training Service Providers, Consultants etc), they are geo-tagged. If you want a OpenStack Public Cloud in Europe, zoom in to Europe or choose from the dropdown list.
The map also gives me a quick view in to where OpenStack clouds/ services are offered globally.
When I asked for the Ratings/ Reviews feature during my chat with the OpenStack Foundation during the summit, pat came the reply ‘It’s already there!’. I have always felt the need for customer-driven ratings/ reviews for OpenStack services, something like Yelp. Very happy to see that in place!
They of course need to be populated; if you could do your part by rating/ reviewing appropriately, it is much appreciated.
There is good amount of details for each listed participant — data center locations for public cloud, APIs supported, version supported, duration and levels for courses, billing plans etc., appropriate to the type of services they are offering. They also appear to be normalized to follow same patterns across different vendors in a category. Such normalized details help consumer to compare apples to apples.
Here are some features/ improvements I would like to see in the Marketplace.
As I had mentioned in my summit report, every body was hiring. With the dearth of developer/ operator resources, it might help to allow listing of jobs and profiles here. Particularly Profiles of developers seeking for new opportunities would help the situation (OpenStack Job Board already lists jobs, but not as part of OpenStack Marketplace)
This has been asked around a lot. I am still not convinced if the Foundation should be doing such certifications, but it is only a logical next step to provide some kind of vetting around the services. I would like to see some kind of vetting here (and having ideas how to do this while I am composing this :))
Developer in me speaks! It would be cool to send in location/ type parameters as query parameters and get the results. Currently, all search is UI based. I need to choose the location/ type by either clicking in the map, or choosing from the dropdown. Lazy me likes something like
http://www.openstack.org/marketplace/public-clouds/service=object&location=us
Next step would be a cool Mobile App :)
I am almost tempted to include a federation of cloud service providers, but we are far from it. So not here, not now.
I am thrilled to see this initiative and view this as a great case of Foundation playing the role of enabler in the OpenStack eco-system, Kudos!
Do your part by either getting your service listed or make informed decision with the help of the OpenStack Marketplace and finally by providing rating/ reviews for the providers who have helped you!
CloudDon - catalyzing modern enterprise IT transformations
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
"
https://medium.com/@sriramhere/openstack-ecosystem-mapped-a98d3eaf6e0?source=search_post---------194,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sriram Subramanian
Jun 4, 2015·2 min read
[Given how dynamic the OpenStack ecosystem is, we knew that we need to update the mapping often. With the acquisition news around Blue Box and Piston Cloud today, we had to do it sooner than expected. Congratulations!]
Download the whitepaper here: Mapping OpenStack EcoSystem
[caption id=”attachment_719"" align=”alignleft” width=”1101""]
OpenStack Eco System Mapped[/caption]
OpenStack ecosystem is a vibrant, diverse, ever evolving one with more than 400 participating companies and more than 18000 individual members. Participants vary from small startups to huge, public companies. Solutions vary from niche offerings to a wide portfolio of offerings. With such a variety and diversity, there is also a possibility of getting overwhelmed.
We have mapped the OpenStack ecosystem and are publishing the results as a free for download position paper. If you are curious about the OpenStack ecosystem or would like to learn more on various options and choices available while adopting OpenStack, this will be of great help.
You may also want to watch the presentation at the OpenStack Summit titled ‘Navigating the OpenStack Eco System’ that was co-presented with Seth Fox of Solinea or download the slide deck here. If you want to check the mind map, it is shared here.
If we had missed any or if you would like us to consider someone, please leave us a comment or reach us @sriramhere
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
"
https://towardsdatascience.com/access-free-google-cloud-public-dataset-with-python-42eb378be72c?source=search_post---------195,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joe T. Santhanavanich
May 17, 2020·5 min read
Good news for Data Scientists! Google is making a hosted repository of public datasets, like Johns…
"
https://medium.com/bandprotocol/google-cloud-integration-with-band-protocol-oracles-and-deep-learning-for-real-time-crypto-price-e6e863070f74?source=search_post---------196,"There are currently no responses for this story.
Be the first to respond.
We are proud to announce that Band Protocol has integrated into Google Cloud Public Data to enable immediate and accurate analysis of financial time series data. With Band Protocol oracle data live on Google Cloud Public Data; traditional, hybrid blockchain and cloud applications can be built to leverage unique data feeds from decentralized oracle services.
At Band Protocol, our teams are empowering researchers and developers to use decentralized oracles for any external data source or type, regardless if the application is natively built on the blockchain or Web 2, through the flexible design of Band Protocol oracles.
console.cloud.google.com
Accurate and reliable price discovery mechanisms are crucial to all capital markets. Google Cloud Public Data has a wealth of data available for unique analysis. This is especially interesting for real-time financial time series data using novel approaches to Machine Learning. In particular, a Keras model implementing an LSTM neural network for anomaly detection is provided. For this article, we will refer to the Band Protocol public dataset available on Google BigQuery.
More interestingly, the data derived from the auto encoder-decoder could be considered a dataset in and of itself. This dataset can then be used to support the creation of a new decentralized oracle through custom oracle scripts on BandChain — further expanding the capabilities and offerings provided by Band Protocol.
On-chain smart contracts on any blockchains supported by Band oracles will then be able to in turn have access to pre-trained neural network and anomaly detection systems to perform complex business logic in a trustless manner without relying on other extra external parties.
Take a decentralized insurance protocol as an example, claims can be triggered and conditioned based on data anomaly detection while computation on-chain remains relatively minimal with little overhead. The idea can be further generalized to different types of software to enable smart contracts to delegate complex or expensive computations to GCP and thus creating hybrid cloud-blockchain applications.
The whole process can be broken down into two parts:
Check out Google Cloud’s article for the full breakdown of technical details:
medium.com
This article is based on the work of Reza Rokni. For more examples of time series data processing in Dataflow refer to this repository:
github.com
With Band Protocol oracles fully integrated into Google Cloud Public Data, this is the first of many use-cases we are exploring with partners to bridge traditional enterprises and blockchain applications. Our focus is to continuously and rapidly expand the support of data available on BandChain — pushing the use-cases far beyond just Web 3 alongside many enterprises.
About Google Cloud:Google Cloud provides organizations with leading infrastructure, platform capabilities and industry solutions. We deliver enterprise-grade cloud solutions that leverage Google’s cutting-edge technology to help companies operate more efficiently and adapt to changing needs, giving customers a foundation for the future. Customers in more than 150 countries turn to Google Cloud as their trusted partner to solve their most critical business problems.
https://cloud.google.com/
About Band ProtocolBand Protocol is a cross-chain data oracle platform that aggregates and connects real-world data and APIs to smart contracts. Band Protocol enables smart contract applications such as defi, prediction markets, and games to be built on-chain without relying on the single point of failure of a centralized oracle. Band Protocol is backed by a strong network of stakeholders including Sequoia Capital, one of the top venture capital firms in the world, and the leading cryptocurrency exchange, Binance.
Website | Whitepaper | Telegram | Medium | Twitter | Reddit | Github
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
626 
626 claps
626 
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
Written by
Business Development & Growth | Band Protocol | Start-Ups & Personal Development |
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
"
https://medium.com/aws-enterprise-collection/new-york-public-librarys-cloud-journey-3e43ba9e3ae7?source=search_post---------197,"There are currently no responses for this story.
Be the first to respond.
“A library is the delivery room for the birth of ideas, a place where history comes to life.” -Norman Cousins
I had the pleasure meeting Jay Haque a few years ago when he started leading the New York Public Library’s (NYPL) cloud Journey. I was on a similar Journey at Dow Jones, and we had the opportunity to share our stories. A few weeks ago, I was delighted to see Jay comment on one of my posts, and after hearing a bit more about how his Journey at NYPL had progressed, I thought his perspective on best practice would benefit the broader market. It turns out that I wasn’t alone, and today Jay and his team were named the winners of the 2016 AWS City on a Cloud Innovation Challenge. Thank you Jay and I’m looking forward to seeing you and the NYPL continue to transform the way you deliver technology for your customers!
(Stories like this one that are based on real experience tend to be the most valuable, and I’d love to host more of these kinds of posts. We liked this one so much that we included in my book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT. If you have a story you’d like to share, let’s talk about posting on my blog!)
-Stephenorbans@amazon.com@stephenorbanRead My Book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT
When Stephen asked me to write about my experience with incubating a cloud center of excellence, I revisited his “7 Best Practices for Your Enterprise’s Journey to the Cloud” post and found that my company’s experience aligns with these practices.
The New York Public Library’s Journey speaks directly to the concept of scale. A large team or a big project is not necessary to start the enterprise cloud Journey, you can start with a small, focused effort, geared at establishing your practice and poising it to scale. Significant top-down support with overwhelmingly strong vision and “air cover” along with financial backing are any technologists’ dream come true, but we know that doesn’t always happen. In absence of this, start somewhere, no matter how small.
Our Journey started with the simple idea of building a configuration management platform in the cloud to prove the redesign of our primary website and the release of our famed Digital Collections site would benefit tremendously from it. We realized quickly that the cloud, and specifically AWS, accelerated project completion times through increased automation, reduced costs with right-sizing, provided redundancies, and enabled seamless scaling capabilities that would otherwise be cost prohibitive. Our Journey had humble beginnings and the resulting success has been extraordinary; the use of AWS by our world-class development, product, project and DevOps teams won us the honor of being named the winner of the 2016 AWS City on a Cloud Innovation Challenge in the Best Practice category.
Working with Control Group, an AWS partner, we devised a plan that not only delivered the necessary infrastructure for a web property but also a configuration management platform, which would accelerate future projects. The core idea here is that, while the web property will one day be retired, the management platform will scale as needed and will automate the infrastructure builds for a limitless number of sites. The project would be completed in no more than 12 weeks with less than 50 percent allocation of the team time.
To give the clearest sense of how our project aligned with Stephen’s seven best practices, I present our experience in the same format. (If you haven’t read the post, I highly suggest you do by going here.)
Top-down executive support gives your initiative purpose and weight. In terms of scale, start with what you can get, even if it’s only one executive or one project. Stay focused on building further executive support as you rack up “wins.” This will help you garner support organically.
When we first discussed the Journey, the Library had two major SaaS projects underway, a newly appointed CTO, limited human resources, and faced potential budget cuts. As you might imagine, the appetite to start a large-scale, focused cloud initiative was low.
Still, we knew we had to start somewhere or risk falling behind industry trends. Our CTO backed building a configuration management system in the cloud as a low-risk, low-resource utilization project that would be completed quickly. Once that project illustrated the value of the cloud, interest from the CTO and other executives grew considerably and would be the foundation of support for the rest of our Journey.
To obtain the necessary support of the project, I focused on the following five factors:
Technologist are always learning — it’s the very nature of what we do. In terms of scale, be mindful of the capacity, or available time, to learn. Then, motivate your team and provide any resources you can. Focus on learning the core elements that are going to provide the greatest immediate return. For us, orchestration, configuration management, and code became the centerpiece of everything, so we focused on learning these things first.
At the start of the configuration management platform project, the team was well aware of our ambitious goals. The weeks leading up to kicking off our first AWS project were filled with discussions about the service: how and where we might leverage it, partner presentations, and learning about what other organization were doing. This built a sense of excitement and strong willingness to learn. As the project began, sufficient “air cover” was garnered by the allocation of resource time through our PMO practice and CTO level support. Finally, in absence of AWS offered courses, we partook in partner provided workshops.
In leveraging a partner to help kickstart our Journey, we focused on fostering a culture of experimentation.
The partner presented technical ideas well in advance of implementation, giving the systems engineering team sufficient time to explore and test-drive the technology in our AWS sandbox environment. The sandbox provided a flexible environment that allowed the systems engineers to quickly build and tear down stacks and experiment with various solutions to complex problems. We explored instantiation of our web infrastructure using various methods including using AMIs, complete orchestration using Puppet, and syncing code from external repositories or S3. This level of experimentation, along with deep-dive discussion with our partner, provided a forum to meld our ideas with emerging best practices, ensuring we effectively weighed competing technologies.
The culture of experimentation seeded in those early days of our Journey is still very much in place now. Engineers and developers with varying specialties are encouraged to experiment with all aspects of the cloud. We architected and incubated a media-transcoding pipeline using Elastic Transcoder during a one-day internal hackathon. We could not have realized this level of autonomy in experimentation without AWS, which provides a means for organizations to experiment without a significant financial investment.
We could not have made ground as quickly as we did without Control
Group. In terms of scale, staff augmentation infused our small systems engineering team with proven AWS expertise. Control Group offers customers established best practices for automating system builds and deployments in the cloud that they also use within their own software development operation. Their practical experience with the AWS, both as a service provider and a user, helped accelerate our Journey by drastically reducing our learning curve.
Partner involvement in the early stages of our Journey supported efforts to educate our team, internal experimentation, and the application of best practices in the work we did. This close relationship enabled continued success.
As our initial AWS project was nearing completion, we focused on the “what’s next” element. In terms of scale, we focused on going BIG. This would require substantial support.
The development and systems engineering teams discussed how to build future projects on AWS. This was the beginning of understanding how roles would change, how we would work together, and realizing how much this way of working would impact overall velocity. We maintained partner support as we moved our primary website into AWS and followed by independently working on the release of the highly anticipated Digital Collections site on the platform. The Digital Collections site was large and complex enough to truly test our ability to operate in the cloud. Our systems engineering and development teams had amassed sufficient expertise in the months leading up to the release and having Control Group at arms reach gave us full confidence in our ability to architect the right solution for the high-demand web site.
We’re still very much working on our CCoE, and even on the concept of DevOps. We’re learning what the competing forces are and asking hard questions about how to balance them. For example, we want velocity, but don’t want to sacrifice integrity of our systems. We want standardization, but not too much overhead. These questions are all asked in terms of optimization: we’ve gone from months to deliver infrastructure to just days, and yet we’re continually asking “how can we do this better and faster?”
This was an absolute necessity for the New York Public Library, as we have a significant on-premises infrastructure that we must continue to operate. Our primary focus is on the building new products in AWS and moving those sites that we can (easily) onto the platform. The organization’s appetite to move legacy content to the cloud is growing; however, the resources necessary to execute a move are at times better utilized towards moving new products forward. In terms of scale, we must pick our migration targets wisely, and consider all our options with legacy systems. Migration is one; deprecation is another. In the meantime, we actively support a hybrid architecture.
We are partially here. We are very good at building web-centric systems on AWS, and these are almost always built in the cloud. In fact, there must be compelling technical reason for us to deploy a new web solution on-premises. We’ve still got some learning to do on how to deploy other types of business applications in the cloud, and even these cases, we tend to ask if a SaaS product is a viable alternative. The road to a cloud-first policy is paved with wins from the Journey; the cost and efficiency benefits speak for themselves and enable the realization that cloud-first is the optimal strategy.
NYPL was just starting its cloud Journey when Stephen and I first met to share our stories. We’ve come a long way since then and the model for success has become clearer in that time. Stephen’s masterful articulation of this in the seven best practices is beneficial to any organization, at any stage, of its Journey. I hope this post was helpful to those considering how and where to start their Journey. I would love to hear about your own experience as it will add to the collective library of success stories, and every story is a learning opportunity.
Jay HaqueDirector of DevOps & Enterprise Computing | New York Public Library | Follow NYPL: NYPL Archivesjayhaque@nypl.org | Follow me: @jayhaque
Tales of AWS in the Enterprise
17 
2
17 claps
17 
2
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
"
https://medium.com/analytics-vidhya/open-data-battlefield-azure-open-datasets-vs-google-cloud-public-datasets-4055fe257959?source=search_post---------198,"There are currently no responses for this story.
Be the first to respond.
Disclaimer: The following content is not officially endorsed by my employer. The views and opinions expressed in this article are those of the author’s and do not necessarily reflect the official policy or position of current or previous employer, organization, committee, other group or individual. Analysis performed within this article is based on limited dated open source information…
"
https://blog.getambassador.io/selecting-an-api-gateway-for-continuous-delivery-of-cloud-native-applications-8ba05fa1c74?source=search_post---------199,"With the vast majority of organisations either assessing a move to public/private cloud platforms or actually migrating workloads, much is being written about determining if your workloads and organisation are “cloud native” or “cloud ready”. The excellent books “The DevOps Handbook”, “Architecting the Cloud” and “The Practice of Cloud System Administration” cover the organisational, design and operation perspectives in depth. Individual cloud vendors like AWS, GCP and Azure have written their own migration guides, each with their own approach. However, at Datawire we often see organisations struggle with the assessment of the cloud-readiness of their development teams, tools and processes. One area in particular, which we will explore in this article, is the selection and use of a cloud native API gateway.
An API gateway is a vital piece of infrastructure, as every inbound (user) request will pass through this platform component — it literally is the front door to your system. The API gateway is the place to implement support for cross-cutting concerns like release management (A/B testing, canary releasing, header augmentation), security (authz, authn), and basic fault tolerance (circuit breaking and exception catching).
This is not a particularly new area of technology, and enterprise organisations have driven the development of API gateways focused around the “productisation” of APIs, such as API monetisation and API management. However, migrating to the cloud — and taking advantages of the associated benefits of “cloud native” technologies like containers (Docker), dynamic orchestration (Kubernetes), and the microservice architecture pattern — often require a different focus with the API gateway.
Increasingly we are seeing clients with dynamic and fast-moving businesses move away from enterprise style gateways towards the more “cloud native” API gateways. Although the newer gateways may not offer the GUIs and “drag and drop” management (which are useful for centralised API management and product teams), they do offer support for infrastructure as code (IaC) and declarative configuration that can be used by individual cross-functional, product-focused teams to release and manage services and business functionality as part of their typical development workflow.
We’ve compiled this table below that highlights some of the features we believe are important for organisations looking to embrace cloud native technologies, architectures, and workflows. We often get asked about Ambassador vs Kong, OpenResty (NGINX) vs Ambassador, or Kong vs Traefik, and although this is by no means a complete list of available API gateways, we believe this is a representative sample from across the industry, and should provide insight for teams looking to implement an API gateway:
Once you have chosen your API gateway, you can learn more about how to integrate a cloud native API gateway into your continuous delivery practices and tooling within out latest blog post, “Continuous Delivery — Ambassador API Gateway”.
The next post in this series, “Next-Level Testing with an API Gateway and Continuous Delivery” covers how you can use your gateway to learn how your service will perform under realistic use cases, load, and failure scenarios.
If you have any questions, then please get in touch, or please join us on the Ambassador Gitter.
Developer-First Kubernetes.
86 
1
86 claps
86 
1
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
"
https://medium.com/swlh/deploying-your-angular-app-to-ibm-cloud-759e20ff4ff7?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
I will be writing about how to deploy an angular application on to the public IBM cloud platform and be able to access the application.
So, to put it briefly here about what this blog post will include, we will create an angular application using the CLI, make our application deploy-ready, build the application, create and access the IBM Cloud, create delivery chains, API keys and finally put in…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lakshmanok/how-to-do-distributed-processing-of-landsat-data-in-python-37abb6ba0bca?source=search_post---------201,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Mar 2, 2021·6 min read
Originally posted on Google Cloud Blog at https://cloud.google.com/blog/products/gcp/how-to-do-distributed-processing-of-landsat-data-in-python
One common data analysis task across the agricultural industry, as well in academia and government (for drought studies, climate modeling, and so on), is to create a monthly vegetation index from Landsat images, which is now available as a public dataset on Google Cloud Platform (source of Landsat images: U.S. Geological Survey). One approach to create such a monthly vegetation index is to write a data processing script that does the following:
If you were the data analyst or engineer assigned to this task you could code this workflow as a Python script, but parallelizing such a script to run on many machines in a fault-tolerant way is quite hard. Wouldn’t it be great if there were a framework that would distribute this script onto many machines without you having to manage any clusters or retries? That’s exactly what Google Cloud Dataflow (which has an open source API in the form of an incubating Apache project called Apache Beam) can do.
Cloud Dataflow provides a fully-managed, autoscaling, serverless execution environment for data pipelines written in Apache Beam. While it excels as a unified batch and stream-processing framework and as an Apache Pig/Apache Spark replacement for doing data processing jobs without having to spin up (and manage) Apache Hadoop clusters, you can also use Cloud Dataflow to parallelize your scripting code across many machines, helping to bring speed, reliability, and scalability to the process.
In the remainder of this post, we’ll explain how it’s done. Although the use case involved is fairly specific, the same principles will apply to many other workloads across different industries.
The Python code is quite straightforward. The Landsat dataset on GCP includes an index file that is read line-by-line using beam.Read on a TextFileSource:
The results from beam.Read are piped to the filterScenes method, which finds scenes that cover the lat/lon in which we are interested. We will get multiple scenes because the spacecraft makes 2–3 passes over the area every month, so we next find the clearest image for each month.
The information about those scenes is written to an output file, and the vegetation index is computed for each scene:
The code above also illustrates how you can pipe the result of a transform (i.e. scenes) to two places in Google Cloud Dataflow. The end result, over Reunion Island (in the Indian Ocean), looks like this:
There are three methods being invoked from the pipeline above: filterScenes, clearest, and computeNdvi. Let’s look at these methods next.
The filterScenes method verifies that the scene contains the (lat, lon) in question.
The SceneInfo class parses a single line of the comma-separated-values file and stores the values. If the scene meets the spatial criterion, the method returns a 2-tuple of the year-month and the scene. Note that filterScenes is a generator method. (In general, Map in Python Dataflow corresponds to lambda functions with one return value per input, and FlatMap corresponds to generator functions that are not one-to-one.)
The method clearest takes a list of scenes and finds the one with minimum cloud cover:
computeNdvi consists of downloading the appropriate images, computing the normalized difference, and uploading the result to Google Cloud Storage. (See the github repo for full code.)
If we are interested in a single point, it is sufficient to simply look for Landsat images that cover that one point (assuming that we are careful to disregard areas within the bounding box that were not actually scanned by the spacecraft). Here, though, we are interested in coverage of an area. This makes the problem a little more difficult since individual Landsat scenes may cover only part of the island:
In our workflow, therefore, we need to look for all Landsat images that cover any part of the island. Then, for each unique path and row of the spacecraft, we should find the clearest image during the month. These images can then be mosaiced to form the Landsat image over the island.
Fortunately, Python (and Cloud Dataflow) are sufficiently expressive to handle this task with ease. Here’s a more sophisticated version of the above script:
The new functions in the above snippet deal with the intersection of bounding boxes and creating appropriate keys:
You can run the above code locally as long as you have Python packages for GDAL (needed to read Landsat) and Google Cloud Dataflow installed; simply run the dfndvi.py script. Alternately, spin up a Google Compute Engine instance, install the above Python packages using install_packages.sh, and run dfndvi.py. Doing that for 2015 and visualizing the resulting images as a movie shows changes in vegetation across the year:
One of the key benefits of Google Cloud Dataflow is not having to run it locally or on a single machine — rather, you can run it at scale on many machines to get the job completed quickly. To do that, change the pipeline runner from DirectPipelineRunner to DataflowPipelineRunner and supply your GCP project id. The run_oncloud.sh script specifies the necessary command-line parameters to lob off the computation to Google Cloud:
Running the script in distributed fashion with Cloud Dataflow also brings orchestration and failure handling to the process. If one of the workers fails, for example, a replacement worker will take over its task. If some step is taking too long, more workers will automatically be provisioned. In addition, the GCP web console allows you to monitor the task as it executes, showing progress and number of elements processed at each step:
In this post, you learned how to process the public Landsat dataset on Google Cloud Storage in a distributed manner, the concepts of which can be applied to many other use cases. The key ingredient here was Google Cloud Dataflow, which lets you distribute your Python scripts over lots of machines without having to configure or manage servers.
Happy coding!
Data Analytics & AI @ Google Cloud
62 
62 
62 
Data Analytics & AI @ Google Cloud
"
https://medium.com/@cloud-opinion/lies-about-cloud-8cc510f2cfd4?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
.Cloud Opinion
Jul 15, 2015·3 min read
Clarification: In my opinion, it is unnecessary to specify “Public” before Cloud. There is Cloud, which is Public and there is Datacenter tech. What people call Private Cloud is improved Datacenter tech.
It is no secret that Cloud is a disruptive tech that is changing the way IT consumes and builds applications. This change is something legacy vendors are struggling to understand and adapt to. When faced with a change that they can’t face , these vendors are using tactics that border on being lies. I will talk about what I see as the Top 5 lies spread by legacy tech vendors.
Lie 1: Cloud is Insecure or “If you want Security and compliance, you should not use Cloud”
This leverages the fear of unknown to scare customers and spread FUD. The reality is far different. Cloud may offer higher level of security and compliance than home grown infrastructure. Most customers have realized it and ignore vendor FUD.
For example, look at level of compliance that AWS provides: http://aws.amazon.com/compliance/
Also, look at security of AWS here: http://aws.amazon.com/security/
Look at security of GCP here: https://cloud.google.com/security/whitepaper
Lie 2: Its expensive
This leverages the fine art of comparing apples and oranges. Vendors compare raw cost of a server with raw cost of a cloud instance. This ignores headcount costs, consulting fees, implementation costs etc. While this may look correct on surface, upon further examination, you will discover that Cloud infrastructure saves you money over time. As your usage increases, you will be able to get even better pricing from Cloud Providers. With careful planning, you can also use reserved instances and save even more.
Look at work by Adrian Cockcroft on this: http://www.slideshare.net/adrianco/cmg-workshop
Lie 3: Don’t put all your eggs in one cloud
This leverages the old tactic of scaring customers into hedging their choices. On the surface this may look like a fine advice, but if you dig deeper, you discover that its the worst advice possible for today.
Best of breed is not a good strategy here, as Cloud is still an evolving technology. Standardize on one vendor to get best pricing and support. You can use second vendor for backups etc, but in general, pick your vendor and go all in.
There may come a time in the future where you want to go to best of breed. Its not today.
Lie 4: Not all workloads are right for Cloud
This is my favorite lie repeated by many a thought leaders at every opportunity they get. The vendors hope to create enough doubt for customers to engage their expensive consulting services. Often this claim is not accompanied by what kind of workloads Cloud is not right for.
Let me be clear on this one. Yes, there may be some workloads that are not fit for Cloud, but its more likely that all your workloads are suitable for Cloud adoption. Legacy apps may be a good example of a workload thats not suitable, but most new app development should go onto Cloud.
Review different types of workload details here: http://aws.amazon.com/solutions/case-studies/all/
Lie 5: Avoid lock-in
This is again a scare tactic employed by vendor thought leaders. They say “Customers should be on the lookout for vendor lock-in and should have in place a multi-vendor strategy”
Let me translate it in plain English for you:
“Bro, if you standardize on Azure, my quotas this quarter are going to get screwed up. Can you please buy atleast our appserver? how about a mobile tool? “
This one doesn’t bother me as much as the other ones. Talking to several vendors during initial tech selection is a good idea to get a better deal from your chosen Cloud provider. But, know that the vendor guys worry lot more about lock-in than you need to.
That’s all folks — as always, one man’s opinion — feel free to agree/disagree here or on twitter.
Parody + Tech commentary.
9 
9 
9 
Parody + Tech commentary.
"
https://medium.com/ibm-cloud/deploying-enterprise-workloads-to-kubernetes-a445daf8af8d?source=search_post---------203,"There are currently no responses for this story.
Be the first to respond.
In March 2017, IBM formed an elite group of executives and technical leaders, including Bala Rajaraman, from its public IBM Cloud organization to address the gap of running enterprise workloads on hybrid cloud infrastructure. In the span of 6 months, IBM delivered a new platform that supports deploying both Cloud Foundry and Kubernetes workloads to a private cloud. In this interview, Bala provides an in-depth overview of the capabilities of IBM Cloud Private and how enterprises can quickly orchestrate their Kubernetes Deployments, whether they are traditional enterprise workloads or the latest cloud native 12 factor applications.
architecht.io
Understand how to bring elastic runtimes to the Enterprise…
21 
21 claps
21 
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
Written by
NCR Executive Director and Chief Architect for Retail Solutions. The opinions expressed here are my own. Follow me on Twitter @todkap
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
"
https://medium.com/@cere-network/cere-launches-the-ddc-testnet-for-early-testing-63bddf8387c8?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cere Network
Dec 24, 2021·4 min read
After 3 years of development, the Cere Network team is proud to announce the public DDC testnet launch of its key innovation piece; the Decentralized Data Cloud!
Cere Decentralized Data Cloud is the world’s first blockchain-based storage solution that is optimized to capture interactions between users, (NFT) assets, and applications that are individually signed and encrypted, along with potential value transfers, to be stored in a tamper-proof and time-capsuled data scheme.
“Cere’s Decentralized Data Cloud takes models like Snowflake’s and makes it much easier for dApps to integrate with decentralized datasets. We have been working closely with the Cere team, and we are excited that developers, companies, and individuals in our ecosystem can leverage smart contracts deployed on Polygon to include trustless data transfers on top of value transfers via Cere DDC. ”
Sandeep Nailwal, Co-founder and COO of Polygon.
The Decentralized Data Cloud is built to solve an actual problem that many businesses and developers are running into these days; the lack of actual data ownership. In an ever-changing world concerning customer data management and monetization, a revolution is now available to every business: a turnkey, decentralized, peer-to-peer, and ethical customer data ecosystem.
With Cere, businesses can easily plug in their existing apps and the power of contextually relevant data, insights, and predictive analytics in real-time to supercharge their value streams.
To learn more about our vision and our journey towards decentralization of (consumer) data, have a look at the Cere Network vision paper.
cere-network.medium.com
Today, we launch our first series of Quickstart Guides that we will keep extending as we progress towards the public launch of the DDC. In this first guide, we will show you step-by-step how you can interact with the DDC and store your data.
Quickstart Guide:
a. Install polkadot.js extension
b. Create an account using extension
c. Go to https://ddc.stage.cere.network/ and connect your wallet
d. Send CERE Test Tokens to your wallet via faucet
e. Pick a Tier and sign-up
2. Interact with DDC using Kotlin client library
a. Install the latest version of the client library from here. For more information, visit the DDC Wiki
b. Extract your public and private application keys from your account using DDC CLI (Install DDC CLI) and plug them in here.
c. Store data
Add the data you want to store
d. Retrieve data
Use public key to configure the consumer
e. Done, your submitted data is now decentralized! In the next DDC portal release, we’ll open up the Decentralized Data/Asset Viewer for inspecting customer data and NFT provenance.
Want to learn more about the technical ins and outs of the DDC? Take a look at the DDC Wiki!
And with this, we’re rounding up our first quickstart guide. Expect many more technical articles on the DDC and its enterprise/NFT security applicability in Q1 ‘22!
Merry X-mas, stay safe ❤
Team Cere
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
63 
63 claps
63 
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/built-to-adapt/demystifying-the-public-or-private-cloud-choice-f1e435c51dd?source=search_post---------205,"There are currently no responses for this story.
Be the first to respond.
Everyone wants to operate like a tech company today. Chances are, your business can’t thrive without improving how you do IT, and executives must decide where to house and process their data. Companies like Liberty Mutual are able to enter a new market in just six months and double the average sales rate, while government organizations are defying expectations with rapidly developed and deployed applications across the board from tax collection to war-fighting.
Your cloud strategy is going to be nuanced. A recent Forrester study found that just four percent of organizations run their applications exclusively in the public cloud; 77 percent of organizations are using multiple types of clouds, both on-premises and off-premises.
So do you go the public or private cloud route? It can be a complicated question. Let’s look at some starting considerations.
Most of IT’s budget and attention is focused on what we used to call “off-the-shelf applications”: email and calendaring, collaboration apps, and industry specific software. These applications are often the slow-moving fodder for your cloud strategy. They should be moved to public cloud first.
For example, Gartner expects almost half of all business users to move their core collaboration and communications systems to public cloud by the end of this year, and more than 70 percent of businesses will be substantially provisioned with cloud office capabilities by 2021. Moving these types of applications off-premises seems like an easy win and, more importantly, frees resources to focus on building out larger software development and delivery capabilities, the core asset for any successful digital transformation.
If you’re collecting user data — location, personal information, credit card information, for example — you have a whole list of compliance issues that will drive your cloud choice. As you sift through various regulations and barriers to decide whether to use a public or private cloud for storing a user’s data, you’ll need to have answers questions like these:
You can drown yourself in a soup of industry and government regulations: HIPAA, PCI, and GDPR, just to name a few. But while compliance issues may seem like an mindless productivity blocker understanding why they exist and working with auditors will help sort your business imperatives.
These regulations are aimed at avoiding nefarious uses such as selling personal data to advertisers or stockpiling profiling data to meddle in politics. As Lauren Nelson notes in her recent analysis of public clouds in Europe, the data management needs of GDPR are driving many organizations to reconsider where they store user data. Often, running their software on private cloud affords organizations more control over data.
To “on the other hand” it, there are cases where using a public cloud service is actually better. Payments come to mind; complying with all of the payment handling and tax regulations globally might be easier to achieve with public cloud-based services rather than building it yourself. Handling sensitive documents might also be better outsourced to organizations like Merrill.
Of course, pure public cloud is rarely an option. Retailers, for instance, often have competitive concerns that drive them away from using Amazon Web Services (AWS), or other cloud software companies might not want to utilize Google’s tools.
Moving to public cloud infrastructure can sometimes cause more headaches than solutions, but nailing down a comprehensive list of technical requirements will create a good checklist. These include operatibility of different database frameworks, load balancing, licensing ramifications, and bandwidth limitations. For example, Chick-fil-A uses a mixture of private and public cloud to support operations by deploying small Kubernetes clusters in each store to support transactions.
When moving to public clouds, engineering teams lose certain operational controls and often need to re-architect their code. The new runtime environments in public cloud often require new skills. None of these concerns are impossible to solve, but take these costs of change to heart in your economic considerations.
For instance, airlines access an immense quantity of interrelated databases. Searching for fares, transferring tickets, tracking baggage, tracking flights, maintenance, reward programs, booking revenue, partner airlines, integrating with each airport — the number of integrations starts to boggle the mind. An airline probably can’t afford to trust only a public cloud environment to run their business. There are too many moving parts to expect a single vendor to meet all their needs.
Different cloud solutions don’t lend themselves to easy comparisons like new phones do: run down a checklist of features and specs, then weigh against the pricetag. Cloud architectures are too complex and need to be visualized too far forward in time. They’re more like buying solar panels, where the upfront cost hurts, but you’re playing a longer game with the investment. However, you need to be sure you’re staying put (to keep the analogy going) with your strategy, features and hardware, such as servers and an ops team. Those can quickly become painful losses if, in a couple years, you hadn’t assessed the overhead costs correctly.
Yet there are some basic starting points. What features of public cloud would be better than private cloud — and how do you assign real financial value to them? For instance, how useful are machine learning tools in the cloud you’re considering? A retailer could use such services easily to start targeting ads or upselling recommended items, and so they might choose Google’s cloud. Or maybe for regulatory reasons, or because the retailer can do it better themselves, they’ll do this processing on their own, private cloud.
That focus on business outcomes is what should drive your choice of public versus private cloud. It’s all too easy to look at either option based purely on cost, which is the old way of thinking about IT: “how cheap can we get this service?” When IT is one of your core business enablers, the better way to think about it is, “how much money can this service make me?”
In this way of thinking, you focus on what type of infrastructure enables your software teams. You want a platform that focuses on delivery speed so you can start designing better, more productive, and profit driving software. In some cases, this might mean just modernizing your existing, private cloud-based stack. Oftentimes, organizations are operating under five, ten, or even decades old notions of how software should be developed and run. Shifting over to a more contemporary, agile approach can drive dramatic results.
For more detailed models, my old research house, 451 Research, has long studied the question of cloud costs, providing several models and price tracking to figure out which side of the firewall is best across various IT needs.
You have to know what you’re building. That may seem obvious, but as Pivotal consults with organizations of all types, you’d be surprised how many engineering teams build in the dark. We designed Pivotal Cloud Foundry to support both private and public cloud, giving you maximal optionality when planning your cloud strategy. Even if a business needs to change providers for certain applications, the development and management layers atop it will remain consistent
We constantly remind our customers to ask the basic questions. The answers evolve.
How much traffic will the application get? Will it only be used internally? Who affects the load? What data handling and process regulations do you need to follow? Will the application branch out to other areas of the business? If it touches the public — will it be mobile? What are your long term plans for running your own data centers? What machine learning applications do you need? How are you going to spread out the computing geographically?
The questions don’t end! In a point of transition like we’re seeing in IT, it’s good to err towards maximizing flexibility to give yourself the most options in the future as needs change. Over the next five years (if not longer!) businesses will experiment with new strategies and business models, and they’ll need an IT partner who’s equality deft and ready for whatever exciting adventure comes next.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
101 
101 claps
101 
Written by
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
How software is changing the way society and businesses are built.
Written by
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
How software is changing the way society and businesses are built.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/containermind/a-beginners-guide-to-kubernetes-7e8ca56420b6?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
Kubernetes has now become the de facto standard for deploying containerized applications at scale in private, public and hybrid cloud environments. The largest public cloud platforms AWS, Google Cloud, Azure, IBM Cloud and Oracle Cloud now provide managed services for Kubernetes. A few years back RedHat completely replaced their OpenShift implementation with Kubernetes and collaborated with the Kubernetes community for implementing…
"
https://medium.com/google-cloud/simplifying-continuous-deployment-to-cloud-run-with-cloud-build-including-custom-domain-setup-ssl-22d23bed5cd6?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
Cloud Run is fully managed Serverless product launched by Google Cloud for public beta availability which offers to run docker image in Serverless environment. With the introduction of Cloud Run allowing to run docker irrespective of run time environment, the limitation of Cloud Function for limited environment and version is mitigated by Cloud Run which can be run live in few minutes. Also, Cloud Run can be utilized on GKE with Istio and Knative.
In this post, we will walk through from github commit to continuous deployment to Cloud Run along with custom domain for http endpoint. The codes and configurations used in this post is on my CloudRun-Demo github repo.
We have a simple http server running on port 8080
For making our final docker image smaller and secure, we are using multi stage build with final artifact on distroless base image.
If you are interested in learning more about distroless image, here is one of my post:
medium.com
Cloudbuild.yaml for Google Cloud Build to create docker image and push to container registry(gcr)
Head over to Cloud Build Trigger and create a new trigger for github(or other source repositories) by adding authentication.
Select cloudbuild.yaml as build configuration. For now, we keep trigger for all branches but the selection can be managed with regex for both branch and tags along with files filter.
This steps adds a webhook trigger on github for all events by default.
Now, for triggering a new build, we can manually “Run Trigger” for specific branch from Code Build Trigger Page or make a change on our code and push to github. We can view the build logs from history page.
In this demo, we are using $SHORT_SHA for tagging docker image which includes the first seven characters of git commit hash. So, our image is of format gcr.io/pv-lb-test/cloudrun-demo:201c141 .
Head over to Cloud Run on the Google Cloud Console and “Create Service” with the “Container Image URL” as we got from the Cloud Build above. Give a service name and regional location as Cloud Run is regional resource. If you want to allow invocation from any user, select the “Allow Unauthenticated Invocations” checkbox.
Additional settings allows us to grant spec(Memory Allocation) for each container and maximum concurrent request a container can handle. If the concurrency is reached, Cloud Run automatically scales out new container. If you want to add more environment variables, the key-value pairs can be added.
Optionally, the service can be created with cli command:
After saving the service, in about a minute, you can get a URL for invocation in format: https://cloudrun-demo-doiyqpty6a-uc.a.run.app
Time to make the game moving !!!
For the deployment of new image to Cloud Run from Cloud Build as commit is pushed to github, we need to grant the deployment access to Cloud Build serviceaccount which is of format [id]@cloudbuild.gserviceaccount.com .
Add following permission to the IAM user from IAM & admin console.
And append the following deployment commands on cloudbuild.yaml file.
Change the [Project-Name] and region based on your favorite Cloud Run service name.
Optionally, we can trigger new version deployment with:
For confirmation, we add a change on our Go application http response and push the commit to github. After some time, we can see our changes deployed on the URL. Also, we can see from the Cloud Build log that our image on gcr is deployed:
Rather than the random URL we got from the service, we can use our own nice domain name linked with Cloud Run. For that, we need to verify the domain ownership with Webmaster Central.
On the Cloud Run page on gcloud console, click on MANAGE CUSTOM DOMAIN and add a mapping with servicename and domain.
Click on “VERIFY IN WEBMASTER CENTRAL” where you can get the domain verification options for various domain registrar or provider.
If you provider is not listed there, simply choose last option “other” which gives two options: CNAME and TXT records.
As I am using Google Cloud DNS for managing my record sets, I am going for CNAME record.
In few minutes, the record will be propagated. You can confirm by entering your domain on whatsmydns. After is succeeded, click on “Verify” on the Webmaster Central page.
On the Cloud Run page, click on “CONTINUE VERIFICATION AND CLOSE”.
Now, if you again click on “ADD MAPPING”, the verified domain appears on the list below the service name selection. If you want to use the primary domain, leave the third option as it is.
Click on “CONTINUE” and it will give few IP addresses to add on “A” and “AAAA” record set of the domain.
Then you can invoke the Cloud Run service with nice domain name.
In few minutes, the SSL certificate issued by Let’s Encrypt Authority is also provisioned for the custom domain.
By this way, we can make continuous deployment to cloud run and enjoy serverless architecture on our own container.
That’s it for now, if you are interested on getting my interesting updates, find me on Linkedin, Twitter.
Google Cloud community articles and blogs
578 
5
578 claps
578 
5
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-academy-inc/moving-your-business-to-the-cloud-2d0e38f31beb?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
Whether the destination is a private, public, or hybrid cloud, it’s clear that companies are continuing to migrate their workloads and applications to the cloud.
The latest RightScale report shows that 95% of the respondents for its 2017 State of the Cloud survey are currently using or experimenting with cloud infrastructure as a service (IaaS). By 2021, Gartner reports that of the organizations using the cloud today, more than 1/2 will not just have a few workloads in the cloud, they will be “all in.” This means that rather than lifting and shifting into the cloud, companies will be refactoring and re-building directly in the cloud. In other words, there’s no going back to on-premises.
How can companies expect to benefit from migrating to the cloud? While every organization’s motivation for migrating to the cloud will be different, the advantages are available to companies of every size and in every sector. Let’s take a closer look at what your business can achieve through cloud computing.
On-demand resourcing essentially means that when you need to provide a resource within the cloud, it’s almost immediately available to you to allocate where and when you need it.
Sourcing additional resources for your infrastructure, whether it be compute, storage or network, will be a lengthy process. In addition to all of the steps necessary for choosing and ordering equipment, you’ll also need to install and configure it before releasing it into the production environment. This whole process can take weeks, and weeks is not good enough in today’s world. Sometimes days or even hours is too long to wait for additional resources, especially if it’s impacting your customers’ experience. The on-demand resourcing aspect of cloud computing alleviates this issue entirely by providing almost instant access to a resource that you have selected and configured via a series of options.
Cloud computing offers you the ability to rapidly scale your environment’s resources both up and down and in and out depending on your requirements and the demands of your applications and services. When scaling up and down, you are altering the power of an instance, perhaps using one with greater CPU power to scale up. When scaling in and out, you are simply adding or removing the number of instances you’re using. The cloud’s on-demand resourcing is what makes this scalability possible. Imagine trying to quickly add additional servers and bring them online within your environment after a sudden surge in demand!
Cloud computing offers huge flexibility and elasticity to your design approach. You can choose to have as many or as few resources as you require without having to guess your capacity up front. Architecting for surges in demand on-premises can be both tricky to plan for and expensive. Consumers now expect to access what they need with an almost instant response. Having failing and slow response times could easily result in customers taking their business elsewhere. The sheer flexibility and elasticity of the cloud allow you to deploy different resources and services within minutes or seconds, with as many or as few resources as you need, when you need them.
With many cloud services, you only pay for what you use. For example, if you only have one server running for two hours and then shut it down, then you only pay for two hours’ worth of compute resources and that’s it. With the cloud, you only pay for resources when you are using them.
Within a data center, although your infrastructure is typically available and running 24/7/365, it isn’t always utilized. The power on coiling costs alone for this infrastructure can be phenomenal over time especially when you’re talking hundreds or even thousands of servers. Wouldn’t it be great if you could just go along and switch off the servers when you’re not using them? You could, but that rarely happens. Even if you did, you’d still be paying for the footprint space that those servers occupy. The cloud offers you the ability to shut down any instance that isn’t in use, and you can even schedule it ahead of time.
By design, many of the core services within the public cloud and its underlying infrastructure are replicated across different geographic zones and regions. This alone can be a real advantage for many businesses. Having offsite replication built into some services offers a significant advantage over on-premises business continuity solutions. Many organizations do not have the luxury of having multiple sites to replicate their data, or they are often too small to operate a wide-scale disaster recovery program. The cloud helps ensure the durability and availability of your data often without additional configuration dependent on the service type. It’s all provided by the vendor as a part of their service.
Public cloud vendors such as AWS, Amazon Web Services, and Microsoft Azure have to operate their security at an extremely high standard. They must adhere to global governance and compliance requirements that cover all industries and sectors. Cloud vendors pour huge amounts of capital into developing their security infrastructure with the best technologies and creating new services to help end users secure their data. As a result, other customers using the cloud are able to benefit from the same level of security that large financial corporations require.
While public cloud vendors provide exceptional security for the underlying infrastructure of the cloud, it’s still up to the end user to then architect security in the cloud using the tools and services and applications available.
Understanding the benefits of the cloud is just a starting point. How can it propel your business forward beyond the constraints from an on-premises solution?And how will it impact your operations and your teams?
We can help you answer these questions. Our course, Should Your Business Move to the Cloud? is designed to help you understand cloud computing from a business perspective. We help you understand the pros and the cons, and we’ll take a look at real use cases to help you and your teams ask the right questions to guide your discussions. Try Should Your Business Move to the Cloud? for free today.
Originally published at cloudacademy.com on November 2, 2017.
Cloud Training Has Never Been Easier.
257 
257 claps
257 
Written by
The proven platform to assess, develop, and validate hands-on technical skills.
Cloud Training Has Never Been Easier. Continuous self-paced training for all cloud technologies.
Written by
The proven platform to assess, develop, and validate hands-on technical skills.
Cloud Training Has Never Been Easier. Continuous self-paced training for all cloud technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/inlets-operator-exposing-services-on-private-kubernetes-clusters-with-a-public-ip-e701c64693ae?source=search_post---------209,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Kubernetes Cloud Providers embedded the knowledge and context of each public and private cloud providers into most of the Kubernetes components. With these providers it is easy to expose Kubernetes services running on the specific platforms using the platform native LoadBalancing constructs. If a user is deploying to an EC2 instance or a DigitalOcean Droplet, then they have a public IPv4 address, but when working behind a corporate firewall, NAT, or within a VM or container, this just doesn’t work.
There are multiple other scenarios where this might be an issue:
Tools like Ngrok are well known for tunnelling services for exposing localhost to the web. These tools implement a multiplatform tunnelling, reverse proxy that establishes secure tunnels from a public endpoint such as internet to a locally running network service using a WebSocket. WebSocket is a naturally full-duplex, bidirectional, single-socket connection. With WebSocket, the HTTP request becomes a single request to open a WebSocket connection and reuses the same connection from the client to the server, and the server to the client.
A client runs in the internal network ( alongside the Applications ) and connects to a remote server with HTTP websockets. The Server then forward the request to the Client over the one of the offered websockets.
Alex Ellis — Inlets combine a reverse proxy and WebSocket tunnels to expose internal and development endpoints to the public Internet via an exit-node. A exit-node is a publicly reachable server on any public-cloud platform with Inlets-Server process configured.
Similar tools such as ngrok or Argo Tunnel from Cloudflare are closed-source, have limited built-in and can work out expensive. ngrok is also often banned by corporate firewall policies meaning it can be unusable. Inlets aims to dynamically bind and discover your local services to DNS entries with automated TLS certificates to a public IP address over a websocket tunnel.
Without inlets user might have to configure required firewall rules for the webhooks to reach applications in internal network. This might be a daunting task as keeping track of all the dynamic connections is impossible.
With inlets all the requests are sent from a publicly hosted exit-node (inlets server) to inlets client residing on-premise. Inlets act as a traffic sink, and thus not prone to any types of abuses. It won’t relay any requests out to the public internet. Instead, inlets suffers from the opposite problem.
As mentioned, by default, a LoadBalancer Service for Kubernetes services is only available on cloud providers, not privately hosted Kubernetes clusters. The cleanest way to get traffic into a cluster seems to be a load balancer. However, it requires an external service usually provided by GCP or AWS that doesn’t come with Kubernetes. With Inlets-operator users can seamlessly enable public LoadBalancer for private Kubernetes Services without having to manage a full-fledged cluster on a public cloud platform.
In Kubernetes setting, inlets is implemented as an operator that make use of CRD (Custom Resource Definition) to manage tunnels and its components. With Inlets-operator users can dynamically create an exit server on any of the supported platforms like Digitalocean, Packet, GCP etc. With each service type ‘LoadBalancer’ a dedicated server is created on the supported public cloud platform with Inlets server process.
Inlets CRD shown below enables users to operate (create/delete/get/describe) tunnels as extension to Kubernetes API.
In this walkthrough the operator and sample applications are deployed on a Kubernetes cluster running on a intel-nuc connected to home private network. Inlets operator is deployed as a Kubernetes deployment and constantly polls information from Kubernetes-API for any services with type LoadBalancer.
Inlets-operator supports multiple platforms (Digitalocean, Packet, Scaleway, GCP), with the supported platforms users can make use of auto provisioning of required components to seamlessly initiate an exit-node. Trying out inlets-operator with Digitalocean:
Creating a API key for authentication:
A Kubernetes secret with the access token created above is created:
A sample deployment(Kuard) is created along with a service — type:LoadBalancer.
As seen below, a kuard-service with type:LoadBalancer is create where the process kicks in.
Inlets-operator detects the above service and initiates creating a droplet on the users Digitalocean account using the credential supplied as Kubernetes secret above.
As the provider selected here is Digitalocean, a droplet is automatically created on the user account with Inlets server process running in the same and a public ip assigned.
As seen below the inlets-server process runs on the droplet created above:
A inlets-client deployment is created on the cluster with the operator and auto-configured with the remote exit-node information.
As shown below the client is configured with the public_ip of the droplet created above as remote server destination and local service (kuard-service) as an upstream (generally how a reverse-proxy like envoy work).
Kuard service created above with type:LoadBalancer will be allocated with the public_ip of the droplet deployed above as an external_ip:
Accessing service running in a private environment with public_ip of the droplet created above.
inlets-server process running on the droplet proxying all the requests to the local kuard application running in an internal network.
Users can use cloud providers DNS:
Users can dynamically delete the services which triggers the delete action and all the resources on the public cloud will be cleaned up.
The operator spec enables users to select other supported providers:
By default, for development inlets is configured to use a non-encrypted tunnel which is vulnerable to man-in-the-middle attacks. Users can use Caddy or Nginx to implement TLS by running the same on the exit-node or using nginx running to proxy the inlets server through nginx. By enabling HTTPS users connect to an encrypted endpoint and the inlets clients can also connect to the server over an encrypted tunnel. cert-magic (a go based cert minting stack) is also mentioned in the roadmap.
As seen, with inlets-operator users can easily procure a publicly accessible endpoint for applications running on private clusters. There are many scenarios where inlets can be handy:
Inlets can be deployed on any ARM based device enabling it to run on devices like Raspberry Pi and can be used as a gateway for incoming traffic to services running internally. This also can optimize cloud costs (average running cost: 5USD/month) by just running a tiny VM for inlets-server without having to run an entire Kubernetes cluster on the cloud for dev purposes.
ITNEXT is a platform for IT developers & software engineers…
116 
116 claps
116 
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://life.wongnai.com/aws-gcp-secretless-access-e8ecadf24d90?source=search_post---------210,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
หลายๆ คนที่เป็นโปรแกรมเมอร์ ถ้าเคยลองเล่น AWS หรือ Google API น่าจะเคยกดขอ API Key จาก Cloud ออกมาเหมือนการใช้ Public API อื่นๆ ซึ่งเมื่อนำไปใช้ใน production ผลที่ตามมาคือเราจะต้องเก็บ API Key นั้นให้มีความปลอดภัย
ในบทความนี้อยากจะเล่าให้ฟังครับว่าเท่าที่ LMWN ใช้ มีท่าไหนบ้างที่จะใช้ API Key ต่างๆ ได้โดยไม่จำเป็นจะต้องเก็บ secret หลายๆ ท่าคนที่ใช้ on premise ก็สามารถใช้ได้เหมือนกันด้วยนะ!
หัวข้อนี้จะอธิบายถึงทฤษฎีก่อนว่าเราจะเข้าถึง API อย่างไรโดยไม่มี secret จะอ่านข้ามไปก็ได้ครับ
ปกติแล้วการใช้งาน AWS API จะมี secret 2 ตัวคู่กันคือ AWS_ACCESS_KEY_ID และ AWS_SECRET_ACCESS_KEY ซึ่งไม่มีวันหมดอายุ การใช้งาน AWS แบบไม่มี Secret นั้นก็คือเราจะยังมี 2 ค่านี้อยู่ แต่วันหมดอายุจะสั้นลงเหลือประมาณ 1 ชั่วโมงจนถึง 1 วันเท่านั้นทำให้ไม่สามารถ hardcode เอาไว้ได้อีก หรือถ้าหลุดออกไปก็ใช้ได้ไม่เกินวันก็ถูกยกเลิกไปเอง โดย secret ที่มีอายุจำกัดเวลาใช้งานบางประเภทระบุ 3 ค่า คือ AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY และ AWS_SESSION_TOKEN จึงจะใช้งานได้
ฟังดูแล้วอาจจะรู้สึกว่ายุ่งยากในการใช้งานเพราะต้องต่ออายุเรื่อยๆ แต่ว่าในการใช้งานจริงแล้วปกติเราจะใช้ AWS SDK ซึ่งมันจะจัดการอายุของ access token ให้อัตโนมัติ ถ้าใกล้จะหมดอายุแล้วก็จะต่ออายุให้ก่อนเรียกใช้ API โดยไม่จำเป็นต้องเพิ่มโค้ดใดๆ
ในฝั่ง Google Cloud แล้ว service account token รูปแบบที่นิยมใช้งานกันจะเป็นแบบ JSON ซึ่งประกอบด้วย private key ของ service account ซึ่งภายใน Google SDK จะนำ private key นี้ไปแลก Bearer token ซึ่งมีอายุจำกัด และ SDK จะ refresh token ให้อัตโนมัติอยู่แล้ว แต่ปัญหาก็คือเรายังจำเป็นต้องเก็บ private key นี้ไว้ในที่ปลอดภัยอยู่ วิธีการต่างๆ ในนี้ก็คือจะได้ Bearer token เหมือนกัน แต่ไม่จำเป็นต้องมี private key
Bearer token ที่ได้มาไม่ว่าวิธีใดจะสามารถใช้ได้ทั้ง Google Workspace API, Google Cloud, Google Maps และ Firebase ยกเว้นไม่สามารถใช้เป็น OAuth 2 Client Secret หรือ Google Maps JavaScript API Key ได้ หรือสังเกตง่ายๆ ว่าถ้า API Endpoint อยู่บน googleapis.com แล้วสามารถใช้งานได้เกือบทุก use case
วิธีพื้นฐานที่หลายท่านอาจจะทราบแล้วคือ ถ้า workload ของเรารันอยู่บน EC2 หรือ GCE เราจะสามารถกำหนด IAM Role/Service account ตอนที่สร้างเครื่องได้
ถ้าหากเราใช้ AWS SDK หรือ Google SDK ในการใช้ API ต่างๆ จากบนเครื่องนี้เราจะสามารถใช้สิทธิ์ของ Role ได้ทันทีโดยไม่จำเป็นต้องตั้งค่าใดๆ แน่นอนว่าจะได้สิทธิ์แค่ใน cloud เดียวกันเท่านั้น
เบื้องหลังแล้วนั้น SDK จะดึง credential ออกมาจาก instance metadata service (AWS, GCP) ที่ IP 169.254.169.254 ดังนั้นถ้าเรามีการรัน workload ที่อนุญาตให้ยิง HTTP request ไป URL ต่างๆ ได้อิสระ ได้ควรจะตั้ง firewall ภายในเครื่องไม่ให้ยิงไปที่ IP นี้ได้ด้วยเพื่อไม่ให้ขโมย access token ของเครื่องได้ (IP นี้อยู่บน Hypervisor ไม่สามารถตั้งบล็อกผ่าน security group หรือ VPC Network ACL ได้)
สำหรับ Software Engineer ที่ต้องการทดลองใช้ AWS API บนเครื่องตัวเองระหว่างการพัฒนานั้น ผมแนะนำให้องค์กรตั้งค่าให้เข้าถึงผ่าน AWS Organization แทนการใช้ IAM Account โดยพนักงานสามารถเข้าใช้งานผ่านโปรโตคอล SAML ได้ เช่น Login ผ่าน Google account แทนเพื่อให้ไม่ต้องบริหาร user account ใน AWS (แต่ยังต้องกำหนดสิทธิ์เองอยู่) หรือถ้าไม่มี SAML IDP ก็สร้าง account บน AWS Organization ได้เช่นกัน
ข้อดีของ AWS Organization คือสามารถอนุญาตให้เข้าถึง AWS Account หลายๆ บัญชีได้โดยไม่ต้องกำหนดรหัสผ่านหลายๆ ชุด, Integrate เข้ากับ AWS CLI v2 ทำให้เวลาพิมพ์ command line แล้วระบบจะขึ้นหน้า login ให้เลยโดยไม่ต้องใช้ 3rd party tool มาขอรหัสผ่านอีก, สามารถกำหนดให้ 1 คน มีหลาย role และเลือกใช้งาน role ได้เพื่อป้องกันข้อผิดพลาด
สำหรับโปรแกรมที่ไม่รองรับการใช้งาน AWS Organization นั้นเราสามารถกด Command line or programmatic access เพื่อขอ AWS Token ชั่วคราวได้ โดยมีอายุนานเท่ากับ session duration ที่ admin กำหนด (ปกติก็จะไม่เกิน 1 วัน)
สำหรับฝั่ง Google Cloud นั้นการรันบน local สามารถใช้คำสั่ง gcloud auth application-default login เพื่อ login ด้วย Google Account ได้เลย จากนั้นก็ใช้ Google SDK ได้เหมือน Login อยู่แล้วไม่จำเป็นต้องระบุ service account อีก โดยจะได้สิทธิ์เท่ากับที่ได้ใน console
การใช้งาน S3 หรือ GCS อีกรูปแบบที่น่าจะคุ้นเคยกันคือ Signed URL โดยแทนที่เราจะกำหนดให้ไฟล์ใน S3/GCS เป็น public เราจะใช้ AWS access token หรือ Google private key sign request ติดไว้ใน URL แทนเพื่อส่งให้ผู้รับ เมื่อผู้รับได้ URL แล้วก็สามารถเข้าถึงได้เสมือนเป็นมี access token นั้นโดยไม่จำเป็นต้องให้ access token และเมื่อ signature ใน URL หมดอายุก็จะไม่สามารถใช้งานได้
ฟีเจอร์นี้สามารถใช้งานได้ผ่าน AWS SDK หรือ Google Cloud SDK แต่ถ้าใช้ secretless access ไปยัง GCS ตามวิธีในบทความนี้ SDK บางภาษาอาจจะไม่รองรับเนื่องจากไม่มี private key ต้องไปยิง API ให้ sign ให้แทน ซึ่งเป็น API ใหม่
ฟีเจอร์หนึ่งที่อาจจะไม่ทราบกันคือนอกจาก Signed URL จะใช้ download ได้แล้วยังมี Signed URL สำหรับ upload อีกด้วยเพื่อให้ end user สามารถ upload ตรงจาก Browser เข้า S3 ได้โดยไม่ต้องกังวลเรื่อง web server ที่มาพักไฟล์
Signed URL สำหรับ upload นี้จะสามารถกำหนด policy ได้ดังนี้
ข้อจำกัดก็คือสามารถอัพโหลดไฟล์ทับได้ และไม่สามารถ validate เนื้อหาภายในก่อนวางไฟล์ได้ (อาจจะผูก bucket ให้ยิง event ไปที่ Lambda เพื่อเช็คและลบไฟล์ที่ไม่อนุญาตตามหลัง) ในตอนนี้ที่ LMWN จึงยังไม่มีการนำมาใช้เนื่องจากความยุ่งยากในการพัฒนาระบบและออกแบบความปลอดภัย
ท่าถัดมาจะเริ่มเข้าใจยากขึ้นเล็กน้อย แต่การใช้งานก็ไม่ยากเท่าไรนัก สำหรับผู้อ่านที่ใช้ Kubernetes อยู่ไม่ว่าจะเป็น EKS, GKE, kubeadm ไม่ว่าบน cloud หรือ on premise เราสามารถให้ Kubernetes pod มี AWS role ได้เลยโดยไม่จำเป็นต้องใส่ credential ใดๆ
หลักการทำงานก็คือ AWS มี Security token service (STS) ซึ่งเป็น API ที่เราสามารถใช้ OpenID Connect (OIDC) หรือ SAML เข้ามาแลกบัตรเป็น AWS Secret ชั่วคราวได้ โดย AWS Organization ที่พูดถึงก่อนหน้านี้ก็ใช้การแลก SAML ผ่าน AWS STS เช่นกัน
แล้ว Kubernetes จะเอาอะไรไปแลกเป็น Token?
สิ่งที่เราจะนำไปแลกเป็น token ก็คือ Service account token โดยแต่ละ service เราจะมี service account (ของ Kubernetes) แยกกัน ถ้าหากเราลอง exec เข้าไปดูใน pod และสั่ง cat /var/run/secrets/kubernetes.io/serviceaccount/token ออกมาดูจะเห็นว่าเป็นรูปแบบ JWT และ sign ด้วย RSA key
สำหรับวิธีการแลก token นั้นจะเป็นไปตาม OpenID Connect โดยสมมุติว่าเราได้ access token มาแล้ว (ก็คือ pod JWT) เราก็ส่ง access token นี้ไปให้ AWS STS แล้ว AWS STS จะไปดึงข้อมูล public key (JWKS) จากผู้ที่ออก token มาดู ถ้าเป็น JWT ที่ออกให้จริงก็จะได้สิทธิ์ไป
สำหรับ endpoint ที่ใช้ดึง JWKS นั้น ใน Kubernetes 1.21 เป็นต้นไปจะมี OpenID Connect Discovery endpoint สามารถตั้งค่า AWS ให้เข้ามาที่ http://kube-api/.well-known/openid-configuration และ token_endpoint ที่ระบุใน discovery endpoint ได้ทันที
แต่หลายๆ ท่านอาจจะสงสัยต่อว่า แล้วถ้ายังไม่ได้อัพเกรด Kubernetes ถึง 1.21 จะทำอย่างไร หรือ Kubernetes API จะเปิด public ก็อาจจะเป็นอันตราย ผมใช้วิธีตามที่ EKS แนะนำคือการสร้าง discovery document ขึ้นมาเองดังนี้
Note: EKS, GKE มีวิธีตั้งค่าที่ไม่จำเป็นต้องทำตามนี้ จะอยู่ในหัวข้อสุดท้าย
ขั้นแรกเราจำเป็นจะต้องสร้าง Discovery document ขึ้นมาก่อนดังนี้
ไฟล์นี้ให้วางแบบเข้าถึงได้เป็น public ที่ URL ตาม field issuer ใน JSON และต่อท้ายด้วย /.well-known/openid-configuration (เช่นถ้าใช้ตัวอย่างตามด้านบน เข้า URL https://www.example.com/k8s/.well-known/openid-configuration แล้วจะต้องได้เนื้อหาในไฟล์ออกมา)
วิธีง่ายที่สุดก็คงจะเป็นวางไว้บน S3 แล้วตั้งเป็น public ในส่วนของ issuer ให้ระบุเป็น https://bucketname.s3.amazonaws.com/ และทดสอบเข้าที่ https://bucketname.s3.amazonaws.com/.well-known/openid-configuration ต้องปรากฏข้อมูลตามด้านบน
ขั้นตอนถัดมาเราจะต้องแก้ไข Kubernetes API Server ให้ชี้ไปยัง URL ของไฟล์นี้ โดยแก้ไข command line argument ดังนี้ (วิธีแก้ไขอาจจะขึ้นอยู่กับ Kubernetes Distribution ที่ใช้งาน)
เนื่องจาก Kubernetes ไม่ได้ใช้ field issuer เป็นการภายใน การแก้ไขตรงนี้จึงไม่เป็น breaking change แต่อย่างใด
ไฟล์ที่สองที่จำเป็นต้องมีคือ JWKS ซึ่งถ้าใครใช้ Kubernetes รุ่นใหม่แล้วก็เข้า http://kubeapi/openid/v1/jwks ไปดาวน์โหลดมาได้เลย สำหรับการสร้างไฟล์นี้ด้วยตนเองให้ใช้โปรแกรมของ AWS โดยดาวน์โหลด source code มาแล้วสั่ง
อย่าลืมระบุพาธของ key ตามที่นำออกมาในหัวข้อที่แล้ว จะได้ไฟล์ keys.json ให้นำไปวางไว้ที่ URL ตาม jwks_uri ใน discovery document (เช่นเดียวกันจะวางไว้บน S3 ก็ได้)
ทั้งไฟล์ discovery document และ JWKS นี้ไม่มีข้อมูลที่เป็นความลับ (ไฟล์ keys มีเฉพาะ public key) จึงสามารถนำไปวางไว้เป็น public ได้โดยไม่ต้องกังวลเรื่อง security แต่อย่าลืมว่าถ้า URL นี้ล่มแล้ว pod จะ login เข้า AWS ไม่ได้ทั้งหมด ถ้าเลือกใช้ S3 ก็จะไม่ต้องกังวลเรื่องนี้ไปด้วย
สำหรับคนที่ใช้ GKE แล้ว GKE มี OpenID Connect Endpoint เป็น public API อยู่แล้ว โดย URL คือ https://container.googleapis.com/v1beta1/projects/<project name>/locations/<zone>/clusters/<cluster name>
เช่น ถ้า project ชื่อ example, zone asia-southeast1-a และ cluster ชื่อ test URL จะเป็น https://container.googleapis.com/v1beta1/projects/example/locations/asia-southeast1-a/clusters/test
สามารถทดสอบโดยเข้า URL https://container.googleapis.com/v1beta1/projects/example/locations/asia-southeast1-a/clusters/test/.well-known/openid-configuration ก็จะได้ discovery document และสามารถเข้าไปดู JWKS URL ได้ด้วย
ตามที่เขียนในหัวข้อที่แล้ว Discovery document และ JWKS นั้นไม่ได้เป็นข้อมูลความลับ API ทั้งสองตัวนี้บน Google Cloud จึงไม่ต้อง authenticate ก่อนดูข้อมูล
สำหรับ EKS URL สามารถดู OpenID Connect provider URL ได้ใน cluster detail
หลังจากเรามี Discovery document URL แล้วให้นำ URL นี้ไปลงทะเบียนบน AWS IAM
หลังจาก Pod สามารถ authenticate ได้แล้วเราจำเป็นต้องกำหนดสิทธิ์การใช้งานให้ pod ด้วย (authorization) โดยสร้างเป็น IAM Role ดังนี้
โดย url-to-oidc-provider เดิมจะมีอยู่แล้วจากที่เลือกในข้อ 3 ให้ระบุเหมือนกันทั้ง 2 จุด ส่วนใน field sub ให้ระบุ service account ที่สามารถใช้ role นี้ได้ เช่น system:serviceaccount:default:app คือ service account ชื่อ app บน namespace default (สามารถระบุหลายค่าใน array ได้)
(Condition ลักษณะนี้สามารถใช้ในการเขียน Policy ได้เช่นกัน ถึงแม้ว่า visual editor อาจจะบอกว่าเป็นเงื่อนไขที่ไม่ถูกต้องก็ตาม)
เมื่อเสร็จแล้วให้จด Role ARN ในหน้า Summary ไว้เพื่อใช้ในขั้นตอนถัดไป
ขั้นตอนสุดท้ายเราจะต้องบอกให้ AWS SDK รับรู้ว่าจะใช้ STS OIDC โดย AWS เพิ่งเพิ่ม support เข้ามา application ที่ใช้จะต้องใช้ AWS SDK ใหม่กว่ารุ่นที่กำหนดเท่านั้น โดยทั่วไปก็คือเวอร์ชั่นหลังปี 2019 เป็นต้นไป
วิธีการตั้งค่ามี 2 แบบ คือใช้ EKS Pod Identity Webhook (ซึ่งสามารถลงนอก EKS ได้ ที่ LMWN เราก็ลงไว้ใน GKE) หรือ inject ด้วยมือก็ได้เช่นกัน
เพื่อให้เข้าใจการทำงานของ Webhook จึงขออธิบายการ inject ด้วยมือก่อน
Pod ที่ต้องการใช้ AWS API นั้นจะต้องมี Kubernetes service account token ที่ระบุ audience เป็น sts.amazonaws.com โดยวิธีการคือให้แก้ไข Deployment ต่อไปนี้
จะสังเกตว่ามีการกำหนด expirationSeconds ไว้ด้วย ทำให้ JWT ใน volume นี้ไม่ได้มีอายุไม่จำกัดเหมือนกับ service account token ปกติ
2. ระบุ volumeMount ใน container spec เพื่อเอา volume เข้าไปใน pod
ตรงนี้ถ้าเคยใช้งาน volume อยู่แล้วก็น่าจะคุ้นเคย ตัว token ก็จะอยู่ที่ /var/run/secrets/eks.amazonaws.com/serviceaccount/token
3. ระบุ environment variable ต่อไปนี้
เมื่อ SDK เห็น environment variable ตามนี้แล้วก็จะเอา token ใน path ที่ระบุไปเรียก STS AssumeRoleWithWebIdentity ให้เป็นอันเรียบร้อย ไม่จำเป็นต้องเพิ่มโค้ดใดๆ
สำหรับการติดตั้ง webhook นั้น บน EKS จะมีให้อยู่แล้ว ส่วน Kubernetes อื่นๆ ให้ใช้คำสั่ง make cluster-up ใน EKS Pod Identity Webhook ได้เลย หรือหากจะปรับแต่งก็สามารถศึกษาได้จาก folder deploy ภายใน Git repository
เมื่อติดตั้งเรียบร้อยแล้ว บน service account ให้ระบุ role ที่จะ assume ดังนี้
Pod ใดที่มีการใช้งาน service account นี้มันก็จะใส่ volume, environment variable เพื่อให้ assume role โดยอัตโนมัติ
ที่ LMWN เองเราก็ไม่ได้ใช้ EKS แต่ว่าเราใช้เทคนิคนี้ทำให้ทั้ง Kops และ GKE สามารถใช้สิทธิ์บน AWS ได้โดยไม่จำเป็นต้องเก็บ secret เลย
ข้อควรระวังอย่างหนึ่งคือ workload บน EC2 ยังมี instance role ของ node อยู่ตามที่กล่าวไว้ในหัวข้อก่อนหน้านี้ ควรจะ firewall ไม่ให้ workload สามารถเข้าถึง metadata service (169.254.169.254) ได้เพื่อไม่ให้ใช้ node role ได้ แต่ต้องระวังให้ boot script หรือ kubelet ยังสามารถใช้งานได้อยู่
สำหรับ workload ที่รันบน GKE และต้องการใช้ Google service account นั้น GKE จะมีระบบ workload identity ที่ใช้งานได้เทียบเท่ากับของ AWS แต่ใช้วิธีดักการเชื่อมต่อไปยัง instance metadata service แล้วคืน service account ของ pod ออกไปแทน สามารถเปิดได้ในการตั้งค่า GKE
ส่วน Kubernetes workload ที่รันอยู่บน AWS หรือ on premise แล้ว เราสามารถใช้ OpenID Connect ได้เช่นเดียวกันกับบน AWS โดยบน Google Cloud จะเรียกว่า Workload Identity Federation
สำหรับการสร้าง OIDC Discovery Document สำหรับ Google Cloud นั้นจะเหมือนกับการใช้ OIDC บน AWS ในหัวข้อก่อนหน้า ถ้าเคย setup ไว้สำหรับ AWS อยู่แล้วก็สามารถใช้กับ Google Cloud ได้โดยไม่ต้องแก้ไขเพิ่มเติมใดๆ
เมื่อตั้งค่าเสร็จแล้วจะได้ Discovery URL (URL ในช่อง Issuer ของ Discovery JSON)
ขั้นตอนถัดมาเราจะลงทะเบียน OIDC เข้ากับ Google Cloud
ในขั้นตอนนี้ให้จด IAM Principal ไว้ด้วย หน้าตาประมาณนี้ principalSet://iam.googleapis.com/projects/<project id>/locations/global/workloadIdentityPools/<pool name>/*
การกำหนดสิทธิ์ว่า pod ใดจะทำอะไรได้บ้างนั้นจะใช้วิธีกำหนดว่า pod นี้ (หรือก็คือ Kubernetes service account ที่กำหนด) สวมสิทธิ์เป็น Google service account อะไร และ service account นั้นทำอะไรได้บ้าง
เท่านี้ก็สามารถใช้ OIDC แลกเป็น service account นี้ได้แล้ว
สำหรับการใช้งานในโปรแกรมนั้นให้กลับไปที่หน้าตั้งค่า Workload Identity Pool ด้านขวาจะมี tab Connected Service Accounts ซึ่งจะปรากฏรายชื่อ service account ก่อนหน้านี้
ให้กด Download config ออกมา ระบบจะถามข้อมูลดังนี้
จะได้ไฟล์ JSON ออกมา ซึ่งไฟล์นี้ใช้งานเหมือน Google Service Account Key ที่เคยใช้งานเดิม (วางไฟล์ไว้แล้วระบุพาธใน GOOGLE_APPLICATION_CREDENTIALS environment variable) แต่ข้างในไฟล์จะระบุเพียงแค่ว่าใช้ Identity pool ใดและเอา OIDC ID token มาจากที่ใด จึงไม่ใช่ secret
(Note: เราเจอบั๊กใน Java SDK เมื่อใช้ format type เป็น text มันจะ validate แบบ JSON ทำให้ใช้งานไม่ได้ workaround คือเพิ่ม subject_token_field_name เข้าไปใน credential_source.format ของไฟล์ JSON ที่ได้มา)
สำหรับที่ LMWN เราใช้ Jsonnet สร้างไฟล์นี้ขึ้นมาอัตโนมัติ แล้ววางเป็น ConfigMap จากนั้นก็ใช้ volumeMount เข้าไปใน pod พร้อมกำหนด GOOGLE_APPLICATION_CREDENTIALS
ในการวางไฟล์เข้าไปใน Pod เราจำเป็นจะต้องวางเองเนื่องจาก GCP ไม่มี pod identity webhook แบบ AWS ดังนี้
2. กำหนด Volume ใน deployment
โดยค่าใน field audience นั้นสามารถดูได้จากใน JSON ที่ download มา
3. กำหนด volume mount
4. ระบุ GOOGLE_APPLICATION_CREDENTIALS=/var/run/secrets/sts.googleapis.com/config/serviceaccount.json
เมื่อครบถ้วนแล้ว application ก็จะมีสิทธิ์ใช้งาน Google API เทียบเท่า service account ทันทีโดยไม่จำเป็นต้องเก็บ secret อีก
ฟีเจอร์นี้เพิ่งออกในปีที่ผ่านมา ดังนั้น application จะต้องอัพเกรด Google SDK เป็นเวอร์ชั่นตั้งแต่กลางปี 2021 เป็นต้นไปด้วย
ฟีเจอร์ OpenID Connect นี้ผมคิดว่าทำให้ Cloud API ต่างๆ คุยข้ามกันได้เพียงแค่มี JWT ที่ sign ด้วย certificate เท่านั้น นอกเหนือจาก Kubernetes ที่เล่าไปก่อนหน้านี้แล้ว เร็วๆ นี้ GitHub Action ก็รองรับการ assume role จาก CI ไปยัง AWS, GCP ด้วย OIDC เช่นกัน
อีกฟีเจอร์หนึ่งที่พูดถึงในบล็อกนี้คือ Service account token projection ของ Kubernetes ที่ทำให้ Pod มี JWT ที่มีอายุจำกัด ผมมองว่าเป็นตัวเลือกที่ดีสำหรับการทำ service to service authentication ภายในแทนที่จะใช้ internal API key หรือ implicit trust ซึ่งไม่สามารถระบุตัวตน service ที่ยิงได้นอกจากอนุมานเอาว่ายิงเส้นนี้มีแต่ service นี้ที่ใช้ ซึ่งในปีนี้เราอาจจะลองเปลี่ยน internal authentication ให้มาใช้วิธีนี้มากขึ้น อาจจะไม่ดีเท่า mTLS แต่ก็มี overhead น้อยกว่ามากและทำงานด้วยได้ง่ายกว่าเยอะ
สำหรับ GitLab ที่เราใช้นั้นปัจจุบันยังไม่รองรับเนื่องจาก Job token ไม่มี audience และ Issuer URL นั้นระบุเป็น GitLab URL ซึ่งเป็น private IP ใน community เองก็มีการพูดถึงเหมือนกัน ก็หวังว่าภายในปีนี้จะออกมาให้ใช้งานกัน ระหว่างนี้ก็คงจะต้องใช้ Instance role สำหรับ builder machine แก้ขัดไปก่อน หรืออาจจะ authenticate ไปหา Hashicorp Vault ที่เก็บ AWS Secret อยู่เพราะสามารถใช้ OIDC ไปยัง Vault ได้แล้ว
Life@LINE MAN Wongnai - Stories from our own people
23 
23 claps
23 
Written by
Software Architect at Wongnai, also blog at blog.whs.in.th
Life@LINE MAN Wongnai - Stories from our own people
Written by
Software Architect at Wongnai, also blog at blog.whs.in.th
Life@LINE MAN Wongnai - Stories from our own people
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/deployment-models-cloud-computing-b17fe1e58d44?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giorgos Myrianthous
Nov 5, 2021·3 min read
The term Cloud Computing refers to the on-demand access to a set of resources such as storage, applications, compute power and other IT services. Cloud users can have instant access to the provisioned services that are scaled based on the…
"
https://medium.com/criteo-engineering/cloud-native-2019-took-place-in-london-and-gather-almost-200-hundred-cloud-practitioners-cc203965ea02?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
Cloud Native 2019 took place in London and gather almost 200 hundred cloud practitioners. It has been an opportunity to exchange point of view on various topics, to extensively discuss testing in this context, and to share feedback on such technologies.
This question was the topic of the opening keynote. It was also a question we have been asked many times at our booth. The general consensus was that nowadays it makes more sense to use a public cloud (AWS, Google Cloud Platform, … ). There are some good reasons to be reluctant to use a public cloud (like the collection of sensitive data). But there are also several injustified reasons. For instance, several speakers mentioned the fear be locked-in within a cloud provider and then to experience a price increase. They dismissed this argument based on price evolution data from the last decade. Yet, this data shows that the situation never occured.. And there are several good reasons to use one. In particular not having to maintain a stack of technos not directly related to one’s business -and the subsequent security risks- as well as the velocity it brings to be able to benefit from an actual prod-proof cloud.
We had several questions on that matter since we’re managing our own private cloud:roughly 50.000 servers and several dozens SRE engineers to maintain it. Actually Criteo is a bit older than AWS so when we started, developing our private cloud was the only option. We started with bare metal and, as we grew, developed expertise on developing and maintaining our private cloud. Now, considering our scale, tooling and expertise, having reached a high level of optimization, it turns out to be cheaper and more efficient to manage our own infrastructure.. Yet we are open to other approaches, and we also use a public cloud for some use cases that make no sense to internalize, for example this is the case of some specific tests and caches.
At Criteo, performance is everything. It implies working smart with the best infra we can afford. It is sometimes public cloud, but as of today we’re mainly focused on our own private cloud.
Interestingly several talks broached this topic at some point. The cloud brings agility but if the testing strategy doesn’t evolve it can hinder this benefit. Also, with microservices new difficulties arise. For instance, we don’t want developers to have to spawn dozens of services on their own computers just to be able to run the regression tests locally.Several aspects were discussed, in particular:- consumer-driver contract tests, to be able to test a service without having to spawn the whole infra-structure- testing in prod by first deploying on a trafic-less prod server -after all the usual regression tests in pre-production have been done- in order to be able to test the integration with the other services in conditions… as close to prod as possible! A little regret on that though: the duration of this talk was a bit short and we didn’t have much time to talk about handling side effects in this context.- testing issues we could never have imagined could occur in prod, thanks to Disaster Recovery Testing games, chaos tests, and a solid monitoring
My team, Test Services is in the process of redesigning the way we’re doing end to end tests at Criteo. Historically we developed sandboxes: an isolated environment used to run tests in our build pipeline. Those environments replicate a lightweight Criteo datacenter on a few servers. It was quite useful back in the days when we had only a few services but it turned out to be more and more complex. And more and more painful to maintain. We’ve hence been in the past quarters, in a process to re-thinking the way we’re tackling this. Those talks have hence been an opportunity to step back on what we’re doing -and to give us new ideas to go one step farther.
One of our Criteo’s was also doing a Lightning Talk: “Discovery, Consul and Inversion of Control for the infrastructure” (slides are available here). During this talk, he explained how infrastructure is evolving every day faster and how mixing several kinds of architectures (real datacenter, containers and various Cloud providers) is hard to handle to an infrastructure point of view since everything is moving faster and faster.He also explained how Criteo implemented a new pattern of architecture for infrastructure called inversion of Control. Which such pattern, infrastructure becomes a live database of all workloads and by enriching the running services, so it becomes possible to decouple all tools, innovate faster and create additional value very easily to match business needs.
Criteo is using this pattern to perform all of its load-balancing, generated automatically alerts, track versions at large scale and with vendor-independent schedulers, meaning hybrid cloud/containers/legacy systems.
Being able to structure your data, bring semantics to infrastructure is probably one of the big challenges of next few years in the era of microservices and Criteo is pioneering it with interesting success.
Long story short: Cloud Native 2019 was a quite interesting place to be.See you next year!
Authors: Guillaume Turri, Pierre Souchay and Clément Boone
Tech stories from the R&D team
4 
4 claps
4 
Tech stories from the R&D team
Written by
The engineers building the advertising platform for the open Internet.
Tech stories from the R&D team
"
https://medium.com/@alibaba-cloud/top-5-security-considerations-for-cloud-deployments-d66b8c401bf6?source=search_post---------213,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 24, 2018·4 min read
As cloud technologies become more mature, we are seeing a rapidly increasing number of organizations adopting a public or hybrid cloud strategy in their IT infrastructure. While cloud technologies offer virtually infinite computing capabilities, many organizations are still wary of the security limitations of public cloud environments. But, is this concern merely a speculation or based on actual facts? Perhaps a better question would be, “What kind of security capabilities does Alibaba Cloud offer?”
In this article, we will address these concerns by discussing the top 5 security considerations you should look out for in a cloud provider. The Alibaba Cloud Security team has also written a detailed security whitepaper, covering all your security concerns of deploying on Alibaba Cloud.
The security features of Alibaba Cloud are built on top of an 11-layer security architecture dubbed the “Security Compass”. The Alibaba Cloud Security Compass consists of four layers oriented at cloud platform and seven layers oriented at cloud users. By abstracting cloud platform and user security, Alibaba Cloud allows users to focus on security at the implementation level, including account, VM, application, network, data, operation, and business security. Please note that there are several items being greyed out in the diagram below. The greyed-out items are the ones currently only available in Mainland China, though they would be made available globally in the future.
Obviously, one of the main considerations when it comes to ensuring security on the cloud is to make sure the products that you use are robust. This means that even without any additional security products and services, the cloud product of your choice must be able to withstand a variety of cyber-attacks.
Alibaba Cloud Elastic Compute Service (ECS) instances can achieve this by employing features such as tenant isolation. Tenant isolation is achieved by providing isolation between the virtual machine management (VMM) system and the customer’s VM, and isolation between customer’s VMs. On Alibaba Cloud, ECS instances that are assigned to different users are isolated, providing the needed security barriers among tenants.
Other security features of Alibaba Cloud products include security group firewalls, whitelisting, resource access management, and anti-IP/MAC/ARP spoofing.
Despite having all these security features, your IT infrastructure is still not completely immune to cyber-attacks. If you want to keep your data and applications secure, you should definitely consider investing in cyber security products offered by your cloud provider.
Alibaba Cloud suite of security products are built on Alibaba Group’s security technologies and experiences over the years. From its free services such as Server Guard Basic and Anti-DDoS Basic versions to value-added ones such as Anti-DDoS Pro, Alibaba Cloud provides a one-stop security solution for all types of businesses.
For users to operate freely across the globe, cloud providers need to adhere to domestic and international information security standards, as well as industry requirements. Cloud providers should integrate compliance requirements and standards into internal control frameworks, and implement such standards by design in their cloud platform and products.
Alibaba Cloud is actively involved in the development of multiple standards for the cloud industry, and is also engaged with multiple independent third parties to verify its compliance. Alibaba Cloud is certified by more than 10 agencies across the globe, and is a cloud service provider with the most complete range of certifications in Asia.
Visit the Alibaba Cloud Security and Compliance Center to see our list of certifications and compliance credentials.
Alibaba Cloud and its customers are jointly responsible for the security of customers’ applications built on Alibaba Cloud. On Alibaba Cloud, customers are responsible only for the security of applications built on top of or connected to the cloud. Alibaba Cloud, on the other hand, is responsible for the security of the underlying cloud service platform and infrastructure. By maintaining this model, customers are able to leverage the underlying security assurance and capabilities that Alibaba Cloud provides. Additionally, customers can rest assured that they have full control over the security of their data.
The Alibaba Cloud Security Whitepaper provides a comprehensive guideline and description of the Alibaba Cloud’s security architecture, which constructs the security foundation of the entire Alibaba Cloud platform.
Additionally, the Security Whitepaper includes detailed descriptions of key security features and capabilities of Alibaba Cloud products. Moreover, best practices are provided within the whitepaper for a number of products to help customers to use Alibaba Cloud more securely.
Data security and user privacy are the top priorities of Alibaba Cloud. Alibaba Cloud strives to provide customers with consistent, reliable, secure, and regulation-compliant cloud computing services, helping customers ensure the availability, confidentiality, and integrity of their systems and data.
Download the full whitepaper by visiting https://www.alibabacloud.com/help/faq-detail/42435.htm
Reference:
https://www.alibabacloud.com/blog/top-5-security-considerations-for-cloud-deployments_593694?spm=a2c41.11567482.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/what-is-hybrid-cloud-hosting-4dda16e8b9a4?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Mar 24, 2015·1 min read
Hybrid cloud hosting is defined in many ways: The combination of public and private cloud, utilizing on-premises and outsourced infrastructures, IaaS mixed with any other type of infrastructure, and the list goes on and on. We define hybrid cloud hosting as any combination of public cloud, private cloud, dedicated servers, and colocation all with a single VLAN, firewall, management portal, and support team — with the ability to incorporate an on-premises environments. Hybrid cloud hosting services came about because companies with a complex infrastructure want the benefits of cloud but require multiple IT platforms due to legacy applications, special IT attributes, heavy traffic spikes, and / or regulatory mandates.
Why would you use hybrid cloud hosting services? The main benefits are:
Cost Savings — A significant cost savings can be achieved by utilizing the public cloud for non-sensitive applications and private cloud for sensitive operations.Increased Agility — You are able to move from one platform to another seamlessly as the needs of your organization change.Maximize Efficiency — Using the best platform for each application or use case enables you to realize efficiency in your infrastructure.So, what do you put where? Here are some rules for deploying a complex architecture to a hybrid cloud hosting model:
Non-intel hardware goes in coloPerformance-critical workload goes onto a dedicated server clusterSecurity or compliance-sensitive workload goes into private cloudEverything else goes into public cloudAdd appropriate levels of Availability, Recovery, Security, Compliance, and Application Services
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/@experoinc/escape-your-on-premise-prison-and-decrease-costs-cb4d4609581d?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Expero
Aug 29, 2018·2 min read
Trying to modernize monolithic legacy applications is hard. Yet, moving to a public or private cloud can dramatically reduce operational costs and speed up delivery of key features. But how do you get there from here? In this seminar we’ll jump into pragmatic ways to improve the way legacy applications are built and deployed as well as how to take an incremental approach to modernize those applications.
What You’ll Learn:
Migrating a monolith to the cloud safely:
Utilizing SAAS products to replace legacy components:
Experts in complex solutions, custom software, cloud-based architecture, & scalable UX across all industries. Solving a problem? — info@experoinc.com 5123281212
Experts in complex solutions, custom software, cloud-based architecture, & scalable UX across all industries. Solving a problem? — info@experoinc.com 5123281212
"
https://medium.com/@krmarko/public-or-private-multi-cloud-is-the-future-how-will-you-manage-it-794d1714b98b?source=search_post---------217,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Nov 6, 2015·2 min read
Any conversation about cloud services usually begins with AWS, but for most organizations, it won’t end there. Whether to fight vendor lock-in, increase the diversity of available services, arbitrage price disparities or maintain control over particularly sensitive information an increasing number are adopting multi-cloud strategies that include both public and private components. As I detail here, although it’s a sound strategy, they quickly run into another problem: managing applications and infrastructure configurations across cloud stacks that don’t share a common API and have very different service definitions and billing models. It’s a seemingly complex task, but hardly a showstopper, with a number of mature software and SaaS options available to automate deployments across a variety of cloud stacks. Yet all the automation tools rely on a common conceptual framework: treating cloud resources as abstract objects that can be configured, run and managed as software code. Hence, the overlap with DevOps methodologies and organizational models.
Read the full column for plenty of statistics about why a multi-cloud future is inevitable for most enterprises and what they need to turn the vision into operational reality. Indeed, there are dozens of software and SaaS products designed to automate infrastructure and application management across multiple clouds. Some focus on specific needs or usage scenarios. For example, Cloudyn is designed for asset and cost management and includes a workload optimizer to identify the most efficient cost-performance deployment option for a particular workload, while CSC, using the former ServiceMesh product focus on cloud governance, security and lifecycle management. Others, like Cliqr, Cloudify and ElasticBox take an application-centric approach to cloud automation.
Yet the most popular multi-cloud products are generally those used by organizations embracing a DevOps approach to cloud management, a tact that extends application programming into the realm of infrastructure configuration and management. The article walks through the most popular options. Any will work on both private infrastructure and across all the major public clouds, however the integration details will vary widely. The choice of product should be dictated by the sophistication and scale of one’s infrastructure and expertise of the IT/DevOps team. I conclude with some other recommendations.
Originally published at markoinsights.com on November 6, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
See all (132)
1
1
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pingcap/tidb-cloud-now-in-public-preview-78171002a2a8?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
May 11, 2021·3 min read
We are excited to announce the public preview of TiDB Cloud, the fully-managed TiDB service by PingCAP. With TiDB Cloud, customers can now easily use TiDB on Amazon Web Services and Google Cloud to quickly build modern, mission-critical applications.
TiDB is the leading open-source, MySQL compatible, distributed NewSQL database in the industry that supports hybrid transactional and analytical processing (HTAP), which allows business to run real-time analytical queries. It also features horizontal scalability, strong consistency, and high availability.
TiDB has been growing for the past 6 years, and is now deployed in production by over 1500 companies worldwide. Open source since day one, TiDB has donated two projects to the Cloud Native Computing Foundation (CNCF) and currently has over 400 contributors to its GitHub repository.
“The mission of PingCAP is to make developing data-intensive applications easy at any scale,” said Li Shen, VP of PingCAP.
TiDB Cloud goes a step further by making deployment, management and maintenance even easier through an online interface. Now, after 6 months in private beta, it is available for public preview.
TiDB Cloud makes deploying, managing, and maintaining your TiDB clusters even simpler with a fully managed cloud instance that you control through an intuitive dashboard. You’ll be able to easily deploy on Amazon Web Services or Google Cloud to quickly build mission-critical applications.
By scaling TiDB clusters with a click of a button, you’ll be able to provision your databases for exactly how much and how long you need them — no longer wasting expensive resources. TiDB Cloud allows Developer, DevOps and DBA teams with little or no training to handle once-complex tasks such as infrastructure management and cluster deployment.
TiDB Cloud also simplifies deployment, scaling, and recovering from outages through its cloud-native, distributed architecture that eliminates the need for manual sharding and complex failover schemes. Overall, this frees up your time to focus on making better applications for your customers.
Ready to give TiDB Cloud a try? Click here and choose from one of the public preview options:
Learn more about TiDB Cloud with our Quick Start Guide and TiDB Cloud blog post. Make sure to follow us on Twitter to stay updated on TiDB news!
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
"
https://medium.com/@alibaba-cloud/public-private-and-hybrid-cloud-on-alibaba-cloud-46f7ec13a896?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 7, 2020·6 min read
By Shantanu Kaushik
Cloud computing is a resource that is made possible with the advancements in networking and compute technology in the past decade. Cloud computing delivers superior and more reliable computing power by provisioning resources like servers, database, storage, applications, and infrastructure for specific tasks and practice.
One of the most prominent features of cloud computing is its scalability, helping businesses churn out more computing power as and when needed. Alibaba Cloud provides such capability to customers through the Elastic Compute Service (ECS). ECS lets you provision more or reduce resources based on real-time demand as presented. Features like Auto scaling and Server Load Balancer (SLB) helps businesses automate this process, and lets them focus on system design while keeping their applications at a higher level of availability.
Cloud Backup and Recovery is another popular feature that businesses take advantage of when using cloud-based solutions. Creating data backups is a key step for businesses to preserve the integrity of customer and business data. Alibaba Cloud offers virtually unlimited data storage capability through its Object Storage Service (OSS) and provides an industry leading solution for backup and disaster recovery practices.
Perhaps the biggest advantage of using a cloud-based solution is that there is no need for physical resource allocation, helping businesses be more agile and keep operating costs at a minimum. In this blog, we’ll be talking about the differences between public, private, and hybrid cloud, and discuss how you can implement them with Alibaba Cloud.
A private cloud follows a model that the name itself suggests. It is a private resource made available to a single organization or a single controlled domain administrator. It is not open to public use and is generally the most secure setup for any business or organization. Type of an enterprise cloud, a private cloud is completely controlled by the organization it belongs to.
The simplest form of a private cloud is an on-premises private cloud. Although this is among the most secure solutions, a private cloud setup requires the provisioning of physical resources, which can lead to increased costs and complex management and maintenance cycles. The only big benefit of this setup is greater control of your servers.
In fact, you can simulate a private cloud environment even when using a public cloud, such as using Alibaba Cloud Virtual Private Cloud (VPC) service. A VPC allows the user to group different servers to form a network that works as a private resource. Allowing internet usage by such VPC grouped servers can be handled by the administrator.
When following virtualization practices, creating a private cloud on a public cloud can provide a highly-secure environment. The provisioned resources are Virtual Machines (VMs) that have their own allocated resources. These resources can be isolated with each other to form a more secure and controlled environment.
A Public cloud is an infrastructure resource that is based on a virtual environment. This virtual resource is accessible via the internet and provides for many online services. These services include data storage, data processing, and many other cloud-based services. So, it is considered a resource available to the public.
As this infrastructure is based on a virtual environment, there is no need for the user to purchase and install any physical servers to get connected. All one needs to do is to purchase an online resource and start working. This makes up for a highly cost-effective service when compared to the traditional model.
Operations and Maintenance (O&M) is a common concern when it comes to managing resources in a private cloud. But on a public cloud, the user is free from any O&M because the physical side of the service has nothing to do with the user. In addition, public clouds typically support a pay-as-you-go pricing model, which lets you pay only for what you use.
As public cloud is an online resource that is accessible to everyone, access control can be used to create different access level to virtualized resources. User account privileges are controlled and the implementation allows the administrator of the public cloud account to grant or deny access to the data. Different levels of permission can be granted or withdrawn by the administrator. However, the complete architecture is not as secure as a private cloud, for obvious reasons.
Public Cloud is the virtual image of the cloud provider that you are leasing the services from. In case of Alibaba Cloud, these cloud services are highly secure and scalable. When it comes to data and resource vulnerabilities, public cloud infrastructure is a secure resource, however, configuration, data management, authentication and encryption are handled by the user or administrator.
When using an IT infrastructure, there are numerous benefits that one can yield by using a public cloud. Public cloud is most beneficial for individuals and smaller organizations as they need not worry about management and maintenance dependencies.
Hybrid cloud is a mixture of both, giving you the benefits from public and private clouds. It addresses the expectations and challenges of Public and Private cloud, and offers solutions accordingly. It provides a certain level of flexibility that public and private cloud platforms lacked.
Hybrid cloud architecture provides workload portability and management across public and private cloud. With the introduction of technological innovation such as Artificial Intelligence (AI) and Internet of Things (IoT), a desperate need of intelligent cloud solutions is on a rise. These solutions can be made possible by using a Hybrid Cloud architecture that delivers a dynamic solution.
A typical Hybrid Cloud architecture includes:
Alibaba Cloud has been the front runner of evolution of designing and upgrading products and services for the cloud, providing organizations with industry-leading tools to tackle even the most complex challenges. The DevOps practices, the Hybrid Cloud solution among many other products like the Web+ service or the API Gateway have gone ahead to set standards that help achieve the most complex of goals for businesses and organizations.
While working with core-competencies, all the products that I used by the Alibaba Cloud platform have worked in-sync with each other and presents a far greater usability scenario for me. Multi-region-support for services, and solution driven design has had a far greater influence on me as a user to further trust the products and services by Alibaba Cloud.
The views expressed herein are for reference only and don’t necessarily represent the official views of Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@jystewart/we-are-renewing-our-cloud-first-commitment-c3f2eaa0af05?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Stewart
Sep 13, 2016·5 min read
We are renewing our Cloud First commitment
A few weeks ago I spoke at an event organised by Oracle focussed on how the public sector uses the cloud. I talked about why we are committed to the cloud, how this commitment has changed how we think about technology, and what we plan to do next. I spoke alongside my Government Digital Service (GDS) colleague Iain Patterson, who updated attendees on the work of Common Technology Services (CTS).
In 2013, the Cloud First policy set out government plans to focus technology commitments on where it can most add value. This put an end to the government running its own data centres, and sometimes running its own software.
The policy (part of the overall Technology Code of Practice) reflected a change of thinking. For a range of crucial components, we can now clearly think about what we need from technology and consume them as products and commodities, rather than build or buy the entire stack as custom pieces.
We can now work flexibly, separating services so they can be defined in ways even computers understand (APIs). We use resources elastically, summoning them when we need them, and paying for them on a utility basis (based on the number of users we have or the amount of processing we’re doing). Cloud First means less of our time is focussed on procurement negotiation and complex licensing, and more on making our tools robust and responsive.
The cloud reduces the time and cost to get something up and running, making change easier and empowering our people to experiment with new tools and approaches.
Change is constant for us, whether it’s changing user needs, a growing understanding of how to meet them, or the wider landscape of changing challenges and opportunities. It’s our responsibility as technologists to minimise the cost of change so that our teams can continually iterate services.
Too often we use the wrong labels and concepts when discussing the technology that powers our services. In thinking about what we want from cloud we need to do better.
Software engineers are trained to talk about very low-level building blocks at the library or microservice level. Meanwhile it’s still too common to find discussions taking place around around large, monolithic concepts like enterprise resource planning (ERP) and case management. This dichotomy in terms reflects the problems with the old fashioned business process design approach.
Even though there’s inherent confusion surrounding software labels in this traditional approach to procurement, the products have to be bought anyway. Complicated compromises have to be made between the functionality of large pieces of software, the cost of customising it, and how to effectively meet user needs. We are locked into a vendor and have to spend a lot of money on configuration.
New practices let us think more flexibly. Being clear about how our systems break down into components let’s us understand how we can use utility and commodity elements and where we should be innovating ourselves. API-enabled software as a service let’s us blend off the shelf components into broader systems using common standards, with a clear understanding of where canonical data lives.
We can make smarter use of what the market offers, and develop the talent, understanding and experience in our own teams. We also have a far richer toolbox than we’ve ever had before to respond to rapidly changing user expectations.
Over the next few months we’ll refresh the Cloud First policy and do more to share existing good practice. For example, we’ll work with our colleagues in security and across government to improve our security advice and do more to make sure that the changes are clearly communicated to the right people.
The Cloud Security Principles were a big step, but it’s clear that worked examples will make them more valuable. We’ve still got lots of myths to bust and best practice to share, to prove that most of the time public cloud is at least as secure as your own hosting.
We’ll also be updating and reviewing the range of tools that we can promote to help departments make the transition. Crown Hosting has helped many departments reduce their datacentre commitments and get a better understanding of what they have and will remain crucial, but not every journey to the cloud is the same and there are a huge range of tools and techniques out there to help.
We’re refreshing our approach to Open Standards, making sure we can identify, select and contribute to standards even more flexibly than before. We need to be clearer with the market about what open standards we expect to see, and share our needs with the groups developing new standards.
We’ll be providing more clarity for civil servants and the market about what we think good looks like for different types of cloud use. There’s huge variation in the maturity of Software as a Service applications and often that’s good. It’s evidence of a fast moving, innovative marketplace. But it’s important we’re clear about what we need from the market so we can have the right interfaces, the right security, and simplicity for users.
We’re bringing colleagues from across government together to look at some of our core business processes and the tools that support them. Over the past few years there’s been a lot of smart thinking, but it’s time for much sharper language and thinking about how we label things, what common components can make things more efficient and so on.
At GDS this involves working together across service design, Digital Marketplace, Common Technology Services and Government as a Platform (GaaP).
The changes in the technology market over the past few years are incredibly exciting. We’re aiming to ensure that the work we do in this area is helpful not just to accelerate our work in government but also to contribute back to the growing field of knowledge about how best to use the tools we now have at our disposal and what they can help us achieve.
Fundamentally, we’re talking about Cloud because we believe that the change it represents is a huge opportunity to put more focus on meeting users’ needs, more rapidly, more flexibly and more effectively.
We would like to hear about any obstacles you have encountered in making the transition to the cloud. Your comments will help us develop our work. Contact the Technology Group or comment below.
Originally published at governmenttechnology.blog.gov.uk on September 13, 2016.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
2 
2 
2 
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
"
https://medium.com/@alibaba-cloud/networking-tuesday-4-nat-gateway-overview-fbc6069fb021?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 15, 2018·2 min read
In our fourth edition of Networking Tuesday we introduce NAT Gateway, Alibaba Cloud’s enterprise-class public network gateway product.
About Networking Tuesday:In a series of eight infographics we will be introducing our suite of Networking products. Each Tuesday we publish an infographic introducing one Networking product, this provides you with a comprehensive overview of its features, benefits and use-cases. Amongst others we will cover EIP (Elastic IP), SLB (Server Load Balancer), VPN Gateway and Express Connect.
For more information on the Alibaba Cloud NAT Gateway product visit our product page.
Previous editions:Networking Tuesday #1: Introducing the Network Product FamilyNetworking Tuesday #2: Introducing Elastic IPNetworking Tuesday #3: What is Express Connect?
In next week’s edition of Networking Tuesday, we will cover our Server Load Balancer (SLB) product. Stay tuned!
Reference:
https://www.alibabacloud.com/blog/networking-tuesday-4---nat-gateway-overview_591114?spm=a2c41.11529660.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
5 
5 claps
5 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.baqend.com/baqend-cloud-now-publicly-available-for-your-applications-759dec35c751?source=search_post---------222,"We are happy to announce the public launch of Baqend Cloud Beta. You can now start using a fast and scalable cloud backend for your apps and websites in just a few seconds. Our basic plan including 100K requests per month is completely free and not limited in time.
Baqend tries to make two things faster: the development process through rich standard functionalities and the resulting application through caching. The core features currently available are:
To create a new app, first register for the dashboard via email, Facebook, Github or Google. Enter a name for your first application which will be the endpoint for your API — in this case my-test-app.app.baqend.com.
The app is now deploying in the cluster while the CDN is configured to accelerate data access. This typically takes 30 seconds or less.
As your plan, choose basic — it’s free and you won’t need a credit card.
The app dashboard opens showing the quickstart. It will walk you through the steps of creating a simple message wall website on your new Baqend instance.
You’re now equipped to start hacking together your first Baqend-based application.
To learn more:
As of now, Baqend Cloud is particularly suited for data-driven or latency-sensitive websites as well as hybrid apps, e.g. based on Ionic. Acto is an example of such an app already running on Baqend Cloud in production.
Within the next three month, we will make three important features available. The File API and SDK will allow convenient up- and downloads of arbitrary files with all the Baqend caching included. With that new feature you could for instance build a location-based image sharing app.
Our Cache Sketch Browser and App Caching will be available soon. With that feature, data is not only served from nearby CDN nodes but also directly from the caches on the end user’s device — without any changes to your app code. The strategy that makes this possible is based on Bloom filters and statistical methods to ensure cache coherence, i.e. to always give you fresh data.
Realtime-Queries are a very powerful abstraction which allow you to subscribe to any changes made to objects and query results. You could for example build a todo app that syncs with all users viewing and editing it in realtime. To achieve our very ambitious latency goals (<10ms + network delay to user) we implemented a scalable Apache Storm-based query matching engine from scratch.
To get a notification once these features are available, you can register to our newsletter. If you have questions, leave a comment or ask in our community forum and we’ll quickly respond. Happy hacking!
On Building a Faster Web
Written by
CEO of Baqend, a technology for faster websites. Background in distributed database systems and caching research.
On Building a Faster Web
Written by
CEO of Baqend, a technology for faster websites. Background in distributed database systems and caching research.
On Building a Faster Web
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/g-cloud-latest/uk-governments-contingent-labour-one-framework-d084c9b7cc31?source=search_post---------223,"There are currently no responses for this story.
Be the first to respond.
There are many barriers within the public sector to consuming cloud services via G-Cloud / Digital Marketplace, but we will take a look at the Crown Commercial Service and the Contingent Labour One framework.
There are many barriers within the public sector to consuming cloud services via G-Cloud / CloudStore, but we will start with the Crown Commercial Service.
As we can see from the changes in the people from CCS working on G-Cloud, it appears that CCS have a policy of rotating staff in/out of the G-Cloud team, which I support. This allows CCS to leverage the innovation from G-Cloud and apply it elsewhere. The only problem with this approach is that we are seeing the initial principles of G-Cloud being diluted as passes from staff to staff.
The Crown Commercial Service (CCS http://ccs.cabinetoffice.gov.uk/) have published a policy that restricts Central Government to a limit of £100K when procuring ICT consultancy via the G-Cloud frameworks, details of the policy can be found here.
Now, this is a poor document at best, as it’s not branded and displays a bias towards the larger suppliers on other frameworks.
CCS define temporary staff as follows:
The provision of workers to cover business-as-usual or service delivery activities within an organisation. Temporary Staff are also often referred to as “Contingent Labour”.
Specialists are normally middle to senior grades, used to provide expertise that is not available in-house, fulfilling functional or senior positions within the organisational structure and ideally engaged on a short term basis.
CCS define consultancy as follows:
The provision to management of objective advice relating to strategy, structure, management or operations of an organisation, in pursuit of its purposes and objectives. Such advice will be provided outside the ‘business-as-usual’ environment when in-house skills are not available and will be time-limited. Consultancy may include the identification of options with recommendations, or assistance with (but not the delivery of) the implementation of solutions.
CCS define ICT consultancy as follows:
The provision of objective IT/IS advice including that relating to IT/ IS systems and concepts, strategic IT/IS studies and development of specific IT/IS projects. Advice related to defining information needs, computer feasibility studies, making computer hardware evaluations and to e-business should also be included.
It’s clear from the CCS definitions that the £100K limit is a restriction on IT Consultancy and NOT the Specialist Cloud Services within Lot 4. This should be explained clearly within the CCS documentation.
The framework is split into three lots;
Lot 1 — Neutral Vendor
“This service provider manages a dynamic market supply chain and competes every medium to high value requirement (i.e. interim managers and specialist contractors) within the supply chain.”
As far as I can tell, many requirements for Cloud/ICT consultancy are being offered via Lot 1 on this framework, which is not surprising given the muddled advice from CCS. The only certain choice in the flowchart is to use Consultancy Labour One.
The only vendor selected to supply services under Lot 1 is Capita Business Services Ltd
There is no documentation available from CCS on how Capita “competes” every requirement.
Contingent labour is classed as one of the central commodities which (i) Commercial Heads from each Department have signed up to use (inclusive of their ALBs), (ii) Commercial Heads have signed off the Contingent Labour Strategy and if departments chose to go outside of this framework they will need Professional Services Procurement Board (PSPB) approval to do so.
Some of the benefits of the Contingent Labour One framework are;
The Contingent Labour One framework has no supporting business case, as documented in an FOI request here.
To provide services through the Contingent One Labour framework the worker (they’re not a Consultant because they are delivering something) is required to provide the following information:
If you contact CCS as a public sector buyer, you will receive confusing advice on if/when you can use G-Cloud to procure ICT consultancy, but this is not surprising based on their own documented guidance.
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
"
https://medium.com/@hasurahq/announcing-hasura-cloud-managed-graphql-for-your-database-and-services-a32ab41e6414?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hasura
Jul 9, 2020·8 min read
I’m delighted to announce the public beta of Hasura cloud!
Try it out here: cloud.hasura.io
This has been something we’ve been working on since January this year. We’ve used the core Hasura GraphQL engine as a base, made sure that we’re able to provide Hasura as a cloud offering where you don’t think about the number of instances, cores, memory, concurrent users, high-availability, realtime monitoring, caching, tracing, rate-limiting and other things that are infrastructure concerns and not really things you want to spend time on.
For those new to Hasura, the Hasura GraphQL engine is an open-source service that uses a metadata engine to connect to new or existing data-sources (like Postgres, REST APIs, GraphQL APIs etc) and provide a managed and secure GraphQL endpoint that internal or external applications can connect to.
We want to make GraphQL invisible to API developers, and give API consumers a GraphQL API they love. The logical next step for us, was to make Hasura GraphQL engine available as a managed cloud service.
Each instance of Hasura is a heavily multi-threaded server that exploits shared memory within the instance to aggressively optimise handling GraphQL queries at runtime. If you’re running Hasura yourself, you can first scale a few Hasura instances vertically and then horizontally. At what CPU or memory threshold should you scale up or scale out? Well….it depends. Hasura’s footprint is different for different kinds of workloads.
If you’re running on Hasura Cloud, you don’t need to care! Hasura cloud doesn’t ask you to think about the number of instances, cores, memory, thresholds etc. Hook up your database and other services, and it just works.
You can keep increasing your number of concurrent users and the number of API calls and Hasura Cloud will figure out how to make sure that your GraphQL API just keeps on giving. 🤓
Ultimately, your API performance will bottleneck on your database. Hasura Cloud ships with monitoring and tracing features that help you understand database query performance and give you insight into what to do. Most DBaaS vendors make it really easy to scale your postgres vertically, or add read replicas to your primary Postgres server.
Incidentally, if your upstream services are also written in a stateless way, especially if you’re using Actions with serverless functions or auto-scaling containers, then these will get automatically scaled by the cloud vendor too. 🤘
TL;DR:
GraphQL query caching
Hasura GraphQL Engine already offers query caching where the internal representation of the fully qualified GraphQL AST is cached, so that it’s plan need not be computed again. For example, when a GraphQL query with variables and session variables comes in, the generated SQL is already prepared as a statement and exact end-user session based variables and the dynamic API variables (query variables) are zoomed through to the database. This makes Hasura’s queries extremely fast.
GraphQL data (response) caching
Hasura cloud goes a step further and adds support for data caching. We’ve taken the first step, and there’s lots more to do, but here’s how Hasura cloud thinks about caching which is something we API developers usually build inside our app server.
There are many purposes to application level caching, but the most common requirement is usually to reduce the load on the underlying data-source.
Caching authenticated API calls automatically is very hard/painful, because of 2 main problems:
Knowing what to cache given an incoming API call made by a specific end-user:
There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors.
- Leon Bambrick ( ) January 1, 2010
Knowing how to update the cache
Because Hasura has enough information about the data models across data sources, and the authorization rules at the application level Hasura has enough information to offer end to end application caching by solving all the problems above.
Today, we’ve launched support for caching authenticated GraphQL API calls that works for shared data (problem 1). How do we handle cache invalidation today? We use a TTL configuration so that state data is refreshed within a time interval.
The good news is that since API writes and workflows pass through Hasura (GraphQL mutations), Hasura can eventually handle automatic cache invalidation too!
Apart from monitoring successes errors (and partial errors) from GraphQL API calls and their responses, Hasura Cloud also adds great support for monitoring websocket connections and subscriptions!
Strange things happen in production and it’s hard to know where the problems came from. Tracing requests and responses, end to end and through the various components in your system in production is critical.
Hasura acts as a fulcrum and coordinating point between your GraphQL API consumer, your data models and your custom business logic or external services that you bring in via HTTP (GraphQL Remote Schemas or REST API Actions or Event Triggers).
Hasura Cloud adds a deep integration with OpenTracing, so that it can create traces, or respect incoming trace spans. Haura also forwards trace-ids and span-ids to services so that a distributed trace can easily be collected across your entire system. For example, this is what a trace collected from Hasura forwarding to an Action, that interacts with the database, whose response is then enriched by Hasura looks like when it’s visualised end to end.
Today Hasura Cloud supports Datadog as a target to send traces to, and we’ll rapidly be adding more vendors that support OpenTracing over time.
When you’re running a GraphQL API in production, you might need to add some rate-limiting rules. Especially if your API is a developer facing API and not just used by an app. Rate-limiting allows you to make sure that your server and your data sources are protected from sudden spikes from malicious users and also allows you to make sure that your customers or tenants consume your API within reasonable limits.
Depending on what you want rate limiting to do, you need to maintain some state across various concurrent API requests to “count” your users’ limits.
Hasura Cloud offers rate limiting for unauthenticated users (based on IP) or parameterized by end-user session variables. Rate limiting rules can set a maximum number of requests per minute based on the parameters above, and can set query depth limits also. Over time, we will add support for dynamic query cost based limiting as well!
Hasura cloud captures queries (optionally stripping out session and query variables since they might be sensitive) and makes it available to you for 2 main uses:
You can export the set of GraphQL queries and mutations that have been captured by Hasura in a given time window and then use it for all sorts of things.
Regression testing: You can run a test in the Hasura console or your favourite test runner against your staging environment as you make changes! This way, you’ll know if any changes you make to Hasura will affect GraphQL queries you’re seeing in production.
Allow-list management: You can also use the set of captured queries to create or update an allow list. For example, your staging project on Haura cloud can capture queries from your application testing suite and use that to help you build an allow list.
Hasura connects to databases and services running on systems owned by you. For example, you can connect to Heroku Postgres, or RDS, or cloud SQL, or your own Postgres running on your VM.
While all communication between Hasura and your services can be encrypted TLS/HTTPS you might further benefit from ensuring that your database and your services only accept connections from Hasura anyway.
You can do this with:
We’ve also started the process of getting our SOC2, ISO 27001 and HIPAA-readiness certifications so that it’s easier for you to start using Hasura Cloud in an enterprise environment. If you’d like to more, please get in touch!
Try it out here: cloud.hasura.io
Originally published at https://dev.to on July 9, 2020.
⚡️ Instant realtime GraphQL APIs! Connect Hasura to your database & data sources (GraphQL, REST & 3rd party API) and get a unified data access layer instantly.
⚡️ Instant realtime GraphQL APIs! Connect Hasura to your database & data sources (GraphQL, REST & 3rd party API) and get a unified data access layer instantly.
"
https://medium.com/rigetti/quantum-cloud-services-opens-in-public-beta-31989e15e36e?source=search_post---------226,"There are currently no responses for this story.
Be the first to respond.
By Betsy Masiello, VP Product
We’re thrilled to open Rigetti Quantum Cloud Services (QCS™) to public beta today. The QCS platform introduces an entirely new access model for quantum programming that is centered on an integrated cloud architecture. Our tightly coupled quantum and classical resources unlock performance gains that enable programs to run as much as 30x faster than on web API models.
Once registered, users have access to their own dedicated Quantum Machine Image, which comes preloaded with all the tools necessary to get started building quantum programs, including pyQuil and our quantum simulator. We’re also deploying two Aspen QPUs to the QCS platform, which users can book with an online reservation system available on the new QCS web dashboard. Beta users will receive $5,000 in credits to use toward running programs on the QPU during their first month.
Today we’re not only opening up access to QCS, we’re also distributing the first set of applications built by our Developer Partners:
More than 30 leading scientists from around the world have signed on as QCS Research Partners. Their work ranges from characterizing and benchmarking quantum hardware to computational research across biology, chemistry, and machine learning. We believe the quantum ecosystem will flourish by working in the open. To that end, our research partners are encouraged not only to publish their results but to share their data and code and open-source the tools and libraries they create on the QCS platform.
QCS is the only integrated, quantum-first cloud platform broadly available to users today. The application-layer performance achieved with QCS enables faster development cycles, accelerating our march toward quantum advantage.
Rigetti Computing
489 
489 claps
489 
Rigetti Computing
Written by
On a mission to build the world’s most powerful computer.
Rigetti Computing
"
https://medium.com/@ArtsandClouds/public-vs-private-vs-hybrid-cloud-exploring-the-use-cases-2154bacafa95?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Peter G. Goral
Aug 18, 2016·4 min read
In a previous post we explored various challenges with cloud implementation and how to mitigate these. But how do you choose the right cloud implementation model for your business — public, private, hybrid?
Before I get into the nitty gritty, let’s first take a look at what the private cloud is. The essential difference between the public and private models is “sharing.” With a public cloud infrastructure there is shared physical hardware which is owned and operated by a vendor, so there’s no maintenance component for the client business. The size of the public cloud means a company can scale capacity and computing power either up or down in just minutes, in congruence with business needs.
The private model however, offers a more customized set-up that is dedicated to a particular business. It’s hosted either on-site or at the provider’s data center, and offers the same scale, agility, and other benefits of the public cloud. Although the scalability is not the same as with public cloud, private cloud allows for more control and security, which establishes it as an ideal choice for larger enterprises such as banks and financial institutions managing personally identifiable information (PII) including those that fall under certain regulatory standards.
With that being said, let’s take a look at some use cases for each as well as explore hybrid cloud scenarios.
With a public cloud, the customer can move the responsibilities of management to the cloud vendor. This movement can result in reduced direct costs, and also gives IT more time to focus on revenue-generating projects such as mobile apps, improved customer portals, and similar work. For some IT professionals, a challenge arises with public clouds because they want them to be integrated to other cloud environments while also running in-house applications.
Consider these use cases for the public cloud:
Despite some of the challenges and associated costs of the private cloud model, many bigger firms are compelled to choose private due to the security risks of public. The potential damage to a company’s brand and the loss of customer trust after a public cloud breach can exponentially surpass the costs of the private cloud. Here are some typical use cases for a private model:
Implementing a private cloud securely can prove difficult unless you utilize the help of a third-party service. This is where a qualified IT consultancy such as TechBlocks can provide critical guidance on the best practices for implementation, and perhaps discuss the case for a hybrid public-private approach.
The hybrid cloud is increasingly the path for organizations that desire a customizable approach with reduced maintenance costs and time. Pursuing a hybrid approach is often the path IT will take to convince upper management that the cloud is safe and a good option for critical data. They can test a tier of data or applications in the public cloud while keeping the bulk of their infrastructure in a private environment. Consider these hybrid cloud use cases:
Looking for a partner to help guide your company through the complexities of cloud migration and implementation? TechBlocks helps companies pick the right storage and application options for their particular business and industry. Visit www.tblocks.com to learn more.
http://blog.tblocks.com/public-vs.-private-vs.-hybrid-cloud-exploring-the-use-cases
Senior Executive BusinessTransformation, Change Management, Social Media Strategist, Accomplished Artist, www.ArtEnvy.Faso.com Radio Show Host
9 
1
9 
9 
1
Senior Executive BusinessTransformation, Change Management, Social Media Strategist, Accomplished Artist, www.ArtEnvy.Faso.com Radio Show Host
"
https://medium.com/@sramana/cloud-stocks-digitalocean-goes-public-with-strong-isv-paas-strategy-454590e7f6b4?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sramana Mitra
Apr 5, 2021·4 min read
I’m publishing this series to discuss a topic that I follow closely — cloud stocks, trends, strategy, acquisitions, and more. I like fundamentals-focused business building, and outline the principles of fundamentals-focused business building in my free Bootstrapping course.
According to an IDC report, the global IaaS and PaaS market for individuals and organizations with less than 500 employees is expected to grow at 27% CAGR from $44.4 billion in 2020 to reach $115.5 billion by 2024. New York-based DigitalOcean (NYSE: DOCN) is a leading player in the segment that went public last week.
DigitalOcean’s Offerings
DigitalOcean was founded in 2012 by Alec Hartman, Ben Uretsky, Jeff Carr, Mitch Wainer, and Moisey Urtesky based on a realization that the cloud was the new way to build modern-day web applications. The founders believed that software developers, entrepreneurs, and SMBs needed better solutions from cloud computing providers to leverage the opportunities provided by innovative cloud infrastructure technologies. DigitalOcean grew to offer infrastructure and PaaS solutions to these organizations and teams without requiring them to have DevOps experience.
To make development easier for developers, DigitalOcean provides on-demand infrastructure and platform tools for developers, startups, and SMBs that are easy to use and access while being reliable and affordable. It provides a range of capabilities to access compute, network, and storage infrastructure as well as software-managed services that provide additional capabilities for managing more robust infrastructure needs.
Today, DigitalOcean caters to more than 570,000 customers in over 185 countries. It has built a developer learning community with over 34,000 developer tutorials, technical guides, and community-generated Q&As. The company has over 5 million developers on its platform, which includes over 30,000 ISVs.
DigitalOcean provides similar services that giants like Amazon and Microsoft provide but focuses primarily on smaller-sized customers and individual developers. It has built up a business by keeping its products easy to use. Unlike Amazon and Microsoft, DigitalOcean does not have a slew of product offerings. It has a few products, including the customizable Linux-based virtual machines that it calls droplets, data-storage options, networking tools, and three databases. It is looking to add analytics software and add data center infrastructure in more places around the world in the coming quarters.
DigitalOcean’s Financials
DigitalOcean earns revenues by charging its customers a monthly fee based on the usage of its services. The growth of cloud computing and developer action globally has resulted in a significant growth of DigitalOcean’s financials as well. Revenues have grown from $203.1 million for fiscal 2018 to $254.8 million in fiscal 2019 to $318.4 million for fiscal 2020. DigitalOcean is still not profitable and reported a loss of $36 million in fiscal 2018, $40.4 million in fiscal 2019 and $43.6 million for fiscal 2020.
DigitalOcean went public last week. Prior to going public, it had raised $455.6 million in 13 rounds of funding, with the most recent round being held in May last year. Its investors include Andreessen Horowitz, Access Industries, EquityZen, Kliwla Family Office AG, Viaduct Ventures, Mighty Capital, Opus Bank, Barclays Investment Bank, East West Bank, and HSBC Bank.
It raised $775.5 million from its IPO where it sold its stock at $47 apiece. It is currently trading at $43.80 with a market capitalization of $4.61 billion.
I believe that DigitalOcean is a formidable player in the space. It boasts of an impressive PaaS strategy as is reflected in the size of its developer community. By keeping it simple, DigitalOcean has successfully built a PaaS ecosystem that will sustain growth for both itself and its developers.
It is early days, but the stock hasn’t had a promising start so far as the timing of the IPO coincides with significant market turbulence in cloud stocks.
Disclosure: All investors should make their own assessments based on their own research, informed interpretations, and risk appetite. This article expresses my own opinions based on my own research of product-market fit, channel execution, and other factors. My primary interest is in product strategy. While this may have bearing on stock movements, my writings tend to focus on long-term implications. The information presented is illustrative and educational, but should not be regarded as a complete analysis nor recommendation to buy or sell the securities mentioned herein. I am not a registered investment adviser and I am not receiving compensation for this article.
Looking For Some Hands-On Advice?
For entrepreneurs who want to discuss their specific businesses with me, I’m very happy to assess your situation during my free online 1Mby1M Roundtables, held almost every week. You can also connect with me during our Rendezvous meetups, and check out my Bootstrapping Course, our YouTube channel, podcast interviews with VCs and Founders.
Founder of the 1M/1M global virtual incubator
Founder of the 1M/1M global virtual incubator
"
https://medium.com/@alibaba-cloud/alibaba-cloud-serverless-kubernetes-service-enters-beta-testing-phase-49f01d286d91?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 5, 2018·4 min read
Worldwide public beta testing for Alibaba Cloud Serverless Kubernetes service began in August 2018. Testing areas opened in Hangzhou, Shanghai, and part of the west coast of the United States including Silicon Valley. Alibaba Cloud Serverless Kubernetes supports sites both in China and abroad as well as supports flexible scaling. Users can deploy Kubernetes container applications in seconds, with no capacity planning or node management required. This service is currently in beta testing for free.
Alibaba Cloud’s Serverless Kubernetes service is intended to lower entry barriers for using container technology and simplify the operations and maintenance of container platforms. In this manner, users can focus on design and application development without having to worry about micromanaging the infrastructure of their applications — things like Kubernetes clusters and servers.
Based on Alibaba Cloud’s elastic computing base architecture, the service fully leverages the security and flexibility of virtualized resources. It is compatible with the Kubernetes ecosystem and supports functions like Workload (Deployment, StatefulSet, Job, Pod), Service, Ingress, Volumes, Service Account, and HPA. It can also integrate fully with a container user’s current application environment.
The service is easy to use, and you only pay for what you use. It is all-inclusive, securely isolated, expandable as needed, and interconnectable. In addition to these advantages, it also strengthens multidimensional technologies such as container scheduling, service discovery, service access, resource management, and sub-account support. For example, users can provide both Layer 4 service access through LoadBalancer Service and Layer 7 service access through Ingress.
Serverless Kubernetes integrates the high portability and agility of containers with the elastic scheduling and isolation provided by Alibaba Cloud’s flexible computing for a wide scope of application. It can quickly and easily be deployed in scenarios as diverse as web applications, back-end services for mobile applications, multimedia processing, data processing, and continuous integration. It is particularly suitable for batch tasks and sudden-demand work.
Many users have actively provided feedback since the release of this product, and our team is continuously adding and upgrading features and optimizing the product experience. Many new features have already been released, such as Exec/Attach, service discovery, Ingress, Service Account, and sub-accounts. We will look at the following three features.
Service discovery is the foundational function of communication among services within a Kubernetes cluster. Whereas traditional service discovery relies on the cluster’s internal DNS component, service discovery within Serverless Kubernetes is based on Alibaba Cloud DNS Private Zone, so users can easily discover services without having to install any components like Kube DNS.
Alibaba Cloud DNS PrivateZone is a private domain name resolution and management service based on the Alibaba Cloud Virtual Private Cloud (VPC) environment. Users can map private domain names to IP resource addresses in one or more customized VPCs while their private domain names remain inaccessible from other network environments.
For now, the service discovery feature only applies to the Intranet Service and Headless Service. After creating the service, you can access services within the cluster by long or short domain name from within the pod:
Long domain name:
Short domain name:
In a Kubernetes cluster, Ingress is a collection of rules that authorizes inbound access to the services of the cluster. You can use Ingress to configure an externally accessible URL, server load balancing, SSL, and name-based virtual hosts.
Within a Serverless Kubernetes cluster, you can use LoadBalancer Service to provide externally accessible Layer 4 services and Ingress to provide externally accessible Layer 7 services. Ingress is easy to use and does not require the installation of Ingress Controller.
In Serverless Kubernetes, the Service Account function is enabled by default, which means that the pod has permission to access apiserver by default. When a container is created, you can find three files in the directory /var/run/secrets/kubernetes.io/serviceaccount/: file namespace, token, and ca.crt. The last two are credentials for accessing the cluster through service account. Through service account, users can only access resources within the serverless clusters in the pod.
Alibaba Cloud project manager Yi Li put it this way.
“If administering a Kubernetes cluster is like driving a powerful sports car with manual transmission, then using Serverless Kubernetes is like riding in a self-driving car — you can just relax and enjoy the freedom of computing. Its advantage is that it strips away complex and time-consuming maintenance and operations work, automatically allocating low-level service resources, a work model more in line with our understanding of the future of cloud computing: users don’t need to pay attention to operations like environment configuration, server administration, and maintenance upgrades — they can just focus on writing application logic.”
Reference:
https://www.alibabacloud.com/blog/alibaba-cloud-serverless-kubernetes-service-enters-beta-testing-phase_593929?spm=a2c4.11965400.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
11 
11 
11 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://architecht.io/3-things-to-read-today-apples-ai-google-s-lower-speed-cloud-network-and-mechanical-turkers-push-4d7fd446432a?source=search_post---------231,NA
https://medium.com/@garstep/amazon-virtual-private-cloud-vpc-on-private-ip-addresses-public-ip-addresses-internet-96143c0ec57?source=search_post---------232,"Sign in
There are currently no responses for this story.
Be the first to respond.
Garrett Stephens
Jun 29, 2017·6 min read
All information is extracted from this video, which is about an hour long:
These are my written notes on the lesson, so this may be a bit difficult to dissect at parts, but I certainly recommend exploring the video further for more in depth lessons on the subject matter. I left “bookmark” links below each of the snapshots to encourage further exploration.
The default VPC contains over 65,000 private IPs… so why not use it? If you instead create a custom VPC, it is more secure and you can customize (define your own IP address range, create your own public and private subnets, tighten down security settings).
“By default, instances you launch into a VPC can’t communicate with your own network.” So you can connect your VPC to your own data center using Hardware VPN Access. “So that you can effectively extend your data center into the cloud and create a hybrid environment”.
To do this, you need a Virtual Private Gateway. On the left side (labelled ‘virtual private gateway’) is the VPN concentrator on the Amazon side. Then on your side, you need a customer gateway, which is either a physical device or a software applicate that sits on your side of the VPN connection. A VPN Tunnel comes up when traffic is generated from your side of the connection.
Can be between your VPCs or other VPCs. VPC A wouldn’t be able to communicate with VPC B or C without a peering connection. Transitive peering does not work, meaning that VPCs need to be directly peered in order to be connected.
Also, VPCs with overlapping CI DRS cannot be peered. These are fine because they have different domain ranges, but if they didn’t there would be a problem.
If you delete the default VPC, you have to contact AWS support to get it back again!
Private IP Addresses are IP Addresses not reachable over the internet. They are used for communications between instances in the same network.
“When you launch a new instance it’s given a private IP address and an internal DNS Hostname that resolves to the private IP address of the instance. But if you want to connect to this over the internet, it’s not going to work. So then you’d need a public IP address which is reachable from the internet. You can use public IP addresses for communication between your instances and the internet. Each instance that receives a public IP address is also given an external DNS hostname.
Public IP addresses are associated with your instances from the Amazon pool of public IP addresses. When you stop or terminate your instance, the public IP address is released, and a new one is associated when the instance starts. So if you want your instance to retain this public IP address, you need to use something called an Elastic IP Address.”
An elastic IP address is a static or persistent IP address that’s allocated to your account and can be associated to and from your instances as required.
Default Net Mask in 20 for Subnets. These preserve up to 4096 IP addresses per subnet. Subnets are always mapped to a single availability zone (see below).
It is good to spread VPCs over availability zones for redundancy and failover purposes. You use Public Subnets for things that must be connected to the internet, such as web servers. Private Subnets either don’t need the internet, or are things you want to protect from the internet, such as database instances.
To attach your VPC to the internet, you need to attach an Internet Gateway, and you can only attach one internet gateway per VPC.
Can now see the VPC has internet attached:
Before our instances can have access to the internet, we need to make sure that our subnet route tables point to the internet gateway.
Every VPC has a default route table. It’s good practice to create a new one to practice with customization.
Above, the main route table is for the locally connected private subnet, which is the default (not connected to the internet). Above, the custom route table is used for the public subnet, connected to the internet gateway, and therefore the internet.
This is where you edit/create a route table and connect it to an internet gateway:
Then, visit “subnet associations” and associate the Public subnet with the route table you just created.
NAT devices [network address translation devices] allow you to give a private subnet to the internet, without allowing the internet to access that subnet.
A NAT device forwards traffic from your private subnet to the internet, or other AWS services, and then sends the response back to the instances. When traffic goes to the internet, the source IP address of your instance is replaced with the NAT device and when the internet traffic comes back again, then that device translates the address to your instance’s private IP address.
A NAT Gateway is recommended by AWS because it is a provided service that gives more bandwidth than NAT Instances. Each NAT Gateway is created in a specific availability zone and is implemented with redundancy.
A NAT Instance is launched from a NAT AMI [Amazon Machine Image] and runs as an instance in your VPC, so it’s something else you have to look after. Whereas, in a NAT Gateway, a fully managed service, you can basically install it and forget about it.
NAT Gateway must be launched into a public subnet because it needs internet connectivity.
Our diagram with security groups:
Now, for Network ACLs:
Our diagram with Network ACLs:
A bit clearer:
As always, thank you for reading! If you’d like to connect, I’ll leave my information below. I thrive in my work only by meeting and learning from new people.
Garrett StephensE: garstep [at] umich.edu
Generalist | *profile image is a piece by Raphael Ramirez, ROTTEN_FILES.exe
101 
101 claps
101 
Generalist | *profile image is a piece by Raphael Ramirez, ROTTEN_FILES.exe
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/digital-leaders-uk/how-the-cloud-can-build-more-innovative-public-services-b62d5237032b?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
Written by Harold de Neef, Group Director, Cloud, Civica
Call it the Amazon effect or the Uberisation of services, most of us expect everything to be as easy and instantaneous as a one-touch car booking or a one-click payment. It doesn’t matter whether we’re interacting with our bank, ordering a pizza or paying council tax — it needs to be simple, intuitive and fast.
These demands continue to evolve. Being able to self-serve is increasingly the norm, whether through apps, online or via voice technology. Indeed, one recent study found that half of UK citizens see digital services as ‘very important’ to their daily lives, while a quarter use them wherever possible. And this is only going to increase. The implications are clear: there is now an expectation to offer citizens digital channels to self-serve.
However, it’s also important to remember that 10% of the UK adult population were still not online in 2018. This could be seen as a challenge or a huge opportunity to change how citizens are supported in the future.
Public service organisations need to strike a balance in providing services via a range of channels to meet the varied requirements of citizens: from the tech-savvy who want to self-serve to the less able who require more support. At the same time, the key opportunity in moving to the cloud is to open the door to new technologies both now and the coming years.
Artificial intelligence, machine learning, chatbots or robotic process automation have the potential to radically alter how public services are delivered. With cloud enabling teams to radically speed up the ability to analyse, process and make decisions based on accurate information, employees can be freed up from administrative tasks to focus on experience-enhancing programmes and outcomes. These technologies currently sound out of reach for public bodies, but they aren’t. This is why, investing in the infrastructure required to deliver them can and should be done now.
A great example of how the cloud is helping improve the user experience is North Yorkshire County Council. Serving over 600,000 citizens, the Council collects a wide variety of payments totalling £5.5 million annually. Due to the region’s size and location, it isn’t always easy for citizens to reach the local post office or council especially during working hours.
By implementing Civica’s cloud payments software, CivicaPay, the Council can now offer an improved service for all citizens, letting them securely pay for services in convenient ways such as PayPal, Apple Pay and PayPoint, as and when suits. An essential added benefit is that this means the Council can now collect income faster, process it quickly and improve cash flow, ultimately improving its financial health.
Cloud computing has been discussed in the context of IT in public services for a number of years now. But the rapid pace of change means it’s undoubtedly the key to providing the experience and service levels that customers want. Increased automation and self-service free up frontline employees to focus on those citizens who require more help. In other words, cloud allows public service organisations to deliver tailored services to their different audiences, without impacting either citizen or employee experiences.
The latter is just as important as the former, even though it often gets overlooked. In the UK, the percentage of people in paid work in the public sector in March 2018 was the lowest proportion since comparable records began in 1999, which means that employees are constantly having to deliver more, with less people. At a time when UK public sector morale remains low after years of austerity-driven pay freezes and budget cuts, it’s important that public employees are able to do their jobs effectively giving them a deserved sense of fulfilment. This should not only boost morale, but also help keep knowledgeable, experienced people in the public sector where they are needed.
Organisations need a reliable, cost-effective infrastructure that can deliver both now and in the future. Updating technology to deliver better services in a cost-effective manner through cloud and new innovations like artificial intelligence, chatbots, or process automation are critical to meet our citizen’s evolving expectations. The only way to do that is through the cloud. While many public sector organisations are already reaping the benefits that cloud delivers, more must be done to accelerate the cloud journey and ensure that our public services are fit for the future. This is important for our public services, our citizens and our country.
Originally published here.
More thought leadership
Originally published at https://digileaders.com on September 3, 2019.
Thoughts on leadership, strategy and digital transformation…
51 
51 claps
51 
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
"
https://medium.com/@actallchinatech/tencent-invites-the-public-to-test-its-new-ai-cloud-81b342158ad3?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
All Tech Asia
Jun 23, 2017·3 min read
Tencent’s new AI strategy is beneficial for all: opening up its core AI techniques to provide better services.
Baidu and Alibaba have both released their core artificial intelligence (AI) techniques to the public, allowing developers and third party small and medium-sized businesses to implement its technology into their operations. Now, Tencent has caught up with its competitors in the AI race by launching its “AI Cloud” which opens up the tech giant’s core technology to the public.
Tencent has shown great ambition to develop its own AI capabilities with the establishment of the Tencent AI Lab in April 2016. The Chinese tech giant has since assembled a team of world class AI scientists, researchers and experts. After poaching Zhang Tong, the former head of Baidu Big Data Lab, to lead Tencent AI Lab this March, the firm is going deeper into AI related research and placing a much stronger bet in the competition of AI-driven products.
This time, the tech behemoth is introducing an “AI Cloud” to the public and is releasing the company’s core technology to provide new AI services, local media reported. It claimed to build the new service based on the existing cloud computing structure, and is providing the techniques it has developed such as computer vision, speech recognition, and natural language processing.
Just last month, Tencent opened a new AI lab in Seattle and appointed a former Microsoft scientist to drive the company’s research on speech recognition and natural language processing, CNBC reported. In an interview with CNBC, Dowson Tong, Tencent’s senior executive vice president and social network group president, said he was a big proponent of offering A.I. as a service.
“I am very optimistic (and) I am very bullish about the future of AI,” Tong told CNBC. “I think by having all these players, big and small, and each with their own expertise, we’re going to see the whole industry prosper.”
The tech company was clearly confident to show what it is capable of in terms of AI development. It brought its Go program “Fine Art” or “Jueyi” to a computer Go tournament in Japan earlier this year and won the championship title after winning all 11 games.
With millions of existing users, Tencent has a strong and solid starting point. The company is best known for WeChat, which started off as a messaging service and has rapidly grown its business to include everyday services such as food delivery and cab hailing into the mega app. The quantity and depth of big data that it has amassed is almost beyond imagination.
“Make AI everywhere” — this is the first thing that greets you upon opening Tencent’s AI lab website. This pretty much says it all in terms of the company’s AI strategy. With the company now opening its core techniques to the public, Tencent is betting big on artificial intelligence. Just as its AI Lab’s website says — “aiming to build the top artificial intelligence team.”
AllTechAsia is a startup media platform dedicated to providing the hottest news, data service and analysis on the tech and startup scene of Asian markets
See all (240)
1 
1 clap
1 
AllTechAsia is a startup media platform dedicated to providing the hottest news, data service and analysis on the tech and startup scene of Asian markets
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/jacob-morgan/how-the-canadian-public-service-is-creating-a-talent-cloud-of-free-agents-cbbda861762d?source=search_post---------235,"There are currently no responses for this story.
Be the first to respond.
Abe Greenspoon is the Program Lead for Canada’s Free Agents, a Government of Canada program launched in 2016 that proposes a new model for workforce mobilization. Abe has been in the public service of Canada for about 10 years.
Click here to listen to the podcast episode
The idea of creating a more autonomous, mobile workforce first came from a report released in 2012 from Deloitte. The report looked at how the government might reorganize itself to better respond to problems of the future and it proposed a concept of a cloud-based workforce based off of the IT cloud computing.
Essentially they have a group of workers in a database “available to do project-based work, move around the organization, solve problems, return to the cloud when they weren’t needed anymore, and then just continue on to different projects.”
So when a position opens up, Abe and his team advertise for it within public service and those who are interested can apply. Abe says that this new way of flexible work has created greater employee satisfaction and better career decision making along with many other benefits.
The process to become a free agent is tough, not just anyone can become a free agent. In order to become one, you have to be willing to continuously learn and grow and you can’t get stuck in one technical field of work. They need to be willing to explore, they have to be curious, and they can’t be scared to fail. Free agents should be quick learners and they should easily be able to adapt because they move around to different roles in different offices quite frequently.
In order to make sure they are hiring the right people, Abe says they use a lot of unconventional hiring tactics including improv and puzzle-solving. It tends to take about three months for people to go through the process of applying, interviewing, and then getting the official offer.
Even though these free agents are technically gig workers, they still receive the benefits a full-time regular employee would typically receive like pensions and health insurance.
Abe believes that this way of working also helps create a sense of purpose for employees as well. He says, “the opportunity to choose your job, to have that autonomy to make those decisions, I think puts you in a better position to find your purpose. I just think, naturally, you’re going to try to look for those opportunities that suit you better, you’re going to think more, and self-reflect more about what environments you’ll thrive in, what environments you won’t thrive in, and to have that ability to choose; it leads to all sorts of other kinds of downstream benefits, I think, once you give people that ability. So, finding your purpose, I think, it’s something we realized over time is, it’s a potentially really interesting outcome to giving people this sort of autonomy for their jobs.”
While this is only being implemented in the public service space at the moment, there are many ways that leaders in the private sector could learn from this concept as well.
How the government of Canada is implementing a cloud-based workforceWhat it takes to be a free agentHow they use games and improv in the hiring processAbe’s view of Universal Basic IncomeHow they handle benefits for flexible workersThe benefits of giving employees flexibility and autonomy
Get the Transcript Here
Learn how you can better equip yourself and your organization to deal with the major workplace changes that are taking place with my free Future of Work Training Series today!
Leadership, The Future of Work, and Employee Experience
1 
1
1 clap
1 
1
Written by
4x Best-Selling Author, Speaker, & Futurist. Founder of FutureOfWorkUniversity.com. Exploring Leadership, Employee Experience, & The Future of Work
Insights and strategies you need to succeed and thrive at work and in life.
Written by
4x Best-Selling Author, Speaker, & Futurist. Founder of FutureOfWorkUniversity.com. Exploring Leadership, Employee Experience, & The Future of Work
Insights and strategies you need to succeed and thrive at work and in life.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-polygon-blog/polygon-blockchain-datasets-are-now-available-on-google-bigquery-67e91a32aaee?source=search_post---------236,"There are currently no responses for this story.
Be the first to respond.
Hosted by Google Cloud as a financial services public dataset, the Polygon blockchain dataset has been listed in the Google Cloud Marketplace, and can be found here.
Polygon aims to serve as Ethereum’s Internet of Blockchains, bringing massive scale to Ethereum via Polygon SDK, supporting stand alone chains and secured chains.
This dataset is part of a larger effort to make cryptocurrency data available in Google BigQuery through the Google Cloud Public Datasets program. The program is hosting a number of real-time cryptocurrency datasets, with plans to expand offerings to include additional distributed ledgers. You can find these datasets by searching for “crypto” in the GCP Marketplace. For analytics interoperability, Google has designed a unified schema that allows all Bitcoin-like datasets to share queries.
This public dataset is hosted in Google BigQuery and is included in BigQuery’s 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset. Watch this short video to learn how to get started quickly using BigQuery to access public datasets.
Google BigQuery makes it simple to query on-chain data in a simple, organized manner on the cloud using standard SQL syntax. You can think of it like a massive indexer that allows you to perform powerful data analysis. You are also able to perform the same or similar queries on multiple blockchains at once and compare them, or track cross-chain activity on interoperable tokens.
Some possibilities unlocked using Google BigQuery on the Polygon Dataset include the ability to:
Blockchain analytics platform Nansen already built a Polygon dashboard that you can check out here.
Interested in learning more about how the data from these blockchains were brought into BigQuery? Looking for more ways to analyze the data? Check out the Google Cloud Big Data blog post and try the sample queries to get started.
Who made this happen? It has been a pleasure to work with Helix Technologies, Nansen, and the Google Cloud Developer Relations team on this project.
We look forward to seeing how the vibrant developer community at Polygon uses this powerful platform to bring further utility to their Dapps!
Other relevant links:
Polygon is the first well-structured, easy-to-use platform for Ethereum scaling and infrastructure development. Its core component is Polygon SDK, a modular, flexible framework that supports building and connecting Secured Chains like Plasma, Optimistic Rollups, zkRollups, Validium etc and Standalone Chains like Polygon POS, designed for flexibility and independence. Polygon’s scaling solutions have seen widespread adoption with 350+ Dapps, ~121M txns and ~1M+ unique users.
If you’re an Ethereum Developer, you’re already a Polygon developer! Leverage Polygon’s fast and secure txns for your Dapp, get started here.
Website | Twitter | Reddit | Telegram
Updates from Ethereum’s top Scaling Solution.
109 
109 claps
109 
Updates from Ethereum’s top Scaling Solution. Let’s bring the world to Ethereum!
Written by
Polygon is the first well-structured, easy-to-use platform for Ethereum scaling & infrastructure development. Follow us on Twitter — twitter.com/0xPolygonTech
Updates from Ethereum’s top Scaling Solution. Let’s bring the world to Ethereum!
"
https://medium.com/google-cloud/analyze-polygon-blockchain-datasets-with-google-bigquery-e079b24d2bee?source=search_post---------237,"There are currently no responses for this story.
Be the first to respond.
Hosted by Google Cloud as a financial services public dataset, the Polygon blockchain dataset has been listed in the Google Cloud Marketplace, and can be found here.
Polygon aims to serve as an internet-of-blockchains for Ethereum, bringing massive scale to Ethereum via the Polygon SDK, supporting both standalone blockchains and secured blockchains.
This dataset is part of a larger effort to make cryptocurrency data available in Google BigQuery through the Google Cloud Public Datasets program. The program is hosting a number of real-time cryptocurrency datasets, with plans to expand offerings to include additional distributed ledgers. You can find these datasets by searching for “crypto” in the GCP Marketplace. For analytics interoperability, Google has designed a unified schema that allows all Bitcoin-like datasets to share queries.
This public dataset is hosted in Google BigQuery and is included in BigQuery’s 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset. Watch this short video to learn how to get started quickly using BigQuery to access public datasets.
Google BigQuery makes it simple to query on-chain data in a simple, organized manner on the cloud using standard SQL syntax. You can think of it like a massive indexer that allows you to perform powerful data analysis. You are also able to perform the same or similar queries on multiple blockchains at once and compare them, or track cross-chain activity on interoperable tokens.
Some possibilities unlocked using Google BigQuery on the Polygon Dataset include the ability to:
After running the query, you will get the results which can then be exported as CSV/to Google Sheets for further analysis. You can also proceed to use Google Data Studio to generate interactive visualizations based on this data, by clicking on the ‘Explore Data’ button.
Blockchain analytics platform Nansen already built a Polygon dashboard that you can check out here.
Interested in learning more about how the data from these blockchains were brought into BigQuery? Looking for more ways to analyze the data? Check out the Google Cloud Big Data blog post and try the sample queries to get started.
It has been a pleasure to work with Helix Technologies, Nansen, and the Google Cloud Developer Relations team on this project.
We look forward to seeing how the vibrant developer community at Polygon uses this powerful platform to bring further utility to their Dapps!
Other relevant links:
Polygon is the first well-structured, easy-to-use platform for Ethereum scaling and infrastructure development. Its core component is Polygon SDK, a modular, flexible framework that supports building and connecting Secured Chains like Plasma, Optimistic Rollups, zkRollups, Validium etc and Standalone Chains like Polygon POS, designed for flexibility and independence. Polygon’s scaling solutions have seen widespread adoption with 350+ Dapps, ~121M txns and ~1M+ unique users.
If you’re an Ethereum Developer, you’re already a Polygon developer! Leverage Polygon’s fast and secure txns for your Dapp, get started here.
More about Polygon: Website | Twitter | Reddit | Telegram | Discord
Google Cloud community articles and blogs
196 
2
196 claps
196 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Polygon is the first well-structured, easy-to-use platform for Ethereum scaling & infrastructure development. Follow us on Twitter — twitter.com/0xPolygonTech
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@qwiklabs/solve-healthcare-challenges-in-this-weeks-work-and-play-game-7bd9ee0208e1?source=search_post---------238,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Mar 3, 2021·2 min read
In partnership with the Harvard Global Health Institute, Google Cloud released the COVID-19 Public Forecasts to serve as an additional resource for first responders in healthcare, the public sector, and other impacted organizations preparing for what lies ahead.
To generate the COVID-19 Public Forecasts, Google Cloud researchers developed a novel time series machine learning approach that combines AI with a robust epidemiological foundation. In this edition of the Work and Play Series, you’ll get hands-on experience with exactly the same tools being used to help solve complex problems and keep people safe and healthy.
Here’s what you’ll do in this week’s game:
Create reports. This is where the game gets more challenging. You’ll face a fictional scenario where you need to provide an analysis of real-world data. As you work through the labs, do you see any ways you might use this technology in your job, or your studies?
The top 10 winners will get an exclusive voucher code for the Qwiklabs perkstore.
Share your game badge on LinkedIn and tag your post with #Qwiklabs #WorkAndPlay and you may see yourself in an upcoming highlights reel. And, 10 random lucky winners who post will be chosen to receive an exclusive gift card with a code to get $75 worth of Qwiklabs credits, to help you continue learning (and maybe even earn a skill badge).
If you haven’t yet joined, what are you waiting for? Join this challenge today!
63 
2
63 
63 
2
"
https://medium.com/@sramana/what-follows-the-unicorn-carnage-8423783a89f0?source=search_post---------239,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sramana Mitra
Mar 22, 2016·3 min read
Predictably, the technology industry is going through an adjustment. Many public stocks in hot sectors like cloud computing have crashed. Overvalued Unicorns are experiencing down rounds, layoffs, and all the other unsavory stuff that they deserve. More Unicorns will turn into Unicorpses as the correction continues. You could say that the industry is in bad shape.
I disagree.
The industry, actually, is full of wonderful companies that offer robust value propositions and excellent business models.
It is the market and the speculators — including the VCs who invested in the Unicorns at crazy valuations — who ran amok.
Of course, you can’t put the entire blame on investors and let the entrepreneurs off the hook entirely. If you accept cocaine, you should have to deal with the consequences.
At any rate, the correction was necessary.
Sanity is welcome.
Let’s look ahead.
I have no doubt that new, exciting technologies will be brought about by new, exciting companies. There are two categories I am particularly keen to see unfold.
The first is in the realm of healthcare.
I am sick and tired of doctors trying to wing it when it comes to diagnosis and treatment. There are hundreds or thousands of symptoms, and about that many different drugs. Doctors seem to be fumbling with side effects of various drugs and how they interact in unison.
In effect, we expect our doctors to do a multivariate optimization of symptoms, drugs, side effects, cross effects, etc. in their heads.
It’s not possible for ALL doctors to do a good job of this ALL the time. They make lots of mistakes. They miss lots of issues. And the loser is the patient.
Instead, all this needs to be done in software.
Key in your medications, symptoms, test data, imaging, etc. and let the software figure things out.
The doctors need help, and the software that can provide this help may be complicated, but not THAT complicated.
There is work going on in this realm. I hope it moves fast. As a consumer, I am more than ready to use it.
Doctors, instead of being threatened, should welcome this new, infinitely scalable, unambiguously useful innovation.
The second is in the realm of finance.
I am not impressed by the wealth management industry. It seems to be yet another set of humans trying to do multivariate optimizations in their heads in real-time with the markets performing gymnastics in parallel. Not possible. Cannot yield great results. This stuff should be handled by software.
Yes, a new class of robo advisors has cropped up in the financial sector, but they have a ways to go still.
There is a third category that is becoming vital to the future of world economies:training and education, especially online, scalable education.
We’re going towards a highly unequal society where only people who have 21st-century skills can thrive. The rest will barely survive. That means, hundreds of millions of people will need to develop these skills rapidly, affordably, democratically.
We’ve seen the first wave of online education innovations already impact large numbers of people. A lot more will happen in this realm. A lot more needs to happen in this realm.
Overall, I am bullish about the REAL Unicorns — companies with REAL value propositions, REAL business models, REAL innovations.
Of the ones that have been hyped up without merit, I say, good riddance.
Photo credit: mollybob/Flickr.com.
Founder of the 1M/1M global virtual incubator
2 
2 
2 
Founder of the 1M/1M global virtual incubator
"
https://medium.com/idgtechtalk/get-started-with-your-data-center-network-redesign-76d994bfb84a?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Whether it’s building private clouds or leveraging public clouds, the race to the cloud is on.
Increasingly, enterprises are choosing multicloud. Cloud and multicloud are, at their core, an evolution of operations. Gone are the days when individual enterprise silos can be architected, deployed, and managed separately. The power of multicloud is about bringing the full enterprise network together as a cohesive entity.
And such change requires more than just products. Enterprises might start with technology, but their successful path to the cloud will be dependent on navigating the tooling and process implications of multicloud.
So, where to start? Look no further. Juniper Network’s data center network redesign kit is a multimedia experience that offers everything you need to know to get started now. The kit includes:
The Juniper data center network redesign kit is all about helping you prepare for the next step in IT:
Juniper Network’s strategy is centered on the transformation that the cloud is driving across all corners of the market. Its focus is on engineering simplicity for customers and partners and helping them in their transition to the cloud.
Earlier this year, Forrester Research listed Juniper as a leader in The Forrester Wave™: Hardware Platforms for Software-Defined Networking, Q1 2018. That finding was reaffirmed when Gartner named Juniper as a leader in the Gartner Magic Quadrant for Data Center Networking 2018.
Growing businesses choose Juniper for its vision of engineering simplicity. Juniper’s multicloud-ready solutions connect and protect your data center, campus, branch, and public cloud with consistent policy and control across your network.
Be prepared for the future. Access the Juniper data center network redesign kit today here.
This article was written by Tom Schmidt.
The #1 tech publisher.
1 
1 clap
1 
The #1 tech publisher. We report the hottest tech trends from leading experts & the biggest brands. Join us every Thursday, 12pm ET, for the #idgtechtalk chat.
Written by

The #1 tech publisher. We report the hottest tech trends from leading experts & the biggest brands. Join us every Thursday, 12pm ET, for the #idgtechtalk chat.
"
https://medium.com/@chicagosean/box-inc-box-to-be-added-to-tradeideas-holly-hot-list-7b26aa2b4801?source=search_post---------241,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sean McLaughlin
May 13, 2018·2 min read
This provider of cloud content management software came public in January of 2015. It promptly traded lower from Day 1 and hasn’t seen higher prices — until today.
HOLLY saw something early last week when she signaled a trade on April 30th, getting subscribers long at $23.14. HOLLY had a false start in $BOX back in March, but the beauty of machine-driven artificial intelligence is, she doesn’t bring any baggage along with her into new trade opportunities, un-emotionally tackling each edge in isolation, unaffected by past outcomes. This allowed her to stick with this name and if you got long at her last signal, your patience is being rewarded.
Even though the move began a week ago, it feels like this stock may just be getting started. Today marks it’s first official blue skies launch into new all times, and I couldn’t be more excited about an opportunity like this. The stock sees average daily volume north of 3 million shares and has a market cap now exceeding $3 Billion. And it has a short float of more than 7% of outstanding shares — every single one of whom is now underwater in their trades and may soon need to scramble to cover their short positions (that are exposed to theoretically unlimited risk), effectively adding tinder to a momentum fire. There’s so much to like about this situation if you’re a long swing trader.
As the tweet above mentioned, I’ll be adding this stock to the HOLLY Hot List at tomorrow’s opening print, hoping for a big move, but trailing the stock with a generous 20% stop loss for when momentum exhausts itself.
The market’s rise in the last two trading days has offered us the ability to raise several trailing stop loss levels in our positions (read: new High Prices!). The biggest leaders of the past 5 trading days have been $NTNX, $GDS, & $HFC (all of which set new high prices levels for us). The most notable laggard has been $NCLH which is sadly closing in our stop loss level.
If you’d like to learn how HOLLY, the Artificially Intelligent Virtual Analyst, can improve your trading and investing results, please visit Trade Ideas.
Good luck out there!
~ Sean McLaughlin (@chicagosean)
[originally published on chicagoseantrades.com on May 7, 2018]
Independent Stocks & Options Trader. Senior Market Strategist @ Trade Ideas. Chief Options Strategist @ All Star Charts. chicagoseantrades.com
See all (134)
Independent Stocks & Options Trader. Senior Market Strategist @ Trade Ideas. Chief Options Strategist @ All Star Charts. chicagoseantrades.com
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/google-cloud-launches-freely-accessible-covid-19-public-datasets-program-9fa72c3f7bf?source=search_post---------242,"Sign in
There are currently no responses for this story.
Be the first to respond.
B. Chen
Apr 2, 2020·7 min read
Data always plays a critical role in the ability to research…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cere-network/cere-to-launch-through-republic-dao-maker-and-polkastarter-after-80x-oversubscribed-private-sale-a1c8a7069470?source=search_post---------243,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cere Network
Apr 1, 2021·3 min read
Cere Network, the first Decentralized Data Cloud (DDC) platform, today announces the public offering on Republic, DAO Maker, and Polkastarter. In order to satisfy the demand for the first public offering of the $CERE token, Cere is offering access to the sale across three platforms, this multi-platform approach is a first-of-its-kind in order to satisfy the demand for the $CERE token, which will be the token’s first public offering, after its own Republic Note token sale.
Cere has previously raised $10 million from a collection of high-profile VCs and, including Binance Labs, OKEx, Republic Labs, Arrington XRP Capital, Kenetic Capital, Fenbushi Capital, and AU21 Capital, amongst others. The most recent $5m raise was 80x oversubscribed and closed ahead of its public launch on Republic. Launched in 2019, Cere is one of the few blockchain companies to count a number of Fortune 1000 companies as clients of their industry-leading decentralized CRM (Customer Relationship Management) tools and Decentralized Data Cloud platform. Cere Network is also supported by a consortium of VCs and ecosystem partners including Gate.io, ZB exchange, Ankr, Ledger Prime, Woodstock fund, JRR Capital, Spark Capital, LD Capital, NGC Capital, QCP Capital, Monday Capital, G1 Ventures, and others.
“We’re very excited to be able to offer the $CERE token for the first time to the public in such a unique and open way,” said Fred Jin, CEO of Cere Network. “This three-platform approach ensures that people can purchase our token in the environment they are most comfortable with. We’re also proud of having built a sustainable business and introducing real-world blockchain solutions to Fortune 1000 companies.”
The company was founded by former executives from Amazon, Twitch, D-Link, and Bebo, and counts the former COO of Salesforce, Rajani Ramanathan, as a member of their Advisory Board. Cere’s utility token is powering a transformative SaaS DeFi framework, fueling the Decentralized Data Cloud Platform, Cere Marketplaces, and SaaS-DeFi ecosystem, as well as being used for staking and governance.
The three platforms where the $CERE token is available are all unique in their own separate ways:
Republic: Republic is an investment platform that allows anyone to invest in private deals across startups, real estate, gaming, and crypto. Republic has facilitated over $300 million in investments through its global community of over one million members. Republic offers digital asset and token sale advisory through its Republic Advisory Services. For more information on how to participate in the CERE sale on Republic, click here.
DAO Maker: DAO Maker’s DAO Pad is a highly anticipated multi-investment platform that allows DAO Maker’s most loyal and diligent community members that stake DAO tokens to participate in a variety of public, private, and even seed rounds of funding for projects. Cere will be the first-ever project on DAO Pad. For more information on how to participate in the “Cere x DAO Maker” sale, click here.
Polkastarter: With Polkastarter, projects like Cere can raise capital from community-driven investors on a decentralized, permissionless, and interoperable platform. Polkastarter hosts initial DEX offerings (IDOs) for projects built on within the Polkadot ecosystem. Starting April 19th, participants interested in supporting Cere can access the CERE token in a secure and compliant environment that unlocks functionality way beyond the current ERC20 standard. For more information on how to participate in the CERE sale on Polkastarter, click here.
Over the last few weeks, the Cere team has been rewarding early supporters for their patience by introducing a series of unique access points to participants that have been whitelisted. These events have been in collaboration with renowned blockchain projects, including Polygon, Marlin, Ankr, Darwinia, Crust, and more.
About Cere Network:
Cere Network is the first Decentralized Data Cloud (DDC) platform in alignment with Polkadot, optimized for service data integration and data collaboration. While most enterprise blockchains are simply distributed ledgers, the Cere DDC platform is built from the ground up to power the new generation of first-party customer data ecosystems. Harnessing similar goals to cloud platforms like Snowflake, Cere’s DDC platform is delivering on a new decentralized level of data privacy, data agility, and data interoperability.
Cere Network is founded by former executives from Amazon, Twitch, D-Link, and Bebo. Cere is backed by Binance Labs, Arrington XRP Capital, Fenbushi Capital, and Neo Global Capital, amongst others. The company is headquartered in Berlin.
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
295 
1
295 
295 
1
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
"
https://virtualrealitypop.com/make-money-while-you-sleep-otoy-wants-to-rent-your-idle-computer-d2dc908aabb9?source=search_post---------244,"Otoy announced the opening of RNDR, a crowdsourced cloud based on blockchain, to the public. RNDR will reduce the costs of producing games and movies while putting Ethereum-based cryptocurrency in the wallets of those who lend the network their idle computer. RNDR’s peer to peer platform now enables the entertainment and games industry to drastically reduce the cost of creating those incredible, realistic scenes in shows like Westworld. An individual could theoretically make several hundred dollars a month.
You’ve probably never heard of Otoy because it works quietly in the background, helping to turn the work of artists, captured as code, into 60 frames per second of high-resolution 2D or 3D images through a process called “rendering”. Otoy adds a proprietary “secret sauce” of AI and machine learning to make the process faster and cheaper while eliminating “noise” or mistakes that sometimes mar the output. “Between AI rendering and de-noising and an ability to move objects around dynamically, we’re approaching being able to have full passive movies built with game engines,” Otoy’s co-founder and CEO Jules Urbach told Venturebeat in an interview at the Games Developer Summit in April.
This growth in demand for rendering services poses a problem referred to as “the GPU Cloud Gap”. An increasingly large amount of market segments are competing for limited resources, and this drives the price up. Centralized data centers that provide the necessary hardware (like Amazon Web Services) have been unable to expand fast enough to meet the demand and match competitive pricing.
The Otoy team has been thinking about ways to harness the countless millions of idle GPUs around the world to create a distributed, secure peer-to-peer network for years. When the Ethereum blockchain came along with decentralized security and currency needed for anonymous buyers and sellers, they were finally able to take what producers are already doing and making it better, cheaper, faster while generating substantial recurring revenue. Otoy maintains the underlying network, and RNDR users and content creators connect to each other within the system buying or selling computing power with Ethereum based “render tokens”.
Once a rendering job begins, a creator’s payment of RNDR tokens will be held in escrow until the job is completed. Throughout the rendering process, users can watch the status of a job and updates of the render, such as scene previews, or maximize the preview mode to see better details in the image. As the job progresses, RNDR token usage increases, and once the job is complete, frames can be downloaded and the token transfer occurs. “Customers are getting back their work and not knowing if Amazon did it or the decentralized nodes did it,” Urbach said.
Don’t spend that Ethereum you’re going to mint by lending your PC to RNDR just yet. RNDR uses a node matching and prioritizing protocol that searches for available rendering power. Depending on the circumstance you may not qualify for a job.
Today’s opening of RNDR to the public launches what the company calls Phase II of its development. Kalin Stoyanchev, Head of Blockchain/Project Lead of RNDR said, “Currently less than 1% of the world’s GPU power is accessible to creators. By harnessing the wasted, idle computing power of millions, with no expense in terms of hardware, we will have access to much more computing power, at a more affordable cost to creators, with the utmost security of their digital assets ownership.”
“RNDR’s next phase of development [Phase III] will be devoted to expanding its global partnerships and growing the platform to reach rendering-streaming through smart contracts and blockchain technology,” said Urbach. “Otoy is seeding the network with the key relationships we have in the industry, principally the major studios that use our technology and to more than ten million developers that use Unity and Unreal Engine.”
The privately owned Otoy was founded in 2008 by Urbach, Alissa Grainger (Co-Founder & President) and Malcolm Taylor (Co-Founder & CTO). Since then, Otoy has grown to over 60 employees across four offices with headquarters in Downtown Los Angeles, CA. The company’s investors include Autodesk and Yuri Milner’s Digital Sky Technologies (DST), and it boasts about a board of powerful advisers from Eric Schmidt of Google, to superagent Ari Emanuel of WME-IMG. The company declined to talk about other investors or its finances.
Those interested in making money while they sleep can log on to RNDR at https://rendertoken.com
This post was originally featured on Forbes.com on July 12, 2018
Virtual Reality, Augmented Reality, Oculus Rift & Go &…
69 
1
69 claps
69 
1
Written by
AR/VR Consultant, Columnist, Author of the AR-enabled books “Metaverse, A Guide to VR & AR” (2018) & “Convergence” (2019). http://forbes.com/sites/charliefink
Virtual Reality, Augmented Reality, Oculus Rift & Go & Quest, PSVR, HTC Vive, Gear VR, Steam, Daydream, 360 Video and more.
Written by
AR/VR Consultant, Columnist, Author of the AR-enabled books “Metaverse, A Guide to VR & AR” (2018) & “Convergence” (2019). http://forbes.com/sites/charliefink
Virtual Reality, Augmented Reality, Oculus Rift & Go & Quest, PSVR, HTC Vive, Gear VR, Steam, Daydream, 360 Video and more.
"
https://medium.com/quarkchain-official/quarkchain-participated-in-the-scientists-forum-cloud-summit-and-introduced-a-new-ledger-model-f20e4dbcc265?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
On December 7th, 2020, the Valued Public Chain Storage Sharing and Ecosystem Configuration Cloud Summit of the 17th China Scientists Forum was held. QuarkChain was invited to participate in the summit again. The summit was hosted and supported by Chinese Scientists Forum Organizing Committee, Business School of Chinese Management Science Academy, and Discovery Magazine, IPFS Drop, and Beijing Matou.org. Gui Chen, Dean of Business School of Chinese Management Science Academy, President of Discovery Magazine, and Chairman of the Chinese Scientists Forum, had a keynote speech. QuarkChain founder & CEO Dr. Qi Zhou attended the forum and gave a speech.
As the first sharing guest of the summit, Dr. Zhou gave a speech entitled “The Future of Public Chain”. Last time, QuarkChain scientist Professor Yaodong Yang provided a macro introduction of the blockchain industry. This time, Dr. Zhou started from the real practices of the blockchain system, taking QuarkChain as an example, and specifically explained the current mainstream blockchain application fields such as assets, DeFi, etc. He also introduced several important reasons that block the further development of the blockchain, such as network throughput, the composability of DeFi applications, etc. Therefore, the future public chain must have features such as: high throughput, low-cost asset issuance and operation, and high composability of applications. The current solutions mainly include: cross-chain/sharding/L2, solved some of the problems but generated new ones such as reduced composability caused by improved performance. Thus, high-performance single-chain technology is the only way for public chains in the future.
QuarkChain proposed a new high-performance single-chain technology. Under the condition that the system has 1 billion accounts, the goal is to provide a 1,000 TPS single-chain capability, with 500GB — 1TB storage requirement. Relying on sharding technology, the performance of the entire network will increase linearly as the number of chains increases. ​This is related to a new ledger model we proposed. QuarkChain can support multiple ledger mechanisms, consensus, virtual machines, and token economies in the same network, thus QuarkChain can use general KT storage instead of The Merkle Patricia Tree (MPT) of Ethereum, which can provide better support for the large-scale application of blockchain technology.
After Dr. Zhou’s sharing, two guests from the Currency Research Institute and Peer-to-Peer Technology shared layer2 technical analysis and the latest research results of Eth2.0, respectively. In the roundtable panel, all the guests discussed ETH2.0 and the public chain current status, sharing and communicating from the security of sharding technology, public chain applications, and the development of ETH2.0. They also illustrated their opinions about the national digital currency and the implementation of business models of blockchain scenarios.
The Chinese Scientists Forum was named by the famous scientist Guangzhao Zhou, Vice Chairman of National People’s Congress Committee and Chairman of Chinese Science and Technology Association. Since its establishment in 2002, the Chinese Scientists Forum has received strong support from the Ministry of Science and Technology, the Chinese Academy of Sciences, the Chinese Academy of Engineering, the Chinese Science and Technology Association, the National Natural Science Foundation, etc. It has become the most influential platform in China’s science and technology community. With the expandation of the forum’s scale, level, quality, and influence year by year, it has become a national technology brand activity with high participation, wide coverage, and great social influence by government, industry, academia and research. It has become a driving force for the development of China’s science and technology industry.
QuarkChain provides a secure, decentralized and scalable…
5 
5 claps
5 
Written by
https://www.quarkchain.io
QuarkChain provides a secure, decentralized and scalable blockchain solution to deliver 100,000+ on-chain TPS
Written by
https://www.quarkchain.io
QuarkChain provides a secure, decentralized and scalable blockchain solution to deliver 100,000+ on-chain TPS
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/top-network/top-network-partners-with-ontology-to-create-next-generation-blockchain-based-communication-network-c6940966df83?source=search_post---------246,"There are currently no responses for this story.
Be the first to respond.
Distributed cloud communication startup TOP Network and public blockchain project Ontology have launched a strategic partnership, aiming to cooperate in creating a blockchain-powered communication ecosystem open to all developers and apps.
Having succeeded in creating multiple communication apps with a large user base, TOP Network is building a communication ecosystem on blockchain. Ontology will provide the underlying blockchain technology for TOP and offer a comprehensive suite of services such as data exchange, data identification and high-performance smart contracts.
TOP Network has been developing a distributed cloud communication network that provides developers with secure, low-cost communication services. The company closed a $6 million Series A round earlier this month, investors including Fenbushi Capital, Danhua Capital (DHVC), NEO Global Capital and LD Capital.
Ontology, a new high-performance public blockchain project and distributed trust collaboration platform, offers next-generation high-performance infrastructure public chain with the complete distributed ledgers and smart contract systems. On the platform layer, Ontology provides general Dapp-supporting technologies including distributed identity framework and distributed data exchange protocol, creating an efficient, open platform for Dapp projects.
“The partnership between Ontology and TOP Network is mutually beneficial,” TOP Network CEO Steve Wei said. “It will provide strong momentum for the ecosystem development of both companies.”
The cooperation will improve the safety of the two ecosystems and drive down development costs significantly.
“The rapid development of communication network has gradually broken down the silos of information, bringing us closer to the goal of the Internet of Everything,” Ontology founder Jun Li said. “Blockchain technology can speed up the whole development process.”
Traditionally, developers have to design and implement communication protocols and deploy and maintain their own networks. This mode tends to slacken development progress, raise costs and increase the risk of security breach. Ontology, a high-performance, developer-friendly public chain platform, offers blockchain-based businesses a powerful array of infrastructure services including ONT ID and ONT DATA. These services will provide strong technical support for TOP Network. TOP Network will provide distributed cloud communication services for developers on Ontology, enabling developers to create high-performance communication and social apps easily and to reduce costs by utilizing spare resources on the network. TOP will also develop apps directly on Ontology, bringing a large number of users to the Ontology ecosystem.
TOP team has rolled out three communication apps: VoIP app Dingtone, encrypted messaging app CoverMe and VPN app SkyVPN. Those three products have a total of 50 million registered users and will be moved to the TOP ecosystem.
In addition to technical support, Ontology will back TOP Network up in fundraising, community operation, and ecosystem development. Ontology’s venture capital firm Ontology Global Capital (OGC) has announced a strategic investment in TOP Network.
TOP Network is a decentralized blockchain ecosystem…
83 
83 claps
83 
Written by
TOP Network is a decentralized blockchain ecosystem composed of public blockchain, DApps & decentralized communication. Website: www.topnetwork.org
TOP Network is a decentralized blockchain ecosystem composed of public blockchain, DApps & decentralized communication.
Written by
TOP Network is a decentralized blockchain ecosystem composed of public blockchain, DApps & decentralized communication. Website: www.topnetwork.org
TOP Network is a decentralized blockchain ecosystem composed of public blockchain, DApps & decentralized communication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/nutanix-clusters-hybrid-cloud-infrastructure-now-available-on-aws-govcloud-us-5675bc0442bd?source=search_post---------247,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Jul 2, 2021·10 min read
FedRAMP Authorized Nutanix Government Cloud Services helps to Enable U.S. Public Sector to Get to a Hybrid Cloud Environment
By Sherry Walshak and Sahil M Bansal
In this age of imperative digital innovation, the announcement that Nutanix Clusters cloud platform now supports the Amazon Web Services (AWS) GovCloud (US) region is a welcome choice for government customers. Nutanix Clusters is a component of the Nutanix Government Cloud Services, which has a FedRAMP authorization at the Moderate security impact level. Government agencies implementing Cloud Smart mandates can now consider another option to securely and seamlessly move their on-premises applications, sensitive data and regulated IT workloads to the cloud, and take advantage of rich cloud services. A built-in native integration with AWS networking makes the initial set-up very easy, allowing government IT teams to typically provision a hybrid cloud environment in less than an hour.
The case for using cloud capabilities in government has been clearly proven — to improve the quality and access of government services to its population; deliver mission-serving applications and services faster; support disaster recovery for critical applications; better secure sensitive systems and data; and drive operational efficiencies and cost savings.
A common question is how best to evolve existing IT environments? Customers have seen a blur of choices as well as overwhelming complexity in evaluating offerings. There is urgency in the government to move workloads to the cloud, and customers tell us it often feels like a “race to win”.
In August 2020, we announced the general availability of the Nutanix Clusters platform on AWS, which enables customers to extend data center capabilities natively into AWS public clouds (typically within one hour). This hybrid cloud solution manages applications and infrastructure in private and public clouds as a single cloud, greatly simplifying the IT environment. In addition, customers can choose the optimal cloud environment for each application based on their service or mission needs. After successfully deploying Nutanix Clusters in commercial AWS regions, Nutanix addressed the additional government requirements and has now announced its general availability on AWS GovCloud which is a component of the Nutanix Government Cloud Services.
Fig 1: Nutanix Clusters: Hybrid Cloud Infrastructure with AWS
Nutanix Clusters runs the core Nutanix hyperconverged infrastructure software stack (including Nutanix AOS, AHV and Prism software) and supports multiple Nutanix products and services on bare metal AWS EC2 instances with support for the following instance types (subject to regional availability):
Nutanix Clusters simplifies infrastructure and seamlessly migrates or extends any virtualized application across platforms — all under a single management console. This single user interface manages license portability across both private and all public cloud environments, eliminating the need for separate teams to manage each environment. This is great news for government IT teams struggling with limited resources and skill sets.
The recent Nutanix Enterprise Cloud Index study shows 87% of U.S. Federal respondents identify a hybrid cloud model of private and public clouds as the ideal IT operating environment for their organization. However, adoption has been slow with only 14% of respondents currently running a hybrid cloud environment. This is not surprising as many of these organizations are struggling with transitioning legacy and mission critical applications to cloud environments. This solution will help government agencies accelerate their journey to hybrid cloud, improving operations and performance.
When evaluating hybrid cloud solutions, most options typically require government agencies to enter into a vendor managed services contract on top of a cloud service provider. These options often present constraints around scope of management control, visibility into issue resolution, license portability from on-prem to public cloud and lack of direct billing with public cloud.
Nutanix Clusters on AWS GovCloud is a simple extension of your on-premise hyperconverged infrastructure to AWS GovCloud, which reduces the inherent complexity of delivering a consistent experience across your datacenter and AWS GovCloud. With Nutanix Clusters on AWS GovCloud, you directly manage your AWS GovCloud environment as you would any other datacenter, without the additional costs and limitations of a managed service. As your requirements change, you can easily shift software license entitlements from on-premise nodes to bare metal AWS EC2 instances without being locked into perpetual licensing. Unlike managed services where you are paying the 3rd party vendor for consumption of the entire solution, Nutanix Clusters (including Nutanix Clusters on AWS GovCloud) allows you to leverage your existing AWS accounts, contracts, discount structure, consumption and billing model. This provides greater control of the hybrid cloud infrastructure as well as more flexibility when migrating apps to AWS.
Fig2: Nutanix Clusters gives YOU the control and delivers YOUR Nutanix software in YOUR AWS GovCloud account
Nutanix Clusters on AWS GovCloud provides an important choice for Federal government customers who want to increase their AWS adoption while maintaining the freedom that a hybrid environment provides between their datacenter and public cloud.
AWS GovCloud (US) is an isolated region designed to allow U.S government agencies to move their confidential data into the cloud to address their compliance and specific regulatory requirements.
The Federal Risk and Authorization Management Program (FedRAMP) provides a standardized security framework for cloud products and services that are recognized by all executive branch federal agencies. Nutanix Government Cloud Services has successfully completed a full security assessment and authorization at a moderate security impact level of 325 controls. The FedRAMP Moderate control baseline equates to a DoD Impact Level 2.
The Nutanix Government Cloud Services are inclusive of other components enabling government customers to optimize their cloud spending, improve their security posture and support remote teleworkers.
Whether increasing capacity for Virtual Desktop Infrastructure (VDI) deployments, leveraging cloud capacity for disaster recovery to help ensure operations, or increasing developer productivity with a parallel dev/test workstream, government customers can seamlessly extend their Nutanix environments to AWS GovCloud and scale as needed. In working with our government customers, we find these typical key use cases:
“Penn National had been backing up everything to tapes and storing them offsite. If they ever had a disaster, it would have taken several days before all their systems were back online. Nutanix Clusters enabled them to complete all the testing for the AWS deployment in one day. They linked their on-prem data protection Cluster with their rapid recovery Nutanix Cluster on AWS, and in less than two hours, the desktops were on AWS. By scaling out their virtual desktop infrastructure into a Nutanix Cluster on AWS, Penn National was able to discontinue the expense of a secondary site, freeing up budget to be used for strategic initiatives.”
Fig3: Key Features of Nutanix Clusters on AWS GovCloud
Nutanix Clusters on AWS GovCloud has important features that support performance and availability, while minimizing cost. These include:
Our experts working with government customers in the trenches have these three insider tips:
“We know Federal agencies are looking for solutions to help them implement their Cloud Smart strategies, but existing offerings lack the necessary security or are cumbersome to deploy and manage,” said Chip George, VP Public Sector of U.S. Sales at Nutanix. “Nutanix Clusters on AWS GovCloud enables organizations to significantly accelerate their cloud adoption, without needing to re-architect mission critical applications that are necessary to the agency’s operations and mission, providing an easy path to a unified hybrid cloud environment.”
We welcome your feedback and look forward to accelerating your hybrid cloud journey!
© 2021 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned on this post are registered trademarks or trademarks of Nutanix, Inc. in the United States and other countries. Other brand names mentioned on this post are for identification purposes only and may be the trademarks of their respective holder(s). This post may contain links to external websites that are not part of Nutanix.com. Nutanix does not control these sites and disclaims all responsibility for the content or accuracy of any external site. Certain information contained in this post may relate to or be based on studies, publications, surveys and other data obtained from third-party sources and our own internal estimates and research. While we believe these third-party studies, publications, surveys and other data are reliable as of the date of this post, they have not independently verified, and we make no representation as to the adequacy, fairness, accuracy, or completeness of any information obtained from third-party sources.
This post may contain express and implied forward-looking statements, which are not historical facts and are instead based on our current expectations, estimates and beliefs. The accuracy of such statements involves risks and uncertainties and depends upon future events, including those that may be beyond our control, and actual results may differ materially and adversely from those anticipated or implied by such statements. Any forward-looking statements included in this post speak only as of the date hereof and, except as required by law, we assume no obligation to update or otherwise revise any of such forward-looking statements to reflect subsequent events or circumstances.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/aergo/pre-announcing-the-aergo-public-token-distribution-event-cf94b5c2fe84?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
Hello AERGO community,
To begin, we would like to thank you all for your outstanding support over the past few months. We’ve received significant interest in our upcoming token generation event (TGE), from loyal individual supporters to large strategic-value-adding organizations.
Moreover, we’ve received substantial support from existing clients and integration partners of our strategic technology partner, Blocko. Many enterprises that plan to help grow the AERGO ecosystem and build the future of the decentralized economy have shown interest in our cause. We understand that things have been quiet over the past few months. This post will outline the information you all have been waiting for.
AERGO envisions a world where radically reshaped trust dynamics create new socioeconomic networks and digitally transform existing ones. Since 2014, we’ve had a first-hand look at how powerful even minimally distributed systems — those based on permissioned blockchain setups — truly are.
We envision a world where radically reshaped trust dynamics create new, open socioeconomic networks; and digitally transform existing ones.
Most of the team behind AERGO comes from Blocko, South Korea’s largest blockchain infrastructure provider. Blocko has developed tailor-made permissioned blockchain products for large enterprises such as Samsung, Hyundai, Kia Motors; and some of South Korea’s largest financial services companies such as Shinhan Bank and Lotte Card.
We now propose AERGO — a fourth generation blockchain platform taking:
…and supercharging them with a team that perhaps has the greatest blockchain and open-source enterprise IT know-how in the world.
We’re looking to take it to the next level, by first understanding the needs of enterprise customers and their associated IT technology partners. We believe that the transformation begins by first understanding enterprise needs, tackling their obstacles, and guiding them forward. Once businesses start to shift their architecture and create new business models around this technology, decentralized economies will form, and the age of the internet of value will truly begin.
As such, one of the major tasks we undertook over the past months was to connect with many global customers around the world. We briefed them extensively on what we’re planning on building.
The endorsement of AERGO’s vision was amazing. The feedback we’ve received is also now very clear.
What these companies would like to see happen next is the following:
This last point is the one to pay attention to. It’s the reason behind our upcoming public token distribution event.
The AERGO project is a serious project — created by not only one the most capable teams of blockchain and open-source developers — but it now also has serious long-term strategic backers behind it. We will always be truthful to our cause — our customers — and our stakeholders.
This includes our community of over 30,000 members. To this effect, and to remain transparent, we would like to provide a summary of the private token sale for the AERGO TGE.
We set ourselves a target of selling AERGO Tokens at a price of US$0.20 with a hard cap of US$30,000,000 USD. In our private token sale, we sold 144,151,452 AERGO tokens and raised US$28,832,654. Tokens will be released over a twelve month period, starting at the upcoming token generation event. (Update 11/18/2018: We raised $30,000,000)
We met all our private sale funding objectives. We are both honored and grateful for the trust shown by our backers in AERGO, our technology, and our team. Despite closing our funding round in early August, we have received over 550 screened additional inbound requests to help us and to back our project directly. The private token sale was oversubscribed by a factor of over twenty times.
As stated earlier, our future customers and their key technology providers have made it very clear what they want from us. To this effect, we are planning to announce a number of initiatives in the coming months to directly invest in the various communities that underpin all our key stakeholder groups:
We will announce details about each such initiative shortly. In the meantime, we will focus on our most important and most immediate campaigns for our general community and our geographic enablers.
Most public token distribution events work as crowdsales. We’ve considered the idea of selling AERGO utility tokens to our general community in this way, though we feel doing this is perhaps not the best way forward for our project.
If our community is an essential component to what we’re trying to achieve here, why would we not simply directly invest in this community ourselves?
Yes, we’re going to invest in you.
To distribute network tokens to those that will end up using the platform, all tokens allocated to our public token distribution event will be rewarded to our most value-adding supporters.
We’re rewarding over US$3,000,000 worth of tokens to our community, with careful consideration on who we grant tokens to to make this event as impactful to our ecosystem as possible. The tokens we distribute will act as rewards for value-adding tasks completed in building the long-term vision of AERGO. This public token distribution event will cater to our most supportive individuals and potential regional partners.
We plan to allow a significant portion of our community to participate in this program. The rewards are not only impactful — they are potentially huge — with up to US$50,000 worth of AERGO token rewards being made available to the highest contributing members of the community
This is not proof of shill. This is not for people operating bots; this is not for people who don’t care about the long-term future of AERGO. Instead, this is for those that understand what we are aiming to achieve and want to help.
Very shortly, we’re going to announce more regarding the following:
This RewardDrop pre-announcement is subject to terms and conditions outlined here.
Moving forward, we’re going to announce specific plans regarding the AERGO community plan. This includes similar community investment plans into our open-source community, our technology providers, and our platform users. Expect more messaging from us on what AERGO truly is — which we will incorporate in our public token distribution event screening process.
We’re keeping our long-term vision in mind in every single step we take. We feel that to achieve our vision of reshaping the digital economy, this is the right thing to do.
If you’re reading this post, you’re invited to join us on our journey to create the most powerful open network in existence.
Be sure to join us on our social media channels to stay updated:
We recommend being in these channels and understanding the project to the best of your ability before the event kicks off, to ensure eligibility.
Keep watching this space, and expect more announcements very shortly.
— Team AERGO
This post is available in Korean here.
Enterprise-ready, open-source hybrid blockchain and…
9.92K 
8
9.92K claps
9.92K 
8
Enterprise-ready, open-source hybrid blockchain and easy-to-use serverless cloud platform for building practical decentralized apps and business solutions.
Written by
Non-Executive President at Aergo
Enterprise-ready, open-source hybrid blockchain and easy-to-use serverless cloud platform for building practical decentralized apps and business solutions.
"
https://medium.com/@alexfclayton/history-of-public-saas-returns-and-valuations-67e49935a34c?source=search_post---------249,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Alex Clayton
Jul 11, 2019·9 min read
Since Salesforce went public in 2004, there have been almost 70 other pure-play SaaS/cloud companies that have followed them in the public markets, and more and more go out each year. In 2018 alone, which was a banner year for SaaS IPOs, there were 16 high-growth SaaS companies filing S-1s. Many private SaaS companies believe that an IPO and/or reaching $100M in revenue or ARR (annual recurring revenue) is the end state when in reality SaaS companies demonstrate staggering returns in the public markets well beyond $100M in revenue. The United States economy, as well as the global economy, have recovered dramatically since the 2008/2009 Great Recession, but SaaS companies on average have outperformed the broader market by a massive margin. The two “SaaS Crashes” post-Great Recession have occurred in early 2014 and early 2016, and apart from those two time periods, it’s been mostly up and to the right. Given all the recent IPOs and high trading multiples, it’s interesting to look at the historical performance of each public SaaS company.
Most high-growth SaaS businesses (public and private) are valued on a multiple of forward revenue with enterprise value over NTM (next-twelve-months) as a primary metric. What is the right price for a high-growth public SaaS company? The all-time average has been ~8x but more recently the weighted NTM revenue multiple for all high-growth SaaS companies is ~15x, which has been propelled by recent IPOs like Zoom (ZM), CrowdStrike (CRWD), and Slack (WORK) which are trading at high NTM revenue multiples — ZM at 41x, CRWD at 34x and WORK at 28x (as of 9-July-2019). In the recent week or so, the high-growth SaaS weighted multiples have finally eclipsed the Q1'2014 highs.
Given where we’re at in the market cycle — which seems like a 10+ year SaaS bull run — I wanted to dig into the trading history of each pure-play SaaS/cloud IPO and how they have traded from inception → today. For the purposes of this analysis, I left out companies like Microsoft and Adobe and only included high-growth, pure-play SaaS/cloud companies that have gone public in the past 15 years — Salesforce is the first company in this comparable set. I looked at all 70 SaaS companies that have filed S-1’s publicly and tracked the trading history from IPO → today (or when they were acquired). 67 of them made it to trading as AppDynamics, Adaptive Insights, and Qualtrics were acquired days before trading.
Below are logos of all the companies used in this analysis (sorted by IPO date from left to right):
The returns for these 70 or so companies have been extraordinary, particularly for the companies that are still independent. Below is a chart showing the multiple returns over IPO price for the 51 independent companies. The average return is 5.3x, and on average this group of companies has only been trading for only 3.4 years. The IPO month and year are below each ticker.
What about for the 16 companies that have been acquired post-IPO? The average return for those businesses has been 2.8x, lower than the independents. See the chart below which outputs the multiple returns over IPO price for the acquired businesses.
The average return for all 67 pure-play SaaS/cloud companies that have traded publicly is 4.7x from IPO price as of 9-July-2019.
It’s clear that going public is just a milestone for the vast majority of SaaS companies as they accrue significant value in the public markets. With that said, we’re in a bull market and valuation multiples have expanded significantly over the past few years (more on that below), but the numbers are the numbers. In aggregate this group of companies has created ~$640B of cumulative market cap in the past 15 years. Some of these companies have been acquired and their market cap is no longer in this data or has been subsumed by others — for example, Salesforce acquired MuleSoft — but it’s gone from ~$2B in 2004 to $640B+ today, an increase of almost 400x from its initial market cap.
Taking a look at cumulative NTM (next-twelve-months) revenue for this same group, it’s gone from ~$200M to $55B+ today, an increase of over 300x. The number increased almost 3x over the past 3 years alone. Again, this includes acquired companies that have been subsumed by others in the comparable set or taken out completely.
Valuation
I also wanted to dig into the valuation multiples for this same set of companies. Given most SaaS companies are valued on a multiple of forward revenue (NTM or next-twelve-months), EV or enterprise value / NTM revenue multiples are a great way to look at valuations over time. The below chart shows market-cap weighted NTM revenue multiples over the past 15 years along with the average and median. As I mentioned before, we have just eclipsed the historical 2014 highs in the past week or so, which has largely been driven by large and fast-growing companies like Zoom, Slack, and CrowdStrike. The historical average market-cap weighted NTM revenue multiple has been 7.6x.
As I mentioned, the returns of this group of companies over the past 15 years have been massive. The chart below indexes the market cap weighted indexed stock price return for every SaaS/cloud company that has traded publicly (starting from their first close →today or acquisition) which comes out to 1,100%+. For reference, the Dow Jones, Nasdaq, and S&P 500 indexes are also included in the chart. Public SaaS companies have vastly outperformed those indexes and I can’t think of any other asset class of this size to have such high returns over the past 15 years.
There have been some outliers— I went and looked at the individual performance of the companies in this group that had share price returns of ~10x+ from their IPO price — it includes six companies; Shopify, ServiceNow, Paycom, Salesforce, Zendesk, and Twilio. As you can see in the table below, these companies have demonstrated incredible returns and revenue growth rates for sustained periods of time. For example, Salesforce has grown revenue at almost a 40% CAGR for 15+ years. Shopify, the company with the greatest share price return from IPO price, has been growing at a 72% CAGR. Revenue growth has been the largest factor in returns for these businesses.
Taking a deeper look into their respective market caps over time, the below chart has indexed market cap from the first day of trading. Each one of these six companies closed their first day of trading ~$1–3B in market cap and have continued to grow. Salesforce, which has been trading for 15 years is now at ~$120B in market cap. Interestingly, the other five companies are accruing value faster than Salesforce did in their earlier years. It’s not totally fair since Salesforce was trading during the Great Recession (the others came after), but an interesting visual. The month and year each company started trading are in parentheses.
On the indexed stock price returns for these same six companies, all of them are also accruing returns faster than Salesforce did. Salesforce has grown their stock price ~3,500% since their first day’s close of $4.30 in June of 2004, and the more recent public companies in Shopify, ServiceNow, Paycom, Zendesk, and Twilio are growing even faster. The month and year each company started trading are also in parentheses.
Another interesting view of these six companies is looking at their respective share price and NTM revenue multiples over time. I outputted each company’s trading history and plotted share price and NTM revenue multiples for each trading day since they've been public.
Shopify (SHOP)
Shopify’s stock price is up 18.9x since IPO and their multiple has been climbing steadily, and more recently the relationship between their multiple and stock price has been more closely related. They are growing revenue the fastest out of the six companies.
ServiceNow (NOW)
On the other hand, ServiceNow was actually trading at a higher multiple in 2012 than they are today. Their stock price is up 16.6x since IPO. More recently, and similar to Shopify, their multiple and stock price have been more tightly coupled.
Paycom (PAYC)
Paycom’s stock is up 16.0x since IPO and they have grown both their multiple and stock price steadily since they went public in 2014.
Salesforce (CRM)
Salesforce, the longest-trading and up 14.0x from their IPO price, has had a more consistent NTM revenue multiple. They dipped significantly in 2008/2009 but have not reached the 15–20x+ NTM revenue multiple levels of some of the other companies.
Zendesk (ZEN)
Zendesk’s stock price is up 10.4x from their IPO price and similar to ServiceNow, previously traded at a higher multiple than they are at today.
Twilio (TWLO)
Twilio is also similar. Their stock price is up 9.7x from IPO price and was previously trading close to 20x NTM revenue shortly after their IPO and now is ~15x. Their stock price continues to rise ahead of their multiple more recently.
The overall returns from public SaaS companies have been remarkable — the average SaaS company has returned almost 5x from their IPO price. For context, most venture capital funds strive to return 3x+ funds over a 10-year fund lifecycle, whereas if you had a dollar in every SaaS IPO you’d be up 4.7x as of 9-July-2019, on an average timeframe of fewer than 4 years. The past 10 years in particular has been an exceptional time for public SaaS companies in many regards, and we’re now above the 2014 NTM revenue multiple highs — on average investors are paying higher multiples today for SaaS companies than at any other time in public SaaS trading history. At the same time, there are companies like Zoom that are growing revenue 100%+ YoY while profitable, at almost $500M in run-rate revenue. No one knows what will happen at the macroeconomic level over the next few months and years (the current bull market has to slow down at some point), but it’s never been a better time to be a high-growth public SaaS company (or investor!). For those companies that do get public, their investors and employees are being rewarded handsomely and it’s clear that going public isn’t the end goal, but just the beginning.
To receive these posts by email, click here.
General Partner at Meritech Capital
See all (87)
779 
8
779 claps
779 
8
General Partner at Meritech Capital
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/redpoint-ventures/a-look-back-at-q1-public-saas-earnings-e392a7f6f4f8?source=search_post---------250,"There are currently no responses for this story.
Be the first to respond.
Q1 earnings season for Cloud businesses is now behind us. The 54 companies that I’ll discuss here (which is not an exhaustive list, but is still fairly comprehensive) all reported quarterly earnings sometime between April 29th — June 9th. In this post I’ll take a data-driven approach to evaluate the overall group’s performance, and highlight individual standouts along the way. As a venture capitalist at Redpoint Ventures, I’ll try to cater my analysis through the lens of a private investor. Over my time at Redpoint, I’ve had the opportunity to meet with hundreds of entrepreneurs who are all building special companies. Through these interactions, I’ve built up mental benchmarks for metrics on which I place extra emphasis. My hope is that this analysis can provide startup entrepreneurs with a framework for how to manage their business around SaaS metrics (e.g., net retention and CAC payback).
Public SaaS Valuations
The last couple months have been extremely volatile for technology (particularly SaaS) stocks. We saw the Vix (an index that measures volatility) hit an all-time high of 82, surpassing its previous max of 80 during the financial crisis of 2008. We saw the Nasdaq quickly drop ~30%, only to fully recover and hit all-time highs last week in a dramatic V-shaped recovery. We saw businesses like Zoom trade at >60x NTM revenue, exceed quarterly expectations by >60%, and nearly double year-end revenue guidance. And finally, the tech IPO markets showed their strength when ZoomInfo successfully priced its IPO to raise ~$1Bn. This represented the largest software IPO in the last decade, and one of the highest revenue multiples for a tech IPO over the same time period. Overall, the basket of 54 SaaS stocks analyzed in this post have created roughly $300Bn in market cap this year alone. As a retail investor, investing equally across this basket of companies at the beginning of the year would have yielded a 37% return year-to-date (while the S&P is down 1% and the Nasdaq is only up 9%). To many, this might seem surprising given the current state of the country, given the struggle for racial justice, the tragedy of Covid-19, and our current high unemployment rates.
So what’s driving the massive tide lifting SaaS stocks? There isn’t one obvious answer, but rather a confluence of factors. First, there are a number of industries that are significantly affected by Covid-19 — travel, hospitality, casinos & gaming, retail, restaurants, etc. Large hedge funds and institutional investors have rotated out of positions in these industries and need somewhere to park this money. What better place than in business with highly predictable SaaS revenue streams, many of whom are seeing Covid tailwinds as digital transformations are pulled forward. Companies like Zoom, Datadog, and Fastly have all seen demand for their products spike. Furthermore, the unit economics of these SaaS businesses are incredibly attractive. Companies like Twilio, with high net retention of 143% (a metric I’ll get into more later), can afford to add 0 new customers over the next year and still grow 43% annually. This is astounding, and something no other industry can claim. The lifetime value of each customer (the value of all future cash flows from that customer) is often multiples greater than the cost to acquire them. The combo of predictable revenue streams, high (and in some cases accelerating) revenue growth, and attractive unit economics have made SaaS an incredibly attractive market throughout these turbulent times. The interesting nuance, of course, is that not all cloud businesses have benefited equally. In our current environment, the high-growth category leaders have seen disproportionate gains compared to their lower growth, “value stock” counterparts. Zoom, Crowdstrike, Bill.com, Twilio, Coupa, Fastly, Zscaler and others have all blown past pre-Covid levels to hit all-time highs in the last couple weeks. But Microsoft, Adobe, Intuit, New Relic and other value stocks have either struggled to nudge past pre-Covid highs or haven’t completely rebounded yet. Overall, this flight to quality has lifted cloud businesses, and compounded to lift the top cloud businesses even more. The bigger question remains what will happen when the economy stabilizes and large institutional investors rotate back into the abandoned industries like travel and hospitality. Only time will tell if the “Covid-bump” for cloud businesses will last!
Q1 Big Winners
Coming out of Q1 there were a few businesses that in my mind stood out as “Big Winners.” These companies showed a combination of incredibly strong results, robust outlooks, and attractive unit economics. The public markets rewarded all of them handsomely. I’ll take a closer look at each of these businesses in the sections below.
Valuation Metrics
Before getting into the Q1 data, it’s important to frame the conversation around valuations. Just about all cloud companies are valued off of a multiple of their revenue. That is, their Enterprise Value / NTM (Next Twelve Months) Revenue. We use forward revenue estimates as a company’s future outlook determines its worth. As you can see from the chart below, we’re currently at all-time-high revenue multiples and valuations, albeit in the middle of a global pandemic. Median forward revenue multiples for all cloud businesses currently sits right around 12x, and the median of the top 5 multiples has expanded the most.
Doubling clicking one layer further, we see high-growth businesses have seen the greatest multiple expansion since Covid. In the below chart I define High Growth as >30% projected NTM growth, Mid Growth as 15%-30% and Low Growth as anything <15%.
Q1 Results Relative To Consensus Estimates
Beating consensus revenue estimates are the first aspect of a successful quarter. So what are these consensus estimates and who creates them? Every public company has a number of equity research analysts covering them who build their own forecasted models, which combine guidance from the company and their own research / sentiment analysis. The consensus estimates are the average of all the individual analysts’ projections. Generally when you hear “consensus estimates” it refers to revenue and earnings (EPS), but for the purpose of this analysis we’ll just be looking at revenue consensus estimates (as this is the metric these companies are valued off). For every public company the expectation is that they’ll beat consensus estimates, because companies often guide research analysts to the lower end of their internal projections. They do this to set themselves up to consistently beat estimates, demonstrating momentum. Cisco, for example, famously beat earnings expectations for 43 straight quarters in the 1990s.
It’s also important to note that when a company is providing guidance for the “next quarter,” they are (in some cases) already halfway through that quarter due to the timing of earnings calls. By then, they generally have a good sense for how the quarter is going and can guide ever so slightly under their internal projections. As the data shows below, the median “beat” of quarterly consensus estimates was ~4% in Q1.
As you can see from the data below the vast majority of cloud businesses beat the consensus estimates for Q1.
As you can see from the data above, 94% of businesses beat their Q1 revenue consensus estimates. The median beat was 3.9%, a top quartile beat was anything >6.3%, and a top decile beat was anything >9.6%. And as you can see from the data, the results are nearly identical from companies with March and April quarter ends.
So how did our “Big Winners” perform in Q1 relative to consensus estimates? Bill.com, Datadog, Twilio and Zoom were all top decile performers. Crowdstrike, Fastly and Shopify were top quartile. Zscaler was above median.
Next Quarter’s Guidance Relative to Consensus Estimates
Guiding above next quarter’s consensus revenue estimates is the second factor of a successful quarter. Generally, companies will give a guidance range (e.g., $95M — $100M), and the numbers I’m showing are the midpoint. Providing guidance that is greater than consensus estimates is a sign of improving business momentum, or confidence that the business will perform better than previously expected. The concept of guiding higher than expectations is considered a “raise.” When you hear the term “beat and raise” the beat refers to beating current quarters expectations (what we discussed in the previous section), and the raise is raising guidance for future quarters (generally its annual guidance, but for this analysis we’re just looking at the next quarters guidance).
As you can see from the data below, there was a difference in the percentage of raises from March to April. My guess here is that there was even more uncertainty around the pandemic’s effects in March, and because of this companies were more conservative in their guidance. They were preparing for the worst. A month later two things happened. Future estimates had more time to be revised down, and companies had a much better sense for the pandemic’s impact on their business. Therefore they didn’t feel the need to guide so conservatively (with a bigger cushion). Here’s the data:
As you can see from the data above, 40% of businesses guided above their Q2 revenue consensus estimates. The overall guidance outlook median came in 1% under consensus estimates. A top quartile guidance raise was anything >1.9%, and a top decile guidance raise was anything >7.4%. Of the companies that provided a guidance raise, the median raise was 3.6%.
So how did our “Big Winners” perform in Q1 relative to consensus estimates? Crowdstrike, Datadog, Fastly, Twilio and Zoom were all top decile performers. Bill.com was barely outside the top decile and Zscaler was above the median.
Growth
Demonstrating high growth is the third aspect of a successful quarter. This metric is more self-explanatory so I won’t go into detail. The growth shown below is a year-over-year growth for reported quarters. The formula to calculate this is: (Q1 ’20 revenue) / (Q1 ’19 revenue) -1.
As you can see from the data above the median growth rate was 31%, a top quartile growth rate was anything >42%, and a top decile growth rate was anything >53%.
So how did our “Big Winners” perform in Q1? Bill.com, Crowdstrike, Datadog, Twilio and Zoom were all top decile performers. Shopify was in the top quartile, and Fastly and Zscaler were above the median.
Net Revenue Retention
High net revenue retention is the fourth aspect of a successful quarter, and one of my favorite metrics to evaluate in private SaaS companies. It is calculated by taking the annual recurring revenue of a cohort of customers from 1 year ago, and comparing it to the current annual recurring revenue of that same set of customers (even if you experienced churn and that group of customers now only has 9, or anything <10). In simpler terms — if you had 10 customers 1 year ago that were paying you $1M in aggregate annual recurring revenue, and today they are paying you $1.1M, your net revenue retention would be 110%. The reason I love this metric is because it really demonstrates how much customers love your product. A high net revenue retention implies that your customers are expanding the usage of your product (adding more seats / users / volume — upsells) or buying other products that you offer (cross-sells), at a higher rate than they are reducing spend (churn). Here’s why this metric is so significant: It shows how fast you can grow your business annually without adding any new customers. As a public company with significant scale, it’s hard to grow quickly if you have to rely solely on new customers for that growth. At $200M+ ARR, the amount of new-logo ARR you need to add to grow 30%+ is significant. On the other hand, if your net revenue retention is 120%, you only need to grow new logo revenue 10% to be a “high growth” business.
I’ve looked at thousands of private companies, and over time have come up with benchmarks for best-in-class, good, and subpar net revenue retention. Not surprisingly, these benchmarks match up relatively well with the numbers public companies reported. I generally classify anything >130% as best in class, 115% — 130% as good, and anything less than 115% as subpar. For businesses selling predominantly to SMB customers, these benchmarks are all slightly lower given the higher-churn nature of SMBs. I consider >120% best in class for companies selling to SMBs (like Bill.com). Here’s the data from Q1 earnings:
As you can see from the data above, the median net revenue retention rate was 117%, a top quartile net revenue retention rate was anything >122%, and a top decile net revenue retention rate was anything >130%. The one point to note: not all companies report this number. It’s fair to assume that the majority of companies who don’t report this metric probably fall into the subpar category. Because of this, the median, top quartile, and top decile numbers mentioned above probably are better than reality.
So how did our “Big Winners” perform in Q1? Datadog, Fastly, Twilio, Zoom and Bill.com were all top decile. I’m including Bill.com here because 120% net retention selling to SMBs is phenomenal. Crowdstrike was in the top quartile, and Zscaler was above the median.
Sales Efficiency: Gross Margin Adjusted CAC Payback
Demonstrating the ability to efficiently acquire customers is the fifth aspect of a successful quarter. The metric used to measure this is my second-favorite SaaS metric (behind net revenue retention) — Gross Margin Adjusted CAC Payback. It’s a mouthful, but this metric is so important because it demonstrates how sustainable a company’s growth is. In theory, any growth rate is possible with an unlimited budget to hire AEs. However, if these AEs aren’t hitting quota and the OTE (base + commission) you’re paying them doesn’t justify the revenue they bring in, your business will burn through money. This is unsustainable. Because of the recurring nature of SaaS revenue, you can afford to have paybacks longer than 1 year. In fact, this is quite normal.
All that said, Gross Margin Adjusted CAC Payback is relatively simple to calculate. You divide the previous quarter’s S&M expense (fully burdened CAC) by the net new ARR added in the current quarter (new logo ARR + Expansion — Churn — Contraction) multiplied by the gross margin. You then multiply this by 12 to get the number of months it takes to pay back CAC.
(Previous Q S&M) / (Net New ARR x Gross Margin) x 12
A simpler way to calculate net new ARR is by taking the current quarter’s ARR and subtracting the ending ARR from one quarter prior. Similar to net revenue retention, I’ve built up benchmarks to evaluate private companies’ performance. I generally classify any payback <12 months as best in class, 12–24 months as good, and anything >24 months as subpar. The public company data for payback doesn’t match up as nicely with my benchmarks for net revenue retention. The primary reason for this is that public companies can afford to have longer paybacks. At $200M+ ARR, businesses have built up a substantial base of recurring revenue streams that have already paid back their initial CAC. Their ongoing revenue can “fund” new logo acquisition and allow the business to operate profitably at paybacks much larger than what private companies (with smaller ARR bases) can afford.
Most public companies don’t disclose ARR (and when they do, it’s often not the same definition of ARR as we use for private companies). Because of this we have to use an implied ARR metric. To calculate implied ARR I take the subscription revenue in a quarter and multiply it by 4. So for public companies the formula to calculate gross margin adjusted payback is:
[(Previous Q S&M) / ((Current Q Subscription Rev x 4) — (Previous Q Subscription Rev x 4)) x Gross Margin] x 12
Here’s the payback data from Q1. Not every company reports subscription revenue, so they’ve been left out of the analysis.
As you can see from the data above, the median gross margin adjusted payback was 29 months, a top quartile gross margin adjusted payback was anything <20 months, and a top decile gross margin adjusted payback was anything <13 months.
So how did our “Big Winners” perform in Q1 relative to consensus estimates? Crowdstrike, Datadog and Zoom were all top decile performers. Bill.com and Twilio were top quartile. Fastly and Zscaler were above median.
Change in Share Price
At the end of the day what investors care about is what happened to the stock after earnings were reported. The stock reaction alone doesn’t represent the strength of a company’s quarter, so the below data has to be viewed in tandem with everything discussed above. Oftentimes the buy-side expects a company to perform well (or poorly), and the company’s stock going into earnings already has these expectations baked in. In these situations the stock’s earnings reaction could be flat. However, it’s still a fun data point to track.
What I’ve shown below is the market-adjusted stock price reaction. This means I’ve removed any impact of broader market shifts to isolate the company’s earnings impact on the stock. As an example, a day after SurveyMonkey reported earnings their stock was up 9%. However the market (using the Nasdaq as a proxy) was up 2% that same day. This implies that even without earnings SurveyMonkey would likely have been up 2%. To calculate the specific impact of earnings on the stock we need to strip out the broader markets movement. To do this we subtract the market’s movement from the stocks movement: (% Change in Stock) — (% Change in Nasdaq)
Wrapping Up
This quarter has been a wild ride for SaaS businesses. As a group they’ve performed quite well during these volatile times in the broader market, and in that sense the future looks bright.
Here’s a summary of the key stats for each category we talked about, and how the “Big Winners” performed. Excluding the share price reaction, Datadog and Zoom are the only two companies to perform at Top Decile levels across every metric and are my Elite Performers of Q1! Hopefully this provides a blueprint for every entrepreneur out there reading this post.
The Data
The data for this post was sourced from public company filings, Wall Street Research and Pitchbook. If you’d like to explore the raw data I’ve included it below. Looking forward to providing more earnings summaries for future quarters! If you have any feedback on this post, or would like me to add additional companies / analysis to future earnings summaries please let me know!
Redpoint partners with exceptional entrepreneurs starting at the earliest stages of the climb
684 
7
684 claps
684 
7
Since 1999, Redpoint Ventures has partnered with visionary founders to create new markets and redefine existing ones.
Written by

Since 1999, Redpoint Ventures has partnered with visionary founders to create new markets and redefine existing ones.
"
https://towardsdatascience.com/bigquery-public-datasets-936e1c50e6bc?source=search_post---------251,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yufeng G
Jun 13, 2018·3 min read
The only thing better than data is big data! But getting your hands on large datasets is no easy feat. From unwieldy storage options to difficulty getting analytics tools to run over the dataset properly, large datasets can lead to all sorts of struggles when it comes to actually doing something useful with them. What’s a data scientist to do?
On this episode of AI Adventures, we’re going to check out the BigQuery Public Datasets and explore the amazing world of open data!
…there’s a 1 TB per month free tier, making getting started super easy.
We all love data. Preferably the more the merrier! But as file sizes grow and complexity increases, it is challenging to make practical use of that data.BigQuery Public Datasets are datasets that Google BigQuery hosts for you, that you can access and integrate into your applications.
This means Google pays for the storage of these datasets and provides public access to the data via your cloud project. You pay only for the queries that you perform on the data. Moreover, there’s a 1TB per month free tier, making getting started super easy.
Looking at the BigQuery public datasets page, we can see there are nearly 40 public datasets. Each dataset in turn has many tables. Thousands of queries from hundreds of projects from all over the world are making use of these vast public datasets.
You can find answers to your most pressing questions about images on the web
What’s really neat is that each of these datasets comes with a bit of explanatory text that helps you get started with querying the data and understanding its structure.
For example, here’s the New York City Tree Census. The page shows us how we can easily find answers to questions like “What are the most common tree species in New York City?” and “How have tree species changed since 1995 in New York City?”. These are all accessible by literally one click from the docs page which opens right into the BigQuery interface!
Another dataset that is quite amazing is the Open Images Dataset. It contains approximately 9 million URLs and metadata for images that have been annotated with labels spanning more than 6,000 categories!
You can find answers to your most pressing questions about images on the web, like “How many images of a trolleybus are in the dataset?” (Spoiler alert: It’s over 3000!)
But I digress. BigQuery Open Datasets is a great way to explore public data and practice your data analysis skills. Combined with tools like Cloud Datalab, Facets, and TensorFlow, you could do some really awesome data science. So what are you waiting for? Head on over to the public datasets page and let your analysis run wild!
For more details and examples, check out BigQuery’s public datasets documentation page and start querying away!
Thanks for reading this episode of Cloud AI Adventures. If you’re enjoying the series, please let me know by clapping for the article. If you want more machine learning action, be sure to follow me on Medium or subscribe to the YouTube channel to catch future episodes as they come out. More episodes coming at you soon!
Applying machine learning to the world. Developer and Advocate for @googlecloud. Runner, chef, musician. Opinions are solely my own.
583 
3
583 
583 
3
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@datapath_io/elastic-ip-static-ip-public-ip-whats-the-difference-8e36ac92b8e7?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Datapath.io
Dec 23, 2016·4 min read
AWS network infrastructure can become complex.
Between integrating a content delivery network (CDN), network optimizer, or dynamic content accelerators, structuring VPCs and instances takes strategic planning.
Within AWS, having Elastic IPs can simplify your infrastructure and make making adjustments easier. With Elastic IPs, your changing environments and business requirements mean you need a way to easily adjust your setup.
That is why knowing what is an Elastic IP, and what are static and public IPs, can help make better decisions in designing your AWS setup.
Elastic IP addresses are used by AWS to manage its dynamic cloud computing services. Within the AWS infrastructure, customers have virtual private clouds (VPCs). Within the VPCs, users have instances. The Elastic IP address is what is used to advertise the data within the instance to the public internet.
If the discussion ended there, you would assume there isn’t much difference between a public IP address and an elastic IP address. That is why we need a bit more explaining about what an Elastic IP address is for.
Within the AWS cloud environment, AWS states that the Elastic IP is used for dynamic cloud computing. The distinction here is important. If an instance goes down within AWS, you want to maintain your IP address, as well as maintain communication with your AWS account.
Thus, an Elastic IP is a combination of a public IP address and a static IP address. It allows you to continue to advertise AWS instances within your AWS network infrastructure.
But, to fully understand the elastic IP, we should understand static IP addresses and public IP addresses.
Your internet service provider (ISP) provides you an IP address. This advertises your computer to the internet.
An IP address is like your home postal address, which is a piece in a larger city block. A city block, in internet language, is a network prefix. To go one step further, network prefixes are part of autonomous systems (AS). When connecting people to the internet, ISPs will assign an IP address to your computer.
The IP address that is assigned to you is most likely a dynamic IP address. This is the most common for residential customers. This means that the IP address changes frequently, which provides customers and ISPs cost savings. In the case of Germany, Deutsche Telekom is typically changing IP addresses every day.
The alternative to this system is the static IP address. This is an IP address that never changes. If you are running a dynamic cloud environment, this would be necessary for advertising your content to the public internet. This is a primary function of AWS Elastic IPs, which contain the static IP address component.
A static IP is useful for various reasons. In cloud computing, a static IP address is advantageous for DNS queries. If IPs are changing, this can affect the content loading process.
Static IP addresses are IPs which do not change. They are common for business and cloud computing, which is why AWS includes this within the Elastic IP framework.
A Public IP address is how the internet identifies you. A public IP address is the IP address that communicates your internet connected device to the public internet.
There is the ability to have a public IP address or a private IP address. A private IP address is for a private network. These are less common, and with your AWS infrastructure, not going to be in your interest when discussing Elastic IPs.
An Elastic IP has the public IP address component, as you need to advertise your AWS instances to the public internet. As the AWS Elastic IP will advertise a public, static IP address that can compensate for fluctuations in your AWS infrastructure, they are necessary in dynamic cloud computing.
The debate between what IP address is right for your business comes down to necessity.
What are you trying to accomplish?
We have said this in a few other networking posts, and the common thought for network design is: “it depends”.
Having an understanding of the different IP address structures available to you can help you make better decisions.
With your AWS Elastic IP address, you have all the requirements for dynamic cloud computing.
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
314 
4
314 claps
314 
4
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/create-and-sell-nfts-on-dshop-powered-by-origin-and-google-cloud-6bd59b66d82?source=search_post---------253,"There are currently no responses for this story.
Be the first to respond.
Non-Fungible Tokens (NFTs) are capturing the public imagination, unlike anything to come previously from cryptocurrency technology. NFTs are often characterized simply as digital collectibles, e.g. cryptocurrency versions of baseball cards, but there are many other interesting use cases for NFTs.
In this post, I will introduce you to the world of NFTs and teach you how to create and sell your own NFTs using the Dshop e-commerce platform by Origin Protocol. As a partner of Google Cloud Marketplace, Origin is excited to introduce this new blockchain technology to developers and creators using Google Cloud infrastructure.
Non-Fungible Tokens (NFTs) are cryptocurrency tokens that are unique or scarce. When something is fungible, it can be easily substituted by an equivalent. For example, a $5 bill can be substituted for another $5 bill or five $1 bills.
In contrast, NFTs can be unique objects, or 1-of-n objects, meaning only n of that object exists. NFTs have been adopted by the NBA and other sports leagues to represent digital trading cards that have video highlights attached to them.
NFTs are also used to represent ownership of physical objects, especially scarce objects such as limited-edition sneakers or fine art. Another use case is NFTs as access tokens to digital content or real-life experiences. An NFT holder could unlock exclusive or private content on the web or redeem their token as a concert ticket. One important aspect of NFTs is that future revenues or royalties to the creator of the token from sales of the NFT on the secondary market can be programmed into the NFT itself.
Origin Protocol has facilitated two high-profile NFT sales, also known as “drops”. In early 2020, we helped Brave Software, maker of the popular Brave Browser, switch from Shopify to Origin’s Dshop platform for their merchandise store. Brave now hosts their store on a decentralized platform that accepts their native cryptocurrency. Brave held a meme creation contest with their community in early 2021 and created or “minted” 30 NFTs in the form of three 10-of-10 NFTs based on the three most popular entries. This collection of meme NFTs was then listed on their Dshop for sale. All 30 NFTs sold out nearly instantly, showing there is an extremely strong demand for NFTs from their community and demonstrating a completely new form of interaction between software companies and their users.
In March of 2021, Origin broke the record for the highest-grossing NFT sale at that time when a tokenized music album was put up for auction. The sale brought in nearly $12 million and made headlines in industry and mainstream publications, including Forbes, Billboard, and Business Insider. Several different collections of NFTs with unique use cases were sold during this auction. All of these NFTs are freely tradeable and transferable by their owners.
More recently, Origin announced NFT sales with Grammy Award-winning hip hop artist Lupe Fiasco and Internet sensation and professional boxer Jake Paul. We plan to power many more NFT sales on our commerce platform in the future.
To get started, we have to set up a Dshop on Google Cloud Platform. The process is fairly straightforward but requires a few steps. We provide both a written guide and a video guide illustrating how to do this:
You can read more about how Dshop works with Google Cloud Platform here.
After setting up your own Dshop, it’s time to create or “mint” your NFTs. You’ll need a Web3-enabled wallet like MetaMask, which is an extension to your browser. You will also need some ETH in your wallet to pay Ethereum network gas fees. Minting NFTs is free on OpenSea, which is a popular NFT platform. A full guide on how to mint NFTs for free on OpenSea is available here and it is important to note that OpenSea primarily supports the ERC-1155 standard. If you want an ERC-721 NFT, you can mint one on Mintable, another NFT platform, for free using this guide. ERC-721 is the older and most established standard for NFTs but ERC-1155 supports more token types and batch transfers. The NFTs you mint are held in custody in your MetaMask wallet and you can transfer them however you like. They are not attached to your OpenSea or Mintable collections or stores.
After you have minted your NFTs, you can create listings for them on your Dshop. To learn how to do this, you can refer back to the video guide, starting at 3 minutes into the video:
www.youtube.com
We recommend putting a link to the NFT (usually in the form of an Etherscan link) in the description for your Dshop NFT listing so potential buyers can verify its existence and scarcity. Alternatively, you can post the link to the NFT from OpenSea or Mintable or another platform. A buyer will be able to purchase your NFT from your Dshop listing and complete payment with ETH or ERC-20 tokens on Dshop. The buyer will enter their Ethereum wallet address as part of the checkout flow. You will then need to transfer the NFT to them via OpenSea or Mintable to complete the sale. You can do this simply by selecting the NFT and clicking “transfer” and inputting the destination wallet address of your buyer. This transfer will require paying a gas fee to the Ethereum network and is a separate transaction from the payment from the buyer. Congratulations, you are now a successful NFT creator and seller!
Conclusion
We hope you enjoyed this guide on how to get started with NFTs. As with any new technology, there is a bit of a learning curve involved. Thankfully, early adoption of decentralized technologies like NFTs is both fun and rewarding and we encourage you to give it a try.
Learn more about Origin
Google Cloud community articles and blogs
875 
1
875 claps
875 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Cofounder Origin, early YouTube, avid traveler
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/syncedreview/googles-tpu-chip-goes-public-in-challenge-to-nvidia-s-gpu-78ced56776b5?source=search_post---------254,"There are currently no responses for this story.
Be the first to respond.
Google announced this morning that its Tensor Processing Unit (TPU) — a custom chip that powers neural network computations for Google services such as Search, Street View, Google Photos and Google Translate — is now available in beta for researchers and developers on the Google Cloud Platform.
The TPU is a custom application-specific integrated circuit (ASIC) tailored for machine learning workloads on TensorFlow. Google introduced TPU two years ago, and released the second generation Cloud TPU last year. While the first generation TPU was used in inferencing only, the Cloud TPU is suitable for both inferencing and machine learning training. Built with four custom ASICs, Cloud TPU delivers a robust 64 GB of high-bandwidth memory and 180 TFLOPS of performance.
Before it opened its TPUs to the public, Google had widely implemented them internally. AlphaGo — the Google AI masterpiece that beat human champions in the ancient Chinese board game Go — used 48 TPUs for inferencing.
Cloud TPU provides a great solution for shortening the training time of machine learning models. Google Brain team lead Jeff Dean tweeted that a Cloud TPU can train a ResNet-50 model to 75% accuracy in 24 hours.
When Cloud TPU was announced, Google offered 1000 free devices for machine learning researchers. Lyft, the second-largest ride-hailing company in the US, has been using Cloud TPUs in its self-driving systems since last year. Says the company’s Head of Software Self-Driving Level 5 Anantha Kancherla, “Since working with Google Cloud TPUs, we’ve been extremely impressed with their speed — what could normally take days can now take hours.”
Alfred Spector, CTO of New York City-based hedge fund Two Sigma, says, “we found that moving TensorFlow workloads to TPUs has boosted our productivity by greatly reducing both the complexity of programming new models and the time required to train them.”
Google’s Cloud TPU is currently only in beta, offering limited quantities and usage. Developers can rent Cloud TPUs at the rate of US$6.50/Cloud TPU/hour, which seems a reasonable price considering their great compute capability.
Google also released several model implementation tools to save developers’ time and effort writing programs for Cloud TPUs, including ResNet-50 and other popular models for image classification, a transformer for machine translation and language modeling, and RetinaNet for object detection.
While Google is not directly selling its TPU chips to customers at this stage, their availability represents a challenge to Nvidia, whose GPUs are currently the world’s most-used AI accelerator. Even Google has used large numbers of Nvidia GPUs to provide accelerated cloud computing services. However if researchers switch from GPUs to TPUs as expected, this will reduce Google’s dependency on Nvidia.
Last year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30–80 times improvement in TOPS/Watt measure. In machine learning training, the Cloud TPU is more powerful in performance (180 vs. 120 TFLOPS) and four times larger in memory capacity (64 GB vs. 16 GB of memory) than Nvidia’s best GPU Tesla V100.
Although it’s too early to crown the Cloud TPU as the AI chip champion, the announcement of its availability has sparked excitement among researchers, and marks the beginning of Google’s ambitious move into the space of AI accelerators.
Journalist: Tony Peng| Editor: Michael Sarazen
We produce professional, authoritative, and…
134 
2
134 claps
134 
2
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nodejs/intel-builds-better-benchmarking-standards-of-node-js-performance-in-public-and-private-clouds-1e24c3228d9a?source=search_post---------255,"Sign in
There are currently no responses for this story.
Be the first to respond.
Node.js
Apr 25, 2018·2 min read
“Node.js enables developers to bring innovation to market in a very fast way. You don’t have to spend months, you can have your app rolled out in a matter of days. Time to market is an unbeatable value proposition.” — Monica Ene-Pietrosanu , Software Engineering Director with Intel
Intel is one of the original members of the Node.js Foundation, joining when the project launched in 2015. Even before the beginning of the Node.js Foundation, Intel actively contributed to the Node.js Project (Intel is the number two contributor to V8, just behind Google). In recent years, the team has expanded its contribution, especially in the areas related to performance benchmarking as Node.js becomes more common in the backend.
We interviewed Intel’s Monica Ene-Pietrosanu for our new video series on “How Companies Benefit from Node.js and the Node.js Foundation.” The title of the series should give you some indication of what we talked about.
Monica Ene-Pietrosanu is a Software Engineering Director with Intel, leading a team of performance engineers and compiler experts in optimizing runtimes for the most popular scripting languages used in data center and Cloud workloads. She’s been a software engineer for the last 20 years, and regularly speaks on many topics from leading software engineering teams to cloud and data center optimization to diversity in tech.
In our interview with Monica, she provides the reasons why her team at Intel is contributing to Node.js, how the group plays an active role in the Node.js Foundation, and how all of this benefits Node.js developers.
Reference key to the efforts Intel is supporting within the Node.js community and more:
We will roll out more videos and stories in this series throughout the year. You can also check them out on the Node.js Foundation YouTube page here.
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
183 
183 
183 
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
"
https://medium.com/@cere-network/closing-the-cere-private-sale-and-opening-up-the-cere-public-sale-whitelisting-c7ae289f27cd?source=search_post---------256,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cere Network
Jan 15, 2021·3 min read
Cere Network, the world’s first Decentralized Data and Finance Cloud platform built on Polkadot, has successfully finalized its private sale round. Today, we’re entering a new, exciting phase for the majority of the Cere community; the start of the Cere public sale process! Keep reading to learn more about whitelisting for the Cere public sale, and how you can even still qualify to invest in Cere at a special private sale price.
We’re extremely grateful for all the inquiries and support of our fantastic community, with over $80M in requested allocation by over 6000 potential investors! After months of hard work and hundreds of due diligence/investor talks, Cere is now closing its highly successful, multiple times oversubscribed private sale. The Cere team chose the investors with whom we had the most synergy, and who could offer the most strategic value for Cere Network. More details on the amount of raised funds, the Cere tokenomics, and future plans will be released very shortly.
“This private round shows the confidence in the project from the wider crypto community and validates the vision we’re executing on enterprise data and SaaS-DeFi. We are humbled to have received such overwhelming demand from investors,” said Cere CEO and co-founder Fred Jin. “It’s great for the team to see this much support from our community. This entices us, even more, to over-deliver on our ambitious roadmap, with the launch of our alpha mainnet as the first big milestone in Q1 2021. “
Unfortunately, it’s impossible for us to reach out individually to all of our supporters that filled in the private sale form. Some of you may be bummed out on missing out on the private sale, where they have put a lot of effort in their motivation and engagement over the past weeks. The Cere team recognizes these efforts and is excited to announce that private sale prices for those who submitted interest are still within reach.
We realize the importance of community contributions and we are working on a community engagement program to ensure that we reward our long-term supporters and engaged community members via special private sale allocations. We’ll release more info on this program in the coming weeks. But first, we have a number of designer related community incentives to push Cere’s reach even further. Because as we all know, meme’s rule the world, and with Cere, we want to have the best memes out there.
For now, we invite every single one of our community members to whitelist themselves via the whitelist form behind the button below.
Don’t wait too long, since we’ll be sending out special newsletters to all whitelisted individuals starting next week!
ABOUT CERE NETWORK
Cere Network is the first Decentralized Data Cloud (DDC) platform in alignment with Polkadot, optimized for service data integration and data collaboration. While most enterprise blockchains are simply distributed ledgers, the Cere DDC platform is built from the ground up to power the new generation of first-party customer data ecosystems. Harnessing similar goals to cloud platforms like Snowflake, Cere’s DDC platform is delivering on a new decentralized level of data privacy, data agility, and data interoperability.
Cere Network is founded by former executives from Amazon, Twitch, D-Link, and Bebo. Cere is backed by Binance Labs, Arrington XRP Capital, Fenbushi Capital and Neo Global Capital, amongst others. The company is headquartered in San Francisco, with offices in New York and Berlin.
Find out more about Cere here: Cere.Network . Join the company Telegram here: t.me/cerenetwork_official.
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
437 
437 
437 
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
"
https://medium.com/avalancheavax/deloitte-leverages-avalanche-to-improve-recoveries-from-natural-disasters-and-public-health-4fa3fd3644bf?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
New cloud-based platform leverages Avalanche blockchain to improve security, speed and accuracy of Federal Emergency Management Agency reimbursements.
Deloitte has formed a strategic alliance with technology firm Ava Labs to enable a new disaster recovery platform that uses the Avalanche blockchain to help state and local governments easily demonstrate their eligibility for federal emergency funding.
Developed with the input and insight of first responders, public works departments, finance authorities and grant-making agencies, the new Close As You Go™ (CAYG) platform helps state and local government officials simplify and streamline disaster reimbursement applications to the Federal Emergency Management Agency (FEMA).
“When disaster strikes a community, state and local officials must act swiftly and deliberately to respond,” said Alex Haseley, principal, Deloitte & Touche LLP and Deloitte’s government and public services crisis management portfolio leader. “Our new Close As You Go platform can play a critical role in helping these leaders be prepared to aggregate and validate the documentation necessary to demonstrate eligibility for funding and reduce the risk of adverse audit findings down the road.”
Using the Avalanche blockchain, CAYG’s cloud-based platform provides state and local officials with a decentralized, transparent and cost-efficient system that empowers both grant makers and funding recipients while minimizing fraud, waste and abuse.
The secure environment provided by Avalanche’s blockchain technology quickly gathers, processes and authenticates required documentation, ultimately improving the accuracy of federal disaster claims. Avalanche is also an eco-friendly platform, which aligns with Deloitte’s commitment to green technology.
“Close As You Go features a user-friendly interface backed by the cutting edge of blockchains, helping state and local governments focus on their recovery, rather than extensive claims processes,” said John Wu, president of Ava Labs. “We’re proud to work closely with Deloitte to offer this new, secure and beneficial technology for communities across the country.”
Key CAYG capabilities include:
See here for more information about Close As You Go.
Deloitte provides industry-leading audit, consulting, tax and advisory services to many of the world’s most admired brands, including nearly 90% of the Fortune 500® and more than 7,000 private companies. Our people come together for the greater good and work across the industry sectors that drive and shape today’s marketplace — delivering measurable and lasting results that help reinforce public trust in our capital markets, inspire clients to see challenges as opportunities to transform and thrive, and help lead the way toward a stronger economy and a healthier society. Deloitte is proud to be part of the largest global professional services network serving our clients in the markets that are most important to them. Building on more than 175 years of service, our network of member firms spans more than 150 countries and territories. Learn how Deloitte’s more than 345,000 people worldwide connect for impact at www.deloitte.com.
Avalanche is the fastest smart contracts platform in the blockchain industry, as measured by time-to-finality, and has the most validators securing its activity of any proof-of-stake protocol. Avalanche is blazingly fast, low cost, and green. Any smart contract-enabled application can outperform its competition by deploying on Avalanche. Don’t believe it? Try Avalanche today.
Website | Whitepapers | Twitter | Discord | GitHub | Documentation | Forum | Avalanche-X | Telegram | Facebook | LinkedIn | Reddit | YouTube |
Ava Labs makes it simple to launch decentralized finance applications on Avalanche, the fastest smart contracts platform in the blockchain industry. We are empowering people to easily and freely digitize all the world’s assets on one open, programmable blockchain platform. Ava Labs was founded by Cornell computer scientists who brought on talent from Wall Street to execute their vision. The company has received funding from Andreessen Horowitz, Initialized Capital, and Polychain Capital, with angel investments from Balaji Srinivasan and Naval Ravikant.
Blazingly Fast, Low Cost, & Eco-friendly
386 
386 claps
386 
Avalanche is the fastest smart contracts platform in the blockchain industry, as measured by time-to-finality, and has the most validators securing its activity of any proof-of-stake protocol.
Written by
Avalanche is the fastest smart contracts platform in the blockchain industry, as measured by time-to-finality. Don’t believe it? Try an app on Avalanche today.
Avalanche is the fastest smart contracts platform in the blockchain industry, as measured by time-to-finality, and has the most validators securing its activity of any proof-of-stake protocol.
"
https://medium.com/@auchenberg/introducing-remote-debugging-of-node-js-apps-on-azure-app-service-from-vs-code-in-public-preview-9b8d83a6e1f0?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kenneth Auchenberg
May 17, 2018·5 min read
Finding and identifying issues with Node.js apps deployed to the cloud can be burdensome process that usually involves local debugging, the sprinkling of console.logson to your codebase, and many re-deployments to get the problem identified and solved.
Today we are changing that, as we are introducing a public preview of remote debugging for Node.js apps deployed on Azure App Service for Linux. Our remote debugging experience brings you the same great debugging experience that you already know from Visual Studio Code when debugging Node.js locally to the Azure Cloud.
Remote debugging for Azure App Service works by taking advantage of a new Azure mechanism that allows us to forward the remote debugging information from your Node process running in Azure to your local computer in a secure way. Once the connection is established we can take VS Code’s built-in Node.js debugger to attach and debug — just like if you were debugging your Node app locally!
You can read more about how it works, and other Azure App Service announcements here on the Azure blog.
To get started you first have to prepare your Node.js app running in Azure and setup Visual Studio Code. Let me show you how.
This section assumes that you already have your Node.js app deployed to Azure using Azure App Service for Linux. If you haven’t deployed your app you can get started here or using the Azure CLI here.
In order to enable remote debugging when your app is running in the cloud, you might need a make a slight modification to your Node app, depending on how you start your app.
Per our default logic for Node Apps on Azure, we’ll try to auto-detect how you start your Node app, if you use bin/www server.js app.js hosting.js or index.js as your main script file. If that’s the case, there’s some good news: You don’t have to change anything!
If you are using NPM scripts to start your Node app, you need to make a slight modification to how you start your app, as we won’t try to fiddle with your app. You know best how to launch your app, so for this public preview, we are asking you explicitly to create a new debugging specific NPM script that runs your Node app with --inspect=0.0.0.0:$APPSVC_TUNNEL_PORT.
This runtime flag tells your Node app to start in debug mode listening on the debugging port specified by Azure, which is exposed as an environment variable.Your scripts section in your package.json should look something like:
2. The next step is to go to Azure Portal, and find your deployed Node.js app on Azure App Service (Linux).
4. Go to Application settings and update your Startup File to your newly configured npm run start_azure_debug script.
It should look something like:
Your Node app is now figured to run with remote debugging enabled.
Yay! 🎉🔥
Next is to get your Visual Studio Code setup going, which is an easy process:
3. If you haven’t logged into Azure from VS Code, you should now click the Azure icon in the sidebar to the left, and login to see your App Service apps.
4. Since remote debugging is a preview feature, you now have to go to your VS Code settings to enable it. You do this by clicking File > Preferences > Settings. modify your appService.enableRemoteDebugging to be true.
5. Now that you have remote debugging enabled, the next step for you is to open the source code for your Node app as your workspace in VS Code.
6. Once you have the source code open next is to find your deployed Node app in the Azure App Service list within VS Code.
Right click and select the new “Start Remote Debugging” option.
Once clicking on “Start remote debugging”, Visual Studio Code will check if remote debugging has been enabled for your app, and if it hasn’t you will be asked to confirm before enabling remote debugging on your behalf.
Once the right configurations has been set, you should now see remote debugging for your Node.js app being started (you can follow the progress in the status bar) and once the debugger is connected VS Code will enter debug mode.
Notice: You might get a prompt from your firewall on Windows, please allow the connection
Bam. That’s it. 🎉🔥
The debugger is now connected, and you can remote debug your Node.js app running in Azure! Try set a breakpoint!
In the March 2018 release of VS Code, we introduced a new debugging concept called Logpoints. When combining Logpoints with remote debugging on Azure you have a powerful combination for seamless production debugging!
Try it out and read more about Logpoints here 👉https://code.visualstudio.com/updates/v1_22#_logpoints
We are excited to bring you real remote debugging for Node.js apps running on Azure App Service (for Linux) directly from Visual Studio Code. Giving you a simply integrated experience that allows you to easily debug and diagnose problems if they occur when running your Node.js apps in our Cloud.
We’d love to hear your feedback: Please find us in GitHub to report issues, provide suggestions, or tell us your success stories.
Happy coding!
/k
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
263 
1
263 
263 
1
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
"
https://medium.com/@the_economist/michael-dell-plots-his-return-to-the-public-market-e12c7e3f5634?source=search_post---------259,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Economist
Dec 10, 2018·7 min read
There are two ways to make money selling technology, goes an old industry saying: unbundling and bundling. Thanks to the spectacular rise in recent years of cloud-computing services from Amazon and Microsoft, many firms have shifted chunks of software…
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/building-serverless-python-data-apis-with-dockers-on-google-cloud-24d4f15cf81?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
May 2, 2020·6 min read
In this article, I’ll show you a simple way to build in minutes a few Data APIs for exploiting data from a BigQuery dataset. These APIs will be deployed with dockers using a GCP serverless service called Cloud Run.
The idea behind is to work with serverless components. First, let’s understand these services…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alexfclayton/public-saas-software-founder-ceo-benchmarking-equity-age-comp-ffa2bc5df155?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Clayton
Aug 30, 2018·5 min read
Scaling a software company from founding to IPO is no easy feat, and if the business raises equity capital, it’s quite dilutive for founders and employees. I benchmarked a group of ~60 SaaS/cloud/software IPOs and looked at a few things — if at IPO the founder or co-founder was still the CEO, equity ownership pre-and post-IPO, age of CEO, and compensation.
Note that in many cases founders/CEOs sell part of their ownership stake prior to going public in secondary offerings during the fundraising lifecycle, or during the IPO, so the post-IPO equity stakes represented below are on average lower than if they had just been diluted down by more new equity. Many have sold into the IPO too among this group. Moreover, founders and employees experience additional dilution from option pool expansion.
Firstly, of this group of companies, in 36 out of the 61 companies (59%) the original founder or co-founder was still the CEO at IPO. The other 25 (41%) had a non-founder CEO at the time of IPO. This comparable set includes the following companies:
Alteryx, Adaptive Insights, AppDynamics, AppFolio, Appian, Apptio, Atlassian, Avalara, BlackLine, Box, Carbon Black, Cloudera, Cornerstone OnDemand, Coupa, Cvent, Demandware, DocuSign, Domo, Dropbox, Five9, Eloqua, ExactTarget, Fleetmatics, HubSpot, Instructure, LogMeIn, Marketo, MINDBODY, MongoDB, MuleSoft, NetSuite, New Relic, Okta, OPOWER, Paycom, Paylocity, Pivotal, Pluralsight, Q2, SuccessFactors, Rally, Responsys, RingCentral, Salesforce, SendGrid, ServiceNow, Shopify, Smartsheet, Splunk, Tableau, Talend, Tenable, Twilio, Veeva, Wix, Workday, Workiva, Yext, Zendesk, Zscaler, and Zuora.
Founder vs Non-Founder CEO at IPO (Total of 61 Companies)
Now take a look at post-IPO equity ownership for the CEOs of this group. After the IPO, the median ownership for a CEO is 6%. The blue bars indicate founders that were still the CEO at IPO, and the red bars indicate CEOs that were *not* founders at IPO. Not surprisingly, on average founders that are still CEOs at IPO have almost 3.5x more equity (after the IPO) than CEOs that were not founders. The output below has each company’s CEO equity ownership stake post-IPO.
CEO Percentage Ownership Post-IPO
Taking a closer look, below is an output of only founder CEOs (from blue bars above). The median ownership is ~12% after the IPO.
Founder CEO Percentage Ownership Post-IPO
Now below is an output that just shows non-founder CEO ownership (the red bars from first chart), and as expected, their respective post-IPO equity ownership is much lower than founder CEOs. The median here is ~5%.
Non-Founder CEO Percentage Ownership Post-IPO
Given IPOs are another form of fundraising, CEOs take additional dilution during the IPO and in some cases, sell into the IPO. Below is an output of both pre and post-offering ownership medians and averages, as well as the implied IPO dilution. This includes all 61 companies.
Pre and Post-IPO CEO Equity Ownership Averages / Medians
How old are CEOs when their companies IPO? The median age for a CEO is 46 years old at the IPO quarter. Founders CEOs are also younger than non-founder CEOs when their company goes public; founder CEOs were 44 and non-founder CEOs were 51, both on a median basis. Additionally, for non-founder CEOs, they’re on average the CEO for 5 years before the IPO.
CEO Age at IPO
With regard to compensation, the numbers vary significantly but below is an output with both annual salary, equity, non-equity and other comp, and total compensation in the IPO year. It’s worth calling out that 3 CEOs did not have a salary or it was at $1/year; Joshua Coates at Instructure, Marc Benioff at Salesforce, and Jay Choudary at Zscaler. Two out of the 61 companies had co-CEOs (Atlassian and Workday). Dan Springer also shows up twice on the list as the CEO of DocuSign and Responsys. The output below shows medians and averages.
Annual CEO Compensation in IPO Year
No two software companies follow the same path from founding → IPO, and there is significant variation within this group of 61 companies. It’s also a moving target as 10+ were added to the comparable set this year alone. With that said, this should shed some light on what a CEO could expect to own by the time his or her company goes public, which is ~12% for founder CEOs and ~5% for non-founder CEOs. Raising equity capital is expensive and most companies need to sell a large portion of their businesses to reach IPO scale but can create significant value by doing so — Salesforce — the largest company in the comparable set, has a market cap of almost $115 billion at the time of this post. It had a market cap of $1.1B at IPO almost 15 years ago.
To receive these posts by email, click here.
General Partner at Meritech Capital
201 
201 
201 
General Partner at Meritech Capital
"
https://medium.com/hackernoon/three-patterns-for-an-effective-cloud-native-development-workflow-6f59525f5bf1?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
Many developers are moving towards “cloud native” development, whether that is taking advantage of the services and convenience of public cloud, or deploying services on their own internal cloud. However, the new architectures and technologies that are emerging as part of the cloud native development space — microservices, containers, orchestrators — require new developer workflow patterns.
In this article I will introduce three patterns that I have found useful as I’ve learned about working with cloud technologies over the past several years.
The ability to define cloud infrastructure as code and provision this on-demand has been revolutionary in the way we deploy software. However, although the initialisation of the infrastructure is fast, it is typically not instantaneous (as you might want, say, in a TDD cycle). This means that developers who require provisioning of infrastructure in order to complete a build and deploy cycle often do not get the fast feedback they require. This can lead to task/context switching becoming a problem. The solutions to this include simulated local development infrastructure, re-usable remote infrastructure, and local-to-production development.
The simulated local development infrastructure pattern can be seen with AWS SAM Local. This tool provides a CLI and Docker-based replication of a production serverless environment in order to enable the efficient local development of AWS Lambda functions that use associated AWS services such as Amazon API Gateway and DynamoDB. This tool can be further augmented with service virtualisation (covered below) to simulate services, and in-memory data stores and middleware, for example LocalStack, which can be used to provide simulations of AWS services like Kinesis and SQS.
The re-usable remote infrastructure pattern is often implemented in a bespoke fashion, with platform teams provisioning multiple test environments that can be leased on-demand by engineers. Typically the configuration and corresponding state (data stores) are reset when the lease is completed, which make this ready for use by the next developer. The open source Kubernaut tool also provides the same experience for Kubernetes, and maintains a collection of initialised clusters that can be leased on-demand.
The local-to-production development pattern is arguably the most cloud native of the patterns, as this involves a developer coding an application against production. Development and test environments must be as high-fidelity as possible in order to get the most accurate feedback, and obviously the most production-like environment there is is production itself. Azure provides dev spaces, which allows an engineer to spin up a managed Kubernetes cluster on-demand and connect a local VSCode editor into this. The tool manages the build and deploy of any code changes into a container, which is then deployed into the dev space in near real time.
The CNCF-hosted Telepresence tool allows developers to proxy their local development environment into a Kubernetes cluster, which allows an engineer to run and debug any code and application locally, as if it was in the cluster. This allows a real-time developer feedback loop, as requests can be made against the production application and the service debugged locally using actual traffic (or shadowed traffic) that is forwarded to the local development environment.
Cloud native systems are typically developed as modular (service-based) systems, which means testing a single service can be challenging due to the required interaction with external service dependencies. Obviously services should be designed to be as cohesive and loosely coupled as possible, which means that the can be developed in isolation. However, when this is not practical, or an engineer wants to drive a more production-like test, techniques like service virtualisation and consumer-driven contracts can be useful patterns.
Modern service virtualisation tools like Hoverfly, WireMock and Mountebank act as proxies that sit between services and systems and capture traffic for later replay. This allows for the execution of tests that span multiple services, and the recording of the associated requests and responses from the dependent services involved. The recording can then be replayed without running the actual dependencies themselves, which is very valuable for running isolated tests in a CI/CD build pipeline. Both of these tools can also be used to generate virtual responses from services that do not yet exist, and Hoverfly allows the injection of faults, which can be used to test the handling of failures in a deterministic fashion.
Consumer-driven contracts (CDC) can also be used to not only drive the design of services outside-in (i.e. TDD for APIs), but also for verifying that a service provides the required functionality and doesn’t regress as the service evolves. There is an excellent article about this on the Martin Fowler Blog, and although the process can appear daunting at first glance, in my experience it does become quite mechanical once a team have experimented with the approach over a few iterations.
Cloud native systems are complex and constantly evolving, and so testing in pre-production typically cannot provide complete validation of functionality and interactions with what is currently running in production. The solution to this problem is to reduce the impact of deployment by canarying — initially routing only a small fraction of production traffic against the new service (the “canary in the coal mine”) and observing behaviour and other KPIs, and then incrementally increasing the percentage of traffic this until the new service is taking all of the traffic.
For developers working with Kubernetes, the open source Ambassador API gateway, which is built on the Envoy Proxy, provide canary testing functionality, which is driven via simple annotations on Kubernetes services. The Istio service mesh also provides canarying, but this has to be configured outside of Kubernetes. With a little glue code, both of these systems can provide an automated canary release of functionality, and an automated rollback if issues are detected.
For developers working with serverless code, similar functionality is offered by many of the cloud vendors. For example, AWS Lambda provides traffic shifting using function aliases, which can be orchestrated to provide a canary rollout. As with the Kubernetes approach above, a developer can write some glue code to automate gradual releases and rollbacks based on AWS CloudWatch metrics.
#BlackLivesMatter
121 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
121 claps
121 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tjholowaychuk/apex-logs-public-beta-48c683464054?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
TJ Holowaychuk
Aug 24, 2020·6 min read
My latest product Apex Logs is now in open beta! Apex Logs is a structured and plain-text log management solution, with a minimal design, simple API, a rich query language, and flexible alerting integrations.
On top of that Apex Logs currently has the most competitive pricing in the industry, up to 10 times more cost-effective than the offerings from Papertrail, Loggly, DataDog, and others at only $0.35/GB (ingested).
Let’s take a look!
I love really simple APIs, I think it should be as easy as possible for customers to interact with your products, so I created and open-sourced the apex/rpc project, a schema-based RPC-style API client & server generator.
The API has a simple contract: all requests are made via POST to a method such as add_events, with a JSON body as input followed by a JSON response body for the output, that’s it!
Here’s an example from the terminal using curl:
And Node.js using the apex/logs-winston transport:
And finally from Go using the apex/log package:
See the API documentation for more information regarding language clients and integrations.
Let’s dive into the interface next.
Apex Logs uses “projects” to isolate events & alerting, so searches can only be made within a single product at a time, but how to structure your logging around projects is your choice.
For example you may have a single project for each product such as MyApp and use the event fieldsenvironment=staging and program=api for filtering, or you could create projects for each environment, for example “MyApp Production”, or you could take it further and have projects per-product, per-team: “MyApp Mobile Production” and “MyApp Backend Production”.
The JSON event fields are presented as clickable in the UI, allowing you to filter the search query further based on the value, in this case showing or hiding all regions matching “us-east-1”.
Apex Logs is index-free and schema-less, you can update your logging conventions as you go without breaking anything. The “Discovered fields” panel on the right is updated to reflect the fields available based on the current search query, and the percentage of occurrences.
Clicking an event brings up a detailed view of each field:
Or if you prefer, click over to the JSON tab to view the raw event, Apex Logs will remember your preference for the next time you view a log event.
Fields in this view are clickable as well, with the options reflecting the type of value, for example numeric fields provide you with further options to filter results above or below the selected value.
Clicking a field in the “Discovered fields” panel such as the AWS Lambda “function” will show you the values, filtered against the current query and time range.
Here we can see that 65% of AWS Lambda function calls were made to alert_processor, by far the most busy function.
Hitting the button next to the search input will bring up a list of recent & saved searches, where you can star any recent searches. Recent searches are personal, while saved searches are available to your entire team.
The last bit of user interface I wanted to touch on is alerting — just give your alert a name, search query, a threshold (the number of matched events) , then select a Slack, Email, SMS, Webhook or PagerDuty integration and you’re done!
There are three display modes for those who prefer line-wrapping or expanded logs.
Now that we’ve checked out the interface, let’s look at the query language.
Apex Logs provides an expressive query language designed for querying both plain-text and structured logs. You’re not restricted to choosing a single style, you can mix-and-match the two logging styles as necessary, for example you may have Apache access logs in plain-text, while your web application uses structured logs.
Here’s the example structured log event we’ll use for the upcoming queries, note that there’s no rigid schema, you can name the fields however you want.
The syntax for querying plain-text logs also works for structured logs, for example the following search terms are treated as uploaded AND tobi@example.com.
To be more specific you can quote whitespace and special characters for an exact match:
The previous examples search against the entire log event text, so the terms “uploaded” or “tobi@example.com” could be anywhere in the event. Let’s get more specific by referencing fields:
The AND operator is implied, but we can add it for readability:
The great thing about structured logging is we can perform more complex comparisons, for example the following query will give us large images uploaded by anyone with a “@example.com” email address, using the * wildcard:
Note the use of size >= 400kb — Apex Logs has built-in units for durations and byte-sizes:
You can read more about the query language in the documentation. If you have any suggestions I’d love to hear them!
SaaS log management solutions such as Loggly and Papertrail provide you with an API endpoint and UI shared with many customers. Apex Logs is self-hosted in your Google Cloud account, once you run the installation command you’ll have a private endpoint for interacting with the API and UI. It’s completely serverless so there’s nothing your team needs to maintain.
Logs can be ingested from anywhere, including AWS CloudWatch, Digital Ocean, or anywhere else using the API integrations, so it’s important to note that just because it’s utilizing Google Cloud’s offerings, you can use it as the logging solution for mobile applications, games, browser apps, to complex infrastructure on AWS or Azure.
If you’d like to stay up to date make sure to follow on Twitter, or subscribe to the public roadmap on GitHub.
Code. Photography. Art.
See all (395)
76 
76 claps
76 
Code. Photography. Art.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/all-you-need-to-know-about-google-cloud-vision-api-6ae8e17969a?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
Google Photos is the first to bring image recognition features to the public. It relies on pattern matching algorithms and image classification. This is the technology which makes it possible for us to search for photos containing a particular landmark or object. By open sourcing Cloud Vision, Google allows developers who are not in a position to build their technology, to take advantage of Google’s underlying image recognition technology to build applications which see and understand content within its images. Cloud Vision API enables your developers to build image recognition and classification features into your application, by incorporating image analytics capabilities in the form of easy to use REST APIs.
For the non-technical crowd, an Application programming interface (API) is a messenger of sorts. An API takes your requests and tells the system what you want it to do then brings the response back to you. A lot like a waiter in a restaurant who takes your order to the kitchen where it’s made and brings the response or the food back to your table. Cloud Vision API immediately categorizes images(queries), detects particular faces, objects and logos and searches for content in those images and displays them (response). This opens up a lot of avenues, like building metadata on image catalog or identifying offensive content. Companies have always encouraged their users to annotate and tag images accurately, even so, the metadata is poor or missing even. Cloud Vision uses artificial intelligence to add metadata to images automatically when they are uploaded to your system.
Phrases like “game-changer” and “”revolutionary technology” have been thrown around to describe Google’s Cloud Vision API. Skeptics will find it hard not to look past the marketing smokescreens, but some developments come with this technology which makes it worth the effort to examine the fine print.
Exposed as RESTful APIs (meaning providing the user(Cloud vision) with an interface to access and manipulate your data(images)), Cloud Vision accepts an image and categorizes it. Your developers then build rich metadata around the images to perform custom searches. Google gave good thought to the three main parameters involved in developing this technology.
Today’s users are overwhelmed by the sheer number of photos they store on their devices and in the cloud. Solving their challenges will influence their photo taking, sharing and tagging behavior.
By shifting your heavy duty to the Cloud, low-powered devices can take advantage of these services through the APIs. Even the app developers who have a homegrown image recognition technology could benefit from adding a subset of the Google Cloud Vision API functionality to complement theirs. The possibilities are immense. Door keys might die out soon if this technology is coupled with the Internet of things to open doors through facial recognition. It can even be used to describe images to visually impaired people.
Image recognition defines an image in words. It will identify objects, facial expressions, landmarks, logos, etc. Visual search is about finding visually similar images, or maybe to find visually similar objects like those identified in your image. Visual search is more of a challenge to develop. It relies a lot on domain expertise to ensure the results are relevant and not merely technically correct. While the ink is not yet dry as to Cloud Vision’s upcoming features, and future scope, the days of offering general image recognition solutions are over. The market will only allow vendors, focusing on areas that require specialized image recognition services to thrive.
Originally published on Product Insights Blog from CognitiveClouds: Top web app development company
Get smarter at building your thing. Join The Startup’s +750K followers.
110 
110 claps
110 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/@clivethompson/how-to-discover-a-new-type-of-cloud-8e19642e9d6f?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Clive Thompson
May 5, 2016·3 min read
Back in 2006, Don Anderson snapped this picture of an amazing cloud formation. He uploaded it the web site of the “Cloud Appreciation Society”, a deliciously-named organization founded 2004.
But it turns out that Anderson wasn’t alone. Other members of the Society were discovering, all around the world, this same formation — where the clouds cut deep, jagged valleys across the sky, producing a freaky, end-of-the-world look. These clouds weren’t new, of course; they’ve been occurring for aeons, and people have probably been taking pictures of them for aeons too. But before the Cloud Appreciation Society started up in 2004, they didn’t have anywhere to share them.
Now they did. Before long, submissions of this odd formation were coming in from all around the US, Europe, and Canada, including gorgeous shots like these:
As the founder of the Cloud Appreciation Society, Gavin Pretor-Pinney, saw these submissions trickle in, he realized his group had collectively stumbled onto something interesting: A new type of cloud.
This formation wasn’t listed in the International Cloud Atlas, published by the World Meteorological Organization since 1896. So Pretor-Pinney devised a name for this cloud — “asperatus”, derived from “a passage in Virgil describing a roughened sea”. He’s been working for years to get it accepted into the next edition of the Atlas. In this week’s New York Times Magazine, there’s a superb piece by Jon Mooallem detailing the whole story.
The discovery of asperatus clouds is a lovely side-effect of what, in my book, I call “public thinking”: When disparate folks publish their observations online, they quickly discover the other people who share their seemingly niche obsession. As Mooallem writes:
Pretor-Pinney assumed that this phenomenon was so rare that, until now, no one had recognized it as a repeating form and given it a name. “As the hub of this network, a network of people who are sky-aware,” he said, “it’s easier to spot patterns that, perhaps, weren’t so easy to spot in the past.”
Today’s wave of “citizen science” is, of course, predicated on precisely this epiphany: The number of everyday folks interested in the natural world massively outnumber the scientist population, so why not harness them? There are now organizations set up have citizens classify galaxies, count hummingbirds, report coyote sightings, and parse reports of plankton.
But what charms me about the story of asperatus is that the discovery was unintentional.
Pretor-Pinney didn’t create the Cloud Appreciation Society specifically so he could identify a new cloud formation. No, he created it because clouds are rad; because staring up the darkening sky brings deep aesthetic delight. (As Mooallem notes, this is the rare group of people who, when their annual convention takes place on a crystal-blue day of gorgeous weather, are utterly crushed.) As goes the koan of open-source software, “given enough eyeballs, all bugs are shallow”. Collect together enough people intrigued with some corner of culture or science or history, let them talk long enough, and odds are they’ll stumble into something the world hasn’t yet seen. Inquiry is often sparked by joy.
BTW, my single favorite picture of asperatus is this one, taken by Elaine Patrick. The sky looks so tangible!
(This post originally appeared at Clive’s blog here.)
I write three times a week about tech, science, culture — and how those collide. Writer for NYT mag/Wired; author of “Coders” and “Smarter Than You Think”
See all (670)
29 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
29 claps
29 
I write three times a week about tech, science, culture — and how those collide. Writer for NYT mag/Wired; author of “Coders” and “Smarter Than You Think”
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chaiyasitbunnag/covid-19-analysis-from-biqquery-public-data-set-3017c7d414a3?source=search_post---------266,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chaiyasit Bunnag
Apr 2, 2020·7 min read
สวัสดีครับ เนื่องจาก work from home ยาวๆ ช่วงนี้ทำให้มีเวลามาทำ sideline project 555 วิเคราะห์ข้อมูลตัวเลขผู้ติดเชื้อ (confirmed cases) จากประเทศต่างๆ ที่ทาง Google BigQuery ได้เปิด public data ให้นักวิเคราะห์และนักวิจัยได้ช่วยกันหา insights บทความนี้ขอนำเสนอ 3 หัวข้อ ดังนี้ครับ :
"
https://medium.com/google-cloud/exploring-san-franciscos-public-data-ccf21cb2bfc3?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
San Francisco. Fog City. The City by the Bay. No matter what you call it, those 47mi² are home to over 800,000 people about whom we can draw outrageous conclusions using the new San Francisco public dataset in BigQuery.
Thanks to the City and County of San Francisco’s SF OpenData project and Bay Area Bike Share, Google BigQuery’s Public Datasets now includes San Francisco public data, including:
SF Protip #1: Don’t leave valuables in your car when parked on Bryant. Especially on Friday. Definitely not before lunch.
In 2016, stolen cars and theft from locked cars accounts for over 21% of all SFPD crime incidents.
Reports of thefts from locked cars peak on Friday at lunch-time and after-work, presumably as folks are arriving at their cars to find them ransacked. Parking on Bryant is easily the best way to maximize your chances of having your stuff stolen.
If you’re looking to minimize your chances of being a victim of grand theft from a locked auto, this heatmap shows where most thefts from cars have been reported — avoid parking there.
SF Protip #2: If you lose something on Bryant, try looking for it on Valencia and Eddy.
Nearly 20% of all lost property was reported on Bryant Street. The best odds for finding something are over on Valencia and Eddy, where more than twice as many items are reported found than lost.
You’re most likely to come back to your car and find it missing entirely if you parked on Mission Street. Especially on Friday.
SF Protip #3: Sunset Boulevard is the leafiest street in San Francisco.
You can get a taste of Monterey by visiting Sunset Boulevard — the leafiest street in San Francisco, with more trees than any other street, including nearly 1,400 Monterey Pines and Monterey Cypress. If you’re looking for a shady spot, the heatmap below shows the density of tree distribution throughout the City.
SF has a greater density of trees than NYC, featuring 2,637 trees per square mile compared to NYC’s 2,242.
Like New York, the most common tree in San Francisco is the London Plane Tree, which represents 7% of all SF trees, compared to 13% of the trees in New York. San Francisco trees are much more diverse, you’ll find 492 different species, compared to New York’s 183.
San Francisco can boast at least 1 example of each of New York’s top 7 tree species, where New York has none of San Francisco’s, apart from the London Plane.
SF Protip #4: San Francisco residents complain more than New Yorkers.
Per capita, San Francisco uses its 311 municipal complaint line more often than NYC. The most common complaints in SF are requests for sidewalk cleanups, rubbish pickups, and graffiti removals.
The biggest growth in complaints is in the categories that already represent the largest volume, with bulky item pickup requests and illegal homeless encampments rising most quickly.
Note that non-offensive graffiti is on the rise, with offensive graffiti is the fastest dropping complaint category. So either San Franciscans are writing less offensive graffiti, or the residents have grown immune to the current attempts to offend.
SF Protip #6: Steep hills make for slower bike rides.
Riders of the Bay Area Bike Share network average 14 minutes per ride — nearly the same as the 15 min average for New York Citi Bike riders. San Francisco is a smaller town though, and riders average only 1.4 km per trip, compared to New Yorkers, who go 1.8 km.
Or put another way, San Francisco cyclists are 0.3 m/s slower than their New York counterparts. I blame the steep hills.
SF Protip #7: Marijuana legalization will reduce arrests in Haight-Ashbury by 7%.
Marijuana represents 26% of all drug related arrests in San Francisco. In 2016, it was overtaken by meth as the most likely drug involved in an arrest.
Overall, drug crime is down 62% since 2003, with fewer arrests for all drugs — except methamphetamine, which is up 14%.
The map below shows all areas where there have been at least 5 drug-related arrests since 2003. Red pins are for marijuana, blue for crack cocaine, pink for meth amphetamine, green for heroine, and yellow represent cocaine.
The larger pins indicate over 200 drug-related arrests in the same location.
The Tenderloin is the center of San Francisco’s drug arrests, particularly for crack cocaine — the arrests for which are clustered around specific locations; methamphetamine arrests, however, are more likely to follow roads.
Marijuana arrests spike in the Haight-Ashbury neighborhood, where they account for 7% of all arrests.
SF Protip #8: The best place to have your house catch fire is Chinatown. The most likely place is the Tenderloin. If you’re lucky they’ll dispatch Engine 3.
The average response time between calling 911 and firefighters arriving at your burning building is 6 minutes 8 seconds, with units arriving at Chinatown blazes at an impressive city-wide best of 4 minutes and 30 seconds.
The fastest fire truck in the SFFD is Engine 3 based in the 4th battalion with an average response time of just over 3 minutes.
Join us here every week, for Today I Learned with BigQuery, as we dig into each of these table in more detail, use the NOAA weather tables to explore the effect of weather on crimes and 311 calls, and compare what we know about San Francisco and other cities, starting with New York.
If you’re new to BigQuery follow these getting started instructions, and remember that everyone gets 1TB at no charge every month to run queries. When you’re done remember to share the results with us using #TILwBQ.
Google Cloud community articles and blogs
56 
2
56 claps
56 
2
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://m.subbu.org/cloud-lock-in-and-change-agility-78d63978ddfd?source=search_post---------268,"We are entering into a future of lock-in. Public clouds are no longer someone else’s computers or hosted services. These are fast becoming proprietary platforms with a bit of open source sprinkled in here and there. Almost everything you need to run an enterprise is now available as a pay-as-you-go platform on AWS, and Azure and GCP are not staying put.
As difficult as it may be to swallow and accept this trend, it is important to recognize that, anything that needs to be procured or downloaded, built, run, operated and maintained is up for lock-in. In this future, all the infrastructure primitives, software lifecycle automation services (including containers and cluster managers), what was once known as middleware, data processing platforms, the mechanics used for security and operations, as well as the all the enterprise data will be locked into to a few public clouds.
I don’t expect any enterprise to successfully operate on, or migrate to a public cloud and yet remain insulated from the cloud provider’s platform lock-in. Sure, there are several open-source and closed solutions (for instance, those in the container, cluster management and PaaS area, big data processing engines etc, various relational and non-relational databases etc.) that aim to help you stay agnostic. But such platforms insulate you at-best from just the mature aspects of these cloud platforms, such as, say the IaaS layer. But none of these help you participate in the massive platform shift happening in public clouds. When you consider this shift, lock-in insulation is like having the cake and eating it too. You can’t take advantage of all the new capabilities to innovate for your business while staying agnostic to the platform.
In this lock-in future, techniques of the past decade and half, such as open source and abstraction layers, won’t insulate us from lock-in. This does not mean that open source, open interfaces and open protocols don’t matter. They do, and will continue to matter to drive a culture of open coding, sharing, learning, collaboration, and interoperability. But not necessarily for lock-in insulation.
So, what’s the answer then?
The answer, in my view, is practicing change agility. Change agility is an organization’s culture of continually practicing significant changes.
During the last fifteen years, most enterprises reacted to lock-in concerns by over-investing in abstractions such as programing frameworks, parsers, databases, communication protocols, application servers, cloud providers, you name it. At the end of it all, what’s left were past regrets and large difficult to change monoliths. Those abstractions, in effect, did nothing other than contributing to a culture of change resistance.
A culture of change agility on the other-hand deals with changes as a matter of course and not as impediments. It embraces techniques like service orientation, asynchronous and decoupled communication patterns, micro-architectures, experimentation, failing fast, tolerance for mistakes, chaos engineering, constant feedback and continuous learning.
An organization adept at change agility does not see lock-in as an obstacle. It sees it as an opportunity to learn, experiment, and partake in creating its own future thus moving up the value chain. Not just once, but continually.
(Cross posted at https://techblog.expedia.com/2016/12/11/cloud-lock-in-and-change-agility/.)
A blog on tech and leadership
55 
4
Some rights reserved

55 claps
55 
4
Written by
See https://www.Subbu.org
A blog on tech and leadership
Written by
See https://www.Subbu.org
A blog on tech and leadership
"
https://medium.com/built-to-adapt/private-and-public-the-best-of-both-clouds-7ae0d97ef085?source=search_post---------269,"There are currently no responses for this story.
Be the first to respond.
Software today requires a support structure that allows for constant iteration. This means organizations can constantly improve on their software, day-by-day or even hour-by-hour. Cloud-based technology has allowed many institutions to embrace and adapt to a cycle of continuous innovation. But today there’s a false dichotomy in the cloud business: that you need to pick between a public cloud — most likely from Amazon, Google, or Microsoft — or a private cloud — one that you have total control over.
I’m here to tell you that you have more options than you think.
Up until recently, the only straightforward way to get on-demand, always improving, app-centric platforms was to use a public cloud. Period. Private clouds hamstrung a business because they were about operations efficiency, not development acceleration. But platforms are helping fix that and put public and private cloud on equal footing.
But there are some specific reasons for going with a hybrid cloud approach:
Take healthcare and banking, which are examples where government regulations may require data locality — meaning you can’t store the data outside of a particular country or region. These businesses may not want to buy or rent dedicated data center space if a public cloud is available in a required location. Alternatively, a public cloud may not have a presence where regulations may require a physical site, so a private cloud might be the best option.
Predictable workloads can be optimal to buy and run on a private cloud because of the cost savings. With unpredictable workloads you are better off renting infrastructure because you can always rent more on-demand, which saves you from having to guess right on how many people will use your new application or service.
For example, one of our customers in the consumer electronics market runs a lot of their software on-premises, in regional datacenters throughout the year where the load in predictable. But during a gift buying season — Mother’s Day, Christmas, and Chinese New Year — they rely heavily on public cloud resources to handle the much higher, variable load that’s required when activating all those new devices.
Ultimately, if your goal as an institution is to preserve optionality and flexibility, then you need to be able deploy and manage your software on both a private and a public cloud.
With modern cloud platforms and using cloud-native principles, you do not have to accept bad tradeoffs by running your cloud in either private or public because there are cloud platforms that run well on both. Full stack, balanced hardware and software engineered systems with cloud native software platforms let you take advantage of both public and private clouds.
The gap between private and public continues to shrink as well thanks to features like the Cloud Foundry marketplace. Based on the Open Service Broker API project, this marketplace gives developers in a private cloud access to public cloud-only services, like Google’s Machine Learning APIs, or Microsoft Azure’s CosmosDB. Obviously, I’m particularly excited about the offerings Pivotal Cloud Foundry Marketplace can provide as well as our recently announced Pivotal Container Service (PKS).
Before you embark on a hybrid journey, here are some questions to consider.
If you enjoyed this post you might enjoy this piece about cloud diversification.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
52 
52 claps
52 
How software is changing the way society and businesses are built.
Written by
i enjoy making software. @pivotal
How software is changing the way society and businesses are built.
"
https://medium.com/thinking-design/adobe-xd-windows-update-9b3cc9ba8805?source=search_post---------270,"There are currently no responses for this story.
Be the first to respond.
It’s been an exhilarating six months since we released the first public beta version of Adobe XD, available for Mac OS, back in March ‘16.
While we were confident in our approach to delivering Adobe XD, there’s nothing quite like the reality of shipping the product, measuring the level of interest, working through the feedback and analysing the usage data, so as to understand whether we were on the right trajectory or whether we needed to rethink the approach.
Thankfully the response has been amazing! We’re excited to announce that we will be delivering several major new capabilities in the coming months, including layers, symbols and real-time mobile preview — culminating in a milestone release on Mac that we believe will be ready for everyday use by UX designers.
When we first started working on Adobe XD, we wanted to focus on a single platform to ensure that we were on the right path to creating something of true value. Focusing on one platform enabled us to iterate and adjust quickly, before committing to multi-platform development. We heard from our existing customers on Mac that they were dealing with the friction that came from trying to use non-Adobe tools alongside those from Creative Cloud, so we decided to start with that platform.
Now that we have established a solid foundation with Adobe XD on Mac, we’re working as fast as we can to “catch up” on Windows. At the same time as we’re catching up, we also want to ensure we’re building a next-generation design tool that takes advantage of the latest hardware and software, so as to really deliver something special for designers on Windows. That means we’re not just porting the product from Mac to Windows, but rather, we’re investing in a completely new Universal Windows Platform (UWP) app that will be available exclusively on Windows 10.
UWP represents the future of the Windows platform, opening up opportunities for us to leverage the latest touch-enabled hardware and to deliver Adobe XD to a future generation of Windows-based devices.
Adobe XD is the first UWP app from Adobe, which means there is a lot of learning along the way. Not only do we need to deliver Adobe XD, but also Windows 10 versions of the supporting libraries and components that every Adobe application relies upon. So, while we understand (and appreciate) the desire to get Adobe XD on Windows as soon as possible, we are taking the time needed to craft a unique UWP-based experience.
While the core feature set will be the same across Mac and Windows, as will our focus on performance and stability, Adobe XD for Windows will be different than on Mac — the experience will be customized for the unique capabilities offered by Windows 10 hardware. For example, only on Windows 10 will Adobe XD offer full support for both pen and touch — meaning that you can fluidly zoom and pan your document, create vector artwork and connect wires between screens of your prototype by using touch-based input. Designing and creating prototypes using touch on Windows feels completely natural and will make Adobe XD feel even more special for Windows users.
As eager as we are to release XD for Windows as soon as possible, our approach is to make it available when it’s ready — we really want to deliver a product that you look forward to using every day.
That said, we also need feedback from designers on Windows to help us get there — if you’re the adventurous type and would like access to a pre-release version of Adobe XD on Windows you can let us know here. We’d love to get your input as we craft our Windows 10 experience.
For everyone else, we’re aiming to deliver our first Windows public beta release towards the end of 2016. That first release will not have feature parity with the Mac version, but you’ll see rapid progress with each of the subsequent monthly releases, getting to an aligned set of capabilities across Mac and Windows versions of Adobe XD before you know it.
We hope you’re as excited about Adobe XD for Windows as we are — we promise it will be worth the wait 🙂
You can reach out to our team here in the comments or @AdobeXD on Twitter — we look forward to hearing from you. In the meantime there are additional answers to common questions we’ve heard about our Windows release below.
Why only support Windows 10 and not earlier versions of Windows?
We’re building Adobe XD from the ground up and wanted to take advantage of the latest hardware and software platforms so as to provide a modern, high performance, future-proofed tool for UX designers.
Will the features on Mac and Windows be exactly the same?
The core feature set across both versions of Adobe XD will be consistent, but on each platform we’ll look to take advantage of native capabilities. We want the Mac OS version of Adobe XD to feel great on Mac and likewise, the Windows version to feel great on Windows 10 — designing and building specific versions of XD for each platform allows us to do this.
Will you support touch and pen features available on devices like the Surface Book?
Yes, absolutely — we’re excited to bring a fully pen and touch enabled Adobe XD experience to our customers using Windows 10.
Will the file format be compatible across Mac and Windows?
Yes.
Why did you work on the Mac version first?
Back when we started working on Adobe XD, we wanted to focus on one platform to ensure that we were creating something of true value, with the ability to iterate and adjust quickly, before committing to multi-platform development. We heard that some of our existing customers on Mac were dealing with the friction that came from trying to use non-Adobe tools alongside those from Creative Cloud, so we decided to start there.
Why did you de-prioritize the Windows version relative to Mac?
Initially, we wanted to focus on one platform to ensure that we were creating something of true value, with the ability to iterate and adjust quickly, before committing to multi-platform development.
What is the roadmap for Adobe XD on Windows?
We’re on track to deliver a first public beta release of Adobe XD for Windows in late 2016, with subsequent monthly releases adding additional features and enhancements based on customer feedback. Once we’ve caught up to Mac features, you can expect continued parity with new capabilities coming to the Mac and Windows versions of Adobe XD at the same time.
Andrew Shorten is Director of Product Management for UX Design at Adobe. He is passionate about improving the quality, richness, and value of digital experiences. Andrew previously developed user interfaces for government and enterprise customers while working at Fujitsu. He has since worked for Macromedia, Microsoft, and Adobe, where he has engaged with designers, developers, digital agencies, and organizations to help them deliver engaging web, mobile and desktop experiences.
Originally published at blogs.adobe.com.
More about Adobe XD:
Stories and insights from the design community.
27 
2
27 claps
27 
2
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-bigquery-public-datasets-now-include-stack-overflow-q-a-5a8a7e371b52?source=search_post---------271,"There are currently no responses for this story.
Be the first to respond.
Thursday, December 15, 2016
Originally published at cloud.google.com.
Exploring hidden trends and relationships in Stack Overflow data is a good lesson in doing SQL analytics with BigQuery.​
Great news: we’ve just added Stack Overflow’s history of questions and answers to the collection of public datasets on BigQuery. This means that anyone with a Google Cloud Platform account can use SQL queries (or some other favorite tool) to dig into this treasure trove of data.
You can find some some sample queries on the Stack Overflow Data documentation page, for example:
Take these questions as a starting point, then feel free to share your results and query variations with us via reddit.com/r/bigquery. And if you have any questions, ask the community on Stack Overflow.
You might be wondering: What’s so special about querying Stack Overflow with BigQuery? After all, Stack Overflow already refers users to Stack Exchange Data Explorer (SEDE), a data focused site where users have shared and prioritized thousands of questions — and that works really well. So, let’s review some of the advantages of having Stack Overflow data in BigQuery too:
Let’s look at an example of joining. We have terabytes of GitHub’s open source code shared on BigQuery. Let’s find out which are the most referenced Stack Overflow questions in the GitHub code — specifically, Javascript.
Here are the most referenced Stack Overflow questions within Javascript code on GitHub:
Or, we can look at GitHub pull-request comments from GHTorrent (also on BigQuery):
Here are the results:
Or, let’s look at Hacker News. What are most popular tags of questions that have been posted there since 2014?
Here are the most popular tags on Stack Overflow questions linked from Hacker News since 2014:
How does that compare to the rest of Stack Overflow?
It would seem that the Hacker News community cares a lot more about Haskell, C, C++, and performance than Stack Overflow as a whole, which lists php, android, jquery, and css within its most popular tags:
If you haven’t tried BigQuery yet, follow this Beginner’s Tutorial, which shows how to analyze 50 billion page views in 5 seconds. Then, you’re ready to feel free to play with any other query or dataset you like: for example, our official public BigQuery datasets, datasets that other users have shared, and of course your very own.
Originally published at cloud.google.com.
Google Cloud community articles and blogs
24 
2
24 claps
24 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://read.iopipe.com/public-cross-account-functions-on-aws-lambda-bcc148303083?source=search_post---------272,"Public and cross-account functions on Serverless platforms such as AWS Lambda offer compelling use-cases to build non-HTTP, non-RESTful web services that skip API Gateway and can be connected directly to event triggers on other user’s accounts.
For instance, a function processing S3 uploads with AWS Rekognition could be connected to a number of S3 buckets across several accounts, with all image processing charges aggregated and billed under one account.
At IOpipe we support creating alarms that execute Lambda functions. We also support webhooks, but to connect those with Lambda would demand using API Gateway, which demands configuration, adds latency, and incurs charges from AWS. By invoking Lambda directly, we offer a simpler and less expensive option, which also allows our user’s Serverless applications to run more natively and inclusively in Lambda.
Here’s how you configure it. You create a function on Lambda. This may be done via a framework, with the AWS cli, or via the web:
We save this as ‘hello_world’. You may choose any name, but this is the name I will use as a reference.
Now, the magic! We must add permissions to the function to allow cross-account invocation. This may only be done using the AWS cli.
Allow invocations via API & CLI:
Allow configuration of event sources:
Skip ahead to Invocations across accounts, unless you’d like to know more about fine-tuning these parameters!
The statement-id is a name for this permission. You would reference this statement-id when removing permissions from the Lambda.
The action is any Lambda API call. The most appropriate one for this use-case is to Invoke, but other permissions could make sense, such as granting permission to a CI/CD system to lambda:UpdateFunctionCode.
The principal argument accepts all origins by specifying ‘*’, but can be restricted invocations from an AWS service id or an AWS account id. This allows restricting your Lambda to invocations originating from s3, or from specific AWS accounts.
When invocations originate from AWS services, their principal will not be an account ID, but an AWS service principal such as sns.amazon.com or s3.amazon.com, and you will need to specify that principal (‘ — principal s3.amazon.com’) and then filter AWS accounts using the ‘ — source-account’ parameter.
Amazon gives you a ARN, which for simplicity of explanation, we’ll get from the AWS dashboard:
Using the ARN, we can simply invoke this from the CLI using another AWS account:
You can also specify ‘ — invocation-type DryRun’ to simply verify access to the function without invoking it, or specify type Event to asynchronously invoke (fire & forget). The — log-type Tail option was specified to get stdout, but may be skipped if you do not desire to see “Hello World” printed.
Cross-account Lambda functions are only useful if your application can trigger them! From an account with access to your Lambda function:
Using this, one could connect their account’s S3 bucket to a shared Lambda, trigger on SNS messages, etc. This only works on “push-style” events such as those published by S3, and not “pull-based” events as used by Kinesis.
Conclusion
I’d love to see your examples on what you’re doing with this, what use-cases our community might have! Let us know in our Slack channel!
Full observability and dev tools for building, shipping…
30 
30 claps
30 
Written by
CTO and founder of IOpipe, Inc. working on Application Operations tools for serverless applications.
Full observability and dev tools for building, shipping, and running serverless applications on AWS Lambda. Profiling, Monitoring, Logging, Metrics.
Written by
CTO and founder of IOpipe, Inc. working on Application Operations tools for serverless applications.
Full observability and dev tools for building, shipping, and running serverless applications on AWS Lambda. Profiling, Monitoring, Logging, Metrics.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/cloud-bursting-with-virtual-kubelet-and-kip-kloud-instance-provider-4b86a479ce38?source=search_post---------273,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and bursts into a public cloud when the demand for computing capacity spikes. This is a model that gauges an organization’s ability to use internal resources to host services critical to their business, and during demand spikes consume resources from public clouds on a pay-as-you-go approach.
Cloud bursting is a powerful concept for modern businesses running their apps in hybrid cloud environments. By completely automating this process companies can save money and improve their SLAs at the same time. Cost savings are realized because infrastructure can be fully used, allowing IT departments to carry a more conservative number of servers in-house, while still having a strategy in place where the application can be expanded or full-on moved to a external public cloud like Amazon’s EC2 during times of peak performance.
Kubernetes provides a unique role in burst computing, as it’s the most common mechanism for managing containers, besides taking the pain out of workload management, Kubernetes enables users to deploy containers at scale required for contemporary computing. Kip from eltol is a Virtual Kubelet provider that allows a Kubernetes cluster to transparently launch pods onto their own cloud instances called cells. Kip’s virtual-kubelet pod runs on a cluster and registers as a virtual Kubernetes node in the cluster. When a pod is scheduled onto the Virtual Kubelet, Kip starts a right-sized cloud instance (cell) for the pod’s workload and dispatches the pod onto the instance.
Pods in the public cloud shown above are individual EC2 instances (every pod is run in its own EC2 instance ) called cells. Cells (cloud instances) are images with just barebones kernel + initrd and doesn’t run kubelet or any container runtime (like docker) where the other components of Kip system takes care of the container lifecycle management. The container images are extracted to a flat directory and are run as processes on the cloud instances (cells) where there is no need for full-blown container engine.
An on-premises Kubernetes cluster shown below is an all-in-one cluster (master + worker) with a virtual kubelet pod registered as a virtual worker node. Any supported CNI can be used for on-prem deployments and it is recommended to use native CNI plugin that integrates with the cloud provider on the public cloud providers where virtual-kubelet pods and regular pods will get their IP addresses from the same network (like VPC) address space.
Virtual-Kubelet configuration is provided through a configmap. The configuration contains kip-provider configuration and optional cloud-init configuration (this cloud-init is supplied as EC2 User Data to Cells created on AWS).
Cloud-init configuration enables users to supply user data for the Cells (EC2 or Cloud instances) created by Kip. Users can specify a cloud-init file in provider.yaml and the cloud-init file will be applied when a Cell is booted by Kip. The cloud-init file can be used to provision users, ssh keys, set hostname, write arbitrary files, run commands using ‘runcmd’ etc. on a kip cell or setup additional packages on the cell.
Provider configuration includes cloud provider credentials, networking information for the cells (network in which instances will be created), cell specific information like: default instance type (the default instance type for the cells, Kip supports dynamic selection of instances based on the Kubernetes resource requests), boot image information (users can build their own AMI’s or use the default ones provided/owned by eltol), CIDR information to configure routes, name-tag (nametag to identify cloud resources that belong to this controller), default volume size (default size for the root volume of cells), itzo configuration (version of the itzo agent to use) etc.
Kip provider implementation on virtual-kubelet procures VPC and other AMI (cell images) information from the configuration specified:
As Virtual Kubelet with Kip provider is deployed on the cluster, the control-plane daemonset pods like kube-proxy, node-local-dns etc. are scheduled on the virtual-kubelet node (as the pod registers with the kube-api as a node).
Kip schedules the above pods as individual EC2 instances (cells) on the configured user account within the subnet specified. As shown below the kube-proxy pods are created as EC2 instances of type t3.nano
The user can force cells in public subnets to only have a private address by setting the following annotation on the pod: `pod.elotl.co/private-ip-only: “true”`.
The virtual-kubelet pod establishes outbound-connections to all cells created on the cloud provider. kipctl (like kubectl) installed on virtual-kubelet can be used to interact and procure information of nodes and pods deployed on the cells.
A sample busybox pod is deployed on the on-prem cluster where the same is scheduled on the virtual-kubelet node using nodeSelector to specifically target virtual-kubelet node.
The pod above is scheduled on the virtual-kubelet node as shown below:
The Kip provider implementation in virtual-kubelet start creating cells on AWS procuring the configured (provider configuration) AMI. As the busybox pod above is not configured with any resource-requests the instance type is selected as ‘t3.nano’ (default instance type). Dynamic instance-type selection is discussed in the section below.
As shown below a cell (EC2 instance) is created of type ‘t3.nano’ with the AMI provided in the provider configuration (the AMI used here is a lightweight Alpine based image). Kip auto configures the required security groups to enable connectivity. The cell is provided with two IP’s from the subnet provided in the provider configuration.
As shown below Kip seamlessly adds tags: namespace, name, node-name. app-label etc. to identify the resource.
The itzo client is installed on the cell and itzo initiates deploying required packages to the cells to enable the busybox application.
The cloud-init configuration above includes setting up of user and SSH-Keys which enables users to access the created cells.
As shown below the busybox container is dispatched as a process running in a slim alpine based instance (cell):
Kip allocates two IP addresses for each cell unless the pod has hostNetwork: one for management communication (between the provider and a small agent running on the instance), and one for the pod. Unless the pod has hostNetwork enabled, a new Linux network namespace is created for the pod with the second IP. Both IP addresses come from the VPC address space. As shown below, the eth0 interface of the instance has ‘10.10.22.13’ and the network namespace ‘pod’ has a veth interface has ‘10.10.25.96’.
Unless an instance-type annotation (pod.elotl.co/instance-type: <c5.large>) is present in the pod, kip will choose the cheapest instance type that satisfies the resource requirements. If specific resources are configured in the spec (using Kubernetes resource requests) of the pod then kip seamlessly selects right-sized instances based on the resource requirements. If no annotations or resources are specified then it will fall back to the defaultInstanceType in the provider.yaml.
The instance selector tries to find the minimum cost instance that satisfies all constraints in the resource spec using various parsers/parameters as shown below:
Kip can also run cells on on-demand spot instances and the same can be configured using annotation: ‘pod.elotl.co/launch-type: spot’.
In the example below a sample busybox pod named ‘busybox-resources’ is configured with memory (15Gi) and cpu (8 vcpu’s) requests which creates a c5.xlarge cell.
As shown below the pod is configured with resource requests and scheduled on virtual-kubelet node using nodeSelector:
As shown below the kip provider identifies the resource requirements and spawns a c5.xlarge cell (cloud instance) without choosing the default image specified in the provider configuration.
A c5.xlarge instance created on AWS to accommodate resource requests:
Kip deployed on Kubernetes cloud services like EKS, AKS, GCE etc. doesn’t require any extra layer of connectivity between pods, as both the pods and kip-cells share the same networking space provided by the cloud-provider. To enable networking between pods on on-prem cluster and pods deployed as cells by kip need a traditional VPN connectivity to the cloud-providers network (AWS-VPC in this example). A traditional site-to-site VPN topology between customer network (private/internal network) to VPC involves customer gateway and a virtual private gateway.
Kip and a VPN client (Strongswan) are deployed in the local Kubernetes cluster. A VPN Gateway is deployed on AWS and associated with the VPC that is configured to be used for the kip-cells. A VPN connection will link the local cluster to the VPC, allowing pods and services to reach each other between the two sides. Kip is configured to create pods in the VPC.
The vpn-client pod comprise a ipsec-container which runs Strongswan and an init-container which configures Strongswan parameters using the configuration provided through a configmap.
aws-vpn-client pod running a ‘ipsec-init’ and ‘ipsec’ containers:
Strongswan configuration is provided as a configmap which include public tunnel endpoints PSK’s and VPC_CIDR. Users can use static routes or BGP for advertisement of routes.
The VPC provided in provider configuration is associated with a internet-gateway:
A VGW and CGW is associated with the VPC above, the CGW IP is the public_ip of the vpn-client pod running on the host_network of the on-prem Kubernetes cluster:
A VPN connection is created between VGW and CGW:
The vpn-client pod running on on-prem Kubernetes cluster with required configuration establishes a tunnel with the VPC:
Tunnel status on AWS:
Tunnel status on vpn-client pod:
Once the tunnels are up, the pod CIDR on on-prem cluster and VPC subnet where the cells are created are tunneled through the VPN connection enabling to-way communication over IPSEC.
A ‘busybox-onprem’ pod is created on the on-prem cluster (the master is tainted to allow creation of workloads) as shown below:
Flannel running on on-prem cluster provides the Pod IP (10.244.0.6):
VPC Subnet provides Pod IP (10.10.25.96) for pod ‘busybox-cloudburst’ running as a cell (cloud instance) on AWS:
As shown below the on-prem pod with a pod_ip from a distinct subnet is reachable from the kip created pod running on AWS:
*******************************************************************
Apart form traditional cloud bursting, kip can be useful in other scenarios like cloud-compute-stretching: deploy subset of workloads across multiple cloud providers operated from a central cluster without individual control-planes deployed on respective providers, high-availability/disaster recovery: business critical applications with tight SLA’s can be run on more than one cloud-provider without having to worry about creation of a full-fledged cluster or infrastructure, ease application migration: as subsets of a same application can be run on different providers there will be no chance for vendor lock-in as the workloads can be easily scheduled on different cloud-provider, multi-tenant-security: nodeless delivers separate compute launch type for each pod. This results in stronger (VM-level) isolation between the pods etc.
Virtual Kubelet with kip runs Kubernetes pods in a “container-as-a-service” fashion. Users instead of provisioning extra capacity that is not used can leverage kip to dynamically create instances on the fly depending on the workloads and scale requirements. Kip provides a intelligent instance filtering scheme which enables choosing the most cost effective instance type and that can exactly cater workloads resource requests — Imagine a scenario where we deploy 10 pods needing 10 GB of RAM, on a node that have 32 GB of RAM, where 22 GB of RAM on the node is ideal incurring monthly costs to be paid for ideal resources. Kip can also be leveraged in advanced hybrid cloud use cases with a single control plane.
ITNEXT is a platform for IT developers & software engineers…
69 
69 claps
69 
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Solutions Architect
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/explore-public-dataset-with-google-bigquery-and-datastudio-30f9279b8d42?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joe T. Santhanavanich
Oct 14, 2020·4 min read
Many time, as Data Scientist, we waste so much time in the data storing, importing, managing, and…
"
https://medium.com/iex-ec/public-worker-pool-connect-your-machine-and-become-an-iexec-worker-over-7000-rlc-to-be-earned-7eb6af73e627?source=search_post---------275,"There are currently no responses for this story.
Be the first to respond.
On June 5 2019, 10:00am UTC, iExec will run a special community event for iExec Workers. For 24 hours, the number of tasks for the public worker pool will be increased, with over 7,000 RLC worth of orders placed on the iExec Marketplace. Using the virtual machine (VM) option, all you need to is connect and get ready to compute!
You’ll also need about 0.1 ETH (to hold in your account) and also 5 RLC to stake as a worker.
Since iExec V3 went mainnet, many machines have connected to the iExec Public Worker Pool and hundreds of RLC have already been earned by workers each day. We’d like to thank you to everyone who has contributed their computing power so far! We’re looking forward to ramping up the number of available tasks for this upcoming community ‘Workerdrop’ event.
On 5 June 2019 10:00 AM UTC, to stress test the platform and push it to its limits, we’re placing a much larger number of work orders for 24 hours, with a total of more than 7,000 RLC to be earned. You’re invited to connect your machine ASAP and get ready for the special rush on the marketplace.
During this event, we’ll be running iExec price feed application, updating the RLC/BTC price feed every few minutes. The price feed application uses the new decentralized oracle framework, a recent addition to the iExec stack.
To participate, you must download and start the VM provided by iExec. Nothing special is required: you need a standard machine (Linux, MacOS, Windows), about 0.1 ETH and also 5 RLC to stake as a worker.
Simply follow our step-by-step guide to becoming to joining the public worker pool and becoming an iExec Worker:
docs.iex.ec
The iExec team is reachable on Slack, Gitter or Telegram. You can subscribe to our newsletter to be the first to know about announcements news and community events.
iExec recently launched the much anticipated iExec V3. See below for more information on the new feature and adoption announcements, including the fact that iExec V4 (High-performance computing and GPU) will be coming much earlier than planned.
medium.com
Website • Blog • Slack • Telegram • Reddit • Twitter • Facebook • LinkedIn• Youtube • Github • Kakao • Instagram • Steemit • Katacoda • Docs
Blockchain-based Decentralized Marketplace for Computing Assets
286 
286 claps
286 
Blockchain-based Decentralized Marketplace for Computing Assets
Written by
CEO and co-founder of Āto | Founder of Strat | Volonteer at Emmaüs Connect | Contrib at Concord
Blockchain-based Decentralized Marketplace for Computing Assets
"
https://medium.com/cloud-security/cloud-security-why-use-a-vpn-e0c9a059e6f8?source=search_post---------276,"There are currently no responses for this story.
Be the first to respond.
Any time you are using public wifi, or in other words a network you don’t own and secure yourself that allows anyone to connect, it’s a good idea to use a VPN (virtual private network). A VPN provides an authenticated, encrypted connection that hides information in about the sites you are visiting from attackers. This protected connection makes it harder to intercept your connections, steal or read your data, or insert attacks into the traffic as it flows from your system to other sites.
To give you an idea what I’m talking about, let me first show some information retrieved by a sniffer when I navigate to Amazon.com in a web browser. When visiting this site, many different URLs are used to pull together the content you see when you visit the home page. One of the domain names is read.amazon.com. If I run a sniffer while visiting that web page, without a VPN, I can see the request to this domain name in plain text.
As an attacker there are many ways I can use this information, not to mention any other information sent clear text in payloads of other packets. On an insecure wireless network, anyone with a sniffer could see this traffic. Even on some of the newer, more secure networks, there may be ways for an attacker to intercept and view this data.
Now let’s look at what I see when connected to an IPSEC VPN. All the packets pretty much look like this — because the data is flowing through an encrypted tunnel:
In addition to limiting exposed data while using an insecure network, A VPN also allows network administrators to create firewall rules for your company systems, to allow traffic only from the VPN connections rather than the whole Internet. Why does this matter? Any time the network rules expose a host to the Internet, any open ports will be scanned and attacked. You can reduce the “attack surface” (what an attacker can attack) by limiting what you expose to the Internet to only what is required. I talked about this in a presentation on AWS network security at our AWS meetup in Seattle. Here’s the most critical slide:
If you don’t believe your systems are subject to attacks when exposed to the Internet, go into your AWS account right now. Start up an EC2 instance. Turn on VPC Flow Logs and wait about 5 minutes. Then look at all the traffic that is trying to hit the system you just turned on in the VPC Flow Logs. Next, turn on SSH or RDP and open up the network to provide access to those services from the Internet. Turn on Amazon GuardDuty. Then wait to see how long it takes GuardDuty to report RDP or SSH brute force attacks. In most cases, it will be less than one day, if not less than one hour.
What if we need to allow someone to administer a system running on AWS, Azure, or Google Cloud from varying remote locations but don’t want to expose SSH and RDP to the entire Internet? One option would be to provide VPN access, which allows the person to connect from anywhere on the Internet to the corporate network. Then only allow access to the Bastion host via RDP or SSH from the corporate network. The VPN will be another layer the attackers have to get through before they can get to RDP or SSH.
One concern with a VPN is the fact that traffic has to be back-hauled back to the corporate network however some creative cloud architecture solutions could get around this problem. Another issue is that you’ll still need to protect your VPN endpoint from attacks similar to those used while cloud penetration testing to simulate real-world attacks. You should implement two-factor authentication with your VPN solution and ensure you are using a secure VPN configuration.
Teri Radichel — Follow me @TeriRadichel
© 2nd Sight Lab 2020
____________________________________________
Want to learn more about Cloud Security?
Check out: Cybersecurity for Executives in the Age of Cloud.
Cloud Penetration Testing and Security Assessments
Are your cloud accounts and applications secure? Hire 2nd Sight Lab for a penetration test or security assessment.
Cloud Security Training
Virtual training available for a minimum of 10 students at a single organization. Curriculum: 2nd Sight Lab cloud Security Training
Have a Cybersecurity or Cloud Security Question?
Ask Teri Radichel by scheduling a call with IANS Research.
____________________________________
2020 Cybersecurity and Cloud Security Podcasts
Cybersecurity for Executives in the Age of Cloud with Teri Radichel
Teri Radichel on Bring Your Own Security Podcast
Understanding What Cloud Security Means with Teri Radichel on The Secure Developer Podcast
2020 Cybersecurity and Cloud Security Conference Presentations
RSA 2020 ~ Serverless Attack Vectors
AWS Women in Tech Day 2020
Serverless Days Hamburg
Prior Podcasts and Presentations
RSA 2018 ~ Red Team vs. Blue Team on AWS with Kolby Allen
AWS re:Invent 2018 ~ RedTeam vs. Blue Team on AWS with Kolby Allen
Microsoft Build 2019 ~ DIY Security Assessment with SheHacksPurple
AWS re:Invent and AWS re:Inforce 2019 ~ Are you ready for a Cloud Pentest?
Masters of Data ~ Sumo Logic Podcast
Azure for Auditors ~ Presented to Seattle ISACA and IIA
OWASP AppSec Day 2019 — Melbourne, Australia
Bienvenue au congrès ISACA Québec 2019 — Keynote — Quebec, Canada (October 7–9)
Cloud Security and Cybersecurity Presentations
White Papers and Research Reports
Securing Serverless: What’s Different? What’s Not?
Create a Simple Fuzzer for Rest APIs
Improve Detection and Prevention of DOM XSS
Balancing Security and Innovation with Event-Driven Automation
Critical Controls that Could have Prevented the Target Breach
Packet Capture on AWS
Cybersecurity in a Cloudy World
50 
50 claps
50 
Written by
Cloud Security Training and Penetration Testing | GSE, GSEC, GCIH, GCIA, GCPM, GCCC, GREM, GPEN, GXPN | AWS Hero | Infragard | IANS Faculty | 2ndSightLab.com
Cybersecurity in a Cloudy World
Written by
Cloud Security Training and Penetration Testing | GSE, GSEC, GCIH, GCIA, GCPM, GCCC, GREM, GPEN, GXPN | AWS Hero | Infragard | IANS Faculty | 2ndSightLab.com
Cybersecurity in a Cloudy World
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/investigating-new-york-city-public-datasets-with-bigquery-4e91c29d62e3?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
Originally published on the Google Cloud Big Data and Machine Learning blog
New York City is home to 8.5 million residents, and more than 50 million people visit this vibrant and dynamic city each year. With so many sights and sounds, it’s easy to get lost in the details, and lose sight of the big picture: How do New Yorkers actually survive the concrete jungle?
Thanks to NYC Open Data, which makes public data generated by city agencies available for public use, and Citi Bike, we’ve incorporated over 150 GB of data in 5 open datasets into Google BigQuery Public Datasets, including:
There’s no cost for the first terabyte of data you process each month, and because BigQuery is serverless, there’s no infrastructure you need to manage or maintain. That means we can focus on querying, joining and visualizing this data to learn more about New York City and the people who make up this bustling metropolis.
As you’ll see below, all these new data sets can be used with existing data, such as NOAA GSOD to discover trends based on changes in weather. We’ll also be continually adding new datasets from other cities, so soon you’ll be able to compare habits and trends between cities and countries around the globe — using BigQuery to better understand the world around us.
Find the New York City streets are you most likely to find a loud party
If there’s something strange in your neighborhood, the right number to call is 311; created specifically for non-emergency municipal inquiries and non-urgent community concerns. What does that include?
The graph below shows the top five reasons why New Yorkers call 311 over the past 4 years.
(To run this query yourself, you can copy/paste the above SQL into BigQuery, or follow this link to my shared query.)
Call volume tells us that it gets noisy in New York, and it also gets very cold. By joining the 311 calls to the NOAA GSOD weather table, we confirm that most calls about faulty heat and hot water happen when the temperature drops — while noise remains a constant annoyance.
There were also 267,887 calls about dead, damaged or dying trees, so you might wonder if there are any healthy trees left in NYC.
Find the Virginia Pines in New York City
The decennial NYC tree surveys from 1995, 2005, and 2015 are all available in BigQuery, and the 2015 survey found the London Planetrees, Honeylocusts and Callery Pears represented almost a third of all trees outside of parks.
Find the only collision caused by an animal that injured a cyclist
There’s a lot of traffic in New York, and while the number of accidents has increased each year, the number of injuries has remained fairly consistent.
Fortunately, the number of deaths has dropped by an average of 9% each year.
As you can see below, “Driver Inattention/Distraction” is the most likely cause of accident and injury, but disregarding traffic control (such as running a red light) is the most common cause of death.
The following graphs show that most traffic accidents happen in Brooklyn, but it’s Midtown and Downtown Manhattan that have the highest concentration of collisions — and Staten Island the highest proportion of deaths per accident.
With motor vehicle accidents resulting in 6 motorist deaths for each cyclist death (and no Citi Bike rider deaths), you might be safer taking a Citi Bike.
Find the Citi Bike record for the longest distance in the shortest time (on a route with at least 100 rides)
Comparing the average duration of 5 of the most popular Citi Bike routes, to taxi journeys beginning and ending within an approximately 50-meter radius of the corresponding Citi Bike stations, we see that for trips under 10 minutes there’s not much difference between taking a taxi or riding a bike.
Share your own insights and visualizations using the hashtag #TILwBQ, and join us here every week for Today I Learned with BigQuery, as we dig into these tables, launch new public datasets, demonstrate BigQuery, share protips and offer interviews with Big Data industry experts.
If you’re new to BigQuery remember that everyone gets 1TB at no charge every month to run queries; you can follow these getting started instructions to… get started.
Google Cloud community articles and blogs
15 
2
15 claps
15 
2
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@arweave/announcing-the-decentralised-public-library-cc3dd0235d3?source=search_post---------278,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Arweave Project
Sep 28, 2018·4 min read
Today we are thrilled to announce a brand new initiative — the Decentralised Public Library (DPL)! A component of the wider Arweave project, the DPL will be laser-focused on creating a truly permanent hand-curated archive of human history.
The DPL will permanently preserve vital pieces of human knowledge, utilising the unique features of the blockweave to safeguard this data, ensuring reliable access for all future generations.
Simply put, information in archives, libraries, data centres, online, and even in our own homes is surprisingly vulnerable to accidental or intentional damage and loss. Just like any physical building, libraries, archives, and data centres suffer from fires, theft, and natural disasters, despite their best efforts.
Sadly, even using a cloud storage service doesn’t guarantee your data’s safety. With a cloud provider, your data can become inaccessible, changed, or even lost for a lot of different reasons, including if:
So, although cloud storage services are simple to use, they don’t provide a truly reliable and permanent means of securing humanity’s collective knowledge-base.
As well as accidental damage, information is also vulnerable to purposeful attack. For example, there are many examples from history when information in archives and libraries has been censored and destroyed by government or other political forces. Sadly, these problems continue today. For example, the Cambridge University Press complied with a branch of the Chinese government and censored hundreds of academic articles, which now can’t be accessed from China.
If you’re interested in how Arweave combats political censorship, check out this talk by our CEO Sam Williams titled ‘Fighting Authoritarianism With Blockchain and Blockweaves’.
Therefore, although many organisations collect and store information, traditional technology can’t guarantee the safety of this data for future generations.
The DPL will solve this problem by utilising Arweave’s decentralised blockweave technology, to build a permanent archive of human knowledge and history!
To learn more about how the blockweave works, you can check out our explanatory blog post here.
In short, the goal of the DPL is to ‘defeat data death’ — that is, to prevent the accidental or intentional loss of information from human consciousness. We want to make information immortal, and we have the technology available to do so.
The DPL work closely with large information sources such as libraries and archives, but also with small and unique sources of primary data sources. For example, if you or a family member have a special collection of documents — perhaps your grandfather’s war diaries, an array of vintage newspapers, or even an old hard drive stocked full of internet memes — the DPL will want to speak with you! The aim is to make the process cost-free, simple, and convenient for those providing data, making it easy as possible to get this information preserved. The aim is to collect a wide array of data from across the globe, though for practicality reasons we will focus on English-language data sources initially, looking at expansion in the future.
The Arweave project itself will donate storage space on the blockweave for use by the DPL, offering the permanent back-up of historically important or obscure data at no additional cost.
As the DPL matures and grows, more and more valuable data will be held on the blockweave and will be intrinsically linked to the AR tokens that you hold. This will drive further growth, adoption, and awareness, especially when information has been altered or removed elsewhere on the internet, but remains in its original form on the weave. In turn, increased awareness will bring even more archivers storing information, developers creating apps, and more miners providing vital storage!
We firmly believe that this will boost mainstream awareness of Arweave as the blockweave becomes the primary, most reliable source of historically and socially important information. This feeds into the long-term mission of the wider Arweave project — the creation of a truly permanent hand-curated archive of human history.
If you have interesting data that you think the DPL should store, please get in touch! You can drop us an email at team@arweave.org any time — we would love to speak to you!
If you are interested in reading more about how Arweave achieves truly permanent data store in a ‘pay once, access forever’ model, check out our explanatory blog post here!
As always, if you want to keep up to date with the Arweave project follow us here on Medium and also over on Twitter.
-arweave-team
A novel data storage blockchain protocol enabling a permanent serverless web and creating truly permanent data storage for the first time.
See all (71)
231 
231 claps
231 
A novel data storage blockchain protocol enabling a permanent serverless web and creating truly permanent data storage for the first time.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-security/how-to-never-have-a-public-s3-bucket-639761508700?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
I was creating a simple website for my nephew’s painting business in Tigard, Oregon in an S3 bucket, and also handled a consulting call on this topic today, so I thought I’d write a quick blog post about it. Do you ever need a public S3 bucket? In a large company, I would say no.
If you aren’t aware, you can host a website in an S3 bucket. It’s really simple and by doing so you don’t have to set up a server. It works well for static websites that don’t need a back end web server or pushing out static files from a dynamic website. You can set up a domain name and…
"
https://medium.com/the-opsee-blog/launching-our-public-beta-3a5a516912f0?source=search_post---------280,"There are currently no responses for this story.
Be the first to respond.
It’s been a little over a year since we started Opsee. We knew that the changing landscape of code, the adoption of the cloud and containerization, would precipitate the need for better monitoring tools. Not just because the infrastructure is different, but because these changes are putting infrastructure in the hands of developers. All the while our colleagues at other monitoring companies have been busy ingesting more data, building ever more baroque dashboards, and generally adding complexity in their never ending quest to build a “single pane of glass”. We, however, suspected that a different approach was called for. So we started talking to potential customers, lots of them. As we talked to developers and operations teams a few themes quickly became clear:
Since then we’ve been building the Opsee product and making the tradeoffs and decisions that we think best serve our target user, the on-call developer. One of the earliest tradeoffs we made was to make Opsee AWS only. It narrows our applicability, certainly, but it also allows us to automate things other products can’t: give us a pair of AWS keys and we can get your entire environment monitored automatically.
Another early call we made was to abandon the traditional monitoring requirement of needing a software agent colocated in every system that a customer runs. Instead we decided that our software could run isolated on its own EC2 instance, shielding the customer from the risks associated with a more invasive agent.
And we also decided to focus on health checks as the means of monitoring services. Metrics are great, but they are too noisy of a data source to alert on. If we want alerts to be clear and actionable then their cause cannot be vague, it needs to be a very clear cut “is this thing working or not?”
Of course, in a real production environment developers care about more than just whether or not a service is responding. Opsee’s assertions capability allows developers to pull out arbitrary data from a health check response and ensure that it conforms to their expectations. And of course, an alert is useless if it doesn’t help you on the path to fixing a problem. That’s why Opsee lets you restart impacted instances directly from the alert.
We aren’t anywhere near done yet, but the team has done an amazing job bringing such a technically challenging product to fruition thus far. It may seem like a cliche, but there’s a lot of truth to the statement, “It’s really complex to make something simple.” And we feel like the product does enough right now to meet the needs of a whole bunch of developers out there. That’s why we’re taking the cover off and declaring the launch of our public beta.
Through the duration of the public beta period Opsee will be free of charge. We’ll be adding tons of good stuff to the product as well, including monitoring for RDS, ECS, and Elasticache. We’ll also be adding support for multiple Opsee instances and multiple logins per account.
So if you’re ready to stop fighting overcomplicated and antiquated monitoring systems, I invite you to Get Opsee and give us a try. Feel free to leave us your feedback here in the comments, or on twitter.
Add us to Your Blogroll
13 
13 claps
13 
Add us to Your Blogroll
Written by
Internet Thousandaire
Add us to Your Blogroll
"
https://medium.com/fhinkel/conference-talks-2017-3c0ed426406f?source=search_post---------281,"There are currently no responses for this story.
Be the first to respond.
Youtube Playlist of my talks
Like everybody else, I’m taking a conference break.
events19.linuxfoundation.org
gotocph.com
www.nodeconf.eu
javascript-conference.com
www.concatenate.dev
skillsmatter.com
events.google.com
www.covalenceconf.com
ng-atl.org
jsconf.co
2017.nodeconf.com.ar
events.linuxfoundation.org
www.meetup.com
revolutionconf.com
nodefest.jp
Franziska will discuss her perspective on Chrome V8 in Node.js, and what the Chrome V8 team is doing to continue to support Node.js. Want to know what the future of browser development looks like? This is a must-attend keynote. (Featured Speakers)
events.linuxfoundation.org
Lightning talk: Node and V8
2017.jschannel.com
Julian Sara Joseph summarized my talk in this post.
dinosaurjs.org
| JavaScript Engines: How Do They Even? — Dinosaur JS 2017
Confreaks TVconfreaks.tv
www.webrebels.org
2017.jsconf.eu
I like the longer version at JSChannel better. But with this video, you get the same content in half the time!
Want to know how JavaScript engines work? Why is JavaScript so fast? What is just-in-time compilation? We’ll look at basic concepts of compilers, challenges posed by modern JavaScript, and how to write compiler-friendly JavaScript. (Slides)
scriptconf.org
Do you care about performance? Memory leaks, megamorphic cache misses, and deoptimizations can slow your app down. We’ll dive deep into the inner workings of V8, Chrome’s JavaScript engine, to better understand what these terms mean. V8 ships with its own profiling tools that can identify such problems. You’ll learn to identify and fix low-level JavaScript performance issues using several V8 developer tools. (Slides)
scriptconf.org
Franziska Hinkelmann of the Google V8 team gave a very detailed talk about Performance Debugging of V8, explaining what the errors shown in the Chrome Profiler mean. It was an incredibly deep-tech talk but insightful. Franziska made sure to point out that optimising your code for the performance tricks of one JavaScript engine is not a good idea and gave ChakraCore several shout-outs. — Christian Heilmann
wolfgang-ziegler.com
This talks was one of my personal highlights since she dared to go really technical in her talk which is a thing that happens way to rarely at tech conferences IMHO. — Wolfang Ziegler
Node.js and JavaScript
30 
30 claps
30 
Written by
Principal Engineering Manager at Microsoft. Node.js Monkey Patcher.
Node.js and JavaScript
Written by
Principal Engineering Manager at Microsoft. Node.js Monkey Patcher.
Node.js and JavaScript
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/kubernetes-graceful-sidekiq-worker-lifecycle-71748db596f?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Jul 11, 2017·3 min read
With our recent release of Container Stacks v2 into public beta we’re totally loving Kubernetes. But as with all love affairs, there are some bothersome aspects that we have to accept and work with. One such aspect is in the inflexibility of the vanilla shutdown sequence provided by Kubernetes.
We’re also prolific users of Sidekiq for the parts of our backend that are ruby-based (we’re running a bunch of other technologies, but we think Sidekiq is hands-down the best for running ruby jobs). As with any background workers, Sidekiq is sensitive to its shutdown sequence. We need to have more control over this.
TL;DR
This article provides a solution to achieve graceful shutdown of Sidekiq workers via the Kubernetes pod shutdown lifecycle.
There is a lot of documentation out there around the current Kubernetes pod shutdown sequence (see appendices for some starting points). NOTE: I say current as this information is only valid as of now… this might change (though I think at this point that is fairly unlikely). The current shutdown sequence looks like the following:
Kubernetes allows you to specify the terminationGracePeriodSeconds (ie. how long it will wait for shutdown after SIGTERM sent) in your spec. Unfortunately Kubernetes doesn't allow you to specify the shutdown sequence itself.
At Cloud 66 we were previously lucky enough to be controlling the shutdown process via our own homegrown scheduler, this enabled us to expose the shutdown sequence to our users directly (in the form of USR1;1h;TERM;10s;KILL for example). But now we need another solution.
Furthermore (and specific to Sidekiq) as we have some very long running jobs (dependent on external resources), we want to have a long wait time; but also want to terminate the workers as soon as they are no longer busy. So our ideal Sidekiq shutdown sequence looks like the following:
Looking at the shutdown sequence above, you’ll see that there is a PreStop hook point called during the sequence. More on this in the Kubernetes Container Lifecycle Hooks documentation. The salient bit of information is essentially that kubernetes will execute some command of your choosing at that hook point, and it will execute it synchronously, waiting for the command to complete before resuming the shutdown sequence.
Using this hook point, we can inject the graceful shutdown behaviour we want for our Sidekiq workers. And because we need ths ourselves (and given that Sidekiq is ruby-based) I put together the following ruby script to do just that!
As the hook command executes in context of your image, you’ll need to include this script inside your image (simply put it in your source code if you’re using Cloud 66 Skycap). Note that the script is executed with the following arguments:
For the example below we’re putting this script in our image in the path:  /tmp/sidekiq_safe_shutdown.rb
And don’t forget to make it executable with:  chmod +x /tmp/sidekiq_safe_shutdown.rb)
Invoking via Kubernetes Manually
If you’re running Kubes directly, then you’ll need to manually modify your Pod spec to include terminationGracePeriodSeconds and invoking the PreStop hook:
Invoking via Cloud 66
If you’re running via our awesome Container Stacks v2 (shameless plug :) then simply add this script to your service.yml with the following line:
And that should be all you need — now when your Sidekiq workers shut down they will do so gracefully!
Request a Demo and learn more about Skycap and Container Stack v2.
Originally published at blog.cloud66.com on July 11, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
27 
27 claps
27 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud-opinion/electricity-from-a-grid-not-a-panacea-360438283f7c?source=search_post---------283,"Sign in
There are currently no responses for this story.
Be the first to respond.
.Cloud Opinion
Oct 18, 2015·2 min read
Electricity from a public grid is hailed as one of the key technological advancements of our generation. Electricity has replaced the need to make dinner on log-powered stove, and has allowed factories around the nation to extend worker hours by using electricity driven bulbs. There is certainly a segment of people that call for 100% adoption of electricity across all businesses and even homes. In this post, we want to discuss why such a strategy is flawed and ill-advised. We also caution that people calling for full adoption of electricity from a public provider may have vested interests.
Consider the following drawbacks of electricity from a grid:
- Putting all your eggs in one basket. If electric grid were to suffer an outage, you will not have an alternative, because by following electric grid advise, you would have gotten rid of your old equipment that runs on coal or logs.
- Lock-in: Often your choice of getting electricity is limited to one provider and they will lock you into them for life. Lock-in restrict you from adopting new technologies if and when they are available.
- Security Considerations: Electricity from grid is still not proven enough to be deemed secure enough for your organization. What if grid provider can not control the fluctuations in voltage and ends up damaging your equipment or worse yet cause injury/death to an employee? Is your business ready to accept these risks?
We believe electricity from a public grid is not suitable for all use cases and majority of our customers should instead adopt a hybrid electric model. In a hybrid electric model, you will manage your coal and log powered equipment and when needed can burst to consume electricity from grid. This allows you to leverage your existing investment and also gives you best of both worlds. You have safety and security of a proven technology while scalability of electricity from a grid provider. In fact, leading industry analysts have published research papers calling for Hybrid electric adoption model. In addition to hybrid electricity adoption, enterprises should also consider local generators that are emerging. These local generators are built by a consortium of vendors under OpenGridGeneration project. While OpenGridGeneration project is yet to produce a working generator, it has very important patterns for you to build your own generator on campus.
In Summary, while electricity from grid is good technology, it is not fit for all use cases. We urge caution of adopting electricity from a public grid.
If you are interested in electricity native appliances, please checkout an upcoming webinar from our consortium.
</parody>
Parody + Tech commentary.
8 
8 
8 
Parody + Tech commentary.
"
https://becominghuman.ai/finding-public-data-for-your-machine-learning-pipelines-e3de0b050c93?source=search_post---------284,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
The goal of the article is to help you find a dataset from public data that you can use for your machine learning pipeline, whether it be for a machine learning demo, proof-of-concept, or research project. It may not always be possible to collect your own data, but by using public data, you can create machine learning pipelines that can be useful for a large number of applications.
Machine learning requires data. Without data you cannot be sure a machine learning model works. However, the data you need may not always be readily available.
Data may not have been collected or labeled yet or may not be readily available for machine learning model development because of technological, budgetary, privacy, or security concerns. Especially in a business contexts, stakeholders want to see how a machine learning system will work before investing the time and money in collecting, labeling, and moving data into such a system. This makes finding substitute data necessary.
1. Six AI Subscriptions to keep you Informed
2. Bursting the Jargon bubbles — Deep Learning
3. How Can We Improve the Quality of Our Data?
4. Machine Learning using Logistic Regression in Python with Code
This article wants to provide some light into how to find and use public data for various machine learning applications such as machine learning demos, proofs-of-concept, or research projects. This article specifically looks into where you can find data for almost any use case, problems with synthetic data, and the potential issues with using public data. In this article, the term “public data” refers to any data posted openly on the Internet and available for use by anyone who complies with the licensing terms of the data./ This definition goes beyond what is the typical scope of “open data”, which usually refers only to government-released data.
One solution to these data needs is to generate synthetic data, or fake data to use layman’s terms. Sometimes this is safe. But synthetic data is usually inappropriate for machine learning use cases because most datasets are too complex to fake correctly. More to the point, using synthetic data can also lead to misunderstandings during the development phase about how your machine learning model will perform with the intended data as you move onwards.
In a professional context using synthetic data is especially risky. If a model trained with synthetic data has worse performance than a model trained with the intended data, stakeholders may dismiss your work even though the model would have met their needs, in reality. If a model trained with synthetic data performs better than a model trained with the intended data, you create unrealistic expectations. Generally, you rarely know how the performance of your model will change when it is trained with a different dataset until you train it with that dataset.
Thus, using synthetic data creates a burden to communicate that any discussions of model performance are purely speculative. Model performance on substitute data is speculative as well, of course, but a model trained on a well-chosen substitute dataset will give closer performance to actual model trained on the intended data than a model trained on synthetic data.
If you feel you understand the intended data well enough to generate an essentially perfect synthetic dataset, then it is pointless to use machine learning since you already can predict the outlines. That is, the data you use for training should be random and used to see what the possible outcomes of this data, not to confirm what you already clearly know.
It is sometimes necessary to use synthetic data. It can be useful to use synthetic data in the following scenarios:
Be cautious when algorithmically generating data to demonstrate model training on large datasets. Many machine learning algorithms made for training models on large datasets are considerably optimized. These algorithms may detect simplistic (or overly noisy) data and train much faster than on real data.
Methodologies for generating synthetic data at large scale are beyond the scope of this guide. Google has released code to generate large-scale datasets. This code is well-suited for use cases like benchmarking the performance of schema-specific queries or data processing jobs. If the process you are testing is agnostic to the actual data values, it is usually safe to test with synthetic data.
When using public data you must comply with any licenses and restrictions governing the usage of this data. Just because a dataset is available publicly does not mean you have a right to use it, and similarly, just because you are unable to identify license terms for a dataset, does not mean that you have a right to use some data. Some public datasets and data repositories have requirements around citing or referencing the data source. Others may have restrictions on the types of uses of that data. It is important to comply with all legal requirements when using data from public sources. There is no substitute for seeking legal counsel before using public data, and know that nothing in this guide constitutes any kind of legal advice.
Web address: https://toolbox.google.com/datasetsearch License(s): Varies, often displayed with search results.
Google Dataset Search is the recommended starting point for any data search. Many popular public data repositories are indexed, including many of the sources listed in this document and almost all government data.
Dataset Search supports the site: operator. Example.
Web address: https://www.google.com/ License(s): Varies, often inconclusive.
Google searches with keywords like “data” and “dataset” will sometimes lead you to the data you need. Google Search has additional features that enable powerful dataset searches:
Web address: https://www.kaggle.com/datasets License(s): Varies but displayed with results; ability to filter searches by license.
Search 10000+ datasets uploaded by Kaggle users or maintained by Kaggle. Many of the datasets are well-described, and some datasets also have example Jupyter notebooks (“Kernels”). Kaggle’s search interface also has filtering by dataset attributes, including license, and summarizes dataset attributes (also including license) with search results.
Kaggle offers an API for easy downloading of datasets.
Web address: https://console.cloud.google.com/marketplace/browse?filter=solution-type:dataset License(s): Varies, stated with Terms of Service for each dataset.
Over 100 datasets are available to all Google Cloud Platform users as BigQuery tables. The quality of these datasets is very high, and many datasets are updated continuously.
These are some of the highest quality public datasets available. It is worth scanning the catalog to see what’s available, since many common needs and use cases are supported by these datasets (weather, genomic, census, blockchain, patents, current events, sports, images, comments/forums, macroeconomic, and clickstream analytics, among other things).
Web address: https://ai.google/tools/datasets/ License(s): Varies.
Google regularly releases AI datasets to the public. Many of the datasets are existing data annotated with labels (either programmatically or manually), and most are unstructured data (text, video, images, sound).
Google Scholar is discussed below with academic literature search
Governments are a popular source of public data, and government data is often shared with generous license terms. But some government data is subject to restrictive licenses. You should not assume that all government datasets are suitable for use.
One unfortunate drawback of government open data sites is that many of the dataset results are redacted, and many “datasets” are hard-to-use file types like pdfs. One easy way to restrict the search on most CKAN cites is to explicitly include an extension in your search (compare to the results without an extension).
Web address: https://www.data.gov/ License(s): Varies, most search records include details.
100,000s of datasets are indexed by the U.S. Federal Government. Most of the data is from the U.S. Federal Government, but datasets from other levels of U.S. government and non-U.S. governments are also partially indexed. Ad-hoc searches show data.gov does a good job of indexing the listed U.S state and U.S. municipal open data sites.
Web address: https://www.europeandataportal.eu License(s): Varies, many search records include details.
This site indexes close to 1 million datasets, mainly data produced by the E.U. and EU member states (and, for now, the U.K.). 100,000+ datasets are in English, though some non-English datasets do have English search metadata. SPARQL querying is supported for searching linked data.
There are many data repositories hosting open government data. For the very thorough (or when the U.S. Federal Government is shut down), http://datacatalogs.org/search has a list of 500+ government open data repositories, and OpenDataSoft created a list of 2500+ open data repositories. Both of these links offer diminishing returns, and many of these long tail repositories are indexed by Google Dataset Search.
Other government data repositories with 10,000+ English datasets
The Freedom of Information Act (FOIA) is a law giving full or partial access to previously unreleased data controlled by the U.S. Federal Government. Similar laws may exist at the U.S. state level, and in other countries. If you think a government agency may have data of interest to you, consider filing a FOIA or similar request. At least one very popular open dataset resulted from a FOIA request.
Making a FOIA or similar request is not always straightforward, though there is help available on the web. In the U.S., most agencies now have specific FOIA procedures that have to be followed (these procedures are usually documented on an agency’s website), and the requestor has to pay the cost of obtaining the information, though the cost is usually minimal.
While a FOIA or similar request may seem like a lot to go through for a dataset, the costs are relatively low if the request is well-targeted and turnaround times are not unreasonable (the FOIA specifies 20 days).
Non-government organizations that host their own data.
Web address: https://archive.ics.uci.edu/ml/ License(s): Varies, may not be explicitly stated.
The UC Irvine Machine Learning Repository is one of the oldest and most popular data repositories. It indexes around 500 datasets and is heavily used in CS research — the number of times the UCI repository has been cited in scholarly research would place it in the top 100 most cited CS research papers of all time.
Web address: https://registry.opendata.aws/ License(s): Varies, but usually listed in a dataset’s metadata.
Around 100 large public datasets are hosted on AWS, usually in S3 buckets. While the selection is small, the quality of the data is high and most of the datasets are voluminous and unstructured — documents, text, images, etc.
A few of the datasets are also available in Google Cloud Storage buckets.
Web address: https://cran.r-project.org/index.html License(s): Varies, but clearly stated with each R package.
CRAN is the centralized repository of R packages, it is not technically a data repository. But the 10,000+ packages available through CRAN have over 25,000 .rda and .Rdata files, and many are real, high quality (mostly structured) datasets. Further, the contents of those datasets are documented in line with CRAN’s very high documentation standards.
Finding Datasets in CRAN. There is no easy way to search only the descriptions of packaged data files in CRAN, though it would be possible to build. But there are ways to do less-targeted searches:
Web address: https://www.openml.org/search?type=data License(s): Varies, but clearly stated with most datasets.
OpenML.org is an online platform for sharing machine learning data and experiments. Over 20,000 datasets are available on the platform, usually with indicated licenses, and many of the datasets have been reviewed by an administrator for quality.
Web address: https://guides.library.cmu.edu/machine-learning/datasets
Discover high quality datasets thanks to the collection of Huajin Wang, CMU.
Web address: https://msropendata.com/
A collection of free datasets from Microsoft Research to advance state-of-the-art research in areas such as natural language processing, computer vision, and domain specific sciences. Download or copy directly to a cloud-based Data Science Virtual Machine for a seamless development experience.
Web address: https://public.enigma.com/ License(s): CC BY-NC 4.0 (non-commercial)
Enigma is a data broker and services provider that maintains a repository of public data. The sources of the data appear to be FOIA requests, open government data, and scraped websites. There is a large quantity and variety of data, and the interface is well-designed.
Web address: http://academictorrents.com/ License(s): Varies, but usually listed in a dataset’s metadata.
Close to 500 datasets are indexed by Academic Torrents. The majority of the datasets are unstructured (video, images, sound, text), and many are exceptionally large, including some terabyte-scale image and video datasets.
Academic Torrents does not host the data itself, instead providing .torrent files and usually a link to a website or paper with more information about the dataset.
Web address: https://www.reddit.com/r/datasets/ License(s): Varies, based on the linked dataset.
Reddit’s r/datasets is a community of about 50,000 individuals dedicated to hunting down datasets. Users post requests for datasets and discuss data finding, usage, and purchase.
Reddit’s search functionality allows searching only r/datasets, and many kinds of hard-to-find data have been discussed and shared on r/datasets. You can also start a new discussion to get help finding a specific dataset.
Search engines and information repositories not specifically intended for data provide ways to track down data. Using Google Search to find data is discussed above.
Web address: https://github.com/search?type=Code License(s): Varies.
Many developers store data with code, so a lot of data is stored on GitHub. GitHub code search indexes the contents of GitHub repos, and supports searching files by extension (example). The advanced search also allows filtering by license, which parses the LICENSE file found in many repos, though the license search may not be accurate or comprehensive.
Web address: https://archive.org/ License(s): Varies.
The Internet Archive is a “digital library of Internet sites and other cultural artifacts in digital form.” It holds a large collection of different types of information, although much of it is disorganized, of unclear origin, and not easily automatically parsed (pdfs, images, audio, and video).
Note that licenses can be difficult to determine. If you cannot find a license, you may still be able to track down the original content owner.
Datasets are vital to research in machine learning and other fields, and close to 1,000 journals require data sharing as a condition for publication. Even when not required, authors often release their data (and code), and the tradition of information sharing in academia means that asking for data used in a publication often results in a response.
Web address: https://scholar.google.com/ License(s): N/A
Google Scholar is the most comprehensive index of academic literature on the web (here’s a short overview of how to use Google Scholar). Simples searches can reveal many papers on a specific use case, and searches targeting specific types of data can be productive as well.
Web address: https://arxiv.org/ License(s): N/A
Arxiv is an archive of academic research (technically preprints) with a focus on the sciences. Because Arxiv is especially strong in computer science and machine learning, searching for types of data and specific machine learning problems can give very relevant results.
Once you’ve identified an academic publication that may be relevant, you have to track down the dataset.
Once you find where the dataset is described, look for citations, web links, or even a dataset name (common when using extremely popular datasets). But sometimes few details are shared (see “Contact the Authors” below).
Many journal articles now have code and data online, and Google Search and GitHub code search will sometimes give results. Searching for the name of the paper may also lead to someone who has implemented the paper’s method independently, who may have their own dataset.
If authors cannot share data, they may be able to put you in touch with whoever provided them their data. Additionally, authors tend to know the problems they publish on well, and can maybe point you to similar datasets.
If authors do offer to share the data with you, make sure you understand the terms/license for using the data before using it. You may need to ask authors directly for this. Be aware it is very common for datasets used in academic research to forbid commercial use.
Sources for specific types of data, sometimes unlabeled.
Many websites list links to popular and useful datasets and data repositories. These ones are high quality, with relatively few broken links and a good variety of datasets and repositories.
Unlabeled data is easier to obtain than labeled data, and labeling data is easy and relatively straightforward. You may not need many labeled examples to make a useful dataset, making self-labeling a reasonable choice when you need a dataset.
Many organizations and teams discount the idea of creating their own labels as being too time consuming, too expensive, or even too boring. But for a demo, solution, proof-of-concept, or research project, modest amounts of labeled data may be all this is necessary. If you do not require a large amount of labeled data, and/or your labeling task is unusual, self-tagging may save you time and money versus a labeling service.
A key to successful do-it-yourself labeling is to not overcomplicate the labeling task. Many labeling tasks can be done with just some data munging and a spreadsheet. Focus on getting labels rather than task piloting and design, tool standup, and reliability measurement.
If you need a labeling tool, try to avoid building your own and consider some free and paid alternatives.
Labeling services are a popular way of getting labeled data. These services recruit and manage workers to do your labeling tasks.
Ideally, a labeling task labels neither too little nor too much data. Learning curves help determine if additional labels will improve performance of a model. Learning curves also provide reasonable estimates of the performance ceiling of a specific model+dataset.
The general idea behind learning curves is to train your model with progressively larger training sets and plot the performance metrics. This blog post goes into greater detail.
If you have some labeled data, you can create more labeled data using machine learning. Be aware that using machine learning to label unlabeled data has a risk - any information or patterns not present in your original labeled data but present in your unlabeled data will go unnoticed. But if you need more labels to show model training on large amounts of data, or if you want to train a model that is particularly data hungry, automatic label generation approaches may be suitable.
If you need additional labeled data to show model training at large scale or to demonstrate a modeling technique that requires a lot of labeled data, consider training a model on the data you have and labeling your unlabeled data with predictions from the model. If the modeling technique you intend to use is discriminative, you can use a similar generative modeling technique to label data and improve the performance of the discriminative model.
Machine learning methods specifically for labeling unlabeled data are an active area of research. Good starting points are scikit-learn’s implementations of label propagation and label spreading.
Rarely will you find the exact dataset that you want, but you can almost always find something close to it. Often close is good enough. If the license of a dataset allows it, consider modifying the dataset into the data you want.
A good dataset for modification has similar structure to your ideal dataset and is governed by the same statistical processes. It may not be from the same domain, or have similar labels or field names — these are what you can change.
Also consider using a substitute dataset instead of modifying a dataset. Sometimes the goals of your machine learning work (especially for demos and proofs-of-concept) can be met by a less-than-perfect dataset that still extends to the problem you are discussing.
Your customer is a publisher of religious books, and you are demonstrating a model that predicts weekly sales of their books per store. This dataset of liquor sales in Iowa is probably suitable, but you may want to change the names of some columns, remove other columns, and change some of the categorical labels to be related to books rather than liquor.
For a research project, you want to demonstrate a machine learning model that counts the number of people in a park. The Stanford Drone Dataset has drone footage of various scenes with labeled bounding boxes around pedestrians. You can use this dataset to create a derivative dataset of still images with counts of pedestrians.
This dataset of pairs of sentences with a 1–5 label of how “semantically related” (same meaning) the two sentences are could be used for other purposes. For example, a modeling pipeline that performs well on this dataset could be the core component of a system that detects patents with a large number of claims that are similar to claims in other patents.
You want to demonstrate a system that can read an email and predict what product(s) are mentioned in the email. This Stack Overflow dataset, specifically the data of questions with topic/programming language tags, could let you demonstrate that predicting meaningful labels from a block of text is possible.
By Bo Yang, Alibaba Cloud Community Blog author.
www.alibabacloud.com
Latest News, Info and Tutorials on Artificial Intelligence…
57 
Watch AI & Bot Conference for Free Take a look.
57 claps
57 
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@arinbhowmick/ibm-cloud-satellite-wins-business-intelligence-group-award-for-business-2021-new-product-of-the-8e01ddd6f7e4?source=search_post---------285,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arin Bhowmick
Dec 7, 2021·3 min read
IBM Design is committed to using our design thinking framework and innovative ideas to solve our clients problems. Our product team at IBM Cloud Satellite worked tirelessly to use this strategy to bring cloud technology to even the most regulated industries. I’m excited to announce IBM Cloud Satellite received a 2021 New Product of the Year award from the Business Intelligence Group’s Big Awards for Business.
The Business Intelligence Group’s annual competition is the premier “____ of the Year” program, recognizing companies, products and people which lead in their respective industries. Applicants were judged by a jury of business executives using a unique scoring system which measures performance across multiple business domains. “We are so proud to reward IBM Cloud Satellite for their outstanding 2021 leadership and achievements,” said Maria Jimenez, chief nomination officer of the Business Intelligence Group. “This year’s group of winners are clearly leading by example in the global business community.”
I’ve seen the dedication the team has put into this product and I’m so proud they’ve been awarded for their efforts.
IBM set a goal to be the top Hybrid Cloud provider, and revolutionize how customers can leverage cloud technology. Utilizing IBM’s Enterprise Design Thinking (EDT) framework, the product team at IBM Cloud Satellite set out to achieve this. The team conducted extensive user research, interviewing clients to better understand their customer journey and learning about a gap in the market which needed to be filled.
Shifting to the cloud can be a complicated process for any company but even harder for highly regulated industries. Strict compliance standards have typically prevented organizations like banks and hospitals from accessing cutting edge technology solutions. IBM Cloud Satellite was created to meet the needs of highly regulated industries in a rapidly evolving cloud market. Following the EDT framework, the product team built Satellite from the ground up using a ‘secure by default’ approach. This approach ensured compliance standards are met and clients feel confident and protected from rapidly growing security threats.
Enabling customers to embrace cloud solutions was just the first step. The product team used their research insights to create an intuitive support experience for customers who were using cloud for the first time. During the team’s beta testing, the process to set up a Satellite location required users to navigate 90 different screens. It was taking up to a month for clients to complete the complicated, multi-step process. The product team collaborated with engineering and product management to create a user experience that educates users about key Satellite concepts using progressive disclosure and cutting edge automation tools. This design solution, in combination with a rapidly evolving set of documentation, reduced the amount of customer support our users needed as they adopted Satellite to a simple 15 minutes process.
The product team at IBM Cloud Satellite turned the concept of the cloud upside down, enabling enterprises to use cloud services on-premises, at the edge, or on other public clouds. I’m so proud of our team at IBM Cloud Satellite for their innovation and diligence. They’ve managed to craft an exceptional product which is breaking barriers in the world of hybrid cloud, and I’m thrilled they’ve been recognized with this award.
Congratulations to the IBM Cloud Satellite product team for this stupendous recognition.
Arin Bhowmick (@arinbhowmick) is Vice President and Chief Design Officer, IBM Software Products, based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
76 
76 
76 
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
"
https://medium.com/thinking-design/from-the-street-to-the-screen-street-arts-influence-on-digital-design-cd3f036cd280?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
Street art lives in public spaces and is a traditional culture analogue for digital media. While physical street art is geographically constrained, it is often ephemeral. If images of graffiti are spread via the internet they’re no longer constrained, but they’re only ephemeral if they’re superseded by our attention on the next new thing. Graffiti can mark a landscape for a long time, or can quickly be washed off, but it’s said that what’s on the internet is on the internet forever.
“Graffiti’s always been a temporary art form. You make your mark and then they scrub it off.”
– Banksy
In the past, graffiti was the art of outlaws and scofflaws. Art that was made clandestinely in the dark of night only to be seen for a few seconds by those standing on a subway platform. Today, street art is part of our urban landscape. The color and flavor of our cities and culture.
The subcultures of skateboarding and hip-hop have co-opted and combined urban art to create video game experiences like Jet Grind Radio (Dreamcast), Revoltin’ Youth (PSP) or Sideway: New York (PS3). Each game incorporates creating or interacting with street art to propel the gameplay forward.
Exploring Physical Spaces
In order to promote tourism to the Rio Summer Olympics and to encourage visitors to explore the the Olympic Boulevard and Lapa District, Instagram created InstawalkRio, a self-guided urban art tour that highlighted the best murals and graffiti walls of Rio. The art tour encouraged Individuals to share their experiences on Instagram using the hashtag #InstawalkRio.
Some artists post their work online to encourage people to explore out of the way places. The video game pixel art of Toronto’s GameBoyOne can be found in less traveled alleys and abandoned buildings, where our favorite 8-bit video game characters live in tucked away spaces.
Graffiti and sculptor Peeta (a.k.a Manuel Di Rita) uses Photoshop, Illustrator and CAD software to create digital graffiti that is used to build 3D sculptures and murals that create depth and hyper realism to traditional street art. Exaggerated curves and angles are digitally modeled from sketches which are then constructed and integrated into physical locations. Traditional spray painting techniques for line work, light volume and shadows are used to give the installations a surreal, industrial look that challenges the viewer’s perception of depth and space.
Augmenting Reality and Projection Mapping
Mixing murals and augmented reality has been a primary way for street art to merge with technology and extend visual experiences online, and this has influenced trends in advertising and digital media.
Gif-iti are animated online paintings that exist ephemerally in the real world. Murals are painted, photographed and repainted. Visitors to his installations can download the GIF-ITI Viewer App to experience what the artist INSA calls “slices of infinite un-reality.” The augmented reality interactive murals can be viewed online or in various locations around the world including Toronto, Paris, Miami and Los Angeles.
The Light Painting WiFi reveals the invisible landscapes of WiFi networks that weave through the our neighborhoods and cities. Using a measuring rod with 80 LED lights that visualize signal strength and long exposure photography to capture the “immaterial terrain” of the internet around the city of Oslo. The images show the invisible information in our urban landscapes.
Graffiti artist Sofles has used traditional graffiti techniques with motion graphics and video mapping on a large scale mural to create Graffiti Mapped, an immersive layered multimedia experience for the White Night Melbourne cultural festival. The image illustrates the scale and physicality of the mural. A parking lot wall is the new cultural canvas.
The V-Motion Projection/Kinects Project is an interactive DJ performance merging gesture based interaction, music and video visualization. As part of an energy drink promotion, V-Motion was part dance performance and part music video that created music through motion capture. Two Kinect cameras captured the movements of hip-hop dancer Josh Cesan and form the vectorized green man avatar that was part of an energy drink promotion and advertising campaign.
Using a portable camera, VJ Sauve brings projection mapping and storytelling into a group bicycling experience. Riders follow Sauve through the streets of Sao Paulo as animations are played on buildings and alleyways. The urban space becomes the canvas on which digital stories are told, enabling a shared experience of exploring and following the story while biking through the city streets.
Additional work involving the merger of pixels and graffiti is being done by the Graffiti Research Lab. They are building open source tools to help artists grow digital street art and foster a social dialog in urban settings. Their Eyewriter Project empowered graffiti artist Tony Quan a.k.a. Tempt One, who is paralyzed due to ALS, to create unique projection graffiti art through eye tracking technology. The pieces are then visualized with lasers in gallery and urban settings.
Street art and technology continue to evolve and intertwine, extending into different areas of design and creating new creative outlets and audiences. The next time you pass a piece of graffiti, consider whether it is simply street art or whether it has a digital component yet to be experienced and discovered. Technology is empowering street artists to find new venues for their creativity mixing analog and digital within cityscapes and fostering community experiences. It’s goes beyond designing for not just any screen size, but for any canvas!
Andrew Smyk is a dad, educator and UX designer with a focus on Mobile Design. He is also a coffee aficionado, avid cyclist, all-round pirate and HTML5 Evangelist. Andrew coordinates a postgraduate program in Interactive Media Management at Sheridan College and writes about how kids adapt and use technology on his blog. Follow him on Twitter: @andrewsmyk
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
10 
10 claps
10 
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kinlane/thinking-about-the-monetization-layer-for-public-data-143dc7a10c2a?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kin Lane
Jan 5, 2017·21 min read
This is my walk-through of the concepts involved with the monetization of public data using APIs. In this work I am not advocating that companies should be mindlessly profiting from publicly available data, my intent is to provide a framework for organizations to think through the process of generating revenue from commercial access to public data, acknowledging that it costs money to aggregate, serve up, and keep data up to date and usable for the greater public good — if public data is not accessible, accurate, and up to date it is of no use to anyone.
I have long argued that companies and even government agencies should be able to charge for commercial access to public data and be able to generate revenue to cover operational costs, and even produce much-needed funds that can be applied to the road map. My work in this has been referenced in existing projects, such as the Department of Interior and Forest Service looking to generate revenue from commercial access and usage of public data generated by the national parks systems. In my opinion, this conversation around generating revenue from publicly available digital assets should be occurring right alongside the existing conversations that already are going on around publicly available physical assets.
Building Upon The Monetization Strategies Of Leading Cloud ProvidersMy thoughts around generating revenue from public open data is built upon monitoring the strategies of leading online platforms like Amazon Web Services, Google, and others. In 2001 a new approach to providing access to digital resources began to emerge from Internet companies like Amazon and Salesforce, and by 2016, it has become a common way for companies to do business online, providing metered, secure access to valuable corporate and end-users data, content, and other algorithmic resources. This research looks to combine these practices into a single guide that public data stewards can consider as they look to fund their important work.
Do not get me wrong, there are many practices of leading tech providers that I am not looking to replicate when it comes to providing access to public data, let alone generating revenue. Much of the illness in the tech space right now is due to the usage of APIs, it is due to a lack of creative approaches to monetizing digital assets like data and content, and terms of service that do not protect the interest of users. My vantage point is the result of six years studying the technology, business, and politics of the API sector, while also working actively on open data projects within city, state, and federal government — I’m looking to strike a balance between these two worlds.
Using Common Government Services As A Target For Generating Much-Needed RevenueFor this research, I am going to use a common example of public data, public services. I am focusing in this area specifically to help develop a strategy for Open Referral but it is also a potential model that I can see working beyond just public services. I am looking to leverage my existing Open Referral work to help push this concept forward, but at the same time, I am hoping it will also provide me with some public data examples that are familiar to all of my readers, giving me with some relevant ways to explain some potentially abstract concepts like APIs to the average folk we need to convince.
For the sake of this discussion things down and focus on three areas of public data, which could be put to work in any city across the country:
Open Referral is a common definition for describing public services organizations, locations, and services, allowing the government, organizations, institutions, and companies to share data in a common way, which focuses on helping them better serve their constituents — this is what public data is all about, right? The trick is getting all players at the table to speak a common language, one that serves their constituents, and allows them to also make money.
While some open data people may snicker at me suggesting that revenue should be generated on top of open data describing public services, the reality is that this is already occurring — there are numerous companies in this space. The big difference is it is currently being done within silos, locked up in databases, and only accessible to those with the privileges and the technical expertise required. I am looking to bring the data, and the monetization out of the shadows, and expand on it in a transparent way that benefits everybody involved.
Using APIs To Make Public Data More Accessible and Usable In A Collaborative WayPublicly available data plays a central role in driving websites, mobile applications, and system to system integrations, but simply making this data available for download only serves a small portion of these needs, and often does so in a very disconnected way, establishing data silos where data is duplicated, and the accuracy of data is often in question. Web APIs are increasingly being used to make data not just available for downloading, but also allow it to be updated, and deleted in a secure way, by trusted parties.
For this example I am looking provide three separate API paths, which will give access to our public services data:
A website provides HTML information for humans, and web APIs provides machine readable representations of the same data, making it open for use in a single website, but also potentially multiple websites, mobile applications, visualizations, and other important use cases. The mandate for public data should ensure it isn’t available on a single website but is as many scenarios that empower end-users as is possible. This is what APIs excel at, but is also something that takes resources to do properly, making the case for generating revenue to properly fund the operations of APIs in the service of the public good.
The Business of Public Data Using Modern Approaches to API ManagementOne of the common misconceptions of public web APIs is that they are open to anyone with access to the Internet, with no restrictions. This might be the case for some APIs, but increasingly government agency, organizations, and institutions are making public data available securely using common API management practices defined by the Internet pioneers like Salesforce, Amazon, and Google over the last decade.
API management practices provide some important layers on top of public data resources, allowing for a greater understanding and control over how data is accessed and put to use. I want to provide an overview of how this works before I dive into the details of this approach by outlining some of the tenets of an API management layer:
These seven areas provide some very flexible variables which can be applied to the technical, business, and politics of providing access to public data using the Internet. Before you can access the organizations, locations, and service information via this example public services API you will need to be a registered user, with an approved application, possessing valid API keys. Each call to the API will contain these keys, identifying which tier of access an application is operating within, which API paths are available, the rate limits in existence, and logging of everything you consume and add so it can be included as part of any operational analytics.
This layer enables more control over public data assets, while also ensuring data is available and accessible. When done thoughtfully, this can open up entirely new approaches to monetization of commercial usage by allowing for increased rate limits, performance, and service level agreements, which can be used to help fund the public data’s mandate to be accessible by the public, researchers, and auditors.
Providing The Required Level Of Control Over Public Data AccessUnderstandably, there concerns when it comes to publishing data on the Internet. Unless you have experience working with modern approaches to delivering APIs it can be easy to focus on losing control over your data when publishing on the web — when in reality data stewards of public data can gain more control over their data when using APIs over just publishing for a complete download. There are some distinct ways that API providers are leveraging modern API management practices to evolve greater levels of control over who accesses data, and how it is put to use.
I wanted to highlight what can be brought to the table by employing APIs in service of public data, to help anyone make the argument for why providing machine readable data via APIs is just as important as having a website in 2016:
It is important to discuss, and quantify this control layer of any public data being made available via APIs if we are going to talk about monetization. Having APIs is not enough to ensure platform success, and sometimes too strict of control can suffocate consumption and contribution, but a lack of some control elements can also have a similar effect, encouraging the type of consumption and contribution that might not benefit a platform’s success. A balanced approach to control, with a sensible approach to management and monetization, has helped API pioneers like Amazon achieve new levels of innovation, and domination using APIs — some of this way of thinking can be applied to public data by other organizations.
Enabling and Ensuring Access To Public Data For Everyone It TouchesProviding access to data through a variety of channels for commercial and non-commercial purposes is what modern API management infrastructure is all about. Shortly after possessing a website became normal operating procedure for companies, organizations, institutions, and government agencies, web APIs began to emerge to power networks of distributed websites, embeddable widgets, and then mobile applications for many different service providers. APIs can provide access to public data, while modern API management practices ensure that access is balanced and in alignment with platform objectives — resulting in the desired level of control discussed above.
There are a number of areas of access that can be opened up by employing APIs in the service of public data:
In a world that is increasingly defined by data, access to quality data is important and easy, secure access via the Internet is part of the DNA of public data in this world. API management provides a coherent way to define access to public data, adhering to the mandate that the data is accessible, while also striking a balance to ensure the quality, reliability, and completeness of the public data.
There has been a lot of promises made in the past about what open or public data can do by default when in reality opening up data is not a silver bullet for public services, and there is a lot more involved in successfully operating a sustained public data operation. APIs help ensure data resources are made available publicly, while also opening up some new revenue generation opportunities, helping ensure access is sustainable and continues to provide value — hopefully find a balance between public good and any sensible commercial aspirations that may exist.
APIs Open Up Many Potential Applications That Support the MissionAs doing business on the web became commonplace in the early 21st century, Amazon was realizing that they could enable the sales of their books and other products on the websites of their affiliate partners by using APIs. In 2016 there are many additional applications being developed on top of APIs, with delivering public data to multiple web sites being just the beginning.
This is just an overview of the number of ways in which a single, or multiple APIs can be used to deliver public data to many different endpoints, all in service of a single mission. When you consider this in support of public services, a bigger picture of how APIs and public data can be used to better serve the population — the first step always involved standardized, well-planned set of APIs being made available.
The Monetization Requirements Around Public Data API OperationsThis is where we get to the portion of this discussion that is specifically about monetization of the operations around publishing and maintaining high-quality sources of public data. Before a sensible monetization strategy can be laid out, we need to be able to quantify what it costs to operate the platform and generate the expected value from everyone at the table.
What are the hard costs that should be considered when operating a public data platform and looking to establish some reasonable monetization objectives?
Understand The Value Being Generated By Public DataNow that we understand some of our hard costs, let’s have an honest conversation about what value is being generated? First, public data has to offer value, or why are we doing all this hard work? Second, nobody is going to pay for anything if it doesn’t offer any value. Let’s stop for a moment and think about why we are doing all of this in the first place, and what value is worthy of carving off to drive monetization efforts.
While there may be other hard costs associated, as well as areas of value being generated, this should provide a simple checklist that any open data provider can use as a starting blueprint. Additional costs can be included on in these existing areas or added on as new areas as deemed relevant — this is just about getting the monetization conversation going.
There are two main objectives in this exercise: 1) understanding the hard costs and value associated with operations 2) assembling into a coherent list so that we can explain to others as part of transparency efforts. When it comes to the business of public data, it is more than just generating revenue, it is about being upfront and honest about why we are doing this, and how it is done — mitigating the political risk involved with doing business with public resources out in the open.
Putting Together A Working Plan Involving Public DataWith an understanding of the hard costs of providing a public data platform and an awareness of the intended value to be generated via operations, we can now look at what details would be involved in a plan for executing this monetization strategy. API management practices are architected for metering, measuring, and limiting access to data, content, and algorithmic resources in service of a coherent, transparent public data monetization strategy.
Here is a core framework of API management that can be applied to public data that can be used to drive monetization efforts:
These are the moving parts of a public data monetization strategy. It allows any public data resources to be made available on the web, enabling self-service access to data 24/7. However, it does it in a way that requires accountability by ALL consumers, whether they are internal, partner, or the public at large. This API management scaffolding allows for the frictionless access to public data resources by the users and applications that are identified as worthwhile, and imposing limits, and fees for higher volume and commercial levels of access.
Speaking To A Wide Audience With This Public Data Monetization ResearchI purposely wrote this document to speak to as wide as possible audience as possible. In my experience working with public data across numerous different industries, there can be a wide variety of actors involved in the public data stewardship pipeline. My objective is to get more public data accessible via web APIs, and generating revenue to help fund this is one of my biggest concerns. I am not looking to incentivize people in making unwarranted profits on top of public data, this is already going on. My goal is open up the silos of public data out there right now, make them more accessible, while opening up the opportunity for delivering to a variety of applications, while also funding this important process.
I wanted to help anyone reading this to craft a coherent argument for generating much-needed revenue from public data, whether they are trying to convince a government agency, non-profit organization, institution, or a commercial company. Public data needs to be available in a machine-readable way for use in a variety of applications in 2016 — something that takes resources and collaboration. APIs are not another vendor solution, they are the next step in the evolution of the web, where we don’t just make data available for humans by publishing as HTML — we need the raw data available for use in many different applications.
I am the @apievangelist , and chief evangelist for @getpostman , sharing stories of the API lifecycle, and helping you make sense of everything API.
6 
6 
6 
I am the @apievangelist , and chief evangelist for @getpostman , sharing stories of the API lifecycle, and helping you make sense of everything API.
"
https://medium.com/crustnetwork/announcement-mainnet-launch-public-access-phase-coming-soon-3c577cf1c9bc?source=search_post---------288,"There are currently no responses for this story.
Be the first to respond.
On August 12th, the Crust mainnet entered the public access phase. Nodes are now allowed to join, and CRU transfers are enabled. The Crust node manual and CRU mainnet mapping tutorial can be found at Crust Wiki (https://wiki.crust.network). Crust mainnet is expected to be launching at the beginning of September, and will open more functions at that time.
Please note:
Nodes on Crust mainnet are divided into different types: Owner Nodes, Member Nodes and Isolation Nodes.
A Group is an on-chain logic organization and consists of an Owner and at least one Member. Member nodes cluster effective storage to an Owner by participating in a Group, increasing the staking upper limit of the Owner.
Owner nodes are the initiator of and in charge of the Group, participating in block generation.
Member nodes act as storage providers in the Group. There can be multiple Member nodes in a Group, and their effective storage can be clustered to an Owner to participate in the block generation competition.
Isolation nodes are full-featured nodes of Crust Network, which undertake core functions such as block generation, storage, and file transfer, equivalent to that of Owner and Member nodes if they were based on the same machine. Therefore, support for SGX is necessary.
How to build the three node types:
https://wiki.crust.network/docs/en/nodeOverview
Hardware requirements for the three node types:
https://wiki.crust.network/docs/en/nodeHardwareSpec
Node flow control tutorial:
https://wiki.crust.network/docs/en/nodeQos
Map from Maxwell to Mainnet
Users need to map their Crust Maxwell CRU back to ERC20 CRU, then map the ERC20 CRU to the mainnet.
https://wiki-maxwell.crust.network/docs/en/claimBack
Map ERC20 to Mainnet
https://wiki.crust.network/docs/en/claims
A guarantor is an account that provides a guarantee for one or more nodes in the Crust Network. Any account with CRU tokens can potentially become a guarantor and provide CRU as guarantee assets. The guarantor provides guarantees for the node to obtain guarantee rewards, and accepts a penalty risk of the node in proportion. Please refer to the Crust Economy Whitepaper for details.
Guarantor Tutorial:
https://wiki.crust.network/docs/en/guarantorGuidance
The storage market will be opened soon after the mainnet launch.
Website | Twitter | Facebook | Telegram | Discord | Subreddit
Decentralized Cloud Blockchain Technology
20 
20 claps
20 
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
"
https://medium.com/@mentormate/why-the-cloud-is-the-future-of-healthcare-90e06d8f8841?source=search_post---------289,"Sign in
There are currently no responses for this story.
Be the first to respond.
MentorMate
Dec 28, 2016·4 min read
Public clouds like Amazon Web Services (AWS) have matured to the point where they can pass the most rigorous compliance concerns and find efficiencies in healthcare process improvement. The companies that market them have also matured in their understanding of the business relationships required (such as a BAA for HIPAA) to support them.
For enterprises with large computing and data problems (such as genomics analysis) to solve, the economics can be as much as 1/10th the cost for comparable solutions with shorter ramp times. Coupled with the ability to apply scalable computing to those arbitrarily large data streams in both near real-time stream and batch modes, the message is clear.
The cloud isn’t just healthy. It is the future of healthcare.
Offerings are now as wide as they are deep. This includes a large number of “big data” capabilities able to handle large input quantities in a variety of structured and unstructured formats from multiple sources including IoT devices, voice, imagery, manufacturing and lab equipment.
Not only is the “shared responsibility” model for compliance well understood and codified, but it has been tested many times over by organizations large and small. There is even a healthy market of value-added resellers providing templated services to simplify HIPAA compliance for startups and established companies alike.
Soon using the suite of AWS services, cloud architects can begin realizing large scale healthcare solutions with interconnected services that can be secured and scaled flexibly to manage healthcare process improvement.
The challenge? Selecting correctly from the multiple options available and knowing that you are trying to hit a moving target. The things you build to bridge gaps today will probably be rendered obsolete within the next year or be easily replaced by new services.
Amazon Web Services (AWS) saw measurable growth in 2016. Amazon announced twenty major services spanning literally a thousand or so important features for new and past services Over the next few years, AWS is projecting that the total market size for cloud managed services (including partner delivered professional services, managed services and migration services) will grow to over $75 billion in the next few years.
As an engineering services provider, AWS is a solid investment. AWS itself netted $13 billion revenue in the trailing 12 months and 55% year-over-year growth. Those numbers are impressive, but the partner ecosystem around them is growing even faster. Many are reporting gross margins as high as 75%.
Now that cloud migrations (of databases in particular) are commonplace in the enterprise tier, “the cloud” has become mainstream. Even relative laggards like healthcare and public sector businesses are expected to grow rapidly in the coming year or two.
I attended my first AWS re:Invent 2016 in December. While I’ve been through basic certifications before and have used AWS in a variety of hosting situations as more than casual user for years, little prepared me for the onslaught of people, new products and in particular, the overwhelming representation of healthcare and life sciences at the conference.
AWS describes the foundational elements for cloud-related service providers as:
While most of the sessions I chose to attend leaned into healthcare process improvement, compliance, security, genomics or life sciences in general, I came away impressed by the sheer number of large biotechnology and pharma organizations that have already committed to the cloud. They are realizing dramatic reductions in the time and cost required to manage large datasets and compute farms for problems including genomics processing.
There is an important transformation occurring referred to as “continuous compliance.” Our tools have matured to the point where — just like with test automation or continuous integration — we can easily build systems that do the logging, monitoring and alerting needed to actively police compliance with rules established for securing cloud environments.
As demonstrated by presenters, AWS is well-equipped to handle healthcare process improvement, governance, compliance, security, HIPAA and the underlying cloud capabilities to support these. This approach to “DevSecOps” seems much easier to accomplish in AWS than it would be in proprietary data centers as the tools are already available and designed to work well together.
The cloud is not a fad or even cutting edge anymore — it is a reality that must be faced by enterprise businesses. In the coming year, architects and solution providers will have the opportunity to provide architectural, development, DevSecOps and testing services at an accelerating pace.
Innovate with us. Click here to access all of our free resources.Authored by Craig Knighton.
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
6 
6 
6 
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
"
https://medium.com/@mohamed-ahmed/https-medium-com-magalix-magalix-node-advisor-2-0-for-kubernetes-is-now-in-public-preview-bd58e284f8cd?source=search_post---------290,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Ahmed
Jun 12, 2019·5 min read
Magalix Node Advisor 2.0 is an advanced analytics service to optimize your Kubernetes cluster. It saves you hundreds of hours figuring out the right combination of nodes to achieve the best performance, highest utilization, and lowest cost. You’ll get the best combination of node types and sizes. You won’t need any specialized knowledge to understand the performance and cost implications of these recommendations. You’ll also get in advance the analysis of expected utilization based on past and future workloads. No complex calculations to figure out the best billing plan and how much you can save. Magalix automatically analyzes your cluster workloads to match it against best capacity and performance.
Register now and get 14-days of free Node Advisor reports.
It is a tedious, time-consuming task to figure out the best type, size, and combination of Kubernetes nodes. You have to look at many performance and infrastructure utilization metrics to estimate the amount of capacity you need. It also requires you to have a deep understanding of different types of provider virtual machines (VMs), their performance, and cost implications. For example, AWS has more than 100 instances types and sizes and the list gets longer every few weeks. It becomes tricky to calculate utilization and resulting buffer inside your capacity with the new nodes and capacity.
Magalix provides a detailed analysis of cluster capacity and shows cost and utilization improvements you could achieve.
Magalix Node Advisor analyzes your current capacity and dozens of metrics to characterize workloads. It figures out memory and CPU patterns to decide on capacity changes in your cluster. Magalix Node Advisor may recommend the need for more cores in fewer VMs but less overall memory for example. The proposed changes are set to achieve three main goals:
Magalix Node Advisor gives you detailed recommendations of the best node combinations you should provision to save money.
To realize the best utilization and lowest cost, you get a detailed plan to create node pools, VM types and VM sizes. Even if you just have the right capacity, the Node Advisor can still save you a lot of money. It tweaks VM types and shuffles capacity within your cluster. For example, in the case of memory intensive containers, you will save 40% if you change your nodes from general purpose instances (M5) to memory intensive instances (R4).
You can now compare different billing plans of your current provider to other cloud providers and select the best that works for your needs. You can now see the updated pricing of your proposed capacity. For example, if you run Kubernetes on AWS, the Node Advisor compares the pricing of the on-demand, reserved instances, and spot instances.
“A $1.5 hourly savings in a 20 nodes cluster saves our users up to $14,000 yearly.” Mohamed Ahmed, CEO — Magalix
Tweak your preferences and update the Node Advisor optimization policies. You can set restrictions on the number of nodes and the buffer size you want inside your cluster. For example, if you set the buffer size to be 30%, Magalix Node Advisor will recommend infrastructure with 70% utilization. You can tweak the policy to optimize for maximum reliability, i.e distribute containers on as many nodes as possible. Or tweak it to increase the density of containers and reduce the number of nodes.
“I was skeptical about Magalix Node Advisor at the beginning. But I was surprised by its accuracy and potential savings that we can achieve.” Amr Farid, DevOps engineer
Magalix Node advisor supports AWS, Azure, GCP, and IBM. It is always up to date with the latest VM families — see detailed supported lists here. You will always be on top of your anticipated cost since Node Advisor continuously updates the pricing.
Register and connect your existing Kubernetes cluster with a single kubectl command. Within a few minutes, cluster metrics will start flowing and our AI engine will engage shortly analyzing your containers and infrastructure. You will get 14-days free full access trial.
Yes. It is a quite complex process to upgrade the capacity of your cluster. We working on some basic scenarios that will help you reach maximum efficiency with full automation. Stay tuned!
We update the pricing and families types as the cloud providers make changes to their billing or pricing model.
Not at this point. We use the existing standard pricing to estimate the cost of the current recommended capacity. This does not require any special access to your cloud provider accounts.
Yes. It assumes that you will keep different your cluster at the same time zone.
Not yet. Please share your thoughts here.
Not at the moment. It will pick a region and optimize cluster capacity and pricing.
Yes. It supports Google Kuberentes Engine (GKE), AWS Elastic Kubernetes Service (EKS), and Azure Kuberenetes Service (AKS)
Magalix Co-Founder, dad, and learner @MohamedFAhmed
15 
15 
15 
Magalix Co-Founder, dad, and learner @MohamedFAhmed
"
https://medium.com/@ibmcloud/orenda-is-the-public-relations-brain-that-never-sleeps-bb6713010ca5?source=search_post---------291,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Cloud Stories
Nov 30, 2016·5 min read
By: Tanya Seajay | Founder and Chief Executive Officer, Orenda Software Solutions
When there are millions of people with interest in and influence over your brand on social media, how do you measure changing perceptions?
That is exactly the question we’ve answered with Orenda Software Solutions.
We’ve created a different way of looking at social media and giving corporations timely insight in an automated way.
“Orenda” literally means the power within all of us to have the ability to overcome. Applying this in context, Orenda gives corporations not only the power to manage their reputations but also the ability to sort through or “overcome” 100,000 Twitter mentions in one day.
I spent more than a decade of my career as a journalist before I moved to public relations (PR). My primary focus as a PR professional is brand and reputation management. Everyone wants to know, “Does this story have legs?” In other words, will people be interested in it? Legs can be a good or bad thing, depending on whether it’s news and you want people to know and talk about it, or if it’s a scandal, in which case you want it to die a quick and painless death.
It’s one thing to track legs in a newspaper environment (remember collecting press clippings?) but another altogether with the proliferation of social media. Thus I became increasingly interested in social media. I was on it and watching it, and I thought to myself: how do you keep track of the millions of people who are talking about a corporation and how that affects perception, brand and reputation? And that was when we began developing the Orenda solution employing cloud and cognitive technologies.
My goal was to break down what people say on social media — give the words positive and negative values — and apply social science principles so that I can dissect the conversation in an instant to determine whether trust and satisfaction were eroding or increasing based on the conversations.
Today, the Orenda solution goes beyond social listening. The software quantifies social data by measuring, categorizing and interpreting it according to established methodologies used in the PR discipline.
By presenting the user with refined comprehensive data that focuses on insight rather than information, Orenda overcomes the problems that excessive amounts of data pose. Our goal is to make decision making as quick and easy as possible.
For example, the Overall Corporate Standing live dashboard shows fundamental factors of a healthy relationship, such as trust, influence, satisfaction, commitment and so on.
Real time with social media is important because there are corporations whose reputation has been destroyed overnight. At the same time, there are companies that are unaware of the constant erosion of their brand because it was happening so slowly.
Orenda looks at both cases. We can also take tiny nuances and see, show and demonstrate where the brand is and what the brand momentum is — either it is going up or down. You can look at it over 30, 60 or 90 days and get a clear picture of whether you are having a bad day or a bad run.
Orenda runs on the IBM Cloud platform. We were with another cloud provider but moved to IBM Bluemix to be able to scale up to any degree. We feel extremely confident that a company’s data set is secure, because IBM takes good care of the precautions and the restrictions that allow people to feel confident that their information is going to be safe.
Orenda is constantly running, so we need a reliable method of collecting data when the workday ends. Since the switch to IBM, we haven’t encountered any problems with data storage or access.
We integrated IBM Watson Personality Insights into our dashboard via the IBM Bluemix development platform. Orenda’s core analysis centers on what people are saying online. With the Watson solution, we can evaluate who is talking about a brand or product. This makes reviewing data more accurate and efficient.
Additionally, trolling — using inflammatory statements to start a reaction — can be difficult for a human to detect, so we’ve been using Personality Insights to indicate whether the sentiment is accurately judged depending on the commenter’s personality traits. We can then determine whether the message is ironic, sarcastic or deceptive.
How could you possibly sort through 100,000 Twitter mentions a day; how many people would you have to devote to them? You no longer need a dedicated team if you have Orenda monitor Twitter and then simply have a PR professional review the key findings using human intuition to draw conclusions. Orenda removes the pressure of going through things manually to try to figure out what’s going on. It also eliminates the need to organize and report the findings, because all the information is available for any team member to see on the dashboard.
We are looking into integrating additional Watson application programming interfaces (APIs), including Tone Analyzer, Natural Language Classifier and Language Translator into the Orenda software. Each of these APIs is almost like a Lego: you can take a Lego and plug it into the Orenda software system. If it works, it works well. We’re able to gain a customer. If it doesn’t and nobody needs it, we just take it out and put in another Lego block. IBM offers that flexibility, and we can grow where the market wants us to grow without a huge amount of time and investment trying to figure out whether it’s going to fit the marketplace.
Orenda is like the focus group that never ends. We believe that if you know better, you do better.
ecc.ibm.com
www.ibm.com
www.ibm.com
To read 2017 IBM Cloud Stories visit http://ibm.co/2iKoD4d .
See all (173)
5 
5 claps
5 
To read 2017 IBM Cloud Stories visit http://ibm.co/2iKoD4d .
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-osaka-region-spring-on-gcp-and-a-bitcoin-public-dataset-b90ea5203b6a?source=search_post---------292,"There are currently no responses for this story.
Be the first to respond.
GCP will soon reach 19 regions as it is “building its second Japanese region in Osaka” (Google blog)
Spring Framework 5 and Spring Boot 2 users rejoice, you can now use your favorite technologies and access the full power of GCP (Google blog). See also Pivotal’s take on this — “Spring Cloud for Google Cloud Platform 1.0 Milestone 2 Available” (spring.io)
Qwiklabs has released two new “baseline” quests, one for Kubernetes Engine, and one for Data, ML, and AI. Start here (qwiklabs.com)
From the “GCP users talk best about the tech” department :
From the “a week without BigQuery is a wasted week” department :
From the “putting TensorFlow versatility to the test” department :
From the “it didn’t really fit into any other category” department :
From the “In Case You Missed It (ICYMI)” department :
If you’re on twitter, here are some updates to Google Cloud’s presence there :
The GCP Podcast episode gets into how Google works with the community on TensorFlow — Episode #113 Open Source TensorFlow with Yifei Feng.
This week’s screenshot is taken from the Spring on GCP blog post
That’s it for this week!-Alexis
Google Cloud community articles and blogs
7 
7 claps
7 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@rpradeepmenon/a-framework-to-prioritize-analytics-use-cases-for-cloud-migration-9659272cafdb?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pradeep Menon
Aug 30, 2021·8 min read
Cloud computing is transforming industries, and it is growing at an unprecedented pace for a good reason. Gartner predicts worldwide public cloud end-user spending will grow 18% in 2021, topping more than $300 billion. There is no doubt that Cloud fuels innovation.
Analytics workloads are especially poised to leverage cloud computing. Cloud offers unlimited scale, on-demand elasticity, and services that are at the edge of innovation. As a result, more and more organizations are pivoting towards the cloud for analytics. A Forbes report suggests that more than 47% of organizations have analytics running on the cloud in 2021 — up from 39% in 2019. This number is likely to go northwards, and it is a good story for the cloud. However, a story is no good without its challenges. In my experience consulting numerous customers across the region, I have found one perennial challenge that these organizations face:
- Is there a framework to identify analytics use cases for cloud migration?
This question is vital because an analytics function comes with its legacy. Organizations have invested millions of dollars in getting it up and running. Numerous data warehouses, data marts, data lakes, operational data stores, and many other legacy systems lurk around the organizations that fulfill a plethora of analytics use cases. Some of these systems are legacy and redundant. However, some of them are lifelines of businesses. Businesses rely on it for reporting, feedback, performance management, and running their day-to-day operations. Some of these use cases have evolved. However, they are throttled by the inefficiencies of the systems. These use cases need a new lease of life. There needs to be a framework that helps to prioritize these use cases. This blog will highlight a framework that one could potentially use to prioritize the analytics use case for cloud migration. If used correctly, they distill clarity with simplicity. We will examine three two-axis models that can prioritize the workloads to move to the cloud.
The framework consists of three models that focus on three key drivers. These drivers are:
Let us investigate each of these models in detail
The first model is the cost model. The cost model considers the dimensions that primarily influence the cost of running the use case in the cloud: compute and storage.
The second model is the agility model. The agility model considers the dimensions that primarily influence the value the cloud creates for the business use case. It focuses on the agility that the cloud delivers for the use case. The dimensions are business agility and architectural agility.
The third model is the availability model. The availability model considers the dimensions that influence how available the use case should be for the business. The dimensions it considers are the expected availability of the use case and the number of architectural components that satisfy the use case. Note that the expectation is for the use case to be available. Not the technology. Let us define the dimensions in more detail.
Now that the three models that can be used qualitatively bucket the use cases for cloud migration are discussed let us bring them together. In practice, these models shouldn’t be used in isolation but instead fused to give a holistic view and determine whether a use case should be moved to the cloud or not. An example of linking these models is as follows:
In the above example, there are seven candidate use cases, and the task is to prioritize them for cloud migration. The guideline for applying these models are as follows:
In conclusion, applying the models discussed in this article provides a quick but systematic framework for evaluating the cloud migration propensity of business use cases. Furthermore, one can extend the two-axis framework with more dimensions based on the context of the specific use case.
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
6 
6 
6 
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
"
https://medium.com/@edbyrne/the-next-cloud-size-disruption-food-and-agriculture-857f67326e3f?source=search_post---------294,
https://medium.com/@dwdraju/using-public-and-private-docker-images-from-gcp-artifact-registry-e093d1f35f1c?source=search_post---------295,NA
https://medium.com/@dcvc/dcvc-portfolio-ipo-elastic-goes-public-opening-at-a-4-4-billion-valuation-ffa9607e14be?source=search_post---------296,NA
https://medium.com/@cloud66/container-stacks-v2-is-now-public-and-backed-by-kubernetes-febee2c765e8?source=search_post---------297,NA
https://netflixtechblog.com/public-continuous-integration-builds-for-our-oss-projects-282174d308ce?source=search_post---------298,NA
https://medium.com/@salmaan-rashid/message-payload-encryption-in-google-cloud-pub-sub-part-2-service-account-public-keys-5803d092cead?source=search_post---------299,NA
https://medium.com/@jonbma/public-saas-index-december-1st-2019-c2a853fb4538?source=search_post---------300,NA
https://blog.codenvy.com/devoxx-2017-972079a1e4be?source=search_post---------301,NA
https://medium.com/google-cloud/twigcp197-f43f4051b163?source=search_post---------302,NA
https://medium.com/@wilderness/end-natural-gas-waste-on-public-lands-14b1e03c37cc?source=search_post---------303,NA
https://medium.datadriveninvestor.com/natural-language-intelligence-building-a-language-bridge-for-business-997ff8bba977?source=search_post---------304,NA
https://medium.com/javarevisited/production-horrors-handling-disasters-public-debrief-9fc8b57b127a?source=search_post---------305,NA
https://medium.com/@openaq/openaq-at-the-2016-amazon-web-services-public-sector-summit-6c5083b01d27?source=search_post---------306,NA
https://medium.com/talktips-a-tinychallenges-installment/talktips-day-20-the-cloud-is-your-friend-9d93e9e55c5e?source=search_post---------307,NA
https://medium.com/@etherealmind/blessay-successful-private-clouds-aren-t-spoken-of-in-public-b9ad0cc8aba2?source=search_post---------308,NA
https://medium.com/@RadwareBlog/ensuring-data-privacy-in-public-clouds-51136c5c1a7?source=search_post---------309,NA
https://medium.com/@psostre/private-clouds-are-cheaper-than-public-clouds-no-im-not-kidding-c98abf9094c4?source=search_post---------310,NA
https://medium.com/@alibaba-cloud/setting-up-a-vpn-connection-between-alibaba-cloud-and-gcp-using-vpn-gateway-592fbea4bd1d?source=search_post---------311,NA
https://medium.com/@alibaba-cloud/a-big-data-based-public-opinion-analysis-system-architecture-anatomy-26a84178b98d?source=search_post---------312,NA
https://medium.com/work-bench/wisdom-from-walking-among-the-cloud-giants-5-questions-answered-by-digitalocean-ceo-yancey-spruill-cb7786849c52?source=search_post---------313,NA
https://digitizingpolaris.com/pivotal-software-files-to-go-public-ready-or-not-665026cc89ac?source=search_post---------314,NA
https://medium.com/syncedreview/china-to-facilitate-ai-unicorns-going-public-80964c8016?source=search_post---------315,NA
https://medium.com/@ibmcloud/modernizing-new-zealand-s-public-warning-system-bb7646e32f58?source=search_post---------316,NA
https://medium.com/@cloud66/managed-kubernetes-on-more-public-clouds-good-news-but-also-distracting-15dd5deca2af?source=search_post---------317,NA
https://medium.com/@auth0/fantastic-public-s3-buckets-and-how-to-find-them-f854e331e421?source=search_post---------318,NA
https://medium.com/@mcraddock/drupal-standard-for-uk-public-sector-57d4dfaa3a29?source=search_post---------319,NA
https://medium.com/@JBradley_DC/data-and-technology-wont-improve-public-participation-and-increase-inclusion-and-promote-equity-345c49e4e9d5?source=search_post---------320,NA
https://medium.com/@sramana/on24-goes-public-with-42-billion-tam-559ba90dfbb4?source=search_post---------321,NA
https://medium.com/@mitzi-flyte/the-silver-linings-to-the-corvid-19-cloud-589f326957f3?source=search_post---------322,NA
https://medium.com/@TheDigitalTP/the-four-misperceptions-of-the-cloud-43943dda2631?source=search_post---------323,NA
https://medium.com/@cloud66/feature-highlights-activeprotect-df679908ecde?source=search_post---------324,NA
https://medium.com/@GregHemmings/thoughts-on-public-enemy-throwing-away-cds-and-the-carbon-costs-of-cloud-computing-e10df7094cf6?source=search_post---------326,NA
https://medium.com/@alibaba-cloud/public-event-sentiment-analysis-in-postgresql-dcb8ec388b25?source=search_post---------327,NA
https://medium.com/g-cloud-latest/reminder-what-is-g-cloud-bd4ab07ff945?source=search_post---------328,NA
https://medium.com/@openbom/looking-for-beta-participants-to-try-the-new-openbom-public-rest-api-5762c7e46f0d?source=search_post---------329,NA
https://medium.com/knoldus/create-a-public-google-doc-via-google-drive-api-in-scala-c51f20a7388b?source=search_post---------330,NA
https://blog.wantoo.io/release-wantoo-goes-public-with-beta-d858037310d?source=search_post---------331,NA
https://medium.com/@alibaba-cloud/why-to-have-and-how-to-support-multi-cloud-environments-b8c3e35b8dbd?source=search_post---------332,NA
https://medium.com/@alibaba-cloud/real-time-database-backup-to-oss-on-the-oss-console-48aaf8d2701b?source=search_post---------333,NA
https://medium.com/@alibaba-cloud/using-container-service-to-build-wechat-applets-cda9bb8e4e4c?source=search_post---------334,NA
https://medium.com/digital-leaders-uk/infrastructure-as-a-service-considerations-for-the-public-sector-eeefbb994126?source=search_post---------335,NA
https://medium.com/structure-series/are-public-clouds-really-safer-than-private-data-centers-f209d1205086?source=search_post---------336,NA
https://blog.mturk.com/aws-worldwide-public-sector-summit-2013-achieving-success-in-the-cloud-b7449114616f?source=search_post---------337,NA
https://medium.com/digital-leaders-uk/webinar-recording-modern-cloud-native-development-using-live-streaming-data-5ea06161c7f8?source=search_post---------338,NA
https://medium.com/@TheDigitalTP/stop-your-people-being-the-biggest-barriers-to-cloud-adoption-b933f2248fdb?source=search_post---------339,NA
https://medium.com/@alibaba-cloud/damo-academy-10-key-trends-shaping-tech-in-2019-54692dabd265?source=search_post---------340,NA
https://medium.com/@digileaders/why-the-public-sector-should-focus-on-the-tech-that-will-make-lives-better-a216397d8107?source=search_post---------342,NA
https://medium.com/nft-cloud/meet-the-trustee-wallet-7d904dc78e07?source=search_post---------344,NA
https://medium.com/knoldus/bookmyhours-com-public-beta-released-a8675f656209?source=search_post---------345,NA
https://medium.com/slidemagic/sliderocket-public-launch-good-enough-to-take-powerpoint-into-the-cloud-1f83e9933fc3?source=search_post---------346,NA
https://medium.com/@nhancv/an-aws-cloud-architecture-for-web-hosting-3-tiers-f3b30f3e2f47?source=search_post---------347,NA
https://medium.com/waospi/washington-state-and-amazon-web-services-to-train-2-500-k-12-students-in-cloud-computing-skills-by-8c9712c307a7?source=search_post---------349,NA
https://medium.com/globallogic-cloud-and-devops-blogs/clouds-compared-aws-vs-azure-vs-gcp-c59519b9d5e4?source=search_post---------350,NA
https://medium.com/@ashleymayer/its-okay-that-your-startup-doesn-t-have-a-communications-strategy-e92f3016c4d9?source=search_post---------351,NA
https://medium.com/gitpod/gitpod-gitpod-online-ide-for-github-6296b907a886?source=search_post---------352,NA
https://medium.com/google-cloud/analyzing-go-code-with-bigquery-485c70c3b451?source=search_post---------353,NA
https://medium.com/google-cloud/how-to-create-a-custom-private-google-home-action-260e2c512fc?source=search_post---------354,NA
https://medium.com/@justkrup/deploy-a-docker-container-free-on-heroku-5c803d2fdeb1?source=search_post---------355,NA
https://medium.com/google-cloud/learning-to-analyze-huge-bigquery-datasets-using-python-on-kaggle-2c6c6153f542?source=search_post---------356,NA
https://read.acloud.guru/how-to-secure-an-s3-bucket-7e2dbd34e81b?source=search_post---------357,NA
https://medium.com/google-cloud/analyzing-covid-19-with-bigquery-13701a3a785?source=search_post---------358,NA
https://marker.medium.com/why-amazon-wants-to-invest-in-the-worst-ipo-of-2020-6db9725891b2?source=search_post---------359,NA
https://medium.com/@srobtweets/building-a-ml-keynote-demo-for-100-000-people-2ae61235d479?source=search_post---------360,NA
https://blog.parsecgaming.com/game-streaming-tech-in-the-browser-with-parsec-5b70d0f359bc?source=search_post---------361,NA
https://medium.com/google-cloud/how-to-query-balances-for-all-ethereum-addresses-in-bigquery-fb594e4034a7?source=search_post---------362,NA
https://medium.com/google-earth/share-your-analyses-using-earth-engine-apps-1ac29939903f?source=search_post---------363,NA
https://medium.com/@tjajal/on-trusting-autonomous-vehicles-the-tech-behind-the-hype-c06b8aae8822?source=search_post---------364,NA
https://medium.com/point-nine-news/dropbox-the-ultimate-mouse-hunter-7312632a33db?source=search_post---------365,NA
https://medium.com/qlc-chain/qlc-chain-and-montnets-announce-joint-venture-to-provide-decentralized-telecom-services-for-f0b48f397366?source=search_post---------366,NA
https://medium.com/google-cloud/github-and-the-power-of-open-source-data-b43b34c1beb6?source=search_post---------367,NA
https://medium.com/google-earth/advancing-earth-engines-sustainability-mission-29a24b15d973?source=search_post---------368,NA
https://medium.com/google-cloud/live-ethereum-and-bitcoin-data-in-google-bigquery-and-pub-sub-765b71cd57b5?source=search_post---------369,NA
https://medium.com/s-c-a-l-e/databases-at-box-when-scale-collaboration-and-enterprise-users-collide-42b93e3e9b51?source=search_post---------370,NA
https://medium.com/@timtech4u/deploying-a-gcp-virtual-machine-instance-with-a-startup-script-fe5431f16e66?source=search_post---------371,NA
https://weeknot.es/weeknotes-s10-ep9-b1b2ee841a93?source=search_post---------372,NA
https://levelup.gitconnected.com/creating-a-custom-vpc-in-aws-b4ea7bf4a71?source=search_post---------373,NA
https://medium.com/google-cloud/how-to-query-ether-supply-in-bigquery-90f8ae795a8?source=search_post---------374,NA
https://medium.com/@qlcchain/qlc-chain-bi-weekly-report-january-16-31-2021-53-fd80d674f7b2?source=search_post---------375,NA
https://open.nytimes.com/agrarian-scale-kubernetes-part-3-ee459887ed7e?source=search_post---------376,NA
https://medium.com/google-cloud/the-rise-and-fall-of-new-york-city-311-complaints-72bd894e0c74?source=search_post---------377,NA
https://medium.com/poets-unlimited/clouded-sky-934d847b5bdb?source=search_post---------378,NA
https://medium.com/google-cloud/open-data-requires-democratizing-analysis-of-that-data-b3910f1f5e77?source=search_post---------379,NA
https://aws.plainenglish.io/infra-as-code-create-aws-vpc-and-subnets-using-terraform-and-best-practices-eaba8c3e1aef?source=search_post---------381,NA
https://medium.com/foundations/my-top-ten-announcements-at-aws-re-invent-97ca32ee841b?source=search_post---------382,NA
https://medium.com/memory-leak/carbon-black-s-1-analysis-not-the-endpoint-just-the-beginning-cae9d6812523?source=search_post---------383,NA
https://medium.com/crustnetwork/crust-network-profit-ark-incentivized-testnet-is-now-starting-888709933672?source=search_post---------384,NA
https://pub.towardsai.net/how-to-do-remote-development-with-vs-code-using-aws-ssm-415881d249f3?source=search_post---------385,NA
https://medium.com/top-network/top-network-and-ultrain-formed-strategic-partnership-to-empower-blockchain-technology-and-eb7e085ab88?source=search_post---------386,NA
https://koukia.ca/azure-logic-app-to-publish-covid19-developments-on-twitter-be8616432e5a?source=search_post---------387,NA
https://read.iopipe.com/the-aws-serverless-application-repository-launches-with-an-iopipe-app-dc34a12ebb00?source=search_post---------388,NA
https://medium.com/ankr-network/ankr-your-pchain-validator-node-today-63b49eddae9c?source=search_post---------389,NA
https://medium.com/@jaychapel/quick-guide-understanding-aws-ip-address-types-839a9eb747f2?source=search_post---------390,NA
https://medium.com/@storjproject/upcoming-milestone-releases-on-the-storj-v3-roadmap-8cb514f9462c?source=search_post---------391,NA
https://medium.com/@chrisnordlinger/10-leadership-lessons-for-2017-from-a-top-10-cio-ad44d351de0b?source=search_post---------392,NA
https://medium.com/ankr-network/ankr-bi-weekly-update-2019-2-a086befd432d?source=search_post---------393,NA
https://medium.com/@krishnan/evaluating-functions-as-a-service-74aa434db51d?source=search_post---------394,NA
https://medium.com/@marysam/5-dos-and-don-t-s-of-your-media-and-bloggers-outreach-list-88866fe14d2a?source=search_post---------395,NA
https://medium.com/redpixie-news/api-economy-28e04467b1b6?source=search_post---------396,NA
https://medium.com/memory-leak/searching-for-success-elastic-found-it-ba27c28c2cb3?source=search_post---------397,NA
https://medium.com/crv-insights/thesis-infrastructure-as-code-scale-or-die-83273243d3ef?source=search_post---------398,NA
https://medium.com/@storjproject/sharing-storage-space-for-fun-and-profit-e5eacfc805b0?source=search_post---------399,NA
