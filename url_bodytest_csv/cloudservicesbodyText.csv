story_url,bodyText
https://blog.brand.ai/how-to-share-a-design-system-in-sketch-1-3-245308f2d7f1?source=search_post---------0,"InVisionApp, Inc.
InVisionApp, Inc.
Freehand is the visual collaboration tool disguised as an online whiteboard.
InVisionApp, Inc.
By using InVision brand assets you agree to follow our guidelines packaged with the media kit. For further information about use of the InVision brand assets, please contact support@invisionapp.com.

"
https://medium.com/rigetti/introducing-rigetti-quantum-cloud-services-c6005729768c?source=search_post---------1,"There are currently no responses for this story.
Be the first to respond.
By Chad Rigetti
Quantum computing is approaching a pivotal milestone called quantum advantage. This is the inflection point where quantum computers first begin to solve practical problems faster, better, or cheaper than otherwise possible. The first demonstration of quantum advantage will be an extraordinary achievement, but it will only be the beginning. Ultimately, quantum advantage will be reached over and over again in new markets and new domains, changing the ways in which problems are solved across industries.
Three key capabilities are essential to achieving quantum advantage. First, users need more qubits with lower error rates. Second, users need computing systems designed to run the hybrid quantum-classical algorithms that offer the shortest path to quantum advantage. Finally, these capabilities must be delivered alongside a real programming environment so users can build and run true quantum software applications.
In August, we announced that we are building 128-qubit quantum computers with the low error rates needed to achieve advantage. These systems are based on our scalable 16, 32, and 128-qubit Aspen quantum processors. And today, to deliver the final key capabilities, we are excited to introduce Quantum Cloud Services.
Quantum Cloud Services is the only quantum-first cloud computing platform. With QCS, for the first time, quantum processors are tightly integrated with classical computing infrastructure to deliver the application-level performance needed to achieve quantum advantage.
Users access these integrated systems through their dedicated Quantum Machine Image. The QMI is a virtualized programming and execution environment designed for users to develop and run quantum software applications. Every QMI comes pre-configured with Forest 2.0, the latest version of our industry-leading software development kit.
We will be granting early access to Quantum Cloud Services in the coming weeks. You can sign up to reserve a QMI today at rigetti.com.
Partnerships with leading startups We’re partnering with visionary startups who are building the first generation of practical quantum applications. These companies include 1QBit, Entropica Labs, Heisenberg Quantum Simulation, Horizon Quantum Computing, OTI Lumionics, ProteinQure, QC Ware, Qulab, QxBranch, Riverlane Research, Strangeworks, and Zapata Computing. They will use QCS to develop groundbreaking applications and as a channel to distribute these applications to the broader community, putting even more tools into the hands of developers and researchers.
The Quantum Advantage Prize Quantum Cloud Services has been designed from the bottom-up to accelerate the pursuit of quantum advantage. We don’t know when the first demonstration of quantum advantage will be achieved, or what shape it will take, but one thing is certain: it will dramatically accelerate progress in unlocking the power of quantum computing for everyone. Recognizing the significance of this achievement, Rigetti Computing is offering a $1 million prize for the first conclusive demonstration of quantum advantage on QCS. More details of the prize will be announced on October 30th, 2018. Stay tuned!
Rigetti Computing
532 
532 claps
532 
Rigetti Computing
Written by
On a mission to build the world’s most powerful computer.
Rigetti Computing
"
https://towardsdatascience.com/building-your-own-deep-learning-computer-and-saving-money-on-cloud-services-c9797261077d?source=search_post---------2,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Chris Fotache
Oct 11, 2019·8 min read
After struggling with Microsoft Azure’s GPU VM’s for a few years, and hearing that Amazon’s AWS is not much better, I decided it’s time to have my own local deep learning machine.
One of my main reasons was that the cloud VM doesn’t have a display, therefore you can’t do anything visually. No big deal if you just train there and then run the model on a local computer but if you need to work on simulation-based robotics projects, those won’t run at all in a virtual environment.
I later found that not only building an almost state-of-the-art machine pays for itself in about 4 months, but it is significantly faster than a cloud server (mainly because of local data transfer speed, since everything is in the same box on the same bus, while the cloud service might have the compute units and storage in different racks — so even if the GPU is faster, it can’t get the data fast enough to benefit from that speed).
My system ended costing just under $3K (compare that to about $800/month you’d pay for an entry-level cloud GPU from AWS or Azure). That was in May 2019, and prices tend to vary a lot so it can be 10% lower and higher at any time. Also, by the time you read this technology might’ve already evolved.
You might ask why go to the pain of building the computer yourself instead of buying a high-end super-computer. It’s because ready-built deep learning systems are insanely expensive. But if you are still afraid to tinker with expensive components and are interested in a pre-built system, I found that Exxact sells some of the most affordable deep learning systems starting at $5,899 (2x NVIDIA RTX 2080 Ti + Intel Core i9) which also includes a 3 year warranty and deep learning stack. They are also recommended by another AI engineer, Jeff Chen.
To make sure everything works together, I recommend using PC Part Picker. It will both show you the lowest price for each component, and ensure that you don’t pick incompatible parts. As for putting things together, YouTube rules. Just type the name of the component and you’ll find several very explanatory videos on how to install it. So now lets go over the components that are required:
CPU
Here you’ll have to make a big choice: AMD or Intel. For my entire life I’ve been an Intel fan, but for this machine the CPU is not the most important part. That is the GPU. And Intel CPU’s can cost twice as much as the AMD counterpart. AMD’s new Ryzen line has very good reviews, and I don’t need to overclock it since I’m not playing video games with it. Therefore I went for the AMD Threadripper 1920x, which has 12 cores and 24 threads, more than enough for my case. It was reasonable priced around $350 but prices were dropping. The alternative would be the 10-core Intel i9–7900 at over $900.
CPU Cooler
AMD CPUs have always ran very hot (one of the main reasons they weren’t that reliable). They still are, so you definitely need a liquid cooler. I went with the Fractal S24 which has 2 fans, at about $115. An alternative is the Corsair H100i.
Motherboard
The main choice about the motherboard is the chipset. The simple rule is: For AMD Threadripper, use X399. For Intel 7900, use X299.
Based on reviews, I went with the MSI X399 Gaming Pro Carbon AC, which has everything I needed for deep learning. You’ll find it at just over $300. Other good alternatives are the Asus ROG, Gigabyte Aorus and Asrock Taichi (just make sure it has at least 40 PCIe lanes). You have to make sure the board design accomodates the size of the GPU, and maybe adding multiple GPU’s. The MSI one has plenty of room, and everything is well placed.
GPU
Now this is the most important component of your deep learning system. You have to go with an Nvidia GPU, and the minimum recommended is the GTX 1080 Ti. Unfortunately, when I was looking, that was impossible to find at its regular price of about $800 (blame gamers? crypto miners?). So I had to go to the next level, the RTX 2080 Ti, which is not easy to find either, but I was lucky to get on an excellent $1,187 deal from EVGA. RTX is the newer generation, with one of the best performance among early 2019 consumer GPU’s. I’m glad I was “forced” to make that choice. If you look around, you might still find deals around $1,200. I think EVGA and Gigabyte are the top manufacturers, and the choices you make are about the cooling system. The EVGA RTX 2080 Ti XC Ultra has dual air coolers and that proved enough so far, it never got to critical overheating.
Memory
For the configuration above, DDR4 is the best choice. Corsair is probably the main manufacturer. And it’s 2019, you need 64Gb. So I ended up with 4x16Gb Corsair Vengeance LPX DDR4. I paid $399 but prices are dropping dramatically, they’re well under $300 by now.
Hard-Drive
SSD is old tech by now. The state-of-the-art is the M.2 standard, and that drive plugs right into the motherboard into a PCIe slot. Going at the main bus speed, this is basically a high-capacity, persistent memory chip. I really liked the 1Tb Samsung EVO SSD M.2. I paid $241 but prices for this also went down towards $200. If you need more storage, you can add a regular SSD drive which should be less than $100.
Power Supply
PCPartPicker will make sure you pick a power supply big enough for your system. There are also other online wattage calculators. With one GPU you probably won’t get close to 1,000W but if you plan to add a second GPU, then you need 1,200W to be safe. EVGA is a solid manufacturer and I picked the EVGA SuperNOVA P2 Platinum 1200 which is around $250.
Case
There are plenty of options here, and it might come down to personal preferences and design, but it’s important to make sure it’s big enough to fit all the components in without being cramped, and to have good air circulation. I went with the Lian-Li PC-O11AIR at $114 because it fit those requirements. It’s very roomy, everything is well placed inside, and there’s good cooling.
Additional Cooling
After you’re done with your build, you might want to add additional fans to improve the air flow. My case came with several fans, but I got additional ones, to fill almost every mounting location. It can never get too cold in a GPU machine that’s gonna crank up convolutional networks. I got an 80mm Noctua for the back, and also a regular 120mm Corsair that I added on top. And yes, I got theRGB one. I didn’t care much about bright shiny colors in my case (since it’s under the desk anyway), but in the end I gave in and bought a cool fan.
Assembly
Like I said, search for each component on YouTube and you’re sure to find detailed walkthroughs about installation. As an example, here are a few that I followed: a build similar to mine, a walkthrough of the MSI X399 motherboard and its components, and a focus on the Threadripper mounting. And read all the installation instructions in the manuals. For example, be careful about the slot locations of the memory units.
Basically, the order of operations is this:
First, prep the case, install the power supply and pull the power cables. Then prep the motherboard, install the CPU and then the M.2 drive. Mount the motherboard in the case and add the CPU cooler. Then add the other fans and connect the power and button / lights wires. Finally install the memory modules and the GPU.
After you’re done and you power up the system, finish with cable management and optimize cooling. For example, I ended up removing most dust filters that were covering the fans. I made an intense GPU-heavy test protocol (training a Yolo model) and kept moving fans around until I got the lowest temperatures.
Software Installation
That’s where the fun really begins, but it’s not the focus of this story. Spring of 2019 — you’ll probably go with Ubuntu 18.04, the Nvidia drivers for your GPU version (do that quick, or the display will pretty much suck), CUDA 10, and then whatever frameworks you use (PyTorch, Tensorflow, etc). And enjoy higher speeds than any cloud GPU you’ve tried at a one time price that pays off in a few months.
Component List
Here’s my parts list, with the prices from April 2019. You can also see updated prices on my PCPartPicker list.
Here are a few other alternative builds, that I’ve used for inspiration and education: Jeff Chen’s, Colin Shaw’s and Wayde Gilliam’s.
Chris Fotache is an AI researcher with CYNET.ai based in New Jersey. He covers topics related to artificial intelligence in our life, Python programming, machine learning, computer vision, natural language processing, robotics and more.
AI researcher at CYNET.ai, writing about artificial intelligence, Python programming, machine learning, computer vision, robotics, natural language processing
670 
3
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
670 claps
670 
3
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/smart-home-cloud-services-with-google-part-2-3901ab39c39c?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
In the previous post of this series, we explored using Cloud IoT Core and Firebase to build a device cloud for smart home devices. We saw how Cloud IoT Core enables us to securely connect constrained devices to Google Cloud, while Firebase constructs a user framework around our device data. As a quick review, here is the cloud service architecture we discussed last time.
Now, let’s look at extending this cloud service to integrate with the Google Assistant through smart home Actions. This enables users to link their account through the Google Home app and control their devices through any Assistant-enabled surface.
If you are unfamiliar with Actions on Google or smart home Actions for the Google Assistant. I recommend reading IoT & Google Assistant part 1 and part 2 by my colleague, Dan Myers, as a starting point.
As a developer, this brings the power of the Home Graph to your devices and gives them context within the user’s home. This context is what enables users to make natural requests, like “What is the temperature in the hallway?”, instead of referring to the device by name.
To build a smart home Action, create a new project in the Actions console. We will add two new features to our device cloud service and configure them in our console project: account linking and intent fulfillment. Let’s start by taking a look at how to integrate with the account linking process.
You can find the sample code described in this post on GitHub.
Users authorize the Google Assistant to access their devices through account linking. This process enables the user to sign in to the account they use with the device cloud and connect the device managed by that account to Google. The Actions on Google platform supports several different account linking flows, but only the OAuth 2.0 Authorization Code flow is supported for smart home Actions.
To configure OAuth account linking, you need to supply two endpoints in the Actions console: one for authorization and the other for token exchange. The authorization endpoint is a web UI where the user can authenticate and agree to link their account with the Google Assistant. It must return an authorization code that uniquely identifies the user.
Since we are using Firebase Authentication for the client apps, we can add a new Angular route to our web client for the user to sign-in and return their Firebase ID token as the authorization code once they authorize access:
Snippet from web/src/app/link.component.ts
Once the authorization flow is complete, the Google Assistant calls your token endpoint to exchange the authorization code for a persistent refresh token. This token does not expire and remains valid unless the user chooses to revoke device access or unlink their account.
Using the Firebase Admin SDK, we can validate and decode the ID token to obtain the UID of the user. If the token is valid, we can generate a refresh token and associate it with the user’s UID in Firestore. This enables us to look up the token again later for validating future requests.
Snippet from functions/smart-home/token.js
Firebase Authentication is an identity provider, but not a complete OAuth solution. This means our device cloud service must augment Firebase by minting and verifying tokens used for access to user data. The example code uses the JWT standard to create a self-encoded token with the following JSON payload:
We are using the JWT.io library for Node.js for all operations related to generating and validating tokens in this example.
The Google Assistant uses this refresh token to request an access token that will authenticate requests for device data. Our example service validates the refresh token signature and checks to make sure it’s the refresh token we expect for that user.
Snippet from functions/smart-home/token.js
OAuth access tokens should expire, which requires the Assistant service to periodically request a new one using the persistent refresh token. This enables the user to revoke their authorization if necessary. The example access tokens contain the same self-encoded payload as the refresh token, but they expire after one hour.
With the authorization and token endpoints in place, we are ready to begin implementing the fulfillment logic for the user’s devices. In this post, we will focus on implementing each intent in the context of our device cloud example, but you can find additional details on these intents and how they work together in the documentation.
We can use the Actions on Google Client Library for Node.js, which handles parsing the fulfillment requests and provides individual callbacks to handle each intent.
Snippet from functions/smart-home/fulfillment.js
At the beginning of each handler, we need to validate the access token provided with the request. Since the access tokens our application provides are formatted as a JWT that has the user’s UID encoded inside, we simply need to verify the JWT signature using our application’s secret to ensure the token came from us, and check that it has not expired. All of this is handled automatically by the verify() method of the JWT.io client library for Node.js.
Snippet from functions/smart-home/fulfillment.js
If the provided token is valid, the method will return the UID, which we will need in the intent handlers to query the proper device data. Let’s examine how our device cloud can interact with each intent: SYNC, QUERY, EXECUTE, and DISCONNECT.
The Google Assistant sends a SYNC intent after account linking succeeds to request the list of available devices. The response tells the Google Assistant which devices are owned by the given user and their capabilities (also known as traits) of each device. This includes an identifier to represent the user (agentUserId) and a unique id for each device.
For the device cloud sample project, this means returning the list of metadata for all devices where the user’s UID is set as the owner.
Snippet from functions/smart-home/fulfillment.js
The SYNC response only contains the device types and their capabilities; it does not report any device state. Below is an example device entry in the SYNC response payload for a light bulb and thermostat:
When users add or remove devices associated with their account, you should notify the Google Assistant through the Home Graph API via Request Sync. Without this feature in your service, users must unlink and relink their account to see changes or explicitly say “Hey Google, sync my devices”. Calling the request sync API triggers a new SYNC intent to allow your service to provide updated device information.
In our example, we can observe when a device node is added or removed in Firestore, and request a sync in each instance. The HomeGraph API will throw an error if that user has not linked their account, so we also need to verify that a persisted refresh token exists for the user in Firestore (created during account linking).
Snippet from functions/smart-home/request-sync.js
The QUERY intent asks for the current state of a specific set of devices (noted by their ids). A QUERY may be sent by the Google Assistant in response to a voice command (e.g. “What is the current temperature in the hallway?”) or to update the UI in the Google Home app.
Snippet from functions/smart-home/fulfillment.js
The device cloud sample project stores this data in the state field for each device using an internal representation of the device attributes. Our QUERY handler converts these attributes to match the device state values required by the Assistant for each trait. Our light bulb and thermostat devices declared support for the following traits:
Below is an example of the device entries in the QUERY response returning the state for each supported trait:
When the user issues a command (e.g. “Turn on the kitchen light”), your service receives an EXECUTE intent. This intent provides a distinct set of traits to be updated for a given set of device ids. This allows a single intent to update a group of traits or devices simultaneously.
Here, we update the contents of the device-configs document for each device, which triggers Cloud IoT Core to publish the configuration change. As we discussed in the previous post, the device will report its new state to Firestore in the devices collection after the change is processed successfully.
Snippet from functions/smart-home/fulfillment.js
The EXECUTE response must return a status code indicating whether each command was successful in changing the device state. If the cloud service can synchronously verify that the command reached the device and updated it, then it returns a SUCCESS. If the device is unreachable, then the service can report OFFLINE or ERROR.
Since our commands are written to Firestore through one document and the result is sent asynchronously back through another, we report PENDING rather than reporting SUCCESS. This indicates that we expect the command to succeed, and we will report the state change when it arrives.
Integrate the Report State API into your service to proactively report changes in device state to the Google Assistant. This is necessary to publish the latest device information to the Home Graph, which enables Google to look up device state without sending additional QUERY intents to your service.
In our example, we can define a new cloud function that triggers on updates to the devices collection. Recall from the previous post that this is where state updates from Cloud IoT Core are published. The function takes the updated device state and forwards it to the Home Graph API.
Snippet from functions/smart-home/report-state.js
The format of the states reported for each device is the same as a QUERY response.
Your service receives a DISCONNECT intent if the user decides to unlink their account from the Google Assistant. The service should invalidate the credentials used to provide access to this user’s devices.
For our example, this means clearing out the stored refresh token we generated during the account linking process. This negates any future attempts to gain a new access token until the user links their account again.
Snippet from functions/smart-home/fulfillment.js
Congratulations! Now your user’s devices are accessible through the Google Assistant. Check out the following resources to go deeper and learn more about building smart home Actions for the Google Assistant:
You can also follow @ActionsOnGoogle on Twitter and connect with other smart home developers in our Reddit community.
Engineering and technology articles for developers, written…
147 
2
147 claps
147 
2
Written by
Android+Embedded. Developer Advocate, IoT @ Google.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Android+Embedded. Developer Advocate, IoT @ Google.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/rigetti/quantum-cloud-services-opens-in-public-beta-31989e15e36e?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
By Betsy Masiello, VP Product
We’re thrilled to open Rigetti Quantum Cloud Services (QCS™) to public beta today. The QCS platform introduces an entirely new access model for quantum programming that is centered on an integrated cloud architecture. Our tightly coupled quantum and classical resources unlock performance gains that enable programs to run as much as 30x faster than on web API models.
Once registered, users have access to their own dedicated Quantum Machine Image, which comes preloaded with all the tools necessary to get started building quantum programs, including pyQuil and our quantum simulator. We’re also deploying two Aspen QPUs to the QCS platform, which users can book with an online reservation system available on the new QCS web dashboard. Beta users will receive $5,000 in credits to use toward running programs on the QPU during their first month.
Today we’re not only opening up access to QCS, we’re also distributing the first set of applications built by our Developer Partners:
More than 30 leading scientists from around the world have signed on as QCS Research Partners. Their work ranges from characterizing and benchmarking quantum hardware to computational research across biology, chemistry, and machine learning. We believe the quantum ecosystem will flourish by working in the open. To that end, our research partners are encouraged not only to publish their results but to share their data and code and open-source the tools and libraries they create on the QCS platform.
QCS is the only integrated, quantum-first cloud platform broadly available to users today. The application-layer performance achieved with QCS enables faster development cycles, accelerating our march toward quantum advantage.
Rigetti Computing
489 
489 claps
489 
Rigetti Computing
Written by
On a mission to build the world’s most powerful computer.
Rigetti Computing
"
https://medium.com/dev-bits/deploying-cloud-services-applications-properly-with-docker-docker-compose-a4d9973a1953?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
TL;PDR(Too lengthy, Please Do Read) I habituated to write lengthy articles! Please jump to the practical second section below if you go into sound sleep when you see lengthy texts
Hi, engineers. You all might be hearing things like docker and docker compose in the world of DevOps. I got a chance to look at the better ways to deploy our production web services. In this concise article, you are going to take the technique of running multiple production containers using Docker. Docker is a virtualization software that allows us to run multiple Operating Systems on a single machine. Docker creates a virtual layer of abstraction which runs independent containers on a single Linux machine. We are gifted with tools in this modern era and we should utilize them to deliver services seamlessly.
Note: I also wrote a programming book. If you are a software developer by chance, please do check it out.
www.amazon.com
In the traditional monolithic cloud architecture, we tie everything inside a bigger box and deliver to the customer over the cloud. Because of the explosion of internet availability, irrespective of size of customer base, companies are aligning their strategies to the cloud. In simple words, they are moving to (SAAS) subscription-based model instead of delivering the application directly. This benefits customer to stick with the service until the experience is good. For companies, this gives a lot of scope in adding features swiftly and that in turn boosts the innovation. Cloud model made companies be more productive and agile. Engineers design and develop a cloud service. When it comes to deploying that service operation engineers chose a cloud VPS(Virtual Private Server). They install everything(App server, Database, Proxy Server) on one single box(traditionally) and try to serve customers from there. This approach has a lot of drawbacks(some flexibility perks though) which is the reason for the emergence of Microservices.
In old approach, these pieces are installed on a VPS.
This approach is not preferred because of the advent of automation and Continuous Integration/Continuous Deployment. We can also capture a snapshot of a given environment to reduce the risks of overstepping into the wrong set of conditions on deploying services.
The concept of micro-services tells to separate the tightly coupled logic and deploy them separately. It means from the above block figure, the application server can be chunked more into independent pieces those talks to each other using HTTP or RPC. It doesn’t mean you need to choose x number of VPS instances to run your services. Containers provide a nice way to simulate isolation within the same machine or server. Docker provides that containerization. If you wrote a service and planning to deploy it on AWS EC2 or any cloud VPS, don’t deploy your stuff as a single big chunk. Instead, run that distributed code in the containers. We are going to see how to containerize our deployment using Docker and Docker Compose.
In this section we are going to do three things:
Source code for this project is available here.
github.com
For the first step, I am using NodeJS and Express to create a simple health check API. For the second step, we are going to use the Nginx image from the docker hub. For the third step, we are using Docker compose to create services and launch the cluster of containers. Instead of Node JS app, one can think Java Spring application or Django with Gunicorn running on the same port.
If you are new to Docker, please go through the documentation on the official website, they have tons of content on it. I will explain about the docker-compose briefly along with installation.
If you are developing a web application(For ex: SPA), you need to write the UI and backend service separately. When you test it, you will run into the CORS(Cross Origin Requests) problem. To overcome you set CORS headers to play nice with incoming requests from different origin. But in production, that approach is strictly discouraged due to security reasons. What should we do? We need to proxy the requests to different port using a proxy server. When we use a proxy server to serve both static files and dynamic services, virtual same origin can be achieved. That negates the CORS.
Make sure you installed Node JS, NPM, and Docker on the host machine(most probably custom Linux or EC2). There are many good installation guides available on the web. Docker compose is a wonderful tool. Install docker compose using these commands.
Hoping we have all above software installed, let us proceed with our illustration. Create a new directory called webService on a Linux machine.
This directory holds the information about our docker containers, source code etc. Create two new directories one for our app service and other for nginx.
Now app will hold the logic for our service. You can also develop that in another place(Git cloned path)and can copy it here before building.
Go inside the app directory and add below files.
Any node project is started by initializing the package.json. Create a package.json like this.
app/package.json
Now let us create the source code for server in app directory
app/server.js
If you see, we are creating a simple express service with a health check endpoint. Now create a Docker file corresponding to this project.
app/Dockerfile
This Dockerfile basically tells that:
Now our app(web service) is ready. Let us create the nginx container information. Come out of app directory and enter into nginx inside webService.
Create a nginx configuration called default.conf. We copy this to override the default configuration that is going to be created inside the container.
nginx/default.conf
We are pointing nginx to create an upstream service and use it to proxy all requests incoming to /api/v1 to forward to our Node web service. But wait! what is app:8080 in the upstream block. It is the service name we are going to create in the docker-compose file later. This Nginx is one service(container) that talks to app service to forward and accept requests. So we need to know which container exactly we are going to talk to.
Let us create a directory for our public resources like index.html.
And add this content to it.
nginx/html/index.html
It is a simple HTML file that on loading makes an API call to health check service and displays response on the web page.
Now add the Dockerfile to nginx directory similar to the above Node app to do few custom things. Here we copy the contents of html directory to the /usr/share/nginx/html and also copy the configuration file default.conf to the /etc/nginx/conf.d/
nginx/Dockerfile
Now we have everything we need. Create the docker-compose.yaml file to instruct the docker-compose to build and launch containers using our Dockerfiles.
./docker-compose.yaml
There is a lot to discuss this yaml file. We are creating two services called nginx and app. Since Nginx need to forward the 80 port we are adding ports command. build command picks up the Dockerfile from given directory. networks command tells which custom network a service uses. We can create a custom network(here mynetwork) using networks command. We are adding the driver as a bridge means the services under this network can talk to each other. Verion “2” tells that this is second type of YAML syntax. docker-compose has Version “1” before.
What actually Docker Network means?
By default, all the containers we create will fall under the same Internal IP range(Subnet). Docker networking allows us to create custom networks with additional properties like automatic DNS resolution etc. In the above YAML file, we are creating a network called mynetwork. The services(containers) app and nginx will lie in the same subnet and can communicate to each other without the need of exposing the web service container to the outside world. In this way, we can make a single entry point to our web service that is through the Nginx service. If anyone tries to access app service directly they cannot do it because it is hidden. This actually secures our application.
Create custom networks to control many network related aspects (IPAM, Static IP, DNS etc) of Docker containers
Let us see how the directory structure ended up.
Now run the docker-compose command to build the docker images first and then launch containers. Do this from webService directory.
It picks the docker-compose.yaml in the current directory and tries to pull and build docker images by running “docker build” on Dockerfile in each service. It spits a lengthy log on to the console. Once this operation is successful we can up the services(containers) using
Now visit the IP of host that running docker. It can be localhost or EC2 VPS.
This page actually requests the /api/v1/healthcheck which first hits Nginx service. Nginx service then proxies that request to app service and gets the result back.
We can also access the service using the API
But we cannot do this
because we are not forwarding the port of app service container to outside world. This actually enforces requests to go only through Nginx(Single Point of Entry). If we have a container that runs database server, that also can be added to the bridged network(here mynetwork) and not exposed to the outside world.
If you make any changes to the nginx configuration or source code, just run docker-compose’s build command once again to update the containers.
Long back we added this line in the nginx configuration file.
Since nginx and app both are bridged using mynetwork, one can access another by the service name. So DNS is already taken care by docker. If this privilege is not available, we need to hard code IP in Nginx configuration file or assign a static IP from the subnet in the docker -compose.yaml file. This is a wonderful thing about docker networking.
Now our service as well as static files are up and running securely.
The entire source code for this project is available here. https://github.com/narenaryan/DockerCompose
Thanks for reading! Hope you are not sleeping :) You can reach me here
A collection of developer experiences from wide domains…
182 
2
182 claps
182 
2
A collection of developer experiences from wide domains like Python, JavaScript and Web Development
Written by
Senior Engineer @ Dolby. All opinions here are mine
A collection of developer experiences from wide domains like Python, JavaScript and Web Development
"
https://medium.com/google-cloud/how-to-test-google-cloud-services-locally-in-docker-d74196147841?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
The “gcloud” command-line tool is amazing, it is super simple and works without problems and you authenticate with that it opens a browser window that then sets the correct values, something that is much easier than generating access and secret token that you have to manually set in your bash config.
However with that comes the question, how do you get that auth inside a docker container running locally.
Especially now that “gcloud auth login” says:
We need to run this command instead that will write down the authenticating information down into a file
That saves it to a file here:
Now you need to change your docker-compose.yml file to have this:
This will make the google-cloud package look for the right place for authentication.
Success, you can use gcloud components inside docker.
Google Cloud community articles and blogs
150 
3
150 claps
150 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
I really like building stuff with React.js and Docker and also Meetups ❤
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/the-future-of-cloud-services-is-borderless-4710914f1b01?source=search_post---------7,"There are currently no responses for this story.
Be the first to respond.
Disclosure: Manifold, the developer marketplace, has previously sponsored Hacker Noon. Use code HACKERNOON2018 to get $10 off any service.
Cloud compute is a commodity. Containerization technology and platforms such as Kubernetes level the compute playing field to the point that it doesn’t matter where your VMs are running.
They remove the risk of vendor-specific implementations and platform quirks. You now orchestrate your infrastructure in a uniform manner across the board.
You no longer make one gargantuan decision to lock yourself in a single ecosystem. You mix and match, evaluating where you can get the best support and price for the capacity you require.
It used to be that you would get as many services as you could from your cloud compute provider. This was safe and easy because every deployment story began with selecting a compute solution that fit your workflow.
Next you would add the additional services you don’t want to manage yourself: DNS, monitoring and logging, email, database. Your cloud compute provider typically offered a “good-enough” version of these, causing you to double down on the ecosystem. It was easy, you just clicked a few buttons in the same place you managed every other service and you were done.
What made it even easier was cloud compute providers would sweeten the pot with large amounts of free credit, giving you the freedom to just keep piling services on.
This is where they set their trap.
Your monthly bill steadily climbed as you added new services, even ones you didn’t really need. Each service had its own pricing scheme and calculator, making it difficult to know exactly what you are signing on for.
Each of these “good-enough” services would have their own platform-specific quirks and implementation details. Only working together through proprietary APIs and integrations.
Soon the honeymoon period ends and the credits dry up. You would face the daunting reality that you had to pony up the cash to keep going because the effort to move was too daunting. You’re locked-in.
The alternative to locking yourself in to one provider’s ecosystem is to purchase the best tool for the job regardless of where you source it from, but this has its own problems.
Now you have multiple people adding services from several vendors to your application.
Your billing is fragmented with different pricing models and no longer comes on a single bill to the appropriate person at the end of the month.
You lose the ability to predict how your costs will scale as your application grows. You lose track of services that are dormant, but still being paid for. You have multiple payment methods and inevitably one will be rejected at some point in time causing downtime and confusion.
There is no single source of truth. You have no visibility into the breadth of services necessary to operate your application, complicating onboarding new developers and the auditing of who has access to what.
Developers have to go to multiple sources to obtain configuration necessary to run your application and one thing we all know is: developers can be lazy. Workflow fatigue results in less than ideal handling of your secrets (values in plain text, stored on disk, copied and pasted), which are critical to security and integrity.
It’s a mess, and annoying to work with.
At Manifold our team embraces the fragmentation of the multi-ecosystem infrastructure. We want to source the best tool for the job regardless of where it comes from. We want our developers to have the freedom to add services when they’re needed and not have to go through lengthy approval processes.
We have an ace in the hole.
We use our own product to manage our cloud services and configuration. Manifold’s marketplace is a single location to find, buy and manage cloud services from multiple vendors that are the backbone of your applications.
We have a number of cross-discipline teams that work on multiple applications, they need to be able to focus on building our applications and not worry about obtaining configuration, secrets, or what service is managed by which team.
Every time a developer provisions a new service, they don’t have to worry about adding billing information. We get a single bill at the end of the month detailing all the services being used by the company.
We consolidate our cloud services, internal services and general configuration in a single location to make a seamless workflow regardless of which platform our application is deployed to.
Each application is grouped into projects, allowing us document exactly which services and configuration are required for which application. No more chasing down people from another team just to get an API token.
Sometimes the services we use aren’t available directly through the Manifold marketplace, this is where we add Custom Configuration resources. This lets us bring any external secrets into the Manifold ecosystem to live side by side with purchased services.
Similarly, sometimes we create our own internal services (workers, APIs, etc) that need to speak to each other, and Custom Configuration lets us bring those along too giving us a single source of truth.
Configuration where you need it
In development, all of our developers use the Manifold CLI to seamlessly inject the required configuration to their application. No more config files, no more plaintext secrets. The configuration variables are securely delivered at runtime from Manifold’s API.
In production, we use Terraform to deploy our infrastructure from code. Using the Manifold Terraform Provider we’re able to define exactly which configuration needs to go where in our architecture, reconciling the most up to date values with every deploy. Our operations team no longer have to worry about having the correct keys for services another team provisioned.
With Kubernetes on the horizon for our stack, as we prepare for a true borderless ecosystem, we implement the same functionality using a Custom Resource Definition that continuously reconciles the correct configuration and secrets with our cluster. This makes it even easier for us to migrate clouds when we want to.
Ecosystem lock-in is no longer the norm. Developers want choice, they want to be able to choose the best service for the job that affords them the best workflow.
As cloud computing becomes a commodity, the portability of your services and config becomes paramount. You need the flexibility to change where your applications are deployed without sinking huge effort into translating to a new platform.
Manifold takes the first steps towards modeling your services and config in such a way that you don’t need to think about which platform your application is being deployed and is continuing to evolve the multi-ecosystem workflow.
#BlackLivesMatter
566 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
566 claps
566 
Written by
Co-founder @ManifoldCo.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder @ManifoldCo.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technicity/cybersecurity-researchers-demonstrate-the-vulnerabilities-of-cloud-services-79f0b812eb2a?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Faisal Khan
Dec 7, 2021·3 min read
"
https://levelup.gitconnected.com/cloud-computing-101-intro-to-cloud-services-and-concepts-2bc89bce2de?source=search_post---------9,"You have 1 free member-only story left this month. Sign up for Medium and get an extra one
Cloud computing has gone from a technology limited to big billion-dollar corporations to one that is accessible to anyone. Most developers probably have forgotten about an EC2 instance running an AWS occasionally because, at the end of the day, it might only be 10$ a month to keep a Linux box going.
This accessibility provides an opportunity for small and medium-size businesses that want to have access to the same level of technology as larger corporations have for a fraction of the cost.
These technologies can help your business scale and take advantage of technologies that used to require multiple system admins and developers to set up. Now all you need to do is access a management console, click a few buttons and you’re there.
Alright, it’s not that simple.
If it were, then everybody would be doing it.
But the first step is knowing what is possible.
So the purpose of this article is to cover some basic concepts that you as a business owner should understand so you know what you can do.
Let’s start with a very important concept, that is a private cloud or sometimes referred to as a virtual private cloud or VPC. This allows your team the ability to set up a virtual network that is only accessible to team members with the correct authorization set up(sometimes this is set up using AWS Workspace, VPNs, etc).
This enables your company to have its network. Instead of having to set up your server and plugging in all your computers into it using ethernet you can just set up your network. This is great because it can allow you to keep a lot of sensitive information away from the internet(or at least a few more layers away). It used to be that companies needed to hire a system admin, set up servers and hundreds of capable to put together their network.
Perhaps you have heard this term thrown around as the new hot buzz word. They use a serverless infrastructure.
How does that even work? How can you have no actual infrastructure and still have code function?
Well, its because serverless does not refer to there being any server involved at all, just that there isn’t a designated to a specific server. Instead, the cloud service provider is only responsible for executing the code. Where it executes is essentially up to the provider.
This allows the cloud service providers the ability to reduce their costs and thus pass on their cost-savings to you, the customer. This has proved very effective at reducing costs when compared to using other services like EC2.
To start this explantation out, virtualization does not always need to be cloud-based. Virtualization has been popular for a while. Essentially, what virtualization lets you do is put multiple environments one piece of hardware. This could be multiple virtual servers, virtual desktops, etc.
For example, virtual desktops, AWS offers something called AWS Workspace. This is essentially a computer you can remote into and use.
Why is this helpful?
As the world continues to push for more remote work, having to get a laptop that is ready to be used by new employees set up and sent to your employees is not only logistically time-consuming, it is also a security risk.
Instead, you can spin up virtual desktops that your new employees can use that day.
As a company grows, it can be difficult to keep track of how many laptops you own and when you need to buy more. This causes many companies to over-buy laptops and often loose track of their resources. The other option is to develop a resource tracking system that often requires a few employees to manage and monitor. All of this can end up costing the company thousands of extra dollars a year.
Going back to the concept of a VPC idea. When you are already utilizing a cloud-based network, having virtual desktops, for example, makes it easy to manage your network. Often much of the rest of your servers, in this case, are technically also virtual. So when it comes to managing the ins and outs of IP addresses and which computers can access your network, having a virtual network simplifies a lot of that.
There are usually just a few settings to configure and your workspace can access the rest of your VPC while external users can’t
Vendor Lock-In is a very important subject that all small and medium-size businesses need to understand because it can impact them financially down the line. The one dark-side to your business becoming reliant on any technology, cloud or otherwise, is Vendor Lock-In.
This is not so much a technical term as it is a general concept. It refers to a point where a company becomes dependent on a vendor for a crucial part of that companies business, and the company can’t switch to a different vendor due to the high switching cost.
For example, let’s say you develop your entire application on AWS. Then you find out you could be paying less by switching to Google’s cloud.
Great!
But what is the cost to switch? If you need to hire a developer just to switch your software, reconnect APIs, and switch all your infrastructure, this might be too expensive for your company to take on all in one shot. Even if down the line it might be cheaper.
This is why it is called Vendor Lock-In.
The cost of switching right now might be too high, even if it saves you money in the long-run. Sometimes, some cloud providers will even put in sneaky clauses in your contract with them that if you quit using their service you still have to pay some ridiculous amount of ending your service early.
So be careful what you pick early on.
In cloud computing, there is a concept referred to as elasticity. This refers to the ability of a system to adapt to workloads by both decreasing and increasing the resources as needed.
The ability to have a system that can adapt is one of the reasons cloud service providers exist in the first place. It’s not just about the hardware but it’s about the software that is managing the hardware. Having software that can quickly scale resources to manage incoming web-traffic or database transactions allows customers to pay for only what they use.
An example of this is AWS’s EC2. This stands for Elastic Computing 2. This allows you as a user
If your business is larger or just utilizes a large number of cloud services, then you might eventually require to migrate from one cloud provider to another. Why your company might do this is because perhaps you could save an impactful amount of money by switching or you might need some of the technical abilities from the other cloud.
If your company is small, then this will probably range from 20k -40k but if you’re much larger then this could easily be above 100k–200k. This depends on what types of services are being migrated. For example, having to rewrite code due to migration will be very expensive whereas just changing where you store your back-ups will be slightly less expensive.
Amazon’s Simple Storage Service (Amazon S3) is a cloud storage service comes that allows you to interface with your stored objects using REST and SOAP.
S3 provides access to a storage system that is fast, reliable, scalable, and inexpensive data storage infrastructure. Several client types, big or small, can make use of its services to storing and protecting data for different use cases.
Amazon S3 offers an object (which are essentially files) storage service with features for integrating data, easy-to-use management and everything else cloud often offers. It can essentially act as a type of file server that can manage your companies content for your websites like videos and photos or be used to develop a data layer for your analytics.
BigQuery is a serverless enterprise-level data warehouse built by Google using BigTable.
This application can execute complex queries in a matter of seconds on what used to be unmanageable amounts of data.
BigQuery supports SQL format and offers accessibility via command-line tools as well as a web user interface. It is a scalable service which allows user to focus on analysis instead of handling the infrastructure.
I enjoy the online web UI that BigQuery has. No need to set up any connectors or download any third-party tools to interact with the data.
Have you ever wished you could just hit deploy and see your website online without too much configuration? With the service AWS Beanstalk, you can easily deploy your web applications developed in Java, Python, .NET, PHP. and several other languages without having to spend too much time configuring servers.
The Elastic Beanstalk service is used to deploy and scale applications using servers such as Apache, Nginx, and IIS. To use this service, you just have to upload the code on AWS, and all of the deployment processes — such as autoscaling, application monitoring, and capacity provisioning and balancing — are handled automatically by beanstalk.
With those basic terms out of the way, our team hopes you have a good baseline for cloud computing vocabulary. There are still plenty of nuances, but having a good base understanding should help you as you are looking to make key decisions.
Coding tutorials and news.
142 
142 claps
142 
Written by
#Data #Engineer, Strategy Development Consultant and All Around Data Guy #deeplearning #dataengineering #datascience #tech https://linktr.ee/SeattleDataGuy
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
#Data #Engineer, Strategy Development Consultant and All Around Data Guy #deeplearning #dataengineering #datascience #tech https://linktr.ee/SeattleDataGuy
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://betterprogramming.pub/how-can-cloud-services-help-improve-your-businessess-efficiency-ea3fb038948e?source=search_post---------10,"Sign in
This is your last free member-only story this month. Sign up for Medium and get an extra one
SeattleDataGuy
Mar 28, 2020·5 min read
In the 20th century, companies relied on servers and computers that were on the premises.
This meant when new servers had to be spun up, it could take weeks or even months to get everything set up. From getting the budget approved, to putting out orders, to having servers shipped and then installed — it was a long and arduous process.
But times have changed and the concept of companies having millions of dollars of unused servers on-site has been replaced by cloud computing services.
Simply put, cloud computing is a remote service that takes the form of infrastructure, software, storage, platforms, and a host of others.
The variety of services offered allows companies to not only reduce their technical hardware costs but also their employee costs. Much of what used to require a whole team of engineers can now be done by one or two developers.
With all the variety, it can be difficult to keep track of all the possible options for cloud service types — let’s take a look at what’s available.
IaaS, or Infrastructure-as-a-Service, is a service model that works as a pool of virtual or physical infrastructure that includes servers, networking, data centers, hypervisor layer, and even virtualization.
Typically, this service also includes management of infrastructure as well as storage resiliency. Through IaaS, companies can install OS, deploy their databases and have the flexibility to constantly change the amount of storage, or environments they’re using. You can use pay monthly, hourly, or weekly — whichever is convenient for you.
IaaS is one of the fundamental service delivery methods in cloud computing. It allows companies to access storage, network, servers and much more, over the internet, via third-party cloud computing service providers. IaaS allows organizations to create an IT environment that meets their business requirements.
Some of the most popular IaaS examples include products likes AWS’s EC2 or RDS. These services allow you to easily spin up a Linux server or a database and scale it to your needs. No need to get a new server, just click create on AWS. There are also AWS workspaces which allow companies to spin up virtual PCs on a virtual private cloud network. This makes it easier to manage security and do remote work.
Startups and small companies work with IaaS specifically to avoid spending more on hardware and software. They require scalability of IaaS and service providers like Google are great for it! You can actually keep exchanging day-to-day documentation on one central sheet via a simple link.
SaaS, or Software-as-a-Service, is a software distribution model that hosts applications online and makes them available for consumers. All you need is an internet connection and a web browser to use these apps.
The biggest benefit of SaaS is that it offers a web delivery model that completely eliminates the need to have an IT staff to install or download applications on every computer. For example, I use Google Docs a lot which not only helps me greatly by offering online storage but is accessible anywhere, on any device, with auto-saving options.
Besides, the vendor manages technical issues which makes app usage more convenient for SaaS users.
Some of the most popular SaaS providers include Google GSuite, Salesforce, Dropbox, and SAP Concur. The best example of SaaS is Gmail, an online email service, that allows you to access the files and applications hosted by Google from any device via the internet. The ability to use the product anywhere you have an internet connection without downloading software or using a product key allows employees to be productive anywhere.
PaaS, or Platform-as-a-Service, is a cloud model that delivers software and hardware tools enabling consumers to develop, run, and test their applications. One of the biggest advantages of using PaaS is that it offers easy migration towards a hybrid model.
PaaS is similar to SaaS, however, instead of delivering software over the internet, it delivers a platform for creating software online. This makes building websites and apps much simpler.
Typically, PaaS delivers a framework for developers so they can create customized applications without having to worry about the infrastructure. PaaS works specifically for developers who can create the design and manage applications online. These applications tend to be highly scalable and always available.
Some of the most popular PaaS service providers include OpenShift, AWS Beanstalk, Google App Engine, and Windows Azure. OpenShift, for instance, includes Linux OS, networking, registry, monitoring, authentication, and container runtime solutions so customers can use OpenShift to set up the infrastructure for their own enterprise developers.
FaaS, or Function-as-a-Service, is a category of cloud services that provides a platform for developers to run and manage the functionalities of their application without worrying about the infrastructure or complexity associated with the development of the app.
The biggest benefit of FaaS is that it offers serverless computing, letting developers deploy their production-ready code to the internet without the hassle of planning, provision, or maintenance of computing resources.
Most Popular examples of FaaS include Amazon Lambda, Microsoft Azure Functions, IBM Cloud Functions, and Google Cloud Functions. Lambda, for instance, executes the code only when it is needed, and scales the requests per day as per requirement automatically. Therefore, it comes in handy when combined with API Gateways to develop one optimal solution.
Cloud computing can help a company scale up their IT solutions fast. It can offer a full-fledged platform that takes care of storage, servers, and virtual desktops. With cloud computing services such as IaaS, PaaS, SaaS, and FaaS, your business can use virtualized infrastructure to manage its IT side.
Not only this, but the virtualization also makes it easier for companies to reduce costs and allow better analytics.
Cloud computing is all about simplifying technological processes. If your organization hasn’t taken up the services yet, you better begin the research to stay relevant in the market.
#Data #Engineer, Strategy Development Consultant and All Around Data Guy #deeplearning #dataengineering #datascience #tech https://linktr.ee/SeattleDataGuy
132 
1
Thanks to Zack Shapiro. 
132 claps
132 
1
Advice for programmers. Here’s why you should subscribe: https://bit.ly/bp-subscribe
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/next-level-german-engineering/how-to-realize-a-future-proof-business-intelligence-platform-using-saps-cloud-services-37c1839479f?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
Markus Hartmann, IT Project Lead at Porsche, reveals Porsches learnings concerning Data analysis and Data processing. He explains how constructing a new Business Intelligence Cloud not only made the Customer Relationship Management way more fruitful, but also made internal reportings much more efficient.
Data is on everyone’s mind these days and increasingly perceived as a key resource. Before its value can be unlocked, however, it needs processing, refinement and analysis. To effectively manage and utilize large volumes of data, it is imperative to have a solid business intelligence strategy in place. At Porsche, we have recently given a lot of thought as to how to approach this matter. One thing that became clear to us is that we needed to expand our business intelligence. As a result, the department of customer relations has overhauled its reporting and analytics capabilities. The partnership with SAP, which spans over twelve years by now, has been instrumental in this process.
In 2007, Porsche established a new Customer Relationship Management program, called CRM @ Porsche (or simply C@P), to facilitate global standardization and harmonization of sales and customer service processes. Porsche envisioned C@P as a comprehensive process platform that would allow the organization to better manage the customer journey and further strengthen the buyer-seller relationship. One major milestone was when we suddenly became a service provider for other brands in the Volkswagen Group. Bentley and Skoda are now also using our CRM services and processes. However, it wasn’t until the mid-2010s, that we fully embraced the latest CRM technologies.
In 2016, when I joined Porsche, we all agreed that we needed a state-of-the-art business intelligence, reporting and analysis solution — something that allows us to visualize data in real-time, is flexible and offers dashboards instead of just raw data. After all, we’re interested not simply in data, but rather in insights. SAP’s Cloud services, the HANA Enterprise Cloud and SAP Analytics Cloud, were an ideal solution for a number of reasons. SAP’s technology is user-friendly, intuitive, offers advanced features, easy scalability and integrates seamlessly in our C@P landscape. What is more, thanks to SAP, our Business Intelligence Cloud now organizes all data in one place, giving us the tools we need to effectively respond to our customers’ needs.
But things haven’t always been plain sailing. For example, previously, our reporting methods were mainly relying on static spreadsheets and PDFs.
In fact, traditional reporting was time-consuming and cumbersome, as each report relied on manual processing. Often, we would hire outside specialists to create reports. But that meant that we were always lagging behind by at least thirty days. Furthermore, the worldwide reporting for all our Porsche dealers was not optimal, as requested reports were no more than tables and raw data. We left it to the sale representatives to download the CRM information, extract it, aggregate it and visualize it.
The solution, then, was to integrate everything onto one platform. We wanted to make CRM information accessible for our users, from anywhere in real-time, and let them share reports easily with others. When I showed our first prototype to sale representatives in 2016, they were exhilarated. What previously took two days could be done in real-time. After completing the original pilot, we rolled out the new service across the company.
Today, we have a future-proof solution. It was amazing to see how the Cloud service advanced over the years. Our Business Intelligence Cloud features full functionality across all platforms and devices, and all CRM data is available in real-time. If you come to a Porsche Center and are interested in buying a new sports car, the sales advisor enters your data into the CRM system, and everything is immediately available and visualized in interactive dashboards in the cloud. Whether in Zuffenhausen, Los Angeles or Tokyo, every sale advisor in every Porsche dealership around the world uses this service. So, coming from throwing raw data at our end-users to really delivering insight in real-time, that is real progress.
Indeed, Porsche now has a clear vision for its Business Intelligence Cloud, and our tools have grown ever more efficient, giving Porsche employees more ways to act on customer information.
To conclude, SAP plays an important role in the field of business intelligence, influencing how we generate insights, keep track of our customers (both existing and potential), and manage and analyze interactions with contacts. More about the strategic partnership with SAP can be found on the Porsche Newsroom.
Markus Hartmann is IT Project Lead at Porsche.
About this publication: Where innovation meets tradition. There’s more to Porsche than sports cars — we’re tackling new challenges, develop digital products and think digital with focus on the customer. On our Medium blog, we tell these stories. It´s about our #nextvisions, smart technologies and the people that drive our digital journey. Please follow us on Twitter (Porsche Digital, Next Visions), Instagram (Porsche Digital, Next Visions, Porsche Newsroom) and LinkedIn (Porsche AG, Porsche Digital) for more.
Next Level German Engineering: Where innovation meets…
42 
1
42 claps
42 
1
Written by
Official Medium Account of Porsche AG | #NextLevelGermanEngineering #createtomorrow | More: newsroom.porsche.com |
Next Level German Engineering: Where innovation meets tradition. The Porsche technology hub to create tomorrow.
Written by
Official Medium Account of Porsche AG | #NextLevelGermanEngineering #createtomorrow | More: newsroom.porsche.com |
Next Level German Engineering: Where innovation meets tradition. The Porsche technology hub to create tomorrow.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/habitat-from-chef-build-deploy-and-manage-your-cloud-services-f35ecf60e25a?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
Habitat lets you build your packages, but also supplies thousands of its own, offering a form of independent package repository that you can roll out to any production environment. The core concept of Habitat is a plan.sh file that contains details needed for Habitat to build a package, and there are a large number of variables you can add to one. Fortunately there a suite of tools to help take the drudgery out of defining a plan file, including a CLI tool for Linux, macOS, and Windows.
Inside the directory of a project, you’d like to enable Habitat for, issue the following command, and Habitat will attempt to set up a plan for you:
I tried running this command in an existing Node.JS project and Habitat generated the following plan file:
The above is a simple plan file, and Habitat allows you to set complex variables and use logic and iteration, find more details in the Habitat documentation on plans.
There are a couple of different ways to build your packages, using the GUI based builder service, or manually using Habitat studio.
To build a package with the builder, log in to the service, create an ‘origin’ and add integration points to it, which are currently the Docker Hub (DH), or the Amazon Container Registry (ACR). The source of your packages can only be GitHub repositories, as Habitat leverages a small application that watches for changes in the repository you specify. Maybe there will be other sources for your packages in the future.
Build your package by clicking the Build latest version button under the Build Jobs tab, or whenever you push changes to the master branch. If you configured an integration to the DH or ACR, then you can always find an up-to-date version of your package there, or you can install one manually on any platform with habitat installed using the following command:
And by default, any package you create is publicly available to any other habitat user.
The studio is a virtual environment that runs inside a Docker container where you can iteratively build, test and run packages and export them to a container registry. Inside of your source folder, fire up Habitat Studio, which is a self-contained, minimal shell environment in which you can develop, build, and package software free from any dependencies that are not specified by your application’s plan.
Inside studio, run:
And studio will start downloading dependencies for you to test your application. When you’re ready to release, export the package:
Of course, modern applications rarely run in isolation, to allow habitat packages to share resources with each other, you define what is shareable in the plan.sh file.
You define what a package can share with pkg_exports, for example, the port:
And what a package can consume from other packages with pkg_binds or pkg_binds_optional:
You can even access these binds from within your code using environment variables and packages for your language that access them such as dotenv in JavaScript.
You can run your packages as Docker containers in any of the myriad ways possible, including Docker Compose, and sharing bound variables between them. The same applies to running habitat-powered containers on ACR; you can run any package you create as a container and share details between them by using environment variables.
One of the biggest issues I had with Habitat was understanding what it is capable of. Its feature set is so rich, exploring what is fully possible is hard. I reached out to the community to ask for a summary of what they designed Habitat for and received a similar answer.
Habitat merges functionality from build systems, CI tools, service discovery and more. The project is in early days, so I suggest you run test projects before moving all your tooling over, but I would love to hear your thoughts on the tool and how it might help you.
Originally published at dzone.com.
#BlackLivesMatter
41 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
41 claps
41 
Written by
I explain cool tech to the World. I am a Technical Writer and blogger. I have crazy projects in progress and will speak to anyone who listens.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
I explain cool tech to the World. I am a Technical Writer and blogger. I have crazy projects in progress and will speak to anyone who listens.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/take-your-cloud-services-to-any-platform-with-the-manifold-cli-a45dbeb9666b?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
At Manifold, we’re dedicated to helping developers find, organize, and connect the best services to their applications. We recognize that developers need efficient workflows, which means making sure Manifold works effortlessly with the most popular tools and platforms. Our command line utility makes it easier for developers to access their Manifold-managed resources and configuration exactly where they need them.
How will the command line utility help? Inevitably your configuration and secrets end up on disk. Sometimes they are accidentally checked into source control. And when someone joins the team you run around to each of the account owners to obtain tokens for them. It’s a huge headache.
Manifold’s command line utility simplifies adding new cloud services to your application, making it as simple as installing a new software package. Once you have services and config managed through Manifold, you can deliver their secrets directly to your app as environment variables. When a new developer is added you just invite them to your Manifold team and they can instantly run your application with the appropriate config. No more tracking down access, no secrets on disk, and you never leave the command line.
See what other integrations we have available, and vote for new ones, by visiting our Integrations page. You can share your ideas for what we should integrate with next to hello@manifold.co.
You can start using our CLI today by following the Getting Started Guide available in our integrations documentation.
If you want to dig into the internals of our command line utility, check out the repo on Github at https://github.com/manifoldco/manifold-cli. There you will find the source code, details on how to contribute, as well as an avenue for logging and bugs or feature requests. And if you like the project, throw us a star ⭐️!
github.com
We’re interested in what you’re building! Whether you’re using our CLI or not, send us a message, to hello@manifold.co, with how you’re using Manifold for and what we could be doing better.
We're determined to make it easy for developers to use the…
424 
424 claps
424 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/how-to-integrate-cloud-services-for-image-upload-in-a-node-js-react-web-app-9cc0aea25015?source=search_post---------14,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mariana Vargas
Feb 18, 2021·6 min read
Can you imagine visiting your Medium profile page and seeing this?
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intel-tech/the-infrastructure-processing-unit-for-the-next-generation-cloud-services-3d813d1d4ed4?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
General-purpose processors like Intel Xeon are the lifeblood of the data center. It’s where most value-added applications execute. But it’s clear that more targeted, or domain-specific, processors are being adopted in the cloud, enterprise, and edge when the advantages can be shown to outweigh the costs.
In order to justify the investment in new hardware and software, this domain-specific processor must hit a high threshold. I believe the Infrastructure Processing Unit (IPU) has clearly passed the bar and is emerging as the most useful domain-specific processor. Let’s look at Intel’s first ASIC-IPU, Mount Evans, to understand this better.
The single biggest argument for any domain-specific processor is performance. This stands true for the IPU as well. The general-purpose CPU core count is scaling up dramatically. These drive huge changes in infrastructure needs. Today’s CPUs spend significant effort handling infrastructure processing instead of value-adding application processing. As network speeds and storage volume scale from 10G to 25G to 100G to 200G, the way in which we handle data processing needs to evolve and scale. A processing unit needs to meet the higher performance demands and understand the type of data and how it is processed.
Intel’s Mount Evans, on the other hand, comes in and pushes performance by building hardware offloads and utilizing right-sized ARM N1 cores for the subset of workloads that make sense in the IPU. Our chip has full vSwitch offload and is capable of supporting 200M packets per second. It can fully saturate a 200G network with remote NVMe storage operations. Mount Evans adds crypto and compression capabilities to make sure every packet sent across the network is secure and to reduce storage media demands.
While any one of these things is possible in the CPU, combining them all together and doing them while doing interesting application work isn’t possible to do well. Intel’s IPU, on the other hand, does all of these things and more in a performant way, freeing up the Xeon to do the value-added applications. Sometimes, the promised performance advantages don’t show up because these specialized processors have been designed for very specific benchmarks. Mount Evans doesn’t suffer from this issue: Intel’s IPU has been designed in partnership with a top cloud provider to provide the top performance under real-world workloads and conditions.
I debated whether to call this “wide applicability” or “TCO”. But they are related, and both apply. In order to adopt a domain-specific processor like Mount Evans, it only makes sense when the investment can be applied broadly and when the Total Cost of Ownership — TCO — -is materially better than the baseline.
In the Cloud, Infrastructure workloads exist everywhere. We’ve seen previous data from Facebook and Google showing this “infrastructure tax” ranging anywhere from 20% to 80% of a workload. As enterprise and edge become more cloudified, this same tax applies as the workloads get partitioned similarly.
In Mount Evans, we’ve accelerated these infrastructure workloads with very flexible hardware that can be adapted to work across Cloud, Enterprise, and Edge. And we’ve utilized an array of ARM cores to provide power-efficient performance across control planes and other infrastructure applications. Together, the Mount Evans design can tackle the infrastructure needs of many data centers and should provide a meaningful TCO advantage.
While performance, wide applicability, and TCO are clear motivators for using an IPU, the adoption of a domain-specific processor often happens when there is a new emerging use case. With Mount Evans and the IPU, we see several broad emerging use cases that help drive the adoption.
First, the cloudification of “compute ”with tenants and Infrastructure providers drives an increasing need for a separation of tenant workloads from infrastructure workloads. By moving to an IPU architecture, the infrastructure workloads can be fully isolated from the tenant, creating greater security (for both), reducing or eliminating noisy neighbor effects, and simplifying life cycle management across both processors. Mount Evans takes this a step further with increased virtualization and Quality of Service capabilities to really enable each tenant and the associated infrastructure to work in isolation from each other.
Second, the need to support Bare Metal tenants on the same infrastructure as virtualized tenants drive this a step further. Mount Evans enables the infrastructure provider to do this. By enabling things like full hardware virtualization, providing NVMe native interfaces to the IPU, and support device emulation, Mount Evans provides the hooks to allow a service provider to leverage the same service models for Bare Metal hosting as they do for VMs and containers.
Third, Mount Evans enables the Infrastructure Provider to move to a completely diskless architecture at the compute node. It does this by presenting the NVMe native PCIe device model to the CPU, but allowing the infrastructure provider to implement the correct network storage backend on the Mount Evans compute complex. This ends up being another huge operational and TCO advantage.
In modern data centers, a typical SSD on a local CPU might use 35% of its storage capacity and 25% of its available IOPS. By moving these off the CPU and across the network, the infrastructure provider can utilize both storage capacity and IOPS. And Mount Evans, with the offloads and RDMA transports, can do this in a highly performant and low latency way.
The IPU in general and Mount Evans, in particular, provides compelling performance advantages versus running the infrastructure workloads on the CPU. These infrastructure workloads apply widely across many if not all data center use cases, and using Mount Evans on these workloads will provide a meaningful TCO advantage because of the highly flexible acceleration hardware of things like vSwitch and the use of smaller efficient cores for infrastructure apps. This, in combination with enabling emerging use cases across the data center, makes using Mount Evans as your IPU the right choice for your infrastructure processing.
Performance varies by use, configuration, and other factors. Learn more at www.Intel.com/PerformanceIndex.
No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software, or service activation.
© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.
Intel‘s leading Technology Innovation blog on Medium. Follow for the latest tech innovation stories.
7 
7 claps
7 
Written by
Intel news, views & events about global tech innovation.
The Intel Tech blog is designed to share the latest information on Intel’s technology innovations, performance, and technical leadership accomplishments.
Written by
Intel news, views & events about global tech innovation.
The Intel Tech blog is designed to share the latest information on Intel’s technology innovations, performance, and technical leadership accomplishments.
"
https://medium.com/@buckh/talks-on-how-on-engineering-building-azure-devops-77001fbf07f?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
Buck Hodges
Aug 26, 2021·5 min read
Back in 2017, when I was still working on Azure DevOps, we put together a series of videos that are deep dives into how we worked. These used to be linked on the Microsoft DevOps page, but they have disappeared from there. So I wanted to put together a page that captures them all.
Note that Azure DevOps was called Visual Studio Team Services (VSTS) at the time (the name changed to Azure DevOps in September 2018). Before that, it was called Visual Studio Online (which was as confusing as it sounds).
For context, Azure DevOps began life as Team Foundation Server in 2003. The cloud was essentially non existent in 2003, so TFS was sold as software you downloaded and installed (or installed from a DVD). In 2010, Brian Harry, who founded and ran the team, recognized that we needed the product to be available in the cloud. While it seems stunningly obvious now, back then established companies (i.e., not startups) were not yet interested in putting their source code in the cloud. So we did this at a point in time when the vast majority of our customers were not interested in moving to the cloud. However, Brian knew it would take quite a while to move to the cloud and change the team from an on-premises software team into a cloud services, DevOps team. His vision was that we would be ready when customers began to move to the cloud. Today, the Azure DevOps cloud service has some of the largest companies in the world using it to plan, write, test, build, and deploy their software.
That context is important because the parts about the architecture of the product are a mix of things relevant to cloud services in general and things that are artifacts of having moved the product from on-premises to the cloud. The approaches to testing, security (DevSecOps), and the organization of software engineering teams are independent of the software being built.
The talks were recorded in front of an audience of MVPs and various people from the field in Microsoft.
Combining Dev and Test/QA into a Software Engineering Discipline Until 2014, Microsoft traditionally had separate developer (SDE) and tester/QA (SDET) roles in engineering. This talk covers our experience merging the dev and test teams into a single software engineering team. This fixed a lot of dysfunction that existed when the two were separate orgs.
Changing a Team’s Mindset about Security (summary) This talk received the highest feedback of any that day. It’s about changing the mindset around security. So often people think of security as a tax — having to run a bunch scans and going through security design reviews. While those are important, what matters most is getting engineers to think about security with every line of code they write. So often it’s easy for engineers to dismiss security bugs in other people’s code (“of course, I would never do that!”). There’s no better way to change a team’s perspective than attacking your own service to make vulnerabilities personal.
Running an Agile Team We often were asked how we did planning, organized our teams, and managed the overall process. In this talk, you’ll learn about our journey and the cadence that the team operated on.
Shift Left to Test Fast and Reliably (summary, blog post) We put a lot of effort into building a highly reliable test system as part of our engineering system. We had tests that were flaky and slow. When you have flaky tests, engineers ignore test failures (so you lose twice — the tests are ignored yet the engineer’s changes were delayed getting checked in). In 2014 I started an effort that resulted in the tests for the product being rewritten. When we were done a little more than 2 years later, you could count on a broken test meaning you had a bug.
Eliminating Flaky Tests Tests are code, so they will have bugs just like the product. So we built a system for identifying flaky tests to ensure they would not be counted as failures until they were fixed.
Testing in Production Even with a great set of tests in your engineering system, you’ll still need to be able to do testing in production.
Cloud Patterns for Resiliency (Circuit Breakers and Throttling) After some painful incidents, we introduced circuit breakers and throttling to be able to handle spikes in load. This is particularly important in a multi-tenant system where one customer can cause bad experiences for other customers in the system.
Progressive Deployment with Feature Flags (summary) Being able to have granular control over what is enabled where and for whom is hugely valuable for being able to minimize negative impact if something goes wrong as well as getting very early feedback from select customers. The original motivation for us to do it came from a long-running live site incident that we had in 2013. Most teams building cloud services from the beginning will include support for feature flags where for us is was something we needed to add when moving to the cloud.
Live Site Culture and Site Reliability Here covered how we monitored the system. One of the best things that we did is run the availability model in near-real time. When we first started doing this, we found a number of issues we had been completely blind to. We also wrote detailed postmortems for our outages for customers (which always made marketing nervous, but our customers loved the transparency). Here’s one of the early ones. The outage September 4, 2018 was the worst I was ever involved with. If you want a good debugging story, read this preliminary postmortem and the final postmortem (it was a great example of why it’s important to keep digging).
VSTS: From Monolith to Cloud Service Azure DevOps evolved from an on-premises product that was monolithic into a set of cloud services. This is most relevant to teams going through a similar journey of moving existing software into the cloud. In this talk I cover telemetry, single points of failure, SQL guidelines — it’s a wide range of topics.
Achieving No Downtime Upgrades through Version Service Updates (summary) Since the service began as an on-premises product, all of the upgrades in the beginning required down time, which is terrible for a service used around the world. So early on we had to make changes to support upgrades that would happen online without users noticing at all. We describe the challenges we had and how we dealt with them.
Safely Deploying Around the World (summary) You never want to deploy everywhere at once. It’s best to deploy in a sequence, starting small and growing as the deployment proves reliable. Here we describe what internally is called “safe deployment practice.”
I also gave a talk at VS Live! in 2017, Lessons Learned from Doing DevOps at Microsoft Scale (slides), that covered some of these topics in an hour. It covers four topics.
Group Engineering Manager for Microsoft Azure Machine Learning. Formerly Director of Engineering for Azure DevOps
52 
52 
52 
Group Engineering Manager for Microsoft Azure Machine Learning. Formerly Director of Engineering for Azure DevOps
"
https://medium.com/sonm/how-fog-computing-disrupts-the-cloud-services-market-46491052fe58?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
This article is the first in a series devoted to the cloud technologies market and the role of fog computing for its participants.
Today, the opportunities presented by cloud services and data storage are transforming business processes, just as the introduction of computers once did. But for business, one unresolved question remains extremely important: how to provide scalability for cloud technology. We are creating the SONM platform in order to solve this challenge.
As the burden on IT increases, cloud technologies are becoming more expensive and are eating up a substantial part of business revenue. The more computing power a company needs, the more the cost of renting cloud infrastructure increases.
The high cost of the services of cloud providers is a result of the vast sums they have spent on the creation of data centers, the creation of network infrastructure and the purchase of server equipment.
SONM offers the use of free capacity of hardware that is already running: PCs, servers and mining farms for fog computing.
Modern cloud technologies have two key aspects: technical and economical. The technical aspect lies in the fact that whereas earlier all computing was done locally on computers within a corporate network, today it is possible to “outsource” this work to data centers. If earlier users installed Microsoft Office on every computer, then now they also have access to the Google Docs and Office 365 cloud services. These services operate remotely and have no direct relationship to the computer, and all operations are carried out in the browser or on remote servers.
From an economic point of view, this fundamental transformation is manifesting itself in the fact that in practically every sphere we are witnessing a move from an ownership model to a temporary use model. The most obvious example of this today is car-sharing, but the same process is underway in IT: while earlier products were sold for a one-time payment, now the model is based on monthly subscription. This means that CAPEX — one-time capital expenditure on the purchase of software — is being squeezed out by OPEX, monthly operational costs.
Read more
SONM is a global fog computing platform for general purpose…
28 
28 claps
28 
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
"
https://medium.com/mobile-app-development-publication/enable-multiple-app-access-to-same-google-cloud-services-cbb13e410ba4?source=search_post---------18,"There are currently no responses for this story.
Be the first to respond.
It has been some time since I worked on GoogleMap. The key is there, and everything is working fine. There’s no need to access to Google Cloud Console.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/get-festive-with-manifolds-12-days-of-cloud-services-%EF%B8%8F-27b745ec3029?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
With the countdown to the holidays underway, we thought it would be fun to turn the spotlight on some of the services on our Marketplace you maybe haven’t seen yet.
Each of the next 12 weekdays, we will feature a different service on our homepage, accompanied by a blog post which will help you get to know the service through tutorials, walkthroughs and insider tips.
We hope that one or more of these services will inspire your holiday hackathon project, or give you some ideas for how to improve an existing application.
For a limited time, click here to redeem coupon code HOHOHO and receive $25 to spend on any of the featured services you’d like to try out
We're determined to make it easy for developers to use the…
151 
151 claps
151 
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
"
https://medium.com/swlh/are-computer-cloud-services-a-secure-option-for-your-business-9d2861022ed5?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
That’s an excellent question.
It’s a particularly important question if you run a small or medium-sized business and are trying to maintain a technological edge to serve your clients better and stay ahead of the competition.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@knolleary/connecting-to-ibm-cloud-services-from-node-red-fc574848c3fb?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nick O'Leary
Jun 5, 2018·5 min read
In the first part of this series I showed how to create a deployment pipeline from Node-RED running locally to Node-RED running in IBM Cloud.
It got the basic pieces into place to let you deploy a version controlled Node-RED application to the cloud. The next task is to connect some other IBM Cloud services to the application.
The existing Node-RED boilerplate comes with some extra nodes that are IBM Cloud-aware. They are able to automatically detect available instances of their respective services using the VCAP_SERVICES environment variable that Cloud Foundry provides.
One such collection of nodes are for the Cloudant database service, which we’re going to add to our Node-RED application.
The challenge is how to develop against those nodes when running locally — outside of the IBM Cloud environment.
Open up the IBM Cloud catalog and select the ‘Cloudant NoSQL DB’ service. Create a new instance, making sure you select the same region as your Node-RED application.
Go to the dashboard page for your Node-RED application and select the ‘Connections’ tab. Find your newly created Cloudant service in the list and click ‘connect’.
It will prompt you to restage the application which will take a couple of minutes to complete.
Once that’s done, go back to the ‘Runtime’ tab on the IBM Cloud dashboard and the environment variables section. You will see a section for VCAP_SERVICES - this is the environment variable that the platform uses to pass the application all of the details it needs to access the connected services. You should see an entry for our newly created cloudant instance - if you don't, make sure the restage has completed and reload the page.
Beneath the credentials is an ‘export’ button — clicking that will download a copy to a file called <your-app-name>_vcap.json.
Copy that file into your Node-RED user directory, ~/.node-red - do not put it under version control.
Edit your local settings.js file - this is the one in ~/.node-red not the one in your project directory.
Add the following just above the module.exports line and then restart Node-RED. Make sure to replace <your-app-name>_vcap.json with the actual name of the file you downloaded.
Your local Node-RED now has access to your service credentials in the same way as your Node-RED in IBM Cloud does.
Open up the Palatte Manager from the drop-down menu in Node-RED. Go to the ‘Install’ tab, search for node-red-node-cf-cloudant and click install.
Once installed, you’ll have a new pair of Cloudant nodes in the ‘storage’ section of the palette. Drag one into your workspace and double-click to edit it. The ‘Service’ property should have the name of your Cloudant service listed. If it doesn’t, check you’ve follow the steps to get your VCAP_SERVICES setup correctly.
Close the edit dialog but do not delete the node — we’ll come back to this a bit later.
Having installed the nodes locally, we need to add them to our project’s package.json file so they also get installed when deploying to the cloud. We can do this within Node-RED by going to the 'information' sidebar tab and clicking the button next to the project name. This opens up the Project Settings dialog.
Go to the ‘Dependencies’ tab where you’ll see a list of the modules our project depends on. This is a combination of modules already listed in package.json and modules which provide nodes we have in our flow. At this point you should have two entries node-red and node-red-node-cf-cloudant.
Ignore the offer to remove node-red from the project as we need that, but do click the 'add to project' button next to the Cloudant module.
If you switch back to the ‘history’ tab you should now have two entries in the ‘Local files’ section — manifest.yml and package.json. If you click on either filename it will show you a diff of what has changed in the file. Check the changes look correct then click the '+ all' button to prepare both files for committing and then commit them. Switch to the 'Commit History' tab and push the changes up to GitHub.
Wait for the Travis build to redeploy you application and then reload it your browser. You should now have the Cloudant nodes available in the palette and, as before, when you add one to your workspace and edit it, your Cloudant service will be selected.
At this point, an application built locally will use the same Cloudant instance as the one running in IBM Cloud.
If we consider the local Node-RED as a development environment and the IBM Cloud instance as the production environment, then they really should use separate instances.
This can be achieved by creating a second Cloudant instance to treat as the development instance. Rather than connect it to your Node-RED application, you can generate a set of credentials from the instance dashboard page.
Update the <your-app-name>_vcap.json file with the new credentials and after restarting Node-RED, your local instance will now be accessing the separate instance.
This post has shown how to connect IBM Cloud services to you Node-RED application with separate development and production instances. It’s another important step to creating production-ready applications with Node-RED in IBM Cloud.
In the next part of this series, we’ll look at how to start building a simple application using this setup. That’s what I said in the previous post, but I really mean it this time.
Originally published at knolleary.
Developer Advocate at IBM. Node-RED project lead. All things MQTT and IoT. A Beardy Dad. My words are my own.
See all (168)
19 
Some rights reserved

19 claps
19 
Developer Advocate at IBM. Node-RED project lead. All things MQTT and IoT. A Beardy Dad. My words are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/g-cloud-latest/g-cloud-and-the-hmrcs-closed-consultation-on-vat-treatment-for-cloud-services-cee7742916d7?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
VAT costs for off the shelf internet ‘Cloud’ IT infrastructures can not be refunded under COS 14 because they are not designed to the specification of recipients.
It is HMRC’s view that VAT costs are refundable under COS 14 where:
(i) a fully managed computer infrastructure using web-based or internet platforms are provided and
(ii) the IT infrastructures are designed to the specification of the recipient (e.g. GD/NHS) and
(iii) use of the infrastructure is for non-business activity.
As you know HMRC are producing a single book of guidance for Government departments and NHS bodies on Contracted Out Services (COS) and claiming VAT refunds under section 41(3) of the VAT Act 1994.
We shared an initial draft with you in July 2014 and received a large number of responses, some after the mid-September closing date. We would like to thank you all for your comments and bring you up to date on our progress.
We are updating the guidance to reflect your drafting suggestions and populating the Headings with additional examples where these have been requested. This is taking a bit longer than we had anticipated and we will publish the first tranche of guidance for 58 Headings on the HMRC website in January 2015. On the remaining Headings you asked for more detailed guidance and examples and we are working on these; they will be published as part of a rolling programme with a view to completing the process by early 2015.
Until the guidance for a particular Heading is published you can continue to rely on existing guidance which remains extant. We would also like to reiterate that this guidance seeks to clarify what can be claimed under the Headings and will not contain any policy changes.
It is HMRC’s view that VAT costs are refundable under COS 14 where:
(i) a fully managed computer infrastructure using web-based or internet platforms are provided and
(ii) the IT infrastructures are designed to the specification of the recipient (e.g. GD/NHS) and
(iii) use of the infrastructure is for non-business activity.
VAT costs for off the shelf internet ‘Cloud’ IT infrastructures can not be refunded under COS 14 because they are not designed to the specification of recipients.
The HMRC has little understanding of the various Cloud delivery models, IaaS, PaaS and SaaS. The guidelines need to take all these models into account.
There’s lot’s more to say about this, but I don’t have the time at the moment. More details here.
This is a letter from the HMRC explaining the process and how to provide feedback.
Here’s the stencil to provide your feedback. Ignore the feedback date, it’s still open. Cloud services are covered under COS heading 14.
The HMRC have released the updated COS guidance available here, but it does not include any new information on the IT and Cloud related services.
It’s great to see the HMRC move the document online
Historically, HMRC provided separate ‘Guidance Notes’ for government departments and health authorities which were written as an aide memoir for finance officials. These notes acted as a supplement to the existing guidance and public notices and highlighted where the rules for government departments and health authorities differ from those which apply to other VAT registered businesses.
HMRC are in the process of transferring the ‘Guidance Notes’ on Contracted Out Services (COS) to an on-line manual, this is being completed as a rolling programme. Details of the COS Headings that have been transferred can be found at VATGPB9700. For any Headings that are not currently on-line, government departments and health authorities can continue to rely on the existing guidance which remains extant.
HMRC has confirmed that COS 14 has been redrafted. See page 3.1 of the minutes from the HFMA VAT Technical Sub-Committee.
New Contracted Out Services (COS) code 14 guidance due out soon.
HMRC is due to re-issue the guidance relating to COS code 14.
The code specifically relates to IT type services and we are led to believe that the guidance may be slightly relaxed to allow further VAT recovery opportunities.
HMRC have permitted VAT recovery on fully managed computer infrastructures for a number of years. However, it has been suggested that you should focus on the terms “managed and serviced infrastructure” and review a computer initiative in its entirety, rather than just considering each contract on a standalone basis.
It’s here, the HMRC has finally published the guidance for COS Heading 14.
HMRC statement below:
“As you know HMRC have been working on producing a single book of guidance for government departments and NHS bodies on contracted out services and claiming refunds under Section 41(3) of the Value Added Tax Act 1994. We published the first tranche of this guidance in March 2015.
We have now finalised the guidance for the remaining headings. Unfortunately, the VAT government and public bodies manual (VATGPB) is currently in the process of being transferred from the HMRC website to GOV.UK and while this is happening it is not possible to publish any further amendments. In the interim you may treat the attached document as the current guidance for these headings.
In the attached document you will also see that we have made some changes to the first tranche of guidance published in March 2015 (pages 14–17) to take into account comments made to us after the guidance was published.
Once the guidance has been published in the VATGPB manual on GOV.UK, this draft document will become obsolete.”
The VAT government and public bodies manual (VATGPB) is currently in the process of being transferred from the HMRC website to GOV.UK and while this is happening it is not possible to publish any further amendments. In the interim you may treat this document as the current guidance for these Headings.
In this document you will also see that we have made some changes to the first
tranche of guidance published in March 2015 (pages 14–17) to take into account
comments made to us after the guidance was published.
Once the guidance in this document has been published in the VATGPB manual on
GOV.UK this draft document will become obsolete.
Everything you need to know about G-Cloud
2 
2 claps
2 
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://digitizingpolaris.com/qubole-ceo-hortonworks-and-cloudera-cloud-services-offer-few-new-benefits-64b0e0e1e8c6?source=search_post---------23,"If you think that putting big data crushing technology that was built for your company’s data center into the cloud will yield big benefits, think again. At least that is what Qubole co-founder and CEO, Ashish Thusoo, told Digitizing Polaris.
To be fair, we asked him why Qubole’s Hadoop-as-a-Service was still relevant when all three of the major Hadoop vendors (Cloudera, Hortonworks and MapR) had their own cloud products. His answer was clear: “We built Qubole so that our customers could take advantage of big data infrastructure on-the-fly,” while the others took their on-premise Hadoop distributions and put them into the cloud.
It’s worth noting that the vendors who may be ill-affected by Quoble’s existence aren’t just the Hadoop providers, Thusoo says that customers of HPE and IBM can do better by consuming services from his firm as well.
For the record,Qubole provides not only Hadoop-as-a-Service, but Hive-as-a-Service, Presto-as-a-Service, and Spark-as-Service as well.
What’s special about Qubole is that it offers an Autonomous Data Platform which leverages machine learning and artificial intelligence (AI) to self-manage and self-optimize workloads. It does this by sending Alerts, Insights and Recommendations (AIR) based on Cloud Agents connected to a data team’s specific data policies and preferences.
The Spot Shopper Agent, for example leverages AWS Spot Instances to find the best deal it can to run jobs and has saved an average of 68 percent on total cloud compute costs, according to Thusoo. He says that companies who use Qubole’s Workload Aware Auto-scaling product save an average of 80 percent.
Because of how Qubole was designed, its customers require less manpower, as few as 1–2 administrators for 5–800 users. For small to mid-size firms this makes big data insights a possibility and to larger enterprises this presents an opportunity to deploy talent where it can return the greatest rewards.
This kind of automation doesn’t take away good jobs, according to Thusoo, it frees smart workers and smart companies to apply their talent where it is more valuable.
News and narratives on the trek to the digital enterprise.
5 
5 claps
5 
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@vrypan/should-my-startup-use-cloud-services-hosted-in-the-us-cc0dcde44ee?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Panayotis Vryonis
Jun 9, 2013·2 min read
For the last 6 months I’ve been working on building a startup that will offer data archiving services. Back then, it looked like a good idea to host our data (our clients data, to be precise) with someone like Amazon AWS, Google AppEngine or Microsoft Azure.
Not any more:
Equally unusual is the way the NSA extracts what it wants, according to the document: “Collection directly from the servers of these U.S. Service Providers: Microsoft, Yahoo, Google, Facebook, PalTalk, AOL, Skype, YouTube, Apple.” — washingtonpost.com
NSLs enable intelligence organizations to send secret requests to web and telecom companies to gather data that is “relevant to an investigation.” They do not need a judge’s approval and come with a gag order. The FBI’s and NSA’s ability to issue NSLs was expanded under the Patriot Act, which passed in 2001 and President Barack Obama renewed in 2011 to give the U.S. government broad-reaching powers to collect data on Americans. — venturebeat.com
I know I’m deeply worried about my data stored by GMail, Amazon, Facebook and the rest. My clients will probably worry too, if I host their data in the US. Why would they want to store their digital archive in a country where government agencies have the right to secretly access it, without a proper court order?
Luckily we have some months before releasing our service to the public. In the meantime, we have to find a reliable workaround to US unreliability when it comes to data protection and privacy.
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
2 
2 
2 
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
"
https://medium.com/@manningbooks/an-overview-of-azure-cloud-services-f36b7f8ca675?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Feb 25, 2020·7 min read
From Azure Data Engineering by Richard Nuckolls
This article discusses Azure Cloud Services and it’s components.
__________________________________________________________________
Take 37% off Azure Data Engineering by entering fccnuckolls into the discount code box at checkout at manning.com.__________________________________________________________________
Microsoft Azure is a cloud services provider. This means Azure provides datacenter services and software which an enterprise traditionally hosted in their offices, in a data center building of their own, or in a hosting providers data center. Information technology resources that an enterprise hosts are referred to as “on-premise” resources. This distinguishes them from resources hosted “in the Cloud”. IT engineers usually have physical access to on-premise resources, but not to cloud resources.
Cloud services providers, like Microsoft Azure and Amazon Web Services, provide three main types of services, classified by the end-user management of the underlying operating system and software. The lowest level of abstraction provides Infrastructure as a Service (IaaS). IaaS services provide resources like virtual machines, routers and firewalls. The provider manages the hardware in their data center, and the end user manages the software and operating system. IaaS resources require technical and developer support to manage operating system and software installation, and create code to run on the servers. The next level of abstraction provides a Platform as a Service (PaaS). PaaS services provide server application hosting such as web servers, databases, and storage. The provider manages the hardware and operating system running in their data center, and manages server applications running on the operating system. The end user configures and uses the applications. PaaS resources require developer support, to create code to run on the server applications. The next level of abstraction provides a Software as a Service (SaaS). SaaS services provide user applications delivered over the internet. Typical SaaS applications include web-based email services or web-based file sharing services, which charge a subscription. The SaaS provider manages all aspects of the hardware, operating system, and software. The end user configures and uses the application. Microsoft has transitioned many of their operation systems, desktop and server applications to IaaS, Paas, or SaaS resources available in Azure.
Microsoft Azure offers both open-source and Microsoft technologies in its cloud services. Azure provides HDinsight for Hadoop engineers and data scientists. HDInsight manages containerized Hadoop processing nodes, with plenty of configuration access and overhead. Azure also provides DataBricks, a SaaS abstraction of the Apache Spark analytics engine. Both provide viable options for operating large analytics systems in the cloud. A third option exists for the Microsoft technologist. By using tight integrations provided by the Azure products, the Microsoft data engineer can build a sophisticated and flexible analytics system using familiar technologies like C#, SQL, and GIT. This article discusses these services and how to use them to build a complete analytics system in the cloud.
Let’s look at each of these services.
Azure Event Hubs provides a PaaS scalable message queuing endpoint, including built-in integrations with Azure Storage and Stream Analytics. Our analytics system uses Event Hubs as the entry point to our data processing pipeline. Using Event Hubs provides our system with a scalable and reliable buffer to handle spikes in the volume of incoming events. Event Hubs accepts both HTTP and Advanced Message Queuing Protocol (AMQP) packets for event messages. Plenty of clients are available for these protocols in your language of choice. These message queues can be read by one or more subscribers.
Events Hubs scale in two ways: first, the endpoint processes incoming messages with a throughput unit, a measure of maximum throughput at a fixed cost. Adding more throughput units allows a higher message rate, at a higher cost. Second, Event Hubs partitions the queue. Adding more partitions allows the Event Hub to buffer increased numbers of messages and parallel reads by subscribers.
Azure Stream Analytics processes streaming data. Streaming data is ordered by a time element, which is why it’s often referred to as events or event data. Stream Analytics accepts streams of data from Event and IoT Hubs and Blob Storage, and outputs processed information to one or more Azure endpoints. It uses a structured query language to query the data stream. The data process can be thought of as fishing a river with a net made of particular shapes. The data flows by the net, and the net captures the matching bits. The fisherman hauls in the net regularly to review his catch. Similarly, the queries pull result sets out of the stream as it flows by.
Stream Analytics scales in two ways. First, each Stream Analytics job can use one or more streaming units, a synthetic metric describing CPU and memory allocation. Each step in the job uses between one and six streaming units. Second, planning parallelism in the stream queries allows Stream Analytics to take advantage of the available parallel processes. For example, writing data to a file in Azure Storage or Data Lake Store can use multiple connections in parallel. Writing data to a SQL Server table uses a single connection, for now. At most, a single query operation can use six streaming units. Each Stream Analytics job can have more than one query operation. Planning the streaming unit allocation along with the query structure allows for maximum throughput.
Azure Data Lake Store stores files. It provides a folder structure interface over an Apache Hadoop file system, which supports petabytes of data. Multiple open source and native Azure cloud services integrate with Data Lake Store. Fine grained access via integration with Azure Active Directory make securing files a familiar exercise.
Azure Data Lake Analytics (ADLA) brings scalable batch processing to the Data Lake Store and Blob Storage. ADLA jobs use familiar SQL syntax to read files, query the data, and output results files over data sets of any size. Because ADLA uses a distributed query processor over a distributed file system, batch jobs can be executed over multiple nodes at once. Running a job with parallel processing takes moving a slider past one.
Azure Data Lake Analytics uses a new coding language called U-SQL. U-SQL is “not ANSI SQL.” (Rhys 1) For starters, WHERE clauses use C# syntax. Declarative statements can be extended with C# functions. Query data comes from tables or files.
SQL Data Warehouse (SQLDW) bears superficial resemblance to a standard SQL Server database, like Azure SQL databases. Most functionality matches: CRUD actions, views, stored procedures. Minor changes are table creation, indexing, and partitioning . The naive user could create a table, insert some data, and use the Data Warehouse like SQL Server databases.
Harnessing the power of SQL Data Warehouse comes from understanding the distributed nature of the underlying technology. Data resides in sixty shards, managed automatically. Queries are distributed across compute nodes, from one to sixty based on your configuration. Data imports from multiple files are spread across available compute nodes. The user controls scaling compute capacity, but storage relies on Azure Storage for scaling and redundancy.
Azure Data Factory automates the data movement between layers. With it, you can schedule an ADLA batch job for creating aggregate data files. You can import those files into SQLDW, and execute stored procedures too. Data Factory connects to many different endpoints for input and output, and can build structured workflows for moving data between them. Data Factory operationalizes these repeated activities.
Azure offers Cloud Shell as an option for managing resources in Azure via the command line. You can access Cloud Shell from the Azure portal, or by connecting to https://shell.azure.com. With Azure Cloud Shell, you can run Powershell or Bash commands to manage your Azure resources.
Imagine your company wants to analyze user behavior in their main website to provide relevant suggestions for further reading, to promote user retention and higher page views. A solution allows generating suggestions based on historical data, recent personalized actions, and machine learning algorithms. Further, the same system could also analyze error events in real-time and provide alerts. You can build a system in Azure which can do the analysis work. The rest of this article walks through use cases, technical tradeoffs, and design considerations when creating and operating each piece of a proposed analytics system which fulfills these functions. Before we dive deeply into each of the services, let’s take a look at the system as a whole. This architectural design uses the six Azure services discussed in this article.
Figure 1 shows how all six services can be assembled into an analytics processing system to monitor error rates and provide “users also viewed” suggestions. In this system, incoming event data follows both a hot and cold path into the user query engine. To illustrate how the event data flows through both paths, lets trace the flow of a typical user action event through both paths. We can see how each path fulfills part of our imagined business requirements for this system.
And that’s all for this article. If you want to learn more about the book, you can check it out on our browser-based liveBook reader here and in this slide deck.
Follow Manning Publications on Medium for free content and exclusive discounts.
See all (32)
2 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2 claps
2 
1
Follow Manning Publications on Medium for free content and exclusive discounts.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@david-bl/increase-flexibility-and-resource-efficiency-with-cloud-services-42ff912079c1?source=search_post---------26,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Bailey-Lauring
May 22, 2016·3 min read
Cloud computing has created a technology revolution for small businesses, offering access to a range of capabilities that typically only larger companies can afford. Using an Internet connection and a Web browser, small companies can tap into software and services as they need them and pay for what they use on a monthly basis, like utility services. Your business can join the “Cloud” to access everything from data backup, accounting software to customer relationship management systems.
Cloud resources are scalable so you can increase capacity to support growth and handle busy periods. One of the most challenging aspects of running a small company is predicting what resources your business will need — enough to scale and take advantage of opportunities, but not so much that you overspend. With Cloud resources, rather than having to predict your needs, you can use just what is needed to manage your growth and enhance your efficiency. For example, if customer and project demands require increased collaboration, you can access collaboration tools quickly and without advanced planning. Your flexibility as an organization rises when you work in the Cloud because your ability to react is greatly improved.
Cloud-based services can help you save money on many fronts, including server maintenance, power and cooling costs, and software licensing and upgrade expenses. Just under half — 49% of SMEs use Cloud computing to lower costs, according to a Microsoft survey.[1] Rather than spending money to maintain hardware that often goes unused, subscribing to software and services for a low monthly fee can help small businesses stretch their budgets further.
But there is a caveat — as your business grows, so does the amount of cloud based software and the data it contains. The sheer volume of stored customer data requires a team to input and manage. SMEs are now searching for cloud software to manage their existing internal business services. One of the excellent features of cloud based services is that they can be easily integrated with one another to ensure that data is accurate and updated. Rather than enter the same data in 2–3 systems using a team of 2–3 colleagues, companies can enter the information once in a cloud based central hub that connects with various web based software.[2]
Start organising your services better by connecting your two most used systems today. Perhaps it’s your CRM and bookkeeping system. See how data moves between the services and scale up your cloud usage from there. Reap the benefits of increased efficiency and flexibility across your business processes.
[1] Microsoft U.S. SMB Cloud Computing Research Summary, Fall 2010
[2] Cloutex customer data Summer 2013
David is the Managing Director of Blu Mint Digital; and writes about Digital Marketing and Entrepreneurship. This article he wrote was written for Cloutex.
Favour: Would you scroll down + tap the green heart to ‘Recommend’ this if you liked it?
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
2 
2 
2 
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
"
https://medium.com/@krmarko/precision-agriculture-eats-data-cpu-cycles-it-s-a-perfect-fit-for-cloud-services-73ca7bef478b?source=search_post---------28,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Aug 26, 2015·2 min read
Agriculture marches to its own version of Moore’s Law, with crop productivity steadily increasing for decades. While past improvements were the result of better plant hybrids, fertilization and production equipment, information technology will be the key to sustaining and perhaps accelerating agricultural productivity. Precision agriculture, a collection of data collection, analysis and prediction technologies that looks like something out of Google, not John Deere, describes a group of technologies designed to collect and analyze detailed information about growing and crop conditions that feed complex models designed to provide actionable recommendations to improve yields and reduce costs. A complex problem that combines sensor technology, data collection, crop modeling and predictive analytics, the computational elements of precision agriculture are ideal for cloud deployment. I explain why in this column.
The field has already attracted the attention of big companies like IBM, which has researchers working on agricultural weather forecasts, models and simulations to improve farm decisions, and Accenture, along with a host of startups as profiled in this Forbes column. Yet farming is a hands-on activity and many of the measurements that feed precision agriculture models require instruments and implementation expertise that small farmers don’t possess.
Details here, but although it’s still relatively small, one estimate shows the precision agriculture market growing at over 13% per year hitting $3.7 billion by 2018, with the rate in emerging markets expected to exceed 25%. According to an investment bank report on precision agriculture, “The entire industry is realizing that a key value driver in the development of precision agriculture is data — collecting it, analyzing it, and using it.” Although data collection will remain a local problem, shared cloud services can accelerate the analysis and lower the barriers to farmers needing actionable intelligence. Precision agriculture will be an interesting field to monitor for both technological advancements and investment opportunities.
Originally published at www.forbes.com on August 25, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
See all (132)
1 
1 clap
1 
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@w12.io/googles-cloud-services-finally-foray-into-blockchain-d3f6319a7381?source=search_post---------29,"Sign in
There are currently no responses for this story.
Be the first to respond.
W12
Jul 26, 2018·2 min read
Google has been among the last major cloud vendors to hold out and not provide a blockchain offering. However, the technology giant has recently partnered with a provider of distributed ledger solutions to enhance the Google Cloud Platform through blockchain technology.
Recently, Sergey Brin, co-founder of Google and current president of parent-company Alphabet, notedthat the tech giant had probably failed to be on the forefront of blockchain technology. He stated:
We probably already failed to be on the bleeding edge, I’ll be honest.
Looking at Google’s competition, Brin’s words seem fairly substantiated.
Earlier this year, Amazon Web Services introduced the so-called “AWS Blockchain Templates” — designated to afford users the ability to create and launch their very own blockchain networks based on Ethereum and Hyperledger. Microsoft also revealed an ambitious blockchain strategy to become a leading provider of blockchain enterprise services and even launched Coco — a blockchain framework specifically designed for enterprise systems.
Google, on the other hand, has thus far failed to position itself as a major player in the field. Nevertheless, Brin took the chance to praise the technology, referring to it and its potential as “extraordinary.”
On Monday, July 23, Google announced a partnership with Digital Asset — a startup company supplying tools for building blockchain-based applications.
Leonard Law, Head of Financial Services Platform at Google Cloud said:
We are delighted to innovate with Digital Asset in the distributed ledger space. DLT has great potential to benefit customers not just in the financial services industry, but across many industries, and we’re excited to bring these developer tools to Google Cloud.
According to the announcement, Google Cloud also joined the private beta of the startup’s developer program — which would grant Google developers the software kit for the Digital Asset Modeling Language in order to explore potential blockchain applications with their clients.
The news follows an announced partnership between Google Cloud Platform and BlockApps — another blockchain startup which facilitates the creation of decentralized apps (DApps).
Despite being admittedly late, Google seems to be making major strides towards adopting blockchain-based technologies — potentially putting the tech giant in a position to compete with Amazon’s AWS and Microsoft’s Azure cloud services.
Source
crowdfunding on blockchain | W12.io
20 
1
20 
20 
1
crowdfunding on blockchain | W12.io
"
https://medium.com/@knoldus/informatica-intelligent-cloud-services-application-integration-introduction-cfbfcb54617?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Knoldus Inc.
Aug 21, 2020·3 min read
In this article we will be talking about Informatica Intelligent Cloud Services Application Integration. IICS is the cloud-based data integration platform that provides CDI(cloud data integration), CAI(cloud application integration) and API management b/w cloud and on-premise applications.
Informatica® Cloud Application Integration (CAI) service offers a single, trusted solution to support any integration pattern, data set, user-type or endpoint to automate business processes, expedite transactions and enable real-time analytics
As IICS is a platform as a service so we don’t need to buy software licenses and servers to install the software but pay for usage of the service and directly start with building application integration.
As we know enterprises are rapidly expanding the application and data to multiple cloud deployments. If we take an example of normal comparison flow, it will traverse through multiple workflows and applications which can be on-premise or on-cloud, for successful completion of it, there should be tight integrity between on-premise, on-cloud applications and databases. Where many enterprises face many issues with the integration technologies i.e expensive,hard to maintain and latency issues etc. These issues can be mitigated by using a single integration platform like Informatica.
It is easy to use drag-and-drop and point-and-click web interface which we use for creating processes(API). A process broadly divided into three parts start step, business logic part and end step. We can have only one start step and many end steps in one process.
With the designer, we can leverage the powerful features like parallel processing, error handling, and decision making, etc.
It provides monitoring and management for CAI. we can view and configure the settings of process server or cloud server but it will depend on our roles and permissions.
After creating a process, connection or service connector, we can deploy the asserts either on cloud server or secure agent. Usually, we deploy our assert on a cloud server if the asset is not using any on-premise repository otherwise we will deploy on a secure agent.
i.e — If we want to use JDBC/ODBC connections in our assert then we will deploy the assert on a secure agent.
Process Developer is an on-premise application for BPA(business process automation) and use an Eclipse-based tool for creating processes which we can deploy anywhere i.e. on-cloud or on-premise.
As we can use Process Designer or Process Developer for creating the processes. Informatica recommends Process Designer to use.
That’s all for this blog, we will cover assets in our next blog till then stay tuned. If you have any more queries or want to know more about it you can add the comment. I am happy to answer them.
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
1 
1 
1 
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
"
https://medium.com/google-cloud/twigcp-7f8e3b186c9a?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
Feb 25th, 2019
Welcome to the weekly Google Cloud Platform newsletter, served every Monday from medium.com/google-cloud/weekly !
Here are the main stories for this past week :
“Cloud Services Platform — bringing hybrid cloud to you” (Google blog). CSP is now open to everyone in beta. If you’re currently using Kubernetes or running a hybrid of on-prem and in-the-cloud, you now have a powerful tool to modernize existing apps at your own pace.
“Jib 1.0.0 is GA — building Java Docker images has never been easier” (Google blog). Jib offer an idiomatic experience in building docker images without installing docker or even writing a Dockerfile.
“Google announces intent to acquire Alooma to simplify cloud migration” (Google blog). Alooma are data migration and integration experts. More support when migrating to the cloud and building data pipelines.
“Google Cloud Certified Grand Challenge” (qwiklabs.com). Get certified by March 31th and win a free ticket to Next ‘19
“Analyze this — expanding the power of your API data with new Apigee analytics features” (Google blog). Which customers are using my APIs? How do I categorize my customers? Should I monetize my APIs? Apigee has the answers.
“Announcing Knative v0.4 Release” (medium.com). Now with additional protocol support — HTTP2 and gRPC, and the ability to upgrade an inbound HTTP connection for WebSocket.
“Making AI-powered speech more accessible — now with more options, lower prices, and new languages and voices” (Google blog). Multi-channel speech-to-text recognition, more voices (roughly doubled), more languages in more countries (up 50+%), and at lower prices (by up to half in some cases).
From the “if you’re a frequent reader of this newsletter you really should attend Next ‘19” department :
From the “have questions? BigQuery probably has the answers” department
From the “Cloud Native and Knative” department :
From still my favorite “Customers and partners talk best about GCP” department :
From the “IoT-to-Cloud, step by step” department :
From the “have you checked the Google Cloud publication on Medium?” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
50 
50 claps
50 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@krishnan/yes-you-are-overpaying-for-cloud-services-b898ab403c54?source=search_post---------32,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krish
Nov 1, 2019·4 min read
Ever since cloud computing came into prominence since late 2000s, there is more focus on the cost savings in the industry and customer conversations. Cloud costs is a much more complex discussion than a simple statement on cost savings. We have heard tons of stories from organizations savings money on the cloud but we have also heard stories about burgeoning costs and move back to On-Premises data centers. In this post, we want to tackle cloud costs and help users consider the nuances are in play.
There are few things about cloud costs that are obvious and one cannot argue against it.
Most organizations are overpaying the cloud providers than what they should actually be paying. This is due to various factors such as
Let us now discuss these two factors little deeper
Predictive Analytics and automation will play a critical role in optimizing cloud costs. By tapping into predictive analytics and automation, users can tap into Spot Instances, Reserved Instances and On-Demand Instances to save a considerable amount of money while also meeting the SLA needs of enterprises. The same thing can be done about cloud storage too. Predictive analytics and automation can store data in the low-cost storage disks while not in use and match higher priced disks based on performance needs. Even container workloads can be run on elastic infrastructure that taps into spot instances. For example, SpotInst Elastigroup is a platform to use all types of instances and save unto 90% on the cloud costs. You can do it for containers using SpotInst Ocean (equivalent to AWS Fargate). Similarly, Qubole offers savings for big data and machine learning workloads using spot instances
To rein in cloud waste, either in the form of runaway instances or unused or under-used instances, it is critical to have a good management and monitoring platform to cut down on the resource waste. Whether it is resource shutdown or right sizing, predictive analytics and automation can help organizations save on cloud costs. Cloud Governance platform like CoreStack or VMware’s CloudHealth are good examples for platforms that can help cut down on cloud waste. Univa NavOps offers similar cost optimization tools for HPC workloads.
Whether or not you accept it, most organizations are overspending on cloud. It is time to wake up and realize that you can have considerable savings in cloud but you need to have a right strategy and use the right set of tools to save costs without incurring additional overhead. Keep in mind that you are not obligated to improve the cloud providers’ margins but you need to save costs for your organization by not overpaying for the cloud services.
Originally posted on StackSense.io
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
2 
2 
2 
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
"
https://medium.com/@alibabatech/alibaba-cloud-launches-cloud-box-extending-public-cloud-services-to-local-devices-8e9fbf929a2b?source=search_post---------33,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Oct 5, 2020·3 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Alibaba Cloud ECS
On September 18, at the Apsara Conference 2020 held in Hangzhou, Alibaba Cloud announced the launch of Cloud Box. Based on the proprietary X-Dragon architecture, Cloud Box integrates the cutting-edge technologies of computing, storage, and networking to provide users with a fully managed cloud service that combines software with hardware for local deployment. For users who need to deploy their business in local data centers, Cloud Box provides the same experience as Alibaba Cloud’s public cloud.
Migration to the cloud has become a trend in various industries, especially with the rapid development of big data, artificial intelligence (AI), Internet of Things (IoT), 5G, and other technologies. Most companies regard digital transformation as a priority strategy. However, due to the requirements of compliance, bandwidth, latency, and cost, some enterprises are deploying some of their business in local data centers, but they also want to obtain the same benefits of low costs, elasticity, and agility as provided by the public cloud.
Cloud Box was created to solve this problem. Based on the X-Dragon architecture, Cloud Box provides users with the same cloud product experience as provided by the public cloud. At the same time, through the Virtual Private Cloud (VPC), Cloud Box facilitates the integration of applications deployed on the premises with other local applications or cloud services. Compared with the on-premises data centers, features of Cloud Box, such as on-demand ordering and pay-as-you-go billing, are provided to help users save the one-time investment of offline user-owned IDCs and avoid the cost of idle capacity caused by low device utilization.
The X-Dragon architecture is a hardware-software integrated computing architecture developed by Alibaba Cloud. This architecture helps users obtain computing capabilities superior to those of traditional physical machines. Based on the “X-Dragon architecture”, Cloud Box provides users with an elastic scaling service in on-premises data centers, high-performance computing capabilities, and excellent I/O performance. Meanwhile, by using Alibaba Cloud’s proprietary network devices, users can connect cloud box to local IT facilities to meet ultra-low latency network demands.
Cloud Box provides all types of elastic computing instances, databases, security, container services, and other products. Cloud Box ensures that instance version upgrades, security vulnerabilities, and patch upgrades are synchronized with the public cloud. Users can immediately have access to the latest products and features available on the public cloud.
Security compliance is one of the important reasons for enterprises to deploy their business in local data centers. Users can deploy Cloud Box in local data centers and store their data locally. The enterprise-level network isolation is achieved through the seamless interconnection between the VPC and the public cloud. Therefore, applications can be accessed over an internal network, which meets the regulatory requirements of customers from special industries.
Based on the capabilities of Alibaba Cloud’s public cloud, Cloud Box provides a fully managed cloud service that integrates software and hardware deployment locally. Users can enjoy the same stable experience and SLA services provided by the public cloud without the need for their own maintenance.
Enterprises have changed their computing requirements from “full scenario coverage” to “ubiquitous location”. Alibaba Cloud offers full-scenario coverage to cover “cloud, edge, and end” computing and has launched a series of new product deployment and operation forms. In the future, Alibaba Cloud will extend the public cloud to a broader space and work with its customers to get ready for the upcoming edge computing and 5G era.
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
8 
8 
8 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@drb/spotify-and-the-cloud-services-tipping-point-724e4ca225a7?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Barnes
Feb 24, 2016·1 min read
Spotify and the cloud services tipping point:
Operating our own data-centers may be a pain, but the core cloud services were not at a level of quality, performance and cost that would make cloud a significantly better option for Spotify in the long run. As they say: better the devil you know…
Recently that balance has shifted. The storage, compute and network services available from cloud providers are as high quality, high performance and low cost as what the traditional approach provides. This makes the move to the cloud a no-brainer for us. Google, in our experience, has an edge here, but it’s a competitive space and we expect the big players to be battling it out for the foreseeable future.
Source: Spotify.com.
It turns out my (former) employer did not share my opinions
2 
2 
2 
It turns out my (former) employer did not share my opinions
"
https://medium.com/@HOSTINGdotcom/colocation-versus-cloud-services-the-pros-and-cons-b400623c3ae?source=search_post---------37,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Oct 7, 2015·3 min read
Here at HOSTING, our cloud solution architects field dozens of phone calls from IT departments that are being pressured to deliver more services in less time. With lean teams and limited resources, they are challenged to find ways to move out of their aging infrastructures and consider alternatives such as colocation and cloud services. However, IT leaders are often uncertain as to which option best fits their needs. While they offer similar benefits, the choice of colocation versus cloud services should be based an organization’s specific technology and business requirements. HOSTING offers the pros and cons of each.
With colocation, organizations own, use and maintain their own equipment. However, they rent space in a colocation facility, sharing the cost of power, cooling, communications and data center floor space with other companies. Colocation is a good option for organizations that want to maintain complete control over their equipment. Companies that must adhere to specific data protection or compliance requirements (i.e., HIPAA, PCI and SOX) opt for a colocation environment.
Colocation is also a viable option for organizations that need to expand the capabilities of their existing data center. Rather than build a new data center which can cost thousands of dollars per square foot, organizations can augment their current data center with a colocation environment.
Finally, some organizations leverage a colocation site as a failover site for disaster recovery, avoiding the time and expense of building a separate data center.
There are two things to keep in mind with colocation:
Check out our blog post, 8 Criteria for Selecting a Colocation Provider, for more insights.
Cloud services are similar to colocation in that organizations realize cost savings through the use of a shared facility. However, with cloud services, the cloud service provider (CSP)supplies and manages the customer’s full hardware infrastructure, including servers, storage and network elements. The CSP’s staff, is also responsible for day-to-day administration of their customers’ cloud environments, including routine maintenance, troubleshooting and issue resolution. Companies that opt for cloud services often eliminate their capital expenses (CapEx) and reduce their operational expenses (OpEx).
One of the key benefits of cloud services is the ability to “outsource” infrastructure management responsibilities to a CSP, freeing up valuable time for their internal IT teams to focus on more strategic, revenue-generating activities.
Another benefit of cloud services is the flexibility to rapidly scale capacity up or down based on an business needs. Finally, many CSP’s offer a “pay-as-you-go” model, ensuring that customers pay for only the resources they use.
As we’ve mentioned before, there are literally thousands of companies that bill themselves as cloud service providers. So it’s imperative that organizations in the market for a CSP vet potential providers carefully (download our complimentary white paper, 20 Questions to Ask When Selecting a Cloud Service Provider to get started.) While by no means exhaustive, following are three criteria to consider when selecting a CSP.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
2 
2 
2 
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/startup-daily/oracle-cloud-services-provider-primeq-to-develop-cloud-campus-in-south-australia-ba89fe309435?source=search_post---------38,"There are currently no responses for this story.
Be the first to respond.
IT software company PrimeQ has announced plans to build a new ‘cloud campus’ in South Australia, comprising an Innovation Centre and Technology Support Hub, which it expects will create 72 new jobs in the state over the next three years.
The development is supported by a $500,000 grant from the South Australian government through its Investment Attraction South Australia arm, for a total value of $2.24 million.
Andrew McAdams, CEO of PrimeQ, said the company’s projects will look to “spark major growth” in cloud services across Australia and New Zealand.
“PrimeQ’s Innovation Centre will build intellectual property capability within this state, placing South Australia leagues ahead in the innovation layer of cloud technology,” he said.
Its Technology Support Hub, meanwhile, will look to keep its professional support services in the state rather than sending them offshore, McAdams said.
The company will also establish a graduate program, working with the state’s universities and TAFEs to build expertise in cloud services.
Founded last year, PrimeQ is focused solely on Oracle Cloud services and solutions. It has already grown to 86 employees across Australia and New Zealand, with the equivalent of 27 full time staff in Adelaide.
Martin Hamilton-Smith, Minister for Investment and Trade, commended PrimeQ on its desire to keep growing in South Australia and said the new jobs created by the company will require highly technical and in-demand skills.
“Its IT graduates program is particularly exciting, as it represents a rare pathway for students to advance to senior ranks in highly skilled careers here in Adelaide. A third of the new 72 jobs will go to IT graduates,” he said.
“The skills and technical capabilities which will be created and fostered under the PrimeQ expansion represent high transformational value and potential to South Australia.”
McAdams added, “We are passionate about creating and retaining high-value technical jobs for South Australians. We partnered with IASA because we want to see jobs stay in South Australia. I want my own children to have technology career opportunities here in South Australia.”
PrimeQ earlier this year revealed its plans for an ASX listing; according to the AFR, it will look to raise up to $10 million at a valuation of $50 million next March, pricing shares at 50 cents.
The announcement comes a few days after the South Australian government appointed US entrepreneur Tom Hadju to the role of the state’s first Chief Advisor on Innovation.
South Australian Premier Jay Weatherill said Hadju will work with the government to find and introduce new ways to grow the local digital economy.
“Global innovators like Dr Hajdu will continue to support the state government’s work around innovation and ensure we take advantage of major growth opportunities in the digital economy and create the high-tech jobs of tomorrow,” he said.
“Part of his job will be to disrupt our innovation system, to introduce new ideas and new ways of thinking and utilise his international connections to grow our digital economy.”
Image: Andrew McAdams. Source: LinkedIn.
news and insights from the Aussie startup ecosystem.
1 
1 clap
1 
news and insights from the Aussie startup ecosystem.
Written by
Founder of tech website Startup Daily.
news and insights from the Aussie startup ecosystem.
"
https://medium.com/@aldeamartinez/apple-cloud-services-generating-more-revenue-than-hardware-d09e637c6cf0?source=search_post---------39,"Sign in
There are currently no responses for this story.
Be the first to respond.
Billy D. Aldea-Martinez
May 2, 2016·2 min read
Apple generates more revenue from services than it does from Hardware Products.
Income from services like iTunes (Music Downloads, Radio, Movies), the App Store, other internet services (iCloud, Apple Pay), and licensing (Lightning connector, Thunderbolt Interface; co-developed by Apple & Intel) has increased 20 percent year-over year.
The App Store alone has increased 35 percent in revenue year-over-year. “Apple Pay is growing at a tremendous rate,” admitted Apple CEO Tim Cook during the company’s earnings call, noting that there are 1 million new users per week.”
This means that means Apple is now benefiting from a steady stream of revenue from existing installed base of devices that are accessing its cloud based services that the company controls.
But after 13 years of steady growth, Apple’s revenue fell in the first quarter and is projected to do so again in the current period. One theory is that it appears that many customers are electing to skip an upgrade to this year’s iPhone 6s.
However, Apple is still massively profitable, pays a dividend, has huge cash reserves, and built-in consumers for future upgrades.
Apple posted quarterly revenue of US$50.6 billion and quarterly net income of US$10.5 billion. Gross margin was 39.4 percent compared to 40.8 percent in the year-ago quarter. International sales accounted for 67 percent of the quarter’s revenue.
For more: The Verge, CNBC, WSJ, Apple Inc
Regional Director, @piano_io, Enterprise AI SaaS | Start-up Board Director | DJ & Record Label Producer.
1 
1 
1 
Regional Director, @piano_io, Enterprise AI SaaS | Start-up Board Director | DJ & Record Label Producer.
"
https://medium.com/@TechJobs_NYC/amazons-profits-grow-more-than-800-percent-lifted-by-cloud-services-82b393ece68a?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jul 29, 2016·1 min read
Amazon’s Profits Grow More Than 800 Percent, Lifted by Cloud Services By NICK WINGFIELD Amazon reported net income of $857 million, the second quarter in a row it has shown a record profit even as it invests heavily in its own growth. Published: July 28, 2016 at 06:00PM via NYT Technology http://www.nytimes.com/2016/07/29/technology/amazon-earnings-profit.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
1 
1 
1 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@jaychapel/7-ways-cloud-services-pricing-is-confusing-1302b9eab7f2?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 7, 2018·7 min read
Beware the sticker shock — cloud services pricing is nothing close to simple, especially as you come to terms with the dollar amount on your monthly cloud bill. While cloud service providers like AWS, Azure, and Google were meant to provide compute resources to save enterprises money on their infrastructure, cloud services pricing is complicated, messy, and difficult to understand. Here are 7 ways that cloud providers obscure pricing on your monthly bill:
For the purpose of this post, we’ll focus on the three biggest cloud service providers: AWS, Azure, and Google. Between these three cloud providers alone, different analogies are used for just about every component of services offered.
For example, when you think of a virtual machine (VM), that’s what AWS calls an “instance,” Azure calls a “virtual machine,” and Google calls a “virtual machine instance.” If you have a group of these different machines, or instances, in Amazon and Google they’re called “auto-scaling” groups, whereas in Azure they’re called “scale sets.” There’s also different terminology for their pricing models. AWS offers on-demand instances, Azure calls it “pay as you go,” and Google refers to it as “sustained use.” You’ve also got “reserved instances” in AWS, “reserved VM instances” in Azure, and “committed use” in Google. And you have spot instances in AWS, which are the same as low-priority VMs in Azure, and preemptible instances in Google.
Operating systems, compute, network, memory, and disk space are all different factors that go into the pricing and sizing of these instances. Each of these virtual machine instances also have different categories: general purpose, compute optimized, memory optimized, disk optimized and other various types. Then, within each of these different instance types, there are different families. In AWS, the cheapest and smallest instances are in the “t2” family, in Azure they’re called the “A” family. On top of that, there are different generations within each of those families, so in AWS there’s t2, t3, m2, m3, m4, and within each of those processor families, different sizes (small, medium, large, and extra large). So there’s lots of different options available. Oh, and lots confusion, too.
If you aren’t familiar with AWS, Azure, or Google Cloud’s consoles or dashboards, it can be hard to find what you’re looking for. To find specific features, you really need to dig in, but event just trying to figure out the basics of how much you’re currently spending, and predicting how much you will be spending — all can be very hard to understand. You can go with the option of building your own dashboard by pulling in from their APIs, but that takes a lot of upfront effort, or you can purchase an external tool to manage overall cost and spending.
Cloud services pricing can charge on a per-hour, per-minute, or per-second basis. If you’re used to the on-prem model where you just deploy things and leave them running 24/7, then you may not be used to this kind of pricing model. But when you move to the cloud’s on-demand pricing models, everything is based on the amount of time you use it.
When you’re charged per hour, it might seem like 6 cents per hour is not that much, but after running instances for 730 hours in a month, it turns out to be a lot of money. This leads to another sub-point: the bill you get at the end of the month doesn’t come until 5 days after the month ends, and it’s not until that point that you get to see what you’ve used. As you’re using instances (or VMs) during the time you need them, you don’t really think about turning them off or even losing servers. We’ve had customers who have servers in different regions, or on different accounts that don’t get checked regularly, and they didn’t even realize they’ve been running all this time, charging up bill after bill.
You might also be overprovisioning or oversizing resources — for example, provisioning multiple extra large instances thinking you might need them someday or use them down the line. If you’re used to that, and overprovisioning everything by twice as much as you need, it can really come back to bite you when you go look at the bill and you’ve been running resources without utilizing them, but are still getting charged for them — constantly.
Cloud services pricing has changed quite often. So far, they have been trending downward, so things have been getting cheaper over time due to factors like competition and increased utilization of data centers in their space. However, don’t jump to conclude that price changes will never go up.
Frequent price changes make it hard to map out usage and costs over time. Amazon has already made changes to their price more than 60 times since they’ve been around, making it hard for users to plan a long-term approach. Also for some of these instances, if you have them deployed for a long time, the prices of instances don’t display in a way that is easy to track, so you may not even realize that there’s been a price change if you’ve been running the same instances on a consistent basis.
In AWS, there are some cost savings measures available for shutting things down on a schedule, but in order to run them you need to be familiar with Amazon’s internal tools like Lambda and RDS. If you’re not already familiar, it may be difficult to actually implement this just for the sake of getting things to turn off on a schedule.
One of the other things you can use in AWS is Reserved Instances, or with Azure you can pay upfront for a full year or two years. The problem: you need to plan ahead for the next 12 to 24 months and know exactly what you’re going to use over that time, which sort of goes against the nature of cloud as a dynamic environment where you can just use what you need. Not to mention, going back to point #2, the obscure terminology for spot instances, reserved instances, and what the different sizes are.
Cloud services pricing shifts between IaaS (infrastructure as a service), which uses VMs that are billed one way, and PaaS (platform as a service) gets billed another way. Different mechanisms for billing can be very confusing as you start expanding into different services that cloud providers offer.
As an example, the Lambda functions in AWS are charged based on the number of requests for your functions, the duration, and the time it takes for your code to execute. The Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month, or you can get 1M request free and $0.20 per 1M requests thereafter, OR use “duration” tier and get 400,000 GB-seconds per month free, $0.00001667 for every GB-second used thereafter — simple, right? Not so much.
Another example comes from the databases you can run in Azure. Databases can run as a single server or can be priced by elastic pools, each with different tables based on the type of database, then priced by storage, number of databases, etc.
With Google Kubernetes clusters, you’re getting charged per node in the cluster, and each node is charged based on size. Nodes are auto-scaled, so price will go up and down based on the amount that you need. Once again, there’s no easy way of knowing how much you use or how much you need, making it hard to plan ahead.
Ultimately, cloud service offerings are there to help enterprises save money on their infrastructures, and they’re great options IF you know how to use them. To optimize your cloud environment and save money on costs, we have a few suggestions:
Cloud services pricing is tricky, complicated, and hard to understand. Don’t let this confusion affect your monthly cloud bill. Try ParkMyCloud for an automated solution to cost control.
Originally published at www.parkmycloud.com on February 6, 2018.
CEO of ParkMyCloud
1 
1 
1 
CEO of ParkMyCloud
"
https://medium.com/@TechJobs_NYC/microsoft-to-donate-1-billion-in-cloud-services-to-nonprofits-and-researchers-989f5fec8d15?source=search_post---------42,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jan 20, 2016·1 min read
Microsoft to Donate $1 Billion in Cloud Services to Nonprofits and Researchers By NICK WINGFIELD Microsoft hopes to give away its cloud services to 70,000 nonprofits in the next three years. Published: January 18, 2016 at 07:00PM via NYT Technology http://bits.blogs.nytimes.com/2016/01/19/microsoft-to-donate-1-billion-in-cloud-services-to-nonprofits-and-researchers/?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
1 
1 
1 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@marknca/keeping-your-sanity-securing-iaas-paas-and-saas-cloud-services-4b7467f24631?source=search_post---------47,"Sign in
There are currently no responses for this story.
Be the first to respond.
marknca
Mar 2, 2016·7 min read
In most organizations today, cloud services are a fact of life. Whether you’re deploying and managing servers in the cloud, building on top of a globally distributed platform, or consuming constantly updated services, the cloud is a fundamental part of your IT service delivery…whether you know it or not.
And why wouldn’t you move to the cloud? The business advantages are clear. You can great reduce the time to deploy new services, reduce your operational burden and costs, and rapidly iterate on new ideas.
There are security advantages as well.
It may require a cultural shift in your organization to accept extending trust to your cloud service providers (CSP), that trust is well place. Top tier CSPs understand that they live and die on their reputation. It’s in their best interests to deliver a secure service to you.
But that’s not to say that you don’t have responsibilities for security when using cloud service. All cloud services (regardless of SPI model; IaaS, PaaS, or SaaS) use this simple model.
Of the main areas of security, the CSP is always responsible for;
Depending on the service, you may be responsible for securing the;
And you are always responsible for;
Put these areas together across all three SPI methods and you get figure 1, “Shared Responsibility Model”.
Looking at cloud security in this manner brings clarity. You can take each type of service (IaaS, PaaS, SaaS) and apply reasonable security controls in order to fulfill your day-to-day responsibilities
It’s important to note that we’re talking about day-to-day responsibilities here. You’re always responsible for the security of your deployments. However you delegate some of the day-to-day work to your CSP. In these cases, you have to trust but verify the work your CSP is doing.
When dealing with IaaS, most of the controls you are used to from the datacenter are still applicable. They’re just delivered in a different manner in order to optimize for the attributes of a cloud environment.
You see this with controls like intrusion prevent and filtering. Traditionally gateway controls, it is now much more effective to deploy them directly on an instance or virtual machine. This maintains the scalability and flexibility of the cloud without sacrificing security.
Platform deployments can be tricky to secure because of how intertwined your application is with the platform itself. This is a service type where secure design, a strong understanding of the CSP’s role, and programmable security controls are critical to a successful, secure deployment.
Securing software delivered as a service is typically accomplished using a combination of a CASB (cloud access service broker) and configuring the native service controls in order to meet your security needs.
While the plan for securing each service type is clear, the pace of change in this space is a major challenge.
Cloud services (of all types) are readily available. It’s never been easier to stand up a new application or service.
This rapid pace of innovation is a huge boon to business. IT is finally a consistent enabler within the organizations.
The challenge is for security to keep pace.
Innovation in at an all time high in the security space but even with current levels of investment and effort, it’s difficult for security controls to keep pace with the new services being developed.
This rapid pace of change is leading to more and more security solutions being required to properly secure the vast number of services that each organization is using.
The average organization uses a lot of services. Ok, I’m sure there’s an actual number but it’s hard to nail down. Depending on the source, the average is somewhere between 5 and 700. So let’s settle on “lots”.
Solid guidance exists on how best to secure each of these services according to your needs. The challenge is stitching the security of each of these services together into a cohesive whole.
The industry (lead by organizations like the Cloud Security Alliance, of which Trend Micro is a member) is working towards a common goal to help address this challenge.
The goal is to be able to provide tools that can organizations can get to easily work together (regardless of vendor) in order to provide a comprehensive security solution around cloud services.
The strategic vision and guidance is already in place with the Cloud Control Matrix (the CCM, a living document currently at version 3.0.1). This document lays out the types of controls that should be applied to various cloud services.
In addition to the CCM, there are a number of efforts in place to help organizations combine the right tools for their security needs. The Cloud Security Open API shows a lot of promise in helping make this a reality.
Separate from these efforts are the individual roadmaps for each cloud security tool. This is a very active and innovate space (yes, I realize I have a bit of bias here but just look around at the number of cybersecurity startups and established companies efforts and I think you’ll agree).
But each of these efforts are a medium term solution at best. What are organizations supposed to do now to address this problem?
When attempting to address this problem today there are 3 main areas where you should focus your efforts.
First you want to try and reduce the organizations overall exposure when it comes to using cloud services.
At solid first action is to attempt to inventory the number and type of services currently in use. To do this you should enlist a combination of technology and old fashioned methods (a/k/a asking teams what they are using).
With a better idea of what you’re attempting to secure, you can then start working with the teams throughout the organization to ensure that they are aware of the risks and security challenges associated with the services they use.
An ongoing discussion and education campaign is a pillar of the good security practice and critical to address the issue of multi-service use.
These discussions will also help inform your internal security policies. A strong, realistic policy will help establish a baseline for all stakeholders. It lays out what the norms for your organization and acts as a standard to compare against for any new business initiatives.
Above all, the responsiveness of your internal IT services is instrumental in reducing your overall exposure. Many teams don’t want to go against policy or organizational standards but don’t have a choice when internal service delivery is unresponsive.
As exposure is inventoried and scaled back (hopefully), your next step should be to implement a robust monitoring practice.
This will require a lot of initial work with an ongoing effort.
The variety of services and security controls applied to those services creates a unique challenge for each organization.
In general, you want to start with the lowest common denominator for monitoring (access logs, basic API access, network traffic, etc.). Where possible these should be tied to business metrics and risk.
For example, knowing that a business unit’s use of a cloud storage service is increasing week over week is a good monitoring metric (GB used) tied to a business risk (the exposure of that data on a 3rd party service).
Due to the nature of the problem, you best approach is a lot of spit, glue, and hope. This step requires a lot of manual effort but is crucial to being able to answer the deceptively simple questions, “where is the organization’s data stored and what’s it exposure?”.
With time, your monitoring practice will mature and you’ll grow to have a better understanding of your business requirements.
The lessons you learn should be applied to selecting cloud services that align with your business needs as well as your security strategy and tactics.
The organization should select services that allow you to easily get data in and out, provide support for standard APIs (or at least logical and well supported APIS), and have a strong reputation for services and security.
Choosing a provider based on these attributes will go a long way to ensuring that you have a consistent approach to onboarding new cloud services.
Building a coherent security practice for organizations using multiple cloud services is a challenge today and will continue to be a challenge for the foreseeable future.
The most efficient way to address this challenge is to focus on;
These three areas create a solid foundation for your security practice.
This will allow you to adapt and grow as the strategy for cloud security evolves, as more and more services support standard APIs, and as security technologies continue to provide innovate solutions that better address the new reality of modern IT service delivery.
This essay was built on a talk I presented at the CSA Summit during RSA 2016, “Defending The Whole. Iaas, PaaS, and SaaS”. It was originally posted in 2 parts on the Trend Micro blog (part 1, part 2). The slides are available on SlideShare.
For some additional thoughts and perspective on my talk, check out this piece by Rob Wright for TechTarget’s SearchCloudSecurity site.
Mark is a seasoned infromation security professional currently focused on researching & teaching cloud security and usable security systems at scale. Catch him on Twitter or at his site, markn.ca.
☁️🔬 Cloud Strategist @LaceworkInc . @awscloud Community Hero. Builder. Working to make security easier for everyone. Opinionated but always looking to learn
See all (2,068)
☁️🔬 Cloud Strategist @LaceworkInc . @awscloud Community Hero. Builder. Working to make security easier for everyone. Opinionated but always looking to learn
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adriennedomingus/using-connection-strings-to-connect-to-cloud-services-82a3d40112a3?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adrienne Domingus
Jul 29, 2020·2 min read
Unless a website is static, it almost certainly connects to other cloud services such as databases or caches that are hosted on servers other than the ones on which main application is running. Connection strings are used to identify where to find the server running the service, what type of driver to use (Postgres, Redis, AMQP, etc.), and what credentials to access it with.
"
https://blog.inovia.vc/the-evolution-of-cloud-services-edf025220d24?source=search_post---------49,"As investors we are fortunate to meet with hundreds of companies. One of the first things, whether right or wrong, we do is to immediately try and put the company in a “bucket” in terms of what they are doing. This helps in identifying the market, competitors and better understand the value proposition. I love watching Paul Graham hosting his “Office Hours” live at TC Disrupt as you can watch him struggle to do this in real-time with the pitching companies.
When we first met Nicolas and Daniel of AppDirect in 2010 we immediately understood what they were doing and why it was important, but we struggled to put the company in a bucket. Were they a SaaS company? PaaS? A marketplace? A provisioning service? We debated it internally and I was recently reminded of this by a tweet from Box’s CEO, Aaron Levie:
Actually, it didn’t matter if we couldn’t ‘bucket’ the company. It was very apparent that Nicolas and Daniel had very strong convictions about cloud-based software and a vision as to how they would evolve and where the market was going. They saw a future demand for a platform to sit between cloud-based services and customers. This platform would provide app discovery, administration abilities, unified billing and employee provisioning for the customer. They also saw the demand across numerous industries to aggregate and sell cloud-based services, or any digital content for that matter.
Nicolas and Dan spent a tremendous amount of time researching, talking to potential customers and envisioning how cloud-based services would be delivered and consumed. They live and breathe it. When I first met them they came across as passionate evangelists and I admit that I was quickly a convert. Others obviously were as well. They had already recruited an amazing founding team, built a great technology platform and had an anchor customer lined up. We were excited to be able to partner with them and invested in their seed round.
Fast forward a year and a half and their vision has continued to come to fruition. So much, in fact, that the vision Nicolas and Daniel had of the market is now referred to by Gartner as Cloud Service Brokerage. A currently small market that Gartner predicts will be the largest category of growth in cloud computing by 2015 and eventually mature to a market in the hundreds of billions.
We are extremely excited with AppDirect’s amazing growth and their leadership position in the CSB market space. Nicolas and Daniel have proven to be amazing company builders and it is great to see them able to execute on their vision. It is with this that we are proud to announce our continuing support in leading their $8.5M Series A. We know that Nicolas and Daniel are just getting started and trust that you will be hearing a lot more about them down the road.
Thoughts and insights on building game-changing companies…
Written by

Thoughts and insights on building game-changing companies and enjoying the journey — from the Inovia Capital team.
Written by

Thoughts and insights on building game-changing companies and enjoying the journey — from the Inovia Capital team.
"
https://medium.com/g-cloud-latest/g-cloud-services-and-use-by-other-suppliers-1916eb71ef44?source=search_post---------50,"There are currently no responses for this story.
Be the first to respond.
Section FW-5 of the G-Cloud contract refers to http://gcloud.civilservice.gov.uk/supplier-zone/assurance for the details of the service assurance verification. This has now been archived and now redirects to https://www.gov.uk/how-to-use-cloudstore. This has a section for Assurance and this refers to the following document.
www.gov.uk
This document contains the minimum set of information that’s expected in a service definition, which includes;
Suppliers will commit to make their IaaS/PaaS G-Cloud services available for purchase by third parties who intend to supply services to government so they can offer SaaS or more traditional infrastructure / application delivery (on same or better terms) etc. This is applicable to both the public and private cloud delivery models, but probably needs to be emphasised for the private cloud.
Everything you need to know about G-Cloud
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
"
https://blog-fr.inovia.vc/the-evolution-of-cloud-services-fab9a8d6ee7d?source=search_post---------52,"Comme investisseurs, nous avons la chance de rencontrer des centaines d’entreprises. Que ça soit une bonne chose ou pas, un de nos premiers réflexes est d’essayer le plus rapidement possible de mettre l’entreprise « dans une boîte », de la catégoriser. Cela nous aide à cerner son marché et ses concurrents et à mieux comprendre sa proposition de valeur. J’adore regarder Paul Graham de YCombinator animer “Office Hours ” en direct, car on peut le voir essayer de faire cela même en temps réel avec les entreprises qui font leurs présentations.
Quand nous avons rencontré Nicolas et Daniel d’AppDirect pour la première fois en 2010, nous avons immédiatement compris ce qu’ils faisaient et pourquoi c’était important, mais nous avions de la difficulté à catégoriser leur entreprise. Était-ce une entreprise de logiciel-service ? De plateforme-service ? Une place de marché ? Un service d’approvisionnement ? Nous en avons débattu à l’interne, et j’y ai repensé récemment à cause d’un « tweet » du PDG de Box, Aaron Levie :
(traduction libre) Voici comment décrire l’infonuagique à nos mères : le logiciel-service c’est Crate and Barrel, la plateforme-service c’est IKEA et l’intégration c’est Home Depot.
En réalité, le fait que nous ne pouvions pas catégoriser l’entreprise n’avait aucune importance. Il était manifeste que Nicolas et Daniel avaient des convictions profondes au sujet des logiciels infonuagiques ainsi qu’une vision de comment l’entreprise évoluerait et de la direction que le marché allait prendre. Ils envisageaient une demande future pour une plateforme qui occupe l’espace entre les services d’infonuagique et les clients. Cette plateforme permettrait de découvrir des applications, d’administrer ses comptes, d’avoir une facturation unique et d’offrir l’approvisionnement aux employés du client. Ils ont également vu la demande, dans plusieurs industries, pour la consolidation et la vente de services d’infonuagique et même pour tout contenu numérique.
Nicolas et Daniel ont passé énormément de temps à effectuer de la recherche, à parler à des clients potentiels et à réfléchir à la façon dont seraient livrés et consommés leurs services infonuagiques. Ils sont passionnés de ce domaine jusqu’à en parler jour et nuit. Quand je les ai d’abord rencontrés, ils étaient clairement des évangélistes qui prêchaient leur vision, et j’avoue qu’ils m’ont rapidement converti. D’autres l’ont manifestement été aussi. Ils avaient déjà recruté une fantastique équipe de fondateurs, bâti une excellente plateforme technologique et avaient un client-clé en mire. Nous étions heureux de collaborer avec eux et avons investi dans leur étape de financement de « seed round ».
Un an et demi plus tard, leur vision continue de prendre forme. Tant, en fait, que la vision que Nicolas et Daniel avaient du marché est maintenant appelée (traduction libre) « cloud-services-brokerage ». C’est un marché actuellement petit, qui deviendra, selon Gartner, la plus importante catégorie en croissance dans l’infonuagique d’ici 2015, et deviendra un jour un marché de centaines de milliards de dollars.
Nous sommes extrêmement heureux de la croissance extraordinaire d’AppDirect et de sa position de meneuse du domaine du courtage d’infonuagique. Nicolas et Daniel ont prouvé qu’ils sont des entrepreneurs de haut calibre, et il est beau de les voir réaliser leur vision. C’est ainsi que nous sommes fiers d’annoncer que nous poursuivons notre soutien en menant l’étape de financement de série A de 8,5 millions de dollars. Nous savons que Nicolas et Daniel ne font que commencer, et que vous n’avez pas fini d’entendre parler d’eux !
Tous les blogues d’inovia en Français.
Written by

Tous les blogues d’inovia en Français. For the english version, click on [EN]
Written by

Tous les blogues d’inovia en Français. For the english version, click on [EN]
"
https://medium.com/@alibaba-cloud/alibaba-cloud-hbase-x-pack-service-a-new-standard-for-hbase-cloud-services-c133a80bec17?source=search_post---------53,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 29, 2019·4 min read
By ApsaraDB Team
On December 13, 2018, the 8th China Cloud Computing Standards and Application Conference was held in Beijing. At the conference, Alibaba Cloud HBase announced the launch of a new X-Pack service that supports the retrieval of SQL, time-series, spatial-temporal, image, and full-text, as well as the execution of complex analyses. From processing to analyzing full-stack databases, customers can use it out of the box to meet their needs for richer business processing and easier use of databases.
In addition, the new cloud HBase service also has a number of exclusive enterprise-level capabilities, including non-perceived separation of hot and cold data, hundreds of TB of backup recovery, dual-active across regions and enterprise-level security, which can solve many challenges such as customer cost and security, allowing customers to focus on business development.
Cao Long (technical director of Alibaba Cloud ApsaraDB HBase and one of the founders of China HBase technology community) and Yang Wenlong (Apache HBase Community Committer&PMC, and head of Ali-HBase kernel) jointly released the X-Pack service
Based on the new release, the capability of HBase has been expanded, enhancing the value of computation and business.
Based on KV, cloud HBase supports a variety of data models, such as time-series, spatial-temporal, image, and full-text and document models, and has built-in rich processing capabilities, thus improving the efficiency of business development by a hundredfold.
Based on the powerful online capability, HBase integrates the capabilities of stream processing, batch processing, OLAP, OLTP, high-speed object storage, and full-text retrieval. It is very suitable for the demands of complex business scenarios of enterprises and provides customers with the ability to integrate business out of the box.
Backup recovery level has increased by more than a hundredfold, with the largest in the database field. The backup and recovery capabilities of traditional databases are often insufficient to deal with NoSQL scenarios. Cloud HBase successfully achieves a backup recovery level of hundreds of TBs or higher through vertical integration of high compression, kernel-level optimization, distributed processing and other capabilities, alleviating customers’ concerns.
Alibaba Cloud takes the lead in supporting non-perceived separation of hot and cold data without affecting customers’ business, to help enterprises to easily handle cold data processing at only 1/3 of the cost without having to modify code.
Alibaba Cloud HBase provides a new cold storage medium for cold data storage scenarios. While having 1/3 the storage cost of an ultra cloud disk while maintaining equivalent write performance, it can ensure that the data can be read at any time. The use of cold storage is very simple. Users can choose cold storage as an additional storage space when purchasing cloud HBase instances, and specify cold data to be stored on the cold storage medium through table creation statements. This reduces the storage cost, obtaining low-cost storage capacity without code modifications, ultimately helping enterprises reduce the overall cost.
According to the introduction, Alibaba Group started to research and put HBase into the production environment as early as 2010. From the initial Taobao historical transaction records to Ant security risk control data storage, it has been utilized for 8 years and tested against 8 years of November 11 shopping sprees. At the same time, the largest and most professional HBase technical team in China has been built, including 4 PMCs and 7 committers, with more than 200 important features in the HBase kernel being contributed by Alibaba. The scale of the group leads the world with in excess of 10,000 units, and the scale of single cluster exceeding 1,000 units.
Since Alibaba Cloud began providing the HBase cloud service in August 2017, it has currently served thousands of large enterprise customers and thousands of online clusters, and covered dozens of industries, such as social networking, finance, governments and enterprises, Internet of Vehicles (IoV), transportation, logistics, retail and e-commerce, helping single users withstand the business pressure of tens of millions of QPS and realizing the efficient storage and processing of hundreds of PB of data.
It is reported that the service is the fastest growing database service of Alibaba Cloud and one of the cloud services with the highest proportion of large enterprise customers. On June 6, 2018, Alibaba Cloud first launched HBase 2.0 to the world, which makes it a well-deserved leader in the HBase field. The Alibaba Cloud HBase team continues to optimize services around customer business, cost, and operation & maintenance, to promote the digital transformation of the enterprise and to bring leading technologies to more customers.
Reference:https://www.alibabacloud.com/blog/alibaba-cloud-hbase-x-pack-service-a-new-standard-for-hbase-cloud-services_594406?spm=a2c41.12532169.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://smarter.glg.it/cloud-services-will-ramp-up-even-more-in-2021-e3be143401fc?source=search_post---------54,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
MJ DiBerardino, Chief Executive Officer at Cloudnexa and GLG Council Member
The COVID-19 pandemic has sent companies to the cloud. Globally, companies spent a record $34.6 billion on cloud services in Q2, a 30% year-over-year increase and up 11% from the previous quarter, The Wall Street Journal reported. Amazon’s AWS and Microsoft’s Azure lead the field. To see where the cloud business may go, GLG spoke with cloud technology expert MJ DiBerardino, CEO at Cloudnexa, a premier partner of AWS. Following are a few select excerpts from our broader discussion.
What are the top trends within infrastructure cloud providers AWS, Azure, and GCP?
The shift to work from home is significant. I’m not talking about Zoom or some kind of chat client. It’s more about enabling employees to be most effective and efficient in their current roles and duties. One of the services that saw a huge uplift across the board has been virtual desktop infrastructure (VDI), or the desktop as a service. Those types of solutions, in which workspaces are provided within the cloud, became critical for employers that had to meet specific types of compliance or work in high-security environments.
As the employee base was not able to travel to the office and be on the corporate network directly, their local devices couldn’t be controlled within the framework of corporate IT. What we saw was companies, especially within the health care industry, had major concerns around data protection, access, and processing. A viable solution was VDIs. That specifically on AWS is called WorkSpaces, and it just saw a huge lift. This trend continues because even though people have come back to the office, the value of being able to centrally manage all employees, ensure compliance, and make sure data isn’t leaving the corporate network has been proven.
Outside of that, data analytics is still hot. Another popular service that’s starting to make a lot of headway is around streaming, including live events.
What are you hearing about IT spending on cloud for 2021?
We’re expecting huge numbers in 2021. We believe next year will be one of the largest years of cloud migration and utilization in recent history. In 2020, we saw a lot of budget freezes and cutbacks. In years past, it’s been heavily migration oriented, but for those who could take advantage of the time and understand that companies are really budget constrained this year, there’s been a lot of focus on optimizations of current utilization, which is good, but migration is what really drives growth. We believe there will be a lot of pent-up demand in the first half of 2021. A lot of companies are doing extensive planning for next year on how to rearchitecture, how they’ll deploy and utilize, and take advantage of their cloud journey.
What are you seeing for AWS in terms of enterprise adoption? Have you seen growth as enterprises continue to work from home?
Absolutely. Enterprises have committed to AWS for the past several years now, and there’s still a healthy stream of projects. There was a little dip that we experienced from a growth perspective in Q2, but it’s really an anomaly. Next year I believe it will be faster paced. Everyone has recognized that they have corporate network weak points. Companies won’t necessarily shut down their data centers overnight, but there will be an acceleration of cloud adoption to ensure they’re supporting customers and employees for the next mishap. They need to adjust and create a model that works all around if a shutdown were to happen again, or just continuing to support this hybrid work-from-home use case.
Looking into 2021 and beyond, which business application platform is best positioned to benefit from the transition to the cloud and why? Microsoft Dynamics, SAP, Oracle?
I would add Snowflake as well. Snowflake can run on AWS and Azure. It’ll see one of the biggest upticks over the next few years. There’s no denying that data is growing — data needs to be warehoused, analyzed, and put to use. Traditional business apps do that well and SaaS solutions are great, but Snowflake excels with the pure infrastructure and platform as a service type of play.
How strong is Azure’s hybrid story, and is it resonating with customers? Will it continue to be a leader?
I believe in the hybrid model. It’s strong. Azure does a very good job at it without question, but compared with other ways of doing hybrid cloud, Azure’s biggest threat is VMware Cloud on AWS. VMware can be supported on any of the big cloud providers, but AWS was one of the first. It has done a good job at ensuring that the solution works well with as few interruptions as possible. Most corporations already utilize and deploy VMware internally. On the other side, Microsoft has had a great solution for many years, but it’s not necessarily an aspect that companies are looking for. We’re in this hybrid multicloud world, especially within enterprise, and it’s rare for enterprises, especially large ones, 100% committing to one stack, one solution. They are hedging and want to utilize at least two, possibly even three or four. It’s difficult to overlook the AWS VMware deployment and how that has really changed and helped enhance hybrid cloud options.
Any updates on Google Cloud Platform?
Google is definitely making great enhancements to its core services. It’s supporting more enterprise-level-grade applications and infrastructure, and that’s good news. Do I view it as a threat? Not necessarily. I still don’t see it taking a huge stronghold within enterprise cloud deployments quite yet. In time maybe, but the way that AWS and Azure are formulating their multiyear large contracts with customers, they own the space now. Google continues to be extremely strong with data analytics. That will drive growth over the next couple of years as more companies start to understand data science and how to utilize it to their advantage. Google has a head start there because it’s been doing it very well for so long now. Now that it’s made this large enterprise play, that’s where the company will start to take some share away.
With IBM spinning off its legacy IT management services business and focusing on its hybrid cloud and AI units, what competitive pressures are there for others?
IBM made a smart decision in terms of sun-setting this legacy IT management and going after more of the multicloud management approach, especially for enterprise. It’s had these contracts with enterprises for so long. From an outsource management perspective, IBM is doing the right thing. It is a top-tier partner across multiple clouds. But I don’t foresee IBM making much headway as a major cloud provider. It’ll stay more on the niche market with AI, machine learning, quantum computing, and things of that nature.
Does Oracle, coming off its TikTok deal, have a path forward in public cloud?
That’s a tough one. It’s a cool story, but really what is it? It’s a couple of large customers at the end of the day. For every large customer Oracle has, other platforms such as Amazon and Azure have many more. The reality is very rarely do I hear Oracle Cloud come up. When it does, it’s about looking to replatform off of Oracle onto maybe a native solution, AWS, Aurora, or something else. That’s not very positive news for Oracle, but a lot of customers are looking to figure out ways where they don’t need to renew their very large enterprise license agreements and save money, but still get the same performance and capabilities.
What are the most important areas to focus on as we wrap up 2020?
There will be a lot of rebuilding, restrategizing, and rearchitecting a lot of the work that has gone on in the past year, which is critical to usage and adoption growth for next year. Everyone had to move quickly to get solutions in place. When that happens, mistakes are made or corners are cut. It’s fine — the solution still holds. Companies will do a lot of self-auditing on these environments and ensure they’re properly built, and where they weren’t, they’ll take the proper remediation steps. Once that’s out of the way, 2021 will come and we should hit the ground running and see a lot more adoption than we’ve seen in the past.
One last point. Data analytics is very real. We’re finally seeing the tide turn on usage around data sciences and analytics and machine learning. It was pretty clear, especially once Snowflake went public, how positive it has been. This is a viable market, and it’ll just get bigger.
MJ DiBerardino is currently the Chief Executive Officer at Cloudnexa, an Inc. 185 company and a Premier Partner of Amazon Web Services, the leading public cloud provider. Prior to Cloudnexa, he was the Director of Cloud Services at Freedom OSS, one of the original AWS partners. Mr. DiBerardino holds the five primary AWS certifications and has been working in the cloud industry since 2007.
This article originally appeared on GLG Insights. To read more from GLG, click here.
Insights from GLG, The World’s Insight Network
Written by
Power your decisions with real-world expertise and insights.
Insights from GLG, The World’s Insight Network
Written by
Power your decisions with real-world expertise and insights.
Insights from GLG, The World’s Insight Network
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/interview-public-cloud-services-in-search-of-the-white-knight-e8401462b0ab?source=search_post---------55,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Oct 19, 2016·3 min read
Hybrid and multi-cloud strategies are near the top of the agenda for IT decision-makers. They understand that a modern, cloud-based IT world shouldn’t just be drawn in black and white. Diversity is needed to purchase services and innovations from a larger number of cloud providers. Private clouds quickly meet their limits here and don’t offer the benefits of a public cloud.
What is the optimal strategy for using public cloud services in enterprise IT? Find the answers in an interview with T-Systems in “Public cloud services: in search of the white knight”.
Mr. Buest, public, private, hybrid: when does which cloud offering become relevant for a company? There’s no catch-all answer here. We are now seeing an increasing number of companies that are intensely engaging with the public cloud, following an “all in” approach. This means, they do not manage a local IT infrastructure or internal data centers anymore, instead they are migrating everything to public cloud infrastructures or platforms, or purchasing what they need under a SaaS (Software-as-a-Service) model. However, these companies are still a minority.
…and that means? At the moment, most companies prefer to use private cloud environments. It’s a logical consequence of the legacy solutions that companies still maintain in their IT. However, we believe that in the future, a majority of German companies will move to hybrid or multi-cloud architectures, enabling them to cover all the facets they need for their digital transformation.
And how can companies coordinate these different solutions in combination? By using cloud management solutions that have interfaces to the most commonplace public cloud offers, as well as to private cloud solutions. They provide powerful tools for managing workloads in different environments and shifting virtual machines, data and applications around. Another option for seamless management is iPaaS: integration Platform as a Service (iPaaS) provides cloud-based integration solutions. In the pre-cloud era, such solutions were also called “middleware”. They provide support for the interaction between different cloud services.
What do companies have to watch out for principally when using these cloud services? They should not underestimate the lack of understanding of the public cloud, nor the challenges associated with setting up and operating multi-cloud environments. The benefits gained from using multi-cloud infrastructures, platforms and services often come at a heavy price: namely, the costs that result from the complexity, integration, management and necessary operations. Multi-cloud management and a general lack of cloud experience are currently the key challenges many companies are facing.
What is the solution? Managed public cloud providers (MPCPs) are positioning themselves as “white knights” or “friends in need”. They develop and operate the systems, applications and virtual environments for their customers — in both the public cloud infrastructures and multi-cloud environments — in a managed cloud service model.
– — –
– — – The interview with T-Systems has been published under “Public cloud services: What really matters“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@toxsltechnologies/how-managed-cloud-services-helps-to-reduce-the-cyber-security-threats-a2c85643e4bb?source=search_post---------56,"Sign in
There are currently no responses for this story.
Be the first to respond.
ToXSL Technologies
Apr 15, 2021·4 min read
The security threats are evolving with the passing days and become more complex. There are a lot of people who think that cybersecurity is something that is only required by large enterprises. However, this is a misconception and it must be cleared that it is directed at all organizations whether small or large.
It helps in early detection and faster response while discovering the threats and vulnerabilities to prevent the breach. Organizations must turn to managed security services that is one of the cost-effective alternative to manage, monitor, detect and investigate data while responding to the cyberattacks.
This report depicts the malware attack that continues to rise with every passing year.
Here are some facts and figures
With the advent of technology the malware attacks became more enlightened. Here are some facts that prove why every organization should have a managed security service provider:
How does Automation help to reduce cybersecurity threats?
Automation allows the security centers to automate the most time consuming tasks while making sure the process runs smoothly. No longer the team has to deal with repetitive tasks, without automation the security teams had to handle everything manually which would waste their tons of hours and valuable resources.
No human intervention is required, thus freeing a lot of time of the team to deal with the real threats. Therefore, automation is crucial because it ultimately allows the team to deal with all the potential alerts in a much effective manner.
Here are the key benefits of Managed Security Services
1. Threat intelligence and analytics
Threat intelligence services will help organisations to detect attacks before and during these phases. Threat intelligence helps to monitor, collect and analyze all the events that proves to be a threat to the organization. Managed security services offers a complete visibility to the organizations that helps them protect their sensitive data and critical infrastructure.
2. Early Threat Detection
After the first attack, the attackers tend to target you again whereas you don’t have time to keep a track of all the attacks which might leave you vulnerable. Attackers today bypass all the rules using the latest techniques and tools. Managed security services uses security analytics and machine learning that discovers the threats and vulnerabilities at the earliest stages. With a combination of AI and threat hunters security services detects the unusual behavior, insider threats saving you from the breach.
3. Incident Response
Despite all your attempts there are a chance that a certain incident. Managed security services offer our clients with a relief that proactively respond and recover from a threat. So outsourcing incident response to a service provider makes your organization free from the complex challenges that requires multiple days to get fixed.
Our incident response capabilities support clients in the immediate, mid-term, and long-term aftermath of an incident with a thorough technical investigation.
4. Containment of breaches and its prevention
These types of incidents happen with a lightening speed. So, if a breach is discovered at the spot there are many actions that needs to be taken. It may be changing configuration in firewall, remove the user account instantly, delete the files, apply virtual patches with web application firewalls. So along with the containment, a perfect response to the incident includes many critical steps that can executed with our collaborative workflow.
How ToXSL is the Right Partner?
As the ever-present cyber threats continue to rise in numbers, it’s crucial for the organizations to implement managed security services in order to detect, prevent, and tackle each cyber threat in a timely manner. There are many choices for the businesses when it comes to finding the perfect partner.
At ToXSL, we offer customized, comprehensive solutions that provide a full suite of solutions. Based on our deep experience across industries and in all corners of business we are a technology leader!
ToXSL Technologies is a leading Web and Mobile App Development company in India. Incorporated in the year 2012, we are ISO 27001 and ISO 9001:2015 certified.
ToXSL Technologies is a leading Web and Mobile App Development company in India. Incorporated in the year 2012, we are ISO 27001 and ISO 9001:2015 certified.
About
Write
Help
Legal
Get the Medium app
"
https://stacksense.io/yes-you-are-overpaying-for-cloud-services-570d344b82f5?source=search_post---------57,"Sign in
Krish
Oct 30, 2019·4 min read
Ever since cloud computing came into prominence in the late 2000s, there is more focus on the cost savings in the industry and customer conversations. Cloud costs are a much more complex discussion than a simple statement on cost savings. We have heard tons of stories from organizations that save money on the cloud but we have also heard stories about burgeoning costs and move back to On-Premises data centers. In this post, we want to tackle cloud costs and help users consider the nuances that are in play.
There are few things about cloud costs that are obvious and one cannot argue against them.
Most organizations are overpaying the cloud providers than what they should actually be paying. This is due to various factors such as
Let us now discuss these two factors a little deeper
Predictive Analytics and automation will play a critical role in optimizing cloud costs. By tapping into predictive analytics and automation, users can tap into Spot Instances, Reserved Instances, and On-Demand Instances to save a considerable amount of money while also meeting the SLA needs of enterprises. The same thing can be done about cloud storage too. Predictive analytics and automation can store data in the low-cost storage disks while not in use and match higher-priced disks based on performance needs. Even container workloads can be run on elastic infrastructure that taps into spot instances. For example, SpotInst Elastigroup is a platform to use all types of instances and save 90% on cloud costs. You can do it for containers using SpotInst Ocean (equivalent to AWS Fargate). Similarly, Qubole offers savings for big data and machine learning workloads using spot instances
To rein in cloud waste, either in the form of runaway instances or unused or under-used instances, it is critical to have a good management and monitoring platform to cut down on the resource waste. Whether it is resource shutdown or right-sizing, predictive analytics and automation can help organizations save on cloud costs. Cloud Governance platforms like CoreStack or VMware’s CloudHealth are good examples of platforms that can help cut down on cloud waste. Univa NavOps offers similar cost optimization tools for HPC workloads.
Whether or not you accept it, most organizations are overspending on the cloud. It is time to wake up and realize that you can have considerable savings in the cloud but you need to have the right strategy and use the right set of tools to save costs without incurring additional overhead. Keep in mind that you are not obligated to improve the cloud providers’ margins but you need to save costs for your organization by not overpaying for the cloud services.
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
This blog helps enterprise decision-makers understand the emerging technologies and it is part of Rishidot Research publications
"
https://medium.com/@krmarko/cloud-services-disrupting-the-storage-market-with-bigger-changes-ahead-watch-out-intel-9ac66e11be38?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Dec 10, 2015·2 min read
Enterprises large and small have been flocking to cloud services and it’s sent the server business into a tailspin. Storage vendors are next to feel the disruptive pain. Recent market estimates show that storage industry growth has moved to ODMs and niche vendors targeting hyperscale cloud services. Indeed, most of the incremental storage, which is going into hyperscale cloud data centers, is being provided by converged, scale out systems. In the context of cloud services, this means they’re using distributed, virtualized storage on top of commodity servers; i.e. JBOD disks paired with cloud-native software.
The implications for top-tier storage vendors are obvious, declining sales and squeezed profit margins, however the ramifications for the rest of hardware supply chain could be equally profound, particularly for Intel. After much delay, ARM server SoCs are finally here and cloud-scale storage systems offer one of their best inroads to the data center. Here’s why and what it means to vendors and hardware buyers.
Cloud-scale storage systems offer one of the best inroads for ARM servers in the data center. By coupling many small cores with hardware modules for things like data compression, parity (RAID) calculations, SSL acceleration (crypto calculations) and integrated storage and network controllers, ARM-based SoCs can be for storage servers what custom mobile chips like Apple’s A-series are to mobile devices: a tailored, all-in-one processing engine.
Read on for more on who’s affected and what to watch for in 2016.
Originally published at www.forbes.com on December 10, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
See all (132)
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-billfold/people-in-greece-can-no-longer-pay-for-international-cloud-services-947da5036b62?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
Remember yesterday when I wrote about Chicago adding an Amusement Tax to cloud-based streaming services like Netflix? And remember how I quoted a few sources that were all “unfair!” and “how will this even work?”
Well, Greece is currently learning exactly how something like this might work. As BuzzFeed reports:
Last weekend, Greece’s government imposed capital controls — restrictions on the ability to take money out of the country — due to an economic crisis that continues to deepen after the country failed to make a debt repayment to the International Monetary Fund. The restrictions were an attempt to ensure cash remains within Greece’s economy and is not simply moved to a foreign safe haven.
However, the move has also had the effect of stopping many payments made using Greek credit cards to online services based outside the country. As a result, ordinary Greeks who are accustomed to using international services such as Apple’s AppStore and PayPal are now finding that they can no longer use popular paid-for internet services due to the financial restrictions.
These restrictions include PayPal, Dropbox, even online subscriptions to newspapers and magazines. (As Manjula Martin told us last week, paywalls aren’t that great for magazines or their readers — and this is just one more reason why.)
It is strange that people are still thinking of the cloud as some amorphous thing that should belong to everybody. The cloud is made of servers located in specific places (many, many specific places) individually managed by various companies that are also located in specific places — Dropbox, for example, is headquartered in San Francisco — and includes objects of value that are sent to specific places, the same way they used to pack DVDs into boxes and ship them to Blockbuster.
And if you bought a DVD at a Blockbuster in Oregon, there wouldn’t be any sales tax, but if you bought it in Washington, there would be sales tax — and now if you pay to stream that same movie in Chicago, you pay Amusement Tax, and if you’re in Greece, you can’t buy the soundtrack on iTunes because of capital controls.
It makes sense, and as much as my heart is chanting “information wants to be free!” I agree with it.
What about you?
Everything you wanted to know about money but were too…
Everything you wanted to know about money but were too polite to ask.
Written by
Freelance writer at Vox, Bankrate, Haven Life, & more. Author of The Biographies of Ordinary People.
Everything you wanted to know about money but were too polite to ask.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-launches-cloud-box-extending-public-cloud-services-to-local-devices-7e5fbc899949?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 22, 2020·3 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Alibaba Cloud ECS
On September 18, at the Apsara Conference 2020 held in Hangzhou, Alibaba Cloud announced the launch of Cloud Box. Based on the proprietary X-Dragon architecture, Cloud Box integrates the cutting-edge technologies of computing, storage, and networking to provide users with a fully managed cloud service that combines software with hardware for local deployment. For users who need to deploy their business in local data centers, Cloud Box provides the same experience as Alibaba Cloud’s public cloud.
Migration to the cloud has become a trend in various industries, especially with the rapid development of big data, artificial intelligence (AI), Internet of Things (IoT), 5G, and other technologies. Most companies regard digital transformation as a priority strategy. However, due to the requirements of compliance, bandwidth, latency, and cost, some enterprises are deploying some of their business in local data centers, but they also want to obtain the same benefits of low costs, elasticity, and agility as provided by the public cloud.
Cloud Box was created to solve this problem. Based on the X-Dragon architecture, Cloud Box provides users with the same cloud product experience as provided by the public cloud. At the same time, through the Virtual Private Cloud (VPC), Cloud Box facilitates the integration of applications deployed on the premises with other local applications or cloud services. Compared with the on-premises data centers, features of Cloud Box, such as on-demand ordering and pay-as-you-go billing, are provided to help users save the one-time investment of offline user-owned IDCs and avoid the cost of idle capacity caused by low device utilization.
The X-Dragon architecture is a hardware-software integrated computing architecture developed by Alibaba Cloud. This architecture helps users obtain computing capabilities superior to those of traditional physical machines. Based on the “X-Dragon architecture”, Cloud Box provides users with an elastic scaling service in on-premises data centers, high-performance computing capabilities, and excellent I/O performance. Meanwhile, by using Alibaba Cloud’s proprietary network devices, users can connect cloud box to local IT facilities to meet ultra-low latency network demands.
Cloud Box provides all types of elastic computing instances, databases, security, container services, and other products. Cloud Box ensures that instance version upgrades, security vulnerabilities, and patch upgrades are synchronized with the public cloud. Users can immediately have access to the latest products and features available on the public cloud.
Security compliance is one of the important reasons for enterprises to deploy their business in local data centers. Users can deploy Cloud Box in local data centers and store their data locally. The enterprise-level network isolation is achieved through the seamless interconnection between the VPC and the public cloud. Therefore, applications can be accessed over an internal network, which meets the regulatory requirements of customers from special industries.
Based on the capabilities of Alibaba Cloud’s public cloud, Cloud Box provides a fully managed cloud service that integrates software and hardware deployment locally. Users can enjoy the same stable experience and SLA services provided by the public cloud without the need for their own maintenance.
Enterprises have changed their computing requirements from “full scenario coverage” to “ubiquitous location”. Alibaba Cloud offers full-scenario coverage to cover “cloud, edge, and end” computing and has launched a series of new product deployment and operation forms. In the future, Alibaba Cloud will extend the public cloud to a broader space and work with its customers to get ready for the upcoming edge computing and 5G era.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iics/ransomware-attacks-against-cloud-services-are-increasing-93b8ec72f114?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eli Cyber Security
Jul 24, 2019·2 min read
A severe ransomware attack has hit the systems of iNSYNQ, a U.S.-based cloud hosting service provider; according to system audit specialists, one of the services most affected by this incident is QuickBooks, a cloud-based platform that provides accounting software and services.
The incident occurred sometime on June 16, though that’s all the information that iNSYNQ executives revealed at the time.
The company released an update on the incident until the week after the attack, mentioning that: “iNSYNQ was the victim of a ransomware attack perpetrated by unidentified threat actors. The incident had a serious impact on the systems where the data of some of our customers is stored, so at the moment it is impossible for us to access this information.”
“After detecting the infection our system audit team began an incident containment protocol, which involved disabling some of the servers in our ecosystem. This procedure aims to protect our customers’ data and information backups,” the company’s statement says.
On the other hand, Elliot Luchansky, the CEO of the company, reported through his social media profiles that the threat actors who perpetrated the attack employed a ransomware variant known as MegaCortex, a new development that has been present in multiple attacks in recent months.
Over the past few months various cybersecurity firms and system audit experts have been analyzing the recorded MegaCortex attacks, finding some similarities in each incident. One behavior detected by experts is that attackers start asking for ransoms of between 2 and 3 Bitcoin, the ransom could rise to 600 BTC if the victims ignores the hackers’ demand. “If you don’t have the money, don’t even waste your time writing to us; we don’t work for charity”, concludes the ransom note sent by the attackers.
The latest updates on the incident state that iNSYNQ decided not to pay the hackers and begin its recovery process using security backups. Similarly, specialists from the International Institute of Cyber Security (IICS) recommend that companies that are victims of this variant of malware use their backups and, if possible, discard the option to pay the ransom, as this only benefits the hackers, providing them with resources to keep up with their illicit activities, and there is no guarantee that hackers will honor their part of the deal.
Originally published at https://www.securitynewspaper.com on July 23, 2019.
Knowledge belongs to the world
Knowledge belongs to the world
"
https://irelandstechnologyblog.com/cloudplayer-lets-you-stream-your-music-from-all-your-cloud-services-cabf799a2bae?source=search_post---------63,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
The cloud despite being around for years is still somewhat of a buzz word and with so many online cloud services trying to get you to store with them it can be tempting to use multiple. I personally use OneDrive and DropBox but many users may have twice this so finding your music when its this spread out is a pain.
One new app by a company called DoubleTwist is CloudPlayer which will allow you to connect all your cloud services and through the app will all you to stream all your music seamlessly.
Once you have connected your given cloud services the app will then begin to search through them, indexing each one and providing cover art for the track. This can take a bit of time if you have a large library and it will also index your local music too.
The app is free to use but via IAP (In App Purchases) you can unlock SuperSound™, EQ, AirPlay, Chromecast and cloud support.
CloudPlayer [Android]
Ireland’s Technology Blog is an Irish technology blog…
Written by
Ireland's Technology Blog - EVERYTHING TECHNOLOGY, MOBILE, TV, INTERNET AND MORE - Technology consultants for Tipp Fm Radio
Ireland’s Technology Blog is an Irish technology blog covering all aspects of technology no matter how small or large. We feature everything from home technology, personal technology, gaming, smartphones and all the latest breaking news from around the world.
Written by
Ireland's Technology Blog - EVERYTHING TECHNOLOGY, MOBILE, TV, INTERNET AND MORE - Technology consultants for Tipp Fm Radio
Ireland’s Technology Blog is an Irish technology blog covering all aspects of technology no matter how small or large. We feature everything from home technology, personal technology, gaming, smartphones and all the latest breaking news from around the world.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ipg-media-lab/the-cloud-services-price-war-may-have-just-started-10cdbbee5e40?source=search_post---------65,"There are currently no responses for this story.
Be the first to respond.
Google, looking to lure startups away from the ever-popular Amazon Web Services cloud, announced a cut in its cloud storage and access rates, effectively kicking off a price-undercutting war with Amazon that seems likely to leave other, third party cloud providers in its wake. On one level, competing with Google and Amazon on a price to features and scale ration seems impossible for companies without their financial clout. To startups, the new progressive pricing models and features mean that Rackspace and Amazon are now only on par with — or indeed less attractive than — Google’s service. As more and more business is done on the cloud, watching the price for access and services is integral to keeping pace with how digital business develops; don’t expect this to be the last time this story crops up in the media this year.
The media futures agency of IPG Mediabrands
Written by
Keeping brands ahead of the digital curve. An @IPGMediabrands company.
The media futures agency of IPG Mediabrands
Written by
Keeping brands ahead of the digital curve. An @IPGMediabrands company.
The media futures agency of IPG Mediabrands
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iics/a-major-cloud-services-company-suffers-massive-ransomware-infection-8f587fc619bb?source=search_post---------66,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eli Cyber Security
Nov 12, 2019·3 min read
The week is just beginning and new security incidents affecting major technology companies have already being reported. According to web application security specialists, SmarterASP.NET, an ASP.NET hosting service provider, was the victim of a serious ransomware attack that could affect its more than 400k customers.
This is the third time this year that a major web hosting company is affected by an encryption malware infection, clear indicator of poor security measures and evolution of the methods employed by threat actors.
Through a message posted on its website the company acknowledged the incident and claimed that it had already begun work on resetting all its systems, as mentioned by web application security experts. However, it is still unknown whether SmarterASP.NET executives agreed to pay the ransom to hackers or instead the information will be recovered from the company’s backups. “Your account is under attack; the perpetrators have encrypted all your data. We are working with experts to retrieve your information and ensure that this does not happen again,” the statement says.
So far the company has not provided further details about the incident and its management, even its telephone line has been disabled.
The attackers not only compromised the customer information of this service, but also took the time to attack the company, disconnecting its website, leaving it inaccessible throughout Saturday. Finally, SmarterASP.NET web application security team regained control of their website on Sunday morning.
Regarding the ransomware variant used by those responsible for this cyberattack, an anonymous user posted on Twitter some screenshots of a compromised computer, where it can be seen that the information was encrypted with an updated version of the Snatch ransomware, which adds the .kjhbx extension to infected files.
So far the company does not seem to have made much progress in the recovery process, as the number of users reporting that access to their accounts and data remains blocked, including files on their websites and back-end databases, it’s still large.
The incident has hit many of the users of this service very seriously, as most of them use SmarterASP.NET as a back end of web applications to synchronize or back up important information. According to web application security experts, since ransomware also affected these databases, it is impossible for website administrators to move their operations to an alternative IT implementation.
In the past few months, experts from the International Institute of Cyber Security (IICS) reported the attack on two other major hosting companies. The first incident occurred at A2 Hosting in May, where hackers used the GlobeImposter ransomware. The next victim was iNSYNQ, which was infected last July with a variant of the MegaCortex ransomware, which prevented the proper functioning of the company’s systems for almost two months; recovery time for SmarterASP.NET is expected to be similar.
Originally published at https://www.securitynewspaper.com on November 11, 2019.
Knowledge belongs to the world
Knowledge belongs to the world
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yakalot/yeah-the-ghost-town-feeling-effect-is-brought-to-you-by-huge-mega-cloud-services-corporations-6d2017de1604?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tim Colby
·Sep 26, 2021
Yeah, the ghost town feeling effect is brought to you by, huge mega-cloud-services corporations from an internet connection near you. And, golly... if you touch their territory without the obligatory fees they will let you know about it and go out of their way to bench anyone out of this universe. So many authors try to crack these monopolistic behaviors like 'don't want to pay to play?' Well here's a little SEO hell we have for you, brought to you by huge mega-cloud-services corporations from an internet connection near you.
I think this is why there are so many substack style shingles out there, brought to you by your balkanized corporate services currently running on an internet near you. So hard to crack this.
1
Grad: Whats-a-mata-U, Mayor: Foggybog, Wi., Awards: Medium response run-on-sentence-king, Medium response all-over-the-place trophy
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@richardyaoipg/the-cloud-services-price-war-may-have-just-started-d2c161980c1f?source=search_post---------69,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Yao
Mar 31, 2014·1 min read
Google, looking to lure startups away from the ever-popular Amazon Web Services cloud, announced a cut in its cloud storage and access rates, effectively kicking off a price-undercutting war with Amazon that seems likely to leave other, third party cloud providers in its wake. On one level, competing with Google and Amazon on a price to features and scale ration seems impossible for companies without their financial clout. To startups, the new progressive pricing models and features mean that Rackspace and Amazon are now only on par with — or indeed less attractive than — Google’s service. As more and more business is done on the cloud, watching the price for access and services is integral to keeping pace with how digital business develops; don’t expect this to be the last time this story crops up in the media this year.
Manager of Strategy & Content, IPG Media Lab
See all (10)
Manager of Strategy & Content, IPG Media Lab
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pragmatic-programmers/cloud-services-are-hardware-under-apis-4cb07b89db49?source=search_post---------70,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Pragmatic Programmers
Jan 28, 2021·2 min read
👈 This Isn’t a Metaphor | TOC | Ever-Evolving Operating Systems 👉
In the mental model of the cloud as a distributed, general-purpose computer, each cloud service is akin to a hardware interface in traditional computing. Just as those were abstracted and managed by an operating system and a programming language, so cloud services can be abstracted and managed. For example, networking in the cloud is usually…
"
https://medium.com/@charles-bastille/i-wouldnt-lump-the-various-cloud-services-in-with-things-like-webflow-8965a79db88f?source=search_post---------71,"Sign in
There are currently no responses for this story.
Be the first to respond.
Charles Bastille
Sep 8, 2020·2 min read
Owen Williams
I wouldn't lump the various cloud services in with things like webflow. With a cloud service you can do things the old fashioned (and still better) way by signing up with AWS (for example) and getting a nice Linux install, enabling you to fire up the command line tool . Most decent developers are at that point ready to go.
I tried webflow just for ducks. I honestly can't quite understand the point behind it. And even if in my brief peek at it I'm just being dense, who knows what kind of code they're writing behind the scenes?
I'm old enough to remember NetObjects Fusion (I think that was the name - it's not worth googling). That and FrontPage wrote such horrific code that they became the butt of jokes in developer communities. Webflow has to be writing code somewhere, too. I wonder what that looks like. GPT-3 and other AI technologies look interesting, but none of these are yet within reach of the average maker wannabe.
Also, let's not mistake tooling for drag and drop. In other words, for example, if I use a scaffolding tool to set up a react or angular or backbone environment, I'm really just taking advantage of somebody's nice script writing skills to save myself hours in mundane development efforts. There are so many excellent tools out there that there are no development shops on earth that don't use them.
At the very end of the day, is there really such a thing as a ""maker"" in the development world? Don't most of us actually *want* to code? Part of the reason I couldn't really give webflow a good evaluation was because I found it boring.
Coding is fun. Drag and drop is not.
And if the idea is to get apps and websites into the hands of non-developers, that is probably not a great idea, either. How many news web sites are now unreadable because of the way their advertising and those awful paid sponsor links are set up? I have to turn javascript off to read almost any news site these days (they seem to work better on phones, though).
Security is also an issue. Do we want non-developers cranking out apps and web sites with no mindset towards security? Do any of these people know anything about setting up secure forms? Do the drag and drop products they use know anything about it?
Where is MagicLand? Pick your favorite bookseller at https://charles-bastille.com All stories © 2021 by Charles Bastille.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Where is MagicLand? Pick your favorite bookseller at https://charles-bastille.com All stories © 2021 by Charles Bastille.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/clouddon/why-another-cloud-services-company-5c69e4ee259c?source=search_post---------72,"There are currently no responses for this story.
Be the first to respond.
If you are here, you know already that we are a new cloud services company. You may ask why another cloud services company? Here are some reasons:
1) Combination of technical expertise and business insights
We have a strong engineering background, spanning over 10 years, with more than 5 years in virtualization and cloud. We also have expertise across multiple cloud platforms — OpenStack, VMware, SCVMM, AWS, CloudStack & Eucalyptus. This accompanied with deeper business insights gives unique value to you, being able to converse with all levels of your organization from CIO to the DevOps engineer.
2) Specializing in OpenStack services
We specialize on OpenStack services. We have deployment experience since Cactus release, familiarity with many types of installations/ distributions, and intimacy with OpenStack internals. We have a few successful deployments under our portfolio. Most importantly, we are passionate about OpenStack’s success!
3) SI partnerships
We have active SI friends globally, currently developing in to more formal SI partnerships globally. Please stay tuned for more details.
Please contact us for your cloud needs!
CloudDon - catalyzing modern enterprise IT transformations
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
"
https://medium.com/@sriramhere/why-another-cloud-services-company-e7c0ec403c88?source=search_post---------73,"Sign in
Sriram Subramanian
Nov 1, 2013·1 min read
If you are here, you know already that we are a new cloud services company. You may ask why another cloud services company? Here are some reasons:
1) Combination of technical expertise and business insights
We have a strong engineering background, spanning over 10 years, with more than 5 years in virtualization and cloud. We also have expertise across multiple cloud platforms — OpenStack, VMware, SCVMM, AWS, CloudStack & Eucalyptus. This accompanied with deeper business insights gives unique value to you, being able to converse with all levels of your organization from CIO to the DevOps engineer.
2) Specializing in OpenStack services
We specialize on OpenStack services. We have deployment experience since Cactus release, familiarity with many types of installations/ distributions, and intimacy with OpenStack internals. We have a few successful deployments under our portfolio. Most importantly, we are passionate about OpenStack’s success!
3) SI partnerships
We have active SI friends globally, currently developing in to more formal SI partnerships globally. Please stay tuned for more details.
Please contact us for your cloud needs!
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
"
https://medium.com/@richardyaoipg/ces-2014-sonys-kaz-hirai-keynote-brings-new-sony-streaming-cloud-services-f43cb0f51d19?source=search_post---------74,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Yao
Jan 8, 2014·1 min read
When Sony CEO Kaz Hirai took the stage at CES, most people were expecting something like the Playstation Now, one of many major product announcements he made. In essence, PSNow is a cloud gaming service for TVs, consoles, and phones; the service debuted at CES where test participants were able to play The Last Of Us streamed over Wi-Fi. The service is coming to a closed Beta at the end of the month, and is expected to roll out at the end of the summer, at which point it could become a very serious competitor to Steam. What most people weren’t expecting, however, was for Hirai to announce other cloud-based services for live TV, DVR, and Video On Demand, which is precisely what he did. It’s more ambitious than the streaming PSNow, and it looks to build out the Sony Entertainment Network in a big way, drawing consumers into a network by bolstering content and storage options, much like 4K and Streaming TV providers have done thus far at CES this year. If Sony can pull it off as well, it will certainly be a big step towards the unified Only-Sony device network that Hirai has wanted for a while now.
Manager of Strategy & Content, IPG Media Lab
Manager of Strategy & Content, IPG Media Lab
"
https://medium.com/ipg-media-lab/ces-2014-sonys-kaz-hirai-keynote-brings-new-sony-streaming-cloud-services-e159a8545356?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
When Sony CEO Kaz Hirai took the stage at CES, most people were expecting something like the Playstation Now, one of many major product announcements he made. In essence, PSNow is a cloud gaming service for TVs, consoles, and phones; the service debuted at CES where test participants were able to play The Last Of Us streamed over Wi-Fi. The service is coming to a closed Beta at the end of the month, and is expected to roll out at the end of the summer, at which point it could become a very serious competitor to Steam. What most people weren’t expecting, however, was for Hirai to announce other cloud-based services for live TV, DVR, and Video On Demand, which is precisely what he did. It’s more ambitious than the streaming PSNow, and it looks to build out the Sony Entertainment Network in a big way, drawing consumers into a network by bolstering content and storage options, much like 4K and Streaming TV providers have done thus far at CES this year. If Sony can pull it off as well, it will certainly be a big step towards the unified Only-Sony device network that Hirai has wanted for a while now.
The media futures agency of IPG Mediabrands
The media futures agency of IPG Mediabrands
Written by
Keeping brands ahead of the digital curve. An @IPGMediabrands company.
The media futures agency of IPG Mediabrands
"
https://medium.com/@dwdraju/calling-google-cloud-services-from-aws-using-iam-roles-without-gcp-service-account-credentials-af918c05384d?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Raju Dawadi
Oct 31, 2021·4 min read
If we are inside Google Cloud and want to call any GCP services, we prefer using Service Account with Access Scope which makes the identity and API access lot more easier. No burden of handling credential keys but the specific service will have access to another service or API call.
But if we want to call GCP services out of Google Cloud environment, service account credential is the normal way. We create a new Service Account, assign the required permission and generate key for that account. Then referencing the key path from code or gcloud cli for authentication purpose.
If you are on AWS environment, the key file with credential is no more required. We can use AWS IAM role as authentication mechanism for calling Google Cloud resources. Yes, this is possible by the use of Workload identity federation. The identity federation can be used with Amazon Web Services (AWS), or with any identity provider that supports OpenID Connect (OIDC), such as Microsoft Azure, or SAML 2.0 Preview. Means, the multi-cloud approach is much broader and easier.
Let’s dive into it by going through what steps we will be doing:
Head to Workload Identity Federation under Google Cloud IAM. I gave it a name aws-identity-pool
Next, adding provider to the pool. Setting provider name as aws and provider id as aws-provider with the AWS account ID for the binding.
And create the pool.
Under Google Cloud IAM, there is Service accounts section. Create a new service account with Workload Identity User role. I gave it a name gcp-aws-identity
Adding the role for user and done!
Head over to IAM/Roles from AWS console and “Create role”.
No permission is required. I saved it with name “AWS_GCP_Identity_ROLE”.
A new instance with port 22 open is preferred for a quick check. Attach the newly created role into the instance.
Run the following command from Google Cloud Shell or any authenticated environment by replacing GCP_ACCOUNT_ID, AWS_ACCOUNT_ID and GCP_PROJECT with your own.
And create the credential config
By using the above generated configoutput.json , we can call GCP service from AWS from the instance which is attached with the IAM role. The json file doesn’t have actual service account credential but only few variables for
DevOps | SRE | #GDE
See all (547)
DevOps | SRE | #GDE
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@NirZicherman/why-you-should-never-pay-for-podcast-hosting-9c39becd7cf7?source=search_post---------77,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Nir Zicherman
Jul 23, 2018·3 min read
In my role as CTO and co-founder of Anchor, I’ve spent the past several years working to make sure that Anchor remains the easiest way to make a podcast ever. And, that it remain 100% free for creators. One of the questions I am often asked is “How is Anchor free?” Podcasters are accustomed to paying to host their podcasts, so why aren’t we charging? What’s the catch?
It’s a great question, and my answer is usually a question, too: “Why would you pay for podcast hosting?” Or, “Why aren’t the other guys free?”
Many podcasts are hosted by a handful of traditional podcast companies that have been around for over a decade. At the time that these companies were founded, the cost of hosting podcasts on the internet was considerably more expensive. Since then, cloud storage and computing costs have plummeted, but the fees paid by podcasters have stayed the same (and in some instances have gone up, even when the companies’ costs have gone down).
Back in the day, you would have had to pay to host video online. But you would never do that today (thanks to services like YouTube, Vimeo, Twitch, and plenty of other free video hosting platforms). Back in the day, you would have had to pay to store your photos online. But that outdated business model has virtually disappeared thanks to platforms like Google Photos, Instagram, Imgur, and others. At Anchor, we believe the notion of charging creators to host their content online is antiquated and unfair. And above all else, it serves as a barrier that prevents the podcasting ecosystem from growing and becoming more diverse, because it limits it to only those voices who can afford to pay.
For almost every single podcast Anchor hosts, the cost to us is less than 10 cents per month. That means that hosting your podcast for an entire year costs Anchor around one dollar. If Anchor were to charge you $10 per month for file storage and basic analytics, we would either be grossly exaggerating our costs, or grossly overpaying our vendors.
Anchor benefits greatly from economies of scale. The easier we make it for everyone to make podcasts, the closer to zero we can drive the average price of hosting everyone’s podcasts. Our per-user costs drop every time we reach a new growth milestone, and will continue to do so. This is because the incremental price of variable costs (like hosting) go down the more we host, and the static costs (like servers) are split as tiny fractions among the many podcasts on Anchor.
People may ask “So if you’re not making money off of me to host… what’s your business model?” We are not in the business of charging you, the podcaster. We want to work with you to help you make money off your podcast, in which case we all win. And that 10 cents per month to host your podcast becomes a negligible cost compared to the revenue we can all earn together as we advance the medium of podcasting together.
Anchor has a singular mission, and that is to democratize audio. Democratization means making it possible for anyone to start a podcast, regardless of experience level, location, socio-economic status, or anything else. But it means more than just enabling anyone to create podcasts. It also means enabling podcasters to create value from their work and ultimately make money off of their podcasts.
Very soon, Anchor will roll out a suite of rich monetization features unlike anything that has ever existed in podcasting. All podcasters, from those with massive followings to those who are just starting out, will be able to make money off of their work. Anchor will share in the revenue in a way that will always be transparent, fair to the creator, and competitive in the market.
We don’t want a podcaster to ever pay for hosting again. We believe real change in this space is long overdue, and we can’t wait to show you what we’re working on.
@anchor co-founder and CTO
See all (133)
3.3K 
32
3.3K claps
3.3K 
32
@anchor co-founder and CTO
About
Write
Help
Legal
Get the Medium app
"
https://netflixtechblog.com/netflix-drive-a607538c3055?source=search_post---------78,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Written by Vikram Krishnamurthy, Kishore Kasi, Abhishek Kapatkar, Tejas Chopra, Prudhviraj Karumanchi, Kelsey Francis, Shailesh Birari
In this post, we are introducing Netflix Drive, a Cloud drive for media assets and providing a high level overview of some of its features and interfaces. We intend this to be a first post in a series of posts covering Netflix Drive. In the future posts, we will do an architectural deep dive into the several components of Netflix Drive.
Netflix, and particularly Studio applications (and Studio in the Cloud) produce petabytes of data backed by billions of media assets. Several artists and workflows that may be globally distributed, work on different projects, and each of these projects produce content that forms a part of the large corpus of assets.
Here is an example of globally distributed production where several artists and workflows work in conjunction to create and share assets for one or many projects.
There are workflows in which these artists may want to view a subset of these assets from this large dataset, for example, pertaining to a specific project. These artists may want to create personal workspaces and work on generating intermediate assets. To support such use cases, access control at the user workspace and project workspace granularity is extremely important for presenting a globally consistent view of pertinent data to these artists.
Netflix Drive aims to solve this problem of exposing different namespaces and attaching appropriate access control to help build a scalable, performant, globally distributed platform for storing and retrieving pertinent assets.
Netflix Drive is envisioned to be a Cloud Drive for Studio and Media applications and lends itself to be a generic paved path solution for all content in Netflix.
It exposes a file/folder interface for applications to save their data and an API interface for control operations. Netflix Drive relies on a data store that will be the persistent storage layer for assets, and a metadata store which will provide a relevant mapping from the file system hierarchy to the data store entities. The major pieces, as shown in Fig. 2, are the file system interface, the API interface, and the metadata and data stores. We will delve into these in the following sections.
Creative applications such as Nuke, Maya, Adobe Photoshop store and retrieve content using files and folders. Netflix Drive relies on FUSE (File System In User Space) to provide POSIX files and folders interface to such applications. A FUSE based POSIX interface provides feature customization elasticity, deployment configuration flexibility as well as a standard and seamless file/folder interface. A similar user space abstraction is available for Windows (WinFSP) and MacOS (MacFUSE)
The operations that originate from user, application and system actions on files and folders translate to a well defined set of function and system calls which are forwarded by the Linux Virtual File System Layer (or a pass-through/filter driver in Windows) to the FUSE layer in user space. The resulting metadata and data operations will be implemented by appropriate metadata and data adapters in Netflix Drive.
The POSIX files and folders interface for Netflix Drive is designed as a layered system with the FUSE implementation hooks forming the top layer. This layer will provide entry points for all of the relevant VFS calls that will be implemented. Netflix Drive contains an abstraction layer below FUSE which allows different metadata and data stores to be plugged into the architecture by having their corresponding adapters implement the interface. We will discuss more about the layered architecture in the section below.
Along with exposing a file interface which will be a hub of all abstractions, Netflix Drive also exposes API and Polled Task interfaces to allow applications and workflow tools to trigger control operations in Netflix Drive.
For example, applications can explicitly use REST endpoints to publish files stored in Netflix Drive to cloud, and later use a REST endpoint to retrieve a subset of the published files from cloud. The API interface can also be used to track the transfers of large files and allows other applications to be built on top of Netflix Drive.
The Polled Task interface allows studio and media workflow orchestrators to post or dispatch tasks to Netflix Drive instances on disparate workstations or containers. This allows Netflix Drive to be bootstrapped with an empty namespace when the workstation comes up and dynamically project a specific set of assets relevant to the artists’ work sessions or workflow stages. Further these assets can be projected into a namespace of the artist’s or application’s choosing.
Alternatively, workstations/containers can be launched with the assets of interest prefetched at startup. These allow artists and applications to obtain a workstation which already contains relevant files and optionally add and delete asset trees during the work session. For example, artists perform transformative work on files, and use Netflix Drive to store/fetch intermediate results as well as the final copy which can be transformed back into a media asset.
Given the two different modes in which applications can interact with Netflix Drive, now let us discuss how Netflix Drive is bootstrapped.
On startup, Netflix Drive expects a manifest that contains information about the data store, metadata store, and credentials (tied to a user login) to form an instance of namespace hierarchy. A Netflix Drive mount point may contain multiple Netflix Drive namespaces.
A dynamic instance allows Netflix Drive to show a user-selected and user-accessible subset of data from a large corpus of assets. A user instance allows it to act like a Cloud Drive, where users can work on content which is automatically synced in the background periodically to Cloud. On restart on a new machine, the same files and folders will be prefetched from the cloud. We will cover the different namespaces of Netflix Drive in more detail in a subsequent blog post.
Here is an example of a typical bootstrap manifest file.
The manifest is a persistent artifact which renders a user workstation its Netflix Drive personality. It survives instance failures and is able to recreate the same stateful interface on any newly deployed instance.
In order to allow a variety of different metadata stores and data stores to be easily plugged into the architecture, Netflix Drive exposes abstract interfaces for both metadata and data stores. Here is a high level diagram explaining the different layers of abstractions in Netflix Drive
Each file in Netflix Drive would have one or many corresponding metadata nodes, corresponding to different versions of the file. The file system hierarchy would be modeled as a tree in the metadata store where the root node is the top level folder for the application.
Each metadata node will contain several attributes, such as checksum of the file, location of the data, user permissions to access data, file metadata such as size, modification time, etc. A metadata node may also provide support for extended attributes which can be used to model ACLs, symbolic links, or other expressive file system constructs.
Metadata Store may also expose the concept of workspaces, where each user/application can have several workspaces, and can share workspaces with other users/applications. These are higher level constructs that are very useful to Studio applications.
Netflix Drive relies on a data store that allows streaming bytes into files/objects persisted on the storage media. The data store should expose APIs that allow Netflix Drive to perform I/O operations. The transfer mechanism for transport of bytes is a function of the data store.
In the first manifestation, Netflix Drive is using an object store (such as Amazon S3) as a data store. In order to expose file store-like properties, there were some changes needed in the object store. Each file can be stored as one or more objects. For Studio applications, file sizes may exceed the maximum object size for Cloud Storage, and so, the data store service should have the ability to store multiple parts of a file as separate objects. It is the responsibility of the data store service to tie these objects to a single file and inform the metadata store of the single unique Id for these several object parts. This Data store internally implements the chunking of file into several parts, encrypting of the content, and life cycle management of the data.
Multi-tiered architecture
Netflix Drive allows multiple data stores to be a part of the same installation via its bootstrap manifest.
Some studio applications such as encoding and transcoding have different I/O characteristics than a typical cloud drive.
Most of the data produced by these applications is ephemeral in nature, and is read often initially. The final encoded copy needs to be persisted and the ephemeral data can be deleted. To serve such applications, Netflix Drive can persist the ephemeral data in storage tiers which are closer to the application that allow lower read latencies and better economies for read request, since cloud storage reads incur an egress cost. Finally, once the encoded copy is prepared, this copy can be persisted by Netflix Drive to a persistent storage tier in the cloud. A single data store may also choose to archive some subset of content stored in cheaper alternatives.
Studio applications require strict adherence to security models where only users or applications with specific permissions should be allowed to access specific assets. Security is one of the cornerstones of Netflix Drive design. Netflix Drive dynamic namespace design allows an artist or workflow to access only a small subset of the assets based on the workspace information and access control and is one of the benefits of using Netflix Drive in Studio workflows. Netflix Drive encapsulates the authentication and authorization models in its metadata store. These are translated into POSIX ACLs in Netflix Drive. In the future, Netflix Drive can allow more expressive ACLs by leveraging extended attributes associated with Metadata nodes corresponding to an asset.
Netflix Drive is currently being used by several Studio teams as the paved path solution for working with assets and is integrated with several media suite applications. As of today, Netflix Drive can be installed on CentOS, MacOS and Windows. In the future blog posts, we will cover implementation details, learnings, performance analysis of Netflix Drive, and some of the applications and workflows built on top of Netflix Drive.
If you are passionate about building Storage and Infrastructure solutions for Netflix Data Platform, we are always looking for talented engineers and managers. Please check out our job listings
Learn about Netflix’s world class engineering efforts…
1K 
12
1K claps
1K 
12
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Silvia Zeamer
Feb 14, 2021·10 min read
This winter, I discovered that Wellesley College, where I am currently a senior studying Media Arts and Sciences, has an archive of over a hundred year’s worth of course catalogues, admissions guidelines, and yearly bulletins. I was immediately electrified by the potential for fascinating data which could be drawn from these documents, but the first step would have to be converting them to text, as there are not many analytical methods which can be run on scans of old, browned PDFs.
Thus began my search for a way to quickly and effectively run OCR on a large volume of PDF files while retaining as much formatting and accuracy as possible. After trying several methods, I found that using the Google Cloud Vision API yielded by far the best results of any of the publicly available OCR tools I tried.
As I could not find any single, comprehensive guide to using this amazing tool to run simple OCR applications, I decided to write this one, so that anyone with a little programming knowledge can put this wonderful tool to use.
In order to run optical character recognition using Google Cloud Vision, you first need to have a Google account. This will allow you to login to Google’s dashboard for cloud services. One of the many services which are accessible from this dashboard is file storage, which we will be using to host the PDF file we will be converting to text.
Because the advanced machine learning algorithms which we will be accessing via the Cloud Vision API run in the cloud, we will need to upload our PDF to a “bucket” of files hosted by Google, so that it will be accessible.
This tutorial will show you how to write the end result, a text file containing all the text in your PDF, to a location on your computer.
3. Click on the dropdown menu just to the right of the logo which says Google Cloud Platform. Mine says “OCR Test”, which is the name of my currently open project, but yours will say something different. A window will pop up with a list for recent projects and a “New Project” button in the top right corner. Click the button to make a new project. Give your project a name which will help you remember what you’re using it for. You don’t need to worry about any of the other fields. Click “Create”. Once your project has been created, make sure to select it by opening the window again and selecting it from the list of recent projects.
4. You should now see the Project Info, APIs, and other information panels for your newly created project, as in the screenshot above. In the “Getting Started” panel on the bottom left, click “Explore and Enable APIs”. This will allow you to choose the Google APIs you want to be able to use for this project.
5. In the menu bar at the top of the screen, click “Enable APIs and Services”. This will take you to the API Library. Search for “Cloud Vision API” and select it.
6. Click “Enable” to make the API available to your project. This will take you to your overview page for the Cloud Vision API. In the top right corner of the screen, click “Create Credentials”.
7. Choose “Cloud Vision API” from the drop down menu under “Which API are you using?” and under “Are you planning to use this API with App Engine or Computer Engine,” select “No, I’m not using them”. Click the blue “What Credentials Do I Need?” button.
8. Now you will be able to create a key so that you can authenticate yourself when you try to connect to the Cloud Vision API. Choose a service account name you will remember, and set your role to “Owner”. Set the key type to JSON. Click continue. You will now be able to download a JSON file containing your credentials.
You now have a project on the Google Cloud Platform, which will be able to use the Cloud Vision API. The next step is to upload your PDF document so that it is stored in the cloud. Then, you can write the script to convert it to text.
9. If it is not already open, click the navigation menu on the left side of the Google Cloud Platform, and scroll down until you see “Storage”. Click on it — this will open a drop-down menu. Select “Browser” from the dropdown menu. At this point, you will need to enable billing if you have not done so already. If you have Google Pay, you can use it here — otherwise, you will need to enter external payment information. This will vary depending upon how you pay, so I will not give instructions. Once you’re done, you should see a dialogue with the option to “Create a Bucket”.
10. Give your bucket a unique name. This is a storage repository within the project you created earlier. Set where to store your data to “multi-region” and the default storage class for your data to “standard”. Click “Create”.
You now have a bucket set up, where you can upload files so that they can be accessed by any APIs which are enabled for the current project. You can upload the PDF file you would like to transcribe by dragging and dropping it from wherever you keep it on your computer.
You are ready to write a program which can access both this file and the Cloud Vision API by connecting to Google Cloud services and providing the key you downloaded earlier.
Now that you have everything you need set up on the Google Cloud side of things, we will move to installing the necessary tools on your computer and using them to extract text from a PDF file.
First, you may need to make some installations. Open your terminal and navigate to a folder where you will keep the python script you write. Enter the following commands.
pip install google-cloud-vision
pip install google-cloud-storage
These use pip to install two Python libraries with tools for interacting with the Google Cloud Vision and Cloud Storage APIs, respectively. Next, run
pip freeze
This will check if you’ve installed everything you should have. You should have the following, although most will likely be newer versions.
If you don’t have any of them, use pip to install the ones missing.
Finally, you need to set your Google Application Credentials — that is, you need to register where you’re keeping the json key you downloaded earlier, so that when you run programs using Google Cloud services, your computer can authenticate itself as belonging to your Google account.
You can find excellent instructions on how to do this on any platform here. Once you have done this, you will be able to run programs which use Google Cloud Services from the command line.
Now we get to the fun part — writing a script to actually perform optical character recognition on our chosen PDF! Make a new Python file and open it with your preferred code editor. I will explain each part of the script I used so that you can understand it as you substitute in your information. You can find the whole of the script here, on my Github, as well. Try to follow along with each step before downloading it to tinker with.
We need to import json so that we can handle Cloud Vision’s outputs. re is a library which will allow us to use regular expressions to match particular patterns in strings.
Vision and storage from google.cloud will allow us to use the Google Cloud Vision and Google Cloud Storage APIs.
2. The next step is to write a function to detect all the places in our PDF file where there is readable text, using the Google Cloud Vision API. Make sure to read the comments in this function, so that you understand what each step is doing.
In addition to the comments explaining this function, here are some things to note. You may expect that when we run Google’s amazing OCR tools on a document, we will get a text file in return. Actually, this function will just output a json file — or several, depending on the size of your PDF — containing information about where there is text in the file. Actually getting the text so we can read it is the next step.
This function takes two inputs. The first, gcs_source_uri is the location of your PDF file in Google Cloud storage. The second, gcs_destination_uri is the location in Google Cloud Storage where you want the json files containing your file annotations to go.
URI is the term for a file location in Google Cloud storage. You can think of it as a URL within Google Cloud Storage, or like a path on your computer. It describes where, in the hierarchy of files you keep on google cloud, a particular file can be found. To find the URI of a file, you can double click on it to see details about it and copy the URI from the table of data you will thus open.
To generate your annotations, you will write a line at the bottom of your Python file calling the async_detect_document function. Mine looks like this.
The first URI is the path to a PDF document stored in my google cloud storage bucket, from which I want to read. The second leads to a folder in which I am saving all of my document annotations.
3. Now that we have annotated our PDF, we can finally use Cloud Vision to go to each location where there is text and read it into a text file! My code for doing this follows. Again, be sure to read the comments.
This function takes just one argument: the URI of the location where we stored our annotations. It will output the results of the transcription into a text file in your currently active directory, in addition to printing them in your terminal.
Here’s how I called it, using the same directory as before.
Congratulations! If all went well, you should now be in posession of a text file containing a line-by-line transcription of all the machine-readable text in your PDF. You may be surprised by how much could be read — it even works on some handwriting.
Here is a side-to-side comparison of some of my results. This is a page from a course catalogue I drew from the Wellesley College archives, dating from 1889. Despite the fact that I used a totally un-pre-processed PDF as an input file for this test, the results are highly accurate, even for names and foreign words.
In my next article, I will demonstrate some methods for pre-processing old text files in order to increase accuracy even more, so stay tuned. If you have any trouble or just want to chat, please get in touch — I love to talk!
Senior at Wellesley College studying Media Arts and Sciences. Future research scientist in HCI and security. Here for human connection <3
554 
10
554 
554 
10
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://blog.expo.dev/how-to-build-cloud-powered-mobile-apps-with-expo-aws-amplify-2fddc898f9a2?source=search_post---------80,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
AWS Amplify is a CLI & toolchain for the client that allows developers to quickly create & connect to AWS cloud services directly from the front-end environment. Amplify lowers the barrier to entry for developers looking to build full-stack applications by giving them an easy way to create & connect to managed cloud services.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/the-microsoft-hustle-355f818161a6?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
Despite their cringeworthy hardware, software, cloud services, and acquisitions, Microsoft is still a brilliant company in this day and age and you would be foolish to underestimate them in this regard. Sure, their last three operating systems have been complete turds and they have been reduced from being able to charge hundreds per license to giving Windows 10 away for free while having to place ads on it, implement invasive telemetry that can’t be disabled, and resort to malware-esque distribution tactics, Windows 7 still has almost twice the market share of Windows 10 and it is almost a decade old. Their Surface line of laptops are even hated by internal employees and their quality is so poor that Consumer Reports had to pull their recommendation. Even worse, they spend more on marketing than R&D, $14.7 billion vs. 12 billion in 2016, which essentially means that they’re a marketing company with a software problem. So what makes them so brilliant? Their partner network.
To those unaware, Microsoft’s partner network consists of consulting firms, software resellers, and the like and is responsible for 95% of Microsofts commercial revenue; making them ol’ Softy’s predominant customer. Most partners offer a wide range of services surrounding the Microsoft stack, including but not limited their implementation, management, and support (IMSs). But, they don’t stand behind Microsoft products because they happen to provide the least defective or most efficient solutions. Instead and on top of having 19% higher profit margins than their nearest competitor, for every $1 of revenue that a partner firm steers towards Microsoft, the firm stands to earn an additional $9.01 through their IMSs at the expense of their clients. For example, if Microsoft products were a $50,000 car that a company depended on, it would come attached with $450,500 in subsequent maintenance costs.
However, the businesses that are trusting Microsoft partners are paying significantly more than the cost of their IMSs, which are less often than half of the story with regard to IT finance. Even in a well-managed environment, productivity loss is almost always the largest IT expense for businesses, regardless of their size. After productivity loss, IMSs come in as the second most costly with hardware/software costs coming in third. Simply put, while the $50,000 car may cost them $450,500 directly through maintenance, it would also see productivity loss ≥ its maintenance costs because of the inherent downtime, increasing the overall financial burden on the company to a minimum of $960,000.
Unfortunately, what is profitable for a typical consulting firm comes at the expense of their clients and there are few situations where Microsoft offers the most economical or productive solution than their competition. In a blog post to their partners, Microsoft even suggested that they “create stickiness” with their solutions, which is slang for “creating profitability” and I have been all but forced to accept that their own products are created with this same mindset. Because of this ploy to make things more profitable, even companies like IBM, the inventor of the PC, have migrated to Apple products after realizing that their Total Cost of Ownership (TCO) was reduced 1/3 that of a Windows PC, which is significant.
“Creating stickiness is all about looking for opportunities to entrench your (our?) solutions and increase switching costs.” -Jen Sieger, Sr. Business Strategy Analyst @ Microsoft
It shouldn’t be difficult to see that how defective or “sticky” an IT solution is relative to how costly it is to implement, manage, and support. As IT solutions become less defective over time, the cost and frequency of their IMSs would also decreases. Conversely, as IT solutions become more defective over time, the cost and frequency of their IMSs increases, becoming more profitable for partner firms. For software companies and just as software becomes more defective over time, it also becomes more expensive to market just the same; re: marketing company with a software problem. I digress, but it is easy to see how partner firms can profit immensely off of defective software solutions while minimizing less profitable and less defective solutions that generate less necessity for their services, not to mention how a conflict of interest can emerge between Microsoft and its partners because of this dynamic between them.
Rather than putting their clients first and aligning their business model with their best interests, Microsoft partners seem to be in complete alignment with Microsoft for obvious reasons; hence the designation of “partner”. Seemingly, in exchange for Microsoft “creating stickiness” or shipping turds if you’re into that whole brevity thing, their partner firms generate the majority of their revenue through their IMSs which Microsoft products generate the most revenue for. Combined with having 19% higher resale profit margins, Microsoft Partners have a significant incentive to stand behind Microsoft products while also also fending off or ignoring competing solutions, regardless of the potential impact to their clients.
Because of this, Microsoft actually seems to have more of an incentive to maintain an ideal amount of “stickiness” or “profitability” for their partners at the expense of their clients and can accomplish this by simply controlling how defective their software is. More defects lead to more downtime, more downtime leads to increased IMSs and productivity losses while consequently becoming more profitable to their partners. Even though Microsoft can’t directly go into their code and create bugs intentionally, they can absolutely impact how defective their code is. On top of laying off a significant amount of their QA and SDET resources which will have an obvious effect on how defective their software are, Microsoft can also tune their defect density by limiting employee headcount elsewhere, which increases the volume of work relative to their employees along with the likelihood of a mistake. Such practices are much more plausible for Microsoft since they also have strong vendor workforce which can pad their FTEs from the attrition.
In a way, Microsoft has found a way to profit off of defective software, which is brilliant in itself and you have to give them credit where it is due. Even though such a relationship dynamic between Microsoft and their partners may actually be a violation of the Sherman Antitrust Act if they’re doing this intentionally, it would still be a brilliant crime if that were the case. They may be the Buick of software companies and their logo may look like painted Borg cubes doing their best Voltron impersonation, but they are anything but stupid and you would be wise to remember that they do not generate almost $100 billion dollars per year by accident.
“High-quality software is not expensive. High-quality software is faster and cheaper to build and maintain than low-quality software, from initial development all the way through total cost of ownership.” -Capers Jones
Click here for the second part of this series
#BlackLivesMatter
1K 
5
1K claps
1K 
5
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@pkerrison/pizza-as-a-service-2-0-5085cd4c365e?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
Paul Kerrison
Sep 1, 2017·3 min read
Recently I was trying to describe the various types of cloud services available for modern IT deployment. Like many, I resorted to an analogy — the ever popular “Pizza as a Service”.
"
https://medium.com/6d-ai/dawn-of-the-ar-cloud-1b31eb4b52ac?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Since Apple’s WWDC conference this time last year, which fired the starting gun for consumer AR with the launch of ARKit, we’ve seen every big platform announce an AR strategy: Google’s ARCore; Facebook’s camera platform; Amazon Sumerian; and Microsoft continuing to build out its Mixed Reality ecosystem. We’ve also seen thousands of developers experiment with AR Apps but very little uptake with consumers. Back in Sept 2017, I predicted that AR Apps will struggle for engagement without the AR Cloud, and this has certainly turned out to be the case. However we are now witnessing the dawn of the cloud services which will unlock compelling capabilities for AR developers, but only if cloud providers get *their* UX right. It’s not about being first to market, but first to achieving a consumer grade UX.
Does anyone remember AR before ARKit & ARCore? It technically worked, but the UX was clunky. You needed a printed marker or to hold & move the phone carefully to get started, then it worked pretty well. Nice demo videos were made showing the final working experience which wow’ed people. The result…. zero uptake. Solving the technical problem (even if quite a hard technical problem) turned out to be very different to achieving a UX that consumers could use. It wasn’t until ARKit was launched that a “just works” UX for basic AR was available (and this was 10 years after Mobile SLAM was invented in the Oxford Active Vision Lab which Victor Prisacariu, my 6D.ai cofounder, leads).
We are entering a similar time with the AR Cloud. The term came about in a September 2017 conversation between Ori Inbar and I as a way to describe a set of computer vision infrastructure problems that needed to be solved in order for AR Apps to become compelling. After a number of early startups saw the value in the term (and more importantly the value of solving these problems), we are now seeing the largest AR platforms start to adopt this language in recognition of the problems being critically important. I’m hearing solid rumors Google won’t be the last $multi-billion company to adopt AR Cloud language in 2018.
Multi-player AR (and AR Cloud features in general) has the same challenges as basic 6DoF AR. unless the UX is nailed, early enthusiast developers will have fun building & making demo videos, but users won’t be bothered to use it. I’ve built multi-player AR systems several times over the last 10 years, and worked with UX designers on my teams to user test the SLAM aspects of the UX quite extensively. It wasn’t that hard to figure out what the UX needed to deliver:
So… putting aside all the application-level aspects of a multi-player UI (such as the lobby buttons & selector list to choose to join the game), the SLAM-synch piece isn’t just a checkbox, it’s a UX in and of itself. If that UX doesn’t deliver on “just works” then users won’t even bother to get to the app-level a second time. They will try once out of curiosity, though….Which means that market observers shouldn’t pay attention to AR app downloads or registered users, but to repeat usage.
Enabling developers to build engaging AR Apps is where AR Cloud companies need to focus, by solving the hard technical problems to enable AR-First apps that are native to AR. This means (as I have learnt painfully several times) that UX comes first. Even though we are a deep-tech computer vision company, the UX of the way those computer vision systems works is what matters, not whether they work at all.
At Google I/O last week, Google announced an update to ARCore 1.2, which included a handful of new features, the most notable was support for AR Multiplayer via technology called “Cloud Anchors”. Everyone registered the headlines… “Multiplayer” and “IOS support” but there wasn’t any discussion (or even any real demo) of the Cloud Anchor UX (there were some nice looking demos of multiplayer running *after* the setup UX was completed… funny that).
So how does Google do AR Mutiplayer? (Note: I don’t have any special insight into the algorithms Google is using, however I’ve built AR multiplayer systems in the past, and at a high level the steps are clear & well known. It’s deep in the relocalization algorithms themselves where the advances are taking place). These are the high level steps involved:
So far so good. There’s nothing really surprising or technically impressive here. In fact, Google has been able to do this for several years via Tango’s ADF files (which are a form of Anchor), though it was a manual process.
One thing that really stood out to me, having worked on these problems for so many years, is that Google didn’t talk about the Multiplayer UX at all. Only that the technology for Multiplayer exists. Demonstrating an understanding of why devs & end users have struggled with multi-player would have allowed them to show how those UX problems have been solved vs the technology problems.
It’s important to point out that I can’t claim to be completely impartial wrt this write up as my startup 6D.ai is building a similar service .
After spending a couple of days using Google’s Cloud Anchors, and talking with other experts who did their own testing, we were able to get a good handle on the UX, and the limits of the system
The first UX challenge, and by far the biggest one for anyone working on building solutions to these problems, is that there is no pre-existing map data for the scene, and thus “Player 1” needs to pre-scan the scene to gather image data in order to build an anchor.
Here’s what we learned:
Even after this experimenting, there’s still a couple of things we don’t know:- exactly what data is passed up, and why does it need to be discarded? Google is carefully vague here. A journalist friend described it as “twisting themselves into a privacy pretzel”.- what about China or private premises (like a military base)? Google cloud services are unavailable in China, and Cloud Anchors seem to depend completely on accessing Google’s cloud (ie no offline mode).
I’m also curious around what this “100% cloud” approach portends for the future direction of AR Core, as persistence & occlusion & semantics move closer towards public release in the next couple of years.
When it comes to Google’s Cloud Anchors, visual image data is sent up to Google’s servers. It’s a reasonably safe assumption that this can potentially be reverse engineered back into personally identifiable images (Google was carefully vague in their description, so I’m assuming that’s because if it was truly anonymous they would have said so clearly)
For the future of the AR Cloud’s ability to deliver persistence & relocalization, visual image data should never leave the phone, and in fact never even be stored on the phone. My opinion is that all the necessary processing should be executed on-device in real-time. With the users permission, all that should be uploaded is the post-processed sparse point map & feature descriptors which cannot be reverse engineered. An interesting challenge that we (and others) are working through is that as devices develop the ability to capture, aggregate & save dense point clouds, meshes and photorealistic textures, there is more & more value in the product the more “recognizable” the captured data is. We believe this will require new semantic approaches to 3D data segmentation & spatial identification, in order to give users appropriate levels of control over their data, and is an area our Oxford research group is exploring.
Here’s what a sparse point map looks like for the scene above (note our system selects semi-random sparse points, not geometric corners & edges, which cannot be meshed into a recognizable geometric space)
The second piece of the puzzle is the “feature descriptors” which are saved by us & also Google in the cloud. Google has previously said that the Tango ADF files, which ARCore is based on, can have their visual feature descriptors reverse engineered with deep learning back into a human-recognizable image (From Tango’s ADF documentation — “it is in principle possible to write an algorithm that can reconstruct a viewable image”). Note I have no idea if ARCore changed the Anchor spec from Tango’s ADF enough to change this fact, but Google has been clear that ARCore is based upon Tango, and changing the feature descriptor data structure is a pretty fundamental change to the algorithm.
This is critical because for AR content to be truly persistent, there needs to be a persistent cloud-hosted data model of the real-world. And the only way to achieve this commercially is for end-users to know that that description of the real world is private and anonymous. Additionally I believe access to the cloud data should be restricted by requiring the user to be physically standing in the place the data mathematically describes, before applying the map to the application.
This reality regarding AR Cloud data creates a structural market problem for all of today’s major AR platform companies, as Google and Facebook’s (and others) business models are built on applying the data they collect to better serve you ads. The platforms such as Apple & Microsoft are silos, so won’t offer a cross-platform solution, and also won’t prioritize cloud solutions where a proprietary on-device P2P solution is possible.
The one factor that I had underestimated is that large developers & partners clearly understand the value of the data generated by their apps, and they do not want to give that data away to a big platform for them to monetize. They either want to bring everything in house (like Niantic is doing) or work with a smaller partner who can deliver technology parity with the big platforms (no small ask) and who also can guarantee privacy and business model alignment. AR is seen as too important to give away the data foundations. This is a structural market advantage that AR Cloud startups have, and is an encouraging sign for our forseeable future.
As ARKit announced the dawn of AR last year, we believe Google’s Cloud Anchors are announcing the dawn of the AR Cloud. AR Apps will become far more engaging, but only if AR Cloud providers deliver a “just works” computer vision UX and address some challenging & unique privacy problems.
At 6D.ai we are thinking slightly differently than Google (and everyone else to be honest). We believe that persistence is foundational, and you can’t have persistence without treating privacy seriously. And to treat privacy seriously it means that personally identifying information cannot leave the device (unless explicitly allowed by the user). This creates a much harder technical problem to solve, as it means building & searching a large SLAM map on device, and in real-time. This is technically easy-ish to do with small maps/anchors, but very very hard to do with large maps. Where small means 1/2 a room, and large means bigger than a big house.
Fortunately we have the top AR research group from the Oxford Active Vision Lab behind 6D.ai, and we built our system on a next-generation relocalizer algorithm, taking advantage of some as-yet unpublished research. The goal for all of this was to get multi-player and persistent AR as close as possible to a “just works” user experience, where nothing needs to be explained, and an end-users intuition about how the AR content should behave is correct. There’s no special “Host/Resolve” steps or “manually enter a room number” or “trust us, your data is personally identifiable, so we throw it away…. but we need it for the system to work…”
Here’s what’s special about how 6D.ai supports maps/anchors for multi-player and persistence:
6D.ai’s relocalization algorithm, to the best of our knowledge from our Oxford Research Lab, cannot have its map or feature descriptors reverse engineered into identifiable visual data based on any research available today. We will continue to ensure our data structures are updated as new research comes to light. Even if you hacked into our system and figured out our algorithm then applied huge compute resources to reverse engineer the data, the best you would get would be the image way above of the circular feature descriptors (which correspond to the desk scene also above). In addition, the source wifi network ID is encrypted on the phone and only the encrypted info is stored, so the hacker at best gets the point cloud & these feature descriptors but has no way of telling where on earth it corresponds to. This also means that we can’t determine the images used to construct the 3D cloud data, even if we wanted to or were requested to by a govt etc, as the 6D cloud never sees the source images or has any way to reconstruct them.
I’ll be honest, when I heard that Google would be launching multiplayer, I had some fears (founder paranoia). It was the first-time some of the hypotheses we had founded 6D.ai upon were going to be tested. Would we have better technology, better focus on developers needs and a better understanding of the desired end-user experience? I’m pleased to say on all those fronts we are looking good.
Obviously, Google has distribution power & it’s a default option for Android devs. But the biggest market problem right now is that developers don’t know about AR multi-player at all, and Google has an ability to invest in educating the market that no startup can beat. We expect that once devs start testing multi-player and start investing real time & money into multi-player apps, then they will see how Cloud Anchors come up short, and see how 6D.ai solves their problems.
6D.ai is building advanced APIs for the AR Cloud. Our closed beta is rapidly taking on new developers.
Building the 3D Map of the World
790 
6
790 claps
790 
6
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dabit3/introducing-the-new-aws-amplify-cli-toolchain-238157905c00?source=search_post---------84,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nader Dabit
Aug 27, 2018·6 min read
AWS Amplify has released a brand new CLI Toolchain that makes it easy to create & configure AWS cloud services on the fly directly from your local environment.
The CLI will allow you to add features like authentication, GraphQL APIs, Lambda functions, web hosting, analytics, storage & more to your existing app without leaving your command line.
"
https://m.subbu.org/cloud-optimization-circus-65f3b47d0c79?source=search_post---------85,"If you are a cloud adopter rapidly adopting cloud services, but not developing the finance governance muscle, you will certainly be visiting the cloud optimization circus frequently.
I compare cloud optimization exercises to going to a circus because those exercises invite all the same characters and emotions that you find in a circus. There is fear (of wasting money), trickery (by folks showing you how much you could be saving), illusion (of savings that don’t exist where you’re told they exist), excitement (of finding savings), and drama (of playing heroics). It may be fun and entertaining once or twice. Not so when you’ve a mission to accomplish, unless, of course, the mission is going to the circus.
Security and costs are the two biggest risks of cloud adoption. Security is a risk because, teams that optimize for agility on cloud tend to ignore security initially only to realize later. Cloud certainly gives the building blocks for security, but it is up to you to use the building blocks in the intended manner. Cost is the second on my list. Cloud is cheaper once you understand how cloud costs work, and develop the governance muscle. Cloud can be very expensive otherwise.
Back in February 2018, I gave a talk at the Container World Conference on Are We Ready for Serverless. One of the key themes of my talk was that serverless frameworks like AWS Lambda are the closest available today to ensure required supply of resources follow the demand for resources. Here is a hypothetical supply-demand chart.
Curve A shows the resource demand. This is the sum total of all resources required to run the business, which in this example varies during the day and the week. Curve B is the ideal supply and spend. In the best case, supply, and hence spend, closely follows the demand. This is possible with serverless frameworks. Curve C is what usually happens in cloud environments. Though supply varies due to auto-scaling and ephemeral usage, such as dev/test activities during the day tapering off over nights and weekends, it usually stays above the resource demand. Curve D shows the supply in data centers where it typically stays flat.
Let’s ignore serverless here. Though it is the most efficient and requires no effort to maintain the spend to tightly follow the demand, only a tiny fraction of total cloud workloads today run on serverless frameworks like Lambda. Serverless potential is yet to be realized at large, and each enterprise will have to carve out its own journey in the coming years.
Majority of cloud workloads today run on virtual machines followed by multi-tenant managed services including network and storage services. Though some managed services bill you for what you need and use, for the vast majority, the task of making the supply (C) to efficiently follow the demand (A) falls on development teams, an assortment of nascent tools, and mostly reactive practices.
However, the task of making the spend efficiently follow the demand is easier said than done. Cost consideration is usually an after thought as most cloud adopters’ early focus remains on speed of delivery and not cost efficiency.
Unfortunately, this topic does not get much attention in the cloud community. Cost worries are usually brushed aside with suggestions like “use auto-scaling”, “use spot instances”, “fix your automation to clean up”, or “turn off your machines when you leave work”. Look at conference talks, meetups and blogs — you will rarely hear about spend management practices, how to project costs, how to understand detailed billing data, how to maintain efficiency, best practices, failures, lessons learned etc. Consequently, most cloud adopters fail to realize the strongest lever that cloud offers — which is to manage the spend to vary with the demand. But such a lever won’t exercise by itself. You need to equip the organization with tools, practices and processes to actually do the work.
For enterprises migrating from traditional data centers to the cloud, spend management is a lever that they don’t have in the data center. In the data center world, you do your best to estimate what you need in a year or so from now, spend all that, and hope it meets the need. There is no turning back if you find yourself with spare capacity. This is why most tech teams operating in traditional data center environments consider data center resources as free for all practical purposes. It would be a great missed opportunity to not deliberately practice efficient spend management as you ramp up on the cloud.
Over the last two+ years of leading cloud migration at work, I’ve had a chance to look at this area very closely, and take part in building a successful cloud finance governance engine to increase cloud spend efficiency. Let me share my observations and experience.
A RightScale post from November 2017 states that about $10 billion is wasted each year across AWS, Azure and Google. Another report by BusinessInsider from Dec 2017 proclaims that “companies waste $62 billion on the cloud by paying for capacity they don’t need”.
These are staggering numbers for sure. In my experience, we can’t quickly project such numbers at the enterprise level without producing a bottom up baseline through exercises like zero-based budgeting applied to every workload. These are time consuming activities involving testing every workload for the best price-performance ratio. Even predictions produced by tools like AWS Trusted Advisor and cloud cost dashboarding tools like CloudHealth fall short in reality as these tools lack the context of the workload. Consequently, most dev teams don’t often pay enough attention to these predictions.
Furthermore, detailed billing reports like the AWS Cost and Usage Report provide a wealth of detailed billing records. Here a few sample billing records.
Depending on your scale and activity, you might see millions of records like these every day. These records span over tens of resource types and product families, and tens of thousands of usage types. As new features are introduced, and as your adoption grows, the volume and detail, and hence the complexity of these records, also grows.
On one hand, having such a wealth of data shows the true power and potential of on-demand pay-per-use model of the cloud. This data can help you understand the implications of your architecture choices, be able to correlate workload patterns with costs, and make price-performance trade-offs. Insights from this data, when gained, can help bring cost awareness to the engineering culture.
On the other hand, most billing tools available today mainly focus on providing dashboards with high level metrics, but not many insights. For example, in one particular case, recommendations from Trusted Advisor showed significant potential savings in certain areas, while analysis of the raw billing data revealed much bigger opportunities elsewhere. The latter required deeper understanding of the billing data to spot inefficiencies.
Developing a deep and solid understanding of billing records is an engineering problem that consumes time and investments. It’s like understanding operating system level metrics. It is not optional to not understand such metrics. You’ve to build tools to process the data, visualize, and then derive insights. You can not also centralize all this to one particular tech team or a finance team, as you need every team spending on the cloud learn to gain their own insights. Such insights need to complement performance metrics to gain awareness of price for performance. This is why I believe that cost awareness must be part of the engineering culture, and it starts with developing an understanding of billing data.
Unlike other drivers like dev agility, availability, and security; cost related practices often tend to be reactionary. Cost concerns come to the front seat only when there is a sense of urgency to reduce cloud costs. Otherwise, cost concerns get left in the garage back home. Whenever there is a realization of cost increases beyond budgets, organizations scramble to conduct optimization exercises, and when the dust settles, go back to the business as usual.
Part of this is due to Problem 1 above, which is not looking at the billing data, and/or not gaining enough insights from the data, and thus not being able to incorporate cost awareness into the engineering culture. The remaining of it is due to the holy trinity of cost, speed, and quality.
In order to produce an outcome of a certain quality, at any given time, you can either move fast while spending more, or move slow and be efficient. You can’t maximize all the three at the same time. The key question to ask therefore is, how much cost inefficiency are you willing to tolerate for a given amount of quality and speed.
Though we hear about wastage on the cloud from reports like those I cited above, apart from a few “how we saved such and such by doing so and so” blog posts, we don’t hear much about building sustainable practices of spend management and governance; and most importantly real stories about failures.
There is a reason why. Most of us mentally equate having to optimize cloud spend to the business not being healthy. We compare it to other usual cost cutting measures that most companies take at various points in their cycles, such as letting people go, avoiding business travel, reducing discretionary spending etc, shutting down offices etc. Though more experienced managers and leaders see these as natural acts of cost governance, common perception remains otherwise. A shadow of stigma follows cost optimization.
However, most successful companies build cost governance into everything they do, whether it is hiring, business travel, discretionary spending, or technology related spending. Cloud costs are no different. Acknowledging that cloud spend is a variable that you can manage, that you must maintain the spend at a certain efficiency, and removing the stigma from cost optimization are essential to building a culture of cloud cost awareness.
Wherever there is a stigma, there is FUD. I’ve heard stories of cost optimization practitioners in the wild that make claims like “we will show you how to save $XXX, just give us $Y”. One friend once shared a story of a consulting team proposing to optimize for a percentage cut of the savings realized. Do you remember “termination assistance” from Up in the Air (pun intended)?
Such approaches might make sense in places where cloud adoption is not strategic, and is treated as a utility, like a third party maintaining your corporate media web site. However, these approaches don’t produce sustainable results for anyone running serious workloads on the cloud. While not rejecting the need to seek help, you’ve to equip yourself with tools, automation and cultural changes. This is the philosophy behind DevOps — you make teams autonomous and hence accountable for development and operations for higher team performance. The same goes for cloud costs too.
This brings me to commonly dreaded term “governance”. Instead of treating cost optimization as a necessary evil, what we need is a practice of cloud finance governance and operations. Optimization is a part and parcel of governance. Here is how I describe cloud finance governance.
Cloud finance governance is pushing for responsible spending practices, and introducing checks and balances. It is about learning to operate spend management levers to trade between between speed, cost efficiency, and sometimes even quality.
Governance is not a bad word. Governance is not bureaucracy. Governance is not introducing roadblocks. When done right, governance is empowering, rewarding, and helps us exercise new muscles. Governance is what responsible families, cultures, societies and businesses must do in order to be adaptable and be resilient.
While prescribing a general purpose blueprint for how to practice cloud finance governance is tricky as each organization needs to determine what works best for them, below are some of the essential building blocks.
You can’t govern and optimize what you can’t measure. I was in situations with charts showing large unallocated cloud spend on a big screen in front, and struggling to explain where that money was going. You can’t optimize, let alone govern, if you don’t know who is spending what. Resource attribution to people and teams is fundamental to operating successfully on the cloud for cost as well security reasons.
There are several techniques to consider to maintain high percentage of attributed costs:
The next step is to gain insights from the billing data. Insights aren’t easy and automatic. I approach this step by first observing cost and usage data across different dimensions (time, regions, accounts, dev/test/production environments, resource types, usage types, instance types, allocated vs unallocated costs, etc), asking questions, making hypothesis, and validating those hypothesis. This is an iterative process over time.
In order to do these, you need access to the raw billing data, stored and indexed in a form that allows fast and easy queries. At work, we built a data warehouse using Redshift and ElasticSearch for billing data. This system loads raw billing data as soon as it lands in S3, merges it with the metadata of people and applications, and loads into an ElasticSearch cluster for queries and visualizations. This process helped us several times to improve our overall understanding of costs, efficiencies and inefficiencies, and areas of improvement.
While we like to automate everything, in reality, automation is never complete, and the degree of completeness varies by what you’re optimizing your automation for.
You may, for example, optimize for speed and availability, and decide to leave older deployments for a week or to allow for rollbacks. You may optimize for performance for your analytics workloads and decide to keep all your offline data in the S3 standard access class, and run the compute on pricey instance types. You may have a bug in your automation that forgets to propagate tags from EC2 to EBS, thus increasing unattributed costs. Your teams may have forgotten to upgrade some legacy EC2 instances that may be pricier for the same performance. I’ve seen all such scenarios and more that lead to waste.
You can improve hygiene by crafting policies (such as “all unattached EBS volumes shall be deleted after 48 hours”), and then automating those policies.
Remember that cloud spend is not a fixed sunk cost. It is a variable expense that can you manage. There are several levers possible:
Furthermore, if you’re still running in the hybrid mode with some apps serving traffic both in your data centers and the cloud, another lever may be to shift traffic one way or the other to balance between variable cloud costs and fixed data center costs.
Forecasting is another important aspect of developing a finance governance process, particularly for those moving workloads from the data center to the cloud, or those building new systems. During such phases, cloud spend tends to increase at a higher rate than in the steady state. Forecasting is less reliable during such phases as you may not have past data to build forecasting models. Regardless, you can correct for this by forecasting more frequently — ramp up some volume of traffic, build models for forecasting, forecast, then ramp up more. Also read Cloud and Finance — Lessons learned from a year ago on this topic.
Forecasting is what you expect to spend in future based on your cloud adoption plans, your team velocity, and the architectures your team is building. The budget tells you how much is being set aside for that area of spend. The difference should tell you how to tweak the plans, architecture, and levers you can exercise to meet the budgetary goal. Usually finance teams determine your budget.
Lastly, incorporate all cost related metrics, and insights into your periodic operational reviews. Most teams use such rituals to review overall KPIs of applications, and the status of projects the teams are working. Add cost related topics to the same. This is a place to observe billing data, to ask questions to develop better understanding of the data, to identify ambiguous areas, and to keep on improving the governance muscle.
To reiterate, cloud gives you many levers to manage costs. Discovering and exercising those levers requires thinking of how to govern cloud costs, and building the automation and processes to develop insights into costs, creating spend management levers, and knowing how to make cost vs speed vs quality tradeoffs.
A blog on tech and leadership
468 
6
Some rights reserved

468 claps
468 
6
Written by
See https://www.Subbu.org
A blog on tech and leadership
Written by
See https://www.Subbu.org
A blog on tech and leadership
"
https://medium.com/@davidmytton/aws-vs-google-cloud-flexibility-vs-operational-simplicity-dca4324b03d4?source=search_post---------86,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Sep 26, 2015·5 min read
I wanted to revisit a theory I first wrote about in my 15 April 2015 e-mail newsletter — how the approach of Amazon Web Services is different from Google Cloud Platform — and add that to my theory on how containers are core to Google’s Cloud Platform strategy.
On the surface, AWS and GCP are very similar, but their approach to product design is actually quite different:
"
https://medium.com/hackernoon/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99?source=search_post---------87,"There are currently no responses for this story.
Be the first to respond.
At Datawire, all of our cloud services are developed and deployed on Kubernetes. When we started developing services, we noticed that getting code changes into Kubernetes was a fairly tedious process. Typically, we had to:
We first automated these steps, but the latency introduced for a two line code change was still annoying (especially for those of us who were used to live reload of interpreted languages).
So we took a step back and asked ourselves what would we like the development process to look like? We came up with two answers. First, we wanted consistency in environments between development and production. And second, we wanted zero latency when testing code changes during development.
We had experienced the pains of trying to figure out why a service that works in development doesn’t work in production or in our continuous integration system. Inevitably, these pains come down to environmental differences. We were keen to create environmental consistency to minimize the chances of this happening.
Luckily, containers provide a great solution for this problem. We create a standard Docker image that is used for both development and production. This Docker image contains all the dependencies necessary to run the service. The Docker client also lets us mount a local filesystem into the container, which lets us edit code using our favorite editor, while running it in the container.
This approach gives us a fast feedback cycle during development, while creating consistency between different environments. Any developer working on the service is able to use the same image, which is also the same image that is run in production.
We loved the container approach for fast development. However, some of our services depend on other running services, and we wanted a way to develop multi-container applications as well.
We first started by experimenting with minikube, but decided it wasn’t a great fit because the container deployment process still added latency. Moreover, minikube required a substantial amount of RAM for some of our services (e.g., ones that required a JVM).
We also looked at Docker Compose, which was easy to try since we were already using containers. We decided not to use Compose because it fundamentally introduced a different runtime environment for our application (Docker) than production (Kubernetes/AWS). This meant we had to maintain two different environments for development and production. This problem became even more acute when we started to factor in applications we run in the cloud (e.g., AWS RDS).
We then experimented with a networking-oriented approach. We were already familiar with port forwarding as a way to access applications in a cluster, so we asked ourselves if there was a way to expand on this concept. We just needed to figure out a way for the local service to access the Kubernetes cluster, and vice versa.
We implemented this concept in Telepresence, which we open sourced early this year. Telepresence substitutes a two-way network proxy for your normal pod running in the Kubernetes cluster. This pod proxies data from your Kubernetes environment (e.g., environment variables, secrets, ConfigMap, TCP connections) to the local process. The local process has its networking transparently overridden so that DNS calls and TCP connections are routed through the proxy to the remote cluster.
Here’s an example. Clone the following repository:
$ git clone https://github.com/datawire/hello-world-python
This repository contains a simple Python application using the Flask web framework:
It also contains a Dockerfile that specifies how to build the runtime container:
Let’s build the development environment locally:
$ cd hello-world-python
$ docker build -t hello-world-dev .
Get the service running in Kubernetes (we’re using the Datawire image so you don’t have to push to a Docker registry):
$ kubectl run hello --image=datawire/hello-world-python --port=8080 --expose
Now, let’s test this service out. In another terminal, let’s start a pod on the Kubernetes cluster to talk to the service.
Normally, when you’re coding this service, you have to go through a process of building your container, pushing it to the registry, and redeploying. Let’s see how this works with Telepresence. Make sure you’re in the hello-world-python directory, and type:
$ telepresence --swap-deployment hello --docker-run --rm -it -v $(pwd):/service hello-world-dev:latest
This command does three things:
We can test this out by making a change to app.py. Open app.py in your preferred editor, and change the “Hello, World” string to anything you’d like. Now, rerun the wget command from remote Kubernetes pod:
And there you have it: you edit your code locally, and changes are reflected immediately to clients inside the Kubernetes cluster without having to redeploy, create Docker images, and so on.
If you use a server that supports auto reload, Telepresence makes this feature useful again — you can edit your server code, save, and immediately test the functionality.
Telepresence has simplified our coding cycle. We’ve made it open source and created OS-native packages for Mac OS X and Linux. We’d love for you to try it out and see if it makes your life easier. For more information, visit https://www.telepresence.io.
#BlackLivesMatter
400 
3
400 claps
400 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CEO, Ambassador Labs. Makers of the Kubernetes Developer Control Plane, CNCF Telepresence, and CNCF Emissary Ingress.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@gercheq/how-500-000-microsoft-azure-sponsorship-might-kill-your-startup-42912f9b22a1?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gercek Karakus
Aug 6, 2018·6 min read
At Raklet, we’re building automated messaging and payments solutions for communities. We got accepted into Microsoft Ventures Accelerator in April 2016 and we got offered $500,000 Azure sponsorship for the 3 years. Yay!
We were really happy to receive the credit as server costs were one of the major costs at that time. We started migrating our infrastructure from a monolithic architecture to a more scalable modular one. We’ve setup local, dev, test, prod resource groups; scaled up our app services and databases. Everything was running much faster, finally!
Then, fast-forward 24 months, I realized that our monthly server costs ramped up above $20,000/month. Unbelievable.
At that point, we only had 12 months left on our sponsorship and we’d run out of credits even before the expiration deadline. So it was time for us to focus on infrastructure optimization instead of growth.
Billing for Microsoft Azure Sponsorship is not on the portal so you have to visit an external website for the usage.
Visiting the https://www.microsoftazuresponsorships.com website, you can only generate an excel sheet similar to the one below:
ResourceIDs and names in this sheet do not match to the ones on Azure portal. So we decided to reach out to the Professional Direct Services. After spending 2 months back and forth with them, they simply gave up and we were left with this problem. There’s no way for Microsoft Azure Sponsorship usage to be tied to actual resources on Azure Portal.
I hope this email finds you well. Please note that my team, the Startups Business Desk, has been closely following the congruent support request #118041818024304. As stated in that support request, it is a technical limitation of Azure Sponsorship reporting that we are unable to provide usage by resource group. We understand that this is an important feature and heavily weighs on your continued use of the platform and apologize that we cannot provide a workaround at this time. Please note that engineering is working to implement reporting by usage group in the future. However, we do not have an ETA. — Microsoft Azure Team
We were left with the excel sheet to figure out what was going on with our billing. So, we built pivot tables to figure out what was going on with our account.
We downloaded the excel file and became a pivot table master. With some tricks, we were able to identify break down of each service such as name, type and resource.
Sounds like the ultimate solution but again, ID or names in this sheet do not map to your resources on Azure.
We were still not able to identify which resources were being used for different resource groups (local, dev, test, prod) out of our 250+ resources. So we had to drill down into each resource to find our way to optimization.
We were able to drop down to $7.5k/month in May and $4k/month in July. Below you can see the breakdown for July 2018.
Let me summarize what we have done for each service type and how much it helped:
With a little bit educated guess work, we can categorize resources to different groups (again in a sheet) and see that our daily costs are around ~$120/day as of August 1st, 2018.
We’re lucky to identify this issue with Azure billing with sufficient time in our hands so that we can prepare accordingly.
We’ve always postponed optimizing our infrastructure in order to focus on growth which is pretty much understandable startups searching for product-market fit. However, years pass by with the blink of an eye and credits expire much faster than anticipated.
Our goal is to drop our costs to $1,000 per month and we’ll see how far we can go…
Thanks for reading this far and please spread the word 🙏
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
Product Architect @Raklet
524 
8
524 
524 
8
Product Architect @Raklet
"
https://medium.com/boostnote/cloud-syncing-backups-5b138f30e1dc?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
It’s easy to sync via Dropbox, GoogleDrive or any file-based cloud services.
Just put your storage to your local folder for syncing.
You can also include your existing storage at Preferences.
Preferences -> Storages -> Add Storage .
You guys can sync data among desktop with mobile app. Introducing article is here.
https://medium.com/boostnote/boostnote-mobile-how-to-synchronize-with-dropbox-95d845581eea
Enjoy Boostnote 😆
We are waiting for your Pull request!
Develop as one, grow as one
232 
3
232 claps
232 
3
Written by
Develop as one, grow as one. Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Written by
Develop as one, grow as one. Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.reactiveconf.com/5-reasons-why-javascript-is-eating-the-world-bbc4aca0a527?source=search_post---------90,"JavaScript is eating the world. Web apps, mobile apps, cloud services, wearable computing, IoT devices — they all rely more and more on JavaScript. Today, understanding JavaScript is vitally important to everyone in IT.
Let’s explore some of the reasons behind this tremendous success of JavaScript.
Programmers build web frontends in React.js, mobile applications in React Native, and desktop apps are handled by Electron. Do you need a high concurrency backend? Are you experimenting with a microservices architecture? Or just building a simple REST API? Then Node.js is for you. Did you know you can write JavaScript for microcontrollers? JavaScript is a universal language.
Having your entire codebase in JavaScript has great benefits. You can reuse a lot of code. You can use the same stack on all your projects, same package manager, linter, task runner, module bundler, IDE, libraries, and all other tools and services you can think of. You don’t have to spend time with solving the same problems over and over again on each new platform.
Great tech teams often consist of T-shaped, full-stack developers because full-stack developers make life easier for those around them. Full-stack teams communicate better and learn faster because they understand each other. It is easier to be full-stack if all your stack is based on JavaScript because you can learn once and then write anywhere.
JavaScript came with a lot of modern features back in 1995 when it was first released. It shipped with dynamic typing, garbage collection, closures, built-in hash maps, and functions as first-class citizens. All the things we expect from modern programming language today, but they were rare 20 years ago.
Unfortunately, JavaScript was misunderstood and earned a bad reputation that stuck with it for a long time. A few really bad design decisions played a part in this, but fortunately, most of them are solved by ES6 and the proper use of ESLint.
When Sebastian McKenzie created Babel, it catalyzed JavaScript’s evolution. New JavaScript features now can be used before browser vendors implement them. This is huge because it means we don’t have to wait several years to benefit from cool new features like destructuring and generators.
There are almost 300,000 npm packages with more than 400 being added every day. This makes npm the biggest and the fastest growing package repository in the world. If you are looking for some library, tool, or DB adapter, odds are somebody has already put it on npm.
Today every service for programmers has built-in support for JavaScript, whether it is AWS Lambda, New Relic, or some new shiny startup making deployments easy.
JavaScript became assembly language for the web. Because of its ubiquitous support, new languages like Elm or ClojureScript choose to transpile to JavaScript and run in a browser or Node.js.
Then there are real cool language extensions like Flow, JSX, or TypeScript that add syntax to JavaScript in a modular way. For example, Flow and TypeScript add optional typing system into JavaScript while JSX simplifies writing React components.
JavaScript is becoming the single platform that abstracts away the differences between various systems. Interestingly, it is the very same goal Java used to pursue with JVM back in the 90s.
JavaScript used to be very slow. This has changed with the V8 release in 2008, which improved the JavaScript performance tenfold! Today’s JavaScript code is only 2–7 times slower than corresponding C++. This makes JavaScript the fastest dynamic language out there.
Actually, server-side JavaScript has gained lot of attention because of its performance. For example, PayPal has increased the requests per second by 100% when they migrated to Node.js from Java. If your server is handling thousands of simultaneous connections, you can’t create a new thread for each of them. This is why asynchronous non-blocking nature of JavaScript makes it a great fit for high concurrency environments.
It is easy to debug JavaScript in a browser. Just hit F12, open dev tools, and start playing around. Write some code to the browser console and get results back immediately. Inspect the state of the program, simulate some function calls, trigger events.
You can even change the source code of your app while it keeps running. This feature is called hot reloading, and it rapidly shortens the developer’s feedback loop. Short feedback loops improve productivity and make developers happy. Happy developers are more productive, and all this gained productivity enables your product to move fast.
JavaScript is a modern language. Maybe it is not as cool as Clojure, Elm, or OCaml, but it is becoming the universal language of the programmers. You can’t avoid it, so you better master it.
Many thanks to @andrestaltz, @mweststrate, and @eveporcello for reviewing this article!
Are you just as excited about ReactiveConf 2017 as we are? Then buy a ticket now! It will cost you only €499 until July 15. The ticket will grant you access to all three days of the event.
Don’t forget to follow us on Twitter, Facebook, and Medium for exciting news and announcements.
– By Samuel Hapák, co-organizer of ReactiveConf
Bringing world-class mobile and web innovators to Prague…
184 
2
No rights reserved
 by the author.
184 claps
184 
2
Written by
Functional programming conference based in Central Europe with top-notch speakers and 1300+ attendees.
Bringing world-class mobile and web innovators to Prague, Czech republic to let them share unique insights.
Written by
Functional programming conference based in Central Europe with top-notch speakers and 1300+ attendees.
Bringing world-class mobile and web innovators to Prague, Czech republic to let them share unique insights.
"
https://medium.com/@drgutteridge/whats-the-deal-with-encryption-strength-is-128-bit-encryption-enough-or-do-you-need-more-3338b53f1e3d?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Gutteridge
May 6, 2016·4 min read
There are a lot of cloud services that tout encryption strength as a measure of how well they guard your data. It is quoted in bits, which is the size of the key. So you see services quoting 128 bit, 256 bit or even 2048 bit.
What about these numbers​? Surely 256 is better than 128, and 2048 is even better yet.
What does it all mean, and how much do you need?
Encryption on computers uses the same principle as encryption of messages over the ages. To conceal information someone scrambles (encrypts) a message using a key. The key could be any text. Before computers it was often an agreed-on passage from a book. To descramble (decrypt) the message the key is used to reverse the encryption and arrive at the original message.
If you don’t have the key you are reduced to cracking the encryption by brute force. This means trying every possible key until you hit upon the right one.
In software, keys are usually a random string of characters. Each character is 8 bits. So, for example, 16 random characters is 128 bits. If you have a monster 256-character key, it is 2048 bits.
Now to crack modern computerized cryptography there are no short cuts. The encryption (scrambling the data) is done so that no extra information can be derived from the encrypted data. The only way to crack it is brute force.
So how long does that take? Can the NSA or other state actors crack all encryption? How many bits of encryption make your data secure?
First thing is, don’t panic. The claims made about the capabilities of code crackers are usually exaggerated. As Edward Snowden said, ‘trust the math.’ Mathematics is a fact. The NSA is constrained by it as much as everyone. So we can take a look at the math and draw some conclusions.
One simple fact is that each bit you add to a key doubles the number of possible keys and hence doubles the amount of time it takes to try all possible keys, i.e. a brute force attack.
One of the faster performed computer tasks is done by bitcoin miners who use massive banks of specialized hardware to evaluate a hashing function 300 quadrillion times a second (that’s three hundred thousand trillion).
But how does the computer power to evaluate their hashing function compare to trying to see if a key is the right one for a set of data in the course of a brute force attack.
Well it is a lot harder to decrypt a block of text with a key than to do what the bit coin miners do, but in the spirit of erring on the conservative side, even though testing a key takes orders of magnitude more computing, we will assume that a state actor could test a million trillion keys a second.
Now if we look at how many possible 64-bit keys there are (multiply 2 by itself 64 times) we get about a million trillion. So that would mean that the maximum possible decryption speed available today would be able to brute force a 64 bit key in a second.
That might sound scary. If you have a 128-bit key, and a 64-bit key can be cracked in a second, surely the 128-bit key will be vulnerable.
Not so fast. There are around 32 million seconds in a year. 32 million is 25 doublings. So if you can crack a 64-bit key in a second it will take a year for an 89-bit key (64 + 25). A million is 20 doublings, so an 109-bit key will take a million years.
Your 128-bit key is still 19 bits longer, which multiplies the time by 500,000. So to crack a 128-bit key with modern hardware is going to take around 500 billion years.
Moore’s law says that computers get twice as fast every 2 years. In cryptography terms that means that advances in computer power will give you one extra bit every two years. That is, if you can crack a 64-bit key in a second this year, you should be able to crack a 65-bit key in a second 2 years later.
On that basis increases in computer power would bring the time to crack a 128-key down to one year 78 years from now and 128 years to bring it down to a second.
Given that our estimates are probably orders of magnitude better than what can actually be done we can conclude that 128 bit encryption is absolutely safe for the rest of the century from known technology.
So is there any technology that will speed up these attacks?
Many people point to quantum computing , claiming it will allow the decryption of long keys in incredibly short times.
However quantum computing is aimed at public-key cryptography which is another type of cryptography. Public-key cryptography is very important in that it is the technology that drives secure communications such as SSL used to secure websites and the digital signing of documents, but it takes much more computer time and hence is not used for encryption of whole documents. Whether quantum computing will ever be practical remains to be seen, but it is not something that would apply to the type of cryptography we are talking here.
Bottom line: If you or your service providers use 128-bit encryption you can relax — there are other things much more serious to worry about.
IF YOU LIKED THIS EXPLANATION CLICK HERE TO CHECK OUT MY BOOK THAT EXPLAINS WHY ENTERPRISE SYSTEMS FAIL — THE REASONS AREN’T WHAT YOU THINK
Dr. Lance Gutteridge has a PhD in computability theory. Presently CTO of Formever Inc. (www.formever.com) where he architects ERP authoring software.
323 
2
323 claps
323 
2
Dr. Lance Gutteridge has a PhD in computability theory. Presently CTO of Formever Inc. (www.formever.com) where he architects ERP authoring software.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/automation-generation/step-by-step-guide-to-run-a-simple-trading-algorithm-in-the-cloud-using-python-alpaca-and-aws-34c899b678b0?source=search_post---------92,"There are currently no responses for this story.
Be the first to respond.
It is always challenging for a new quant trader to get an algorithm up and running live in the cloud. In my last post, I wrote an instruction for using PythonAnywhere.
This walkthrough focuses on the basics of setting up AWS with Python, the Alpaca Trade API, and running a basic algorithm.
aws.amazon.com
docs.alpaca.markets
AWS may seem daunting at first, but is well worth the time to get comfortable with. The benefits of AWS include billing by usage and a free tier of cloud services, meaning you can get up and running with zero upfront cost!
Let’s jump in with a fresh account.
https://aws.amazon.com/
AWS will ask you for a credit card and phone number to verify your account, but you do not need to spend a dime to get started.
Go to the console sign-in.
When you sign in, “Launch a virtual machine” with EC2 on your front page. Click that.
(Or, search “EC2” and click the “Launch Instance” button.)
Click the “Amazon Linux 2 AMI” free tier eligible option.
Use the t2.micro free tier eligible and click “Review and Launch”.
You’ll get notified to set a security group if this is your first time create a new key pair and download the .pem file. You’ll need the path to this file later.
Click “Launch Instances”. Click the instance id link on the following page to go to your EC2 instances.
Your instance will take a bit to initialize, while it’s doing that, copy your public DNS.
Open a terminal window (I’m doing this from Mac OSX) and type the following:
My example:
(If you ever use a different image, your log-in may be ubuntu@[yourDNShere])
*Some troubleshooting may be required. In this case, my connection was timing out, so I searched “EC2 timeout” and found the solution. In my security group, I had to specify port 22 as inbound traffic. This was because I originally skipped the security group step above, by default your security group should include port 22 for inbound traffic.
The first time you try to connect, you’ll probably get “bad permissions” as a result:
All you have to do fix this is run:
Now run the command again. This screen means you did everything correctly:
Enter python -V to see what version of python you’re running. By default, I get 2.7. Run the following to install Python3.7:
Now your EC2 instance should be good to go! Keep this window up for later.
For this tutorial, we’ll be running the following sample algorithm. The strategy is a simple EMA crossover, checking a list of stocks every 1 minute.
Open a new terminal window, and run the following:
My example:
Go over to your original EC2 window, and run ls to check if the upload worked.
You should see your algorithm listed in the directory. Test it by running python3 apca_5min_ema.py.
In order to keep our algo running without quitting when we disconnect, we can use a handy Linux command, screen. Go ahead and run it.
This will pop up a new, blank terminal. It’s actually another window of your terminal. Now run python3 apca_algo.py.
Hit CTRL + A + D to detach the screen and return to your normal terminal.
Now you can type screen -ls to see your process is still running.
Typing tail apca_log.log confirms this as well, seeing the results in our log file.
Hit CTRL + D to logout of EC2 altogether.
Log into your Alpaca account, and confirm your orders are being placed.
To get back to your algo, log into EC2 with SSH.
Run screen -ls to see what screens are running. You should see something like:
That long screen id is what you’ll type in next to reconnect:
Alternatively, if you just want to quit the screen you can use ps aux | grep apca_5min_algo.py to see the process ID:
Then, you can use screen -XS [process id] quit to quit the screen immediately. My example:
Finally, confirm if you’ve quit your algo with screen -ls .
There you have it! This should give you a good starting point for spinning up new EC2 instances, navigating AWS, and managing your algorithms.
docs.aws.amazon.com
www.howtoforge.com
Technology and services are offered by AlpacaDB, Inc. Brokerage services are provided by Alpaca Securities LLC (alpaca.markets), member FINRA/SIPC. Alpaca Securities LLC is a wholly-owned subsidiary of AlpacaDB, Inc.
You can find us @AlpacaHQ, if you use twitter.
News and thought leadership on the changing landscape of…
334 
3
334 claps
334 
3
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Written by
Python, trading, data viz. Get access to samchaaa++ for ready-to-implement algorithms and quantitative studies: https://samchaaa.substack.com/
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
"
https://towardsaws.com/invoking-lambda-functions-locally-with-aws-sam-d406f7bed91a?source=search_post---------93,"Sign in
Kisan Tamang
Jun 15, 2020·4 min read
In my D2D work as a Back-End Engineer, I am quite intensively working on AWS Serverless. Serverless is a new concept in software development.
Serverless is a way to describe the services, practices, and strategies that enable you to build more agile applications so you can innovate and respond to change faster.
"
https://faun.pub/use-these-tools-to-optimize-your-aws-costs-38e6bb2404cf?source=search_post---------94,"This article was originally published on The Chief I/O: Use These Tools to Optimize your AWS Costs
"
https://levelup.gitconnected.com/system-design-idea-robust-streaming-data-processing-2e9224c33d3f?source=search_post---------95,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Streaming data processing, while providing benefits such as freshness and smoother resource consumption compared to their batch counterpart, has historically been associated with disadvantages like being unreliable and having approximate results. Those disadvantages, however, are not inherent characteristics of streaming data processing itself, but rather…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/migrating-to-the-cloud-without-screwing-it-up-ee8455ca0c9e?source=search_post---------96,"Sign in
There are currently no responses for this story.
Be the first to respond.
Victoria Drake
Oct 5, 2019·9 min read
For an application that’s ready to scale, not using managed cloud architecture these days is like insisting on digging your own well for water. It’s far more labour-intensive, requires buying all your own equipment, takes a lot more time, and…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dadi/dadi-partners-with-agorai-89951bafca6a?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
The democratisation of the internet is a founding principle of DADI cloud services and today we are proud to announce a partnership with Agorai — the blockchain-based marketplace offering fair access to artificial intelligence software and data assets.
Agorai’s mission is to ensure the benefits of AI are controlled not only by large organisations, but also by smaller businesses and individuals that would not otherwise be able to afford to leverage the technology. People from all over the world will be able to contribute their own AI applications, tools and data assets and receive payment based on the usage of their contributions.
The Agorai marketplace will run on the DADI network, plus we’ll work with their team to further develop AI capability within our technology. It’s a great fit for DADI, which also aims to bring the balance of computing power back to the people by offering cloud services free from the control of large corporations such as Amazon Web Services, Microsoft Azure and Google Cloud.
The announcement has been made today to coincide with a keynote speech by Agorai CEO Josh Sutton at the World Blockchain Forum in Dubai.
“Integrating with DADI will provide our participants with a much-needed choice of how they interact with our platform, keeping costs low, distributing compute power and ensuring anyone can access our platform to make use of data assets, or to access AI tools and applications,” he said.
We’ll be working closely with Josh and his team to support the Agorai marketplace as it moves towards launch later this year.
Written by Paul Regan. Paul is the Product Director at DADI.
Faster. Greener. More Secure. The future of the cloud is Edge.
332 
332 claps
332 
Upholding the founding principles of the Web through the democratisation of computational power.
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
"
https://medium.com/complete-coding/what-is-amazon-web-services-code-bytes-115e43929e77?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
Technology and computing is an increasingly large part of everyday life for most businesses. Whether it’s hosting a website, having an app or even building and running some custom software, our use of computing has become integral to our working life.
With this increasing use of tech comes the consequences: an increased need for servers, databases and the people to run and maintain them. This can become very expensive and you often end up with a sub-optimal solution due to cost, time or personnel limitations.
AWS provides a solution to these problems by allowing you to rent their servers and use tools that they’ve built. They have server warehouses in 15 locations across the world and 103 services that you can use.
Cost can be a huge limitation for lots of companies when looking to use servers, with servers often costing £40,000 and costing another £9,000 a year to maintain.
AWS works on the basis of you pay for what you use. That means that there’s no massive upfront cost and if you stop using it you stop paying for it. This means that you can try out a new server or technology without having to invest tens of thousands of pounds.
If you have an on-site server, or even have a hosted server, then you need to consider the maintenance, redundancy and security costs. If you don’t already have anyone who knows how to work with the system you may end up needing to hire a systems engineer, averaging £60,000 a year!
You also need to make sure that all of your data is secure, physically and digitally. These are costs that businesses often don’t consider and can prove expensive, such as the recent WannaCry spread.
If you use AWS then you don’t have to worry about any of this. All of their sites have been designed with security, redundancy and maintenance in mind. The cost of the staff, maintenance and upgrades is all accounted for in the price of the services.
Storage is becoming an increasingly large component in a lot of businesses. Whether it’s historical records, user data, analytics or media, we all have more data and need to store it all safely and securely.
AWS has a few services that cover all aspects of data storage and access. Amazon Glacier can be used for archive storage, Amazon S3 for general instant access storage and Amazon EBS for dedicated block storage for your EC2’s.
You really don’t want to loose your data either and configuring redundancy into your own systems can be difficult and expensive. With AWS storage solutions, setting up redundancy protection can be configured with the flicking of a switch if it isn’t automatically applied, such as with S3 and EBS.
Computation is the most common and powerful part of AWS in my opinion. These services allow you to effectively utilise all of the other services to provide functionality to your company and customers.
The two main computing services are Amazon EC2 and AWS Lambda.
Amazon EC2 is the more traditional compute service where you can rent time using a selection of servers. These can vary from tiny, single core machines to clusters of multi socket beasts. You can also rent constantly for something like a web hosting server, or rent for just a few hours to run a mathematical model. This gives you massive flexibility and means you only pay for what you need.
AWS Lambda is a very different service. These run more like remote functions that you can execute. They can be as simple or as large as you need and start up when they are triggered and shut down when they’re finished. They can be triggered by almost anything from
Almost every company needs to use databases to store a huge range of information and they are vital for the daily running of those companies. Amazon has a range of services that you can use to store relational and non-relational data.
Amazon provides RDS which allows remote hosting of your relational databases, whether SQL, Oracle or most other relational database systems. If you want to use non-relational databases then Amazon provides an inbuilt service called DynamoDB which you can work easily with, through the AWS SDK. Unfortunately MongoDB isn’t supported in the same way that SQL or Oracle are, so you’ll need to spin up an EC2 to host your MongoDB databases.
For shorter term storage there is also Amazon ElastiCache. This service allows you to use Redis or Memcached to store data in memory, providing even faster, volatile memory.
There are a lot of services and systems available through AWS but this means there needs to be a strong set of tools to manage it all. Luckily that is exactly what there is.
Amazon CloudWatch is the service that you can use to monitor every other service in AWS. It can collect logs, events and metrics from any other service, providing you with detailed insights into all of your products. This is extremely useful for bug hunting as all the data is in one place. You can also trigger alerts or automated actions in real time based on the logs.
When building services it is always a good idea to limit the abilities of each part to what it needs to do, reducing the chance that one part breaks another. To control the permissions of each of your products AWS has the Identity and Access Management (AWS IAM). You can create a role for a Lambda that gives it access to read from S3 and full access to DynamoDB but no other permissions. You can also create users with certain permissions. This is important if you have to provide access keys for product that exist outside of the AWS ecosystem.
AWS Elastic Beanstalk is a service for deploying and scaling web apps and services that you’ve developed. It provisions and operates the infrastructure and manages the application stack for you, so you don’t have to spend the time or develop the expertise.
We’ve only just scratched the surface of the AWS services and there are loads of other services to empower your company. One of the main advantages of AWS is how easily the services work together, enabling fast prototyping and stable production.
If you’ve enjoyed this then make sure to mash that clap button and follow me as I’ll be doing more of a deep dive into a lot of the AWS services.
Coding tips, tricks and tutorials
189 
189 claps
189 
Coding tips, tricks and tutorials
Written by
I'm a software developer currently building Chat Bots for E-Commerce companies. Outside of coding I love to go Rock Climbing and Traveling.
Coding tips, tricks and tutorials
"
https://medium.com/@padhai/upcoming-lean-mean-disruptions-in-higher-education-in-india-4b2831f4029c?source=search_post---------99,"Sign in
There are currently no responses for this story.
Be the first to respond.
PadhAI
Jan 3, 2020·5 min read
The Indian IT industry is going through times of change. Automation, managed cloud services, and the economic slowdown are cited as some of the possible causes. The major consequence is however clear: mid-management is being trimmed. It is reported that over 17,500 mid-level employees have been laid off, and more are apparently coming. However, hiring continues for freshers, with over 87,000 being projected to be recruited in the coming year. Also, top-level management have their hands full in navigating their organisations through these tough times.
We see two well-understood and fundamental causes for the lean mean. One, technology is rapidly cutting into work that is repetitive. Two, technology is efficiently enabling meaningful interfaces between humans at scale and at distance. The former puts a premium on unique or complex work, while the latter reduces the premium on managing people and projects. At the cross-hair of both these trends, is our mid-level IT manager and hence the pink slips. It is worthwhile to understand which other roles may be similarly affected by a thinning middle.
For the higher education industry, we can similarly map three roles.
While the above three roles are pulled out separately, they do not necessarily map to different individuals. In most private engineering colleges for instance, a faculty member plays the roles of the facilitator and the teacher. In more renowned institutes, faculty members have partial facilitation support with teaching assistants, while they focus more on teaching and knowledge discovery.
Like in the case of the IT industry, the bottom and top layers are indispensable. Facilitation is crucial: For high learning efficacy, each learner’s unique needs should be singularly attended to. At the other end, knowledge discovery is the essential: It provides clarity and direction in a fast changing tech-driven world.
The scale of teaching, however, remains suspect. Firstly, teaching is often replicated: Across multiple institutes, teachers teach the same courses year after year. For instance, last semester, we taught the Operating Systems course to the undergraduate Computer Science class at IIT Madras. Such “core” courses are repeatedly taught often deriving from the same select set of textbooks and resources. Secondly, technology is ready to enable a teacher to efficiently teach large student audiences. The rise and maturing of Massively Online Open Courses (MOOCs) over this decade has established this. (Note that most MOOCs today suffer from very poor learner engagement due to lack of personalised facilitation, which is called out as a separate role in our structure).
Courses would be taught by select top faculty members who are deeply involved in research and product development. Large number of students, in thousands, would take up these courses on online platforms. Facilitators will enable learning for individual students in manageable student to facilitator ratios like in today’s college classrooms. Students would take up proctored examinations across the country and receive online diplomas and degrees.
The roles of many individuals and organisations would fundamentally change. Faculty at the top institutes of the country would primarily focus and be judged on knowledge discovery. Instead of teaching 3 courses a year within their institute, they would teach 1 course in say 3 years to large student communities. This would afford time and resources to prepare and design courses at definitive levels. Faculty at other institutes would effectively become hands-on facilitators with emphasis on empathising and connecting with individual students.
Colleges around the country would become hang-out spots for students virtually enrolled into different online courses. Colleges would be primarily judged by their ability to provide infrastructure facilities to their resident students. Institutes like the IITs would be home to top researchers and offer standard classroom courses on advanced topics. They would significantly reduce their administrative overheads of managing large student enrolments in core courses.
Several disruptions are needed to enable a lean mean higher education stack. Robust technology platforms are required to support online education at scale. Such platforms should enable interactive lessons across disciplines, well beyond providing just virtual programming labs.
Proctored examinations with certified quality levels will be a major operational cog. Ensuring high credibility on these tests would be essential.
Recruiting, skilling, and networking facilitators would be an important human-resource disruption. There should be greater social and commercial acknowledgement of the challenges in and the value of facilitation. Finally, virtual platforms for placements will a necessity. Such platforms should provide companies a single interface to students across physical locations. Successful startups across these areas spanning technology, operations, and human resources would create immense value.
For one, school students would limit their focus on entrance examinations as admission to virtual schools would not be limited by seats. Instead, focus will shift to doing well throughout the degree program and earning the degree with higher relative grades. This would mean that students would bring higher levels of energy into their more meaningful college lives. Finding suitable employment with one’s degrees will remain the prime motivation for students.
In a flat virtual university setup, students will focus more on creating differentiating experiences to catch the attention of recruiters. Students will enjoy greater choice in structuring their programs and even careers. It would not be surprising if students choose to interleave work and education based on their own lived experience.
Powering your AI take-off
57 
3
57 
57 
3
Powering your AI take-off
"
https://medium.com/hackernoon/top-5-%D1%81loud-based-services-for-it-companies-to-use-in-a-daily-routine-8acf674638dd?source=search_post---------100,"There are currently no responses for this story.
Be the first to respond.
Throughout our work, we have been constantly improving our tools, skills and practices in order to achieve better results in work, communication and even relationship between our clients and us. So we came up with a list of cloud services that we believe are essential for any IT company, either big or small, and that help greatly in everyday routine. We find these services to be really helpful and we will discuss them below to give you a quick overview.
Docker is a technology that provides extra layer of automation of OS-level virtualization on Linux and Windows. Docker allows organizing a controlled environment for running programming products. At any time, it is possible to run any amount of identical processes (microservices) and it helps us make sure that our server works on the necessary operating system version, which has all the functions.
The user does not have to perform any additional functions as well. In order to deploy the project you will only need to perform one command instead of installing dozens of apps. Another Docker advantage is the opportunity to use each build version multiple times.
Why do you need it? Time-saving, efficient work with no complexities.
According to Datadogs report at the beginning of March 2016, 13.6 percent of their customers had adopted Docker. One year later that number has grown to 18.8 percent. That’s almost 40 percent market-share growth in 12 months.
JetBrainstm Youtrack is a bug and issue tracker and is an incredibly helpful tool. It is also a project management software.
It’s main focus is query-based issue search and the program can manipulate issues in batches, customize the set of attributes and create custom workflows.
What else is cool about Youtrack is its ability to integrate with a number of external tools, including CVS, Clearcase, TeamCity and etc. It can even import the issues from JIRA (whish is widely used by a lot of companies) and can also build an integration with GitHub.
Why do you need it? Great aid in bug tracking and issue tracking, compatible with many tools.
If to believe the Issue Tracking Tools Survey 2016 by JetBrains Youtrack is not so popular as Jira, therefore has the same satisfaction rate as Jira (80%) and also ranked for best features. Report says that Youtrack is the most recommended tool.
JetBrainstm Webstorm is a JavaScript IDE (“Integrated development environment” — a software application for software development) that is equipped for both client-side and server-side development with Node.js. It has a lot of cool features, the most outstanding and significant would be: code completion, code check and analysis “on the go”, convenient navigation, convenient system of refactoring and debugging, opportunity to work with version control system, Live Templates, etc.
After installing the Webstorm for the first time, the developer gets an editor with extensive functionality. You can change almost anything, from the theme to synchronization by FTP.
Webstorm supports languages that are complied in JavaScript (TypeScript, CoffeeScript, etc.). And for more detailed set-up there are a lot of available packages.
Why do you need it? Easy development with lots of cool features to choose from.
According to Slant WebStorm took the first place as the best IDE for Node.js (tool to debug Node.js) and for Dart languages. Is one of the best JavaScript (TypeScript, React Native) IDE’s and editors and has a discount for students.
We’ve spoken for numerous times about the importance of good and consistent communication between both team members and between you and the client. Slack is a tool that helps you a lot in keeping up the communication, storing all necessary files and easily searching for anything, from a file to a keyword.
Besides keeping all communication in one place, it also integrates with a lot of familiar services like GitHub (enables notifications and allows you to view the code checking), Trello (another great source for task management and tracking), Google Hangouts, etc.
In addition, you can share and test code snippets in Slack and have private one-to-one groups.
Why do you need it? Great environment with consistent communication, opportunity to share files and track down a lot of information.
We found some stats for you. How many people use slack? This report says that slack has 5 million DAU (daily active users) for June 15, 2017. An average amount of time users are active on Slack per weekday 320 minutes.
CircleCI allows you to optimize a lot of development processes, to start with. One of its most important features is ability to test the code added by developers. The tests are automatic, thus transferring this part of work from the developer to the program.
As well CircleCI allows to organize automatic deployment of the project. Depending on the setup, the program can independently deploy the project on such services as Amazon, Heroku, etc. Also DevOps engineer can use his own settings for the services that are not included in the list of possible settings.
Another advantages are execution of any other scripts and debugging feature.
Why do you need it? Work optimization, time-saving, quality control.
These are the tools that we are using in Dashbouquet on a daily basis and highly recommend for any IT company to deploy. If you have any other thoughts on useful apps, please share with us! Because we are always eager to learn new things and try them out.
Written by Dmitry Budko
Want to learn more? Check out here
#BlackLivesMatter
29 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
29 claps
29 
Written by
Collection of posts from those who build Dashbouquet https://dashbouquet.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Collection of posts from those who build Dashbouquet https://dashbouquet.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-capital/an-interview-with-ceo-jared-grey-from-alqo-bitfineon-on-altcoin-magazine-cb9133c17b4a?source=search_post---------101,"There are currently no responses for this story.
Be the first to respond.
Hello Jared. Thank you for your generosity to allow us to get to know about ALQO and its Ecosystem. Firstly, please introduce yourself and your co-founders of the ALQO project.
Thanks, Emil, I appreciate this opportunity to share information about ALQO and Bitfineon with your readership. My name is Jared Grey, my background is in IT consulting and business development. I was an independent IT consultant for 14 years before becoming the CEO for Bitfineon GmbH, the private company that handles development for the ALQO crypto-currency. Kevin Collmer, our lead developer and CTO, has a background in video game development, specifically as the lead technical artist for CryTek, a German-based video game developer famous for developing the CryTek gaming engine. Our COO, Moritz Heimberg has a background in IT project management and is responsible for handling daily operational activities within Bitfineon GmbH.
Can you give us a detailed rundown of the $ALQO project?
Bitfineon GmbH aims to develop a complete all-in-one financial and cloud services ecosystem with ALQO at the center, powering the financial technology products we develop. ALQO started out as a simple masternode coin project, but the team’s ambitions quickly grew to include the ALQO universe and planetary system under the direction of former CEO, Kaan Hamarat. My plan as active CEO is to continue with the original vision and increase scope where it can add value to the ALQO ecosystem.
Very shortly we will release our cutting-edge crypto and fiat trading platform, Bitfineon. Bitfineon has been developed from scratch and includes a revolutionary matching and trading engine that executes trades in a unique manner which decreases latency to nearly instantaneous execution, regardless of the number of pairs being traded. With its unique software architecture, which utilizes GPU processing instead of the legacy standard CPU method, we’ve been able to achieve benchmarks of 63mm tx/ps, scalable in 2mm tx/ps increments where hardware restrictions are remedied. We anticipate Bitfineon’s engine will become the standard for crypto exchanges, and for traditional financial market exchanges as well.
Your ecosystem consists of the Bitfineon, Liberio, Janus, and CoinFolium. Can you briefly explain to us what they are respectively?
I’d be happy to explain the ALQO ecosystem, you’ve listed the current publicly revealed planets within the ALQO universe of products — I will provide a brief overview for each of them. Bitfineon is our next generation crypto and fiat exchange, regulated and licensed by FINMA in Switzerland (with plans to seek regulatory approval and licensing for operation of fiat services in the USA within Q1 2019). It will provide traders of all kinds a cutting-edge trading experience, it will include several unique industry disruptive products and services as well as fundamentally change the way exchanges operate at a performance level. We believe it will set a new standard in engineering for exchange architecture and challenge competitors who use white-label solutions to “up their game”. Many of the limitations in the current crypto exchange landscape are due to a lack of innovation with existing software solutions, and the custom developed nature of Bitfineon aims to change this when launched.
Liberio is ALQO’s high performance, user-friendly, cross-platform thin client for storing, managing and tracking your ALQO on any device. It utilizes ALQO’s distributed cloud network to provide seamless, one-click access to every service offered across the ALQO ecosystem using your Thumb ID. Thumb ID is a unique identifier for your Liberio wallet that allows users to send and receive XLQ between each other without having to share an alphanumeric wallet string, we think this will help adoption amongst the general public who may be interested in using ALQO but are unfamiliar with traditional crypto address methods — a typical sticking point in the mass adoption discussion. Additionally, Liberio features a first-of-its-kind cold staking feature that is materially different from existing cold staking methods claimed by other projects. The unique encryption method within Liberio allows it to secure the ALQO blockchain while allowing users to stake their coins without a hot-wallet running. Liberio also is a crypto first project that shards and stores private keys across a masternode network, the ALQO masternode network. Several other features, like Libchat, further build the utility-case for Liberio above-and-beyond what typical wallets provide. You can read more about Liberio on our website: https://alqo.app/#liberio
This brings us to JANUS, which is ALQO’s blockchain analysis and intelligence platform. JANUS will replace the existing ALQO explorer, which is limited in functionality. JANUS will provide tools for an in-depth review of the ALQO blockchain, as well as forensic tools for deep intelligence discovery of the chain. We plan to utilize JANUS to increase transparency of the ALQO services suite, as well as give users greater control over chain data analytics. One feature I’m really excited about is the 3D modeling of data presented in a beautiful UI which will be released in the next few weeks.
Lastly, CoinFolium will be our coin metric and price history discovery tool. It will accompany Bitfineon Arena (a Bitfineon exchange service focused on developing a social experience for traders) in providing necessary analytics for assisting with fundamental and technical analysis of a project.
Who’re the team behind ALQO and the Bitfineon exchange respectively?
Personally, I’ve been an avid IT participant for over 20 years. I grew up at the beginning of the Internet-era and my academic background is in Computer Engineering. I’ve been a small business owner, focused on IT consulting, for the past 14+ years. My IT career began at Dell Computer Corporation in Nashville, TN while I was attending college. I’ve been aware of Bitcoin and crypto for several years but became seriously active in the market over the past couple of years. My involvement in other projects led me to the opportunity with ALQO and Bitfineon, and I couldn’t be happier. The ALQO team is determined, talented and passionate.
Our CTO and lead developer, Kevin Collmer, has a background in video game development and worked as lead technical artist in developing shader technology for CryTek’s famous CryEngine gaming engine. He has a bachelor’s degree and MSc in IT. He ran his own game development company until focusing his development skills on the ALQO universe and its accompanying planets. A highly capable and talented developer, he is the technological visionary behind ALQO and Bitfineon GmbH.
Our COO, Moritz Heimberg, handles our daily operations and helps organize and focus our own ongoing business operations. He has a background in B2B and B2C IT project management. He is head of our IT support department and oversees ALQO masternode server maintenance.
Raphael Scheidler is a seasoned community manager with 3+ years of social media leadership and customer service experience in the fintech startup sector. In combining a rigorous technical academic experience with professional roles in the public and private sector, he has exemplified his keen ability to organize online communities, assist in brand strategy administration, and utilize blockchain technology at an expert level for ALQO and Bitfineon GmbH.
Samuel Hochauf is the enthusiast within the team as a member solely recruited from the community. With his MSc in EE and 15+ years of experience in the semiconductor industry mainly consisting of database engineering and big data analytics, he brings value to the team from a completely different viewpoint. His main objective is being part of the community management team while playing an advisory role regarding system development and operations.
Igor Melkozerov is studying computer science & system engineering at Ostfalia in Wolfenbütel. Igor has experience in IT Administration as well as in embedded system engineering & development. His main task is the maintenance of ALQO’s website as a front-end developer.
Nastja Deines is an architecture student at the Darmstadt University of Applied Sciences. She is the designer in the ALQO Team and helped us to design the ALQO Logo, Coinfolium and the Masternode APP.
Tell us about ALQO’s cold staking feature, and how does it compare to Stakenet’s Trustless Proof of Stake (TPoS) and Particl’s Cold Staking?
Stakenet’s TPoS differs from Glacier as the former requires that you surrender a third type of key, a staking key that allows a 3rd party to stake, but not spend, your balance. They are called XSN merchants and they do so for a fee as that’s the incentivization mechanism. Glacier, on the other hand, employs the same level of decentralized trustlessness that’s inherent to the ALQO network and leverages ALQO’s existing cloud infrastructure (over 2.5k masternodes) to stake for you. By running an ALQO masternode, you implicitly sign a contract to allow the use of your hardware to stake any member’s balance as it’s hardcoded masternode service functionality.
The two biggest advantages offered by Glacier over Stakenet’s TPoS offering is redundancy (over 2.5k nodes vs. a specific merchant that could go offline), and economy (not having to pay a fee as the staker unlike with TPoS’s required staking fees).
Particl’s PPoS implementation, while is technically a cold staking solution, requires that users set up a fully synced and up-to-date full node (called a “staking node”) and sign a smart contract using their private key. The wallet can then be taken offline, but the staking node must remain online and continuously connected to the network in order to stake. This creates multiple barriers to user entry, namely network and hardware availability, user’s level of expertise, setup time and learning curve, as well as increased downtime time risk in terms of income continuity and an overall narrower attack vector (single staking node vs. ALQO’s 2.5k masternodes).
In comparison, Glacier allows the user to stake any amount, fee-free, immediately, with zero learning curve requirements while informing the user of the ideal staking input size to maximize staking ROI. Simply move your funds to your Liberio address and that’s it, it’s as user-friendly as it sounds, which in our view is a hard requirement for achieving mass adoption.
The truly innovative element with Glacier’s implementation is a non-trivial evolution in ALQO’s consensus mechanism, by effectively shifting standard Proof-of-Stake into a PoS-MN hybrid consensus layer. In many ways, our unique hybrid consensus mechanism is superior even to Delegated Proof-of-Stake as masternodes constitute a scalable array of validators authorized to stake on a public address’ behalf, as opposed to having a narrow list of semi-centralized delegates. This also renders collusion impossible as unlike delegates, masternodes cannot manipulate or interfere with the block validation process in any way. There are currently no known cold PoS implementations that rival ALQO’s Glacier feature stack, robustness and added security layer.
The launch of Bitfineon is close. Do you have any beta release program or preview of this Swiss-based exchange?
We are very excited to release our upcoming exchange, as we believe the products, services, and technologies that we’ve built, are industry disruptive in a highly positive way. We hope to set a new bar for crypto exchanges to operate by, increased transparency, an intuitive user-experience, unique order types, and a beautiful and intuitive user interface. We’ve released a few sneak-peeks via social media channels, as well as a live-stream of the user-interface design. We’re currently preparing to release the closed beta in the next several days if all remaining items are completed without issue. You can sign-up at https://www.bitfineon.com to join the beta program and once it’s available, you will be notified to create an account.
How does ALQO compare to an exchange coin like BNB as an exchange coin?
ALQO [XLQ] is fundamentally different from BNB, it is (XLQ) a former PoW (PoS now) fairly distributed cryptocurrency coin that will be the lifeblood for the Bitfineon exchange and other financial products, such as PayInX (our crypto payment gateway). It is an independent currency which can be used for payment settlement for anyone, anywhere, independent of its use on Bitfineon. While you will receive some similar benefits, like reduced trading fees (similar to BNB), when trading with XLQ pairs on Bitfineon, it will provide more use-cases than the BNB token. Additionally, all XLQ holders will be able to participate in our unique sharepool which will allow you to lock your XLQ on Bitfineon every 24 hour period and earn a share of the trading fees collected on Bitfineon. You can find out more information about the sharepool in our explainer video here: https://youtu.be/CE5lpo0aoG8
Tell us about the ALQO Cloud Network.
Taken from our website,
“ALQO’s Cloud Services are based on a highly-distributed network of over 2,500, always-on global replication servers called masternodes, making it one of the most resilient decentralized cloud networks in the world, and delivering unparalleled levels of resource redundancy, network capacity of takedown resistance.”
Each product within the ALQO cloud services ecosystem utilizes the masternode network for specific tasks. For example, Liberio utilizes the ALQO masternode network to store fully encrypted and sharded private keys, in a redundant manner, across the network. With this type of innovation, ALQO has proved itself as a leader in masternode development. We will continue to develop our roadmap and build more use-cases for our masternode network, read more about planned roadmap items on our website, https://alqo.app.
What’re the achievements that you are most proud of since the beginning?
I’ve been CEO for 3 months, after assuming the role from Kaan, my beginning starts a bit later than the rest of the team. For me, personally, the greatest achievement has been the launch of Liberio. It’s a technological innovation within the crypto-space. A truly decentralized thin-client wallet which makes use of the ALQO masternode network; it stores encrypted data, sharded, across the masternode network in a fully redundant manner.
What has been the greatest challenge to overcome so far?
As for most start-up projects and businesses, the greatest hurdle is always funding. Bitfineon GmbH operates in a very lean manner, and currently, the main business principles dedicate the developer’s fee to operational expenses and development. As founders, at times, we have contributed significantly from our personal funds to facilitate operational liquidity. Our belief in the ALQO vision is a key motivator for us to “put our money where our mouths are”.
What’s the three most interesting bullet points on your roadmap for the coming 2–3 quarters?
Personally, I’m super excited about the development of our own smart contract language, entitled ‘GAVEL’, and the integration of smart contracts into the ALQO blockchain. I believe this will attract developers to create dApps on ALQO’s blockchain and, subsequently, grow the awareness and use-cases of ALQO. We believe there are some serious shortcomings in currently available smart contract languages and we aim to reduce those shortcomings with the release of GAVEL. With its release, we’ll be able to issue our own tokens on the ALQO blockchain, these will be under the ‘AGCR’ standard. I firmly believe these roadmap milestones will differentiate ALQO from typical masternode projects and demonstrably illustrate our commitment to innovation.
How can we connect with ALQO and how can we support your project?
The best way to connect with the ALQO universe and become involved in our community is to join one of our many social media outlets, Discord, and Telegram, and follow us on Twitter and Facebook. We have comprehensive information available about ALQO and its planets on our website, https://alqo.app, you will find all social media links there too.
You can download our english whitepaper here, though it is currently being rewritten to include our updated roadmap and other additions to the ALQO universe.
Thanks for taking the time to go through this interview with me — do you have any last words?
I appreciate the time to inform your readers about the exciting and innovative financial technology we’re developing at Bitfineon GmbH. I encourage your readers to come to join us on Discord and find out more information and get to know us personally, we’re available daily to chat with interested newcomers.
Before moving on, make sure to press follow, leave a clap or 46, share today’s highlight and if you missed the last article, click here.
Read about the Altcoin Magazine Mastermind Event here.
Follow us on Twitter, InvestFeed, Facebook, Instagram, LinkedIn, and join our Discord and Telegram.
The purpose of ALTCOIN MAGAZINE is to educate the world on crypto and to bring it to the hands and the minds of the masses.
A publishing platform for professionals now available on https://thecapital.io
264 
Head over to https://thecapital.io, sign up and publish your first article today! Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
264 claps
264 
Written by
A publishing platform for professionals to see what’s trending in business, finance, and tech
A publishing platform for professionals in business, finance, and tech
Written by
A publishing platform for professionals to see what’s trending in business, finance, and tech
A publishing platform for professionals in business, finance, and tech
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://javascript.plainenglish.io/the-ultimate-guide-for-integrate-aws-amplify-authentication-for-react-native-15a8eec10890?source=search_post---------102,"Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/katalon-studio/setting-up-mobile-automation-project-in-macos-with-katalon-studio-11aab3ff0efe?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
Using Katalon Studio, a tester can design mobile app automation tests for both Android and iOS to run on physical devices, cloud services or emulators.
This tutorial explains how to setup a mobile app testing project for Katalon Studio on macOS. It assumes that you are familiar with automated testing.
Read more: Setting up Mobile automation project in Windows with Katalon Studio
Katalon Studio requires the latest installation of Appium and Node.js. Please setup as the following steps:
/usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
brew install carthage
brew install node
brew install npm
Note: To make sure Node.js is installed correctly, you run the command which node on Terminal.
npm install -g appium
You will need to install and configure Xcode in case of testing on iOS devices. Please set up Xcode as follows:
mkdir -p Resources/WebDriverAgent.bundle
sh ./Scripts/bootstrap.sh -d
xcodebuild -project WebDriverAgent.xcodeproj -scheme WebDriverAgentRunner -destination 'id=<udid>' test
Note: <udid> is the device UDID of your mobile device.
For Android devices:
For iOS devices:
To test an iOS native application file (.ipa or .app file), make sure the file is already built and signed properly to deploy on the device. Follow these steps to check if an application file is already built and signed correctly:1. Open Xcode and navigate to Window/Devices2. Choose your device from the Devices list3. Press the “+” button and choose your application file
4. If installed successfully, the application will appear in the Installed Apps section as shown below
You can modify extra Desired Capabilities when executing automation test in Katalon Studio.
To define Desired Capabilities for local execution using Chrome, Firefox, IE, Safari or Edge, please access Project > Settings > Execution > Default > Mobile > iOS (or Android).
The example below shows Desired Capabilities settings for Android to enable Unicode inputs.
Refer to Desired Capabilities for which properties of each web browser are supported by Selenium. For Desired Capabilities to be used with Appium, refer to Appium server capabilities.
You may use external libraries in your test project. Go to Project > Settings > External Libraries to add new or remove existing libraries for Katalon Studio. External libraries are stored in the Katalon Studio project’s Drivers folder.
The added libraries can be imported and referenced in the Script View of Katalon Studio:
For more helpful tutorials from Katalon Studio, do not hesitate to visit our Resource Center!
Read more: Setting up Mobile automation project in Windows with Katalon Studio
Source: Setting up Mobile automation project in macOS
An all-in-one test automation solution for Web, API, Mobile & Desktop application testing
323 
323 claps
323 
Written by
#Katalon, a powerful and free #automation solution for Web, API, Mobile and Desktop application testing. #Programming #SoftwareDevelopment #Testing
A complete test automation solution with continuous integration trusted by hundreds of thousands of developers and testers.
Written by
#Katalon, a powerful and free #automation solution for Web, API, Mobile and Desktop application testing. #Programming #SoftwareDevelopment #Testing
A complete test automation solution with continuous integration trusted by hundreds of thousands of developers and testers.
"
https://medium.com/swlh/triggering-cloud-storage-with-cloud-functions-in-java-177d7c3c1208?source=search_post---------105,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Functions is a serverless execution environment for building and connecting cloud services. With Cloud Functions, you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any…
"
https://medium.com/manifoldco/cloud-jumping-swap-hosted-services-easily-with-manifold-49d93a3d19f1?source=search_post---------106,"There are currently no responses for this story.
Be the first to respond.
Using some part of the cloud—whether AWS, Azure, Google Cloud, or others—has become a standard part of modern web development. RightScale’s 2018 State of the Cloud reported 97% of respondents using at least one cloud service.
In most instances, it’s easiest to buy into one cloud ecosystem so you have fewer things to manage. But when prices of a service increase, the platform changes, or you need to add an outside service to the mix, how do you handle switching one service from your vendor-locked cloud, let alone several?
Manifold’s free developer tools provide everything you need to build-your-own cloud from existing providers. To see how, I’ll be taking an example application with authentication, hosting, and account email services, and moving the app off AWS and onto independent providers for all three parts.
Our example app is nothing more than a login with an authenticated page, and a password reset via email. So we’re using the following services:
You can see the code for the starting application here, in the master branch.
The credentials for the services are all fed in via environment variables through a .env file. Though the values aren’t saved in the app, you can see all the variable names needed in the .env.example file.
With Manifold, you can hook up any service you want—even those not listed on the Services page! But for this example, our usecase is satisfied by the existing services of Mailgun and JawsDB Postgres, so we’ll use those two. But again, once the application uses Manifold, those can be swapped out easily at any time.
To start, sign up for a free manifold account (only takes a few seconds if you authenticate with GitHub). Then, from our project window, we’ll run the following:
Mac (via Homebrew):
Linux/Windows:
Note: you can read more about this install script on the Manifold CLI Guide.
Then, log in with manifold oauth -- github if you registered via GitHub, or manifold login if you signed up with email / password. Now comes the fun part!
Run the following to add a resource, and the friendly CLI will walk you through provisioning (it even gives you pricing and descriptions in-terminal!):
For JawsDB, selectjawsdb as the service type, and kitefin for the lowest-cost plan (you can adjust the plan later from the Dashboard).
Complete the remaining prompts like region and name with whatever you wish.
Our app is still using the RDS creds in .env, so we’ll need to update those. Those can easily be grabbed from within the Dashboard UI, but we’ll stick to the CLI for simplicity. Simply run:
…and you’ll see the credentials exposed into your terminal session (if you need to output a project, run manifold export --project projectname). We already have an .env already, but for future reference, you can optionally specify an output file like so:
What you should see from JawsDB is all the credentials rolled into one URL:
Pluck apart those values from the URL into .env to swap your app.
There’s no GUI for exporting your AWS Postgres database, but the standard pg_dump will work just fine. Assuming your RDS instance is publicly accessible, run:
You’ll be prompted for your password so it’s not saved in your shell history. To output a development.dump file from AWS. Then simply import into JawsDB:
You’ll likely see some errors like ERROR: role does not exist because of username differences, but it should succeed nonetheless. You can verify by running psql $JAWS_DATABASE -h $JAWS_HOST -U $JAWS_USER and \dt to show a list of tables. You can follow up with a query to double-check the data made it (e.g.: SELECT * from users;).
Migrating our database was about as easy as it gets, but not all service-swapping is quite as rosy. To switch from Mailgun, we will have to modify app code a bit in addition to a simple .env update.
Add Mailgun to your Manifold account, either through running manifold create locally, or via the Dashboard. Their free tier is perfect for testing.
Once you’ve added it, click the Open Mailgun Dashboard button at the top of the screen:
From there, you’ll want to Add Custom Domain:
Follow the instructions that follow for adding Mailgun to your domain, which is necessary to avoid spam filters and whatnot. Back in our app, we’ll swap node-ses with mailgun-js:
Then in our email config, we’ll make a few changes that swap out the SES client with Mailgun’s (you can see the diff on the Pull Request from the sample app). Overall, not too bad! The API, fortunately for us, ended up being similar. We only renamed two env variables—so as to not be confusing later— and had to change message to text in our email call.
Once some test emails are sent, we’re pretty much in business!
Our app was originally hosted on Amazon Elastic Beanstalk, and moved to Zeit Now. I expected to glean some insight from migrating hosts, but came up short—the process was unremarkably quick and painless. The only lesson I learned is so glaringly obvious, it’s almost not even worth saying:
If you use cloud services that work on any host, you can deploy to any host at any time.
“Duh,” you’d say. And you’re not wrong! But it’s not bad advice to keep in mind, either.
Projects are the best way to associate related resources together, especially if you’re using several in the same application or suite of apps. You can create new projects with:
Associate resources to that new project by running:
The biggest advantage to projects, besides organization, is one-click .env file downloads. Clicking either the “download .env” or “Show credentials” buttons will yield all values for all resources in a project. Huge time-saver when it comes to updating access tokens!
If you want to make it even more secure and load secrets from memory, try using manifold run to start your app—you can inject env vars directly from Manifold without any of them living in your file system! The sample app has an example of how to do this in package.json.
Team projects are even more useful. With team projects, you can invite and uninvite other developers to access shared resources, and pay for it all with one set of billing info. To create a team project (assuming you’ve created a team with manifold team create), first switch to a new team:
And then you can run manifold projects create from that new team context to create a project for that team (currently, you can’t convert an individual project to a team project, but it’s not too bad to move existing resources from an old project to a new one—you don’t have to start from scratch again).
Still debating how much effort it’d be to migrate cloud services for your app? Here are some good and bad practices that should give some indication:
Migrations may not ever be fun, but with tools like Manifold’s CLI it can come pretty darn close.
The best migration may be none at all, but when the time inevitably comes, Manifold makes it as painless as it possibly can be.
We're determined to make it easy for developers to use the…
384 
384 claps
384 
We're determined to make it easy for developers to use the cloud services they love.
Written by
Web performance, animation, and image optimization
We're determined to make it easy for developers to use the cloud services they love.
"
https://medium.com/@oldmanpete/the-curious-case-of-google-and-microsoft-disagreeing-on-a-word-count-d6d917cbd71c?source=search_post---------107,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pete
Jan 27, 2017·3 min read
I remember when I was doing my degree, because I’m too wordy I’d have all manner of headaches making my essays fit the word count. The 5 or 10 percent leeway often saved my arse. I don’t miss it, it was tedious. Recently though, I have been having to proof read someone else's work, specifically with a view to reducing word count. That’s where all the fun started.
“I need to lose at least 15 words off this”
I was informed. Fair enough seems like a simplistic task, I proceeded to remove all the superfluous words, but importantly I didn’t keep count. I knew I’d gotten rid of more than required. Gave the reviewed document back and thought no more about it. It came back again for more chopping after some editing and slight rewriting, well rather than review the entire thing again, I diffed the new doc and the original edit for only the changes. I can’t remember the number I was told to lose the second time around, but it did not match with the additions to the new document. Curious.
I was reviewing it in Google Docs, the original was being written in Word (Online version), when I quizzed the author for the word count they believe it was at, it did not match what Google Docs thought it was at. The doc was returned and checks were made to ensure we were definitely looking at the same document, we were. What the hell?
Something was definitely afoot.
Let’s shove the EXACT SAME DOCUMENT into three different word editors.
Google Docs:
Word Online:
Word Desktop:
So you can see that all three programs give different answers to the word count of one document. When you’re a student at the limit of your word count this means you are potentially going to submit a document that breaches your limit purely based on using the ‘wrong’ program to count it. Like being a student doesn’t offer enough stress.
I didn’t run this through LibreOffice/OpenOffice because that would involve installing them and I am personally of the opinion that they suck. However I do have notepad++ with TextFx plugin which gives word counts, and that has it as:
So there you go, four tools, four different results. I’m not counting manually to find out who is correct, so I am declaring them all rubbish.
InfoSec architect, analyst and researcher. Suffering from full time imposter syndrome.
See all (83)
22 
3
22 claps
22 
3
InfoSec architect, analyst and researcher. Suffering from full time imposter syndrome.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-set-up-an-flexible-and-scalable-data-analytics-platform-quickn-easy-5fb3a4c83745?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christianlauer
Jun 28, 2020·4 min read
Building a classical On-Premise Data Warehouse via ETL processes and OLAP cubes often ends in delayed projects, additional costs and headache for IT managers…
"
https://medium.com/@IndianGuru/using-go-for-ibm-watson-services-7051b9f73a0?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Satish Manohar Talim
May 9, 2016·3 min read
When you register, you receive a free 30-day trial for all Watson services. After 30 days, free use of Watson beta and experimental services continues. All other Watson services, such as the generally available “Personality Insights” and “Tradeoff Analytics” services, offer a specified number of free API calls on a monthly or daily basis. Pricing varies for each service after you exceed the specified number of free calls.
Once you have registered for Bluemix, you are ready to begin creating your first Watson services application!
Getting service credentials in Bluemix
The above video shows how to get started programming with the Watson Developer Cloud services by obtaining service credentials on Bluemix.net.
For this article, we shall select the “Text to Speech” service.
Note: Service credentials (“username” and “password”) are different from your Bluemix account username and password.
IBM Watson Developer Cloud services support two typical programming models for HTTP communications:
We shall be using the “Direct interaction with a service” model in our program “gowatson.go”.
gowatson.go
We shall write a Go program called “gowatson.go” which will convert written text to natural-sounding speech.
I have created a folder “gowatson” which will hold the Go source code “gowatson.go”.
We shall be running our program at the command prompt in the folder “gowatson” as “go run gowatson.go”.
Here’s the complete Go code for “gowatson.go”.
Let’s understand our code:
The endpoint URL is:
To synthesize text to speech, you call the service’s “synthesize” method to pass input text to the service, which returns spoken audio in response. The GET “synthesize” method accepts the text to be synthesized via its required “text” query parameter. Use this version of the method for simple text that is easily accommodated on the URL.
The “synthesize” method has the following parameters:
The HTTP GET version of the method accepts input text specified by the “text” query parameter. You specify the input as plain text or as SSML, both of which must be URL-encoded. Speech Synthesis Markup Language (SSML) is an XML-based markup language that provides annotations of text for speech synthesis applications.
“SetBasicAuth” sets the request’s Authorization header to use HTTP Basic Authentication with the provided username and password.
After you run the program, a file “talk.wav” should be downloaded in the folder from which you ran the program.
Have fun!
Senior Technical Evangelist at GoProducts Engineering India LLP, Co-Organizer GopherConIndia. Director JoshSoftware and Maybole Technologies. #golang hobbyist
12 
Some rights reserved

12 claps
12 
Senior Technical Evangelist at GoProducts Engineering India LLP, Co-Organizer GopherConIndia. Director JoshSoftware and Maybole Technologies. #golang hobbyist
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/getting-started-with-manifold-e340f3df3813?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
Manifold is the marketplace for independent developer services. A place for you to find, organize, and connect the best cloud services directly to your team’s workflow, under a single account, with one bill at the end of the month.
We offer two different methods of creating a Manifold account: connect your GitHub account, or enter your email address and a password. If you sign up by email, you’ll need to verify your address to continue.
Manifold allows you to organize all of your cloud services under a single account. Teams make it easy to share access to the service’s configuration and secrets, their web control panel, and aggregates everything on a single bill.
This means no more chasing down who owns which account, or who can give you the API keys you need to communicate to core services.
Teams also makes developer on-boarding super simple — invite a new person to your team and they gain access to exactly what they need.
Projects enable you to create logical groupings of services that belong to a single application, or component of your infrastructure.
Each project has a unique name, and can house as many services as you need. You then will be able to access the configuration for all the services in that project together. Because Projects aid in visualizing your stack, it simplifies on-boarding even further.
Your Manifold Dashboard starts by showcasing all the independent developer services that are available to be provisioned. All services can be filtered by Category so you can easily find the one you want.
There are three steps to getting started with your first service:
Manifold then takes care of provisioning exactly what you need from the provider.
If you have services which are not available through Manifold, or want to bring all of your configuration together in one project you can Bring Your Own Service. Using our encrypted key value store you can add any secrets or configuration you need to import.
Most services come with their own dashboards where you can view information about your usage and control your preferences. Manifold makes it easy to share access to these through Single sign-on. One click and you’ll be redirected, authenticated, to the service’s control panel.
Now that you have a project established, it’s easy to deliver your configuration and secrets directly to your application through one of our workflow integrations.
With development use the Manifold CLI to inject your config as environment variables through manifold run.
When you’re ready to deploy to production you can use one of our integrations to easily deploy your cloud services with your existing infrastructure.
Redeem the coupon GETSTARTED and receive $10 towards any service.
We're determined to make it easy for developers to use the…
321 
321 claps
321 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/katalon-studio/setting-up-mobile-automation-project-in-windows-c6f5366f0905?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
Using Katalon Studio, a mobile tester can design automation tests for both Android and iOS apps on physical devices, cloud services or emulators.
This tutorial explains how to set up a mobile automation project and test a mobile app using Katalon Studio on Windows. This tutorial assumes that you are familiar with the general principles of automated testing and have some experience with Katalon Studio IDE.
Unlike Web application testing, to test a mobile app, you need to install Node.js and Appium in addition to Katalon Studio.
Read more: Setting up Mobile automation project in macOS with Katalon Studio
Note: Make sure you install Node.js into a location where you have full Read and Write permissions.
npm install -g appium
Notes: Katalon Studio also supports mobile emulators. You can start emulators and launch your virtual devices. As long as they can be recognized by using the adb command, it should be fine to execute mobile tests with them.
You can modify extra Desired Capabilities when executing automation test in Katalon Studio.
To define Desired Capabilities for local execution using Chrome, Firefox, IE, Safari or Edge, please access Project > Settings > Execution > Default > Mobile > Android (iOS option is available in macOS)
The example below show the desired capabilities settings for Android to enable Unicode input.
Refer to https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities for information on which properties of each web browser are supported by Selenium. For Desired Capabilities used with Appium, refer to http://appium.io/slate/en/master/?ruby#appium-server-capabilities.
With this setting, you can add or remove your external libraries from Project > Settings > External Libraries.
The added libraries can be imported and utilized in the Script View of test cases:
You need to save your mobile application (APK files for Android) to a location where your computer can access with full permission.
For more helpful tutorials from Katalon Studio, do not hesitate to visit our Resource Center!
Read more: Setting up Mobile automation project in macOS with Katalon Studio
Top 10 API Testing Tools
Source: Setting up Mobile automation project in Windows
An all-in-one test automation solution for Web, API, Mobile & Desktop application testing
282 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
282 claps
282 
Written by
#Katalon, a powerful and free #automation solution for Web, API, Mobile and Desktop application testing. #Programming #SoftwareDevelopment #Testing
A complete test automation solution with continuous integration trusted by hundreds of thousands of developers and testers.
Written by
#Katalon, a powerful and free #automation solution for Web, API, Mobile and Desktop application testing. #Programming #SoftwareDevelopment #Testing
A complete test automation solution with continuous integration trusted by hundreds of thousands of developers and testers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/serverlessguru/serverless-end-to-end-tracing-troubleshooting-performance-monitoring-with-lumigo-5538de199590?source=search_post---------112,"There are currently no responses for this story.
Be the first to respond.
Lumigo is a platform that primarily focuses on debugging distributed serverless applications on the AWS cloud. Services like X-ray from AWS do a pretty good job at tracing requests with your application but the support for event driven systems isn’t quite there yet. X-ray also falls short in terms of piecing up fragments of certain chained…
"
https://mondaynote.com/creating-an-aws-in-orbit-nothing-less-f2f41943d1f3?source=search_post---------113,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
by Frederic Filloux
For a satellite operator, a launch is always the moment of utmost stress. Especially when its two spacecraft are put in orbit simultaneously. But this particular Wednesday, June 30 on Launchpad 40-A at Cape Canaveral, Loft Orbital is literally betting the farm. After four years of hard work, in San Francisco, Toulouse, France, and Golden, Colorado, the two satellites dubbed YAM-2 and YAM-3 (for Yet Another Mission, YAM-1 was a drawing board concept), are supposed to reach a low earth orbit, 500 km above earth, in less than two hours.
With this “Transporter-2 Mission,” SpaceX reaffirms its status as the de facto bulk carrier for the modern space age with at least three Transporter missions scheduled each year. Today, it carries no less than 88 satellites for dozens of customers of all kinds, ranging from the US military to foreign space agencies and private companies. Most of the assets are mini-sats — roughly the size of a washer — and a flurry of “CubeSats.” The previous Transporter-1 mission, on January 24, carried 143 satellites and the whole system is so flexible and commoditized that when a client is not ready, SpaceX can just substitute Starlink satellites to reach the nominal payload of 23 tons that will be released on polar, sun-synchronous orbits.
For the inaugural launch, the president and co-founder of Loft Orbital, Antoine de Chassy, has invited the Toulouse team to his home located in Hossegor, a notorious surf spot on the French Atlantic coast. Most of them have aerospace or signal analysis engineering backgrounds. They are quite young and enthusiastic. It looks like a student gathering at an American college, especially since English is widely used at Loft.
But what was supposed to be a fun evening is now turning into a nerve-racking experience.
The flight has already been postponed three times by SpaceX. Four days earlier, less than 24 hours before lift-off, an acoustic blanket inside the payload fairing needed to be readjusted; the 70-meter Falcon 9 had to be brought back to the assembly building, its 13-meter top reopened in a cleanroom. And on Tuesday, as the countdown was underway, an helicopter had entered the exclusion zone, prompting the launch director to freeze the countdown… 11 seconds before the lift-off. Given the narrow launch window of 58 minutes to reach near-polar orbit, the flight had to be scrubbed, once again.
So today is the day. Hopefully.
At 3:31 pm Florida time (9:31 pm in Europe), Falcon 9 clears the Cape Canaveral tower, bending its trajectory southward to reach the speed of 8 km/second necessary to stay in orbit. Eight minutes into the flight, the second stage of the rocket has completed its orbital injection, coasting at 28,631 kilometers per hour at an altitude of 207 km over the Gulf of Mexico. Nearly at the same moment, the first stage landed safely back to the Cape, giving a spectacular view of the booster spitting a huge plume of flame with the ocean in the background.
That was the easy part, so to speak. After all, this is the 100th consecutive successful mission in a row for Falcon 9, which, so far, had a 98% success rate, so the uncertainty lies elsewhere.
53 minutes into the flight, the deployment of the satellites begins. The rocket is now cruising at an altitude of 537 km. They are attached to the main structure horizontally, like the branches of a Christmas tree, and deployed in a precisely choreographed sequence.
Loft Orbital’s YAMs sats are in positions 3 and 26 respectively. At T+1 hour and 21 seconds, as the vehicle is passing over the southern tip of Sri Lanka, the video feed from SpaceX shows the smooth expulsion of the first sat. The deployment operator announces quietly “YAM-3, separation confirmed”.
We can see and hear everyone cheering at Loft’s San Francisco headquarters and in Toulouse, where the two mission control centers are located. Eight minutes later, right above the Himalayas, YAM-2 is also silently released from Falcon 9 (below).
Time to celebrate a job well done…
But the suspense is far from over as the two satellites must now come to life going through a delicate process. The two birds have a slightly different design. YAM-3’s two solar arrays are attached to the sides of the body of the spacecraft, while YAM-2, has a deployable solar panel that can be oriented in different directions. So the satellites need to rotate to present their solar arrays to the sun, which will recharge their rapidly depleting batteries. To manage their position, they use various instruments such as a magnetotorquer — a coil that, when activated, interacts with earth’s magnetic field, using Lorentz force — a system of “reaction wheels”, and even a stellar camera which points to deep space and compares the stars as it sees them to onboard celestial maps.
If for some reason, the vehicles fail to catch the light, they could run out of juice and become another piece of space junk wandering in orbit for the next thirty years. So every 45 minutes — that is half of an orbit — the team is anxiously awaiting a signal of life. The faint packet radio signal will be collected by a pair of ground stations located on Svalbard near the North Pole and in Antarctica. These are the two points the satellites will have in their sight at every orbit. The stations are operated by Kongsberg Satellite Services, a Norwegian firm that manages 50,000 satellite contacts every month, another sign of the commoditization of space.
Right now, nearly two hours into the launch, the YAMs are silent. In the San Francisco office that took the first monitoring shift, the crew remains confident. They are right. Around midnight in France, 3 pm in California, the sats send a burst of hexadecimal data, saying that both are alive, kicking, and — more importantly — oriented towards the Sun.
The two Loft satellites carry a dozen payloads, ranging from an observation camera for the United Arab Emirates government, to an IoT experimental data transmission for Eutelsat, a payload for the DARPA (The advanced projects agency of the Pentagon) part of its Blackjack constellation, to ONERA, a French aerospace lab. Real customers — and relieved ones.
This first launch is the culmination of years of hard work from the Loft Orbital team.
The company was created in 2017 by three experienced space executives: Antoine de Chassy, 57, is the most senior with both scientific and business training. He was the CEO of Astrium Geo-Information in Virginia for six years and held various managing positions in the French aerospace complex, especially in the imaging business. Pierre-Damien Vaujour is the CEO based in San Francisco. He is a top-level aerospace engineer trained in France and the United States who worked at NASA’s Ames Research Center, among other experiences. The third, Alex Greenberg (the COO), has an Aerospace business consulting background when they all met five years earlier working at a San Francisco-based space data analytics company.
“We started with a few strong ideas on the new space business”, explains de Chassy.
“One, software is eating space as it does on the ground. That’s a critical element. Until recently, the space industry was mostly hardware-based within one-ton sats taking 5 years to design and build. Now, we think of a set of missions with the software cycle in mind. We buy the bus [the infrastructure of the satellite] from a supplier [Loft has currently two, soon more], we customize it with various types of payloads, and then we will continuously improve the platform by pushing software updates. It’s like a Tesla, where the performance and the value of the car are extended through Over-the-Air (OTA) updates”.
Except that in the case of the small-sat business, OTA updates are sent to an object zapping through the sky at 28,000 km per hour, 500 km above the Earth, and with a trickle data transfer rate of less than one megabyte per second.
These incredible constraints actually led to the development of ultra-compact software packages. The part that is similar to an operating system requires no more than 20Mb of memory space (I will devote a full episode of the series to the peculiar world of space-specific software, especially Cockpit, the amazing user interface developed by Loft Orbital for its customers to translate the complexity of orbital mechanics and allow anyone to manage a satellite from an iPad).
“The second key aspect is our customer-centric approach as opposed to a top-down, supplier design process”, states de Chassy. “We start with the needs of the client: what they want, the value they will extract from our service, and how much they will accept paying. We are completely inverting the legacy economics of space, shifting from CAPEX — a huge investment in a platform — to OPEX — a lease of a module or a usage fee — which will open the field to a whole new set of customers and uses”.
Loft is even building a software platform that will automatically calculate the price to charge a customer, large or small, based on the parameters of the mission.
In some instances, the company will be able to use the clients' payload capabilities when they are idle, which happens most of the time. In doing so, Loft will be able to vastly increase its array of customers.
In a typical scenario, the same orbital path will, for instance, involve a seven-minute fly-by over Norway to monitor fishing activities, then a border surveillance mission above central Europe, maritime monitoring of the Bosphorus, then various observations above Egypt, Kenya, and Mozambique. During that time, as much data as possible — visible spectrum and hyperspectral imagery — are processed onboard to minimize the downlink that will occur during a six to nine-minute transfer above the Antarctic ground station. Ksat will then beam the data to a geosynchronous platform as there is no fiber optic cable in the area. After that, the satellite will continue its course, this time towards North for a series of observations above Western Australia.
“We are building a distributed architecture of space sensors. This incredibly flexible system allows us to accommodate any type of client both before and during the course of the mission as we can reconfigure our payloads and use residual capabilities at will. In some cases, we will become clients of our own customers. We will have the infrastructure, we can flash software as needed for any kind of mission”, sums up the company’s president..
These operations are extremely complex, as they involve constant switching between clients, payloads, and a delicate power optimization for spacecraft that usually deliver only 50 watts on average (peaking at 150 watts), with each payload consuming less than a smartphone.
Loft Orbital has in fact a dual model of rideshare and timeshare. For the first one, clients have paid for their own task-specific payloads on which they have full control, as Loft does not see the data they are collecting. For the latter, the client pays on a usage basis, usually a combination of onboard resources (electrical power, processing power, time, data transfer) and available pre-existing payloads.
Some clients will even acquire capabilities to be activated at will. That’s the case for DARPA, which wants to be able to activate images and listening sensors on short notice in case of a strategic emergency — if an asset is suddenly blinded by a laser for instance. Having prepositioned flexible assets will then become critical.
Business-wise, Loft’s missions are meant to be profitable at launch. The company is aiming at operating margins consistent with the service sector and eventually with the software sector.
One of the consequences will be genuine democratization of space with some clients paying millions of dollars for a multi-year mission or a fraction of it for a small one. Loft is even willing to work out a business model that will make space affordable for NGOs or research.
The number of countries accessing space resources will increase drastically — a critical evolution in the context of climate change-related weather disruptions across the globe.
The analogy with cloud providers such as AWS will involve a customer able not only to access the available resources on a satellite but to run applications with the same simplicity it opens an instance on Amazon, Google, or Microsoft. A group of Earth sciences students will for example be able to upload software operating a hyperspectral camera to test an algorithm designed for a specific type of observation.
With this win, Loft Orbital is now facing a string of rich people’s problems. The first one is the change of scale of the company that plans to double in size in the next year to about 120 people spread between San Francisco, Toulouse, and Golden, Colorado. Large recruitment of engineers is already underway.
Then a new round of financing will occur this year as investors were waiting for the outcome of the June 30th launch. Since its inception, Loft has raised $16 million in capital, which is quite modest compared to some of its competitors that raised several hundreds of millions. With proven technologies, real customers, and multiple bookings for its next flights, Loft won’t have any problem securing a comfy Series B.
The ability to ramp up the pace of missions is now what keeps the team up at night. While signing up new customers should not be much of a problem, everything will have to scale accordingly: payload design, software developments, day-to-day mission control, customer service, etc.
Next year, Loft expects to launch several missions. The YAM-4 flight is already booked by the Canadian Space Agency, which will send a payload beaming quantum communication keys from space, and discussions are well underway to fill the seats for YAM-5. In 2023, Loft expects to launch one mission every quarter. In the next few months, it will also announce the sign-up of other prominent partners eager to use the company’s assets circling the globe.
The sky is no longer the limit.
— frederic.filloux@mondaynote.com
In upcoming episodes of the series, we will cover the economics and the geopolitics of New Space, the funding ecosystem of this risky business, the amazing software that powers these spacecraft, the Big Tech strategies, and some technical considerations such as sustainability. I will also focus on companies I find particularly daring in the field. These articles will be published roughly once every two weeks — providing I can resume traveling in the United States in the fall. In between, I will produce some media stories and some episodes of the Future of Cars series in between. So stay tuned and susbcribe!
Media, Tech, Business Models viewed from Palo Alto and…
111 
1
111 claps
111 
1
Written by
Editor of the Monday Note
Media, Tech, Business Models viewed from Palo Alto and Paris
Written by
Editor of the Monday Note
Media, Tech, Business Models viewed from Palo Alto and Paris
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adamalthus/the-third-wave-b4ec5380079a?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan Murray
Dec 4, 2017·7 min read
Amazon’s New Cloud Services Deeply Disrupt Traditional IT
tl;dr Amazon’s announcements at it’s 2017 developer conference define the beginning of new era in IT. The company announced a raft of new managed services that enable customers to focus on extracting business value from their application investments without the demands and costs of designing, building and operating complex infrastructures. Central to this third wave of cloud computing are new managed services for serverless-computing,machine learning, data analytics, and the Internet-of-things.
Amazon is executing a strategy to maximize time-to-value. The third wave of cloud computing will enable customers to focus on the business logic of their applications, derive analytical insights from real-time data streams and leverage the predictive power of deep-learning models all while leaving the infrastructural complexity and operational ’heavy-lifting’ to Amazon.
Businesses that have not already made the move to cloud face increasingly stark choices: Either leap-frog to third wave managed services or be for ever at a strategic and competitive disadvantage.
CEOs and Boards of Directors, who lack the understanding — or are unwilling — to make these choices are failing in their fiduciary responsibilities.
Amazon’s re:Invent developer conference held last week in Las Vegas was attended by forty three thousand souls. That Amazon can attract that many paying customers to their annual technical event underlines the company’s dominant and influential position in the cloud computing business.
Amazon’s cloud business can be traced back to 2002 but was formally re-launched as Amazon Web Services (AWS) in 2006. The AWS business now generates over $18B in annual revenue for Amazon and — despite aggressive competition from Microsoft and Google — AWS’ market share grew from 39% to 44% year-over-year. In the last five years AWS have launched 3,951 new services and features.
The pace of innovation in the AWS business ensures that Amazon remains not just the market leader but also the thought leader in cloud computing. Amazon’s announcements of new services and features at re:Invent every year have a habit of defining the agenda for the cloud business and set the benchmark for others to aspire to. This years conference was no different
The cloud business — since it’s inception in 2006 — can be divided into two distinct waves. Early cloud computing capabilities enabled the first wave of customers moving existing application workloads from private data centers to utility infrastructure managed by Amazon. Customers gained the advantage of reducing capital investments in complex physical data centers while tapping the cloud’s ability to provide utility infrastructure on demand, with costs based on consumption.
Most applications moved during the first wave could not take advantage of the native scaling and resiliency of the cloud but a second wave of migration emerged as customers built a new ‘cloud native’ applications composed from a growing portfolio of cloud services available from AWS — and other cloud providers. These new ‘native’ applications were architected to deliver the attendant advantage of cloud computing’s scaling, resiliency, management and security capabilities.
Leveraging the dynamic scaling and resilient fault tolerance advantages of the cloud environment still requires customer’s architects and developers to configure clusters of servers, wire together complex stacks of inter-connected software, implement complex networking topologies and configure and manage multiple layers of data storage technology. To deliver all of this customers still need to hire and retain advanced architecture design, development and operations skills.
Ultimately, all of the inherent complexity of building cloud applications has limited the impact of cloud computing to companies that can afford the human capital investments — or the financial investment required to pay someone else to do the work.
The third wave of cloud will remove the need for customers to concern themselves with the architectural and operational complexity of the computing and data infrastructures that underpin their applications. All of that complexity will be neatly packaged and abstracted away through a set of managed services offered by Amazon and other cloud providers.
“The only code you will write in the future is business logic. Everything else will be managed services”Werner Vogels — CTO, Amazon
Perhaps the most indicative of Amazon’s third-wave announcements was Amazon Sagemaker. Testing, deploying and leveraging deep-learning models is an incredibly complex undertaking. SageMaker provides data-scientists with an end-to-end suite of tools to develop and deploy sophisticated deep-learning models without having to deal with complex infrastructure issues.
Amazon is utilizing it’s deep-learning capabilities to optimize it’s own services and to deliver a growing portfolio of deep-learning enabled services for including object recognition in video, voice transcription and translation and natural language analysis.
The natural language analysis service — AWS Comprehend — is indicative of the nature of third wave cloud services. Comprehend provides a fully managed — deep learning enabled — natural language analysis capability delivering entity extraction, key phrase identification, source language identification, sentiment analysis and topic modeling. All of this can be done on real-time feeds of textual data and does not require the building, deployment or operational management of complex infrastructures by the customer. Businesses can focus on leveraging the analytical power of the service while Amazon ensures it scales, is reliable and secure. Customers only pays for the resources used in per second billing increments.
Amazon’s investments in a new serverless-computing model are anchored around AWS Lambda. As the name — serverless — suggests this new approach to application development removes the need for customers to configure or deploy server resources. Lambda — which is growing in adoption by 300% YoY — is a managed service that automatically executes software functions when triggered by events. Those events could be anything: from a user clicking on a button on a web page to a smart sensor sending an updated piece of data. AWS Lambda is rapidly becoming the hub for a wide range of Amazon services. A wide array of sophisticated cloud applications can now be composed from existing AWS services, linked and coordinated by the execution of Lambda functions.
Amazon made a number of key announcements in the data storage and management domain that underline the move to a third wave managed cloud computing model. Amazon Aurora is the company’s cloud native relational database. Aurora is one of the fastest growing services in AWS history with 250% YoY growth. At re:Invent, Amazon announced Aurora Serverless which provides database startup, shutdown and auto-scaling based on application demands without customers having to manage any database instances or infrastructure. Customers are billed only for the resources utilized while the database is running.
Amazon DynamoDB is the company’s cloud scale NoSQL database for unstructured data management. The company announced DynamoDB Global Tables last week which is a new fully managed services providing automatic data synchronization and fault resilience across a globally deployed database infrastructure. This new capability removes the need for customers to configure complex replication and data distribution topologies and infrastructures. Further reducing operational complexity Amazon announced DynamoDB Backup and Restore providing on-demand and continuous backup — with point-in-time — restore for DynamoDB tables.
One announcement of particular interest to enterprises will be Alexa for Business (AfB). Amazon is enabling their hyper-popular voice-activated smart agent technology for the office environment. AfB will integrate with common workplace technologies to provide full automation of meeting rooms, Audio-Visual presentations, meeting scheduling and other tasks. The AfB service will enable business customers to deploy and manage Alexa devices at scale and will enable individuals to integrate their personal and business Alexa profiles while at the office.
There were significant number of additional announcements at re:Invent 2017 including several related to the Internet-of-Things (IoT), new ways to manage computing instances and new types of compute instances optimized for deep-learning and other computationally intensive applications, a new graph database for relationship based data topologies called Amazon Neptune and AWS Cloud 9 a new integrated environment for developers to enable rapid construction, testing and deployment of AWS based applications.
Amazon’s announcements at re:Invent 2017 clearly mark a transition in the maturity of cloud computing and point the way to a fully managed third wave of cloud applications.
The transitions we’ve witnessed in the cloud computing business are analogous to those witnessed in the automobile industry — albeit over a vastly accelerated time-scale. In the last 150 years we evolved from craft-built motorized horse carriages, to mass produced cars to now driverless vehicles. The first wave of cloud computing was built by hand, the second wave of cloud enabled mass production of applications by the sewing together of component services.
Driverless vehicles abstract away all of the complexity of the driving process from the consumer while conveying them safely and comfortably from A to B. The third wave of cloud computing will do for business what driverless cars will do for motorists — sweeping away the underlying complexity that plagues business system IT while vastly improving time-to-value and the impact of investments in new data and deep-learning enabled capabilities.
Organizations face critical choices about the velocity and effectiveness of their transition to the cloud. Amazon’s announcements this week make those decisions imperative.
Companies that have not yet made the jump to cloud face stark choices: Either leap-frog to a third wave fully managed model or face the prospect of being for ever at a strategic disadvantage to the disruptors in their industry.
Companies that have already migrated their IT to the cloud must keep pace with these changes and the new third wave model or again face being at a significant strategic disadvantage.
Being behind by two or even one generation of cloud technology places companies at significant competitive disadvantage: The equivalent of selling horse drawn buggies in a world of driverless cars.
EVP & CTO @ Digital Prism Advisors
17 
17 
17 
EVP & CTO @ Digital Prism Advisors
"
https://medium.com/@jaychapel/saas-vs-paas-vs-iaas-where-the-market-is-going-fcc46771731d?source=search_post---------115,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 24, 2019·4 min read
SaaS, PaaS, IaaS — these are the three essential models of cloud services to compare, otherwise known as Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Each of these has its own benefits, and it’s good to understand why providers offer these different models and what implications they have for the market. While SaaS, PaaS, and IaaS are different, they are not competitive — most software-focused companies use some form of all three. Let’s take a look at these main categories, and because I like to understand things by company name, I’ll include a few of the more common SaaS, PaaS, and IaaS providers in market today.
Software as a Service, also known as cloud application services, represents the most commonly utilized option for businesses in the cloud market. SaaS utilizes the internet to deliver applications, which are managed by a third-party vendor, to its users. A majority of SaaS applications are run directly through the web browser, and do not require any downloads or installations on the client side.
Prominent providers: Salesforce, ServiceNow, Google Apps, Dropbox and Slack (and ParkMyCloud, of course).
Cloud platform services, or Platform as a Service (PaaS), provide cloud components to certain software while being used mainly for applications. PaaS delivers a framework for developers that they can build upon and use to create customized applications. All servers, storage, and networking can be managed by the enterprise or a third-party provider while the developers can maintain management of the applications.
Prominent providers and offerings: AWS Elastic Beanstalk, RedHat Openshift, IBM Bluemix, Windows Azure, and VMware Pivotal CF.
Cloud infrastructure services, known as Infrastructure as a Service (IaaS), are made of highly scalable and automated compute resources. IaaS is fully self-service for accessing and monitoring things like compute, storage, networking, and other infrastructure related services, and it allows businesses to purchase resources on-demand and as-needed instead of having to buy hardware outright.
Prominent Providers: Amazon Web Services (AWS), Microsoft Azure (Azure), Google Cloud Platform (GCP), and IBM Cloud.
SaaS, PaaS and IaaS are all under the umbrella of cloud computing (building, creating, and storing data over the cloud). Think about them in terms of out-of-the-box functionality and building from the bottom up.
IaaS helps build the infrastructure of a cloud-based technology. PaaS helps developers build custom apps via an API that can be delivered over the cloud. And SaaS is cloud-based software companies can sell and use.
Think of IaaS as the foundation of building a cloud-based service — whether that’s content, software, or the website to sell a physical product, PaaS as the platform on which developers can build apps without having to host them, and SaaS as the software you can buy or sell to help enterprises (or others) get stuff done.
The SaaS market is by far the largest market, according to a Gartner study that reported that enterprises spent $182B+ on cloud services, with SaaS services making up 43% of that spend.
While SaaS is currently the largest cloud service in terms of spend, IaaS is currently projected to be the fastest growing market with a CAGR of 20% plus over the next 3 to 4 years. This bodes very well for the “big three” providers, AWS, Azure and GCP.
What’s interesting is that many pundits argue that PaaS is the future, along with FaaS, DaaS and every other X-as-a-service. However, the data shows otherwise. As evidenced by the reports from Gartner above, IaaS has a larger market share and is growing the fastest.
First of all, this is because IaaS offers all the important benefits of using the cloud such as scalability, flexibility, location independence and potentially lower costs. In comparison with PaaS and SaaS, the biggest strength of IaaS is the flexibility and customization it offers. The leading cloud computing vendors offer a wide range of different infrastructure options, allowing customers to pick the performance characteristics that most closely match their needs.
In addition, IaaS is the least likely of the three cloud delivery models to result in vendor lock-in. With SaaS and PaaS, it can be difficult to migrate to another option or simply stop using a service once it’s baked into your operations. IaaS also charges customers only for the resources they actually use, which can result in cost reductions if used strategically. While much of the growth is from existing customers, it’s also because more organizations are using IaaS across more functions than either of the other models of cloud services.
Originally published at www.parkmycloud.com on May 21, 2019.
CEO of ParkMyCloud
28 
2
28 
28 
2
CEO of ParkMyCloud
"
https://blog.exploratory.io/importing-stripe-payment-data-visualize-and-schedule-with-exploratory-3b1422a9cf8b?source=search_post---------116,"Stripe is a developer friendly online payment service that many apps and cloud services use. It takes care of the nasty part of the payment including managing the customer’s credit card verification and processing.
We at Exploratory use Stripe for the payment as well. We love it, and we can’t imagine how we would have done without it. But only the problem is, it is not easy to understand the data in a truly meaningful way. It provides a dashboard page but just like any other cloud services, those canned reports aren’t cutting it.
For a starter, there is no way to understand visually and geographically where your customers are located or where the money is coming from.
And, any subscription based businesses want to understand the trend of the customers or revenues by segments such as new revenue vs. recurring revenues, instead of just the total revenue.
Further, any startups use more than a few, at least, cloud services to interact with customers or users, such as Intercom for customer interactions, MailChimp for mail campaign, Google Analytics for the website traffic monitoring, etc. This means, in order to gain deeper insights or answer to critical business questions like “Which segments are the most profitable?”, “Who are most likely churn in the next few weeks or months and why?”, “Which marketing campaigns or onboarding activities are more effective for converting to paid customers?”, and so on, you need to combine all, or at least some, of these data together and analyze on top of the combined data.
If you are a frequent reader of this blog you know it’s super easy to do such once you import Stripe data into R with Exploratory.
With Exploratory v3.3, we have now added Stripe data source out of the box. With a few clicks, you can create a connection with Stripe account through OAuth in Exploratory, you can quickly import Stripe’s various data sets available. And not only that, you can also schedule to refresh the data automatically at Exploratory Insight Cloud so that you can always on top of your latest payment data from Stripe.
Let’s take a look at how you can import data from Stripe, do a quick analysis, and schedule to refresh the data automatically.
First, select ‘Import Cloud Apps Data’ from the data frame menu.
Select ‘Stripe’ from the list.
This will open Stripe Data Import dialog.
If this is the first time for using Stripe data, you want to create a connection first by clicking ‘Create’ button.
This will open a pop-up window asking you to authorize Exploratory to access to your Stripe data.
Just in case if you are not familiar with OAuth, this doesn’t mean that you are allowing Exploratory to access to your data anytime. This authorization enables Exploratory to talk to Stripe for extracting the data in a read-only mode only when you want to access to your Stripe data via Exploratory.
Anyway, clicking ‘OK’ button will open a web browser window to sign into your Stripe account if you haven’t.
And once you signed in and authorize Exploratory in the next screen, you will see the following confirmation message.
Now go back to Exploratory Desktop and click on Refresh button.
Now you should see the newly created connection for Stripe under Connection.
There are various Stripe data sets. For this example, I’m selecting ‘Charges’ data set to see where the money is coming from.
Once you click ‘Import’ button, you will see the data imported like below under Table view.
You can also go to Summary view to quickly see the summary information at glance.
There is country information in this data set so we can quickly visualize where the money is coming from with World Map.
If you don’t have World Map yet, you can quickly install it from Map Extension dialog, which can be opened by clicking ‘Setup’ button next to ‘GeoJSON’ parameter. You can check out GeoJSON Map Extension Introduction blog post for more details.
blog.exploratory.io
Select ‘World Map’ for GeoJSON type, then select ‘ISO_A2’ (ISO Standard Alphabet Codes) as the key property for this GeoJSON Map so that we can map the data by using this key values.
Then I’m assigning ‘source.country’ column, which has the ISO standard country codes like US for the United States, ES for Spain, etc., to ‘Key Column’ and ‘amount’ column to ‘Color’.
I’m using Black background theme to make the countries stand out visually better.
Once you get your Stripe data into R with Exploratory, you can do all sorts of things by taking advantage of the power of R, such as data wrangling, applying statistics or machine learning algorithms, etc. with Exploratory’s UI. In the next post, I’m going to show how you can combine ‘Charges’ data and ‘Subscriptions’ data from Stripe and analyze monthly recurring revenue by segments like New, Recurring, and Churn.
But for today, there is one more thing.
Let’s say you have discovered useful insights with your Stripe data in Exploratory. Wouldn’t that be nice if you can access them from anywhere and anytime and they are always automatically refreshed with the latest data?
That’s when you want to publish your insights to Exploratory Cloud where you can schedule to refresh the data from Stripe automatically and share them with your team in a secure way.
Once published, you can see them listed at My Insight page, which will open automatically if you have the account and already signed in.
You can check out this blog post for more details on how to publish, share, and schedule your insights quickly.
blog.exploratory.io
If you want to try this out quickly, I have shared it as an Insight Template, which you can install directly inside Insight Template dialog UI in Exploratory Desktop.
Click ‘Install’ button to install and click ‘Import Data’ to start.
If you don’t have Exploratory Desktop yet, you can sign up from here for a free trial!
cran.r-project.org
cran.r-project.org
github.com
Unpacking Data Science One Step At A Time
10 
Thanks to hide kojima. 
10 claps
10 
Written by
CEO / Founder at Exploratory(https://exploratory.io/). Having fun analyzing interesting data and learning something new everyday.
Unpacking Data Science One Step At A Time
Written by
CEO / Founder at Exploratory(https://exploratory.io/). Having fun analyzing interesting data and learning something new everyday.
Unpacking Data Science One Step At A Time
"
https://medium.com/@educative-inc/cracking-the-gcp-certification-exam-how-to-prepare-c07f612065b9?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Educative Team
Jun 25, 2021·7 min read
Google Cloud Platform (GCP) is a suite of cloud services that run on the same infrastructure. GCP offers a range of services such as cloud storage, database management, developer tools, and more. Companies can improve their own products, services, and technologies by using the Google Cloud Platform. With cloud skills in such high demand, a Google Cloud certification is a great way to boost your resume and help you stand out from the competition. Today, we’re going to dive deeper into the topic of Google Cloud certifications and discuss benefits, exam questions, preparation, and more.
We’ll cover:
A Google Cloud certification demonstrates that you have experience with Google Cloud technologies. It shows hiring managers that you can maintain and implement Google Cloud products, services, and technologies into a company’s workflow.
The demand for cloud skills is high and will only continue to grow. Over 80% of hiring managers say that cloud certifications make applicants more desirable. A certification will also expand your career opportunities and most likely result in a significant pay increase.
Here’s what you need to do to get Google Cloud certified:
A Google Cloud certification will only be attractive to companies that actually use (or plan to use) the technologies. Microsoft Azure and Amazon Web Services (AWS) have a larger market share in the public cloud than Google Cloud, so getting a certification from one of those companies may be better depending on what the hiring company uses.
Although Google Cloud is behind Azure and AWS in market share, it’s used by many Fortune 500 companies because of its simplicity and great features. Google’s Professional Cloud Architect certification is currently the highest-paying cloud certification available.
Tip: If you have experience with other cloud services, you may consider implementing a multi-cloud infrastructure.
Google Cloud currently offers ten certificates that fall under three different levels: foundational, associate, and professional. Each level of certification is geared toward a different type of user and requires different background knowledge and experience. Let’s take a look at the certificates!
Foundational-level certifications confirm that you have general knowledge of cloud concepts and Google Cloud technologies, features, services, capabilities, and more. The certificate available at this level requires no prerequisites with Google Cloud. This certification level is ideal for you if you’re in a non-technical position and want to add value to your organization by learning more about Google Cloud.
Foundational certification:
Foundational Cloud Digital Leader
A Cloud Digital Leader knows and can explain the abilities of Google Cloud’s main products and services and can explain how cloud solutions support businesses.
This is a great certification for you if you:
Associate-level certifications focus on the core skills of monitoring, deploying, and maintaining projects on Google Cloud. The certificate available at this level recommends you have at least 6+ months of experience building on Google Cloud. This certification level is ideal for you if you’re newer to the cloud and want to use this learning path to eventually transition to professional-level certifications.
Associate certification:
Associate Cloud Engineer
An Associate Cloud Engineer deploys apps, controls operations, and handles business solutions. An Associate Cloud Engineer can also use Google Cloud Console and the command-line interface to execute tasks.
This is a great certification for you if you:
Professional-level certifications cover core technical job duties and evaluate developed skills in design, implementation, and management. The certificates available at this level recommend you have at least 3+ years of industry experience with at least 1+ years of experience on Google Cloud. The certifications at this level are ideal for you if you have industry experience and experience with Google Cloud products and solutions.
Professional certifications:
Professional Cloud Architect
A Professional Cloud Architect helps organizations leverage Google Cloud services and technologies. They have a strong understanding of cloud architecture and can create and manage strong, safe, scalable solutions to support business goals.
This is a great certification for you if you:
Professional Cloud Developer
A Professional Cloud Developer builds, tests, and deploys scalable cloud-native apps using Google’s practices, technologies, and tools.
This is a great certification for you if you:
Professional Data Engineer
A Professional Data Engineer makes data-driven decisions by performing data analytics and creates, builds, secures, and monitors data processing systems with a focus on security and compliance. They also deploy and train machine-learning models.
This is a great certification for you if you:
Professional Cloud DevOps Engineer
A Professional Cloud DevOps Engineer handles development operations that balance speed and reliability. They use Google Cloud tools and technologies to build software delivery pipelines, deploy services, and track incidents.
This is a great certification for you if you:
Professional Cloud Security Engineer
A Professional Cloud Security Engineer helps organizations build and implement secure infrastructures using Google’s security technologies.
This is a great certification for you if you:
Professional Cloud Network Engineer
A Professional Cloud Network Engineer enforces and maintains cloud network architecture.
This is a great certification for you if you:
Professional Collaboration Engineer
A Professional Collaboration Engineer uses business objectives to create and implement policies, practices, and configurations. They use their knowledge of mail routing and identity management to support secure data access and communication.
This is a great certification for you if you:
Professional Machine Learning Engineer
A Professional Machine Learning Engineer builds and uses machine learning models to solve business problems. They work with other job roles to collaborate on development processes and successful model solutions.
This is a great certification for you if you:
Note: For more in-depth information about each certificate, you can visit Google’s Cloud certification site. Each certification provides exam guides where you can view topics and case studies that may be included in the exam.
There are many jobs that benefit from a cloud certification. Let’s take a look at some of the most in-demand, along with their average salaries:
Note: Cloud certifications can help even non-cloud engineers, IT professionals, and others stand out from other applicants.
The exam question types vary in form and subject. Questions can be multiple-choice, coding questions, or case studies. Depending on your certification, you could answer questions on subjects like:
The Google Cloud certification exams are known to be difficult and rigorous, so it’s very important to engage in certification training prior to taking your exam. The exam doesn’t provide you with a score at the end, only a Pass or Fail result. These results don’t provide you with much insight into which areas you performed well in and which areas you need to improve in. This is a major reason why you need to take your preparation seriously.
Let’s discuss some ways you can do some Google Cloud training and prepare for your exam:
When studying theory, Google Cloud documentation is a solid resource. Dedicate time to studying and understanding the How-to and Concepts sections of the documentation.
For the practical skills, you’ll have to practice often and thoroughly. Use the Google Cloud Platform practice exams, and check out other resources, tutorials, and hands-on labs available online.
Read through the featured products and use the exam guides to determine which ones will be relevant to your preparation. Read the documentation for those products and focus on developing a strong understanding of them.
For the Professional Cloud Architect exam, Google provides you with sample case studies that you can use for practice. Read through the case studies and try to create solutions for them.
Google Cloud Platform provides you with sample questions that help familiarize you with the format and potential content that may be covered on your exam. It’s important to note that the sample questions don’t claim to fully represent the range of topics or level of difficulty of the actual exam. You shouldn’t use your results from the sample questions to predict your exam result.
The questions you’ll practice will depend on the certification you plan to get. We took a look at the certification exam guides and pulled some questions from them. Let’s take a look:
Cloud skills are in high demand and that demand continues to grow. A Google Cloud certification will demonstrate to potential employers that you have proficiency in the Google Cloud Platform. The certification exams are known to be difficult, so it’s important to prepare with the best resources available.
Happy learning!
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
12 
12 
12 
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
"
https://medium.com/@azifali/amazon-cloud-services-and-shit-like-that-7d22030eef85?source=search_post---------118,"Sign in
There are currently no responses for this story.
Be the first to respond.
Asif Ali
Apr 4, 2015·5 min read
You surely know that Amazon Cloud services has become the object of fascination for this generation’s CTOs and DevOps in SF when you can hear people talking about them on the side walk, at the restaurants and most public bathrooms.
So lets for a moment take a step back and analyze the phenomenon that is Amazon Cloud Services. It is already one of the fastest growing software businesses in history at $5+B (estimated) in revenues.
Dynamo and Redshift have especially become suddenly prominent with widespread adoption.
The most logical argument is startups don’t have to manage the infrastructure and they can focus on the core functionality of app. This is a valid argument. The other benefit is the ability to scale when necessary and scale back when not.
I am totally bought by these arguments however, I am not convinced that Amazon Cloud Services represent value to startups in the long term. It can be extremely expensive, non performant (for certain kinds of apps) and can create a vendor lockin problem requiring you to be stuck with Amazon for life without an easy way out.
I am also afraid that Amazon’s cloud is making software technologies less competent and people are deciding based on what they are comfortable with rather than what is the best solution for the problem.
Plenty of VC money means no one cares how much they are burning until of course they run out of money an realize that they were spending $500k on Amazon each month.
Many startups talk about how they stored large amounts of data, processed them but no one talks metrics when it comes to cost to store / compute that data. It apparently is not important anymore because, “hardware is cheap”. But it is not, Amazon can be very very expensive as startups start to scale.
To prove the point, I thought i’d analyze TimeHop’s blog and their DynamoDB implementation and costs associated with the adoption of tech.
One Year of Dynamo DB @ Timehop
Kevin Cantwell, Lead architect of Timehop writes at https://medium.com/building-timehop/one-year-of-dynamodb-at-timehop-f761d9fe5fa1
2,675,812,470
That’s the number of rows in our largest database table. Or it was, one year ago today. Since then, it’s grown to over 60 billion rows. On average, that’s roughly 160mm inserts per day.
When I think about it, that seems like quite a lot. But because the database we use is Amazon’s DynamoDB, I rarely have to think about it at all. In fact, when Timehop’s growth spiked from 5,000 users/day to more than 55,000 users/day in a span of eight weeks, it was DynamoDB that saved our butts.
A couple of points
TimeHop’s use case and the decision to use DynamoDB is not clear.
Kevin doesn’t clearly mention the reasons for DynamoDB or what other options he considered when he shifted to DynamoDB. The usage of MongoDB for their use case wasn’t apt but okay if that was their first version or proof of concept. DynamoDB was chosen because he read somewhere that DynamoDB was good. It is important when you choose a particular technology, you analyze it for its compatibility to your use case.
DynamoDB’s cost and throttling mechanisms is not suitable for writes, but the first TimeHop’s use case seems to be primarily fairly large amount of writes (when users sign up) — peaking at 275m writes at 55k signups per day. Even for 20k concurrent writes, DynamoDB is expensive (when compared to a custom implementation that involves an in-memory db).
Just for DynamoDB, and the pre-cache Redis infrastructure (not including other servers), my estimation is that they are spending over $10k a month. Add the rest of the infrastructure and that number is close to $20k-$30k (Redis cluster, UI, Image serving machines, CDN etc..)
Scaling out DynamoDB for writes can be very expensive.
There was no information on how many users will possibly sign up concurrently. For 1k concurrent signups, 5–6m items will need to be written into Redis and then subsequently batch written into DynamoDB. Since we don’t know how Timehop is able to access the data (in parts, or in full), these assumptions maybe wrong. But having 5–6m items means enabling a larger Redis cluster to be able to support that many writes. We also need enough reader / batch writers into DynamoDB processes and their compute instances which I have not considered in this chart below.
Scaling out DynamoDB for reads can be very expensive too.
200k concurrent reads (which is not that much at all for a high traffic consumer application), the costs would be $18,000 per month just for DynamoDB alone. 200k reads are possibly with just a few servers in a custom implementation using any in-memory db. But considering 60 billion items (approximately about 60TB of data @ 1k per item), I would think that all of the read / write infrastructure can be spread between 10–20 large nodes nodes which share read / write / compute and storage. This is considering the fact that there will be for about 180TB of data on SSDs with 3 factor data replication. 10–20 nodes is not much considering that such an infrastructure can easily be scaled to provide 10X more than the 200k concurrent reads that DynamoDB at $18k would provide.
TimeHop’s architecture seemed to be designed around DynamoDB rather than their use case.
The use of Redis (which is a key value store itself like Dynamo) was forced onto TimeHop thanks to the limitations that DynamoDB imposes on them. The limitations are not technical…but merely related to costs.
TimeHop has two primary use cases
For use case (1). a messaging queuing / streaming + async writes into an in-memory db would be ideal. Bursts of traffic can be accommodated by writing messages into a resilient queue like Kafka. Messages can be asynchronously processed to be written into any in-memory cluster or database for fast lookups.
For data writes on (1) and for use case (2) RocksDB (http://rocksdb.org/) with SSDs might just be perfect.
Conclusion: By using DynamoDB, TimeHop has added needless costs, complexity and bottleneck in case they would need to scale out. It would be a lot cheaper if they implement a shared nothing, fault tolerant fast lookup based system that used SSD for storage and lookups and get the same level of functionality with a lot more throughput at a fraction of the cost.
Technologist. Entrepreneur.
See all (910)
12 
12 claps
12 
Technologist. Entrepreneur.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/illumination-book-chapters/digital-intelligence-chapter-10-ff6ef122376c?source=search_post---------119,"There are currently no responses for this story.
Be the first to respond.
Chapter 1, Chapter 2, Chapter 3, Chapter 4, Chapter 5, Chapter 6a, Chapter 6b, Chapter 7a, Chapter 7b, Chapter 8, Chapter 9, Chapter 10, Chapter 11, Chapter 12, Chapter 13, Chapter 14, Chapter 15, Chapter 16, Chapter 17, Chapter 18, Chapter 19, Chapter…
"
https://medium.com/hackernoon/surface-digging-ibm-watsons-discovery-2ae4605cf65f?source=search_post---------120,"There are currently no responses for this story.
Be the first to respond.
Watson Discovery is the best-in-class cloud-native insight engine. It is at the forefront of all other cognitive AI-driven cloud services.
It deep dives into the dataset to unlock unprecedented answers. Mainly tuned to understand and oversee the past, existing and future trends along with surfacing the unique patterns obtained from both the niche and liberal data to aid businesses.
Watson Discovery is two years old now and the team behind Discovery has launched a handful of milestone features to date. Of which, anomaly detection, relevancy training, the semantic match, passage retrieval are some of the noteworthy pieces. Without missing out on the basic enhancements in user experiences, language assistance and deployment support.
Eliminate unwanted governing regulations and measures by implementing Element Classification. Here is a descriptive document by IBM to help you kick-start with its first-of-a-kind Element level Classification. Giving an overview, this will give you deeper insights into sentence-level elements.
Such as…
This enhanced feature utilizes trained and tested model for procurement contracts and financial services regulations.
If you want to visualize the data inside your insight engine, Visual Insights is the way to do it. Visual Insights will group all the data into connections by leveraging the power of Watson Discovery’s understanding of data.
The connections revolve around elements that are logical actualities such as relationships, concepts, locations, organizations, and more.
You can explore the dataset or document from the graphical representation or via the separate entities given in the form of tag-cloud view. In case if you lack a dataset, check out IBM’ news collection.
More about IBM Watson’s — Discovery in the next article.
#BlackLivesMatter
80 
80 claps
80 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Analyst Relations at Vue.ai | Democratizing A.I | LGBTQ ally
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/why-i-love-aws-lambda-reason-good-part-integration-auto-operation-scalable-nodejs-event-driven-b638b845dc84?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
My team and I have played around Lambda in different aspect, such as CI/CD pipeline (CodeCommit/CloudFoamtion), image processing and database query. Lambda has the form of water, which can penetrate to every corner of AWS services, great integrated with other services. And the event driven design by event source object. I would say Lambda is a masterpiece of cloud services.
Lambda is an effortless operational paradise, I just focus on Nodejs development without worrying about deployment, logs, public URL configuration.
— Perfect integration with powerful AWS services — API Gateway and Lambda. It helps to implement an production API with public URL, just within few clicks.
— It is completely event driven, it will only run when invoked. This is perfect for application services having quiet periods followed by peaks in traffic. Also this advantage helps and it is 100% no operations
— Scalability, Lambda can instantly scale up to a large number of parallel executions, for which the limit is controlled by the Number of concurrent requests. Downscaling is handled simply by automatically terminating the Lambda function execution when do code finishes running.
— https://www.contino.io/insights/5-killer-use-cases-for-aws-lambda
#BlackLivesMatter
27 
27 claps
27 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Algorithm Artist and tech blogger from Macau, Co-Funder of golding.cc. ex @yahoo coder. https://www.linkedin.com/in/waheng
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@arinbhowmick/ibm-cloud-satellite-wins-business-intelligence-group-award-for-business-2021-new-product-of-the-8e01ddd6f7e4?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arin Bhowmick
Dec 7, 2021·3 min read
IBM Design is committed to using our design thinking framework and innovative ideas to solve our clients problems. Our product team at IBM Cloud Satellite worked tirelessly to use this strategy to bring cloud technology to even the most regulated industries. I’m excited to announce IBM Cloud Satellite received a 2021 New Product of the Year award from the Business Intelligence Group’s Big Awards for Business.
The Business Intelligence Group’s annual competition is the premier “____ of the Year” program, recognizing companies, products and people which lead in their respective industries. Applicants were judged by a jury of business executives using a unique scoring system which measures performance across multiple business domains. “We are so proud to reward IBM Cloud Satellite for their outstanding 2021 leadership and achievements,” said Maria Jimenez, chief nomination officer of the Business Intelligence Group. “This year’s group of winners are clearly leading by example in the global business community.”
I’ve seen the dedication the team has put into this product and I’m so proud they’ve been awarded for their efforts.
IBM set a goal to be the top Hybrid Cloud provider, and revolutionize how customers can leverage cloud technology. Utilizing IBM’s Enterprise Design Thinking (EDT) framework, the product team at IBM Cloud Satellite set out to achieve this. The team conducted extensive user research, interviewing clients to better understand their customer journey and learning about a gap in the market which needed to be filled.
Shifting to the cloud can be a complicated process for any company but even harder for highly regulated industries. Strict compliance standards have typically prevented organizations like banks and hospitals from accessing cutting edge technology solutions. IBM Cloud Satellite was created to meet the needs of highly regulated industries in a rapidly evolving cloud market. Following the EDT framework, the product team built Satellite from the ground up using a ‘secure by default’ approach. This approach ensured compliance standards are met and clients feel confident and protected from rapidly growing security threats.
Enabling customers to embrace cloud solutions was just the first step. The product team used their research insights to create an intuitive support experience for customers who were using cloud for the first time. During the team’s beta testing, the process to set up a Satellite location required users to navigate 90 different screens. It was taking up to a month for clients to complete the complicated, multi-step process. The product team collaborated with engineering and product management to create a user experience that educates users about key Satellite concepts using progressive disclosure and cutting edge automation tools. This design solution, in combination with a rapidly evolving set of documentation, reduced the amount of customer support our users needed as they adopted Satellite to a simple 15 minutes process.
The product team at IBM Cloud Satellite turned the concept of the cloud upside down, enabling enterprises to use cloud services on-premises, at the edge, or on other public clouds. I’m so proud of our team at IBM Cloud Satellite for their innovation and diligence. They’ve managed to craft an exceptional product which is breaking barriers in the world of hybrid cloud, and I’m thrilled they’ve been recognized with this award.
Congratulations to the IBM Cloud Satellite product team for this stupendous recognition.
Arin Bhowmick (@arinbhowmick) is Vice President and Chief Design Officer, IBM Software Products, based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
76 
76 
76 
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
"
https://medium.com/@polkadotters/crust-network-decentralized-storage-protocol-for-polkadot-bb2318e757d0?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Polkadotters | Kusama & Polkadot validators
Mar 8, 2021·4 min read
Data storage and cloud services are really important in today's internet world. With blockchain technology coming into this space, the data can finally be secured using a decentralized platform such as Crust Network .
Curst Network is an incentive layer protocol for decentralized storage within the Web3 ecosystem. Its multi-layer architecture can support decentralized computing layers and also build a decentralized cloud blockchain technology that values data privacy and ownership.
Crust is adaptable to distributed storage layer protocols such as IPFS (Inter Planetary File Systems), P2P network architecture such as DAT and DHT technology (Distributed Hash Table).
Crust Network decentralizes data storage by distributing them into the different nodes within the network. Crust Network has two main goals to solve — Crust Cloud and storage for DApps.
Simply, Crust Cloud is a decentralized place for your data. Users can easily download and upload files similarly as they can now do it in Google Drive, Dropbox, or iCloud, but in a decentralized manner — meaning that they are still owners of the data.
Crust Cloud is based on the Crust decentralized storage protocol and provides encrypted wallets, staking, and cloud storage. In the future, it will bring the data market as well.
New DApps are being created every day and most of them have large data requirements. Developers and individual businesses need to store the data in a secure way as well as they need them quickly and easily at hand. Crust provides fast and inexpensive storage, especially in comparison to current cloud providers such as AWS.
Crust uses GPoS (Guaranteed Proof of Stake) consensus model and also MPoW (Meaningful Proof of Work).
GPoS use storage resources as a guarantee. Each node is required to provide storage space to make the staking process possible.
Crust Network is based on Substrate and it follows BABE & GRANDPA mechanism. The security is improved by the fact that the attacker needs to have a lot of Storage resources besides just owning CRU tokens to act maliciously.
MPoW is not a consensus, it’s an off-chain workload report powered by Trusted Execution Environment (TEE) to inspect storage work and generate work reports within TEE, and then upload the report to the chain.
On top of MPoW workload reports, there is a GPoS consensus. GPoS is derived from Polkadots NPoS (Nominated Proof of Stake) consensus and it generates blocks and distributes block rewards that are based on MPoW workreports. It follows two main directions — environment direction and workload direction.
Two co-founders are Leo Wang and Bova Chen. Leo Wang has previous experience in distributed storage and cloud computing, Bova Chen used to manage several funds as an investment director.
Crust Network will provide decentralized storage technology for Plasm Network, which is a Layer-2 scaling solution. Besides this “Polkadot ecosystem cooperation”, Crust will be cooperating with Maskbook, blockchain platform FLETA and 3Commas.
The native token of Crust Network is called Crust Token=CRU. It has several different use-cases including:
Initial Supply is 20,000,000 CRU
Besides CRU token, there are CSM (Crust Storage Market) and Candy tokens, that might be used in activities and marketing promotion.
Storing sensitive data in big, centralized data silos is not ideal. On the other hand, blockchains are too slow and inefficient for storing large amounts of data and also suffer from scalability issues. Privacy on the blockchain might also be a problem because sensitive data are accessible to the whole world, even if they are encrypted.
So, there is where Crust Network comes into play. Based on the Substrate, Crust will be interoperable with all Polkadot ecosystem projects. It will provide decentralized cloud storage for the whole ecosystem, not just Plasm Network, with whom Crust has already closed a partnership. Crust supports multiple storage layer protocols such as IPFS and DHT. It leverages TEE tech for storage work validation and Polkadot’s Substrate tech framework for the chain. It aims to build a decentralized cloud ecosystem that values data privacy and ownership.
Crust Network has a few advantages that need to be mentioned. They have low hardware requirements, fast data sealing (now <60s for 32G data) so that means a better user experience. And the integration to the Polkadot ecosystem will resolve into simple integration with Web3 DApps in the future. Crust is also privacy-oriented because all the data are encrypted, secured, and sovereign thanks to the TEE technology.
Crust Network will be one of the first projects that would like to secure a parachain slot on Kusama. We are very excited to have this project in the ecosystem and we believe it will be one of the first slot winners!
If you like this article, consider supporting us by changing your present Kusama validator to POLKADOTTERS
Stay tuned for further information about the Polkadot ecosystem projects here on Medium, our Twitter channel Polkadotters as well as in our Facebook Group Polkadot unofficial! If you are feeling really generous, you can send some DOTs to our donate address 🙂
16etYNuwvwbZYxy4FifqMq4KwZCMVSmTJ4XtE3WE3Cn1fQRQ
Czech bloggers & community builders. We are validators of Polkadot, Kusama, Darwinia, Crab, Bifrost, HydraDX, StaFi, Centrifuge under the name: POLKADOTTERS
83 
83 
83 
Czech bloggers & community builders. We are validators of Polkadot, Kusama, Darwinia, Crab, Bifrost, HydraDX, StaFi, Centrifuge under the name: POLKADOTTERS
"
https://medium.com/@cloud66/run-your-owncloud-on-any-server-with-containers-and-cloud-66-a2c800a551d7?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Sep 19, 2017·5 min read
Tired of file sharing/sync/cloud services like iCloud and Dropbox? Maybe you want to have more control over your own assets. Why don’t you run your own service using ownCloud?
This blog post will guide you how to run your ownCloud? on any server with Containers and Cloud 66.
ownCloud is a self-hosted file sync and share server. It provides access to your data through a web interface, sync clients or WebDAV while providing a platform to view, sync and share across devices easily — all under your control.
ownCloud’s open architecture is extensible via a simple but powerful API for applications and plugins and it works with any storage.
To just play around with ownCloud on your local box, just install docker and docker-compose (instructions here) and use the following docker-compose.yml.
Run docker-compose up and hit the end-point http://localhost:8080 and the first thing is to create an admin username and password.
Install the Desktop Client on your Mac/PC/Linux box and connect to the running ownCloud server.
NOTE
In this example, SQLite will be used as a database.  For larger installations (production), we recommend choosing a different database backend. Especially when using the desktop client for file syncing the use of SQLite is discouraged.
It’s time to run it in production on any cloud provider or on your own server.
Good to know, we are going to deploy ownCloud on our Cloud 66 for Containers offering. Cloud 66 for Containers is a complete solution for building, running and maintaining containerized apps in production. Allowing you to run a multi-tenant setup on your own server(s).
A service.yml is Cloud 66 service definition file to tell which services we want to run on our infrastructure. Want to know more about service.yml? Check our documentation.
Given the ephemeral nature of containers, it’s important to consider storage solutions to avoid data loss. That’s the reason you find the volume statement in service.yml
Hit deploy! and wait until we done with all the heavy lifting. After a couple of minutes, you deployed ownCloud!
When you hit visit Site you can setup ownCloud. Like in the local example we enter our username and password and we hit configure Database. We choice MySql and enter the credentials you find in the Mysql info page.
Use the private IP, DB name, DB App Username, DB App Password for the ownCloud MySQL credentials.
Fill out all the information and hit Finish setup! Congratulations, you running your own Cloud service.
Of course, this installment is not secure because we are using HTTP and not HTTPS. Cloud 66 support Let’s Encrypt (FREE!) SSL or bring your own SSL.
We take care of all the heavy lifting and when the SSL certificate is created and installed, you have your ownCloud running with SSL!
Look the green lock ;-)
Sometimes you need to run multiple applications on the same server. This could be because none of those applications have enough traffic to justify having a dedicated stack for itself or it could be because all the apps on the stack share many resources. What ever the reason, you can achieve multi-tenancy for your stacks with Cloud 66 for Containers.
Just add a traffic_matches: [""yourdomain.com""] to you service.yml and the traffic is redirected to the right service on your cluster.
Cloud 66 provides a lot of extra services like database management (backups/restores), scaling your databases, your services and servers and a complete CI/CD pipeline for your own containerized apps.
For now, enjoy your ownCloud!
That was a fun ride running ownCloud on any server with Containers and Cloud 66. Give us a spin a let us know what you think of our services.
Originally published at blog.cloud66.com on September 19, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
20 
20 claps
20 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@reinaldonormand/what-foreign-founders-should-know-in-order-to-create-a-global-tech-company-85c9f8132247?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Reinaldo Normand
Nov 28, 2017·7 min read
Widespread availability of cloud services and the explosion of Internet access via smartphones allowed global entrepreneurs to easily launch digital businesses. However, according to CBS Insights, the majority of the world’s unicorns are currently based in the US (54%) and China (23%).
A lot of articles have been written about the possibility that the next wave of tech giants could come from anywhere in the globe, but this hasn't really materialized so far. In this article, I'll explore why this is the case and what founders should know if they want to create the next Facebook or Alibaba.
Copycats are opportunistic ventures
Startup copycats may sound good in principle, as they are less risky and ride the market optimism expressed around the original business model. However, over the long term, the majority of copycats, if not all of them, will experience a premature death or become irrelevant if not acquired by a larger company.
My working theory is that copycat business models only work in China, a country with a population of 1.4 billion people and more than 860 million Internet users. Furthermore, the Chinese government has a non-official policy of blocking foreign competitors when they threat local incumbents or disturb “the social order” (i.e.: can not be controlled or censored by the authorities). Therefore, any entrepreneur outside China willing to copy a successful startup from the US will probably never achieve a global status.
The Groupon clones from the 2010s are a great cautionary tale. Between 2010 and 2012, many clones around the world were acquired by the original Groupon, media groups and other investors at the peak of their hype.
Brazil, at one point, had more than 1,400 Groupon clones. All of them are now dead or became irrelevant. Even Peixe Urbano, Brazil's most successful copycat, fell from grace. Despite being valued at hundreds of millions of dollars in 2012, the site was sold to Baidu in 2014 by a tiny fraction of its former valuation. It still continues to operate but on a much lower profile.
The same phenomenum occurred all around the world and the copycat model began to be sabotaged within. The original Groupon, valued at $16 billion in November 2011, is now (as of Dec' 17) worth less than $3.2 billion. The stock reflects the acquisition of Living Social, in 2016, the second most successful Groupon clone.
Copycatting is not a viable business model because it is not scalable and sustainable enough to vindicate high valuations outside China and the USA. Even Rocket Internet, the world’s #1 copycat factory, with investments in hundreds of companies, had their stock slammed by investors in the last years (valuation in Dec' 17 was lower than $3.4B). I bet their valuation will deteriorate over time.
The recent wave of Uber clones, in my opinion, will suffer the same fate of the Groupon clones, with the exception of Didi. After raising so much money to invest in subsidies, the valuation of ride sharing clones will probably drop dramatically as they won't be able to compete with their deep pocketed inspiration (Uber). Investors, entrepreneurs and local ecosystems may suffer immense setbacks.
If you are an entrepreneur who wants to build an influential global company, my advice is to focus on original products that create demand and/or solve real problems anywhere. Or, if you decide to enter a competitive category, do not only copy the pioneers but build something people do find useful and add a repeatable and scalable business model.
For instance, the instant messaging Viber, born out of Israel, began to differentiate from its competitor WhatsApp by selling stickers, games and credits for international VoIP calls. The startup was acquired by Rakuten in 2014 by $900 million and as of Sept. 2017, it boasted more than 920 million users registered.
The same successful story can be told about Spotify, the Swedish music streaming service that outlived pioneers such as Pandora Radio and heavyweights such as Apple Music, Youtube Music and Amazon Music. Spotify focused on affordable pricing, great selection and easy to use interface on all platforms. The startup is valued northwards of $13 billion.
It is ok not to be the first but it is not ok to not be the best. By only copying someone else, you""ll always be one step behind and at the will of the copied business' performance. Don't cut corners, innovate and focus on the long term.
You can’t have a global company anywhere
The media loves to repeat the mantra that anyone, anywhere, can create the next Google. This is true as talent is not country, race or religion specific. However, the statement implies that global companies can grow in any city in the world. In my experience, this is far from the truth and it is more likely that you'll need to move your operations to the closest tech cluster.Why? There are several reasons.
The first is related to talent. If you reach the success of a Facebook, Amazon or Tencent, you'll need to scale your startup by hiring hundreds or even thousands of the best engineers, designers and product managers available. The chances are that your hometown location won't be able to provide the tech talent necessary to sustain this hyper growth unless you are in a large metropolitan area. The availability of great universities churning capable professionals all year around is of utmost importance, therefore it is not a coincidence some regions in the US, Europe and China host today's top tech clusters. Also, global tech businesses are conducted in english and again, unless you're born in a Scandinavian or english speaking country, attracting english speaking talent to where you are located is really tough.
Secondly, great startups need a streamlined environment to thrive and, unfortunately, the business environment in most of the world is full of corruption, high taxes and bureaucracy that will slow down even the best entrepreneurs. Try to do business in any emerging country and you'll understand what I am talking about. Any startup that wants to go global and be influential need a place that obey the rule of law and honor contracts. In many cases, potential small and medium sized startups are incorporated in other jurisdictions to make sure they can prosper.
And last but not least, to be successful on a global scale, your startup and your team must prosper by merit and not by who you know. In other words, your ecosystem needs to foster a meritocracy based mindset instead of a relationship based one. If you live in a place that is not progressive enough regarding entrepreneurship, diversity and modern management techniques, you're doomed from the start and the chances are that your success will be local or, at best, regional.
I believe the only type of tech startup that can be run from anywhere and still be successful globally to a certain scale are gaming companies. This is more due to the unstable nature of the gaming business and the widespread distribution channels provided by Apple and Google than anything else.
Focus on growth, not on revenue
In most places in the world, sound economics are practiced and focusing on turning a startup profitable seems to be the right thing to do. Therefore, companies are valued by a method called discounted cashflow (DCF). Sophisticated financial buyers interested in acquiring a company will run “as complex as they want” calculations to figure out how much they are willing to pay today for the rights to earn a business’ future free cash flow.
However, the Internet and smartphones have changed things upside down.The problem in the digital world is that most tech startups may take many years to generate positive cash flow and are usually pre-revenue when evaluated at the present. That would make their valuation to be zero for the near future.
To solve the problem, some ecosystems like Silicon Valley basically abolished the DCF model for a framework that is seen by outsiders as controversial, opaque, and subjective. As an extrapolation, this methodology could be analog to quantum physics: people know it exists, but they have no idea how it works.
In order to elaborate a fair startup valuation, Silicon Valley type investors take into consideration the founding team’s track record, the potential of disrupting a large market, growth rate (users or any other metric), retention, the existence of proprietary technologies, how feasible is it to scale up the operations, and the “momentum” of that business.
In practice, what happens with this Silicon Valley methodology is that the valuation of an early stage startup becomes what the founders are able to sell to investors. A good story, salesmanship, confidence, a great team and supply and demand are key factors that may define the valuation. Regular economics are not taken into consideration because these startups are too early and pre-revenue. Like before the Big Bang, the common rules cannot be applied.
Investors are willing to pay so much for tech startups with no profit because they are more likely to grow exponentially and be worth a much higher multiple than conventional companies, as the majority of startups can scale up without a proportional increase in fixed costs. Facebook, for instance, lost money from 2004–2008 and, in 2016, made $10 billion in profit. Its market capitalization, as of September 2017, was worth 37 times its earnings.
My advice to future tech entrepreneurs willing to start world changing companies is to embrace the Silicon Valley framework and, if possible, move to a mature ecosystem where investors won't use discounted cash flow to evaluate your startup. In the digital space, an early focus on profits may end up crippling the startup growth and thus its survivability in the long term. The cosmetics may not make sense or look reassuring, but they definitely work if you become the #1 or #2 in your vertical.
In non-mature ecosystems, though, if you need money to grow your business, you''ll be left with investors using the traditional DCF model, which may work only for e-commerce or business models that generate quick cashflow. However, if you are competing globally, it is unlikely you''ll be able to raise enough money to grow as fast as Airbnb, Slack or Uber and win the long term game.
The value of your company will probably follow the rules of the ecosystem you are based on. You cannot have Silicon Valley priced rounds in places where investors value cash flow. If this affects your survival or competitiveness, your options might be to either migrate or change your company’s profile to focus on cash flow. Both come with many challenges and must be thought through carefully.
Thanks for reading.
CEO and co-founder of InnovaLab (http://www.innovalab.us). I blog about tech, innovation, entrepreneurship and culture. Opinions are my own.
35 
35 
35 
CEO and co-founder of InnovaLab (http://www.innovalab.us). I blog about tech, innovation, entrepreneurship and culture. Opinions are my own.
"
https://medium.com/@nutanix/devsecops-the-next-wave-of-cloud-security-1196adc7c5f3?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Dec 26, 2016·2 min read
The adoption of DevOps, agile and public cloud services among businesses worldwide is increasing by the day. These are seen as the major shift in enterprise IT, and as the next wave after Internet. Thanks to digital democratization, due to which businesses have to be nimble to remain competitive. That said, security threats and cybercrime continue to outsmart this business despite having cutting-edge security wall around them. To this end, DevSecOps was born to bridge the security gap into DevOps, just as DevOps bridged the development and operations divide.
Plugging in the right chord: Security into DevOps, on the cloud
Business leaders now understand that moving to the cloud is not just any tech adaptation, but it is more about speed of service delivery and dynamic scalability. One of the most significant paybacks of the DevOps has been better software quality delivered faster, even on the cloud.
Cloud technology dissolves enterprise perimeter, the key construct around which security solutions have been developed. Earlier,security concerns were holding back many businesses from jumping on to the cloud bandwagon. And when the idea of perimeter and boundary was once again threatened by new security requirements such as those warranted by Bring Your Own Device (BYOD) policies, the IT industry slowly started to embrace the cloud. Security professionals are now leveraging real-time analytics and have also adopted “Continuous Security” in clear parallel to the “Continuous Integration” and “Continuous deployment” approach of the DevOps movement.
Image Source: RSAConference, 2016, DevSecOps In Baby Steps
DevSecOps Tools: Filling in the Security Gap
Many enterprises have started to explore ways of making application quality and security testing more scripted, continuous, and automated. With DevSecOps, they are taking an automation approach for security tests throughout development, even on the cloud. They are even integrating security-feature design and implementation into the development lifecycle in ways that wasn’t possible before.
With DevSecOps on the cloud, security becomes an essential part of the development process itself instead of being an afterthought.
Read the full article in our blog post to learn more about DevSecOps - the next wave of cloud security, here.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
See all (807)
7 
7 claps
7 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Talend/serverless-a-game-changer-for-data-integration-32c0b7f07b1d?source=search_post---------127,"Sign in
There are currently no responses for this story.
Be the first to respond.
Talend
Sep 19, 2018·3 min read
Following Amazon, Google launched Google App Engine in 2008, and then Microsoft launched Azure in 2010.
At first, cloud computing offerings were not all that different from each other. But as with nearly every other market, segmentation quickly followed growth.
In recent years, the cloud computing market has grown large enough for companies to develop more specific offers with the certainty that they’ll find a sustainable addressable market. Cloud providers went for ever more differentiation in their offerings, supporting features and capabilities such as artificial intelligence/machine learning, streaming and batch, etc.
The very nature of cloud computing, the abundance of offerings and the relative low cost of services took segmentation to the next level, as customers were able to mix and match cloud solutions in a multi-cloud environment. Hence, instead of niche players addressing the needs of specific market segments, many cloud providers can serve the different needs of the same customers.
The latest enabler of this ultra-segmentation is serverless computing. Serverless is a model in which the cloud provider acts as the server, dynamically managing the allocation of resources and time. Pricing is based on the actual amount of resources consumed by an application, rather than on pre-purchased units of capacity.
With this model, server management and capacity planning decisions are hidden from users, and serverless code can be used in conjunction with code deployed in microservices.
As research firm Gartner Inc. has pointed out, “serverless computing is an emerging software architecture pattern that promises to eliminate the need for infrastructure provisioning and management.” IT leaders need to adopt an application-centric approach to serverless computing, the firm says, managing application programming interfaces (APIs) and service level agreements (SLAs), rather than physical infrastructures.
The concept of serverless is typically associated with Functions-as-a-Services (FaaS). FaaS is a perfect way to deliver event-based, real-time integrations. FaaS cannot be thought of without container technologies, both because containers power the underlying functions infrastructure and because they are perfect for long-running, compute intensive workloads.
The beauty of containers lies in big players such as Google, AWS, Azure, Redhat and others working together to create a common container format — this is very different from what happened with virtual machines, where AWS created AMI, VMware created VMDK, Google created Google Image etc. With containers, IT architects can work with a single package that runs everywhere. This package can contain a long running workload or just a single service.
Serverless must always be used together with continuous integration (CI) and continuous delivery (CD), helping companies reduce time to market. When development time is reduced, companies can deliver new products and new capabilities more quickly, something that’s extremely important in today’s market. CI/CD manages the additional complexity you manage with a fine grained, serverless deployment model. Check out how to go serverless with Talend through CI/CD and containers here.
Talend Cloud supports a serverless environment, enabling organizations to easily access all cloud platforms; leverage native performance; deploy built-in security, quality, and data governance; and put data into the hands of business users when they need it.
Talend’s strategy is to help organizations progress on a journey to serverless, beginning with containers-as-a-service, to function-as-a-service, to data platform-as-a-service, for both batch and streaming. It’s designed to support all the key users within an organization, including data engineers, data scientists, data stewards, and business analysts.
An organization’s data integration backbone has to be native and portable, according to the Talend approach. Code native means there is no additional runtime and no additional development needed. Not even the code becomes proprietary, so there is no lock-in to a specific environment. This enables flexibility, scale and performance.
The benefits of serverless are increased agility, unlimited scalability, simpler maintenance, and reduced costs. It supports a multi-cloud environment and brings the pay-as-you-go model to reality.
The serverless approach makes data-driven strategies more sustainable from a financial point of view. And that’s why serverless is a game changer for data integration. Now there are virtually infinite possibilities for data on-demand. Organizations can decide how, where, and when they process data in a way that’s economically feasible for them.
Official news and insights from Talend, a leader in data integration for cloud and big data.
30 
30 
30 
Official news and insights from Talend, a leader in data integration for cloud and big data.
"
https://medium.com/@kyleake/cloud-comparer-a-public-cloud-comparison-ilyes-68d1d9772a7a?source=search_post---------128,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
May 29, 2019·2 min read
Disclaimer: This is my personal Medium blog, therefore anything I post, share, and comment don’t reflect my employer.
Hello all, this post is nothing but to promote “Cloud Comparer by Ilyes”, which compares the various managed cloud services offered by the major public cloud providers in the market — AWS, Azure, Google, IBM, Oracle, and Alibaba. “The…
"
https://medium.com/@jaychapel/wasted-cloud-spend-to-exceed-17-6-billion-in-2020-fueled-by-cloud-computing-growth-7c8f81d5c616?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2020·4 min read
More than 90% of organizations will use public cloud services this year, fueled by record cloud computing growth. In fact, public cloud customers will spend more than $50 billion on Infrastructure as a Service (IaaS) from providers like AWS, Azure, and Google. While this growth is due in large part to wider adoption of public cloud services, much of it is also due to growth of infrastructure within existing customers’ accounts. Unfortunately, the growth in spending often exceeds the growth in business. That’s because a huge portion of what companies are spending on cloud is wasted.
Before we get to the waste, let’s look a little closer at that growth in the cloud market. Gartner recently predicted that cloud services spending will grow 17% in 2020, to reach $266.4 billion.
While Software as a Service (SaaS) makes up the largest market segment at $116 billion, the fastest growing portion of cloud spend will continue to be Infrastructure as a Service (IaaS), growing 24% year-over-year to reach $50 billion in 2020.
Typically, we find that about ⅔ of enterprise’s average public cloud bill is spent on compute, which means about $33.3 billion this year will be spent on compute resources.
Unfortunately, this portion of a cloud bill is particularly vulnerable to wasted spend.
As cloud computing growth continues and cloud users mature, you might hope that this $50 billion is being put to optimal use. While we do find that cloud customers are more aware of the potential for wasted spending than they were just a few years ago, this does not seem to be correlated with cost optimized infrastructure from the beginning — it’s simply not a default human behavior. We frequently run potential savings reports for companies interested in using ParkMyCloud, to find out whether or not they will benefit from using the product. Invariably, we find wasted spend in these customers’ accounts. For example, one healthcare IT provider was found to be wasting up to $5.24 million annually on their cloud spend, an average of more than $1,000 per resource per year.
Here’s where the total waste is coming from:
Idle resources are VMs and instances being paid for by the hour, minute, or second, that are not actually being used 24×7. Typically, these are non-production resources being used for development, staging, testing, and QA. Based on data collected from our users, about 44% of their compute spend is on non-production resources. Most non-production resources are only used during a 40-hour work week, and do not need to run 24/7. That means that for the other 128 hours of the week (76%), the resources sit idle, but are still paid for.
So, we find the following wasted spend from idle resources:
$33.3 billion in compute spend * 0.44 non-production * 0.76 of week idle = $11 billion wasted on idle cloud resources in 2020.
Another source of wasted cloud spend is overprovisioned infrastructure — that is, paying for resources are larger in capacity than needed. That means you’re paying for resource capacity you’re rarely, or never, using.
About 40% of instances are sized at least one size larger than needed for their workloads. Just by reducing an instance by one size, the cost is reduced by 50%. Downsizing by two sizes saves 75%.
The data we see in ParkMyCloud’s users’ infrastructure confirms this, and in the problem may be even larger. Infrastructure managed in our platform has an average CPU utilization of 4.9%. Of course, this could be skewed by the fact that resources managed in ParkMyCloud are more commonly for non-production resources. However, it still paints a picture of gross underutilization, ripe for rightsizing and optimization.
If we take a conservative estimate of 40% of resources oversized by just one size, we find the following:
$33 billion in compute spend * 0.4 oversized * 0.5 overspend per oversized resource = $6.6 billion wasted on oversized resources in 2020.
Between idle and overprovisioned resources alone, that’s $17.6 billion in cloud spend that will be completely wasted this year. And the potential is even higher. Other sources of waste include orphaned volumes, inefficient containerization, underutilized databases, instances running on legacy resource types, unused reserved instances, and more. Some of these result in significant one-off savings (such as deleting unattached volumes and old snapshots) whereas others can deliver regular monthly savings.
That’s a minimum of about $5 million wasted per day, every day this year, that could be reallocated toward other areas of the business.
It’s time to end wasted cloud spend. Join ParkMyCloud in taking a stand against it today.
Originally published at www.parkmycloud.com on March 10, 2020.
CEO of ParkMyCloud
See all (317)
26 
26 claps
26 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sonm/a-new-chapter-in-data-safety-is-fog-safer-than-cloud-42780697dcbd?source=search_post---------130,"There are currently no responses for this story.
Be the first to respond.
In his book ‘The Big Switch’, Nicolas Carr compares the evolution of cloud services to the distribution of electricity. Before distribution networks were introduced, most companies had to produce their own electricity. The arrival of power plants coupled with the development of distribution networks later made it possible to generalize the use of electricity through a simple connection.
Cloud has certainly arrived in a similar fashion. The evolution of data computing influenced many companies to move their data centers to third party cloud servers, whose own data centres consist of hundreds and thousands — or even millions — of servers all around the globe. Although they are optimized, these data centers do not offer transparency for they are away from users’ access; they are also limited in terms of performance and latency.
The issue that plagues the current cloud computing infrastructure is centralization. The market has slowly fell victim to the increasing monopolization. It means that the world’s data computational needs are now focused on only few data centers, which solely becomes the reason for increased costs, downtimes, and degrading security and privacy.
Imagine why service providers like Dropbox faces an outage that lasts for two long days? Or how a service like Code Space shuts it doors after its console gets hacked; its data eventually deleted? Or why the private login details and pictures of many celebrities get leaked online? It is simple to narrow these events down to one thing: The traditional cloud paradigm is flawed.
Fog Computing: Get Lower for Higher Standards
The market is in dire need of a solution that shifts the load of centralized cloud services. It should be able to disseminate the data computing load between devices, thus forming a web of distributed cloud systems for the development of real-time communication services in the cloud.
Recently, the distribution of mini data centers at the end of the network was discussed in order to be able to process the real-time data of the Internet of IoT objects or to provide mobility in a 4G network. This is called Fog Computing.
Fog Computing shifts the cloud computing paradigm and moves it to the lower level of the network. Instead of processing some task using the cloud, users can use all the devices surrounding us: personal computers, smartphones, even coffee makers and traffic lights. To be able to gather computational data from nearby servers also improve security and privacy standards, for the data becomes localised and within reach.
SONM, a project which hybridizes Fog with an open-source PaaS technology named Cocaine, helps the monitoring and searching of cloud computing services within a cluster. The project believes Fog computing can be a safer option, but when alone, it remains vulnerable to untrusted local hubs. The project developer elaborates:
“End users can always choose a hub manually according to their choice. Trust element plays a very important role in this sphere — as no corporation around the world has the technologies, which guarantee 100% data safety — neither Amazon nor DO, minor clusters or fog technologies. There is always a chance that someone dumps your traffic. Always.”
Adding that Amazon and SONM almost offer the same level of security to its users, the developer states that users can choose to trust either or neither.
“You can either trust Amazon or not. Same here: you either trust some particular hub or not. You can simply choose some of the trusted hubs or clusters and forbid data proceeding for the other hubs — as simple as that. I may decline from the topic but the problem of trust is actually the problem of all social networks overall — and the same problem exists in the blockchain sphere: banks orientate themselves on private blockchain simply as it provides corporate papers privacy.”
Join the ICO on June 15th : ico.sonm.io
Sonm Website: http://sonm.io/Bitcointalk ANN: https://bitcointalk.org/index.php?topic=1845114.0Bitcointalk Bounty Thread: https://bitcointalk.org/index.php?topic=1881191.0Medium: https://medium.com/@sonm/Slack: https://sonmio.signup.team/Telegram (English) @sonm_engTelegram (Russian) @sonm_rus
SONM is a global fog computing platform for general purpose…
54 
54 claps
54 
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GorillaStack/3-easy-steps-to-save-money-on-aws-f8ae35e8e001?source=search_post---------131,"Sign in
There are currently no responses for this story.
Be the first to respond.
GorillaStack
Nov 2, 2015·3 min read
We all love using AWS for the breadth and depth of their cloud services. Combining these services with the ability to interact with virtually everything through their API’s, it has never been easier to deploy new services and applications.
Developers and infrastructure engineers often have a field day with their new found productivity and flexibility… all until they receive a tap on the shoulder from their red faced boss telling them they have gone way over budget.
We’ve put together a handy list of some easy wins for quick AWS Savings:
Power cycling instances and autoscale groups is one of the safest ways to save money in AWS. If you have development and staging environments, they usually aren’t required 24/7 — so you should be able to switch them off when everybody packs up and goes home, effectively your servers sleep when your team does.
The most practical way to achieve this, is to tag your resources according to their role or environment and then run a scheduled job which turns off all at your dev and staging instances at say 8PM; and then schedule another job to fire them back up at 7AM in the morning so they are ready to go when they early birds come in.
With autoscale groups, it is a similar process but instead of turning your servers off, you set your minimum, maximum and desired instances to zero for the period you want to switch the servers off and then change it back when you want them to come back on.
The following is a great way to save money and ensure that your teammates tag resources correctly. Tagging is a key element of making your AWS account more manageable and also makes it much easier to understand where your dollars are being spent.
To achieve this, just run a scheduled job that will terminate resources in your AWS account which are missing the tags that you have defined as required, for example ‘Name of Project’.
Puppet labs have released an open source Python library that you can use if you don’t feel like writing up your own code — https://github.com/puppetlabs/py-awsaudit.
Another great open source tool for increasing your tag quality and cost accountability, is Auto Tag by GorillaStack which automatically adds a tag for the user who created the resource!
Spot pricing can allow you to save up to 90% on some resources, yes that wasn’t a typo — 90%. Early on, many people were hesitant about using spot pricing because it was not well understood and difficult to utilise with many workloads, Amazon has been working hard to make spot pricing more accessible.
You can now use spot pricing when launching Elastic Map Reduce clusters, in EC2 Autoscaling groups, Elastic Beanstalk and AWS Cloudformation to name a few. If you are a cost conscious customer, it has never easier. With the launch of the spot fleet api, spot pricing is really starting to become more accessible and will deliver amazing savings.
By implementing one or two of these strategies you can stand to save a significant chunk from your bill and get your boss off your back about the budget.
If you need help getting a handle on your AWS bill today, without writing custom code and setting up jobs, GorillaStack can help. We currently have two key products to help you save, Power Cycle and Power Scale.
Out of the box, these tools give you intuitive scheduling, timezone support and the ability to snooze or cancel if you need to keep your servers or autoscale groups alive when you’re working late.
You can also receive flexible AWS billing alerts about your spend into tools you know and love like Slack and Hipchat, to make sure you never get caught out spending more than you expect.
GorillaStack is here to help your resources go to sleep when your developers do and to create tools that provide significant AWS Savings. Our aim is to make the Cloud a better place.
Originally published at blog.gorillastack.com on November 2, 2015.
AWS Cost Management — Be a DevOps Hero with our automated cost optimisation software for Amazon Web Services
53 
53 
53 
AWS Cost Management — Be a DevOps Hero with our automated cost optimisation software for Amazon Web Services
"
https://blog.marshal.io/marshal-uncovers-1-5-million-data-threats-in-48-hours-7e82661fb103?source=search_post---------132,"We created Marshal to be the easiest way for anyone to discover exposed sensitive data in their cloud services. Since we officially opened up the public beta on Wednesday, Marshal has scanned nearly 500,000 files to help people uncover more than 1.5 million pieces of sensitive information that were exposed in their cloud accounts.
Marshal works by plugging into your Box, Dropbox, Google Drive or Microsoft OneDrive accounts. Once you give it permission, Marshal scans all the files in your accounts to find exposed sensitive data threats such as Social Security Numbers, credit card numbers, phone numbers and email addresses.
As we all store more data in the cloud and share that data with more people, it’s easy to lose track of what information is lying there, unprotected. We built Marshal because we believe every business should know exactly what it’s sharing.
If you are one of the people who tried Marshal (or are going to), drop us a line and let us know what you think! We’re always looking to make improvements to the service.
Direct reports from the team behind Marshal.
5 
Thanks to Matthew Anderson. 
5 claps
5 
Written by
GigaOm Research Director.
Direct reports from the team behind Marshal.
Written by
GigaOm Research Director.
Direct reports from the team behind Marshal.
"
https://medium.com/@nutanix/aws-cloud-per-second-billing-a-game-changer-for-devops-engineers-f2028ee9083c?source=search_post---------133,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Oct 4, 2017·3 min read
When AWS launched it’s cloud services in 2006, the per hour billing model for compute resources was a game changer for the IT world. Since then, AWS has taken over the cloud computing market as a leader, while broadening the range of cloud services offered every year. However, the per hour billing was one of the major issues for many customers, who had applications that used EC2 for only a few minutes, but had to pay for the entire hour.
Recently, AWS announced the support for per second billing of it’s EC2 service (Compute) that could further revolutionize its usage, and help reduce costs for companies that take advantage of auto-scaling architectures. Here are the top 5 per second billing implications for DevOps teams.
Autoscale As You Want Without Cost Worries — Most engineers were vary of auto-scaling at will, due to hourly charges of EC2 even though they might need additional compute capacity for 15 minutes. With the per second billing, you can be worry free and just scale your EC2 (compute) as and when you need.
No More Custom Logic For Hourly Batches — Many customers use EC2 for jobs and batch processing purposes. Due to the hourly pricing, the engineers have written custom logic to benchmark and constantly tune the number of nodes in their batch clusters to ensure they are cost optimised. This one will free up headaches for DevOps engineers using EC2 for batch processing and jobs cluster.
Need To Speed Up EC2 Launch — With per second billing, it’s important to take advantage of pre-baked AMI’s for launching your EC2 instances, instead of complete configuration setup on every instance launch. This will reduce costs anywhere from 5 to 15 minutes. It’s time to create pre-baked AMI’s for faster instance launches and to save costs.
Bench-mark AWS Lambda or EC2 — Be prepared to compare whether to use Lambda or EC2 for new workloads and applications than ever before. Choose the best path to your goal stringently as the serverless Lambda has just gotten a lot more competitive than EC2.
Your Big Data Just Got Easier — With EMR also supporting per second billing, your job has become a lot easier. As a DevOps engineer, you no longer need to constantly benchmark your EMR clusters for the number of nodes, and the job processing optimization for the number of times per hour billing.
The per second billing is not only beneficial for AWS customers to lower costs in many cases, but also in making the DevOps engineer’s life easy.
These implications are just few of the many ways in which DevOps engineers can benefit from the per-second billing for AWS. Did we miss anything? We would love to know if it is a boon or a bane for all the DevOps engineers out there. Leave a comment with your thoughts on this.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
See all (807)
38 
38 
38 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cameroncoward/the-gnubee-personal-cloud-2-is-an-affordable-open-source-nas-567f23c68323?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cameron Coward
Oct 12, 2017·2 min read
When it comes to data storage, online cloud services have stolen most of the headlines for the past several years. It’s easy to see why: cloud storage is cheap, convenient, and secure. But, that doesn’t mean third-party cloud storage is the only way to store your data, or even the best way. If you’re the type of person who needs to store a lot of data and to access it quickly, a more traditional NAS (network-attached storage) system might suit you better.
NAS is a really straightforward storage solution: it’s just a handful of hard drives that are tied together and connected to your network. Generally, they’re setup for both redundancy and increased storage. So, if you wanted triple redundancy, you could connect six 1TB drives together and still have 2TB of storage available.
The GnuBee Personal Cloud 2 (GB-PC2) is a completely open source NAS system that allows you to access your data from anywhere, replicating the convenience of standard cloud services. The GB-PC2 can accept 3.5” hard drives, unlike its predecessor (the GB-PC1) which uses 2.5” drives. It can even be employed as a web server, because the brain of the unit is its own computer.
There are a few benefits to going with a NAS device like the GnuBee Personal Cloud 2: you can store large amounts of data for just the cost of the hard drives (after purchasing the GnuBee), you can be assured you’re the only one with access to that data, and because it’s on your local network you can get to large files quickly. The GB-PC2 is currently being crowdfunded on Crowd Supply, and can be purchased for as little as $249.
Author, writer, maker, and a former mechanical designer. www.cameroncoward.com @cameron_coward
See all (22)
4 
4 claps
4 
Author, writer, maker, and a former mechanical designer. www.cameroncoward.com @cameron_coward
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/a-quick-look-at-azure-data-studio-7b34783c7b8a?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
Microsoft has been on a solid run of introducing cloud services over the last few years. The tech giant has been transitioning from its iconic position as the quintessential office productivity software…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pcmag-access/inside-the-cutting-edge-data-technology-behind-the-us-open-ec43d99bb5cc?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
IBM took us inside the cloud services and predictive analytics behind the tournament—and showed off the Watson power behind IBM SlamTracker and new Cognitive Highlights.
By Rob Marvin
Data is a huge part of what makes sports so interactive. It has evolved from baseball card stats and marking scorecards to SportsCenter and fantasy sports, and now mobile apps and real-time stat tracking with predictive analytics. Sporting events such as the US Open Tennis Championship have had to evolve along with these trends.
IBM has been the US Open’s technology partner for more than a quarter century, but this is the first time Watson was invited to the party. IBM gave PCMag a behind-the-scenes tour of its data command center at the USTA Billie Jean King National Tennis Center in Flushing Meadows, New York, to see how it’s equipping everyone at the stadium with real-time player and match data, from broadcasters and line judges to statisticians and fans — all while using Watson cognitive services to improve different aspects of the fan experience.
The US Open website and mobile apps for Android and iOS are already running on IBM’s cloud infrastructure, and there are a handful of other Watson application programming interfaces (APIs) from the IBM Bluemix hybrid cloud platform integrated in different ways.
At the 2017 US Open, the company announced a new solution called IBM Watson Media. Featuring a new AI tool called Cognitive Highlights, it analyzes match videos with machine learning and cut the best clips into highlight videos. IBM also rolled out a revamped mobile app featuring significant natural language and conversational improvements to its Cognitive Concierge chatbot, which I tried out on iOS.
Then there’s the Speech-to-Text API that automatically generates subtitles for all the video clips and interviews from the tournament. And the Visual Recognition API runs facial-recognition analysis on every photo taken by USTA photographers, and tags players and celebrities.
IBM packed more Big Data and predictive analytics into the tournament app as well with IBM SlamTracker analytics, which pulls in both real-time and historical player, match, and tournament data to generate responsive data visualizations predicting the outcome of sets, break points, and other pressure situations based on data patterns. This year, SlamTracker is even analyzing player and ball position data.
Real-time data, cloud-computing power, and Watson’s cognitive services are pulsing under the surface throughout the US Open, but here are the coolest examples we found while wandering around behind the scenes.
The newly announced IBM Watson Media solution comes with all sorts of bells and whistles. It performs metadata content searches and makes AI-informed recommendations on video content, along with speech-to-text analysis for closed captioning and a “spotlighting” feature to identify violent or adult content that might require further screening.
At the Open, Cognitive Highlights identify each match’s most important moments by analyzing historical data, crowd sounds and reactions, and players’ facial expressions. Watson then ranks and auto-curates the highlights for the video production team and cuts them into highlight packages.
IBM piloted the tech at this year’s Master’s golf and Wimbledon tennis tournaments, but the US Open marks a public partnership with the US Tennis Association (USTA) and IBM Watson Media’s coming out party as a business product. Each day during the tournament, the USTA will post Watson’s Highlight of the Day on its Facebook page, and fans who mark favorite players on the app will get push notifications with player highlights cut together by Watson Media.
The player and match stats and graphics on these screens are being fed in real time to the broadcasters and commenters announcing each match in the broadcast booth. If John McEnroe throws out an interesting stat about Andy Murray’s past US Open performances over the past 10 years, it’s because this guy sent him the data.
There are plenty of monitors in the command center, but no racks of servers. Everything runs through IBM Cloud, which has a multi-active architecture of seven public and private data center locations to scale up or down based on tournament activity. This also supports Watson for Cyber Security, which helps security analysts monitoring US Open digital platforms for hacks and breaches by mining unstructured data and cybersecurity research for proactive threat detection and endpoint protection.
Workers throughout the stadium, from line judges and stadium personnel to statisticians and media correspondents, can access real-time scores, serve speed radar, in/out ball locations, and a host of other metrics throughout the game using these IBM-powered USTA tablet apps, which aggregate sensor data from around the court.
Watson’s Visual Recognition API processes every official photo taken and uploaded to the USTA’s publishing tool, and scans it for facial recognition to quickly identify the players on the court and scan for any celebrities in the stands before the photo is officially published.
USOpen.org and the mobile apps both offer on-demand videos of match clips, recaps, and pre- and post-match interviews. The Watson Speech-to-Text API eliminates the need for a transcriber by automatically generating captions as soon as a video is uploaded to cut down on the delay before publishing it in the video section. The USTA still needs to edit the transcripts, but Watson is learning more player names and tennis terms as the tournament progresses.
SlamTracker’s predictive analytics is all about situational analysis. The Watson Machine Learning technology generates three “Keys to the Match” for each player based on eight years of Grand Slam tennis data, combined with real-time data on stats like aces and percentage of wins on first serve to predict probabilities in a situation like a fifth set tiebreaker. It even factors in player style models.
This year, the player and ball position data gathered feeds into pace-of-play analysis and proximity measures of how close a ball is to the baseline.
Watson’s Natural Language API is a good, quick concierge on the ground in Flushing Meadows. The chatbot has nixed the beta tag it carried last year, and now processes thousands of messages and responses throughout the course of the tournament. You can use it for everything from finding food or getting quick match data to getting directions to the nearest stadium bathroom.
This interactive demo area outside Arthur Ashe Stadium is more or less a test bed for all the ways IBM is experimenting with Watson, machine learning, the Internet of Things, and Big Data in real-world applications. If you’re spending a day at the Open, the demos are a fun way to kill some time between sessions.
Using IBM Bluemix, the Watson IoT platform, and Node-RED, an open-source tool for connecting hardware devices to online software and services, you can use brainwaves to move a connected BB-8 toy. Put on a headset and summon your mental powers to roll the little guy across the miniature court.
Using an adjusting stationary bike and an Oculus Rift headset, IBM simulates how Watson gave ultracyclist David Haase real-time race insight during a biking marathon. The technology uses IBM Watson Analytics, IBM Predictive Analytics, and Watson IoT services to monitor environmental factors like weather and incline. I gave it a try…the simulation didn’t go easy on the resistance.
Watson tries its hand at composing, creating a rhythm based on US Open sounds — bouncing balls, crowd noises, and squeaking sneakers — input from a drum pad. Then Watson layers some music on top to create an original track. This was the most gimmicky demo of the bunch. My song sounded pretty terrible…but most of that was probably due to my terrible drumming skills.
This system built on Bluemix uses Watson to pull in player info, team stats, and league-wide analytics to draft a team. The interactive series of displays the demo showed off was full of fake team names, but real sports organizations like the Toronto Raptors are actually using the solution.
In a tool called Maker’s Mosaic, Watson analyzed my Twitter profile to identify personality traits like agreeableness, introversion, and openness, and correlate them with a professional tennis player. The personality section was pretty dead-on, but after tabbing over to the values and needs sections and seeing just how deeply Watson probed my tweets to develop a complex and eerily accurate personality profile. I’ll admit, I was a little creeped out.
Pepper is a physical robot used to demo Watson’s natural language processing, conversational abilities, computer vision analytics, and a host of other cognitive services as IBM trains Watson using deep learning techniques. The goal, the company said, is to build a whole middleware layer around embodied cognition. For now, Pepper can hold a pretty decent conversation, use body language to show emotions like surprise and celebration, and give out high-fives.
Read more: “IBM Brings Watson to Mobile Device Management”
Originally published at www.pcmag.com.
PC Magazine: redefining technology news and reviews since…
16 
16 claps
16 
Written by

PC Magazine: redefining technology news and reviews since 1982.
Written by

PC Magazine: redefining technology news and reviews since 1982.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codait/reading-the-open-source-nutrition-labels-5fa78c50a870?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
IBM used to be a closed-source company. Customers would come to us because it was the only place to get software we built behind closed doors in our research labs. The picture today is very different. We still have exclusive technologies such as Watson and Db2, but there are also familiar open-source projects operated as-a-service by IBM.
Each such service is based on an open-source project but may be renamed, to distinguish it from the original. IBM’s implementation may have additional features such as a web-based dashboard, backup tools, or unified authentication.
Each of the open-source projects is built with one or more programming languages, using a framework and external libraries — the tools themselves will be open-source projects in their own right. They run on a Linux distribution, which itself is a soup of open-source projects, libraries, and tools. When a particular open-source tool is deployed, it may even run in containers orchestrated by other open-source utilities.
This is all possible because free, open-source software is both free of charge and free to re-model in other forms (within the limits of the license agreement), but it is certainly not free to produce. Many, many human hours are needed to create, maintain, and improve these products. IBM doesn’t just take from the open-source community, it is an active contributor to the products and tools it uses.
Sometimes, it may not be clear that the IBM product you are using is actually based on an open-source project. So let’s clear this up with list of IBM products and the open-source projects they’re built on.
IBM Cloudant is a JSON document data store built on Apache CouchDB. They are broadly equivalent, but Cloudant adds geospatial support and free-text search which are open-sourced, but don’t ship with vanilla CouchDB.
IBM Message Hub is a scalable, distributed message buffer and is based on Apache Kafka. Message Hub topics can be used to feed large streams of data for analysis or storage.
IBM Blockchain allows private blockchains to be built where all the participants know that the transaction history is secure and tamper-proof. It is built on the Linux Foundation’s Hyperledger project.
IBM Cloud Functions allows tiny microservices to be deployed as scalable “serverless” actions. It is a deployment of the Apache OpenWhisk project which combines nginx, Apache Kafka, Apache CouchDB, and Docker, amongst others.
This one’s easy! Spark is a distributed data-processing framework and it should be no surprise that IBM’s Spark service is based on Apache Spark. It is available in Jupyter notebooks and in conjunction with BigInsights, a proprietary IBM technology.
IBM Bluemix is IBM’s platform-as-a-service and offers bare metal, virtual servers, Cloud Foundry applications, and Docker & Kubernetes containers running against a host other managed services.
The lovely folks at Compose provide a range of data layer services based on open-source technologies:
The Compose offerings are multi-node deployments of the service, not just the single-node instance you get when you download and run some software on your machine. They also add an access control layer on top of your service and add logging/monitoring underneath it. There’s an API, command-line tooling, and a web-based dashboard to control your service, plus other tools such as backup, restore, and version management.
The next time you’re perusing the Bluemix Catalog, remember: Many of these services are made on equipment that also processes open-source software and infrastructure tools, and may contain traces of all of the above.
Things we made with data at IBM’s Center for Open Source…
6 
6 claps
6 
Things we made with data at IBM’s Center for Open Source Data and AI Technologies.
Written by
Developer @ IBM. https://glynnbird.com
Things we made with data at IBM’s Center for Open Source Data and AI Technologies.
"
https://medium.com/realistic-life-management-rlm/really-going-paperless-finally-5e059ebe48b2?source=search_post---------138,"There are currently no responses for this story.
Be the first to respond.
I don’t know about you, but I have finally found a combination of easy ways to eliminate paper records from my life.
First, and most important, is the ease with which paper documents can now be digitized — not with cumbersome scanners but with the…
"
https://medium.com/building-the-open-data-stack/managing-distributed-applications-in-kubernetes-using-cilium-and-istio-with-helm-and-operator-for-9652d71d6432?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
Oct 21, 2021·10 min read
This post will show you the benefits of managing your distributed applications with Kubernetes in cross-cloud, multi-cloud, and hybrid cloud scenarios using Cilium and Istio with Helm and Operator for deployment.
In our recent post on The New Stack, we showed you how you can leverage Kubernetes (K8s) and Apache CassandraTM to manage distributed applications at scale, with thousands of nodes across both on-premises and in the cloud. In that example, we used K8ssandra and Google Cloud Platform (GCP) to illustrate some of the challenges you might expect to encounter as you grow into a multi-cloud environment, upgrade to another K8s version, or begin working with different distributions and complimentary tooling. In this post, we’ll explore a few alternative approaches to using K8s to help you more easily manage distributed applications.
Cloud Native Computing Foundation (CNCF) provides many different options for managing your distributed applications. And, there are many open-source projects out there, that has come a long way in helping to alleviate some of the pain points for developers working in the cross-cloud, multi-cloud, and hybrid cloud scenarios.
In this post, we’ll focus on two additional approaches that we think are very good:
In our first post on the topic of how to leverage K8s and Cassandra to manage distributed applications at scale, we discussed the use of DNS stubs to handle routing between our Cassandra data centers. However, another approach is to run a mix of global Istio services and Cilium global services side by side.
Cilium provides a single zone of connectivity (a control plane) that facilitates the management and orchestration of applications across the cloud environment. Istio is an open-source, language-independent service networking layer (a service mesh) that supports communication and data sharing between different microservices within a cloud environment.
Cilium’s global services are reachable from all Istio managed services as they can be discovered via DNS just like regular services. The pod IP routing is the foundation of the multi-cluster ability. It allows pods across clusters to reach each other via their pod IPs. Cilium can operate in several modes to perform pod IP routing. All of them are capable of performing multi-cluster pod IP routing.
You may already be using one of these tools. If you are, you can add one on top of the other to extend their benefits. For example, if you already have Istio deployed, you can add Cilium on top of it. Pod IP routing is the foundation of multi-cluster capabilities, and both of these tools provide that functionality today. The goal here is to streamline pod-to-pod connectivity and ensure that they’re able to perform multi-cluster IP routing.
We can do this with overlay networks, in which we can tunnel all of this through encapsulation. With overlay networks, you can build out a separate IP address space for your application, which in our example here is a Cassandra database. Then you would run that on top of the existing Kube network leveraging proxies, sidecars, and gateways. We won’t go too far into that in this post, but we have some great content on how to connect stateful workloads across K8s clusters that will show you at a high level how to do that.
Tunneling mode in Cilium encapsulates all network packets emitted by pods in a so-called encapsulation header. The encapsulation header can consist of a VXLAN or Geneve frame. This encapsulation frame is then transmitted via a standard User Datagram Protocol (UDP) packet header. The concept is similar to a VPN tunnel.
The takeaway message here is really that there are a lot of options that exist in the container networking interface (CNI) space and with service mesh and discovery that can help to eliminate most if not all of the heavy lifting around DNS service discovery and ensuring end-to-end connectivity, you need to effectively manage your distributed applications.
These products not only provide all of that functionality bundled up into a single solution (or maybe a couple of solutions), but they also offer some pretty big additional benefits over simply using DNS stubs. With DNS stubs, you still have to manually configure your DNS and IP routing, map it all out and document it, and then automate and orchestrate it all. Whereas, these products offer observability, ease of management, and most importantly, a Zero Trust architecture, which would be nearly impossible to achieve with a DNS-only based solution.
Cilium has done a great job creating a plug-in architecture that runs on top of eBPF. This provides application-level visibility that allows you to start creating policies that go beyond what you may have seen or leveraged before. For example, say you want to create a firewall rule to ensure that your application can only talk to a specific Cassandra server. You can actually now take that down a few notches to create a rule that allows read-only access or restricts access to specific records or tables. That’s just not something that’s possible with the existing tooling we’ve used in the past, whether that’s VPNs and Firewalls.
The other thing is that all of this has created a lot of complexity and “Kubeception” around layers upon layers of overlay networks. So, it can be challenging to ensure you have visibility and to properly instrument everything, especially if you’re managing DNS on your own. You’ll also have to start collecting logs, gathering metrics, creating dashboards, and doing other things that together add a lot of additional overhead.
However, if you look at projects like Cilium Hubble and Istio Galley, you can see that you not only get all the instrumentation to manage this stuff out of the box, but you also get observability into the health of your pods and fine-grained visibility that you won’t get with traditional tools.
This observability is a huge advantage because it allows you to also instrument on the monitoring side to build out powerful metrics reporting with tools that can tightly integrate with Prometheus. Once you do this, you can get metric data on the connectivity between all of your pods and applications and determine where there may be latency as well as what policy is potentially being impacted.
Of course, the ability to instrument all this isn’t new. We’ve probably all been there and done that, collecting logs to some central log aggregator, building custom searches, etc. But with these services, we can now get this out of the box.
So how do we get from all the great things we’ve talked about in these slides to actually deploying your applications into a cloud, multi-cloud or hybrid cloud environment?
Since you’re no longer working in a single region or cluster anymore, there’s going to be a bit of juggling involved. You might be pushing manifest and resources to each cluster one by one. Or maybe you’re templating things out and using tools like Helm or perhaps some GitOps or other pipeline tools to make sure that you are staging appropriately and you’re working through different environments. But really, there’s still a lot more that is required when you’re working on multi-cluster deployments.
So one example here is Helm. If you’re using Helm, you’re going to have a release per cluster, which means you’re going to have to maintain and manage to switch between those various contacts and make sure you’re upgrading the right way. And in case things go sideways, you’ll also need to know how to stage a change or roll back a change before you switch over and do operations in the other cluster or the other region. And when you go beyond two regions, there’s even a bit more complexity.
Now I’d like to call out the Operator Framework here, and more specifically the Operator SDK and the individual operators that make up a number of the things we’ve covered here.
Some of these tools are really starting to level up with multi-cluster functionality where in some cases you’re running instances of their operator inside of each of the clusters, and they communicate and lock and perform when they go to perform various actions. In other cases, you might have a control plane where you’re running the operator and it’s reconciling resources in the downstream clusters.
Maybe we have an Ops K8s cluster, or maybe just us-west4 is running the operator, but it’s communicating with the Kube API and us-east1. We’re currently doing that in the K8ssandra project where we’re going from Helm charts to an operator that has Kube configs and serves the confidentials to talk to remote API servers and to reconcile resources across those boundaries. We do this because some operations need to happen serially.
Maybe if a node is down in one data center and we don’t want to do a certain operation in another data center, having operators that can communicate across those cluster boundaries can be really advantageous, especially when you’re talking about orchestration.
The conversation we started on The New Stack blog and have continued here has focused a lot on manually managing things versus having cloud-native technologies that can manage them for us, whether that be service discovery or routing tables, or even just adjusting the packet in flight to indicate what cluster they need to go to and eventually, what pod they need to reach.
When you think through the application of these technologies and how you might best use them to manage your distributed applications, the single most important takeaway we’d like to leave you with is…
You need to plan your deployments before you start spinning up your K8s clusters.
Having the right people together to hash out your approach before you wade in will help you identify any limits in your system and other important factors that need to be considered. For example, maybe you have a scarcity of IP addresses. Maybe you’re running one big cluster, and now you’re talking about many small clusters. Or maybe you run clusters more along business lines or for certain Ops teams.
How are you going to start to venture into this multi-cluster multi-region space and ultimately, how are you going to build the plumbing and the pipes between those systems so they can communicate with each other?
Theoretically, a single team could do this planning. But, that’s probably not going to turn out well. It’s far more likely that you’ll need to involve several teams, including people from operations and people that run the cloud accounts. If you’re operating in a hybrid or multi-cloud environment, you’ll probably also have some network people involved, too. For example, there may be some firewalls that need to be adjusted in certain ways.
Planning your approach upfront is enormously beneficial and will help you avoid some pretty big problems when you move into implementation. For example, it can be very difficult to make changes once you’ve launched your cluster because you can’t just change the Classless Inter-Domain Routing (CIDR) (the IP address space) your pods are running in at that point. You would instead need to migrate them. By doing some of this planning upfront, you can avoid this and a lot of other unfortunate situations.
Follow the DataStax Tech Blog for more developer stories. Check out our YouTube channel for tutorials and here for DataStax Developers on Twitter for the latest news about our developer community.
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
4 
4 
4 
We’re huge believers in modern, cloud native technologies like Kubernetes; we are making Cassandra ready for millions of developers through simple APIs; and we are committed to delivering the industry’s first and only open, multi-cloud serverless database: DataStax Astra DB.
"
https://blog.jeremylikness.com/managing-data-in-the-cloud-82c5fa0be9d1?source=search_post---------140,
https://medium.com/codex/windows-11-is-a-reflection-of-the-new-microsoft-6ee97f2d5dde?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
There are times when a product is very clearly a reflection of the company that makes it. When OnePlus released the Nord last year, the device felt like a testament to the companies original philosophy of delivering a high-quality product at an aggressive price point that makes you question why similar products are so expensive. It was a product that felt like a reflection of Carl Pei, the co-founder of the…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@marysam/5-dos-and-don-t-s-of-your-media-and-bloggers-outreach-list-88866fe14d2a?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maria Podolyak
Oct 11, 2017·3 min read
For past 10 years my experience has been in the area of cloud services or SaaS, mobile and desktop apps. Couple of months ago, I started working on my first Kickstarter campaign for a gadget. So now most of people I communicate with are super smart in their own “offline” fields, but their digital marketing techniques are outdated.
My epiphany is that people in this community approach media and bloggers lists in a way that could be more efficient. I came up with five things you should or should not do, if you want to improve your media and bloggers outreach.
No #1 You don’t need Alexa rating while doing the press and blogs list
Probably, Alexa rating was a good source of reference at beginning of 2000’s. But today marketers have more date to make decisions. For example, you may use SimilarWeb Chrome Plugin.
SimilarWeb has all the basic data you need to make the decision about the site: 1) traffic (is it high or is it low?), how many people visit it on the monthly basis; 2) is the traffic coming from the country you are interested in? 3) what are the site’s traffic sources? Does the site look like a quality destination?
No #2 You shouldn’t download a “ready-to-go” media list or use Submit.co as is
What you have to do is a week doing research, which journalists (not general media) and bloggers may be interested in your product, since that’s the stuff they write about everyday.
Sure, you can use other project’s lists or sites such as Submit.co for media and blogs reference and brainstorming, but that’s pretty much it. There are more than 2000 media outlets and blogs with traffic over 100K unique users a month in the US. You can target better.
There are more than 2000 media outlets and blogs with traffic over 100K unique users a month in the US.
No #3 Your outreach list should contain more than 20 journalists and bloggers
Less than 10% will respond. 10% of 20 is 2. Do the math. Yes, you need more quality leads.
No #4 The spreadsheet of your media outreach should have at least 3 different tabs
By tabs I mean different topic groups of journalists and bloggers or different journalists’ profiles. You can’t target all of them with a single proposition or pitch. A mom or pop blogger needs a different pitch than an educational media journalist.
Find groups of journalists and bloggers that can be interested in your product, divide them and treat them differently. Create different email pitches for each group. For example, you’re selling a wooden toy that looks good in the log cabin setting. Your target groups are: 1) eco living bloggers 2) toy bloggers and journalists 3) interior design bloggers and journalists 4) country living design bloggers, etc.
No #5 You send your emails, relax and wait. Not a good idea
You have to do follow ups. People are people, they can be on vacation, be in a bad mood, sick and tired. As a result your single email is already buried by those who came after you. Bump it up. Provide additional information in the next follow up email, what happened to the product since you sent the first one? And, of course, choose your timing wisely — don’t send an update Friday afternoon.
And by all sakes, guys, don’t spam them. Treat journalists and bloggers as your clients. Giving them what they want and when they want it is the key to quality marketing.
Marketing Consultant for Startups based in North Carolina
2 
2 
2 
Marketing Consultant for Startups based in North Carolina
"
https://medium.com/software-daily/the-end-of-infrastructure-744ad9ec5c40?source=search_post---------143,"There are currently no responses for this story.
Be the first to respond.
Cloud services providers have historically been built with an infrastructure component and a platform component.
Infrastructure-platform companies will continue to grow, but newer cloud providers build abstractions for developers that do not expose infrastructure.
The simplicity and focus of these newer cloud providers give them a competitive advantage against infrastructure-platform companies, while the infrastructure-platform companies have richer, more complex offerings.
The future is bright for every cloud provider who offers unique services rather than fighting at the margins for competitive advantage in an existing market.
Infrastructure-platform companies like Amazon, Google, and Microsoft represent an era where most developers need to interact heavily with individual machines.
Digital Ocean co-founder Moisey Uretsky portrays the future of cloud providers relative to the past:
Firstly, clouds are built backwards, not forwards. This is the main mistake that just about every company that tries to compete in the cloud makes. Amazon built AWS internally and then decided to resell it to developers. That gives them several advantages. First, they are able to capitalize on the scale of their operations from the primary core business which means they can start with an extremely large amount of capacity from day one. Because this service is needed internally to run their core business, even if the cloud fails publicly, they will still continue to invest in it internally so it minimizes their risks dramatically…
Clouds with a public infrastructure component have historically been “built backwards, not forwards”, if we are talking about Amazon, Google and Microsoft. These big three are the current icons of the Infrastructure Era: when cloud computing means offering access to both a platform as a service and the infrastructure beneath it.
Developers are suspicious of Amazon, Google, and Azure because the platform-as-a-service can lock them in.
Digital Ocean capitalizes on the anxiety of the Infrastructure Era by offering the simplest version of infrastructure with the lowest switching cost.
Digital Ocean decouples its brand from the platforms built on top of it by partnering with companies like GitLab and Mesosphere — Digital Ocean gets to evolve its PaaS offerings without brand risk or the risk of technical debt from building its own APIs.
For the past two years, Dropbox has been ridiculed by critics for its lack of new features, other than acquisitions.
Dropbox’s core product development looked stagnant because the core engineering team was focused on something that was not user-facing. Dropbox was undergoing a massive project — quietly migrating its cloud from AWS to Dropbox’s own data centers:
The whole process took two years. A project like this, needless to say, is a technical challenge. But it’s also a logistical challenge. Moving that much data across the Internet is one thing. Moving that many machines into data centers is another. And they had to do both, as Dropbox continued to serve hundreds of millions of people. “It’s like a moving car,” says Dan Williams, a former Facebook network engineer who oversaw much of the physical expansion, “and you want to be able to change a tire while still driving.”
Dropbox just announced its first big consumer feature since building new infrastructure. Project Infinite might have been cost prohibitive if Dropbox had not built its own data centers.
Now that Dropbox is done changing the tires on the moving car, there will likely be a long line of products that will have Dropbox critics going through blog archives and editing their bleak premonitions.
Dropbox exemplifies why the narrow lens of commodity is the wrong way to look at the cloud.
Zero-sum analysts have been framing Dropbox as a narrowly defined competitor to Box. Box encourages this, because Box imagines a world in which work and life are cleanly partitioned — Box is for your work stuff, and nothing else.
The lines between work and life are increasingly blurred for knowledge workers. By solving for the consumer, Dropbox future-proofs for the enterprise — and, inevitably, the developers.
In the coming years, Dropbox’s cloud will eat into market share of AWS, GCP, and Azure. The most obvious reason is user experience. Look at all the icons on the AWS dashboard:
Azure and GCP also have feature-heavy UIs.
Critics have mocked the spare interface of Dropbox, but it is much easier to add icons than it is to remove them. Developers will choose Dropbox for the same reason consumers use it: simplicity.
Digital Ocean’s Moisey extends his history-focused perspective to Microsoft:
Looking at each [modern cloud provider] individually we see that IBM’s play to acquire softlayer was mainly to preserve their revenue base off of their existing customers. Microsoft, very much ended up in the same scenario as well. Given the large amount of revenue both companies have in the enterprise segment, these offerings will generate revenue, however, their growth will be slow, because they are simply canabalizing their own revenue and preventing it moving to AWS.
Microsoft is actually expanding the market of enterprise knowledge workers, not cannibalizing it.
As Amazon decays into Oracle and Google blossoms into AWS, Azure is doing something unique. Microsoft wants to turn business analysts into developers.
Solving this problem doesn’t require any fundamental technological breakthroughs, just tenacity and iteration — which money can buy. Since Microsoft has plenty of money, there is very low execution risk to Azure’s strategy.
Enterprises who are locked into Microsoft will have their cadence of migration to the cloud dictated to them by Microsoft.
A well-educated young investment banker recently said to me “the Microsoft Office suite of technologies is just unparalleled”. This statement is true to the extent that every company he has worked for uses Microsoft end-to-end.
An average software engineer is steeped in open-source, and the Azure stack seems confusing. But an average enterprise CIO is not steeped in open-source, and does not speak in engineering terms.
“Fast data”, “data lake”, “data-at-rest”, and “data-in-motion” seem like buzzwords to engineers, but they are the language which the enterprise CIO understands.
Similarly, Event Hubs and Stream Analytics may not excite a software engineer because these technologies aren’t built to empower software engineers — they are built for the business analyst.
Clearly a business analyst has no desire to write code — but even among developers, there are signs of growing infrastructure fatigue: containers, serverless architecture, and No-Ops suggest a desire to deal with smaller modules, less burdensome configuration, and simpler APIs.
Infrastructure-as-a-service is today’s assembly code. With Azure’s strategy of software development without code, Microsoft might end up building something uniquely beloved by developers.
As AWS, GCP, and Azure compete at the infrastructure-platform level, newer companies are building simpler abstractions with better interoperability.
Stripe and Uber will eventually have developer platforms with the feature cardinality of today’s AWS. Today, Stripe and Uber are built on AWS, but they can always backfill their own infrastructure like Dropbox if they need to compete for commodity pricing.
Amazon, Google, and Azure look similar right now, but over time they will be as differentiated as Stripe, Dropbox, and Uber.
AWS will cater to legacy customers and developers who don’t want to learn a new cloud stack. Google’s cloud will be a better, cheaper AWS — the Gmail to Amazon’s Hotmail. Azure will be the cloud for people with a suit and tie.
http://softwareengineeringdaily.com
3 
3 claps
3 
http://softwareengineeringdaily.com
Written by
Software Engineering Daily host www.softwareengineeringdaily.com
http://softwareengineeringdaily.com
"
https://medium.com/codex/nextcloud-and-kubernetes-in-the-cloud-with-kuma-service-mesh-ebebf66e0de5?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
I recently decided I wanted to start cutting third-party cloud services out of my life. I purchased a shiny Raspberry Pi 400 (which reminded me of the Amiga computers of my youth) and decided to try Nextcloud on it as a personal cloud. It was a far quicker process than I expected thanks to the awesome NextCloudPi project. Within twenty minutes, I had a running Nextcloud instance…
"
https://medium.com/dadi/microsofts-sinking-technology-e69159b65fa7?source=search_post---------145,"There are currently no responses for this story.
Be the first to respond.
This morning Microsoft announced Project Natick — an attempt to make data centres greener by submerging them under water.
It’s a costly experiment that has sunk a white cylinder packed with servers just off the coast of Orkney with the hope water will satisfy the need to keep the machines cool without the expensive systems used in conventional warehouse farms.
We have just one question: why? If you are looking for efficiency for cost and environment, why drown expensive dedicated kit in the ocean when there’s a wealth of unused data storage in homes and offices across the globe?
This is a key benefit (and much of the motivation) behind DADI technology — built to harness spare computing power found in domestic devices such as laptops. In addition to the performance and cost benefits of a distributed network built on blockchain, DADI leverages devices already manufactured and in use — saving not only damage to the environment for building massive server farms, but additionally, it seems, the cost of hiding them in the sea.
We also imagine a small node in your living room will be easier to service than one bolted inside a sealed metal tube and surrounded by marine life…
For more information on the DADI network, head this way.
Faster. Greener. More Secure. The future of the cloud is Edge.
12 
12 claps
12 
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/core-components-of-azure-iot-ddee157c9f71?source=search_post---------146,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 31, 2019·4 min read
Azure Internet of Things (also known as Azure IoT) is a collection of cloud services managed by Microsoft that monitor, connect and control billions of IoT assets. Basically, this is a solution that operates in the cloud and is made up of 1 or more IoT devices and 1 or more back-end services that communicate with one another. Organizations across all industries use Azure IoT to help them improve their business and achieve their IoT goals.
There are three main parts that make up an IoT solution — devices, back-end services, and communication between the two. In this blog, we’ll dig in a little more into these components, different IoT services, and possible challenges.
IoT devices are pretty much anything that has a sensor attached to it and can transmit data from one object to another or to people with the help of the internet. Typically, they are attached to a particular object that operates through the internet, enabling the transfer of data among objects or people automatically without human intervention. It’s also important to note that many of these devices can communicate through a Wi-Fi chip as well. Some examples of IoT devices that work with Azure IoT may include:
With Azure IoT Hub you can connect, manage and scale your IoT devices to communicate securely with back-end services in both directions. Here are some examples of how this communication works:
Here are some of the functions a back-end service can provide:
Microsoft offers eight IoT services in Azure. With so many different options it can be confusing to figure out which one best fits your needs. Depending on how much help and control you want in building your own solution will affect which service is the best one for you. Here are the available services and what they can be used for:
All IoT devices have different characteristics when compared to other clients, such as apps and browsers. Azure IoT devices, tools and data analytics may help you manage these to achieve your IoT goals. But, adopting IoT technologies can present its own set of challenges. While reducing IoT application costs and easing development efforts are important things to consider when implementing an IoT solution, connecting devices securely and reliably is often the biggest challenge most organizations encounter when using IoT services.
Originally published at www.parkmycloud.com on December 19, 2019.
CEO of ParkMyCloud
5 
5 
5 
CEO of ParkMyCloud
"
https://medium.com/cudos/%EF%B8%8F-cloud-computing-market-to-grow-5x-by-2030-b7da507fda02?source=search_post---------147,"There are currently no responses for this story.
Be the first to respond.
The cloud services market could grow 5 times by 2030. Technological developments and direct investments are fuelling this growth, while COVID-19 and the associated lockdowns and work from home policies also contribute. Ultimately, the market must be competitive, efficient and decentralised for all stakeholders to benefit.
According to a recent report, the global cloud services market could reach $1,620,597 million by the end of the decade. Based on a valuation of $325,689 million in 2019, this translates to compounded annual growth of nearly 16%.
The shift of enterprises toward cloud-based services and advancing technologies such as artificial intelligence (AI) and machine learning (MI) are the primary drivers behind this growth. On the funding side, significant investments by counties such as China, India, the U.S., and the U.K. in cloud-based projects could boost growth further, according to a recent study.
Another critical market expansion catalyst is COVID-19. First, consumers have been spending more time at home due to lockdowns. As a result, cloud-based services such as video-on-demand (VoD) and over-the-top (OTT) media have become more popular.
Second, many public and private sector organisations have implemented ‘work from home’ (WFH). Consequently, the need for software as a service (SaaS) based communication and collaboration tools has also increased. Jointly, these two factors have accelerated market growth.
Nonetheless, it is vital that the market is fair and competitive for consumers to benefit as well. Furthermore, this growth must happen in a sustainable way that honours the environment and utilises existing computing resources to minimise waste. Finally, a decentralised approach would allow individuals with spare computing power to earn by participating in the market while delivering a cheaper service for consumers.
The Cudos Network is a layer 1 blockchain and layer 2 computation and oracle network designed to ensure decentralised, permissionless access to high-performance computing at scale and enable scaling of computing resources to 100,000’s of nodes. Once bridged onto Ethereum, Algorand, Polkadot, and Cosmos, Cudos will enable scalable compute and Layer 2 Oracles on all of the bridged blockchains.
Next Generation Cloud
51 
51 claps
51 
Next Generation Cloud
Written by
Next Generation Cloud
Next Generation Cloud
"
https://medium.com/tokyo-fintech/electronic-signatures-in-japan-ee8a2cca1c97?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
The Electronic Signatures and Certification Business Act in Japan came into effect in the year 2000, way before cloud services took hold. This would not necessarily be an issue if the Act had been formulated in a technology-neutral way, however, it includes the requirement of “property” (assuming usage of an IC card and card reader at that time), which makes it outdated and overtaken by technological progress, and in fact would would exclude objectively better (purely digital) solutions. With the government due to present their “Japan Growth Strategy”, which will include elements of regulatory reform, voices have grown louder that lobby for amendments to the Act as part of the package. Since the definition of electronic signatures in the Act is applied to over 170 laws and regulations, the restriction that this wording imposes on the spread of electronic signatures cannot be overlooked.
It is also noteworthy that the expert council put in place before the 2000 law was promulgated, the “Study Group on the Legal System of Electronic Commerce”, also included outright cultural, business-process-oriented aspects in their consideration, intentionally creating a “pause” in the decision making process to consider the prudence of finalizing the document.
There are some problems in enabling substitution of a document with a handwritten signature by an electronic document with an electronic signature, while such substitution is useful from the standpoint of parties. For example, in certain cases a handwritten signature or a seal is required not only for the purpose of clarifying the author of the document but also for the purpose of ensuring prudent decisions by requiring a certain form, so when such substitution is permitted in such cases, the aim of law, ensuring prudent decision, may be spoiled. In enabling such substitution, the aim of each provision should be investigated.
The general legal principle that contracts are valid if parties reach an agreement, whether they agree verbally, electronically, or in a physical document is not expressly stipulated under Japanese legal codes. The Japan E-signature Law recognizes electronic signatures as a method of entering into agreements, including conditions for the presumption of legal authenticity. In case of legal disputes, parties may have to present evidence in court. Cloud-based digital workflow/document management solutions would be one way to prove the existence, authenticity and valid acceptance of a contract.
Some legal processes explicitly require handwritten signatures or wet ink seals (hanko), typically these are formal notarizations as described in the Civil Code, the Act on Land and Building Leases, and the Act on Voluntary Guardianship Contract.
While the use of personal and corporate seals is a major obstacle in migrating towards digital business practices and facilitating “テレワーク” (telework, as working-from-home is called in Japan), it is by no means the only one. In a recent self-assessment, the Cabinet Office published the requirements for applying to the various subsidy and support schemes accessible during the pandemic. They also appropriately focused on the availability of online applications, and the acceptance of electronic document submissions — all these factors need to be addressed together, otherwise a remote user experience cannot be achieved.
If you found value in this article, please “clap” (up to 50 times).
This article is part of our Tokyo FinTech Publication, please follow us to read more from our writers, like hundreds of readers do every day.
Should you live in Tokyo, or just pass through, please also join our Tokyo FinTech Meetup. In any case, our LinkedIn page, Facebook page and our Instagram account are there for you as well.
一般社団法人 (General Incorporated Association) Tokyo FinTech is…
51 
51 claps
51 
一般社団法人 (General Incorporated Association) Tokyo FinTech is registered as a non-profit organization in Japan, promoting the domestic ecosystem through innovation
Written by
Passionate about strategy & innovation across Asia. At home in Japan. Connector of people & ideas.
一般社団法人 (General Incorporated Association) Tokyo FinTech is registered as a non-profit organization in Japan, promoting the domestic ecosystem through innovation
"
https://medium.com/syncedreview/ai-biweekly-10-bits-from-july-w4-august-w1-b45e33e48eff?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
July 23rd — Google Plans to Bring Blockchain to Cloud ServicesGoogle teams up with New York distributed ledger solutions startup Digital Asset. In a Google blog post the company says it is exploring new ways to use distributed ledger technology. Digital Asset CEO Blythe Masters said “We’re partnering with Google Cloud to provide developers with a full stack solution so they can unleash the potential for web-paced innovation in blockchain.”
July 24th — Microsoft Teams up with Walmart against AmazonWalmart is pushing its digital transformation of retail. The company announced a partnership with Microsoft, which will provide a full range of cloud solutions including Microsoft Azure and Microsoft 365. The five-year agreement is seen as a challenge to Amazon’s lead in online shopping.
July 26th — Microsoft Plays Big in HealthcareMicrosoft officially launches its healthcare unit, which will use its cloud capabilities and data analytics to build products. The company has expanded its healthcare operations into a multi-billion-dollar business that now employs 1,100 people.
July 26th — VC Investment in Canada Hits US$900 Million The AI sector is still a hot destination for Canadian venture capital according to the PwC Money Tree report for Q2 2018. Funding rose 104 percent compared to Q1, while the total number of deals and investments also reached an all-time high.
July 30th — Uber Suspends Autonomous Trucks DevelopmentUber says it will suspend its autonomous truck project and focus solely on autonomous cars. In 2016 Uber acquired Otto, an autonomous truck startup founded by former Google engineers. Last year Google expanded the truck project from Texas and created the Uber Freight app to connect drivers and shippers.
July 31st — Skyline AI Raises US$18 Million in Series ASkyline AI is a real estate investment firm that utilizes artificial intelligence and data science to reinvent commercial real estate investment. Their proprietary AI and machine learning platform incorporate both supervised and unsupervised learning models into the entire commercial real estate acquisition and ownership process.
August 1st — Tesla Building Its Own AI ChipsTesla CEO Elon Musk said Tesla plans to begin building specialized AI chips itself. It’s hoped the Hardware 3 project will help the Tesla team better understand the algorithms behind autonomous driving and boost system and vehicle performance.
August 1st — LG Establishes AI Research LabLG launches a new LG Electronics AI Research Lab in Canada, an extension of the LG Silicon Valley AI Lab in Santa Clara, California. The Toronto AI Lab is a part of LG’s North American strategy to actively shape the future of AI through collaborations with academia and startups.
August 3rd — Enel Green Power Works with Raptor Maps on O&M DronesThe US subsidiary of Enel Green Power is working with startup Raptor Maps to use drones to inspect PV solar power plants. The partnership will use AI and machine learning algorithms to improve the O&M process and aims to reduce problem detection time from days to hours.
Aug 3rd — HighRadius Simplifies Accounts Receivable ProcessHighRadius introduces Integrated Receivables, a credit-to-cash platform that simplifies the time-consuming accounts receivable process with solutions for credit, collections, deductions, cash applications, electronic billing, and payment processing. HighRadius customers report more than 90 percent of invoices are auto-cleared and auto-reconciled without human intervention.
Author: Synced Industry Analyst Team | Editor: Michael Sarazen
Follow us on Twitter @Synced_Global for more AI updates!
Subscribe to Synced Global AI Weekly to get insightful tech news, reviews and analysis! Click here !
We produce professional, authoritative, and…
6 
6 claps
6 
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
"
https://medium.com/@alibaba-cloud/fault-tolerance-with-application-high-availability-or-batch-compute-32030cd03e7?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 9, 2019·9 min read
By Afzaal Ahmad Zeeshan, Alibaba Cloud Community Blog author and Alibaba Cloud MVP.
Undoubtedly, the dynamism and strength of the cloud and its services is the most prevailing topic of this entire decade. However, there comes huge challenges with this ease of dynamic cloud environments. Cloud is prone to faults and errors; different services behaves differently and can generate a series of unexpected errors. Here comes the system ability to handle these errors gracefully, moreover, the actual challenge is to make this handling effective in the long run.
For the continuous working of different components and their expected results, the appropriate implementation of fault tolerance is of paramount importance as responsiveness, scalability, and resilience all the core features of cloud computing demand efficient handling and insightful planning to foresee all the possible pitfalls a system (either hardware or software) can generate at any point in time.
In this article we will talk about two opposite areas where we are supposed to deal with fault tolerance differently — applications requiring high availability due to all the time user interaction and batch computing which is operational-specific domain. Comparatively it is more challenging to make applications fault tolerant which are user or consumer-based, they have interactions and demand to be available every minute, because growing business cannot afford any down time and requires high throughput with highly competitive response time too for their mission critical applications.
Whereas, in the case of batch computing, which is used mostly in data analysis or report generation, are most commonly related to background jobs scheduling and internal operations, with no front tier end user interactions. Thus, this is less critical area in terms of enduring, predicting and detecting faults and unexpected behavior. The reason for this is that customers are not waiting for a response, as quick as possible. However, highly capable systems and planning is required because in batch compute we work with very huge datasets of Tera and Peta bytes of data, its analysis and modeling, thus failure in this analysis can cause loss of useful insights.
If you want to explore the options that are available on Alibaba Cloud for you that can help you provide a better performance for the end users, as well as to maintain how the data is processed in batches for your warehousing or analysis content, this article is for you.
Therefore, we get a wide range of recommended solutions to deal with any sort of anomalies which your system can face based on the nature or service which you are using; either it is consumer-oriented application, hardware machines, such as servers and network components or any other standalone external service.
As far as existing fault tolerance techniques are concerned, there are some major parameters to consider; such as, scalability, reliability, response time, security and performance. For Scalability fault tolerance means, the increasing nodes, resources and load should not affect the resilience of your overall application. For reliability fault tolerance ensures that, the application should always generate expected and accurate results even in the cases of high pressure and peak hours. Throughput and response-time define high output, tasks executions which are completed. And response time is the time which your system or algorithm takes to respond to any query.
Hence, we see, cloud computing alludes to a super-dynamic behavior that in turn can yield many unexpected faults and failures. Therefore, for the robust performance and expected functioning of the system in any of the beforementioned areas, these unexpected events should be handled adequately and effectively.
To ensure application high availability we get multiple approaches; the most common one is the reactive technique which means that we solve problem once it has occurred. These measurements include service or node restart, replication and switching from one machine to another in the case of crashing or downtime. The another widely followed technique is the proactive one, this include proper check and balance, preemptive migration, detection of the faulty components and their replacement with the working ones. Also, we get failure detectors which keeps evaluating the performance and reproducing reports, so based on the results team take further actions suspecting the current scenario.
However, some or in fact most of the times these methodologies require highly skilled team, extra resources and cost. And if these measures were not taken as required things can go overly unexpected resulting huge loss. Here comes the Alibaba Cloud Application High Availability Service (AHAS). This service is based on a widely adaptive cloud model — Software as a service (SaaS). This ensures your application high availability by implanting effective fault tolerance strategies, such as, architecture discovery, controlling and facilitating resources for high traffic and load distribution. It enables your application to get recovered quickly in case of any breakdown in a cost-efficient way. Since this is a SaaS based model, you only pay for the service and features you use, and everything is managed automatically by Alibaba Cloud.
AHAS — Alibaba Cloud Application High Availability Service is based on some function modules that provide stable and highly competitive features to make the application actively fault tolerant. These include but are not limited to controlling the traffic to load balance, or to control and forward the packets of data to a specific web app instance to provide the service.
The topology function module of AHAS ensures the automatic architectural level detection. Mostly, users work on different environments including development, testing and production, AHAS cater a mechanism using which it can automatically detect the application topology to implement recovery and tolerance solutions in a respective fashion. Also, it displays all the required and in used dependencies in visualized manner to monitor their performance continuously. The topology basically is generated as a graph in the portal, which shows how the users are using the services and which services forward the users to which service, up ahead.
Other than this, AHAS uses highly competitive artificial intelligent modules and models to detect all the other dependent and third-party services or resources, this helps AHAS to control the traffic flow and to maintain load balancing as well as high-availability since it knows when a node has gone down.
AHAS ensures high availability capabilities assessment using its comprehensive assessment function module. For the assessment, the architectural information which was gathered from the topology analysis is used, using this information it suggests the test scenarios to evaluate the suspicious risks and any failure point in the component. This can be done either via polling, or by sending a response to the AHAS service at specific intervals.
Based on the information it gets from the topology function module, it maps this information with the recommended settings of every component. Then it suggests and run different test scenarios and then generate their report.
To ensure the high availability and protection of user experience, AHAS provides different ways to mitigate the risks of failing any resource. It uses AppGuard to provide an expected experience to every user by restricting the traffic flow on down or faulty nodes and redirecting it to the highly available nodes.
Furthermore, handling of traffic control and degradation is of paramount importance, they simply are the main features of any application. AppGuard ensures the high availability of these two componenets; it controls the inbound traffic and degrade the failed dependencies to secure the functioning of other depending dependencies. AppGaurd supports Java framework and manages multiple granular level information such as QPS, number of threads, throughput and response time etc. This service can be easily checked on the documentation, as it is a bit broader topic to cover here.
All kind of traditional and modern applications such as monolith or distributed as well as the microservice applications can easily use AHAS to enable the high availability and fault ensuring the fault tolerance in your application. Since this is a SaaS option, it only requires a couple of clicks on the portal, and your service is up and ready to maintain the high-availability of your service.
Now that we have discussed how to provide a smoother experience to your end users, now let us study how to improve and maintain the services that your data warehouses are using. Batch compute makes heavy processing possible by launching different processes in parallel pipelines. There are huge datasets containing tera and peta bytes of model data such as for predictive analytics and analytical computing for the data warehouses which are not aimed to target real-time processing. Due to the system criticalities and load, the possibility of failures is huge, but their impact of failure is comparatively small and controllable. Because, the Alibaba Batch Compute Service is majorly used for the internal operations and heavy job scheduling. Since batch computing is not the consumer or end-user specific service operational teams are mainly concerned about their handling and configuration. Apart from just the configurations, with batch jobs the only consideration is the valid outputs. Operations teams usually understand that a batch service must take some time, thus they can either scale the resources up or they can wait, but what does not change is the fact that operations teams require a valid report being generated. And if your data gets lost mid-way, or a job fails, voiding your data, then you do not have any other option than to restart from the beginning.
Clusters with hundreds of machines are used for practical and scalable deployment of streaming frameworks at companies specifically at Alibaba for managing and maintaining their ecommerce solutions, or even other organizations like Google and Yahoo. However, as the number of machines in a cluster grow, it increases the likelihood of failure of a single machine inside a cluster at any given moment. Failure of machines includes node failures, which are mostly caused by the memory overhead of the data, network failures, software bugs, including provisioned software or proprietary software and resource limitations. Moreover, streaming applications run for indefinite period, which increases the chances for disruption due to unexpected failures or planned reboots. Failures in DSPEs may cause an application to block or produce erroneous results. Therefore, along with scalability and low latency requirements, many critical streaming applications require DSPEs to be highly available.
Fault tolerance in the batch computing is the extensively discussed area. As there are hundreds of different clusters and machines are involved for the batch processing, and the number of nodes keep growing with the time. The chances of failure in any node inside the entire cluster increases. Moreover, these scheduling jobs and modeling keep running for massively huge time periods which exhaust the system and resources. This might result in any sort of unexpected behavior; master or slave machine get down, crash in system, abruption in connectivity services, network failure, software or hardware failure etc. Hence there are a variety of solutions which are being followed by experiencing the existing anomalies while working with batch computing.
And normally, each time a batch job fails, it either would lose the data, or corrupt the overall report. Batch Compute normally takes care of this and automatically re-provisions an instance that performs the job again. Since the job knows the batch of data it was provided, Batch Compute manages the overall availability and purity of the reports that are generated.
The passive technique for detecting and enduring fault during batch computing ensures that all the pre-check and pre-build check point are met prior of the job execution. Just as testing the connectivity, access rights and availability of the required resources. Moreover, the checkpointing can be handled in an asynchronous manner to avoid the overhead. Batch compute offers quick recovery and support distributed job scheduling and proactive checkpointing to mitigate chances of getting and unexpected errors and loss of data.
Batch compute creates new VM and node instances as per the requirement dynamically. On click resource launching and enabling technique via API call makes the recovery reliable and instant.
Batch Compute support distributed model for the data caching and backup. It runs huge number of simultaneous nodes and instances to improve the overall downtime and reliable reference point at times when any of the machine crashes, this way log files do not get missed.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.datadriveninvestor.com/is-splunk-making-money-market-mad-house-dc1e80e57435?source=search_post---------151,"Splunk (NASDAQ: SPLK) shows that cloud services companies may not make money from Coronavirus.
Notably, Splunk (SPLK) which creates Big Data management software reported a -$288.61 million operating loss for quarter ending on 30 April 2020. That operating loss grew from -$7.77 million in the previous quarter.
In addition, estimates Splunk’s quarterly revenue growth rate fell from 27.18% on 31 January 2020 to 2.17% on 30 April 2020. Moreover, Splunk’s quarterly common net loss grew from -$22.73 million on 31 January 2020 to -$305.58…
"
https://medium.com/it-dead-inside/cloud-computing-stick-to-open-standards-82dadba800a0?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
Let’s travel back to the late 80s. A rising software company known as Lotus released its answer to database technology, known as Lotus Notes.
"
https://medium.com/vmacwrites/deploy-an-image-recognition-app-without-provisioning-a-kubernetes-cluster-96d5193a4ff0?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
Updated: 04th December 2020
Follow the steps in the solution tutorial and use the companion code sample to learn about IBM Cloud™ Code Engine by deploying a text analysis application.
You will create a Code Engine project, select the project, and deploy Code Engine entities — applications and jobs to the project. You will learn how to bind IBM Cloud services (e.g., IBM Cloud Object Storage and Natural Language Understanding) to your Code Engine entities. You’ll also learn about the auto-scaling capability of Code Engine, where instances are scaled up or down (to zero) based on incoming workload.
We recently announced IBM Cloud Code Engine as the newest platform to host all of your cloud native workloads. With Code Engine, you can enjoy the cloud again.
Code Engine is a fully managed, serverless platform that runs your containerized workloads, including web apps, microservices, event-driven functions, or batch jobs. Code Engine even builds container images for you from your source code. Because these workloads are all hosted within the same Kubernetes infrastructure, all of them can seamlessly work together. The Code Engine experience is designed so that you can focus on writing code and not on the infrastructure that is needed to host it.
Code Engine helps developers by hiding many of the complex tasks, like configuration, dependency management etc., Code Engine simplifies container-based management and enables you to concentrate on writing code. It also makes available many of the features of a serverless platform, such as “scale-to-zero.”
Check out the solution tutorial for easy-to-follow steps.
If you have feedback, suggestions, or questions about this post, please reach out to me on Twitter @VidyasagarMSC or use the feedback button on the tutorial to report a problem on its content. You can also open issues.
The tutorials section has a feedback form on the side where you can comment on the content. If you have suggestions on the existing tutorials or ideas for future additions, please submit your feedback.
Originally published at https://www.ibm.com on September 30, 2020.
Tech writings by Vidyasagar Machupalli
2 
2 claps
2 
Tech writings by Vidyasagar Machupalli
Written by
Developer, IBMer, Speaker, Blogger, Teetotaller, Geek & many more…
Tech writings by Vidyasagar Machupalli
"
https://medium.com/@mslavescu/race-ai-in-the-cloud-634959fc9036?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marius Slavescu
Feb 21, 2017·1 min read
Interesting discussion about cloud AI services, members from both teams, Google and Amazon, talk about their services:
https://news.ycombinator.com/item?id=13694031
As part of Open Source Self Driving Car Initiative we use these services to train AI, beside our members and collaborators desktop machines with powerful GPUs.
A few months ago I created a public Amazon EC2 image, with the Docker described here and used it for the Udacity SDC challenges.
I forked the instructions for Docker and AWS images here, we will continue to update that AWS image as part of the OSSDC infrastructure/development tools and we will look into Google, Microsoft and Alibaba Cloud AI services also.
Join our efforts in ossdc.org
Featured image from: Melpomene/Shutterstock
STEMCA Inventor co-author, robotics/STEM education and DIY/maker platform, and Founder of GTA Robotics and OSSDC communities. Follow me at @gtarobotics
2 
2 
2 
STEMCA Inventor co-author, robotics/STEM education and DIY/maker platform, and Founder of GTA Robotics and OSSDC communities. Follow me at @gtarobotics
"
https://medium.com/@grebler/weighing-of-local-vs-cloud-116907b67ad?source=search_post---------155,"Sign in
There are currently no responses for this story.
Be the first to respond.
Leor Grebler
Feb 17, 2017·2 min read
Despite more cloud services for voice (speech to text, NLU, text to speech) becoming available, Internet service becoming faster, and new router technology making WiFi more stable, there’s been an equally explosive growth in embedded solutions. These include phrase spotting software, ASR engines, dialogue management, sound analytics, emotion detection, and far field audio conditioning.
The result is two peaks are forming — local and cloud — that need a reliable bridge. About a year ago, Phil Lewer wrote about the hybrid approach to building voice interaction. We’ve spoken with many hardware companies that are starting to become confused with the myriad of embedded solutions that are now available and will soon become available to enabled ambient voice interaction. The result of the technology that’s come out is a two-order of magnitude increase in the different options that are available to the device maker.
Considerations that companies often have to take into account are:
How is the device powered? (battery, plug, or both)
What type of interaction is it? (single turn, multiple turns)
Push to talk or handsfree?
Near or far field?
AVS, Google Assistant, something else, or multiple services?
What is the desired price point?
BT, Wifi, other connectivity, multiple sources, or none?
What type of environment will the device be used in?
The more clarity device maker has in how they want their product to function, the easier are the decision on components and local services that can be employed. Where there are overlapping services or benefits, the tie breaking can come down to benchmarking these in a prototype of the product.
Independent daily thoughts on all things future, voice technologies and AI. More at http://linkedin.com/in/grebler
25 
25 
25 
Independent daily thoughts on all things future, voice technologies and AI. More at http://linkedin.com/in/grebler
"
https://medium.com/@callmejv/how-small-businesses-roam-smarter-324a4481b4f8?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jessica Valenzuela
Aug 11, 2015·5 min read
As the internet and cloud services enables small and medium businesses to go global easier, faster and leaner than ever before, one thing has remained unchanged: expensive roaming data. The SMB needs modern ways to go global without breaking the bank in telecom expenses, so we’ve put together five super-effective ways SMBs can roam lean.
It’s a counterintuitive approach–the more businesses that rely on lean internet solutions to go international, the more expensive it becomes for employees to access those technologies while abroad. SMBs are not positioned to negotiate better contracts and discounts with carriers. For instance, AT&T won’t even negotiate business package roaming terms under 150 devices.
SMBs and the Enterprise are spending upwards of 20% of their monthly telecom bill in just roaming fees.* U.S. businesses alone spend billions of dollars annually. If things stay the way they are, network providers are set to make a cool $42 billion in roaming fee revenues by 2018, according to Juniper Research.
That’s an astronomical cost for data and voice, and it hurts small businesses the most.
Unlocked devices are a great way to get around roaming fees, but it takes some effort to make it work. U.S. businesses are at a big disadvantage when it comes to unlocked devices in comparison to EU countries. For instance, if you or your business purchased phones on an AT&T contract, your devices are locked to work only with that specific carrier until the contract has expired — or in the unlikely scenario that you paid full retail price for each device.
However, AT&T does offer the option to unlock phones for international travel up to five times per year. Once the typical two-year contract term is finished, you can also request to have the device unlocked permanently. Other U.S. carriers offer similar services, but they won’t do it automatically — you have to contact a representative (per device) and wait for the changes to be made on their side.
The benefit is that once unlocked, your devices can use prepaid SIM cards in whichever country you visit — off of the roaming network charges.
Pros: Extremely cheap prepaid data and voice options while abroad.
Cons: An unlocked phone implies you’ll switch the SIM card out while traveling — which means your contacts can’t get in touch with you at your typical mobile number, nor will they recognize your international number. It also requires you to pay a visit to a local carrier every time you arrive somewhere.
If you can’t beat the device or network provider costs, then it’s time to find an app. When a business wants to deploy data and voice cheaply to a mobile workforce, apps are the most immediate and effective solution. Roaming apps help companies avoid costly travel — and they don’t require any huge setups or implementation processes.
Roaming alert apps like Dial IQ intercept and redirect communications to free options over Wi-Fi, or integrate with preferred and industry recommended VoIPs. It alerts users when they are making costly roaming choices, and helps businesses see exactly how much they saved on roaming data volumes and international calls and SMS.
Most importantly these tools are about education and awareness. Apps that help your users connect company-preferred platforms while traveling help them reduce roaming overages.
Pros: Easy to deploy and save money immediately, saves roaming costs up-front and integrates with contacts and company VoIPs.
Cons: Apps don’t prevent costs at the network level, so users must heed the notifications and make conscious decisions to contribute to saving on roaming charges.
The Enterprise and Fortune 500 have ways of managing their telecom spend, usually in the form of complex TEM software, which handles automated inventory, reverse auction negotiations, and fancy things that the SMB generally doesn’t have the bandwidth (or budget) to implement.
But managing telecom spend is crucial for individual entrepreneurs, small and medium businesses to work abroad while keeping costs down. For the SMB, a seamless experience to determine a cheaper roaming option and concierge services are the more affordable and most powerful tools for cutting costs.
Dial IQ for instance, provides users with roaming subscription and prepaid options via VoIP. The communications platform also offers peer-to-peer messaging at no cost over WiFi. Users can create localized numbers and redirects calls to avoid incurring roaming charges on the home network. Localized numbers also let groups or teams communicate over their local numbers or WiFi.
For the enterprise, Dial IQ offers travel notifications to both the end-user and their manager at home, creating an opportunity to notify their local carriers and adjust their rates before charges are incurred.
Pros: Concierge technology that offer seamless call options are extremely simple to deploy, add new users, and start roaming immediately. Users can even configure changes while abroad.
Cons: Assistive roaming technologies and localized presence all require WiFi to some extent, and localized number technology is not always available in all countries.
Your roaming bill can all boil down to your company’s mobile VoIP technology. Services like Skype, Citrix or Jabber all offer SMS, video and conferencing tools over WiFi.
Whichever service you choose will affect how your business roams. For instance, Skype Business doesn’t allow users to transfer an existing number to their account, but an inexpensive mobile VoIP service will. Skype also doesn’t allow a user to call emergency services (like 911), while a mobile VoIP does (it’s linked directly to the smart device).
Pros: Mobile VoIP services save you a ton of money on international calls — best if you have a salesforce that makes use of voice as it makes calls abroad (and calling abroad from home) cheaper.
Cons: It’s not always easy to find a mobile VoIP that’s perfect for your company; you’ll have to shop around for rates, countries, device support, and deployability for starters.
Dial IQ is your seriously simple and fun way to find free Wi-Fi, cheap global calls and messaging. Only Dial IQ let’s you stay connected without roaming. Want to hear about our progress and launch day? Stay in touch.
I love what we’re building and growing @ https://gogoguest.com
2 
2 
2 
I love what we’re building and growing @ https://gogoguest.com
"
https://medium.com/@Apiumhub/cloud-computing-a-growing-trend-in-2017-a91ec066c43d?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
May 25, 2017·8 min read
In the past few years, cloud computing has become a common word. In fact, According to the Worldwide Semiannual Public Cloud Services Spending Guide, worldwide spending on public cloud services will grow at a 19.4% CAGR, from almost $70 billion in 2015 to more than $141 billion in 2019 with companies investing in cloud services for new competitive advantages. In the following overview about cloud computing you will get more information about what is cloud computing, the different types of service models, the benefits of cloud computing and most importantly, the cloud computing trends in 2017.
To summarise it, it’s more or less when you store data and programs on a cloud instead of storing it on the hard drive of a computer. What do you need to access the data? An internet connection. So yes, it’s about sharing, storing, processing and managing resources delivered over a network of remote servers hosted on the Internet.
Here, we talk about offering services that are related to hardware. It’s basically using a third party provider for hosting a hardware. It can be said that you are paying to access hardwares over the net, this can mean server or storage services. An example would be if you have a subscription with a hosting company to store files on their servers with a pay per use model.
Leading vendors:
Here, you are getting a development platform on the cloud. What does that mean? You get access to a platform that allows you to develop and build applications that run on systems software or hardware that belong to other companies. For example, developing an ecommerce web that is running on a vendor’s server.
Leading players:
Ss its name says it, here, you get a complete software on the cloud. So you are running a complete application on the system of a third party, hosted on the cloud. A very common example would be a CRM system, here at Apiumhub we use ZOHO that enables us to get a sort of overview of our sales cycles.
Leading players:
A private cloud is when an infrastructure is either hosted on-site on a company’s intranet, or hosted in the data center of a service provider and the resources are not shared with other organizations. We use it to increase storage capacity and the power of the processor. It is often used by big companies with big amounts of data or that have strict regulations about their data and certain types of regulations. The main advantage of going for a private cloud is that it offers a higher level of control and security. Also, it’s more customisable and therefore adapter to the specific IT requirements of companies. In general, the disadvantage would be that the maintenance and management is part of the responsibility of the company.
On the public cloud, the data of a company is stored in the data center of a third-party provider, on a shared hardware, and the storage and processor capacity are not owned by that company. Usually, small to medium sized companies use public cloud computing due to various advantages it provides them; as it’s not their hardware, it implies that there are no maintenance costs to the client and that they are not responsible for the management. Another advantage is that the time it takes for testing & deploying is decreased. Basically, it delivers agility, scalability and efficiency. Although security breaches are kind of rare, some businesses get scared off by that when it comes to public cloud.
The name is quite clear, hybrid cloud is a mix of private & public cloud services, it’s almost as if you get the best of both! What is does is that is allows to move between both clouds and enables you to leverage the beast of what each one offers. The main advantage is that you get huge flexibility and much more options, you can for example put the most critical operations on the private cloud and the rest that on the public, increasing your agility.
Now that we all kind of got what is cloud computing and the different types we’ve got, here’s the part that keeps on evolving and that is really great to know about; cloud computing trends in 2017!
The development of native apps keeps on increasing and cloud providers are more and more focusing on how to provide services for those applications that are more complex and that need things like time-based analytics, omni-channel support, and microservice support.
You probably all know what IoT is and how much it grew in the past year. Well, it’s still the beginning. In 2017, we might reach millions of sensors and other devices coming online! All of is mostly focused around smart cities, Connected Buildings, Predictive Maintenance and autonomous traffic
Don’t be shocked to see more and more image recognition and voice interfaces. Between the release of TensorFlow by google and the three new machine learning services announced by amazon, you can be sure that in 2017 it will continue on growing, specially because it is becoming easier for developers to use and integrate into applications.
Since 2015, the use of hybrid cloud has continued to grow and is expected to grow even more in 2017. Making hybrid cloud work implies having an audit function to verify that the service still fits for purpose. This is getting us to a new position “a cloud service broker” that is responsible for defining services and choose the best way to manage and secure them.
When talking about next-generation cloud applications, containers are very important. In the last two years, many companies have started using container technologies like Docker, to help them standardize the way they package and deploy code. It enables developers to really manage code.
This technology is crucial to agile development and to microservice architectures. As we will get more applications directed towards microservices, container platforms that run microservices will be more on demand. Other than being one of the most exiting cloud computing trends for developers (at least it’s the case here at Apiumhub), this obviously implies that there will be new challenges, for example companies will have to be more careful with security, monitoring, storage and networking issues.
As the demand for the cloud services is that high and will continue on growing, scalability and maintenance costs are therefore more and more important. Hyperconverged infrastructure solutions is great because it offers to help by proposing pre-integrated compute and storage resources that will help companies when it comes to getting their cloud implementations running faster. Hyper converged platforms will be more used for cloud architecture in 2017 and will become the default infrastructure platform when it come to building the private part of a hybrid cloud.
Cloud is not seen as if it’s only for small companies anymore. Huge players have started to notice the advantages and facility of it, the fact that it cuts costs and risks. So yes, we will continue on seeing the transitions of infrastructures into the cloud.
But more specifically, the public cloud. In fact, C levels are getting more and more comfortable when it comes to hosting software in the public cloud and we expect this to be one of the cloud computing trends that will grow the most.
Last year we saw many self service solutions and data has become more and more easy to push on the cloud without having a technical background. Hopefully we will soon say goodbye to the complexity of data integration & transformation, it might be more of a copy/paste action.
With all those growing cloud computing trends, there are much more companies using the cloud for their business, there will be more security standards that will try to get better migration to the cloud and therefore encouraging more businesses to integrate and adopt the cloud. This increases the demand for cloud expertise. Cloud-focused training are more frequent and training programs are focusing on cloud security, hosted databases, and infrastructure as a service.
Enough with cloud computing trends! By now you must have understood that with cloud computing you get many benefits. In a way you are enabling access to data from almost anywhere and with the growth of digital devices around us, we are just making it all more efficient and available. Here are the five main benefits of cloud computing:
With a purchased software you usually get yearly releases but when you’re using cloud computing services, it’s easy, you can get an upgraded system immediately. So you get the latest versions when they are released, including new features and functionalities and that on a regular base and with the latest technology. This can mean that you get up-to-date versions of software and upgrades to servers and processing power.
Yes, you reduce the costs. Different costs. First of all, as companies have smaller (or none) data centers by using cloud computing, it implies that you reduce costs because you don’t need to buy equipment, hardware, facilities, utilities, etc. you reduce the number of servers and software costs. Other than that, you are also lowering the staff costs and system maintenance costs.
Granting access to your employees means that you are boosting their flexibility. In fact, you can access, edit and share data from anywhere you are, because you only need a device and an internet connection (some apps even work offline). Most of employees are quite happy when they know that they can take their work anywhere. Happy employees means higher productivity.
In fact, it’s much more flexible. You can play with your capacity, scale up or down your storage following your specific needs, may they be changing or not. If your needs increase or decrease, you really don’t need to worry about it.
Obviously all businesses want to protect their data. There are many situations, which might rarely happen, but when they do, represent a tremendous problem as for example, natural disasters or power failures. Well with the cloud, everything is backed up in a secure place. This means that you can always access your data unless your device is broken or that you have no connection. But that’s not as big of a problem than losing the whole data, you just need to get another device. Your data is in the cloud, you can access it no matter what happens to your machine.
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
See all (7,844)
2 
2 claps
2 
Software architecture, web & mobile app development www.apiumhub.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ideasware/buckle-up-2ceff6aa454a?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Peter Marshall
May 18, 2017·1 min read
I suggest you give this some deep thought. Google’s AI transformation will have it leading in cloud services in 3–5 years, and give the world it’s first trillion dollar company. And in ten years it will be Baidu and Alibaba as well, and China will be in the lead forever, until the robots arrive to stay because AGI works VASTLY better than humans ever could. In our lifetime — buckle up.
https://www.wired.com/2017/05/google-rattles-tech-world-new-ai-chip/
I am extremely interested in AI, especially the not-so-good side of AI weapons and AI war, although the good parts are magnificent and wonderful too, naturally.
2 
2 
2 
I am extremely interested in AI, especially the not-so-good side of AI weapons and AI war, although the good parts are magnificent and wonderful too, naturally.
"
https://medium.com/@nderground-net/when-i-was-working-on-the-architecture-of-nderground-i-looked-at-many-cloud-providers-including-96f0d86d57c?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
nderground
May 4, 2016·1 min read
Ron Skurat
When I was working on the architecture of nderground, I looked at many cloud providers, including Heroku, Digitalocean, Engine Yard, Linode, Google and Azure.
Joking aside, I was not wild about supporting Bezos’ march toward world domination. However, when it came to cloud services, no one was close to Amazon. Amazon has better services and better documentation. By hosting on Amazon I can write less software and I run a distributed system with almost no system administration. Amazon also have very good support (I’ve gone up to San Francisco, to the Amazon Loft, and gotten free consulting help).
I think that other companies are starting to wake up to what Amazon has achieved with Amazon Web Services. For sometime I believed that the other companies didn’t even realize how far behind Amazon they were. They may realize now, but Amazon has poured billions of dollars into AWS and they are not going to be easy to catch up to.
nderground was a social network designed for privacy. nderground.net never took off and has been shut down. See topstonesoftware.com and bearcave.com.
See all (366)
1 
1 clap
1 
nderground was a social network designed for privacy. nderground.net never took off and has been shut down. See topstonesoftware.com and bearcave.com.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@neemz/digital-trends-big-data-and-cloud-services-and-their-business-impacts-3c74d2dcc1a0?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nima Torabi
Mar 25, 2020·10 min read
In today’s world, we are progressively adding to the amount of digital data whether as individuals or firms across the value chain. For example, during the manufacturing process, factories are equipped with sensors that continuously assess or track the performance of machinery and parts that move through the assembly process. Due to this, the world’s stock of information has been growing, opening doors to new digital technologies and services such as Big Data and Cloud services.
While in the 90s, more than 95 percent of stored information on the planet was analog, only 5 percent was stored in the digital format. Then, at the turn of the century, in 2000, digital accounted for 25 percent of the total stored information at around 55*10¹⁸ bytes or 55 exabytes. By 2003, digital and analog data were equally balanced at 50% and in 2007 digital information reached 94% of the total world storage capacity at 300*10¹⁸ bytes or 300 exabytes. An IDC and Seagate whitepaper forecasts that by the end of 2020, we will reach 40*10²¹ or 40 zettabytes in stored digital information and expects 175*10²¹ or 175 z-bytes .
www.washingtonpost.com
Big data has three characteristics that define it:
Big data means that there are large loads of data, in zettabytes, that come in structured and unstructured formats, and can be exchanged in real-time, which then contributes to the growth of the volume of this data.
This growing load of digital information is usually stored on a laptop, a mobile phone, or server with an IP address and these devices are globally connected in a network. This means that there exists the ability to link different pieces of information, correlate them, and build inferences, almost effortlessly, leaving a multiplier effect on the value you can extract from it.
But just because a company has a bunch of servers where it dumps every single piece of data, doesn’t make it a big data business. A company needs to be able to translate that data into a competitive advantage to create value and business impact from it. The most common uses of big data by firms include:
For example, the competitive advantage of Netflix is not only just making videos available online but also to improve the whole experience of discovering videos of interest. For this, Netflix collects enormous amounts of data and analyzes users’ watching habits to generate personalized recommendations and offerings. Netflix also analyzes what people like to watch and why, and uses this data as a basis to produce its series and movies.
www.bcg.com
neilpatel.com
digital.hbs.edu
www.mckinsey.com
www.mckinsey.com
Trust in a data and analytic sense as it applies to personal data or PII, is defined in two ways:
A majority of corporations are extremely bad in managing the consumer sentiment aspect of digital trust, and this could have a couple of reasons:
If consumers are creeped out by corporate data misuse, they end to act in ways that are brand destroying and revenue harmful. Surveys suggests that in the first year of data misuse — in today’s level of understanding of what a data misuse is by the mass market — can create a 7- 8% drop in revenues, and in year two, this figure drops to 2–3%.
Today, when there is data misuse, ~20% of the population perceives it as one, but these figures are increasing as consumers are educated. And if the figure increases from 20% to 40%, the 7–8% drop in revenues will mount up to 14–16%, which is a significant figure.
Additionally, if consumers trust an organization with their data, then they are 7–10 times more likely to let you the firm do new stuff with their data. So there is both a sustainable competitive advantage in being a trusted steward of data, and there is a serious brand and revenue hit risk of inadvertently crossing the mistrust line.
So the challenge for corporations is to make a shift from a pull-based model, whereby consumers are pulling their personal information in, to a push-based model, where the company ensures that the user understands what data is stored and used, and when the firms process the data in new ways, they provide room for users to engage with the firm in some form or fashion. The big mental shift here is understanding that data misuse has real consequences and making the pivot from a non-transparent to a transparently user engaging, data corporation, is key to progress in the digital age.
www.businessinsider.com
www.cnbc.com
www.forbes.com
hbr.org
www.bcg.com
The amount of available data is growing and it needs to be stored and processed somewhere, and this is where Cloud service comes to play.
Int the last two decades, advanced processing power has flown more into the supercomputer versus our personal devices, and communication speed doubled its growth rate when we entered the 21st century, whereas, processing power has been growing roughly at the same rate since the 90s. Therefore, it has become relatively more efficient for us o waste a small amount of time transferring data to save a lot of processing time by using the supercomputer, in the cloud. This is the shift to the cloud.
The shift to cloud services will be creating a lucrative market. the total cloud market services industry will be growing fast and is projected to reach 350 billions dollars in 2022.
www.gartner.com
Depending on how deep we want to be involved, one can be involved with four layers of cloud services:
www.bcg.com
There are three general benefits of using Cloud services:
For a typical company, cloud services can reduce costs by 20–50%. You can find finer details in the articles below.
www.bcg.com
www.mckinsey.com
home.kpmg
Hyperscalability is the key to a successful cloud solution. Hyperscalability is the definition of an architecture that can scale appropriately with increased demand. This is important because clients desire reliability and customizable microservice solutions, and usually, due to network effects factors in the digital economy, demand can grow suddenly overnight. Today, hyper-scalable solutions make up ~20% of the data-center market, and they will grow even further in the years to come.
avataracloud.com
www.mckinsey.com
The amount of stored digital data is increasing exponentially and Big Data can be defined as an enormous amount of unstructured, fast moving data that can be traced, connected, and analyzed to generate business value or transform business models. Earning consumer trust in using their personal data will be the source of a long lasting competitive advantage. Cloud services are growing in demand, driven by fundamental shifts in technology economics. Cloud provides four varying layers of services bringing performance, agility, and cost reduction to firms. Cloud services will need to provide hyper scalable architectures to mitigate demand growth risks.
Never not learning — let's connect on LinkedIn> https://www.linkedin.com/in/ntorab/ — support writers by becoming a member> https://neemz.medium.com/membership
See all (40)
1 
1 clap
1 
Never not learning — let's connect on LinkedIn> https://www.linkedin.com/in/ntorab/ — support writers by becoming a member> https://neemz.medium.com/membership
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/is-netapp-making-money-market-mad-house-5e785fa10619?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
The shift to work from home and the digital economy is not helping cloud-services provider NetApp Inc. (NASDAQ: NTAP).
In fact, NetApp’s revenue growth has shrunk for the last five quarters. Notably, Stockrow estimates NetApp’s revenue growth fell by 12% in the quarter ending on April 30, 2020. Plus, NetApp’s revenue growth fell by 10.17% in the quarter ending on January 31, 2020.
Moreover, NetApp’s quarterly revenues fell from $1.592 billion on April 30, 2019 to $1.401 billion on April 30, 2020. In addition, NetApp’s quarterly…
"
https://medium.com/pvs-studio/huawei-cloud-its-cloudy-in-pvs-studio-today-ddbfb70b4985?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
Nowadays everyone knows about cloud services. Many companies have cracked this market segment and created their own cloud services of various purposes. Recently our team has also been interested in these services in terms of integrating the PVS-Studio code analyzer into them. Chances are, our regular readers have already guessed what type of project we will check this time. The choice fell on the code of Huawei cloud services.
If you’re following PVS-Studio team posts, you’ve probably noticed that we had been digging deep in cloud technologies lately. We have already published several articles covering this topic:
Right when I was looking for an unusual project for the upcoming article, I got an email with a job offer from Huawei. After collecting some information about this company, it turned out that they had their own cloud services, but the main thing is that the source code of these services is available on GitHub. This was the main reason for choosing this company for this article. As one Chinese sage said: “The accidents are not accidental”.
Let me give you some details about our analyzer. PVS-Studio is a static analyzer for bug detection in the source code of programs, written in C, C++, C#, and Java. The analyzer works on Windows, Linux, and macOS. In addition to plugins for classic development environments, such as Visual Studio or IntelliJ IDEA, the analyzer has the ability to integrate into SonarQube and Jenkins:
When I was doing some research for the article, I found out that Huawei had a developer center with available information, manuals, and sources of their cloud services. A wide variety of programming languages were used to create these services, but languages such as Go, Java and Python were the most prevailing.
Since I specialize in Java, the projects have been selected in keeping with my knowledge and skills. You can get project sources analyzed in the article in a GitHub repository huaweicloud.
To analyze projects, I needed only a few things to do:
Having analyzed the projects, we selected only three of them, which we would like to pay attention to. It is because of the fact that the size of the rest Java projects turned out to be too small.
Project analysis results (number of warnings and number of files):
There were few warnings, which tells us about high quality of code, all the more so since not all warnings point at real errors. This is due to the fact that the analyzer sometimes lacks information to distinguish the correct code from the erroneous one. For this reason we tweak analyzer’s diagnostics day by day with recourse to the information from users. You’re welcome to see the article “The way static analyzers fight against false positives, and why they do it”.
As of analyzing the project I picked over only the most hotshot warnings, which I’ll talk about in this article.
V6050 Class initialization cycle is present. Initialization of ‘INSTANCE’ appears before the initialization of ‘LOG’. UntrustedSSL.java(32), UntrustedSSL.java(59), UntrustedSSL.java(33)
If there is any exception in the UntrustedSSL class constructor, the information about this exception is logged in the catch block using the LOG logger. However, due to the initialization order of static fields, at the moment of initializing the INSTANCE field, LOG isn’t initialized yet. Therefore, if you log information about the exception in the constructor, it will result in NullPointerException. This exception is the reason for another exception ExceptionInInitializerError, which is thrown if there had been an exception when the static field had been initialized. What you need to solve this problem is to place LOG initialization before INSTANCE initializing.
V6005 The variable ‘this.metricSchema’ is assigned to itself. OpenTSDBSchema.java(72)
Both methods set the metricSchema field, but the method’s names differ by one ‘s’ symbol. The programmer named the arguments of these methods according to the name of the method. As a result, in the line the analyzer points to, the metricSchema field is assigned to itself, and the metricsSchema method’s argument is not used.
V6005 The variable ‘suspend’ is assigned to itself. SuspendTransferTaskRequest.java(77)
Here is a trivial error related to carelessness, because of which the suspend argument is assigned to itself. As a result, the suspend field won’t be assigned the value of the obtained argument as implied. The correct version:
As often happens, the V6007 rule breaks ahead in terms of warnings quantity.
V6007 Expression ‘firewallPolicyId == null’ is always false. FirewallPolicyServiceImpl.java(125)
In this method arguments are checked for null by the checkNotNull method:
After checking the argument by the checkNotNull method, you can be 100% sure that the argument passed to this method is not equal to null. Since both arguments of the removeFirewallRuleFromPolicy method are checked by the checkNotNull method, their further check for null makes no sense. However, the expression, where firewallPolicyId and firewallRuleId arguments are re-checked for null, is passed as the first argument to the checkState method.
A similar warning is issued for firewallRuleId as well:
V6007 Expression ‘filteringParams != null’ is always true. NetworkPolicyServiceImpl.java(60)
In this method, if the filteringParams argument is null, the method returns a value. This is why the check that the analyzer points to will always be true which, in turns, means that this check is meaningless.
13 more classes are similar:
V6008 Potential null dereference of ‘m.blockDeviceMapping’. NovaServerCreate.java(390)
In this method, the initialization of the m.blockDeviceMapping reference field won’t happen if the blockDevice argument is null. This field is initialized only in this method, so when calling the add method from the m.blockDeviceMapping field, a NullPointerException will happen.
V6008 Potential null dereference of ‘FileId.get(path)’ in function ‘<init>’. TrackedFile.java(140), TrackedFile.java(115)
The constructor of the TrackedFile class receives the result of the static FileId.get(path) method as a third argument. But this method can return null:
In the constructor, called via this, the id argument doesn’t change until its first use:
As we can see, if null is passed as the third argument to the method, an exception will occur.
Here is another similar case:
V6008 Potential null dereference of ‘dataTmpFile’. CacheManager.java(91)
NPE again. A number of checks in the conditional operator allows the zero object dataTmpFile for further dereference. I think there are two typos here and the check should actually look like this:
V6009 The ‘substring’ function could receive the ‘-1’ value while non-negative value is expected. Inspect argument: 2. RemoveVersionProjectIdFromURL.java(37)
The implication is that this method gets a URL as a string, which is not validated in any way. Later, this string is cut off several times using the lastIndexOf method. If the method lastIndexOf doesn’t find a match in the string, it will return -1. This will lead to StringIndexOutOfBoundsException, as the arguments of the substring method have to be non-negative numbers. For correct method’s operation, one has to add an input argument validation or check that the results of the lastIndexOf method are non-negative numbers.
Here are some other snippets with a similar way things are:
V6010 The return value of function ‘concat’ is required to be utilized. AKSK.java(278)
When writing this code, its author didn’t take into account that a call of the concat method won’t change the host string due to immutability of the String type objects. For correct method’s operation, the result of the concat method has to be assigned to the host variable in the if block. The correct version:
V6021 Variable ‘url’ is not used. TriggerV2Service.java(95)
In this method, the url variable isn’t used after its initialization. Most likely, the url variable has to be passed to the uri method as a second argument instead of functionUrn, as the functionUrn variable takes part in the initialization of the url variable.
V6022 Parameter ‘returnType’ is not used inside constructor body. HttpRequest.java(68)
In this constructor, the programmer forgot to use the returnType argument, and assign its value to the returnType field. That’s why when calling the getReturnType method from the object, created by this constructor, null will be returned by default. But most likely, the programmer intended to get the object, previously passed to the constructor.
V6032 It is odd that the body of method ‘enable’ is fully equivalent to the body of another method ‘disable’. ServiceAction.java(32), ServiceAction.java(36)
Having two identical methods is not a mistake, but the fact that two methods perform the same action is at least strange. Looking at the names of the above methods, we can assume that they should perform the opposite actions. In fact, both methods do the same thing — create and return the ServiceAction object. Most likely, the disable method was created by copying the enable method’s code, but the method’s body remained the same.
V6060 The ‘params’ reference was utilized before it was verified against null. DomainService.java(49), DomainService.java(46)
In this method, the author decided to check the contents of a structure of the Map type for null. To do this, the get method is called twice from the params argument. The result of the get method is passed to the checkNotNull method. Everything seems logical, but it’s not like that! The params argument is checked for null in if. After this it is expected that the input argument might be null, but before this check, the get method has already been called twice from params. If null is passed as an argument to this method, the first time you call the get method, an exception will be thrown.
A similar situation occurs in three other places:
Today’s large companies can’t do without usage of cloud services. A huge number of people use these services. In this view, even a small error in a service might lead to problems for many people as well as to additional losses, racked up by a company to remedy adverse consequences of this error. Human flaws should always be taken into account especially since sooner or later everyone makes mistakes, as described in this article. This fact substantiates usage of all possible tools to improve the code quality.
PVS-Studio will definitely inform the Huawei company about the results of checking their cloud services so as to Huawei developers could dwell on them, because one-time usage of static code analysis covered by this articles (1, 2) can’t fully demonstrate all its advantages. You can download the PVS-Studio analyzer here.
PVS-Studio is a tool for detecting bugs and security…
3 
3 claps
3 
PVS-Studio is a tool for detecting bugs and security weaknesses in the source code of programs, written in C, C++, C# and Java. It works under 64-bit systems in Windows, Linux and macOS environments, and can analyze source code intended for 32-bit, 64-bit and embedded ARM platfor
Written by
The developer, the debugger, the unicorn. I know all about static analysis and how to find bugs and errors in C++, C#, and Java source code.
PVS-Studio is a tool for detecting bugs and security weaknesses in the source code of programs, written in C, C++, C# and Java. It works under 64-bit systems in Windows, Linux and macOS environments, and can analyze source code intended for 32-bit, 64-bit and embedded ARM platfor
"
https://medium.com/@yegor256/10-continuous-integration-services-3b429313a34e?source=search_post---------163,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yegor Bugayenko
Oct 5, 2014·1 min read
Every project I’m working with starts with a setup of continuous integration pipeline. I’m a big fan of cloud services, that’s why I was always using travis-ci.org. A few of my clients questioned this choice recently, mostly because of the price. So I decided to make a brief analysis of the market. The article analyzes 10 hosted continuous integration services, which can build a private repository.
Continue reading…
CEO @ Zerocracy
1 
1 
1 
CEO @ Zerocracy
"
https://medium.datadriveninvestor.com/can-teradata-make-money-in-the-cloud-market-mad-house-42864e9f43c7?source=search_post---------164,"There are currently no responses for this story.
Be the first to respond.
The Teradata Corporation (NYSE: TDC) could profit from the astronomical growth of the cloud services market.
To explain, Teradata (TDC) specializes in cloud analytics. For example, its Teradata Vantage analytics architecture can unify Analytics Platforms, Data Lakes, and Data Warehouses.
To elaborate, a is a centralized repository that can store enormous amounts of structured and unstructured data. In addition, different analytics can run in Data Lakes. Amazon Web Services (AWS) claims data lakes outperform…
"
https://medium.com/@nutanix/nutanix-clusters-hybrid-cloud-infrastructure-now-available-on-aws-govcloud-us-5675bc0442bd?source=search_post---------165,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Jul 2, 2021·10 min read
FedRAMP Authorized Nutanix Government Cloud Services helps to Enable U.S. Public Sector to Get to a Hybrid Cloud Environment
By Sherry Walshak and Sahil M Bansal
In this age of imperative digital innovation, the announcement that Nutanix Clusters cloud platform now supports the Amazon Web Services (AWS) GovCloud (US) region is a welcome choice for government customers. Nutanix Clusters is a component of the Nutanix Government Cloud Services, which has a FedRAMP authorization at the Moderate security impact level. Government agencies implementing Cloud Smart mandates can now consider another option to securely and seamlessly move their on-premises applications, sensitive data and regulated IT workloads to the cloud, and take advantage of rich cloud services. A built-in native integration with AWS networking makes the initial set-up very easy, allowing government IT teams to typically provision a hybrid cloud environment in less than an hour.
The case for using cloud capabilities in government has been clearly proven — to improve the quality and access of government services to its population; deliver mission-serving applications and services faster; support disaster recovery for critical applications; better secure sensitive systems and data; and drive operational efficiencies and cost savings.
A common question is how best to evolve existing IT environments? Customers have seen a blur of choices as well as overwhelming complexity in evaluating offerings. There is urgency in the government to move workloads to the cloud, and customers tell us it often feels like a “race to win”.
In August 2020, we announced the general availability of the Nutanix Clusters platform on AWS, which enables customers to extend data center capabilities natively into AWS public clouds (typically within one hour). This hybrid cloud solution manages applications and infrastructure in private and public clouds as a single cloud, greatly simplifying the IT environment. In addition, customers can choose the optimal cloud environment for each application based on their service or mission needs. After successfully deploying Nutanix Clusters in commercial AWS regions, Nutanix addressed the additional government requirements and has now announced its general availability on AWS GovCloud which is a component of the Nutanix Government Cloud Services.
Fig 1: Nutanix Clusters: Hybrid Cloud Infrastructure with AWS
Nutanix Clusters runs the core Nutanix hyperconverged infrastructure software stack (including Nutanix AOS, AHV and Prism software) and supports multiple Nutanix products and services on bare metal AWS EC2 instances with support for the following instance types (subject to regional availability):
Nutanix Clusters simplifies infrastructure and seamlessly migrates or extends any virtualized application across platforms — all under a single management console. This single user interface manages license portability across both private and all public cloud environments, eliminating the need for separate teams to manage each environment. This is great news for government IT teams struggling with limited resources and skill sets.
The recent Nutanix Enterprise Cloud Index study shows 87% of U.S. Federal respondents identify a hybrid cloud model of private and public clouds as the ideal IT operating environment for their organization. However, adoption has been slow with only 14% of respondents currently running a hybrid cloud environment. This is not surprising as many of these organizations are struggling with transitioning legacy and mission critical applications to cloud environments. This solution will help government agencies accelerate their journey to hybrid cloud, improving operations and performance.
When evaluating hybrid cloud solutions, most options typically require government agencies to enter into a vendor managed services contract on top of a cloud service provider. These options often present constraints around scope of management control, visibility into issue resolution, license portability from on-prem to public cloud and lack of direct billing with public cloud.
Nutanix Clusters on AWS GovCloud is a simple extension of your on-premise hyperconverged infrastructure to AWS GovCloud, which reduces the inherent complexity of delivering a consistent experience across your datacenter and AWS GovCloud. With Nutanix Clusters on AWS GovCloud, you directly manage your AWS GovCloud environment as you would any other datacenter, without the additional costs and limitations of a managed service. As your requirements change, you can easily shift software license entitlements from on-premise nodes to bare metal AWS EC2 instances without being locked into perpetual licensing. Unlike managed services where you are paying the 3rd party vendor for consumption of the entire solution, Nutanix Clusters (including Nutanix Clusters on AWS GovCloud) allows you to leverage your existing AWS accounts, contracts, discount structure, consumption and billing model. This provides greater control of the hybrid cloud infrastructure as well as more flexibility when migrating apps to AWS.
Fig2: Nutanix Clusters gives YOU the control and delivers YOUR Nutanix software in YOUR AWS GovCloud account
Nutanix Clusters on AWS GovCloud provides an important choice for Federal government customers who want to increase their AWS adoption while maintaining the freedom that a hybrid environment provides between their datacenter and public cloud.
AWS GovCloud (US) is an isolated region designed to allow U.S government agencies to move their confidential data into the cloud to address their compliance and specific regulatory requirements.
The Federal Risk and Authorization Management Program (FedRAMP) provides a standardized security framework for cloud products and services that are recognized by all executive branch federal agencies. Nutanix Government Cloud Services has successfully completed a full security assessment and authorization at a moderate security impact level of 325 controls. The FedRAMP Moderate control baseline equates to a DoD Impact Level 2.
The Nutanix Government Cloud Services are inclusive of other components enabling government customers to optimize their cloud spending, improve their security posture and support remote teleworkers.
Whether increasing capacity for Virtual Desktop Infrastructure (VDI) deployments, leveraging cloud capacity for disaster recovery to help ensure operations, or increasing developer productivity with a parallel dev/test workstream, government customers can seamlessly extend their Nutanix environments to AWS GovCloud and scale as needed. In working with our government customers, we find these typical key use cases:
“Penn National had been backing up everything to tapes and storing them offsite. If they ever had a disaster, it would have taken several days before all their systems were back online. Nutanix Clusters enabled them to complete all the testing for the AWS deployment in one day. They linked their on-prem data protection Cluster with their rapid recovery Nutanix Cluster on AWS, and in less than two hours, the desktops were on AWS. By scaling out their virtual desktop infrastructure into a Nutanix Cluster on AWS, Penn National was able to discontinue the expense of a secondary site, freeing up budget to be used for strategic initiatives.”
Fig3: Key Features of Nutanix Clusters on AWS GovCloud
Nutanix Clusters on AWS GovCloud has important features that support performance and availability, while minimizing cost. These include:
Our experts working with government customers in the trenches have these three insider tips:
“We know Federal agencies are looking for solutions to help them implement their Cloud Smart strategies, but existing offerings lack the necessary security or are cumbersome to deploy and manage,” said Chip George, VP Public Sector of U.S. Sales at Nutanix. “Nutanix Clusters on AWS GovCloud enables organizations to significantly accelerate their cloud adoption, without needing to re-architect mission critical applications that are necessary to the agency’s operations and mission, providing an easy path to a unified hybrid cloud environment.”
We welcome your feedback and look forward to accelerating your hybrid cloud journey!
© 2021 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned on this post are registered trademarks or trademarks of Nutanix, Inc. in the United States and other countries. Other brand names mentioned on this post are for identification purposes only and may be the trademarks of their respective holder(s). This post may contain links to external websites that are not part of Nutanix.com. Nutanix does not control these sites and disclaims all responsibility for the content or accuracy of any external site. Certain information contained in this post may relate to or be based on studies, publications, surveys and other data obtained from third-party sources and our own internal estimates and research. While we believe these third-party studies, publications, surveys and other data are reliable as of the date of this post, they have not independently verified, and we make no representation as to the adequacy, fairness, accuracy, or completeness of any information obtained from third-party sources.
This post may contain express and implied forward-looking statements, which are not historical facts and are instead based on our current expectations, estimates and beliefs. The accuracy of such statements involves risks and uncertainties and depends upon future events, including those that may be beyond our control, and actual results may differ materially and adversely from those anticipated or implied by such statements. Any forward-looking statements included in this post speak only as of the date hereof and, except as required by law, we assume no obligation to update or otherwise revise any of such forward-looking statements to reflect subsequent events or circumstances.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@joshualenon/shadow-it-puppets-4d2fd20a263?source=search_post---------166,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joshua Lenon
Jun 10, 2016·3 min read
Your law firm employees are using cloud services without your knowledge.
In a recent survey, 61% of businesses had employees who created a mobile app for use at the office in 2015, without any input, help, or even awareness by their IT department. These rogue employees are using the growing breed of cloud apps that build custom software. IT security consultants call these quasi-authorized apps “Shadow IT.” Employees are building Shadow IT apps that replace paper forms and manual processes, and they’re doing it in a matter of hours. That client intake checklist you’ve relied upon for over a decade? It’s been replaced with a Google Apps for Business digital form and spreadsheet.
Your firm is using these apps already; you just didn’t realize it.
I’m to blame for this.
At Clio, we’ve posted several guides on how to build similar app integrations. We’ve hosted webinars on automating you law firm. We post our API publicly, so that your tech-savvy employees can build their own versions of Clio. We’re making it easy for your law firm colleagues to build their own tools, automate their own tasks, and speed up their productivity.
Interconnectivity functions are going to be the bedrock of the next generation of legal technology. Legal tech will not just connect vertically, like when “Wexis” overcharges you to connect their tools together, but also horizontally between providers. Right now, any legal technology provider or law firm can build an integration with Clio. A legal ecosystem is being built that let’s specialized tools connect with firm management, playing to the strengths of each tool.
The downside to this interconnectivity is that your information proliferates, ending up in places you may not even realize exist. The chance of inadvertent disclosure grows with each new app plugged into your office IT. Privacy and confidentiality concerns grow exponentially when you think of apps plugging into apps, plugging into yet more apps. Without strong oversight, your data can end up anywhere.
So, what should you do when law firm employees can build new IT infrastructure whenever they feel like it?
First, encourage your employees to build this type of tool. The worst thing you can do is make them feel like they have to hide this activity from you. They’re going to be building Shadow IT anyway. You want them to be able to brag about the improvements they’ve designed, rather than hiding their screens from you when you approach. Participation is your best means of getting insight into when employees are plugging tools together. These discussions also let you give feedback and perform due diligence on the systems being built. You can quickly determine what combinations are beneficial, and which are risky.
Second, make sure your tools have adequate logging and reports. Even if an employee doesn’t tell you about their latest SaaS solution of the week, you want to see when new tools plug into your current ones. Token requests, API connections, and even credit card charges give you insight into what your employees are doing behind your back. Periodically reviewing your IT infrastructure is now a fact of modern legal practice.
Third, examine which parts of the firm’s workflow is holding people back. Most Shadow IT is being built because people know that better methods exist. We prefer to use tools that make work easier. Choose IT infrastructure with ease-of-use as a feature. Odds are your employees will like the tools provided. They won’t feel the need to build their own guerilla workflows.
It’s never been easier to build new software. To your employees, that’s a good thing. To you, it’s a potential nightmare. Don’t let Shadow IT catch you in the dark.
Clio's gentleman Lawyer-in-Residence. I'm interested in intersections of law & technology. Practicing an #AltLegal career.
1 
1 
1 
Clio's gentleman Lawyer-in-Residence. I'm interested in intersections of law & technology. Practicing an #AltLegal career.
"
https://medium.com/@pkay225/ive-been-pretty-skeptical-on-apple-to-the-point-of-viewing-them-as-an-inert-pile-of-cash-led-by-a-78e5004bdc42?source=search_post---------167,"Sign in
There are currently no responses for this story.
Be the first to respond.
Peter Kay
Jun 16, 2017·1 min read
Jean-Louis Gassée
I’ve been pretty skeptical on Apple to the point of viewing them as an inert pile of cash led by a boring supply chain guy. A dead man walking.
Their cloud services are a disaster and AMZN and GOOG are light years ahead of them in that space. You have contacts there — is the Apple culture capable of meaningful innovation anymore? Their moat, iOS and their devices shrinks daily because of Android.
Problem solver: Product, Data Science, Knowledge Management. Tried a couple of startups: failure is informative but not fun.
2 
2 
2 
Problem solver: Product, Data Science, Knowledge Management. Tried a couple of startups: failure is informative but not fun.
"
https://medium.com/@lynnlangit/10-legacy-cloud-considerations-44b2a5073706?source=search_post---------168,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Nov 19, 2021·3 min read
Working with clients over the past twelve years, I’ve seen cloud services evolve more quickly than knowledge about how best to use them. To that end, my recent contracts have involved more and more modernization work of legacy cloud applications. We discover common opportunities for improvement across many of these applications.
Discover the current state in appropriate detail via patterned inquiry .
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/nerd-for-tech/how-to-debug-microservices-in-the-cloud-59aa10cd9653?source=search_post---------169,"There are currently no responses for this story.
Be the first to respond.
The growth in information architecture has urged many IT technologies to adopt cloud services and grow over time. Microservices have been the frontrunner in this regard and have grown exponentially in their popularity for designing diverse applications to be independently deployable services.
Trivia: In a survey by O’Reilly, over 50% of respondents said that more than 50% of new development in their organization utilizes microservices.
Using isolated modules, microservices in the Cloud stray away from using monolithic systems, where an entire application could fail due to a single error in a module. This provides developers with much broader flexibility for editing and deploying customizable codes without worrying about affecting separate modules.
However, this approach brings along unique challenges when there is an accidental introduction of bugs. Debugging microservices in the Cloud can be a daunting task due to the complexity of the information architecture and the transition from the development phase to the production phase.
Let’s explore what these challenges are and how you can seamlessly navigate around them.
The growth in the demand for microservices brings along complex infrastructures. Every cloud component, module, and serverless calls often conceal the infrastructure’s actual intricacy, making it difficult for DevOps and operations teams to trace and observe the microservice’s internal state, based on the outputs. Microservices running independently makes it especially difficult to track any user requests existing in the asynchronous modules, which might cause a chain-reproduction of errors. It also means that detecting services that are interacting with each other might become susceptible to these errors too. These factors make pinpointing the root cause of any error or bug a daunting task for developers.
Since many microservices come together to build a system, it becomes complicated to monitor its state. As more microservice components add to the system, a complex mesh of services develops with each module running independently. This also brings forth the possibility that any module can fail anytime, without affecting other modules.
Developers can find it extremely hard to debug errors in some particular microservices. Each of them can be coded in a different programming language, have unique logging functions, and are mostly independent of other components.
It is also unpredictable for developers to monitor the performance and state errors when moving the codes from the development phase to the production phase. We can’t predict how the code will perform when it processes hundreds of thousands of requests on distributed servers, even after integration and unit testing. If the code scales inadequately or if the database isn’t able to process the requests, it’ll make it almost cryptic for developers to detect the system’s underlying error.
Here are some microservices-specific debugging methods, which can help you in navigating around the challenges mentioned below:
Unlike traditional debugging methods, third-party tools can help the DevOps teams set breakpoints that don’t affect the debugging process’s execution by halting or pausing the service. These methods are non-intrusive and allow the developers to view global variables and stack traces, which helps them monitor and detect bugs more efficiently. It also allows the developers to test hypotheticals about where the issues might arise without halting the code or redeploying their codebase.
Any system with a multitude of microservices makes it extremely difficult to track requests. While you might think that building a customized platform for observability might be the answer to this issue, it would consume a lot of time and resources in its development.
Fortunately, many modern, third-party tools are designed to track requests and provide extensive observability for microservices. These tools come packed with many other benefits, such as distributed and serverless computing capabilities.
For instance, tools like Thundra can help you monitor user requests that are moving through your infrastructure during production, assisting developers in getting a holistic overview of the coding environment, pinpointing the source of bugs, and debugging it quickly.
It’s an uphill battle for a system to realize that there is an error or bug in the first place. The system must automatically track any exceptions as they occur, thereby helping the system identify repetitive patterns or destructive behaviors like leap year error, errors in a specific version of the browser, odd stack overflows, and much more.
However, capturing these errors is only half the battle won. The system also needs to track variables and logs for pinpointing the time and conditions under which the error occurred. This helps the developers in replicating the situation and finding the most effective solution to remove the error. Comprehensive monitoring can significantly simplify the process of debugging in production.
With modern microservices, debugging can be a very complex process for anyone. The ability to trace user requests and predicting how well the code can scale is very complicated. However, modern tools can make it easier for developers to monitor, detect, and resolve errors. LOGIQ is a one-stop-shop for microservices monitoring and observability, that lets you leverage the power of machine data analytics for infrastructures and applications on a single platform.
Microservice architectures are designed to be quickly deployable, and with the right set of tools, debugging becomes much simpler for the developers.
From Confusion to Clarification
25 
Subscribe to our weekly News Letter to receive top stories from the Industry Professionals around the world Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
25 claps
25 
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cpucoin/equilibrium-partners-with-cpucoin-in-preparation-for-their-mediarich-content-cloud-launch-in-2020-d9ec1021a44b?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
CPUcoin
Dec 19, 2019·4 min read
CPUcoin and Equilibrium live test large-scale decentralized CPU/GPU sharing economy network to power on-demand content cloud services
CAYMAN ISLANDS, December 18, 2019 — CPUcoin (cpucoin.io), developer of the Compute Generation Network (CGN), today announced its partnership with Equilibrium, the company that pioneered dynamic imaging and video auto-assembly technology. As part of this agreement, CPUcoin will launch the first dService (decentralized server apps) powered by CPUcoin’s CPU/GPU MediaGen sharing economy, based on the MediaRich all content server for rendering, transcoding, normalization and streaming over 450 file types.
The CPUcoin Compute Generation Network (CGN) is already available for production batch and dynamic image transformations with the total computing power capable of processing millions of digital assets per hour. Existing customers of Equilibrium will be offered early access to the sharing economy and two new product offerings are currently in beta on the network to be launched as part of MediaRich Cloud.
MediaRich currently powers multiple Fortune 500 companies including Walmart, Adidas, Warner Bros, and The Department of Energy. Equilibrium is the publisher of the patented, massively scalable MediaRich engine for enabling automatic real-time personalization, preparation and delivery of office files, sound, music, images and video to website, wireless and portable devices from any original file to any viewer on-the-fly. CPUcoin will provide Equilibrium MediaRich with automatic scalability while enabling connectivity in Equilibrium’s cloud content publishing offering and the company’s OneViewer for Microsoft Office 365, as well as integrations into other systems via APIs and widgets to be made available for others to use on the CPUcoin Compute Generation Network.
“The partnership with Equilibrium enables CPUcoin to onboard large-scale enterprises directly on to our network, thus driving enterprise adoption and enhanced utility of the native token, something rarely seen in the blockchain space. Moreover, Equilibrium will provide us with the first scalable ‘real world’ use case for decentralized compute power by solving enterprise-class tasks for batch processing and simple on-demand delivery requests for user experience requirements,” said Oliver Jensen, Head of Product at CPUcoin, “it does this through a decentralized sharing economy without requirements to pre-set up hardware, deal with load balancing, resiliency or other issues associated with scaling services, all while rewarding users (miners) for their machines and enabling developers to scale their business at a fraction of the cost of other hosted environments.”
The first dApp launched by Equilibrium is a new hosted decentralized freemium product called MediaRich® Publisher, which enables instant collaboration across hundreds of file formats. The second dApp, ThumbnaiLIT™, is a simple upload utility (delivered as a series of widgets) for solving the consistent issues surrounding thumbnailing and ingesting any format into any user experience (i.e. no more filetype, preview or size problems to upload your photo, office or PDF file into any web page or user profile).
Already with hundreds of machines on the testnet, CPUcoin represents the first true enterprise-grade global sharing economy solution for unused CPU/GPU power. CPUcoin technology enables anyone to install a miner client and collect funds for unused compute time. Unlike traditional crypto currency mining, the CPUcoin miner adds any machine to the CGN grid compute network instantly. Then all requests are sent to the CPUcoin Compute Generation Network providing a platform for automatically deploying and scaling Decentralized Server Applications and DApps that are enterprise-class. The Compute Generation Network pays users for the idle time of servers, mobile devices and PC computers. The CGN effectively eliminates the hassle and high expenses associated with provisioning, managing and scaling applications on a global scale, while providing end users with up to five times more computational throughput than traditional cloud providers like AWS Lambda.
About EquilibriumFounded in 2004, Equilibrium provides on-premise or cloud-based software to centralize, streamline, automate, and optimize digital asset performance and workflows for a wide variety of verticals including architecture, engineering, construction, packaging, manufacturing, marketing agencies, corporate communications, and government organizations. Visit: https://equilibrium.com to learn more.
About CPUcoinCPUcoin is developing a first-of-its-kind Compute Generation Network (CGN), a unique, flexible, and scalable Infrastructure-as-a-Service offering for a new sharing economy of miners. CPUcoin provides a distributed system for delivering services that power DApps (Decentralized Applications) for both consumers and enterprise-class users. The CGN is a general-purpose ecosystem, uniquely architected to excel in providing services that meet demand in real-time, performing the specialized work needed to generate and deliver vast amounts of content, data analysis, artificial and business intelligence, web and over-the-top data or other content consumed by DApps globally. The CGN ecosystem is exclusively powered by a new utility token called CPU Coin (Ticker symbol: CPU on Probit.com).
CPUcoin Trading on ProBit Exchange
CPUcoin is now available at ProBit Exchange
Current CPUcoin (CPU) Pairings: CPU/USDT & CPU/KRW
To mine CPUcoin, please install the miner at https://cpucoin.io/downloads
Media Contact: cpucoin@transformgroup.com
Creating the Content Generation Network. For more information please visit: https://cpucoin.io/
1 
1 
1 
Creating the Content Generation Network. For more information please visit: https://cpucoin.io/
"
https://medium.com/@amitashwini/what-is-enterprise-mobility-7cb2f522cb48?source=search_post---------171,"Sign in
There are currently no responses for this story.
Be the first to respond.
Amit Ashwini
May 11, 2015·5 min read
Thanks to enterprise mobility, more employees are able to work away from the office by using cloud services and mobile devices. Not only does this term refer to workers and their mobile devices, but the mobility of corporate data as well. If an employee needs to use their iPad to show information to a Client, they can easily upload their corporate presentation from their PC to their cloud storage service. Is this system flawless? Absolutely not, but it does show enormous potential when it comes to changing the infrastructure of the entire corporate world. Let’s take a moment to discuss a few of enterprise mobility’s most critical points.
When we consider mobile infrastructure, it’s easy to see that it provides the very foundation on which enterprises create their mobile solutions. This means that it must be both pervasive and well-designed. However, the infrastructure is quite often neglected due to arising issues within the business that need immediate attention. And because users are not complaining about the wireless network, business owners put this particular issue on the back burner. What these companies soon come to realize is that there are several benefits to virtualization. They experience improved security; their business has continuity, and they gain access to their legacy applications.
As technology evolves, people are expecting to not only use their phones to manage their personal life but their professional life as well. In fact, the line has become so blurred that many don’t acknowledge the term ‘office hours’ because work is always just a click away. They view mobile technology as a way to improve customer satisfaction and increase their profit margins.
Because of this, just as websites were rapidly created in the 90’s, mobile applications are being created with the exact same fervor. As a matter of fact, they can’t be created fast enough; and this is often a huge feat considering obstacles such as the short life cycles of devices, the lack of application development tools, privacy issues, costs for development, the shortage of development expertise and perhaps most importantly, the security risks.
It’s important to keep in mind that just because a company goes mobile, it doesn’t mean they won’t still face fundamental information security risks. Enterprise security teams often find themselves at a lost when it comes to controlling the mobile endpoint. IT organizations are often faced with figuring out the best ways for users to authenticate when they are operating tablets and smartphones. This is why there is currently a huge focus on using mobile device services as authenticators for gaining accessing. Third-party software vendors want to fill the void in order to solve security and identity issues.
Just as there are kinks within the security, enterprises are also struggling to adapt to their new employment norms that are taking place as a result of wireless networks, social media, handheld devices, and cloud-based solutions. Mobility is changing the way people interact with these tools. One now has to open several applications in order to do work on their desktop PC.
Many Global Industry Analysts are predicting that by 2018, the global market for enterprise mobility will have reached $218 billion dollars. This is because mobile devices are able to increase the speed of operations. They are extremely good at data collection and help companies make faster decisions. They lead to quicker reporting of certain events and reach a wider audience. They have the ability to improved productivity due to data collection in real-time. And among many other things, they also help to reduce a company’s expenses. Ultimately, enterprise mobility means that a company will attain higher profits based solely on its efficiency gains.
A researcher at Digital Workplace Group pointed out 10 distinct advantages of enterprise mobility. He feels that it helps with portability; it leads to greater digital communication and engagement amongst employees. It helps with availability; it increases productivity and can deliver crucial operational information — and in real time. It allows the company to share; it improves customer service and encourages learning and the transfer of knowledge. It gives the company access to data in context; this allows for better customer service and increased productivity.
Enterprise mobility captures your data in real time; this way there is great productivity throughout the entire process. It offers an improvement in user experience; there is a higher level of engagement among the company’s workforce. It makes it so that there is personal ownership of these devices which can help to reduce the costs and increase engagement. This system creates independence which leads to efficiency and a reduction in costs. It gives access to geolocation which improves and enhances productivity. Also, built-in cameras allow for data scans and video recording.
Your company will ultimate experience and increase in operational speed and efficiency. This is especially true in industries that depend on maintaining tedious webs of activities to keep the business in operation. You can expect to collect your data more efficiently. This helps you to make better decisions and allows you to share information in real-time.
Business owners will benefit from faster reporting which benefits your mobile sales apps and allows you to better allocate your resources. Your company will experience an increase in its visibility, and this is especially very useful to the oil industry.
However, one of the main benefits is that you will be able to collaborate more. This will take place through social networking. And if you are a large company with a multi-level hierarchy, this will encourage cross-functional problem solving and teamwork.
If companies learn how to institute BYOD (Bring Your Own Device) policies wisely, then it may be just what’s needed to take the company to a new level. And though mobile technology can be somewhat disruptive when it comes to communication, content, social software and collaboration, it can also create a better user experience overall. Enterprise mobility may seem challenging and tedious, but it can lead to seamless interactions among employees, employers, and their customers.
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
1 
1 
1 
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
"
https://medium.com/@thehosk/there-is-cycle-of-recreating-applications-when-the-old-applications-go-out-of-support-62899b7c9ace?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben ""The Hosk"" Hosking
·Sep 6, 2021
There is cycle of recreating applications when the old applications go out of support.
LC/NC offer powerful tools that create applications that are hosted in cloud services. This by itself is appealing. A standard for small applications , no servers and backups.
For small applications LC/NC is a good choice because it can be faster and cheaper.
The problems will come later when you have 100’s of small applications to maintain and all these need to be tested each year when Microsoft (or other) updates its software.
Lots of these applications will be low quality and result in a build up of technical debt. Supporting these will be not be fun and they will struggle to find people who want to do this for a career.
1 
1 
Software dev (C#, Java) → Solution architect Dynamics 365, Power platform on enterprise projects | Avid reader | Life long learner
"
https://medium.com/@wkrzywiec/hi-im-really-happy-that-you-ve-liked-it-f30da3743b61?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wojciech Krzywiec
·Sep 13, 2021
seggi SMS
Hi, I'm really happy that you've liked it!
Unfortunately I'm not an expert with AKS (and actually for most of cloud services), but I guess it can be done using official CLI tool for Azure.
I'm guessing ofc, but I would assume that from your local you need to login to Azure (using CLI), configure kubectl to be connected with AKS and you're good to go. Maybe official docs will help You - https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough
After that you should be able to run Helm from your PC.
And regarding keyvault, I'm afraid I don't know. Probably there is something in docs, maybe here - https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver ?
1 
1 
1
Java Software Developer, DevOps newbie, constant learner, podcast enthusiast.
"
https://medium.com/daniels-tech-world/how-to-save-gsuite-and-gmail-storage-space-by-archiving-old-emails-and-files-to-aws-s3-e562dacc99ac?source=search_post---------174,"There are currently no responses for this story.
Be the first to respond.
As well as taking backups of my desktop and cloud services, another really cool tech practice I’m into* is periodically cleaning up my inbox and cloud-hosted filesystem.
My inbox seems to be getting progressively busier and more cluttered these days, so I’m trying to up the frequency of these clean-outs to once every three months rather than my hitherto annual swoop.
I’ve just closed up a long project with a client.
So I thought that this was a good opportunity to demonstrate — and document — my current methodology.
Here’s how I do it.
*Sarcasm
The first step is to make sure that every unique project — or client you’re working with — has its own label in G Suite.
I’m pretty insane about this and have set up labels — and corresponding rules — for just about every conceivable purpose.
E.g.:
I a label for my utilities (“Bills”) and under that sub-labels for power, my cellphone plan, my water bill, and my healthcare provider.
I have a label for my IFTTT “it’s going to rain today” notifications and another for correspondence from my accountant.
So, if you’re also a freelancer / small business owner, set up labels for your clients. (I have mine sub-divided into current clients, past clients, and prospective clients).
Next, I create Gmail filters to automatically route inbound messages to the correct folder.
Generally, the easiest way to do this is to filter by sending address, but you can also:
HaGihon delivers water to my apartment in Jerusalem. And they send their e-bills from the domain printernet.co.il.
So, I create a filter to route all email from that address into the Gmail label I created for water bills.
The asterisks (*) symbol denotes a wildcard — so email from any other address at that domain will also route to the folder:
I typically also tick the “Never send it to spam” box — because I know it’s not spam and I need to see it — so I can “whitelist” the domain and prevent it getting caught up in my spam folder.
And I tick the “also apply filter to X matching messages” box in order to get all existing messages from that sender into the folder.
If this were a client, then I would also want to capture all the outbound (sent email) in the label I just created, again making sure that I apply a wildcard operator.
So I would create a filter in the opposite direction — just swap ‘me’ for your email:
Next, I set up label called ‘For Archiving’.
I’ve nested this under another label called ‘System’.
Let’s say I wrap up a project with a client.
I don’t really need or want to have all the back and forth with my point of contact cluttering up my inbox.
So I’ll archive that label in my S3 bucket.
For the purpose of this demo, I batched a few emails I don’t need under a label called ‘Demo Old Client’.
I then moved the label under ‘For Archiving’.
Now I want to initiate a Google Takeaway — but only capture the label that I want to archive.
Firstly, deselect all GSuite services other than Mail.
Click on ‘All Mail data included’.
And unselect “Include all messages in mail”.
(FYI: despite this, the UI will still automatically select some folders that you probably don’t want to delete. So be sure to deselect these if you want to keep them.)
Just tick the old clients that you want to archive and click OK:
Unless all these messages come with large attachments, email is pretty light. So the download link shouldn’t take more than a few seconds to generate.
The few folders I decided to export came to just 8 MB:
In AWS S3, I created a bucked called archivedemail.
After downloading the Takeaway, you’ll want to unzip the archive and navigate a few folders into the directory to find the actual label archives.
These end with the file extension .mbox.
And that’s basically it.
I then select Glacier as the storage class because it’s unlikely that I will ever need to access the correspondence again:
Now that the emails are safely archived, I can delete all the messages from the labels in G Suite.
And finally, I’ll delete the labels themselves:
I’ll repeat the same process with files.
I don’t use Google Drive as my main cloud storage system but this process should work the same if you use Drive File Stream to mount your Google storage to the local filesystem.
Again, I’ll push the file to an S3 bucket using AWS Console and set the storage class to something like Glacier.
Finally, I use a service called MultCloud.com to automatically sync files I don’t need to an S3 bucket.
In order to do this, I’ll attach my S3 bucket and cloud storage via Multcloud:
Then, I’ll create a folder in my primary cloud storage called S3_Autoarchiver:
And then I’ll create a sync between the “autoarchive” folder and my S3 bucket:
And then put this on a daily schedule:
AWS S3 is (almost) as affordable as object cloud storage gets.
At the time of writing, it’s just $0.023/GB for the first 50 TB of storage in the US East region with free data transfer in:
I’ve found periodically archiving all unneeded files and emails to it from Gsuite and my other cloud services a great way to keep within storage limits — and keep clutter and old projects from distracting me.
If you a main cloud storage system and have set up some storage space in AWS then you should give it a shot too!
Technically-minded business ghostwriter Daniel Rosehill…
I post about Israel, technology, and marketing communications—with liberal doses of opinion—as often as I can. If you'd like to receive those posts as notifications in your inbox, subscribe here. 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Written by
Daytime: tech-focused MarCom. Night-time: somewhat regular musings here. Or the other way round. Likes: Linux, tech, beer. https://www.danielrosehill.com
Technically-minded business ghostwriter Daniel Rosehill offers some how-tos, opinions, and general geekery. Particular interests: Linux, multimonitor computing, GPUs, cloud computing.
Written by
Daytime: tech-focused MarCom. Night-time: somewhat regular musings here. Or the other way round. Likes: Linux, tech, beer. https://www.danielrosehill.com
Technically-minded business ghostwriter Daniel Rosehill offers some how-tos, opinions, and general geekery. Particular interests: Linux, multimonitor computing, GPUs, cloud computing.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aioz-network/aioz-network-smart-caching-7e2f090173e4?source=search_post---------176,"There are currently no responses for this story.
Be the first to respond.
Edge networking is a sophisticated and dynamic computing architecture that brings cloud services closer to the end-user, boosts responsiveness, and decreases backhaul traffic. Edge networks’ primary dynamic aspects include user mobility, preferences, and content popularity. Temporal and social characteristics of the material, such as the number of views and likes, are used to assess the worldwide popularity of content. Such estimations, however, are not always successful in being mapped to an edge network with specific social and geographic features. Machine learning techniques are employed in next-generation edge networks to forecast content popularity based on user preferences, cluster users based on comparable content interests, and improve cache placement and replacement tactics. Our work is to examine machine learning techniques for edge network caching within the AIOZ Network itself.
Figure 1: A general form of network caching.
Cloud data centers and content delivery networks (CDN) provide back-end data storage required by mobile apps, which have low latency, mobility, energy efficiency, and high bandwidth. The latency between mobile users and geographically dispersed cloud data centers/CDNs is greatly influenced by distance. Many networking technologies have been proposed and implemented to bring processing and caching capabilities closer to end-users to deal with this problem. Mobile Edge Computing (MEC), fog computing, cloudlets, and information-centric networks are examples of these emerging technologies. Using in-network caching, edge networks have reduced the burden on backhaul networks while solving the problem of excessive content delay. Key research questions in caching in mobile networks include the popularity of multimedia, base stations, mobile users, and proactive and reactive to cache. Although the questions are straightforward, the solution for caching still needs to be optimized more.
To answer the question above, machine learning in wireless networks is used to estimate future user requests based on time-series dynamic mobility, popularity, and preference information. Given the limitations of user data available in edge networks, ML models that learn without prior knowledge are frequently used to optimize caching options.
Edge-based Intelligence for Edge Caching
Edge caching refers to a user-centric cache on the edge with limited storage capacity and stores material relevant to edge users. Because of the edge’s limited storage capacity and the dynamic features of the user (mobility, content access, etc.), the caching choice differs from CDN caching. In edge networks, Optimal Caching decisions are based on a variety of inputs which corresponding tasks that need to be optimized by learning, including:
Although mentioned tasks can also output caching decisions, we further leverage Deep Heuristic to anticipate the mentioned input characteristics to optimum caching issues through Edge-based Intelligence for Edge Caching.
Figure 2: Edge-based Intelligence for Edge Caching.
A high-level conceptual framework of ML-based edge caching is depicted in Figure 2. Social awareness, mobility patterns, and user preferences are fed into an ML framework, federated across multiple nodes, and stored at dispersed MEC servers, F-RAN, or CDN. The social networks, D2D communications, and V2I communications are used to provide the ML-based caching decision framework inputs. Intelligent and optimum decisions are sent back to caches handled by network virtualization methods. Caching placement strategies are being innovated with the aid of UAV-mounted caches that could also be directed towards user populations and real-time events. For more details, let’s examine three mentioned optimum tasks in the Smart Edge Caching System that play an essential role in giving out caching decisions.
Our system uses the cloud-based architecture (master node), which leverages supervised and deep learning in two phases to estimate video popularity. Then, the system automatically pushes the learned cache decisions to the BSs (slave nodes). First, a data collector module gathers information on the number of video IDs requested to make predictions of popularity and class of videos based on supervised learning. Deep learning algorithms are used to estimate future video content request numbers. See Figure 3 and Figure 4 for more details about the system model and the architecture of the master node.
Figure 3: System model of learning-based caching at the edge.
Figure 4: Master Node Design.
The Algorithm below conducts the training processes for the estimation model at the cloud data center to improve the prediction accuracy. This method takes as inputs the best model m* and a subset of raw data d[t=1 : t=j]. This method produces an optimal trained model for predicting popularity scores. First, the accuracy measurement metrics mtacc, mvacc, mpacc are set to zero. The learning rate lm and the regularization rate m are then assigned to fixed values of 0.001.
Alg. 2 depicts the caching procedure at the BS to optimize cache hit. This algorithm’s inputs include user requests, BS log files, and learned models from the cloud data center, which are utilized to forecast popularity scores. This algorithm’s result is a decision on whether to save the predicted popular content.
Caching for mobile users is vital since wearable high-tech devices are more and more popular. Thus, mobility prediction is essential for optimizing the cache hit of mobile devices. We utilize Temporal Dependency, Spatial Dependency, and Geographic Restriction to forecast user locations in mobile networks.
The mobility models depict the movement of mobile nodes and the changes in position, velocity, and acceleration over time. Based on the Temporal Dependency mobility model, prediction systems presume that mobile node trajectories may be restricted by physical properties such as acceleration, velocity, direction, and movement history. The estimation is based on the premise that mobile nodes tend to travel in a correlated way and that the mobility pattern of one node is influenced by the mobility pattern of other adjacent nodes.
For Geographic Restriction, node trajectories are affected by the environment, and geographic constraints restrict mobile node mobility. Similarly, buildings and other barriers may obstruct pedestrians. We employ a completely cloudified mobility prediction service that enables on-demand mobility prediction life-cycle management to address this issue. The mobility prediction method is based on the Dynamic Bayesian Network (DBN) model, and the rationale for adopting DBN is that its present position determines the next place visited by a user, (ii) the movement duration, and (iii) the day that user is in motion. Figure 5 illustrates how to relocate content for mobile users. The output relocation contents are considered to access cache hit.
Figure 5: Content relocation architecture for a cache hit.
As shown in Figure 6, we describe a system model of the F-RAN RAN’s slice instances. Customized RAN slice instances are deployed to enable K0 hotspot UEs and K1 V2I UEs. Set K0 to represent hotspot UEs and K1 to represent V2I UEs. High rates are required by the hotspot UEs in the hotspot slice instance. As a result, M0 remote radio units (RRUs) are spread across K0 UEs and linked to the cloud server through fronthaul. M1 F-APs with popular content stored is situated in specific zones to reduce the strain on fronthaul. F-APs communicate with the cloud server through the Xn interface. In the V2I slice instance, the F-AP mode and RRU mode are also accessible when V2I UEs require delay assured data transfer.
Figure 6: A system model of RAN slice instances in F-RANs, wherein the deep Q-network (DQN) is utilized to decide to perform the content caching in F-APs and mode selection for UEs.
Here, we have employed a deep reinforcement learning (DRL)-based approach, in which a DQN is built using historical data (visit the Algorithm below for the process of DRL).
The AIOZ Network is a Layer-1 Blockchain-based Content Delivery Network that is about to bring a revolution to the entertainment industry.
AIOZ Network utilizes Blockchain for better content distribution by leveraging the power of decentralization. Unlike the traditional data centers operating on a centralized model. distributed Content Delivery Network (dCDN) uses Nodes for storing, streaming, and transferring data.
The AIOZ Network uses a faster, cheaper, and more robust infrastructure for content streaming making it more affordable, faster. This allows the AIOZ Network to provide a higher quality service.
By using this revolutionary technology, the AIOZ Network can efficiently change the way the world streams content of all sorts, improving the quality of life of many and shaping the re-evolution of information and knowledge for future generations.
For recent updates and progress about the AIOZ Network, join our community here:
Homepage | Medium | Twitter | Telegram | E-mail
AIOZ Network is an Infrastructure Web3 Media Blockchain
50 
50 claps
50 
Written by

AIOZ Network is a Layer 1 Inter-Operable Multi-Chain Web3 Content Delivery Network
Written by

AIOZ Network is a Layer 1 Inter-Operable Multi-Chain Web3 Content Delivery Network
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/three-reasons-to-add-alibaba-cloud-to-your-multi-cloud-strategy-4d0885835a87?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 28, 2018·5 min read
From the minute you chose one of the world’s most popular cloud computing companies as your strategic cloud services provider, your world became multi-cloud. In a multi-cloud world, there’s now every reason to consider and embrace the relative newcomer, Alibaba Cloud. Not just as a specialist niche player to support business in Asia, for example, but as a full-service second-source supplier offering all the services, globally, you would come to expect from a leading cloud provider.
Multi-cloud is the use of two or more cloud computing services. A Multi-Cloud Strategy is the deliberate and considered choice of the partner providers and the mix of services commissioned from each. In the initial stages of cloud computing, many organizations looked to diversify their cloud vendors due to reliability concerns and multi-cloud was seen as a way to prevent data loss, downtime or vendor lock-in.
While these are still concerns, they are becoming lower priority. The modern multi-cloud deployment may now be driven by a business’s technical or strategic goals and different providers selected on their merits to support different parts of the business. This can include utilizing the most cost-effective cloud solutions over different periods, leveraging access speed and services offered by vendors or assisting large-scale deployments to different geographical regions.
The essence of success now is to pick your various cloud services providers each for their specific strengths. And Alibaba Cloud has at least three specific strengths which mark it out from the crowd:
• If you’re doing business with China or Asia, Alibaba Cloud has to be part of your cloud mix.• If you seek sheer scale or reliable computing power and performance, it has to be Alibaba Cloud.• If you are concerned about security — again, Alibaba Cloud may be your best choice.
Alibaba Cloud is the #1 public cloud vendor in China with China’s largest cloud network, comprising seven data centers in Mainland China and more than 1,000 CDN nodes connected by a multi-line BGP backbone network and multiple Availability Zones in each region. More than one-third of China’s top 500 companies, among them energy giant Sinopec, automaker Geely, and telecoms company China Unicom, are Alibaba Cloud customers, as are numerous government agencies like China Customs as well as the 2022 Beijing Olympics.
In addition to the deployment regions in China, Alibaba Cloud has two in the United States and one each in Hong Kong, Singapore, Sydney, Kuala Lumpur, Tokyo, Frankfurt and Dubai. As part of a multi-cloud strategy, an organization may choose to host in Virginia with one cloud vendor and in Frankfurt with Alibaba Cloud. This would ensure vendor separation and regional separation and still provide local high-speed delivery to clients in both regions.
Sister companies to Alibaba Cloud in the Alibaba group of companies can also help organizations set up and do business in China. The payments company Alipay , the logistics company Cainiao (which handled 812 million delivery orders in one day at the 2017 11–11 Global Shopping Festival) and the e-commerce company Tmall (which is also China’s largest third-party platform for brands and retailers), could all provide serious support to companies trading in the region.
These strengths are combining to make Alibaba Cloud a serious second-source cloud provider and an emerging alternative, if not the primary provider, in many regions in the world and for many applications.
The sheer scale of Alibaba Cloud’s operations is incredible. The world’s premier showcase e-commerce events each year must be the pre-Christmas shopping days known as Singles Day (or the 11–11 Global Shopping Festival to be exact), Black Friday and Cyber Monday. The first of these runs on Alibaba Cloud — yes, the entirety of the 11–11 Global Shopping Festival is run on platforms operated by Alibaba platforms — and in stark summary supports over USD $25BN of business in one day compared roughly to Black Friday’s $8BN and Cyber Monday’s $7BN. The value of mobile transactions on this one day is roughly ten times higher in China than it is in the U.S.
This is some scale, some resilience and some power.
In 2017, the 11–11 Global Shopping Festival saw 1.5 billion payment transactions, with Alibaba Cloud processing 325,000 orders per second at peak and the payments platform, Alipay, supporting 256,000 payment transactions per second at peak. All this was up some 40% on the previous year. That takes cloud computing horsepower, scale and security to a level that can definitely be described as world-leading.
So they shop, but Chinese people also travel. At the annual Spring Festival, another great national event where everyone, it feels, travels on the same few days, the Alibaba Cloud platform hosts the railway ticket site 12306.com and supports up to 40 billion page views a day leading up to peak travel time.
With this sheer scale of operation, you would expect Alibaba Cloud to focus on the highest levels of resilience and security to mark it out from all other cloud services providers — and it does. It offers a suite of purpose-built cloud products to protect against common attacks in China and safeguard sensitive data and online transactions. Anti-DDoS Basic enables you to mitigate attacks by routing traffic away from your infrastructure, including protection at the application and volumetric level. Anti-DDos Pro , Web Application Firewall (WAF) and Server Guard defend against massive DDoS attacks and isolate cloud networks to operate resources in a secure environment with VPC.
Sometimes companies are driven to multi-cloud by forces beyond their control. In late 2017 when the website of one Australian company, Mishi, was attacked and the company blackmailed to pay for restoring the site, Mishi turned to Alibaba Cloud for help. They deployed Alibaba Cloud WAF Pro and Enterprise versions as a countermeasure to repeat attacks.
Even with all this smart and secure technology, Mishi cited the personal service which provides 12/7 tech support, fast response times, and bilingual cloud solution architects as “the most compelling reason” behind their initial decision to choose Alibaba Cloud.
As a Singapore registered company, Alibaba Cloud complies with high-level international certifications to guarantee data security. Alibaba Cloud’s security products are trusted by over 40% of websites hosted in China, which includes the free Anti-DDoS Basic service, automatic snapshots, triplicated backups, and advanced services that have set records in DDoS attack protection. Alibaba Cloud is also the first cloud services provider to receive CSA STAR Certification and the first cloud services provider to be certified with the ISO27001 Information Security Management System Certification in China.
It’s worth noting too that in addition to scale in China and security, Alibaba Cloud provides website hosting and support with registration and legal compliance for organizations who want to trade online with and within China. It’s ICP compliance service, for example, provides a one-stop registration service through the Alibaba Cloud Management Console and personal support from the bilingual teams.
Getting started with Alibaba Cloud is as simple as setting up an account at www.alibabacloud.com. It’s free and you will get $300 credit to get a server up and running in minutes at no cost. From there, you can incorporate projects in to your multi-cloud world as you need them: if you’re doing business with China or Asia, if you need awesome scale or if you are concerned about security.
For related webinars check out https://resource.alibabacloud.com/webinar/index.htm
Reference:
https://www.alibabacloud.com/blog/Three-Reasons-to-Add-Alibaba-Cloud-to-Your-Multi-Cloud-Strategy_p558004?spm=a2c41.11339584.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@ewindisch/things-will-change-b469da5eb2b8?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Erica Windisch
Jul 18, 2017·23 min read
This article was originally written in the beginning of 2014, originating as a Keynote I gave at USENIX. I made some small edits toward the end of that year. It was never finished and there remain placeholders for graphics. Trends highlighted here have happened in some ways, such as the invention and increased popularity of GraphQL. Serverless technology is changing how we manage and use immutable infrastructure. Many lessons are yet to be learned, and I may yet publish a revised article! — Erica Windisch (July 2017)
Hello.
Today I come to you to speak of a future. A future of Change. Not just of Change Management, but of the world. Of the internet. Of our industry. I talk about changing not just the future, but managing the changes of things come.
TL;DR? This article is the long-form version of the following deck:
Today, you are probably Managing servers, VMs, and perhaps containers. You might also manage network infrastructure.
But whether you are managing physical machines, virtual ones, or both… if you are here today, you are probably using configuration management, or planning to use configuration management in your organization.
Today, primarily, these solutions look like:
<< Graph >>
If you are really savvy, machines boot into a set of preconfigured images. You might even use Chef-Zero, Chef-solo, or Masterless Puppet. Yet, they all largely follow the same paradigm.
<<img>
One of my problems with these tools is that they are overly complex. The fact that idempotency is a concern and a design tenant only highlights the legacy that is encapsulated into these products. It isn’t that idempotency is bad, necessarily, it is that it presumes we might EVER run something twice. In fact, a design tenant should be that we never allow running configuration management twice. That our provisioning is WORM — write once, read many. We use configuration management to build an artifact and we use that artifact to boot our images. That is, one way of expressing immutable infrastructure.
Are we managing configurations, or are we managing systems, and is there a difference? Systems configuration and management are indistinguishable.
The easiest way to enable WORM-based provisioning today is to use containers.
You’ve probably heard by now, but Containers are coming. It’s happening. It’s been happening for a long time. However, whereas docker is new, Containers are not. They have been around for well over a decade now.
But, what are containers? They are just a security construct. They are not a mythical box. They are not even virtualization which employs interesting hacks around CPU interrupts and, more recently, processor extensions. No, they are just processes on Linux with a sprinkling of security.
While the security version of the story sounds like virtualization, it isn’t. Neither is it virtualization, or is it really LIKE virtualization. Instead, it’s capability not unlike SeLinux and Apparmor.
This is why I find questions about the security of Docker and containers so interesting — because it is not a removal of any layer of the system, it is the addition of one. It is the addition of constraints on processes that were not there before.
However, what is disruptive is the newfound portability and transportability of those containers. What we have done is not to containerize processes on a host, but to enable self-contained application images to become portable between systems.
Portability and transportability are key valuesthat Docker Engine introduces to containers.
Of course, containers are NOT a replacement for configuration management. The fact that they simplify and enable WORM-based provisioning reduces or, in some cases, eliminates the need for other configuration management tools in the provisioning chain. However, there remains a need to manage change.
It’s not just about securing your processes in a container, although that is part of it, it is about portability. But it is not just about portability of the image, it is about the portability of the services that run within them.
Containers are a single part of a wider paradigm shift away from servers. When we view our applications as running on a platform, rather than running on servers, it’s not hard to see the writing on the wall:
The operating system is dead
Now, this might appear overly dramatic. However, look at the new containers-centric operating systems such as boot2docker, CoreOS, and Project Atomic — RedHat’s new operating system. We can bring up a kernel running Docker, and Docker runs self-contained applications that are built FROM your traditional distributions, but are not actually running those distributions. Instead, they build an image based on a distribution and run a single process from it. In the container micro-services model, the application runs in a container and only uses shared libraries, possibly uses its package for the application service itself, and less commonly utilizes the distribution’s binaries.
Two years ago, I said to a colleague: The kernel is dying.Now I fear the distribution is dying.
We are following a trend of thinning the fat. Two years ago, when I spoke of the kernel dying, I was referring to Linux in the sense that it was time for the kernel to Fast and tighten its belt. That with virtualization supporting a homogeny of hardware, our kernel would become thinner. And in a sense, this might still prove true…
But, what is currently happening, is that Linux as a kernel is, in way, becoming resurgent. Containers mean that the kernel is relevant again. The kernel is not just a place to manage hardware drivers, but a layer of protection for your applications. Without the kernel, applications would be protected by nothing but their hardware or VM — and would be absent the controls offered by SeLinux, AppArmour, and cGroups. Interpreted and bytecode languages such as Python, Ruby, and Java suddenly have advantages to running on Linux — whereas a couple years ago, the greatest friction to leaving Linux was the lack of alternatives and poor support for BSD on various clouds, we’re increasingly entering or reentering an age where the friction of leaving Linux will be based upon the capabilities of the kernel and the platform itself.
Containers technology finally leverages the things the kernel does well.
With changes in how people use Linux, with virtualization and containers, Linux will to change to these paradigms. While there will always be someone wanting their unique driver in the kernel, the number of drivers people will use in production and those that ship with Linux are likely to decrease. This will happen as more users only use Linux in virtualized environments, or with containers.
Honestly, my view of containers is probably a bit forward-leaning than most. I believe the pendulum will swing back toward virtualization again as we see containers and traditional virtualization as we saw with para-virtualization and hardware virtualization. That the pendulum may even swing a few times before we settle on what I’m calling Hybrid-Containerization. With Hybrid Containerization, various syscalls will have “constrained” versions that may operate with more constraints. We might even utilize techniques of virtualization.
There is, of course, the matter of orchestrated per-tenant VM clusters which run clouds of containers — such as Google does.
Regardless, the point is that micro-services means compartmentalization, and with containers means small distributions and a smaller attack surface per-service (as to whether or not it reduces the total attack surface is a matter for another debate)… but it certainly means smaller pieces.
It makes sense, really. Because micro-services are a misnomer. They are commoditized processes, just as virtual machines have commoditized hosts.
The great thing about commodities is that they become Just Another Thing.
Its close alignment to the Internet of Things isthe disruptive element of the micro-services movement.
You’ve probably heard by now, The Internet of Things is upon us. It is happening, but it has been happening for a only a short time now. It is happening, but the builders of this new world come in many shapes and sizes. Embedded software is notoriously bad, it is buggy, hard to manage, support, and update.
But… Suddenly, we want to connect these devices. We’ve already been connecting these devices in the form of PDUs, serial consoles, switches, and routers. And we have done a terrible job at it. Certainly, there are variants of these devices that are great, but it is the exception. Yes, I include Cisco and Juniper in the “exceptions”. Other vendors? Especially the low-cost options? Anyone that has ever used a “web-managed” switch will tell you that they are not very good.
Those problems often come from the costs and constraints of the hardware, limited expertise in writing such software, a frequent necessity to write larger portions of low-level stack which most higher-level systems folks on servers haven’t had to write since the 70’s and 80’s, since BSD sockets became a thing, or in the 90’s at the latest — since WinSock became a thing.
It’s no surprise that embedded network programming is usually bad, if most developers are working, not only with 80’s hardware, but with developer libraries of a similar vintage — as far as networking is concerned.
Undoubtedly, the best solutions prove to be the ones built on top of more capable hardware that can leverage modern software libraries — and most of that runs Linux, either via a small customized bootstrap userspace, or with a distribution such as Android.
Furthermore, even for those devices that ARE good integrate poorly with our configuration management tools. If we cannot manage the devices we already know and use, how will we manage the devices to come?
First, Linux will grow around supporting these new platforms.
For years, we’ve been hearing about the Cloud, about Cloud Computing, as well as “cloud this and that”.
What is the Cloud? It is not centralization, but rather automation of device management. Unfortunately, the term has been overloaded to expressing any Service, rather than the unique automation of physical and virtual devices.
If we accept that the term has changed — and I think most now accept this, that the Cloud basically means: It’s on the internet.
However, what we don’t have is the hyper-connected, federated cloud of our devices. This is the new internet paradigm. The one of connected services and automation.
This is what we must build — and what it seems what is to come — is a new cloud. That cloud will be…
So MQTT can’t change things well, but…
State changes. We like to pretend it doesn’t. The concept of immutable infrastructure is terribly broken at worst, and a misnomer at best. It’s true that Docker images, for instance, are WORM media. However, the containers themselves operate on a Read/Write copy. However, even if the disk were Read-only, the memory would still be mutable. That is to say, there would still be state.
There is no such thing as stateless.
Immutable infrastructure doesn’t kill the Chaos Monkey.
The reality is that while little in the universe is static, we like to treat things as constants. We want immutable infrastructure. To create images and disregard their changes.
However, we can not prevent state from changing.We cannot kill the Chaos Monkey.
Often infrastructure fails and we do not want to tear it down, we not only wish to preserve it: but we must. For years, I’ve been building systems with disposable units of compute. However, it is naive to think we can simply throw away VMs or containers — we want to preserve their state for archival and analysis. In fact, this is one of the properties of VMs which make them yet better than containers — the ability to easily and affordably snapshot memory as an artifact.
Not to say we cannot do that with containers, but memory snapshots and suspend to disk are more mature with virtualization technologies than they are for Linux processes.
If anyone is still questioning the value of memory and disk artifacts from your “stateless” containers:
<<Security incident management>>
Think about a system where your response to a security incident is to shutdown your hosts, VMs, or containers? With a fully and truly immutable infrastructure where we presume there is no local state ever worth saving, you’d have discarded your application memory and possibly even your disk contents that will contain invaluable information your incident investigation. Thus, state is an essential part of your security story and absolutely stateless infrastructure cannot be part of any serious security story.
The implicit state of a system should not be deemed disposable.
I should note that one need not abandon “immutable infrastructure” or 12-factor, only that they are models that may not be complete and may need to be used as a basis for your infrastructure strategy, not -AS- your infrastructure strategy.
Because…
Because everything has State, everything will change. That means your servers, your VMs, containers, and coffee machines. In fact, my espresso machine has a PID, meaning it has a temperature probe connected to a feedback loop that controls a relay to regulate temperature. The machine is constantly changing temperature, something it measures on an interval. That, of course, is State.
The Internet of Things is more than just about connecting and controlling Things. It’s about sensors and the artifacts of reading those sensors. Those sensors are detecting external change and influence. They are measuring input. That is the state of those things, of those sensors. It changes, will always change, and is in fact, the whole purpose for its existence.
We measure change and respond with effect to affect the state transition we desire.
Sometimes the State that changes has nothing to do with your Thing,but is an artifact of knowledge.
We cannot control change, we can only effect it and manage it.
Like other Things, Micro-services will expose an API and in that API, change will happen. The state of that service will change. An API request changes a field in a database. A user logs in and a token is created, that is state change. A log file is written… etc
This is different than the implicit change, yes? It is the explicit change that happens in a stateful service.
What’s amazing is how well Micro-services map to physical Things. One takes input from sensors, and the other takes input from an API… each time, of course, we have change.
APIs and sensors, both, are inputs.
Really, the biggest difference between micro-services and physical things is that those services and those containers — or VMs — is that they are entirely virtual and may run on a larger physical thing. Just as a physical thing might have multiple sensors…
Things will change.
Because Containers are Things, they too will change. Bitrot happens. Disks fail to make writes. Memory becomes corrupted. Out-of-memory killers run. Services are attacked and compromised.
We need to manage containers like we manage Things. Solutions that do not do this are not good long-term solution for managing containers.
What I speak of has theorems and proofs to back it up.
Applying the CAP theorem, or Brewer’s Theorem as it is otherwise known, we can state this as the inability of a distributed system to provide the high availability and partition tolerance implied by immutable infrastructure and 12-factor design while also providing the consistency necessary for reliable state management in a fully disposable manner.
Purist immutable infrastructure and 12-factor design dictate stateless design. As we know, true stateless doesn’t exist. Now, the CAP theorem basically means that we cannot simply rely on logging, consensus algorithms, and other distributed computing solutions to kill the Chaos Monkey.
What it means is that we must accept we might lose state. In fact, what it means is that we’re guaranteed to lose state. Important state such as who stole credit card numbers from your database, or took off with a copy of your private key.
We must accept this.
Because we cannot kill the Chaos Monkey, we need to know how to collect its droppings. 12-factor and immutable infrastructure fail to acknowledge this.
You need to Manage Change, not prevent it.
Lets not fool ourselves. These tools are designed for the old paradigm. Not to say they are not useful. However, the companies that have designed and built these tools have done so with the perspective of providing or supporting centralized infrastructure for managing provisioning.
They are not designed for speed. They are not designed for micro-services. They are not designed for Things. They are designed for creating change, not for capturing it.
Some of the companies, or at least some of the engineers at those companies, have recognized the disruption. I’ve seen amazing effort from Chef and Saltstack, in particular, toward adapting either their thoughts, if not their processes, to the disruption brought by the Internet of Things and containers.
However, to be honest, I don’t believe those companies necessarily need to evolve. They do something well and it might be good enough to leave well enough alone. They might become more niche, but they have a solid market base.
My fear is less that they’ll fail to evolve,but that they’ll lose their identities in the process.
In fact, some of these tools are great at tasks such as scheduling and lifecycle management, even if we disregard their value for managing configuration.
Furthermore, and I’ll use Chef as an example as I know it best of all these solutions — Chef defines resources, which are simply objects, and work well as abstract units that map cleanly to the thought of managing Things.
Technically, for instance, one could today use Chef as a cloud orchestration solution —in fact, I understand this was the initial scheduler solution for the Deis project.
Regardless, this talk is less about the state of current tools and more about the tools we need tomorrow and the fact that the tools we have today don’t yet solve tomorrow’s problems.
I expect new competition to flourish around greenfield solutions for solving tomorrow’s configuration management problems.
The old guard configured the interface of Linux filesystems and processes, but the new guard will configure the interface of APIs.
New solutions will be to configure Things through their own APIs, rather than through custom agents.
The best thing is that this will work on all Things, all devices, and across Operating Systems — even Windows.
Unfortunately, our current tools don’t solve these problems. We can no longer abide configuration management tools that use specialized agents.
To change the things of tomorrow, we’ll use their APIs.
OpenStack Heat is an interesting example of something that Changes Things today. It defines Resources, each which may be considered as a Thing. Each is managed independently and orchestrated together using a Heat Orchestration Template (HOT).
First, it is a good example of how compartmentalization provides flexibility, but it also highlights the complexity of managing many pieces. To solve this, Heat has what it calls Provider Templates. These allow creating what are effectively recipes which aggregate management of multiple things into a single resource — or Thing.
The worst thing I have to say about Heat is that OpenStack as a whole tends to be overly insular, making the adhoc use of components such as Heat, an uncommon exercise. Still, Heat supports standalone installation.
Last week, I spoke with Jesse Robbins, founder and former CEO of OpsCode — now Chef, about the Chef Server implementation. Now, I haven’t confirmed it, but my understanding from our conversation was that Chef explicitly forms a graph relationship with its nodes…
In some ways, Chef might already offer many of the right things for the next generation, if only at a relatively local, non-global scale.
Beyond that, Chef also changes Things. It defines Ruby objects that map to Resources, where each resource is — effectively, a Thing. Now, the hierarchical model is different than we might like for the Internet of Things context, but the actual recipe pattern would be surprisingly aligned should that it be mapped not to local resources, but distributed ones.
Yet, the next generation of web technologies will make aspects of Chef server, Puppet Server, Ansible and others obsolete.
Just as Hypertext provided an implicit graph, linking and building relationships between websites — our next generation of web technologies will offer an explicit graph to provide discovery and inventory.
If we do it right, discovery and inventory of services — of things — will be a built-in feature of that next generation technology. It will be here by default, rather than the exception to the rule.
Application-specific discovery and inventory mechanisms won’t go away,no more than Gopher has.
Remember Gopher? It was pretty cool… until hypertext killed it.
Granted, it will only have an eventually consistent view of the universe, but that’s already true of all configuration management systems today. It’s true of the World Wide Web, and according to Brewer, true of all distributed, highly-available, and fault-tolerant systems — any such system that we can build with global reach.
In the world where everything is connected, nothing is connected. When we saw computers coming, we built a hyper-connected world-wide-web. For those that remember it, hyper is a word that now feels so quaintly 90’s, or worse, archaic. However, it’s probably the right word for what we need today:
We cannot afford to build networks of things that look like Service-oriented-architectures. There are aspects of SoA that make sense in a hyper-connected world. Certainly, the World-Wide-Web terminates at webpages which are no longer static, but backed by N-tier applications.
I’m not saying we have no need for architecture in the World of Things, but that MQTT is not enough.
The MQ Telemetry Transport is a lightweight pub/sub protocol. I have nothing bad to say about MQTT, except to say that it’s incomplete. That’s probably a good thing.
MQTT does some things great. It provides a way of publishing sensor data to many clients. It provides a buffer between slow devices and fast clients. In most deployment models, it breaks out of NAT since we, as an industry, have failed to adopt IPv6.
What it does not do is provide command, control, or configuration. It’s web-pull, not web-control. Change and control mechanisms can be built on MQTT, but it’s just not good at it.
MQTT solves important problems related to accessing sensor data,but not all the problems of accessing and controlling things
If MQTT is our HTML, where is our HTTP? MQTT doesn’t provide a sufficient analogue to REST
The REST pattern has been important to building modern web APIs. However, those APIs are failing us.
REST implies that our services are resource-centric and we use a proper set of verbs for information retrieval and performing changes.
But unfortunately, the primary issue with REST is that it’s not a protocol. It is at best a guideline. That’s not strict enough for building a hyper-connected web.
Solutions such as WADL, Swagger, and API Blueprint seek to solve the deficiencies of REST. Yet, it’s not yet clear if they’re good enough and adoption remains key.
Finally, REST also implies idempotency.
Idempotency as a requirement for REST is a challenge for many services of the Internet of Things. Yet, we cannot necessarily presume machines can process requests fast enough to repeat them, nor do they have the memory to provide sufficient state modeling.
Protocols such as MQTT provide value as a buffer to support idempotency for REST access to Things, but this works better for retrieving data, rather than creating or updating it.
No, we need hyper-connected APIs because…
My favorite Cloud is Uber. First, it makes Cloud really easy to explain to layman, but it’s also just really cool. Uber has a cloud of vehicles. Each vehicle provides a service, managed by a driver. Users of the Uber cloud request resources and Uber schedules a resource to the user, provided by a service, delivered by a driver.
Long term, will services such as Uber and Lyft be powered by autonomous vehicles? Their cloud uses Things to orchestrate people, to provide a service. Tomorrow, it’s likely they’ll be orchestrating things from end-to-end.
To build these next generation services we need first class events. MQTT does this well. We also need first-class command and control. That’s what HTTP already does well. Sure, unlike MQTT, it’s heavy… but change is a heavy process. It’s the sort of thing we only need to do on capable devices, not on your smallest microcontrollers. But there are other protocols as well, such as COAP — a lightweight protocol that translates to HTTP.
My intention here isn’t to mandate which protocols we use,but to raise awareness for the need of such protocols and their adoption.
Protocols such as WebSockets and SPDY / HTTP2 provide multiplexing of multiple channels over a singular TCP connection. In a sense, we might expect these to be a sort of new document-type, or proto-type, if you prefer.
MQTT itself is a protocol, not a document-type… but if it doesn’t do all the things we need, we either need a different protocol, or multiple.
We can either run multiple protocols over different ports, which would be horribly complex — not to mention painful, or we can decide on a single wrapper protocol.
This will allow us to provide native HTTP, COAP, or other protocols alongside others.
I’m sure, of course, everyone has seen this…
Every time you create a new standard to solve a problem, you have two problems
I can’t say what the standards will be. MQTT might not the right solution at all. Better REST might not be a solution. WebSockets, SPDY? I don’t know.
What I do know is that we have problems that are not addressed by the current protocols and current solutions.
To accomplish the goals we are seeking, we need to Upgrade The Internet. That might read a bit bombastic…
But we want to do more. We want devices to connect. We want those devices to know not just who to speak to, but how to speak to others.
/Tim Burners Lee/ — “Google could be superseded by the Semantic Web”
Tim Burners Lee coined the term ‘semantic web’ which he defines as “a web of data that can be processed directly and indirectly by machines”.
The time of the Semantic web has come. In fact, we need more than a simple semantic web: We need semantic apis as well.
What is semantic? — Semantics is the study of meaning. It focuses on the relation between signifiers, like words, phrases, signs, and symbols, and what they stand for, their denotation.
Recognizing that what we want is a Semantic Web gives us context for researching and understanding the efforts that have come before.
Note that I just said, “a” semantic web. Lets not commit to any protocols or document-types just yet!
Sometimes called the Web 3.0, the Semantic web seems to be the thing always on the horizon — we’re always on the cusp of it, but we can never achieve it. Companies live and die by its dream.
Initial efforts to build a semantic web created the Resource Descripition Framework — RDF, the Web Ontology Language — OWL, and of course was coupled highly to XML.
In an industry that has gained distain for XML in favor of more manageable formats such as JSON and YAML, to the degree that even the W3C effectively redacted on XHTML with the publishing of HTML 5, added to the fact these protocols have failed to gain parlance in the common vernacular of web developers… you can imagine that
The RDF and OWL efforts have been near complete failures.
Of course, there are also tons of articles on how the Semantic web has failed, why you shouldn’t use it — in its current form, etc.
That isn’t to say the concept and model isn’t right. It was simply too early. The web wasn’t ready. We needed web 2.0 before building web 3.0.
The Semantic Web should not just express the context of things, but provide discoverability.
In fact, it’s been argued that DNS already solves these problems.
Yet, it’s flawed. Critically flawed.
DNS is hierarchical, which is okay, but queries are also hierarchical, with queries for record types (I.e. relationships). It’s designed for a small number of relationships per node with responses returned via a single packet. Yes, one single packet. The EDNS RFC with TCP packets allows arbitrarily long DNS responses, as long as they can fit inside a packet, anyway.
Of course, large UDP packets are not likely to get to their destinations and large TCP packets will fragment. Beyond that, an amazingly few number of engineers seem to remember that TCP packets are limited to 64 kilobytes. That’s 43 fragments for each packet that has to be successfully routed.
Yet maybe 64K ought to be big enough for anybody. Maybe not. Even if we agreed to work within these constraints, 64 kilobytes might just be enough — as long as we are comfortable making trade-offs and hacks.
But do we really want to make those hacks? I say not.
Instead, we probably need new, smarter protocols.
There are a number of individuals and vendors alike attempting to find solutions to bridge the gap between today’s protocols and tooling and tomorrow’s world. Most are seeking to make it easier to build, support , and manage applications. Few of them appear to be seeking to building a global, hyper-connected web.
One of the few efforts is next week’s W3C Workshop on the Web of Things.
Others are now working on a project called libswarm, a project which originated at Docker. Libswarm and swarmd seeks to solve the problems of providing service connectivity. It has a design such that it can be backed by services such as MQTT. Where you might have used RabbitMQ before, you would now use MQTT, and where you once used HTTP, you’ll use Swarm. That is, Swarm isn’t the solution, it’s a lego-piece that helps build a wider solution, just as TCP and HTTP are lego pieces of the Web today.
Swarm builds on libchan, which builds on top of existingprotocols such as SPDY and HTTP2, or WebSockets.
Swarm is not semantic, not yet, anyway. However, it can provide an interface to semantic APIs. It’s HTTP to your Hypertext. The problem is that while MQTT is semantic, it isn’t explicitly hyper-connected.
Of course, we — the community — are working on it.
Again, this isn’t to say Swarm must be the future. It’s too early to pick winners, and it’s not about winning the battles over protocols — it’s about winning together.
The key point here is collaboration and agreement on goals.
So far, most or all of the other projects and efforts appear to be silos. They are not hyper-connected, they are not semantic. They are just brokers for connecting devices with no apparent aspirations beyond those humble beginnings.
Regardless, it’s not a competition. The point isn’t to make any single solution win, but to find A single canonical solution, just as we did with TCP, HTTP, and HTML.
At worst, it might be disruptive to companies building products around silo-based solutions, but it’s not competitive — it’s evolutionary.
Needs to be about all Things, not all Devices. Where all things are services and all services are Things. Many of those services will be hosted by the infrastructure and servers we know and love today.
We need new services and configuration management solutions that work across devices, clouds, and containers.
And we need to bet on free, open, and semantic hyper-connected protocols.
Today, we are changing our servers, but tomorrow — We want to change the world.
Thank you.
CTO and founder of IOpipe, Inc. working on Application Operations tools for serverless applications.
See all (349)
1
1
CTO and founder of IOpipe, Inc. working on Application Operations tools for serverless applications.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/g-cloud-latest/uk-governments-contingent-labour-one-framework-d084c9b7cc31?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
There are many barriers within the public sector to consuming cloud services via G-Cloud / Digital Marketplace, but we will take a look at the Crown Commercial Service and the Contingent Labour One framework.
There are many barriers within the public sector to consuming cloud services via G-Cloud / CloudStore, but we will start with the Crown Commercial Service.
As we can see from the changes in the people from CCS working on G-Cloud, it appears that CCS have a policy of rotating staff in/out of the G-Cloud team, which I support. This allows CCS to leverage the innovation from G-Cloud and apply it elsewhere. The only problem with this approach is that we are seeing the initial principles of G-Cloud being diluted as passes from staff to staff.
The Crown Commercial Service (CCS http://ccs.cabinetoffice.gov.uk/) have published a policy that restricts Central Government to a limit of £100K when procuring ICT consultancy via the G-Cloud frameworks, details of the policy can be found here.
Now, this is a poor document at best, as it’s not branded and displays a bias towards the larger suppliers on other frameworks.
CCS define temporary staff as follows:
The provision of workers to cover business-as-usual or service delivery activities within an organisation. Temporary Staff are also often referred to as “Contingent Labour”.
Specialists are normally middle to senior grades, used to provide expertise that is not available in-house, fulfilling functional or senior positions within the organisational structure and ideally engaged on a short term basis.
CCS define consultancy as follows:
The provision to management of objective advice relating to strategy, structure, management or operations of an organisation, in pursuit of its purposes and objectives. Such advice will be provided outside the ‘business-as-usual’ environment when in-house skills are not available and will be time-limited. Consultancy may include the identification of options with recommendations, or assistance with (but not the delivery of) the implementation of solutions.
CCS define ICT consultancy as follows:
The provision of objective IT/IS advice including that relating to IT/ IS systems and concepts, strategic IT/IS studies and development of specific IT/IS projects. Advice related to defining information needs, computer feasibility studies, making computer hardware evaluations and to e-business should also be included.
It’s clear from the CCS definitions that the £100K limit is a restriction on IT Consultancy and NOT the Specialist Cloud Services within Lot 4. This should be explained clearly within the CCS documentation.
The framework is split into three lots;
Lot 1 — Neutral Vendor
“This service provider manages a dynamic market supply chain and competes every medium to high value requirement (i.e. interim managers and specialist contractors) within the supply chain.”
As far as I can tell, many requirements for Cloud/ICT consultancy are being offered via Lot 1 on this framework, which is not surprising given the muddled advice from CCS. The only certain choice in the flowchart is to use Consultancy Labour One.
The only vendor selected to supply services under Lot 1 is Capita Business Services Ltd
There is no documentation available from CCS on how Capita “competes” every requirement.
Contingent labour is classed as one of the central commodities which (i) Commercial Heads from each Department have signed up to use (inclusive of their ALBs), (ii) Commercial Heads have signed off the Contingent Labour Strategy and if departments chose to go outside of this framework they will need Professional Services Procurement Board (PSPB) approval to do so.
Some of the benefits of the Contingent Labour One framework are;
The Contingent Labour One framework has no supporting business case, as documented in an FOI request here.
To provide services through the Contingent One Labour framework the worker (they’re not a Consultant because they are delivering something) is required to provide the following information:
If you contact CCS as a public sector buyer, you will receive confusing advice on if/when you can use G-Cloud to procure ICT consultancy, but this is not surprising based on their own documented guidance.
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
Written by
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
Everything you need to know about G-Cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pvalerio/mwc-2021-big-gamble-479cf1a5bf4c?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pablo Valerio
Mar 13, 2021·3 min read
Mobile World Congress Barcelona is the largest professional technology show in the world. Every year, in normal circumstances, over 100,000 people come to the show to see new technologies, attend keynotes from the top leaders in the ecosystems, and, especially, do business.
That’s why premium space at the Fira Barcelona halls is hard to find. Many companies have the exact location for their booths and pavilions reserved year after year.
Big exhibitors such as Huawei, Samsung, Ericsson, and Nokia, among others, have large pavilions in the first three halls of the Fira Gran Via complex. Many of them have several booths in different locations. Additionally, companies such as Sony, Dell-VMware, Intel, Deutsche Telekom, Telefonica, ZTE, and others have large pavilions at Hall 3, one of the most visited during the event.
Unless something better becomes available, keeping your location is critical for all those companies to prepare for the following year and make sure they get the best return at the show.
For many tech journalists covering the show, including me, the first thing we have in our agendas for MWC is Ericsson’s press conference on Monday morning. The conference, held at 8 am at their 65,000 ft² pavilion, has become a tradition for most of us.
Obviously, that didn’t happen in 2020, as the pandemic finally scrapped MWC. Last year, after the cancellation, the GSMA promised exhibitors to keep their location for this year’s conference and a substantial discount instead of refunds.
Earlier this week, the Swedish infrastructure giant was the first to announce its withdrawal from MWC 2021.
Right after Ericsson's announcement, TelcoDR CEO Danielle Royston posted on LinkedIn that she would be “happy to take over Ericsson’s stand space in Hall 2.” She didn’t have to wait much for the GSMA to answer. Also, on LinkedIn, they immediately said: “Let’s do it.”
Within three days, the GSMA sold the space originally reserved for the Ericsson pavilion to TelcoDR, a cloud services consultancy looking to launch internationally at the show.
Next year, when Mobile World Congress goes back to its usual dates and full mobility would be restored, Ericsson might have to find another location at the show.
Yesterday, the GSMA announced TelcoDR “will create a stand exclusively focused on ways to leverage the public cloud in telecom”. Having one of the biggest pavilions at MWC, Royston thinks she can even put a tennis court in the space:
The move, which the GSMA has praised as a successful addition to MWC, also serves as a warning to other companies thinking of withdrawing from this year’s conference.
“Public cloud is crucial to the future of telecommunications and will enable operators to transform their networks, IT workloads and processes to create massive efficiencies, innovate and become much more profitable,” Royston says.
Other companies such as Nokia, Sony, and Oracle, who already announced their withdrawal of this year’s show, could face the same deed and not have a spot for 2022. Many other European tech companies are thinking of skipping the conference this year, and now they might think twice about it.
Let’s see how this gamble pans out for the GSMA and the Mobile World Congress. Still 14 weeks to the official start of the show. Hopefully, for the GSMA, Barcelona, and the entire communications market, we can at least have a glimpse of normality this year.
Tech journalist, engineer. Based in Catalonia, covers international tech events, and Smart Cities. https://citiesofthefuture.eu and http://iot.eetimes.com
See all (321)
Tech journalist, engineer. Based in Catalonia, covers international tech events, and Smart Cities. https://citiesofthefuture.eu and http://iot.eetimes.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@afhill/calculating-roi-return-on-integration-eebb39a9bf0e?source=search_post---------182,"Sign in
There are currently no responses for this story.
Be the first to respond.
Andrea F Hill
Dec 28, 2016·2 min read
Cloud services and open APIs are the darlings of the software world. A company can focus on a key service or solution and pull in complementary services from across the web. No need to try to displace an incumbent cloud storage platform when all you really care about is {insert-your-value-proposition-here} .
End-users have caught on too; expecting their services to talk to each other, even if they have to manually connect them via Zapier or IFTTT.
But if you’re a software provider, you have a plethora of possible integrations to enable. How do you determine the value of offering an integration?
Consider first what you’d hope to achieve with a successful integration. Integrations can serve as a tactic to help with many of Dave McClure’s Pirate Metrics (Acquisition, Activation, Retention, Revenue, Referral). So before you throw your developers at a technical problem, be clear on which business objective you’re trying to meet.
Although this effect is fast diminishing as app stores explode, creating an app and launching it in a marketplace can be considered a form of technology-driven marketing. If an existing Slack user searches for time-tracking and you happen to have an integration, their barrier to trial is lowered as you already integrate with their workflow.
Works great if: your product is self-explanatory: easy to try and recognize the value of.
Not great if: your product requires a lot of configuration, or the user is not the same person as the buyer.
Again: by creating an integration within frequently used tools, you can help keep your product top-of-mind and “in-context”.
Many customer engagement tools like MixPanel and Intercom shine when the product is using your product, and they fall back to email, as an out of context contact mechanism. If your target custoemr spends his time in another workflow tool, you want to be there too.. This could take the form of onboarding via a conversational agent.
Integrations can also be a way to capture value from your customer, as it helps them self-segment.
A few years ago, I spoke with someone who was working on a platform that integrated with Evernote Premium. His target customer was someone who already already committed to pay for Evernote, to be a power user. He recognized that willingness to pay for one productivity app rendered the customer more likely to use and see the value in his app as well.
Another example is having premium integrations (Salesforce, for example) only available at higher pricing tiers.
Introducing integrations may seem obvious, but the costs add up. Be clear on your goals for the project, so you can weigh the costs against the possible returns.
Sr UX Specialist with Canada Revenue Agency, former web dev and product person. 🔎 Lifelong learner. Unapologetic introvert. Plant-powered marathoner. Cat mom.
See all (819)
Sr UX Specialist with Canada Revenue Agency, former web dev and product person. 🔎 Lifelong learner. Unapologetic introvert. Plant-powered marathoner. Cat mom.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@giuliano/how-egnyte-integration-strategy-is-serving-low-code-customers-f4720ee3bed0?source=search_post---------183,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giuliano Iacobelli
Dec 13, 2017·2 min read
With the adoption of cloud services in every department, teams across companies need to work much more quickly, access more information than ever before and make data-driven decisions in real-time.
IT can’t keep up with the demand and applications made by citizen developers are more likely to meet exact business goals. There’s no better person to create and customize these applications than a line of business owner who knows exactly how the solution will be used.
Egnyte, leading content collaboration solution, is very aware of this and that’s why they have been partnering with industry leading Integration Platform as-a-Service (IPaaS) solutions to better serve their “low or no-code” customers.
“Integrations are no longer isolated to software engineers tucked away in a dark corner of the office writing custom code. Solutions like Stamplay allow users to easily connect and automate common business processes without the requisite computer science degree.” — Tom Bingham, Partnerships, Egnyte.
At Stamplay we’re very happy to be partnering with a company that has clear ideas and roadmap for integrations. To learn more about how Egnyte’s strategy is making easy for software vendors and customers alike to integrate with their content collaboration solution you can read the full story here.
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
"
https://medium.com/@wtfmitchel/as-someone-that-helped-build-exchange-online-i-can-vouch-for-what-you-said-with-regard-to-81c7df0c2b48?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mitchel Lewis
Oct 7, 2017·1 min read
As someone that helped build Exchange Online, I can vouch for what you said with regard to businesses reducing their costs by moving to cloud solutions. But you should realize that Microsoft wasn’t the first solution provider to offer hosted Exchange solutions (for example). With Office 365 and Azure, Microsoft was forced to go the route that they did for risk of losing significant amounts of business to Amazon, Google, Salesforce, and the like, but they did not lead the way.
Another thing to consider is that Microsoft had to put heavy incentives behind their cloud services in order to get their partners on board with them and not competing cloud services such as the ones mentioned earlier. Microsoft also brags to their customers about their products resulting on roughly 30% more revenue for services offered by their cloud partners in comparison to their competition.
Engineer, Farmer, and Hellion
Engineer, Farmer, and Hellion
"
https://medium.com/@namify/join-radix-at-the-biggest-cloud-festival-df58eb2a4aff?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
www.namify.tech
Mar 8, 2018·2 min read
Regarded as the foremost global event for cloud services and internet infrastructure, WHD.global is the mecca for the entire Web Hosting community. This year, WHD.global has undergone a serious branding makeover and, in its brand-sparkling-new avatar, is now called CloudFest! The scintillating tech festival is slated to be held in Rust, Germany from March 10th to 16th, will be hosted at Europa Park, one of the largest theme parks in Germany.
Last year, over 7000 attendees experienced this 7-day carnival of online creativity and technology only to get inspired, and fuelled with new insights and knowledge, and with new connections and collaborations. Tech enthusiasts, visionaries, evangelists, and the who’s who of the cloud ecosystem and hosting industry will be in presence at this annual event.
The new branding is simply a clearer indication that this year will pack an even stronger punch. Previous editions of this one-of-a-kind hosting event have always proven to be a fun and energetic experience; one of the primary reasons why we are enthusiastically looking forward to attending it this year!
Radix is geared up to participate at CloudFest 2018 and experience, for the 5th year in a row, this grand festival of tech innovation. Our presence will be amplified through 5 of our registrar partners and some quirky branding.
Here’s a sneak peek into what you will see at CloudFest from Radix:
From Radix, Sandeep Ramchandani (CEO), Neha Naik (Director, Channel Partnership), Varun Punjabi (Sr. Account Strategist), and Joel Rasquinha (Manager, Strategic Partnership) will be available throughout the exhibition days. If you are around, do go over and say hello to them!
See you at CloudFest 2018.
Namify is a brand builder for passionate people in a hurry to get their ideas floating on the Internet.
See all (123)
Namify is a brand builder for passionate people in a hurry to get their ideas floating on the Internet.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/aws-re-invent-2017-quick-re-cap-8818a7ce8088?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Dec 5, 2017·4 min read
This year, AWS re:Invent witnessed the launch of diverse new wave cloud services and products. Vijay Rayapati, the CEO of Botmetric experienced the event firsthand and we thought it was only appropriate that readers travel through this journey with him. If you’ve missed re:Invent this year, don’t worry, here’s your chance to catch up with all the action and big announcements that happened.
These new key services can be a turning point in the cloud world. Experiment to see what suits your needs and makes cloud management easier. Botmetric is in the initial steps to integrate these services into its platform. “We want to implement as many services as we can to make our customers cloud management journey as easy as possible” Vijay concludes. While Botmetric rocked at booth 329 with Alexa magic and the release of public cloud report we are in a constant endeavor to improve our customer’s experience on cloud as smooth and easy as possible.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@krmarko/public-or-private-multi-cloud-is-the-future-how-will-you-manage-it-794d1714b98b?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Nov 6, 2015·2 min read
Any conversation about cloud services usually begins with AWS, but for most organizations, it won’t end there. Whether to fight vendor lock-in, increase the diversity of available services, arbitrage price disparities or maintain control over particularly sensitive information an increasing number are adopting multi-cloud strategies that include both public and private components. As I detail here, although it’s a sound strategy, they quickly run into another problem: managing applications and infrastructure configurations across cloud stacks that don’t share a common API and have very different service definitions and billing models. It’s a seemingly complex task, but hardly a showstopper, with a number of mature software and SaaS options available to automate deployments across a variety of cloud stacks. Yet all the automation tools rely on a common conceptual framework: treating cloud resources as abstract objects that can be configured, run and managed as software code. Hence, the overlap with DevOps methodologies and organizational models.
Read the full column for plenty of statistics about why a multi-cloud future is inevitable for most enterprises and what they need to turn the vision into operational reality. Indeed, there are dozens of software and SaaS products designed to automate infrastructure and application management across multiple clouds. Some focus on specific needs or usage scenarios. For example, Cloudyn is designed for asset and cost management and includes a workload optimizer to identify the most efficient cost-performance deployment option for a particular workload, while CSC, using the former ServiceMesh product focus on cloud governance, security and lifecycle management. Others, like Cliqr, Cloudify and ElasticBox take an application-centric approach to cloud automation.
Yet the most popular multi-cloud products are generally those used by organizations embracing a DevOps approach to cloud management, a tact that extends application programming into the realm of infrastructure configuration and management. The article walks through the most popular options. Any will work on both private infrastructure and across all the major public clouds, however the integration details will vary widely. The choice of product should be dictated by the sophistication and scale of one’s infrastructure and expertise of the IT/DevOps team. I conclude with some other recommendations.
Originally published at markoinsights.com on November 6, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
See all (132)
1
1
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/backend-as-a-service-baas-for-efficient-software-development-6a7a142af477?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 15, 2018·10 min read
First-generation cloud services, such as Alibaba Cloud IaaS and PaaS products, eliminate the need for managing servers and O&M systems by deploying cloud data centers. Backend-as-a-Service (BaaS) represents the second-generation cloud service platform. BaaS can further simplify and optimize cloud computing resources and provide all-in-one cloud services including development, O&M, and service management.
BaaS can package public cloud data center resources based on frontend application scenarios and provide them to developers through simple interface invocation. With those benefits, developers can focus on studying users, creating and designing app software, and developing mobile-end apps. This greatly simplifies the development procedure, development cycle and personnel, as well as capital investment while accelerating the launch of apps to the market. The objective for the development of the BaaS architecture is to solve business development efficiency problems. From this article, you can understand the development trends of current software development models.
With the proliferation of the Internet and the booming number of mobile-technology-based start-ups, the time for implementing an idea is as short as a few months. With intense competition, if a company fails to do so, other similar products may come out, which is true for O2O, Internet of Things and Internet finance. For entrepreneurs, launching products quickly, going through market tests and occupying the target market is undoubtedly a quick-action battle. To survive and thrive in such a competitive marketplace, you must run your business efficiently and effectively. Rooted in the furnace of the mobile internet, the BaaS architecture has gained importance.
For entrepreneurial developers, the priority is cost and efficiency. They normally lack technical accumulation, and they need to develop their businesses by proving their business models with low costs. In this context, cloud service providers become their best options.
When entrepreneurs’ technical teams are trying to implement their architectures, they will be delighted to find that dedicated entrepreneurial technical companies are providing some of those required common features. Being open to developers as cloud services, these features can either provide certain portions of the required functionality or complete features. On this basis, the cloud service ecosystem is formed and can provide entrepreneurs with various technical services.
Let us assume that you are the CTO of an O2O entrepreneurial company and are considering developing a business platform from an O2O idea. In this age of the mobile internet, mobile app and web features are mandatory. You are likely to face the following challenges:
●Deployment and maintenance of servers●Development of apps and websites●Development of backend servers●Platform functions like authentication and authorization, file storage, pushing and communication, mapping, payment, social sharing, verification and security, intelligent identification, searching, and user behavior analysis●Services like Activity management
As a CTO, to overcome these challenges, you need to recruit backend, frontend, iOS, Android and O&M engineers. However, when trying to design and implement each of these features, you may find that some of these recruits are technically not sound. On the other hand, implementing these features independently and quickly requires high costs, time, and resources.
However, by researching the technical market, you can find that these basic services are already available as solutions from dedicated companies, so all you have to do is integrate those solutions to save development costs.
You can find entrepreneurial technical companies that provide dedicated services in almost every common functional area. With sharp business acumen, these technical companies can quickly respond to market needs, and this boosts the emergence of new technical architecture services. Entrepreneurs can directly utilize such rich solutions to meet their business needs and quickly develop product platforms for end users.
In fact, those entrepreneurial and technical platform companies are building a complementary relationship between themselves. While entrepreneurial companies are using the services from technical platform companies, the latter can obtain more data, and the former can obtain free and improved services. Through competitions, technical platform companies can enhance their services and offer developers a better development experience. This allows those companies to take the lead in their respective business fields.
In this mobile internet and cloud computing age, the technical platform companies can encapsulate certain features as services to acquire new users (in the form of developers). The strategy has resulted in the popularity of the BaaS architecture soaring in recent years.
Today, so many systems exist in Taobao’s technical departments, and we know little about their specific roles. To figure this out, we can go over those systems by using the domain layered model:
Surprisingly, we find that there are hundreds of systems supporting a relatively small number of services. By exploring those systems in depth, we can find the following major problems:
Here, you can easily see the waste of development and physical resources. Those systems have also become a huge burden that requires lots of effort and resources to maintain, upgrade, develop, run, and monitor. To perform code-level reconstruction on a software program that is complicated and difficult to maintain, you probably require architecture-level reconstruction to overcome the system-level complexity.
Alibaba Cloud’s developers have deep technical understanding as well as extensive practical industry experience in cloud technology. On the one hand, Alibaba Cloud has outstanding business scenarios that require excellent technical skills to maintain; on the other hand, internal developers have to go through highly intensive development practices, which make them come out on top.
While outsourcing internal systems, for example, to develop a new business system, Alibaba Cloud was able to determine some of the best practices that developers need to follow during different phases.
●Development phase:Developers need to consider how to design databases, separate databases, and tables, and ensure high security, concurrency, and performance. In this case, developers need to use the databases (MySQL and Hbase), message middleware (notify and metaq), cache (Tair), distributed invocation (HSF) and .J2EE.
●Maintenance phase:Alibaba Cloud’s attitude towards system stability is that stability prevails over everything. During one recent promotion event, most development teams focused on ensuring system stability by reviewing system architecture and dependency strength and designing traffic limitation plans. Thus, high concurrency, system performance optimization, and JVM have also become strengths of Alibaba Cloud developers.
However, we must go over the following questions: Is it necessary for developers to master so many skills in developing business systems in addition to implementing business logic? Shouldn’t business system developers focus on developing business logic? Shouldn’t system stability and high backend concurrency performance be implemented by less advanced and professional teams, why does each development team have to do the same job? Is this the result of job division, planning or the technical architecture?
In Alibaba Cloud, there is a trend: after a certain period, technical system developers are prone to running businesses, and business system developers are prone to building platforms. On the one hand, this phenomenon reflects the developers’ desire to promote themselves based on KPIs; on the other hand, this reveals their confusion about career development resulting from ambiguous objectives.
To be specific, business development teams need to meet business requirements while technical development teams need to provide various capabilities for business development and ensure underlying support services. Such architectural specificity ensures unambiguous assignment of responsibilities.
To solve problems of complexity, we need to learn from enterprise middleware software programs. For example, for traditional banking businesses, different internal systems can combine various standards such as EIP and ESB to work out complicated businesses together.
Software engineering draws many references from construction engineering. In the construction field, China Grand Enterprises is famous for constructing the Broad Pavilion overnight at the Shanghai Expo 2010.
Recently, China Grand Enterprises constructed a high-rise with 57 floors in 19 days. Their constructional renovation achievement demonstrates China’s progress in today’s world. Their achievement lies in the innovative construction method, namely constructing buildings by following the “standard construction model.
If you can standardize and modularize your accumulative technical skills so that business teams can quickly utilize those skills, you will be able to gain the expected “platform” capabilities.
For Alibaba Cloud, its technical accumulation has formed a complete system from cloud infrastructure construction to middleware, to e-commerce systems.
This system covers almost all technical fields, and the accumulated skills can support the world’s largest e-commerce businesses. To solve current problems, it needs to review existing processes and “modularize” its capabilities in addition to combining those capabilities through “standardization.”
In this way, Alibaba Cloud development teams can effectively develop separate capability modules while business systems can use these modules in the standard way.
Building a capability supermarket allows you to use refined market management to transform the development model from an open market to a modern supermarket. As we all know, supermarkets are bound to replace open fairs during urbanization development.
The reasons are simple. Essentially, supermarkets outshine open fairs due to their chain of operation, which features centralized procurement, distribution, and management. Centered around the linking system, chained supermarkets rely on the network of mass outlets, develop sales revenue based on the centralized procurement system, and make a logistic profit out of modern distribution centers. They route marketing information to the processing and manufacturing industry to develop OEM products and even form a supply chain to develop manufacturing profit. That is to say that chained operation expands standardization, routinization, and industrialization to the circulation domain to reduce costs dramatically while providing consumers with tangible benefits.
In this centralized management and standardization approach, chain operation can earn maximum scale and efficiency advantages. Similarly, business development teams should also learn from this modern supermarket operation model to improve work efficiency through centralized management and standardization.
Nowadays, uneven business development and technical planning have created a gap in Alibaba Cloud’s operation. By reviewing its internal development ecosystem, Alibaba Cloud found that different development teams develop based on separate department businesses, resulting in cases of repeated construction of resources. Technical and business departments cannot fully support and complement each other while they do not have a clear understanding of each other’s capabilities or overall market capabilities. This results in redundant workloads that create a similar situation to that of repetitive procurement in open markets. To solve this problem, we can take cues from modern supermarkets for their overall management and planning of operation models, layouts, stocks, and capabilities. In this way, it is possible to manage the responsibilities and capabilities of different development teams explicitly while ensuring close cooperation among them.
Metaphorically, capabilities are like products in a supermarket. You can purchase the required capabilities, while at the same time able to discard the unsalable ones. You can enhance salable capabilities, and avoid repetitive capabilities. You can also centrally plan categories of capabilities, with their utilization monitored and charged accordingly.
By doing this, you can formulate an effective mechanism with which a newly-developed capability will be available to all business teams, and it can undergo further development to meet the requirements. In this way, different development teams will not develop similar capabilities repeatedly, preventing wastage of resources. (For example, think about how many rule systems are there internally.)
By operating capability developers and consumers, one can create an efficient ecosystem to avoid resource wastage. By using the supermarket capability, consumers can understand all internal capabilities while capability developers can respond to market needs to develop required capabilities.
.
Legacy enterprise middleware uses enterprise integration systems to coordinate complicated business problems among multiple systems. Similarly, Alibaba Cloud’s business development also requires the coordination of its systems and needs to respond to complicated businesses quickly.
Many scenarios need data synchronization, such as the data synchronization between business systems and search systems and the data exchange between ODPS offline data and online data.
Currently, we achieve data interaction of these systems by custom APIs or scripts of specific systems. To develop such standards, developers need to understand each system’s APIs. Once those standards are ready, like enterprise integration, it will significantly lower the development threshold of developers and improve development efficiency.
Soon, the new tiers of the mobile internet cloud-computing era (UI, MBaaS, and platform) will replace the three-tier architecture of the J2EE time (presentation, middle and data service tiers).
With the new architecture, complicated business systems can become simple and loosely-coupled as well enable easy sharing of data and interfaces internally and externally.
As shown in the figure above, most of Alibaba Cloud’s development architecture stays in the tightly-coupled state, or it partially transforms into the SOA a;;rchitecture. Ideally, it needs to move to a third development architecture to meet complex business needs among systems in a simple, standard and interchangeable way.
Also, for business development teams, their development capabilities must align better with the frontend. In this way, those teams only need to retain JavaScript and RESTful APIs in their technology stacks while they can focus on understanding business models and logic to quickly build business systems and implement business innovations.
For backend teams, they need to focus on implementing platforms and services. To implement those services, they need to upgrade their development architectures from the J2EE era (such as MVC and RPC) to new architectures of the cloud era such as microservices, EDA and CQRS. They should enhance their understanding of system complexities and utilize servitization to meet the needs of business teams.
With the upgrade of those architectures, division of responsibility for development teams will have better definitions, for example:Development team -> frontend, interaction and business logicBackend team -> platforms, services, and stability
By building an optimum ecosystem around data for applications (businesses), developers and platforms, it is possible to share all developers’ expertise and experience effectively. Further, all developers can review and follow the development of various businesses and the designing and implementation of architectures. Meanwhile, by developing development expansion and module standards, developers can take the initiative to submit their capability modules, which you can purchase through the platform “capability supermarket.”
This article looked at some of the development trends of current software development models. We discussed how the adoption of the Internet and mobile technologies has revolutionized the business ecosystem, with entrepreneurs able to implement their ideas quickly by leveraging services of technical platform providers. We also looked at certain challenges that are likely to come in our way, and finally discussed some recommendations by Alibaba Cloud on overcoming these challenges.
Reference:
https://www.alibabacloud.com/blog/Backend-as-a-Service-(BaaS)-for-Efficient-Software-Development_p519851?spm=a2c41.11292175.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/a-big-test-for-online-learning-during-the-covid-19-outbreak-d089e502f438?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 15, 2020·12 min read
In order to win this inevitable battle and fight against COVID-19, we must work together and share our experiences around the world. Join us in the fight against the outbreak through the Global MediXchange for Combating COVID-19 (GMCC) program. Apply now at https://covid-19.alibabacloud.com/
From the team at Alibaba Cloud Research Center.
All the predictions about online education market trends for the 2020 winter holiday in China turned out to be wrong. Originally, there was a high probability that the online education market would only calmly get through 2020 because the fierce competition during the “Summer War” in the kindergarten through 12th grade (K-12) education segment in 2019 had not completely faded away. In order to compete for new users and online traffic in the 2019 summer education market, ten major K-12 online education enterprises engaged in a fight for market share. According to a conservative estimate by industry insiders, they invested over 5 billion Chinese Yuan (over 700 million US dollars) in marketing, new user attractions, and commissions for high-quality teachers in less than 100 days.
“It was another capital-driven battle. People threw their money into the violent competition.” Or at least that’s how a reporter from Southern Weekly described this “Summer War”, which brought slightly over 10 million new users to only the top companies. The reporter also pointed out the hidden concerns behind the K-12 online education market: The capital exhausted in the “Summer War” greatly pushed up the costs of each enterprise. However, the renewal rate in the autumn season following the marketing campaign was expected to be less than 30% for most enterprises. In addition, the new regulations implemented by the Ministry of Education for the online education industry also increased operating costs in the short run.
But all the predictions about the online education market trends made for the 2020 Chinese New Year holiday turned out to be wrong.
Many industry insiders made similar predictions: The online education market will definitely be “cold” in 2020. “We may have to tighten our belt for a while to avoid freezing stiff or even to death in this winter.”
No one could have predicted that the coronavirus disease (COVID-19) would break out over the Chinese New Year holiday. When the Ministry of Education immediately issued the “Classes Suspended but Learning Continues” initiative, students from around the country flocked to online classrooms. Although this caused the industry penetration rate surged nearly 10-fold, online education enterprises faced some unprecedented problems:
Ho could they provide and support quality services to swarms of new users? How could they maintain system stability during peak traffic hours? How can they deal with teamwork, organizational processes, and educational administration? How can they make the best use of their technologies with limited manpower and resources?
And how fierce would this big test be? Well, here is some data to give you a glimpse of the situation:
So how did these K12 online education enterprises fare with this big test?
There was over a 10-fold growth increase in peak traffic. Such a traffic spike is a severe challenge for any online education enterprise. First, when facing such a challenge, an online education enterprise must ensure they a sufficient number of staff, this including teachers and teaching assistants, routine management personnel, and O&M technical engineers. To give a bit more context, in order to provide high-quality course content for primary and secondary school students, Yuanfudao mobilized 356 leading teachers, 412 teaching assistants, 151 technical engineers, and other relevant staff to continuously provide free live streaming classes during the COVID-19 outbreak. However, these human resources were still nowhere near enough.
The biggest difference between online education and conventional offline education is the very nature of being online. Online education provides educational services through the Internet, smart devices, and the digital courses that are broadcasted to students. Therefore, it is very important to ensure that online services that serve as the backbone of these e-learning services are operating continuously and stably and without lag, high latency, disconnection, or downtime.
With that said, it is unwise to depend on conventional data centers and IT architecture. The first reason is that it takes as long as three to six months to purchase, deploy, and use equipment for conventional on-premises data centers. This amount of time makes it hard for an enterprise to be able to cope with sudden traffic peaks in a reliable and sustainable manner. Second, the traffic peaks produced by the COVID-19 outbreak are only temporary. Therefore, purchasing large numbers of physical servers and bandwidth would produce considerable waste after the outbreak is over. Third, the manpower and time costs of development and O&M would be unnecessarily high because multiple technology stacks are required to cope with the traffic spikes in a conventional data center. In particular, the technology stacks used for distributed storage, distributed computing, video encoding and decoding, video encryption, and CDN distribution would all be required.
Only employing cloud services would be able to meet such challenges in a cost-effective manner. Let’s describe the experience of a certain online education enterprise during the COVID-19 outbreak as an example. The enterprise scaled up its system three times and almost doubled its consumption of cloud resources in less than a week. However, the most recent time, it took less than 10 hours to inventory resources, deliver requirements, and scale up the system. The scale-up was completed at during the only morning hours, allowing millions of primary and secondary school students to attend online classes on the same day.
But the reality of the situation is that cloud computing provides much more than what we just described for online education. It can provide several things that can add value to online education services.
Below are some of the advantages cloud computing services were able to bring to China’s online education market.
1. Fast launch even when starting from scratch: In Huanggang, Hubei Province, it took only one day to implement an online teaching platform through an online live streaming platform for the senior students (12th grade students) at Huanggang High School. As the technological foundation, ApsaraVideo services provided by Alibaba Cloud helped by autoscaling resources, processing massive volumes of data, intelligently applying the data, and deploying cloud products in vertical video scenarios such as live streaming and video on demand. This comprehensively ensured that the system could run smoothly in high-concurrency scenarios.
2. Deep penetration and wide coverage: Because teachers and students live all over the country for larger non-localized online platforms, only a large number of cross-city, cross-province, and cross-country connections can meet the needs of students located in more remote or rural regions. At Alibaba Cloud, edge nodes are built based on the providers’ edge nodes and networks. This ensures the high stability of real-time live streaming and interactive networks. In addition, with our global coverage and multi-layer penetration features, edge nodes enable deep penetration and wide coverage of online education services.
3. Use on demand: The auto-scaling feature of Alibaba Cloud can cope with peak traffic, while conventional virtualization technologies usually result in a severe waste of resources outside of peak times. With the popularization of cloud-native technologies, containers have become a new option for elastic scaling. Based on the DevOps System, containers can be elastically scaled up and down. In addition, the emerging serverless approach not only ensures high availability of applications during traffic spikes and plunges, but also reduces idle computing resources during low-traffic periods.
4. AI + education: AI technologies will revolutionize the online education industry, especially in terms of efficiency. After online practice exams are taken by millions of people, AI can implement automatic correction, intelligent evaluation, and national ranking. In online classrooms, AI can provide intelligent customer services, automatically convert online classes to audio files for students to review afterward, and automatically analyze the video and audio files of teachers and students to evaluate the class quality based on its content. Just as Zheng Zibin, the CTO of VIPKid, pointed out at the Apsara Conference 2019, with the help of the cloud and AI technologies, online education is entering the AI era with individualized teaching being its focus.
5. Security and legal compliance: By their very nature of being online, online services exist several security challenges and compliance risks. In fact, in recent months, cyber attacks to online education platforms have increased dramatically. Online education and e-learning requires complete security protection solutions to prevent service interruptions caused by cyberattacks and other security threats. Alibaba Cloud provides comprehensive security assurance in the form of technologies, products, solutions, experiences, teams, and methodologies.
In the face of the “stress test” of this outbreak, the overall performance of the online education enterprises in China has been remarkable. This is not only reflected in its large scale coverage geographically speaking but also in terms of its coverage of different online teaching scenarios, including pre-class, in-class, after-class exercises.
As the “Classes Suspended but Learning Continues” Initiative was implemented from the Chinese central government to local governments, a great number of primary and secondary school students and parents have become deeply familiar with online education. This has played a very positive role in promoting the overall online education market. However, it’s too soon to tell whether this will usher in a new extended period of rapid development.
Nevertheless, the COVID-19 outbreak has had a profound impact on the online education market. And, at present, we can draw the following conclusions.
1. Education authorities at all levels will become active promoters of online education. The value of online education has been fully demonstrated in diversifying teaching methods and improving educational equity and inclusiveness. Online education will become a standard learning tool and method all across China. After the COVID-19 outbreak is over, more conventional education authorities and schools will engage in online education or even establish innovative partnerships with online education enterprises. For example, primary and secondary education alliances have emerged in Beijing, Shanghai, and Guangzhou. In the future, online education will be deeply integrated with learning in physical classrooms.
For this, education authorities and schools can benefit from a full-stack integrated solution that has lower barriers to adoption, high integration, and full coverage of IaaS, PaaS, and SaaS and build the solution in a public cloud. This lowers the requirements for technical engineers, simplifies installation and deployment, and speeds up system acceptance and launch.
2. Learning plans and course content will be more personalized. In the future, the personalization-oriented online education platform will automatically and intelligently formulate personal learning plans and trajectories and identify strengths, weaknesses, and learning preferences of each student. This will help teachers develop better learning strategies and programs for different students. In addition, based on knowledge graphs, solutions will be able to automatically produce course content, such as automatically storing teaching content and adding and translating subtitles.
3. Online education enterprises will build brand new digital and intelligent evaluation systems. Such systems can provide automatic correction, intelligent evaluation, and national rankings after online practice exams are taken by millions of people. In addition, the systems will be able to monitor the quality of courses, recognize English pronunciation, and analyze error-prone questions and learning models. These new education systems will use digital and intelligent technologies to profoundly change conventional learning systems and evaluate the interaction between teachers and students, making teaching and learning interaction truly natural, smooth, and comprehensive.
4. Alibaba Cloud full-stack solutions will become the key support for online education. Currently, online education is provided in various forms, including open classes, live streaming, video-on-demand, VR, AR, one-on-one (1-to-1), multi-party interactive teaching and learning, and student-to-student interactive learning. More new forms are likely to emerge in the future. This means that the technological stacks for online education are extremely complex. For example, the solution must support tens of thousands of concurrent online users, multiple interactive methods, and even automatic translation. Therefore, online education enterprises must deploy innovative solutions that allow fast iteration on the cloud.
During the COVID-19 outbreak, full-stack solutions that integrate the capabilities of Alibaba Cloud, the DAMO Academy, Youku, and DingTalk were deployed on Alibaba Cloud. A full-stack solution can provide users with many convenient and innovative services by leveraging the capabilities of terminals, such as DingTalk, Youku, Mini Programs, and user-defined applications.
5. Online organization, intelligent collaboration, and service digitization will determine the winners in the online education field. Nowadays, the need to manually review the legal compliance of live streaming education content results in high operating costs. The daily repetitive consulting service with sparse business opportunities occupies a large amount of manpower. The various multi-directional communication between teachers, students, parents, and managers greatly decreases daily collaborative efficiency.
To achieve rapid development, high efficiency, cost reduction, and fast collaboration, online education enterprises must enable online and digital collaboration between teaching and operation teams. In particular, it is essential to empower marketing and teaching services through digital and intelligent technologies. For example, the operating efficiency of online educational enterprises can be effectively improved simply by extracting the three to five minutes of key content and providing simultaneous translation and collaborative notes.
At VIPKid, thanks to the capabilities provided by Alibaba Cloud, the accuracy of automatic course quality detection has been obviously improved. Specifically, the detection of the content, time points, and instances of encouraging words spoken by English teachers has attained an accuracy of 90%, and the detection of the speech rate of English teachers exceeds 90%.
The outbreak of COVID-19 did warm up the online education market in the “winter” of the industry. However, this was not a real blessing. Here, I would like to end with the words of Tang Xiaozhe, a technical director at Hujiang Education Technologies Co. Ltd, in a recent media interview:
“We have reached a consensus internally that this (outbreak) cannot be said to be an opportunity for our development, because the price of the outbreak is extraordinarily high. During such a difficult period, it is our duty to help more schools, institutions, and teachers provide students with stable online learning services through the Internet and go through the difficult time together.”
At present, Alibaba Cloud has launched special policies, such as providing products and technologies for all Internet customers free of charge during the outbreak.
The ApsaraVideo series products are now available for free, including ApsaraVideo Live, ApsaraVideo VOD, and Real-Time Communication (RTC). And in terms of security, free consulting services and free trials of security products are provided. Alibaba Cloud also offers discounts on services such as Short Message Service, Voice Messaging Service, and Phone Number Verification Service. In addition, Alibaba Cloud has launched a “Classes Suspended but Learning Continues” alliance. Currently, we have partnered with several alliance members to build “Classrooms in the Air” for Wuhan Education Cloud. This provides free cloud resources and services to ensure that local primary and secondary schools can carry out online education according to their teaching plans.
While continuing to wage war against the worldwide outbreak, Alibaba Cloud will play its part and will do all it can to help others in their battles with the coronavirus. Learn how we can support your business continuity at https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/three-reasons-to-add-alibaba-cloud-to-your-multi-cloud-strategy-5aed8662d7e3?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 6, 2018·6 min read
From the minute you chose one of the world’s most popular cloud computing companies as your strategic cloud services provider, your world became multi-cloud. In a multi-cloud world, there’s now every reason to consider and embrace the relative newcomer, Alibaba Cloud. Not just as a specialist niche player to support business in Asia, for example, but as a full-service second-source supplier offering all the services, globally, you would come to expect from a leading cloud provider.
Multi-cloud is the use of two or more cloud computing services. A Multi-Cloud Strategy is the deliberate and considered choice of the partner providers and the mix of services commissioned from each. In the initial stages of cloud computing, many organizations looked to diversify their cloud vendors due to reliability concerns and multi-cloud was seen as a way to prevent data loss, downtime or vendor lock-in.
While these are still concerns, they are becoming a lower priority. The modern multi-cloud deployment may now be driven by a business’s technical or strategic goals and different providers selected on their merits to support different parts of the business. This can include utilizing the most cost-effective cloud solutions over different periods, leveraging access speed and services offered by vendors or assisting large-scale deployments to different geographical regions.
The essence of success now is to pick your various cloud services providers each for their specific strengths. And Alibaba Cloud has at least three specific strengths which mark it out from the crowd:
• If you’re doing business with China or Asia, Alibaba Cloud has to be part of your cloud mix.• If you seek sheer scale or reliable computing power and performance, it has to be Alibaba Cloud.• If you are concerned about security — again, Alibaba Cloud may be your best choice.
Alibaba Cloud is the #1 public cloud vendor in China with China’s largest cloud network, comprising seven data centers in Mainland China and more than 1,000 CDN nodes connected by a multi-line BGP backbone network and multiple Availability Zones in each region. More than one-third of China’s top 500 companies, among them energy giant Sinopec, automaker Geely, and telecoms company China Unicom, are Alibaba Cloud customers, as are numerous government agencies like China Customs as well as the 2022 Beijing Olympics.
In addition to the deployment regions in China, Alibaba Cloud has two in the United States and one each in Hong Kong, Singapore, Sydney, Kuala Lumpur, Tokyo, Frankfurt and Dubai. As part of a multi-cloud strategy, an organization may choose to host in Virginia with one cloud vendor and in Frankfurt with Alibaba Cloud. This would ensure vendor separation and regional separation and still provide local high-speed delivery to clients in both regions.
Sister companies to Alibaba Cloud in the Alibaba group of companies can also help organizations set up and do business in China. The payments company Alipay, the logistics company Cainiao (which handled 812 million delivery orders in one day at the 11–11 Global Shopping Festival) in 2017 and the e-commerce company Tmall (which is also China’s largest third-party platform for brands and retailers), could all provide serious support to companies trading in the region.
These strengths are combining to make Alibaba Cloud a serious second-source cloud provider and an emerging alternative, if not the primary provider, in many regions in the world and for many applications.
The sheer scale of Alibaba Cloud’s operations is incredible. The world’s premier showcase e-commerce events each year must be the pre-Christmas shopping days known as Singles Day (or the 11–11 Global Shopping Festival to be exact), Black Friday and Cyber Monday. The first of these runs on Alibaba Cloud — yes, the entirety of the 11–11 Global Shopping Festival is run on platforms operated by Alibaba platforms — and in stark summary supports over USD $25BN of business in one day compared roughly to Black Friday’s $8BN and Cyber Monday’s $7BN. The value of mobile transactions on this one day is roughly ten times higher in China than it is in the U.S.
This is some scale, some resilience and some power.
In 2017, the 11–11 Global Shopping Festival saw 1.5 billion payment transactions, with Alibaba Cloud processing 325,000 orders per second at peak and the payments platform, Alipay, supporting 256,000 payment transactions per second at peak. All this was up some 40% on the previous year. That takes cloud computing horsepower, scale and security to a level that can definitely be described as world-leading.
So they shop, but Chinese people also travel. At the annual Spring Festival, another great national event where everyone, it feels, travels on the same few days, the Alibaba Cloud platform hosts the railway ticket site 12306.com and supports up to 40 billion page views a day leading up to peak travel time.
With this sheer scale of operation, you would expect Alibaba Cloud to focus on the highest levels of resilience and security to mark it out from all other cloud services providers — and it does. It offers a suite of purpose-built cloud products to protect against common attacks in China and safeguard sensitive data and online transactions. Anti-DDoS Basic enables you to mitigate attacks by routing traffic away from your infrastructure, including protection at the application and volumetric level. Anti-DDoS Pro, Web Application Firewall (WAF) and Server Guard defend against massive DDoS attacks and isolate cloud networks to operate resources in a secure environment with VPC.
Sometimes companies are driven to multi-cloud by forces beyond their control. In late 2017 when the website of one Australian company, Mishi, was attacked and the company was blackmailed to pay for restoring the site, Mishi turned to Alibaba Cloud for help. They deployed Alibaba Cloud WAF Pro and Enterprise versions as a countermeasure to repeat attacks.
Even with all this smart and secure technology, Mishi cited the personal service which provides 12/7 tech support, fast response times, and bilingual cloud solution architects as “the most compelling reason” behind their initial decision to choose Alibaba Cloud.
As a Singapore registered company, Alibaba Cloud complies with high-level international certifications to guarantee data security. Alibaba Cloud’s security products are trusted by over 40% of websites hosted in China, which includes the free Anti-DDoS Basic service, automatic snapshots, triplicated backups, and advanced services that have set records in DDoS attack protection. Alibaba Cloud is also the first cloud services provider to receive CSA STAR Certification and the first cloud services provider to be certified with the ISO27001 Information Security Management System Certification in China.
It’s worth noting too that in addition to scale in China and security, Alibaba Cloud provides website hosting and support with registration and legal compliance for organizations who want to trade online with and within China. It’s ICP compliance service, for example, provide a one-stop registration service through the Alibaba Cloud Management Console and personal support from a bilingual team.
Getting started with Alibaba Cloud is as simple as setting up an account at www.alibabacloud.com. It’s free and you will get $300 credit to get a server up and running in minutes at no cost. From there, you can incorporate projects in to your multi-cloud world as you need them: if you’re doing business with China or Asia, if you need awesome scale or if you are concerned about security.
Want to learn more about Alibaba Cloud? Sign up to watch webinars about Alibaba Cloud products, services, and technologies.
Reference:
https://www.alibabacloud.com/blog/Three-Reasons-to-Add-Alibaba-Cloud-to-Your-Multi-Cloud-Strategy_p304612?spm=a2c41.11212064.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@drb/5-lessons-from-amazon-web-services-a10ccf62394e?source=search_post---------191,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Barnes
Apr 22, 2015·2 min read
Nobody expected Amazon to own cloud services. But they stumbled in and took a dominant share of a huge, growing market. Here’s 5 lessons all businesses can learn from Amazon’s success.
AWS was not a logical strategic step. Amazon had some of the most scalable server infrastructure in the world and found a new revenue stream with new customers by renting it out to other businesses.
Meanwhile, renting supercomputer capacity was a much stronger strategic fit for Sun but they lacked the capability to pull it off.
Look at your strengths and find ways to share them. You never know where it will lead. And beware of logical strategies that you don’t have the skills to pull off.
Amazon started out trying to sell a platform for ecommerce vendors, but soon realized they had much bigger, easier fish to fry. AWS ended up a simpler, better product in a much larger market.
Niches are important. But simple general purpose tools that can help lots of people are even better. Think Twitter, Dropbox, Basecamp, Trello, Instagram. Many of these started as niches that pivoted into something general purpose.
Try to offer something simple that millions of people or businesses will want.
The obvious customers for AWS was big companies that needed computing power and wanted to save money. Amazon ignored these customers at first, going after tiny garage companies with nothing to lose and everything to gain. These small customers were more tolerant of faults and outages. AWS didn’t just save them money, it offered them something they couldn’t afford any other way.
Start at the bottom: people who will be grateful for your empowering, low cost product even if it’s rough or unreliable. They’re the perfect target for your MVP.
By appealing to small and early businesses AWS became a platform for growing businesses. Now some of the hottest startups are built entirely on AWS, giving AWS kudos across tech.
Most AWS customers don’t grow at crazy speeds but enough do. Appeal to businesses that are small now but will be big tomorrow. Grow with them and help each other.
Amazon didn’t stop. They’ve rapidly launched a range of services offering databases, computing, and machine learning capacity. All of these follow the same model: launch a rough, magical product for the small and grow with your customers until you’re ready for the enterprise.
There’s always another competitor ready to take your market. As you grow you will have to fight incumbants and userpers. Davids and Goliaths. Keep growing and keep fighting.
It turns out my (former) employer did not share my opinions
It turns out my (former) employer did not share my opinions
"
https://medium.com/@BMCSoftware/establishing-metrics-to-assess-your-cloud-vendors-eb36ae74f353?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
BMC Software
Jan 12, 2017·3 min read
Once accessible to only the biggest companies with the deepest pockets, today’s cloud services are available to any size company with even the most modest budget. A 2016 IDC study reveals that 58 percent of organizations surveyed use public or private clouds, a 24 percent jump from a mere 14 months ago.
When entrusting your most critical data to the cloud, however, you need to use a service provider you can depend on. A standardized set of objective metrics enables you to see how successfully your provider can meet your business needs. There are many aspects to take into account when selecting a cloud vendor, but we’ve narrowed it down to the top three metrics you’ll want to consider first.
The average cost of a data breach is roughly $4 million, according to IBM, and the damage it can do to your company’s reputation is incalculable. Your service provider should have a strong, multilevel cloud security strategy that protects your data from threats both within and outside your organization.
For example, private cloud vendor Orange Business Services relies on BMC Cloud Lifecycle Management to safely distribute cloud resources to its customers and minimize the risk of exposed data. The platform’s Role-Based Access Control allows Orange to restrict user access points to specific roles and activities.
An article on InfoWorld explains, “Compliance policies often stipulate how long organizations must retain audit records and other documents. Losing such data may have serious regulatory consequences.”
BMC Cloud Lifecycle Management’s compliance automation feature reduces the risk of exposure during maintenance, patching and configuration tasks.
Ask your vendor to provide metrics on its security features to ensure they meet your business requirements.
2. Cost
While a data breach could cost your business millions, a solid cloud environment can save you a bundle of money. Gina Longoria, Senior Analyst for Servers at Moor Insights & Strategy, writing for Forbes, suggests that you should review the impact of cloud services on your capital and operating expenses as well as indirect costs like potential downtime. These calculations provide you with a thorough evaluation of your vendor’s total cost of ownership.
While you may think deploying on-site server hardware and software will keep costs down, Longoria notes that using the IT resources and expertise of a managed cloud provider can far exceed the savings of having an on-site data center that you manage yourself.
Most costs associated with cloud services are quantifiable, so you should run the numbers to make sure they align with your budget.
The most moderately priced and secure cloud platform in the world won’t help you meet your business goals if it isn’t available when you need it. Availability and performance are essential to providing a reliable cloud experience. BMC TrueSight Operations Management platform, for instance, uses analytics-based performance monitoring to mitigate issues through automated workflows and catch problems before they occur.
Dienst ICT Uitvoering (DICTU), a division of the Dutch Ministry of Economic Affairs, uses TrueSight Operations Management to oversee 12,000 workstations distributed over 200 locations. Its proactive analytics help DICTU’s IT department identify performance and behavioral trends across the cloud environment. The company credits the monitoring platform with accelerating the diagnosis and resolution of problems that could adversely impact its workflow and productivity.
Delivering a reliable and available cloud environment is a complex process with a lot of components. The service provider you select should be able to provide metrics that demonstrate it has the infrastructure and downtime mitigation processes in place to consistently support your business workflows.
While it is often assumed that cloud vendors are providing a secure and cost-effective service, you need to have data to back up these beliefs. Use these three metrics to ensure your cloud vendors are meeting your company’s needs.
BMC is a global leader in software solutions that help IT transform traditional businesses into digital enterprises for the ultimate competitive advantage.
See all (2,258)
BMC is a global leader in software solutions that help IT transform traditional businesses into digital enterprises for the ultimate competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/measuring-the-great-indoors/out-of-the-weeds-and-into-the-clouds-microservices-d54b45835fa0?source=search_post---------193,"There are currently no responses for this story.
Be the first to respond.
Today cloud services and standard data protocols have created a big shift from large monolithic software products to smaller series of modular component applications called microservices that run on the cloud.
The shift towards these smaller components enables a few important things that spatial designers should be aware of:
Microservices makes it easier for designers to play with design ideas because it allows for a wider ability to innovate then off-the-shelf monolith services without falling into a zone that becomes to onerous to program.
This is a course at Columbia University’s GSAPP exploring…
Written by
Spatial tech, design computation, organizational behavior, equity, and gifs. Adjunct Assist Professor @ColumbiaGSAPP. Director of Product, @SidewalkLabs.
This is a course at Columbia University’s GSAPP exploring techniques for working with data from the physical world, with the aim of understanding and manipulating dynamic, interactive environments, taught by Gaby Brainard and Violet Whitney.
Written by
Spatial tech, design computation, organizational behavior, equity, and gifs. Adjunct Assist Professor @ColumbiaGSAPP. Director of Product, @SidewalkLabs.
This is a course at Columbia University’s GSAPP exploring techniques for working with data from the physical world, with the aim of understanding and manipulating dynamic, interactive environments, taught by Gaby Brainard and Violet Whitney.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@robtiffany/deploy-emm-solutions-with-conditional-access-capabilities-58fb3d038d4d?source=search_post---------194,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rob Tiffany
Jul 13, 2016·2 min read
Let’s face it, your BYOD employees aren’t too thrilled about installing an EMM app, agent or container on their device. It feels like an intrusion on one of your most personal possessions and breeds mistrust. That said, the BYOD world is all about gives and gets. Unless your company enforces a corporate-liable policy and buys every employee a smartphone, a compromise must be made to ensure the security of corporate data. This is where the use of the carrot comes into play.
While the BYOD trend was initially about allowing employees to use their mobile devices for work, the trend has shifted. Now you encourage your employees to use their devices because it makes them more productive anywhere, anytime. Whether your company is just allowing or actually encouraging employees to use their devices for work, you have to overcome the “hassle factor” and suspicions of company spying that deters them from EMM enrollment.
First, your Mobile COE must perform exhaustive due diligence to select the most unobtrusive EMM package available with the fewest steps to install that still meets your company’s needs. Next, this system must prohibit access to the systems, apps and data employees want most until they enroll. Some packages even limit access via MAM functionality. Anyway, if you want email, you have to enroll. If you want to access SharePoint, you have to enroll. You get the idea. Gives and gets.
Reduce risk to your business by restricting corporate system access to only those devices enrolled in an EMM solution.
Learn more in, “Mobile Strategies for Business: 50 Actionable Insights to Digitally Transform your Business.”
Click here to buy it on Amazon and Transform your Organization Today!
Visit http://robtiffany.com and follow me at https://twitter.com/robtiffanyto get my insights on Enterprise Mobility, the Cloud, the Internet of Things and Digital Transformation.
VP & Head of IoT Strategy @ Ericsson | Exec Director @ Moab Foundation | Board Member | Founder | Author | Inventor | Speaker | Navy Submarine Veteran
See all (4,915)
VP & Head of IoT Strategy @ Ericsson | Exec Director @ Moab Foundation | Board Member | Founder | Author | Inventor | Speaker | Navy Submarine Veteran
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/foundations/microsoft-azure-and-office-365-resourcing-issues-4df5b8efcc18?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
As must be clear to everyone by now, there has been a massive spike in demand for public cloud services since the coronavirus outbreak first hit us. Microsoft report that whole countries have gone from zero use of cloud to deliver teaching to 100% coverage of cloud-based remote learning in a matter of weeks. MIcrosoft Teams has probably borne the brunt of that demand.
It is therefore not surprising that we are beginning to see the first signs of resourcing problems. Yesterday, the Register reported that ‘Azure appears to be full’: UK punters complain of capacity issues on Microsoft’s cloud and I’ve seen similar reports elsewhere.
If you get errors when trying to provision VMs on Azure, the advice from Microsoft appears to be:
Obviously the last of these needs to be treated with some caution. Although all Azure regions are built to the same level of compliance there are obviously factors to consider with relocating data: for example, although the EU Model Clauses and Privacy Shield are recognised as GDPR-compliant, they are likely to require ongoing monitoring by the data controller.
That said, a recent blog post on the Information Commissioner’s Coronavirus blog says:
“We are a reasonable and pragmatic regulator, one that does not operate in isolation from matters of serious public concern. Regarding compliance with information rights work when assessing a complaint brought to us during this period, we will take into account the compelling public interest in the current health emergency.”
Remember that the cloud providers are prioritising government and emergency services use of the cloud.
Freeing up capacity for emergency health service use by temporarily moving less critical stuff to less-stressed regions seems to chime very well with that “compelling public interest”. Though, in the current emergency, are there any regions that are “less-stressed”?
Note that I do not believe that Microsoft will move data out of the region into which customers have put it — however, what customers choose to do is their decision and we may well be tempted to created resources outside of our normally prefered regions in response to the current crisis.
To the above three pieces of advice, I would add two more:
Sorry, I appreciate that these are both very obvious. But the main point is that we all need to provision resources carefully and being mindful of the wider impact on others. Otherwise we just become part of the problem.
Clearly, capacity issues in Azure will affect, and be affected by, capacity issues in Office 365. Microsoft have scaled their Office 365 capacity vastly, particularly in response to increased demand for Teams. Therefore, many of the same considerations will apply. That said, I’m unclear around best practice here, specifically in terms of how to provision Office 365 resources in order to minimise the impact on underlying services. As Office 365 is delivered as SaaS we, as customers, have relatively little control over the underlying resource allocation. But Microsoft are making adjustments to various features to maximise utilisation.
In all cases I would advise strongly against panic buying. We all know where everybody buying too much toilet roll gets us.
(Thanks to my colleague Andrew Cormack for advising on the GDPR-related aspects of this post).
Originally published at https://cloud.jiscinvolve.org on March 25, 2020.
A blog by andypowe11
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/@alibaba-cloud/alibaba-cloud-supported-chinas-national-primary-and-secondary-schools-go-online-a40e02ab900e?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 8, 2020·5 min read
Bolster the growth and digital transformation of your business amid the outbreak through the Anti COVID-19 SME Enablement Program. Get a $300 coupon package for all new SME customers or a $500 coupon for paying customers.
By Alibaba Cloud ApsaraVideo team.
Classes throughout China were suspended due to the coronavirus epidemic, but learning continued online.
On February 17, the Cloud Platform for China’s National Primary and Secondary Schools, with the full support of Alibaba Cloud, was officially launched. This platform brings together high-quality educational resources and provides them for free. It was designed to provide online classroom services for nearly 180 million primary and secondary school students throughout China in an attempt to minimize the impact of the epidemic on student education. These cloud classrooms were built based on the cloud computing resources and technologies provided by Alibaba Cloud.
The Cloud Platform for National Primary and Secondary Schools provides six learning resource modules: epidemic prevention education, moral education, special subject education, curricular learning, electronic course materials, and video education. The curricular learning module selects high-quality content from the curriculum resources developed nationwide in recent years, supplemented by recordings from top teachers in Beijing and other regions as needed. The provided courses cover 12 subjects at the junior and senior high school levels and will be made available in the coming weeks. The platform plans to launch 169 classes in the first week.
In support of the Cloud Platform for National Primary and Secondary Schools, Alibaba Cloud provided a wide range of its products and services, including Elastic Compute Service (ECS), Cloud Storage, Content Delivery Network (CDN), and Short Message Service (SMS). These products and services ensured that the platform enjoyed the massive resource storage and processing capabilities it needs and that course notifications and user verification information were sent out correctly. Alibaba Cloud CDN, specifically, ensured smooth access to the platform’s mass library of video courses.
To this day, CDN has deployed more than 2,800 edge nodes worldwide with a bandwidth reserve of 130 Tbit/s. It effectively solves access problems due to insufficient network bandwidth, high user access traffic, volatile transmission links, and unevenly distributed network nodes. After successfully handling the annual Double 11 Shopping Festival and 2018 World Cup in Russia, Alibaba Cloud has developed a set of mature solutions and comprehensive contingency plans for high-concurrency video services, as well as sufficient technical resources.
CDN has already implemented system optimizations and O&M assurance measures to respond to the epidemic-driven surge in traffic that faces online education services. After being charged to support the Cloud Platform for National Primary and Secondary Schools, Alibaba Cloud set up a support team to coordinate capacity expansion, services, and contingency plans. The team deployed all the necessary resources within seven days, ensuring sufficient network bandwidth for the platform and stable video transmission links.
To date, CDN has served more than 300,000 customers worldwide, accelerating more than 1 million domain names every day, and undertaking billions of updates. It is widely used in the entertainment, e-commerce, sports, finance, government and enterprise, and education fields, where it has accumulated a wide range of successful business practices. During the 2018 World Cup in Russia, CDN hosted 70% of network-wide traffic and provided the support that allowed 24 million people to watch a single game online. During the 2019 Double 11 Shopping Festival, CDN provided an excellent 4K viewing experience, with a lag rate of less than 3%, for 51.44 million viewers. It also supported an interactive e-commerce experience during the peak traffic period, which reached 100 million QPS. Recently, CDN has also expanded its support for DingTalk video conferencing and other online services, providing strong assurance for tens of millions of people who try to work and learn online. In addition, the CDN system received a Grade III certification in the Classified Protection 2.0 system. This indicates that it is suitable for businesses that require high security, such as the government, finance, and enterprise office systems. It also provides customers with one-stop solutions for edge cybersecurity and acceleration and helps professional technicians create secure network distribution capabilities.
For different types of online education businesses, CDN provides scenario-specific acceleration solutions. For websites and apps, CDN provides high-quality node resources worldwide for intelligent scheduling and nearby access, which effectively improves site responsiveness. For large online classes, it provides multi-center BGP stream ingest that is suitable for high-concurrency scenarios, while integrating auto scaling and system contingency plans to effectively reduce lag and latency. CDN also provides solutions such as resource prefetching and deployment protocol optimization, which improve hit rates, guarantee millisecond-level responsiveness, and ensure a smooth viewing experience.
Since the outbreak of the epidemic, Alibaba Cloud has provided support to dozens of education departments, educational and training institutions, and Software-as-a-Service (SaaS) partners. With our wealthy cloud resources and professional video technology, we are helping to ensure reliable and smooth online learning for students across the country. To empower small- and medium-sized enterprises in the education industry, Alibaba Cloud ApsaraVideo launched the Online Education Support Program. In this program, ApsaraVideo (LiveVideo, VoD, RTS, RTC, and Short Video) provides free product suites and millions of Chinese yuan in subsidy funds to help enterprises build applications faster and more economically. With this program, we are practicing working together to get through this difficult time.
While continuing to wage war against the worldwide outbreak, Alibaba Cloud will play its part and will do all it can to help others in their battles with the coronavirus. Learn how we can support your business continuity at https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@grigorkh/what-is-cloud-native-computing-and-how-cncf-contributes-to-industry-47856ce0fc78?source=search_post---------197,"Sign in
There are currently no responses for this story.
Be the first to respond.
Grigor Khachatryan
Dec 9, 2020·4 min read
Cloud computing has become the leading method for scaling up workloads and growing businesses at a steady rate. It allows companies to build and run scalable applications in dynamic environments known as clouds.
Cloud technologies allow integration of multiple systems, offering a new platform designed to enable easy management and detailed…
"
https://medium.com/@auth0/auth0-is-now-part-of-uks-official-digital-marketplace-g-cloud-9-framework-a542d635db95?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Auth0
Jun 17, 2017·1 min read
Auth0 is excited to announce our inclusion into the Cloud Services Category of the G-Cloud 9 Framework, part of the United Kingdom’s official Digital Marketplace. Inclusion to this digital marketplace enables UK government agencies to purchase crucial cloud and security tools online without having to submit specific tenders or contracts to use these services. It is our hope that inclusion into this framework will help make robust identity and authentication management a more accessible and seamless experience for the public sector.
Read more!
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
"
https://medium.com/@alibaba-cloud/three-reasons-to-add-alibaba-cloud-to-your-multi-cloud-strategy-f7e4817c7fd1?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 14, 2018·6 min read
From the minute you chose one of the world’s most popular cloud computing companies as your strategic cloud services provider, your world became multi-cloud. In a multi-cloud world, there’s now every reason to consider and embrace the relative newcomer, Alibaba Cloud. Not just as a specialist niche player to support business in Asia, for example, but as a full-service second-source supplier offering all the services, globally, you would come to expect from a leading cloud provider.
Multi-cloud is the use of two or more cloud computing services. A Multi-Cloud Strategy is the deliberate and considered choice of the partner providers and the mix of services commissioned from each. In the initial stages of cloud computing, many organizations looked to diversify their cloud vendors due to reliability concerns and multi-cloud was seen as a way to prevent data loss, downtime or vendor lock-in.
While these are still concerns, they are becoming lower priority. The modern multi-cloud deployment may now be driven by a business’s technical or strategic goals and different providers selected on their merits to support different parts of the business. This can include utilizing the most cost-effective cloud solutions over different periods, leveraging access speed and services offered by vendors or assisting large-scale deployments to different geographical regions.
The essence of success now is to pick your various cloud services providers each for their specific strengths. And Alibaba Cloud has at least three specific strengths which mark it out from the crowd:
· If you’re doing business with China or Asia, Alibaba Cloud has to be part of your cloud mix.
· If you seek sheer scale or reliable computing power and performance, it has to be Alibaba Cloud.
· If you are concerned about security — again, Alibaba Cloud may be your best choice.
Alibaba Cloud is the #1 public cloud vendor in China with China’s largest cloud network, comprising seven data centers in Mainland China and more than 1,000 CDN nodes connected by a multi-line BGP backbone network and multiple Availability Zones in each region. More than one-third of China’s top 500 companies, among them energy giant Sinopec, automaker Geely, and telecoms company China Unicom, are Alibaba Cloud customers, as are numerous government agencies like China Customs as well as the 2022 Beijing Olympics.
In addition to the deployment regions in China, Alibaba Cloud has two in the United States and one each in Hong Kong, Singapore, Sydney, Kuala Lumpur, Tokyo, Frankfurt and Dubai. As part of a multi-cloud strategy, an organization may choose to host in Virginia with one cloud vendor and in Frankfurt with Alibaba Cloud. This would ensure vendor separation and regional separation and still provide local high-speed delivery to clients in both regions.
Sister companies to Alibaba Cloud in the Alibaba group of companies can also help organizations set up and do business in China. The payments company Alipay, the logistics company Cainiao (which handled 812 million delivery orders in one day at the 2017 11–11 Global Shopping Festival) and the e-commerce company Tmall (which is also China’s largest third-party platform for brands and retailers), could all provide serious support to companies trading in the region.
These strengths are combining to make Alibaba Cloud a serious second-source cloud provider and an emerging alternative, if not the primary provider, in many regions in the world and for many applications.
The sheer scale of Alibaba Cloud’s operations is incredible. The world’s premier showcase e-commerce events each year must be the pre-Christmas shopping days known as Singles Day (or the 11–11 Global Shopping Festival to be exact), Black Friday and Cyber Monday. The first of these runs on Alibaba Cloud — yes, the entirety of the 11–11 Global Shopping Festival is run on platforms operated by Alibaba platforms — and in stark summary supports over USD $25BN of business in one day compared roughly to Black Friday’s $8BN and Cyber Monday’s $7BN. The value of mobile transactions on this one day is roughly ten times higher in China than it is in the U.S.
This is some scale, some resilience and some power.
In 2017, the 11–11 Global Shopping Festival saw 1.5 billion payment transactions, with Alibaba Cloud processing 325,000 orders per second at peak and the payments platform, Alipay, supporting 256,000 payment transactions per second at peak. All this was up some 40% on the previous year. That takes cloud computing horsepower, scale and security to a level that can definitely be described as world-leading.
So they shop, but Chinese people also travel. At the annual Spring Festival, another great national event where everyone, it feels, travels on the same few days, the Alibaba Cloud platform hosts the railway ticket site 12306.com and supports up to 40 billion page views a day leading up to peak travel time.
With this sheer scale of operation, you would expect Alibaba Cloud to focus on the highest levels of resilience and security to mark it out from all other cloud services providers — and it does. It offers a suite of purpose-built cloud products to protect against common attacks in China and safeguard sensitive data and online transactions. Anti-DDoS Basic enables you to mitigate attacks by routing traffic away from your infrastructure, including protection at the application and volumetric level. Anti-DDos Pro, Web Application Firewall (WAF) and Server Guard defend against massive DDoS attacks and isolate cloud networks to operate resources in a secure environment with VPC.
Sometimes companies are driven to multi-cloud by forces beyond their control. In late 2017 when the website of one Australian company, Mishi, was attacked and the company blackmailed to pay for restoring the site, Mishi turned to Alibaba Cloud for help. They deployed Alibaba Cloud WAF Pro and Enterprise versions as a countermeasure to repeat attacks.
Even with all this smart and secure technology, Mishi cited the personal service which provides 12/7 tech support, fast response times, and bilingual cloud solution architects as “the most compelling reason” behind their initial decision to choose Alibaba Cloud.
As a Singapore registered company, Alibaba Cloud complies with high-level international certifications to guarantee data security. Alibaba Cloud’s security products are trusted by over 40% of websites hosted in China, which includes the free Anti-DDoS Basic service, automatic snapshots, triplicated backups, and advanced services that have set records in DDoS attack protection. Alibaba Cloud is also the first cloud services provider to receive CSA STAR Certification and the first cloud services provider to be certified with the ISO27001 Information Security Management System Certification in China.
It’s worth noting too that in addition to scale in China and security, Alibaba Cloud provides website hosting and support with registration and legal compliance for organizations who want to trade online with and within China. It’s ICP compliance service, for example, provides a one-stop registration service through the Alibaba Cloud Management Console and personal support from the bilingual teams.
Getting started with Alibaba Cloud is as simple as setting up an account at www.alibabacloud.com. It’s free and you will get $300 credit to get a server up and running in minutes at no cost. From there, you can incorporate projects in to your multi-cloud world as you need them: if you’re doing business with China or Asia, if you need awesome scale or if you are concerned about security.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@Intersog/5-essential-tools-for-startup-project-management-7edb1bbd6771?source=search_post---------200,"Sign in
There are currently no responses for this story.
Be the first to respond.
Intersog
Jul 15, 2019·4 min read
In this article, you will know about five SaaS solutions and cloud services and their alternatives that can help decision makers carry out efficient project management in startups and small businesses of all kinds. Those tools have basic versions and are available for free. Some advanced features are available in paid versions. However, the basic functionality in any of the described solutions is sufficient to run startup project management at scale.
You may consider combining any of these tools with existing software solutions in your startup if it makes practical sense. For example, MailChimp and Zoho CRM could make a good fit for marketing automation and project management in a digital marketing startup.
It is a must-have tool for digital marketers, project managers, and team leaders to do creative email and content marketing. MailChimp allows automating many routine tasks in digital marketing you can imagine so that you can focus on optimization and growth. That’s why many experts suggest using MailChimp as a standalone solution for startup project management and marketing automation.
Here’s how MailChimp can help you run effective project management and marketing:
Look at SendinBlue, Campayn, and Mailerlite as suitable alternatives to MailChimp, especially if you are looking more affordable email marketing solutions for your startup project management activities.
Bitrix24 is a powerful business suite for startups and enterprises to manage customers, projects, and day-to-day activities in teams and departments. Its real-time collaboration and communication tools such as chat, video conferencing, company-wide social networking allow users to negotiate issues, discuss project progress, manage documents, and do other things essential for startup project management.
Bitrix24 feature highlights:
If you are not entirely satisfied with Bitrix24, here are six worthy alternatives to give a try:
Zoho is well known as a comprehensive CRM and sales management solution for growing startups and enterprises. It has a modular system that offers powerful standalone tools for salespeople, marketers, project managers, and analysts.
Zoho is available as an all-in-one solution which you can use for free. You can add up to three users to manage your marketing campaigns and project management initiatives in Zoho. An extended paid version offers workflow management, advanced analysis, and custom reporting.
When it comes to business communication software, MS Skype is often considered the #1 solution for startups. But not this time. It is Zoom, an online meeting assistant to organize and run audio and video meetings, calls, and chats. Zoom.us is one of the most reliable teleconferencing solutions available on the market today.
The free version of Zoom allows for up to 100 participants and 40 minutes in one HD video conference. The software lets easily see who is currently speaking and share files and docs. Participants can use Zoom’s client application and a mobile phone to dial into the conference.
Breakout room is an excellent feature for project teams and groups to join the same conference with full audio, video, and screen share capabilities. The data transmission during the call is SSL encrypted. The software is available for mobile and desktop users.
There are now a number of money transfer solutions for startups and enterprises. TransferWise is one of the most reliable and affordable online payment services available at competitive rates and fees for businesses.
TransferWise offers easily executable payouts API and dashboards to make one-off transfers and batch payments for projects as well as manage currencies and pay suppliers and contractors for project management services.
Here are a few benefits TransferWise has to offer for emerging startups and small businesses:
Payoneer, Skrill, Neteller, and Entropay can be great TransferWise alternatives for small businesses. Take a look at each, and see if one seems right for your startup project management.
Originally published at https://mymanagementguide.com
Chicago-based provider of full-cycle custom software engineering and IT staffing solutions with own R&D Centers in North America and Europe.
See all (1,164)
Chicago-based provider of full-cycle custom software engineering and IT staffing solutions with own R&D Centers in North America and Europe.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-toolbox-running-cli-in-docker-263540debb74?source=search_post---------201,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 15, 2018·2 min read
The Alibaba Cloud Command Line Interface (CLI) is a unified tool to manage your Alibaba Cloud services. With just one tool to download and configure, you can control multiple Alibaba Cloud services from the command line and automate them through scripts.
The CLI uses the SDK of various products internally to achieve the intended results. This installation can be hard to maintain considering the frequent releases of new SDK versions. This can also be cumbersome if you don’t have access to a machine with the pre-requisites installed.
Docker comes in handy in this situation. The key benefit of Docker is that it allows users to package an application with all its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have the high overhead and hence enable more efficient usage of the underlying system and resources.
This document explains a pre-packaged Alibaba Cloud CLI Docker image that could be used to launch the CLI at will. It also discusses a way to make the container data persistent.
To get started, the Alibaba Cloud CLI Docker image needs to be fetched. Depending on your use case run one of the commands below (this will also fetch the latest image if one had not been fetched earlier):
For both of these invocations, the CLI must then be configured:
Note: The docker image can be configured to have the initial CLI configuration to be persisted. This should only be done in computers that the user has trust in. The parameter highlighted needs to be replaced.
Run the below command to pass the local folder reference that will store the CLI parameters
Environmental variables are also honored:
This document explains a simple way to access the Alibaba Cloud CLI without going through the complexities of maintaining an environment on its own. This will also free the host computer from the dependencies that will be installed as part of the CLI installation.
Reference:
https://www.alibabacloud.com/blog/alibaba-cloud-toolbox---running-cli-in-docker_593883?spm=a2c41.11873527.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@patrickhq/team-intuz-reviews-aws-on-clutch-co-745e13eaeae8?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pratik Rupareliya
Oct 3, 2016·6 min read
Intuz holds a proven experience and expertise in offering wide range of cloud services, especially for Amazon Web Services. We have a team of AWS certified architects and developers who excel in delivering intelligent cloud solutions. By considering Intuz’s passion for cloud technology, a leading B2B review and research provider firm Cluch.co conducted the review of AWS from Intuz as its customer. Here is the deserving review of AWS provided by Mr. Patrick (Director of Growth at Intuz) to the Clutch team.
Original source: https://clutch.co/cloud/review/scalability-cost-efficiency-aws
Recommendations:
Intuz utilizes a variety of cloud technologies with a focus on the suite of tools provided by AWS [Amazon Web Services]. By utilizing these resources, Intuz has been able to increase operational efficiency and decrease infrastructure costs. AWS is recommended to clients who have a need for on-demand scalability.
Introduce your business and what you do there.
Intuz Group is a global custom mobile development and cloud solutions company. Intuz offers a wide range of IT Solutions such as mobile application development, cloud services, AWS solutions, UI/UX [user interface/user experience] design, app marketing, custom web applications, and custom software development.
With over 12 years of experience in the field of Information Technology, Patrick is a techno-commercial leader heading Intuz as Director of Growth.
What business challenges have you been able to overcome by utilizing these technologies?
Cloud technologies can overcome multiple business challenges, some of which we have been experiencing with our clients.
One of our clients was experiencing increased traffic and was finding it difficult to scale resources instantaneously to meet the increasing demand. So by using AWS, we overcame this scalability issue by providing them prompt scalable resources and load balancing solutions.
Few of our clients needed rapid cloud deployment and focus more resources on product development. The AWS cloud provides very prompt setup and configuration of IT infrastructure, sometimes within minutes, and enables increased productivity for our clients.
Migrating and hosting to cloud providers like Rackspace and AWS has allowed us to reduce operational costs while increasing IT effectiveness.
We have developed several real-time products and solutions that needed seamless transfers and instant access across multiple nodes around the globe. AWS’s global presence with high-end cloud infrastructure at almost all prominent places vastly improved time to deployment and resource availability for businesses who wanted to expand geographically.
In scenarios where an on-prem data-center-wide disaster recovery system was too tedious of a job with expensive infrastructure and implementation, we were able to implement Cloud DR systems much more quickly and easily while allowing far better control over resources.
How do you implement cloud technologies?
We have a separate division of cloud solutions in our company with AWS certified architects, developers, and operational members who can implement AWS technologies for clients. Below is the list of the core AWS services that we are providing to clients:
How to costs compare to other infrastructure options?
On-premise systems have high hardware costs, and along with it comes the high maintenance and upgrade costs as well. Technologies like visualization also allow efficient utilization of resources but lack the scalability and on-demand cost effectiveness of a true cloud solution. In cloud technology, you only pay for exactly what you use and how long you use it, so you don’t waste your money on buying extra resources. So, while an on-premise system can cost thousands of dollars, migrating to a cloud would only cost a few hundred every month depending on your requirements and commitments.
In comparing technologies, it’s a price tango between AWS and Azure, with the latter following the price drops and changes of the former, while the Google Cloud strives to get more users by making prices more competitive. Check how the 3 market leaders fare up to the costing structure.
What are your reasons for using these specific platforms?
These platforms are industry leaders in providing highly efficient cloud ecosystems. Our preference of AWS comes from the fact that it is effectively documented, has seamless integrations and has the most widespread tools and services available on a single platform.
Were there any software features/tools that you were really impressed by?
Outsourced IT
The day-to-day administration, care, and feeding of supporting systems moves away from you to the service provider. This could free up internal IT resources for higher-value business support and allow you to put IT budget dollars toward efforts that advance your business.
Quick Setup
Cloud setup and deployment is relatively quick and easy. Plus, servers, appliances, and software perpetual licenses go away when you use such a service.
Pay-as-you-go
An example could be found in Software-as-a-Service (SaaS) applications available today that allow the off-loading of basic IT requirements to cloud service providers. You pay for what you need and use, but you do not have to continue to invest in many of the products used to support the network and systems, such as spam/anti-virus, encryption, data archiving, email services, and off-site storage.
Scalability
By using the AWS cloud, you can also temporarily scale your IT capacity by off-loading high-demand computer requirements to an outside provider. As a result, as mentioned above, you pay for only what you need and use, only at the time when you need it. This also allows dynamic experimentation and prototyping.
Looking back, are there any areas of the platform that you feel could be added or improved upon?
By default, AWS does not allow you to take all the resources you need. Indeed, there are hard and soft limits that you should understand, before deploying to AWS. Root EBS volumes of Marketplace AMI [Amazon Machine Images] instances cannot be mounted on non-root devices. Also, sometimes individual resource price is higher than the other providers.
Have you had to interact with the platform’s support team or reference their support resources?
AWS web support resources are very insightful and helpful in building AWS ready systems as well as to perform all migration, deployment, and recovery activities on AWS. The AWS APN Portal allows knowledge up-gradation to AWS partners along with knowledge sharing, training, and allowing seamless 24/7 connectivity to AWS Partner Teams. Time to time, we have received excellent support from the AWS team to solve various issues during implementation, and they have helped us to understand the overall AWS ecosystem better. Coordination between the cloud providers, solution experts, and end-clients is always a key, and the support staff at these technology providers have been able to provide a seamless coordination. We also have a certified AWS Solution Architect and DevOps team in-house that enables instant support and intelligence for all cloud-related activities for our own team as well as to our clients. On one occasion, while migrating our client’s infrastructure to AWS, the email outgoing from the application ended abruptly and needed a solution immediately. Our DevOps team was able to identify the issue as a soft-limit by AWS on non-amazon mail servers. We contacted the support team, discussed our situation, and they were very prompt in removing the limits. This allowed a real smooth migration with continuous availability of all app
OVERALL SCORE: 5
AWS has been a great experience overall. With the amount of tools available through AWS and the lack of immediate competition, AWS is one of the best cloud providers right now in the market.
Originally published at blog.intuz.com on October 3, 2016.
Techno-commercial leader heading Intuz as head of Strategy.
Techno-commercial leader heading Intuz as head of Strategy.
"
https://medium.com/@richarddolan/to-make-the-most-of-the-cloud-global-msps-are-key-e87951089658?source=search_post---------203,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Dolan
Jun 16, 2016·3 min read
It would hardly be an exaggeration to say that the rise of managed cloud services is one of the most significant IT stories in recent years. Cloud computing is now firmly established as an essential resource for businesses of all kinds, offering organizations the opportunity to add new IT capabilities while improving efficiency and driving down costs.
In order to take full advantage of the cloud’s potential, businesses must seek out and partner with the right managed services providers. With this in mind, it’s imperative for many firms, especially large or expanding enterprises, to look for global MSPs. Trying to operate an international organization without the support of such an MSP will lead to complications, inefficiencies and disappointment.
The growing significance of MSPs can be seen in a study from MarketsandMarkets of the global cloud managed services market. According to this study, this market is on pace for major growth in the coming years. The total value of this market is projected to reach $118.4 billion in 2020, up from $52.2 billion this year. This represents a compound annual growth rate of 15.5 percent for this period.
Obviously, such figures clearly demonstrate the demand for cloud MSPs. However, they also hint at the increasingly global nature of cloud operations. Hosted solutions are becoming increasingly popular in every region in the world, and while some of this is driven by cloud adoption among smaller firms, much is also inevitably the result of expanding cloud efforts among multinational enterprises.
For these latter businesses, a global MSP is critical to any comprehensive cloud strategy, for several reasons.
Probably the single biggest reason why a global MSP is so important for enterprises is the issue of latency. It takes time for data to travel from a cloud service provider’s data center to its customers’ end users, and the greater the distance, the longer the wait. For a small or midsized firm, this is likely not much of an issue, as they can simply take advantage of a regional MSP’s offerings. However, the same is not true for a company with multiple branch offices and remote workers scattered across the globe. For such an enterprise, latency can become a serious drag on productivity, as well as employee morale.
By choosing a global MSP with established data centers in multiple cities around the world, enterprises can avoid such complications, as there will always be a viable data center within each branch’s region. This not only reduces or eliminates the risk of latency for end users, but also enables enterprises to embrace the follow-the-sun model for maximum productivity and collaboration throughout the organization. Without an MSP partner with globe-spanning data centers, follow the sun becomes fraught with difficulties, creating more problems than it solves. With a global cloud MSP’s help, though, this strategy can yield tremendous dividends for enterprises, providing a serious competitive advantage.
To gain the full benefits that a global cloud has to offer, it’s critical for enterprises to not only partner with a global MSP, but also to choose the right MSP. Many MSPs remain regional, while not every “global” MSP has a genuinely global reach.
Richard is the Senior Vice President of Marketing at Datapipe
Richard is the Senior Vice President of Marketing at Datapipe
"
https://medium.com/@auth0/japans-digital-transformation-driving-public-cloud-spend-21c208b0229b?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Auth0
Apr 23, 2018·3 min read
Shedding legacy systems through digital transformation is increasing the global spend on public cloud services, according to a recent IDC forecast. By the end of 2018, worldwide spending on public cloud is expected to reach $160 billion — a 23.2% increase over 2017. While the United States is still spending the bulk of those billions, Japan’s rapidly growing market is expected to contribute at least $5.8 billion to that spend.
With increased reliance on cloud services, comes an increased awareness of the potential for cyberattacks, especially since Japan’s government cloud faces consistent attacks. On April 3, Japan’s National Center of Incident Readiness and Strategy for Cybersecurity warned that official email addresses for about 2,000 government officials had been leaked, reports the Japan News, noting that each year, government agencies face more than one million attacks.
“Without the right protections,” says Auth0 CISO Joan Pepin, “government cloud can be at least as vulnerable as public cloud. Authentication — making sure that users are who they claim to be — is a foundational security control.”
Certain industries, like the discrete manufacturing, manufacturing, and professional services industries that will take up to 43% of Japan’s spend, are turning to IoT to increase their capabilities, according to IDC. While these devices offer rapid, non-touch action as well as the collection of massive amounts of data, they also open organizations to various vulnerabilities, since the IoT industry lacks a consistent set of standards and protocols.
More than 20 billion connected devices are expected to be online globally by 2021, according to Gartner, bringing with them the potential for geometrically increasing risk. These vulnerabilities are easily exploited by hackers — botnet controllers increased by 140% in 2017.
““Without the right protections,” says Auth0’s @CloudCISO_Joan, “government cloud can be at least as vulnerable as public cloud. Authentication — making sure that users are who they claim to be — is a foundational security control.””
TWEET THIS
Given all the cybersecurity risks, Japan, like several other governments around the globe, took significant steps to protect personal data when the Act on the Protection of Personal Information (APPI) came into force last May. Although focused on data protection and privacy like the EU’s GDPR, APPI focuses on protecting the rights of individuals and extends protections to include personal identifier codes, and aggregated information in the “business operator’s” database. The EU and Japan are currently in discussions regarding mutual “whitelisting.”
Based on these regulations and stricter policies around data management in the region, the need for enterprises in Japan to prioritize identity management is undeniable. We addressed this demand by establishing a formal Auth0 presence in Tokyo last year, and launching our website in Japanese: https://auth0.com/jp, and are continuing to dedicate significant resources to this region. Auth0 is uniquely suited to address the challenges of Japan-based and global companies looking to expand in this growing market. If you’re based in Japan and would like to learn more about how Auth0 can create a first line of seamless and secure defense, please reach out to yuichiro.kuwano@auth0.com.
“Auth0’s authentication can help protect industries like discrete manufacturing, manufacturing, and professional services expected to take up to 43% of Japan’s $5.8B public cloud spend spend in 2018.”
TWEET THIS
Originally published at auth0.com.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
"
https://medium.com/@alibaba-cloud/using-oss-to-publish-a-static-website-a78818d8f16e?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 17, 2020·3 min read
By Alibaba Cloud Academy
Interested to learn more about Alibaba Cloud? Check out our course on Using OSS to Publish a Static Website in China and get certified today!
In this blog, we’ll talk about everything you need to host a static website. First, let’s talk about the differences between static websites and dynamic websites.
Static websites consist of a series of HTML files, each one representing a fiscal page of the website. On static websites, each page is a separate HTML file. For example, when you visit the website’s home page, the things you are viewing are the same as the actual home page file. If two pages contain a lot of identical content, such as the same filter, if you want to update the filter, you must do it twice — once on the first page and once on the second page.
Dynamic websites use server-side languages, such as PHP or GSP, to dynamically build the website when a user visits the page. When a user goes to the web address, the server finds different pieces of information, writes it onto a single, cohesive page, and presents the final version on-screen. As the name suggests, compared to a static website, on a dynamic website, the content is generated dynamically and changes regularly. Along with using server-side languages, dynamic websites may generate different HTML for each request. The page contains a server-side code that allows the server to generate unique content when the page is loaded. When building a dynamic website, a content management system (CMS) is often used to save development time and take advantage of its rich plug-ins.
Now that you know the differences between static and dynamic websites, we can talk about what you need to host a static website. To publish a static website, all you will need is a web server running an HTTP server service like Apache or Nginx on it. Then, you can upload your static website to the appropriate directory.
There is also another way to do it. You can use a web hosting service (or a service that supports web hosting) to upload your steady website. Then, you can leave the underlying O&M tasks to the service provider. You could use the Alibaba Cloud Elastic Compute Service (ECS) to get a cloud server and run an HTTP server service like Nginx on it. You could also use a web storage hosting service like Alibaba Cloud Object Storage Service (OSS).
We suggest using OSS because it supports web hosting. When comparing the O&M costs between OSS and ECS, OSS is more cost-effective. You can find the full guide by clicking here.
You could also consider creating a serverless static website, but we’ll leave that topic for another time. If you’re interested, you can learn more about it in this article.
You may be wondering, “how can I build a website in China without being in the country?” Alibaba Cloud has a simple solution. You will need an Alibaba Cloud account, a functioning OSS instance, and a valid ICP license for your website. If you’re not sure how to get an ICP license, check out this blog to learn more. You can also take the Clouder course on “Building a Static Website in China.”
Ready to test your knowledge? Take the Using OSS to Publish a Static Website in China course and get certified today!
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@jaychapel/how-msps-can-educate-customers-on-cloud-cost-models-d16f135bfe03?source=search_post---------206,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 17, 2019·3 min read
Part of the role of any managed service provider managing cloud services is to guide their customers through the process of creating and evaluating cloud cost models. This is important whether migrating to the cloud, re-evaluating an existing cloud environment, or simply understanding a monthly cloud bill. Many customers may be more familiar with on-prem cost models, so relating to that mindset is crucial. Here are a few important things to keep in mind when educating your customers about cloud costs.
One of the biggest shifts in mentality that must be made when evaluating cloud cost models is the shift from predominantly Capital Expenditures with on-prem workloads as compared to predominantly Operational Expenditures with cloud workloads.
As one of our customers explained the mindset problem:
“It’s been a challenge educating our team on the cloud model. They’re learning that there’s a direct monetary impact for every hour that an idle instance is running.”
Another contact added: “The world of physical servers was all CapEx driven, requiring big up-front costs, and ending in systems running full time. Now the model is OpEx, and getting our people to see the benefits of the new cost-per-hour model has been challenging but rewarding.”
Deploying a project in a private cloud involves lots of up-front purchases and ongoing maintenance, including servers, power, hardware, buildings, and more. On top of the actual purchase cost, you must account for amortization, depreciation, and the opportunity cost of those purchases.
Cloud workloads often work on a pay-as-you-go model, where you pay only for what services and features you use and how long you use them. This provides organizations with almost no Capital Expenditures for these resources, but results in a dramatic increase in Operational Expenditures. Neither is necessarily a bad thing, but your job as an MSP is to clearly articulate this shift so the customer can understand why the ongoing costs appear so much higher. And, of course, you’ll have to incorporate your own value into the equation.
For on-prem services, the details of the cost model don’t usually require detail about what software or service is actually running on the physical machine. A database server and a web server may have different specs, but everything becomes normalized to the physical hardware that must be purchased as a one-time fee. This provides a certain level of simplicity in your calculations, but still must account for all the additional physical factors like power, air conditioning, redundancy, cabling, racks, and maintenance.
Cloud services not only charge based on time used, but also have very different costs for each service. A database server and a web server are going to have very different cost structures, and will show up on your monthly bill as separate items. This often makes the bill look much more complex, but the flip side of that is that you have many opportunities for optimization and cost allocation.
Creating cloud cost models for your customers can require a big mental shift from other cost models, but it’s an important step for current and future IT projects. Understanding what the options are, what the costs are, and what your usage will be, are all factors. Make sure to convey all of these aspects to the stakeholders of your client in a clear way to avoid the surprise bill at the end of each month.
Ultimately, the market for cloud managed services is growing, which is good for managed service providers. As customers migrate to the cloud, they will need cost optimization expertise, which is a great angle for MSPs to get a foot in the door.
Originally published at www.parkmycloud.com on March 7, 2019.
CEO of ParkMyCloud
4 
4 
4 
CEO of ParkMyCloud
"
https://medium.com/@jystewart/getting-past-off-shoring-132b74665498?source=search_post---------207,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Stewart
Jul 20, 2017·7 min read
One of the factors many organisations (including governments) agonise over when deciding whether to use public cloud services is whether or not services and data can be stored “off shore”. It’s not a topic we tend to discuss very well.
“Off shore” usually means stored in data centres in other countries but can sometimes mean in facilities within the originating country but operated by foreign-owned companies.
For UK organisations looking at infrastructure as a service that conversation is dissipating now that the three biggest players all have UK data centres, but switching to UK data centres is really just dodging the issue rather than looking at how and why decisions are made.
It was great to hear Ian McCormack from NCSC addressing offshoring in his spot in the keynote at the AWS Public Sector Summit in DC recently.
“It’s often said to us that due to the global nature of a service it’s somehow inherently less secure than if exactly the same service was hosted on a datacentre in the UK. But actually that just doesn’t stand up to technical security outside of particular national security type applications.” — Ian McCormack, NCSC
For the vast majority of applications from the vast majority of organisations, the physical location is not a factor in confidentiality or integrity. There may be compliance requirements that force decisions on you, or there may be performance reasons to choose particular geographies, but not security.
That said, the topic comes up so often that it seems worth breaking down some thoughts on how you might approach the issue if you want to really consider the risks. Which hosting companies you use, who they’re owned by, and where their various assets are hosted should be considered within your overall risk assessment.
A risk model is only as good as your understanding of the service it’s protecting. Before starting on anything you should make sure you have a solid grasp of the service expectations, what its impact on other services is, and so on. That will help you understand any trade-offs that need to be made, and also help understand whether any offshoring concerns might be coming from.
The following thoughts are based on a set of conversations over the past couple of years. They’re far from comprehensive, but I’m regularly in situations where I find people who don’t know where to start with breaking down these issues and it seemed worth sharing even some sketchy thoughts.
Before getting into detail on the particular risks, it’s worth first considering the scenarios where the location of data might be important.
Are you solely concerned about data security, or are your concerns about making sure your services keep working in the unlikely event that all network connections out of the UK fail?
If that unlikely event is a real consideration for you would you need to get your services back up and running immediately, or will backups that let you rebuild locally be sufficient? Most of the time it’s going to be more important that you’re running in multiple locations than whether one of those locations is in your home country.
Once you understand your context, you can go into the next level of detail. Roughly speaking there are three areas of risk that people are concerned about when they trust their services to a third party, regardless of the classification of that data.
There is also a further risk that we don’t often discuss, which is that the availability of services will be disrupted due to the complexity of international network routing.
It’s worth noting that I’m assuming you are using a robust cloud provider and are applying good practices to your cloud usage so that the chances of other customers affecting your services are very small.
The risks relating to staff at cloud companies accessing your data are similar whether your data is entirely contained within the UK or is stored elsewhere.
Before worrying about where the data is, you should be thinking about what impact comes with disclosure of the data. For much of what we do simply being careful about how we use a tool will minimise that impact. For example, if we’re using a project management tool we shouldn’t include personal data or credentials in what we store.
When you do have data that needs to be restricted then many infrastructure as a service providers will share information about the measures they take to make it very difficult for their staff to access customers’ data. Increasingly details of those measures are available publicly. Those measures apply whether the data is stored in the UK or outside it.
Non-UK ownership of companies, or non-UK residence of data centres is a reality of most modern internet services. There have been a number of legal cases around the world over the past few years beginning to test to what extent governments can compel companies to provide their customers’ data to law enforcement agencies or litigants in certain cases.
The full ramifications of those cases are still unclear and the legal situation will continue to vary significantly from jurisdiction to jurisdiction, but there are other things we should consider before getting into the detailed legal situation.
Once again, we need to understand the risk associated with a court granting access to the data we have in a service. That will largely depend on the way any data that is disclosed will be handled and what guarantees we have offered to our users. Access to a very specific record granted via a warrant and committing the accessing parties to hold the information carefully, is very different from a court allowing various parties to hold full copies of a database without protection.
We then need to consider the likelihood of such an order and the practicalities of fulfilling it. These cases are extremely rare and likely to remain so.
For governments, in the very rare circumstance where such situations did arise, most foreign governments are likely to use diplomatic channels to address requests of this sort that touch on government-owned data. Not to do so would risk a diplomatic incident and that is rarely worthwhile. Those diplomatic channels give us an opportunity to find other ways to address the situation.
Regardless of whether you’re a government, where your data is stored on third-parties’ servers that doesn’t mean it’s entirely out of your control. Just because your data rests on someone else’s server doesn’t mean you can’t encrypt it and store the keys elsewhere, or take other similar steps.
There are always risks that hostile actors will want to interfere with your service, and that’s something that should be considered as part of the general threat modelling and risk assessment for a system regardless of where it’s hosted.
Infrastructure security is incredibly important but far too often people focus on that at the cost of application security, which is where the easiest to exploit vulnerabilities are usually found. Regardless of where an application is hosted you should be managing the application security appropriately. With that, you should be taking appropriate and proportionate steps to maintain the integrity of the data in your services. For example, when using large-scale public cloud services you should implement industry standard encryption of your network traffic, and thinking about how you’d detect tampering with your data at rest.
It is possible that certain types of attacks will be easier if data is in another country, particularly disruption to the availability of a service. If your service is genuinely critical, you should already have plans to make it resilient against network outages, for example by deploying software you run to multiple “regions” or by ensuring that your software-as-a-service providers do similarly.
It’s also worth noting that there are limits to what you can reasonably prepare for or protect against.
One example of where an organisation has thought ahead about that and set out to be realistic, is the threat model for UK-OFFICIAL. The UK government accepted the possibility that determined and highly capable foreign governments may be able to access some data and services:
“This model does not imply that information within the OFFICAL tier will not be targeted by some sophisticated and determined threat actors (including Foreign Intelligence Services) who may deploy advanced capabilities. It may be. Rather, a risk based decision has been taken not to invest in controls to assure protection against those threats, i.e. proportionate not guaranteed protection.”
For the vast majority of what any of us do, considerations about where to host your data and services come down to good architectural practices: are our services designed to be resilient, fault-tolerant and responsive enough to meet users’ expectations?
In some cases being comfortable hosting services off-shore brings very real advantages, not just because it gives us access to a wider market of suppliers but also because it allows for geographical resilience, or for better services to those based outside the UK.
The main thing that’s important whatever we’re doing is to maintain awareness of what we’re using and how. In a cloud-centric world that no longer means understanding every server, but having a good sense of where companies you’re dependent on are owned and operated and for high-availability services how their network connectivity is provided.
Originally published at jystewart.net on July 20, 2017.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
"
https://medium.com/@HOSTINGdotcom/test-your-disaster-recovery-dr-savvy-quiz-4576346a17d1?source=search_post---------208,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Oct 8, 2015·3 min read
According to a recent study by IDG, evaluating or investing in cloud services and disaster recovery (DR) solutions rank #1 and #3 respectively in the list of IT leaders’ top priorities for 2015. Of the 400 global IT executives polled, approximately 43% plan to invest in DR solutions. Of the those IT executives that have a DR strategy in place, 62% plan to reassess or expand it moving forward. In the spirit of fun and learning, HOSTING invites you to test your disaster recovery savvy by taking our quiz below.
If you answered mostly “A’s”– You are a DR rock star who understands that downtime is never an option for your business. HOSTING cloud security experts can review your current DR plan to uncover potential cost savings and performance enhancements.
If you answered mostly “B’s” — You are on the way to becoming a DR Rock Star, but need to step up your game. HOSTING cloud security experts can help you put together a comprehensive DRaaS solution that will generate high-fives from your executive team.
If you answered mostly “C’s” — There is still hope for you, but you need to act fast — before your next outage occurs (and it will happen). Contact HOSTING today to create and implement a DRaaS solution ASAP.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://lab.wallarm.com/this-week-march-15-17-the-first-and-foremost-world-class-technology-conference-in-the-hosting-and-e4cfd5bda6be?source=search_post---------209,"This week (March 15–17) the first and foremost world-class technology conference in the hosting and cloud services industry, WHD.global, is taking place in Rust, Germany. The trade show, established 12 years ago, brings more than 6.000 leading experts and insiders together for high-value networking, informative discussions, and shared experiences. Exhibiting companies include Acronis (a Wallarm partner), Dell, HP, Lenovo, Odin, AMD, Samsung, and Microsoft.
Wallarm, the leading provider of cloud-capable Web Application Security solutions for enterprises and service providers will also be present. If you are interested in learning about our product, how it’s different from traditional WAFs, or in seeing an online demo, please contact our VP Sales (EMEA) Stephan Masyuta-Hesslich at sales@wallarm.com.
I agree to Wallarm Privacy Policy.
Webinars
More insights
Subscribe for the latest news
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/@alibaba-cloud/5-ways-to-minimize-the-security-risks-of-the-cloud-e342e89d0e0e?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 4, 2018·2 min read
Like real clouds, cloud services have holes. The Cloud Security Alliance (CSA) has warned that the shared and on-demand nature of cloud computing introduces the possibility of new security risks that can erase any gains made by the switch to cloud technology1.
Below is a list of five ways enterprises can minimize security risks when moving to the cloud.
Organizations can curtail the danger to their applications and data in the cloud if they replicate the same security processes that have worked for them on premise in the cloud, wherever possible. This measure will also build confidence in moving data over to the virtual cloud given that tried and tested methods will be employed to protect data.
It’s imperative to investigate a cloud service provider before any decision is made to adopt their services. Experienced providers understand the specific security needs of different industries, have multiple security measures available and provide timely and accessible support. Adequate understanding and careful review of any agreement to be made with a provider is also essential, especially with regard to their data breach policy as this is critical for handling emergencies.
The massive amount of data normally stored in the cloud makes the prospect of a data breach frightening since there’s a lot to lose. The CSA has recommended organizations use multifactor authentication and encryption to protect against data breaches of the cloud. A comprehensive security solution should also employ ongoing data monitoring.
When companies make the decision to transfer data and applications to the cloud, prior to the migration taking place, pre-emptive steps can be taken to reduce the security risks once the move is actually made. For example, CIOs can ensure their staff are trained with how to use the cloud securely once it is instituted in order to avoid security mishaps arising from human error following migration.
Gartner estimates that less than one-third of enterprises have a documented cloud strategy and even with the increasing adoption of the cloud, mapping out a cloud strategy is still perplexing to CIOs. This is unfortunate, because a cloud strategy can help ensure an organization is fully up to speed regarding how to protect data. An effective strategy should encompass all dimensions of security, including how to keep data secure, what to do if data is breached, what data is too risky to move to the cloud (such as confidential data) and so forth.1https://chapters.cloudsecurityalliance.org/southwest/files/2016/04/The-2016-Dirty-Dozen.pdf
Reference:
https://www.alibabacloud.com/blog/5-ways-to-minimize-the-security-risks-of-the-cloud_p69783?spm=a2c41.11109582.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@richarddolan/gartner-reports-by-2020-cloud-shift-will-affect-more-than-1-trillion-in-it-spending-e2e089322343?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Dolan
Jul 26, 2016·2 min read
“Cloud-first strategies are the foundation for staying relevant in a fast-paced world. The market for cloud services has grown to such an extent that it is now a notable percentage of total IT spending, helping to create a new generation of start-ups and “born in the cloud” providers.” — Ed Anderson, research vice president at Gartner.
Last week Gartner announced that more than $1 trillion in IT spending will be directly or indirectly affected by the shift to cloud during the next five years, making cloud computing one of the most disruptive forces of IT spending since the early days of the digital age. If you missed the news, here are the highlights as reported by Gartner:
There’s no question that cloud growth is on the rise. In fact, check out one of our favorite articles on cloud market predictions and analysis in Forbes here: Roundup of Cloud Computing Forecasts And Market Estimates, 2016. Another Forbes favorite illustrates how managed services that complement your virtual infrastructure are often the lynchpin to cloud success: Managed Services: The Key To Your Cloud Strategy Success.
To read a detailed analysis regarding Gartner’s latest news, see Market Insight: Cloud Shift — The Transition of IT Spending from Traditional Systems to Cloud.
If you are unfamiliar with Datapipe’s history with the leading analyst firm, be sure to check out the following pieces:
Richard is the Senior Vice President of Marketing at Datapipe
See all (297)
Richard is the Senior Vice President of Marketing at Datapipe
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@womenwhocode/looking-back-at-forge-devcon-2016-39c344d76ad1?source=search_post---------212,"Sign in
There are currently no responses for this story.
Be the first to respond.
Women Who Code
Jul 26, 2016·2 min read
Looking Back at Forge DevCon 2016
June 15th — 16th I attended Forge DevCon held by Autodesk. I met interesting people, I learned about the APIs and cloud services that make up the Forge platform, and I experienced a ton of new technologies.
As a self taught web developer with a little less than a year of programming/CS experience, almost every piece of the conference was completely new to me, which made the experience even more fun and exciting.
In between keynotes and sessions, I made sure to stop by every booth to play with their cool VR/ augmented reality technologies and check out various 3D scanning devices/software.
During the speaker sessions, I frequented the SF JavaScript track.
Async JavaScript
* My takeaway: Async in JavaScript is still a challenge being taken on with different approaches.
Build 3D components for the web
The conference came to an end with a series of mesmerizing 3D web art. All of the visualizations and demos were live. Everyone loved it.
A big thank you to Women Who Code for sponsoring my ticket to Forge DevCon! It has been an incredible experience and has encouraged me to explore the web even deeper.
Original Post
[
]
(https://www.womenwhocode.com/donate)
Originally published at www.womenwhocode.com.
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
"
https://medium.com/@krmarko/the-inexorable-rise-of-cloud-it-rightscale-takes-the-pulse-2b0f9f899e96?source=search_post---------213,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Feb 19, 2015·5 min read
Those of us that expound the benefits of cloud services must calibrate our analysis and opinions with real world data about what actual operators of business IT systems are doing. Although logic and historical analogies may argue that utility IT services will inevitably displace most on-premise infrastructure, actual observation can throw logic right out the window. For several years, RightScale, one of the leading developers of cloud management software, has canvassed its customer database to assess the state of the cloud, which this year is best summarized as healthy, maturing, but still developing.
The survey’s headline number finds 93% of respondents using cloud services, most on both private and public infrastructure, but adoption is wide, not deep. Noteworthy is the fact that 30% of these only use public cloud, a figure that’s undoubtedly skewed by the fact that 45% of respondents hail from organizations with fewer than 100 employees. Indeed, as I wrote last year, the ethos among tech startups is to keep IT infrastructure as lean as possible, owning little more than some laptops and a broadband router, while using the cloud for all the things corporate IT departments traditionally provide. While SMBs are more “cloud focused” by RightScale’s definition, a greater share of large enterprises, 83%, are using cloud services in some form.
Most large enterprises employ a hybrid of public and private cloud, with just 23% opting for an all-public strategy. Doing the math (since RightScale doesn’t break this out separately), that implies about a third of smaller organizations rely solely on public cloud. Despite broad cloud adoption, RightScale’s data show that most large enterprises are still kicking the tires and not yet fully committed to cloud infrastructure. Over two-thirds of respondents have less than 20% of their application portfolio running on clouds. Of the remainder, it’s doubtful many run more than half of their workloads.
Companies clearly aren’t yet ready to cede total control of IT infrastructure to the likes of Amazon and Microsoft Microsoft. The reluctance isn’t only due to IT foot-dragging, but evidence of shortcomings in legacy applications that render many infeasible to operate on shared, virtualized cloud systems. The survey finds almost half of organizations say less than 20% of their applications are even cloud-ready.
RightScale’s data does show an encouraging increase in cloud understanding and maturity among central IT organizations. For example, the security bug-a-boo, long a source of cloud FUD, is waning as 41% of respondents cite it as a significant challenge, down six points from last year. Indeed, 28% of respondents in corporate IT now view public cloud as a top priority, up 10 points from a year ago.
Adopting DevOps, that is merging software development, QA and IT operations into a lean, agile organization committed to constant innovation and fast failure, is now standard practice at two-thirds of the organizations represented in RightScale’s survey, with large enterprises leading SMBs in adoption. Of DevOps users, Docker application containerization is red hot, with nearly half either using or planning to deploy the lightweight alternative to system-level VMs. Given containerization’s benefits of lower resource usage, portability and rapid deployment across systems and consequent higher level of code sharing, and with all the major cloud providers offering some form of Docker service, we expect usage to ramp substantially in the coming year.
Cloud proponents, including me, often tout its cost savings, while many organizations like the financial flexibility provided by moving fixed capital expenses to recurring and adjustable operating costs. While important, these benefits are among those least cited by Rightscale’s respondents as cloud benefits. Instead, cloud adopters are induced by the instant availability, scalability and high availability of cloud services. Indeed, speed, agility and reliability trump cost and performance and cost as key drivers of cloud service growth.
When it comes to public cloud usage, AWS remains the most popular, however its growth among RightScale respondents is plateauing. Meanwhile, use of Microsoft Azure IaaS doubled since last year’s survey. In a sign that the old tech enterprise IT providers are playing catch-up in the cloud era, VMware VMware vCloud AIr, IBM IBM Softlayer and HP Helion Cloud brought up the rear. Indeed, vCloud and Helion actually lost share over the past year. The data confirms what I and other analysts have predicted, that economies of scale favor the largest cloud services. In summarizing one of the four disruptive factors affecting the data center market, Gartner put it this way, “Traditional managed service providers (MSPs) and infrastructure providers are failing to deliver compelling alternatives to platform as a service (PaaS) from Amazon, Google Google, IBM, Microsoft and Baidu. MSPs are relegated to providing basic transport, or, at best, become managed service brokers. Amid this churn, traditional vendors find it increasingly hard to compete.”
While private clouds are still growing, even among large enterprises, private cloud usage has peaked. The share of respondents using VMware vSphere, vCloud and Microsoft System Center was virtually unchanged since last year. At SMBs, private cloud adoption dropped across the board.
One could quibble that RightScale’s survey isn’t a statistically ideal measure since there’s some selection bias in only questioning those that have already expressed an interest in cloud software, however only 24% are RightScale users. However, with over 900 respondents, about a third of them from large enterprises with at least a thousand employees, and a good cross-section of respondents from various industries, regions, job categories and level, RightScale’s data is an important indicator of cloud adoption trends, product usage and concerns.
Although cloud adoption is broad and shallow, with a strong preference for hybrid public/private systems, the momentum is clearly towards the largest public services like AWS, Azure and Google. By exploiting growing cost advantages, higher efficiency and resource utilization and a richer set of platform services, public clouds will increasingly be the infrastructure of choice for new business applications and technology services.
Originally published at www.forbes.com on February 18, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
See all (132)
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/open-cloud-alliance-openness-as-an-imperative-adc3f656d3f6?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·1 min read
Future enterprise customers will be accessing a mixture of proprietary on-premises IT, hosted cloud services of local providers and globally active cloud service providers. This is a major opportunity for the market and all participants involved. This is especially so for small hosters with existing infrastructures, as well as for system integrators with the appropriate know- how and existing customer relations.
In light of this, in this strategy paper Crisp Research investigates the challenges that both provider groups are facing in this situation, and deals with the most important aspects and their solutions.
The strategy paper can be downloaded under “Open Cloud Alliance — Openness as an Imperative“.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@jaychapel/cloud-per-second-billing-how-much-does-it-really-save-b020c8f70b01?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Nov 10, 2017·4 min read
It has been a little over a month since Amazon and Google switched some of their cloud services to per-second billing and so the first invoices with the revised billing are hitting your inboxes right about now. If you are not seeing the cost savings you hoped for, it may be a good time to look again at what services were slated for the pricing change, and how you are using them.
Starting with the easiest one, Google Cloud Platform (GCP), you may not be seeing a significant change, as most of their services were already billing at the per-minute level, and some were already at the per-second level. The services moved to per-second billing (with a one-minute minimum) included Compute Engine, Container Engine, Cloud Dataproc, and App Engine VMs. Moving from per-minute billing to per-second billing is not likely to change a GCP service bill by more than a fraction of a percent.
Let’s consider the example of an organization that has ten GCP n1-standard-8 Compute Engine machines in Oregon at a base cost of $0.3800 per hour as of the date of this blog. Under per-minute billing, the worst-case scenario would be to shut a system down one second into the next minute, for a cost difference of about $0.0063. Even if each of the ten systems were assigned to the QA or development organization, and they were shut down at the end of every work day, say 22 days out of the month, your worst-case scenario would be an extra charge of 22 days x 10 systems x $0.0063 = $1.3860. Under per-second billing, the worst case is to shut down at the beginning of a second, with a highest possible cost for these same machines (sparing you the math) being about $0.02. So, the best this example organization can hope to save over a month with these machine with per-second billing is $1.39.
On the Amazon Web Services (AWS) side of the fence, the change is both bigger and smaller. It is bigger in that they took the leap from per-hour to per-second billing for On-Demand, Reserved, and Spot EC2 instances and provisioned EBS, but smaller in that it is only for Linux-based instances; Windows instances are still at per-hour.
Still, if you are running a lot of Linux instances, this change can be significant enough to notice. Looking at the same example as before, let’s run the same calculation with the roughly equivalent t2.2xlarge instance type, charged at $0.3712 per hour. Under per-hour billing, the worst-case scenario is to shut a system down even a second into the next higher hour. In this example, the cost would be an extra charge of 22 days x 10 systems x $0.3712 = $81.664. Under per-second billing, the worst case is the same $0.02 as with GCP (with fractions of cents difference lost in the noise). So, under AWS, one can hope to see significantly different numbers in the bill.
The scenario above is equally relevant to other situations where instances get turned on and off on a frequent basis, driving those fractions of an hour or a minute of “lost” time. Another common example would be auto-scaling groups that dynamically resize based on load, and see enough change over time to bring instances in and out of the group. (Auto-scale groups are frequently used as a high-availability mechanism, so their elastic growth capabilities are not always used, and so savings will not always be seen.) Finally, Spot instances are built on the premise of bringing them up and down frequently, and they will also enjoy the shift to per-second billing.
However, as you look at your cloud service bill, do keep in mind some of the nuances that still apply:
Overall, per-second billing is a great improvement for consumers of cloud resources…and will probably drive us all more than ever to make each second count.
Originally published at www.parkmycloud.com on November 10, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@mattculbertson/the-emergence-of-identity-and-access-threat-prevention-as-explained-by-451-research-293907d9bcae?source=search_post---------216,"Sign in
There are currently no responses for this story.
Be the first to respond.
Matt Culbertson
Feb 6, 2019·3 min read
Enterprises continue to embrace cloud-based architectures, and cloud services are a significant contributor to a forecasted $3.8 trillion in IT spending this year. But increasingly, organizations are finding the one-size-fits-all cloud approach to be obsolete. For many workloads and services, firms are surprisingly moving assets back to on-prem and hybrid environments to address unique challenges like network complexity and a chronic shortage of security staff.
"
https://medium.com/@jaychapel/yes-its-okay-for-msps-to-help-their-customers-save-money-on-cloud-c1b3aee58f88?source=search_post---------217,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 23, 2016·3 min read
The concept of a managed service provider (MSP) helping customers to save money on cloud services is a tricky one. If customers purchase cloud services through the MSP, it may seem that reducing the amount the customers spends on cloud would reduce the MSP’s revenue. And is the idea of “saving money” really an outcome customers seek from their service providers?
We’ve been grappling with some of these questions lately. We spent our first year on-boarding customers directly to the ParkMyCloud platform. Recently, we turned our attention to potential partnerships with MSPs and Cloud Consulting firms. When we talk with them, we ask: what are your clients’ key priorities? How do you seek to deliver additional value?
In our various conversations with MSPs, we pay attention to how they prioritize helping their customers save money. We heard that although cost reduction for clients was seen as important, it was often framed as a way for customers to get more bang for the buck — not as a reduction in total spend. MSPs reported that their clients typically have annual budgets that MSPs can spend on their behalf across all cloud or IT services. Therefore, staying within budget across all services was the primary goal, but any dollar saved on cloud compute services could then be put to work in other areas of the business. This keeps the end user satisfied by giving them more value per dollar, and the MSPs satisfied by providing more, and stickier, services to their customers.
In addition to cost savings, MSPs want to deliver productivity gains to clients. This can be done by directly implementing solutions on clients’ behalf. Increasingly, however, MSPs prefer to put tools in place that their clients can then use to optimize their own cloud infrastructure. Although many small businesses don’t have the technical expertise necessary to migrate their technology infrastructure to the cloud, once they are up and running, they are often able to self-manage parts of their own infrastructure.
As one MSP recently said to us, “we could probably write custom scripts for our customer to turn things on and off, but that really doesn’t scale. To be honest, I think they would prefer controlling their own environment”.
We know from our own experience that a number of our clients use ParkMyCloud as their go-to tool for scheduling, managing and reporting on their EC2 usage.
As the role of the traditional MSP continues to evolve, the most successful providers increasingly seem to understand that :
Although there will be many goals against which MSPs and cloud consultants are measured, it seems clear that reducing/optimizing cloud spend and empowering customers with the right tools to manage cloud are two side of the same coin and key for MSPs to succeed.
[1]Managed Services Face Uncertainty Over Cloud. http://blogs.wsj.com/cio/2016/06/30/managed-services-face-uncertainty-over-cloud-report/
CEO of ParkMyCloud
1 
1 
1 
CEO of ParkMyCloud
"
https://medium.com/@yakalot/i-havent-noticed-as-much-of-this-maybe-i-m-on-at-weird-times-or-readers-might-be-on-a-different-1d5a32fd8eb7?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tim Colby
Dec 31, 2021·2 min read
Sherry McGuinn
I haven't noticed as much of this, maybe I'm on at weird times or readers might be on a different server than writers in the partner pgm. Don't know. Yeah, I've seen strange behavior from time to time but not as periodic as you report.
So now I'm going to guess wtf is going on, because, just because. Assume Medium has a crap load of contractors like, cloud services, IT dept. (really? they got one of those?), contracted pgm services, and no doubt more tons of complexities. So Medium is running what appears to be an always up pgm 24/7/365 and has to do updates all the time and sometimes make patches on the fly for this and that. Okay, what could possibly go wrong in this covid-labor-shortage-supply-f'k-ups-wonder-f'k era we is all in?
Is this repetative occurrence at a time of day when the overall netwwork traffic load is high all over the place? Is the main Medium pgm having processing speed bumps when some extra daily updates are scheduled? Does anything run itself, always? Software has come a long way over the decades but it ain't perfect by any stretch, just close to keeping it going, IMO. How deep is the Medium IT pgm bench, hello, is there anyone left over there or is it all contracted out? 🤷‍♂️
I know it sounds like I'm making excuses for this product, and, I guess I am...? How about this, forget Medium, go to any business nowadays, how well is it running? Huh? Okay, maybe all the marketing depts. are working everywhere, sort of, but lots of other stuff is almost held together with duct tape and bailing wire.
Now have a happy new year. :-)
Grad: Whats-a-mata-U, Mayor: Foggybog, Wi., Awards: Medium response run-on-sentence-king, Medium response all-over-the-place trophy
See all (271)
25 
1
25 claps
25 
1
Grad: Whats-a-mata-U, Mayor: Foggybog, Wi., Awards: Medium response run-on-sentence-king, Medium response all-over-the-place trophy
About
Write
Help
Legal
Get the Medium app
"
https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249?source=search_post---------220,"The year was 2012 and operating a critical service at Netflix was laborious. Deployments felt like walking through wet sand. Canarying was devolving into verifying endurance (“nothing broke after one week of canarying, let’s push it”) rather than correct functionality. Researching issues felt like bouncing a rubber ball between teams, hard to catch the root cause and harder yet to stop from bouncing between one another. All of these were signs that changes were needed.
Fast forward to 2018. Netflix has grown to 125M global members enjoying 140M+ hours of viewing per day. We’ve invested significantly in improving the development and operations story for our engineering teams. Along the way we’ve experimented with many approaches to building and operating our services. We’d like to share one approach, including its pros and cons, that is relatively common within Netflix. We hope that sharing our experiences inspires others to debate the alternatives and learn from our journey.
Edge Engineering is responsible for the first layer of AWS services that must be up for Netflix streaming to work. In the past, Edge Engineering had ops-focused teams and SRE specialists who owned the deploy+operate+support parts of the software life cycle. Releasing a new feature meant devs coordinating with the ops team on things like metrics, alerts, and capacity considerations, and then handing off code for the ops team to deploy and operate. To be effective at running the code and supporting partners, the ops teams needed ongoing training on new features and bug fixes. The primary upside of having a separate ops team was less developer interrupts when things were going well.
When things didn’t go well, the costs added up. Communication and knowledge transfers between devs and ops/SREs were lossy, requiring additional round trips to debug problems or answer partner questions. Deployment problems had a higher time-to-detect and time-to-resolve due to the ops teams having less direct knowledge of the changes being deployed. The gap between code complete and deployed was much longer than today, with releases happening on the order of weeks rather than days. Feedback went from ops, who directly experienced pains such as lack of alerting/monitoring or performance issues and increased latencies, to devs, who were hearing about those problems second-hand.
To improve on this, Edge Engineering experimented with a hybrid model where devs could push code themselves when needed, and also were responsible for off-hours production issues and support requests. This improved the feedback and learning cycles for developers. But, having only partial responsibility left gaps. For example, even though devs could do their own deployments and debug pipeline breakages, they would often defer to the ops release specialist. For the ops-focused people, they were motivated to do the day to day work but found it hard to prioritize automation so that others didn’t need to rely on them.
In search of a better way, we took a step back and decided to start from first principles. What were we trying to accomplish and why weren’t we being successful?
The purpose of the software life cycle is to optimize “time to value”; to effectively convert ideas into working products and services for customers. Developing and running a software service involves a full set of responsibilities:
We had been segmenting these responsibilities. At an extreme, this means each functional area is owned by a different person/role:
These specialized roles create efficiencies within each segment while potentially creating inefficiencies across the entire life cycle. Specialists develop expertise in a focused area and optimize what’s needed for that area. They get more effective at solving their piece of the puzzle. But software requires the entire life cycle to deliver value to customers. Having teams of specialists who each own a slice of the life cycle can create silos that slow down end-to-end progress. Grouping differing specialists together into one team can reduce silos, but having different people do each role adds communication overhead, introduces bottlenecks, and inhibits the effectiveness of feedback loops.
To rethink our approach, we drew inspiration from the principles of the devops movement. We could optimize for learning and feedback by breaking down silos and encouraging shared ownership of the full software life cycle:
“Operate what you build” puts the devops principles in action by having the team that develops a system also be responsible for operating and supporting that system. Distributing this responsibility to each development team, rather than externalizing it, creates direct feedback loops and aligns incentives. Teams that feel operational pain are empowered to remediate the pain by changing their system design or code; they are responsible and accountable for both functions. Each development team owns deployment issues, performance bugs, capacity planning, alerting gaps, partner support, and so on.
Ownership of the full development life cycle adds significantly to what software developers are expected to do. Tooling that simplifies and automates common development needs helps to balance this out. For example, if software developers are expected to manage rollbacks of their services, rich tooling is needed that can both detect and alert them of the problems as well as to aid in the rollback.
Netflix created centralized teams (e.g., Cloud Platform, Performance & Reliability Engineering, Engineering Tools) with the mission of developing common tooling and infrastructure to solve problems that every development team has. Those centralized teams act as force multipliers by turning their specialized knowledge into reusable building blocks. For example:
Empowered with these tools in hand, development teams can focus on solving problems within their specific product domain. As additional tooling needs arise, centralized teams assess whether the needs are common across multiple dev teams. When they are, collaborations ensue. Sometimes these local needs are too specific to warrant centralized investment. In that case the development team decides if their need is important enough for them to solve on their own.
Balancing local versus central investment in similar problems is one of the toughest aspects of our approach. In our experience the benefits of finding novel solutions to developer needs are worth the risk of multiple groups creating parallel solutions that will need to converge down the road. Communication and alignment are the keys to success. By starting well-aligned on the needs and how common they are likely to be, we can better match the investment to the benefits to dev teams across Netflix.
By combining all of these ideas together, we arrived at a model where a development team, equipped with amazing developer productivity tools, is responsible for the full software life cycle: design, development, test, deploy, operate, and support.
Full cycle developers are expected to be knowledgeable and effective in all areas of the software life cycle. For many new-to-Netflix developers, this means ramping up on areas they haven’t focused on before. We run dev bootcamps and other forms of ongoing training to impart this knowledge and build up these skills. Knowledge is necessary but not sufficient; easy-to-use tools for deployment pipelines (e.g., Spinnaker) and monitoring (e.g., Atlas) are also needed for effective full cycle ownership.
Full cycle developers apply engineering discipline to all areas of the life cycle. They evaluate problems from a developer perspective and ask questions like “how can I automate what is needed to operate this system?” and “what self-service tool will enable my partners to answer their questions without needing me to be involved?” This helps our teams scale by favoring systems-focused rather than humans-focused thinking and automation over manual approaches.
Moving to a full cycle developer model requires a mindset shift. Some developers view design+development, and sometimes testing, as the primary way that they create value. This leads to the anti-pattern of viewing operations as a distraction, favoring short term fixes to operational and support issues so that they can get back to their “real job”. But the “real job” of full cycle developers is to use their software development expertise to solve problems across the full life cycle. A full cycle developer thinks and acts like an SWE, SDET, and SRE. At times they create software that solves business problems, at other times they write test cases for that, and still other times they automate operational aspects of that system.
For this model to succeed, teams must be committed to the value it brings and be cognizant of the costs. Teams need to be staffed appropriately with enough headroom to manage builds and deployments, handle production issues, and respond to partner support requests. Time needs to be devoted to training. Tools need to be leveraged and invested in. Partnerships need to be fostered with centralized teams to create reusable components and solutions. All areas of the life cycle need to be considered during planning and retrospectives. Investments like automating alert responses and building self-service partner support tools need to be prioritized alongside business projects. With appropriate staffing, prioritization, and partnerships, teams can be successful at operating what they build. Without these, teams risk overload and burnout.
To apply this model outside of Netflix, adaptations are necessary. The common problems across your dev teams are likely similar — from the need for continuous delivery pipelines, monitoring/observability, and so on. But many companies won’t have the staffing to invest in centralized teams like at Netflix, nor will they need the complexity that Netflix’s scale requires. Netflix’s tools are often open source, and it may be compelling to try them as a first pass. However, other open source and SaaS solutions to these problems can meet most companies needs. Start with analysis of the potential value and count the costs, followed by the mindset-shift. Evaluate what you need and be mindful of bringing in the least complexity necessary.
The tech industry has a wide range of ways to solve development and operations needs (see devops topologies for an extensive list). The full cycle model described here is common at Netflix, but has its downsides. Knowing the trade-offs before choosing a model can increase the chance of success.
With the full cycle model, priority is given to a larger area of ownership and effectiveness in those broader domains through tools. Breadth requires both interest and aptitude in a diverse range of technologies. Some developers prefer focusing on becoming world class experts in a narrow field and our industry needs those types of specialists for some areas. For those experts, the need to be broad, with reasonable depth in each area, may be uncomfortable and sometimes unfulfilling. Some at Netflix prefer to be in an area that needs deep expertise without requiring ongoing breadth and we support them in finding those roles; others enjoy and welcome the broader responsibilities.
In our experience with building and operating cloud-based systems, we’ve seen effectiveness with developers who value the breadth that owning the full cycle requires. But that breadth increases each developer’s cognitive load and means a team will balance more priorities every week than if they just focused on one area. We mitigate this by having an on-call rotation where developers take turns handling the deployment + operations + support responsibilities. When done well, that creates space for the others to do the focused, flow-state type work. When not done well, teams devolve into everyone jumping in on high-interrupt work like production issues, which can lead to burnout.
Tooling and automation help to scale expertise, but no tool will solve every problem in the developer productivity and operations space. Netflix has a “paved road” set of tools and practices that are formally supported by centralized teams. We don’t mandate adoption of those paved roads but encourage adoption by ensuring that development and operations using those technologies is a far better experience than not using them. The downside of our approach is that the ideal of “every team using every feature in every tool for their most important needs” is near impossible to achieve. Realizing the returns on investment for our centralized teams’ solutions requires effort, alignment, and ongoing adaptations.
The path from 2012 to today has been full of experiments, learning, and adaptations. Edge Engineering, whose earlier experiences motivated finding a better model, is actively applying the full cycle developer model today. Deployments are routine and frequent, canaries take hours instead of days, and developers can quickly research issues and make changes rather than bouncing the responsibilities across teams. Other groups are seeing similar benefits. However, we’re cognizant that we got here by applying and learning from alternate approaches. We expect tomorrow’s needs to motivate further evolution.
Interested in seeing this model in action? Want to be a part of exploring how we evolve our approaches for the future? Consider joining us.
By Philip Fisher-Ogden, Greg Burrell, and Dianne Marsh
Learn about Netflix’s world class engineering efforts…
12K 
32
12K claps
12K 
32
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@memority/who-are-memoritys-beneficiaries-58ecdb453d0e?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Memority
Jun 15, 2018·2 min read
Memority platform is a place where two groups of customers meet: data owners and hosters. But it’s not just the storage, it’s the self-sufficient ecosystem that includes many applications to meet the needs of business, government organizations and individuals in the super-secure storage of all kinds of valuable data.
For data owners Memority solves the problem of information storage with maximum level of security due to the storage algorithm that uses its own private blockchain network based on Ethereum. Data owner can store data in an encrypted form in a decentralized and completely secure manner, paying for the storage with MMR tokens. System automatically stores files in 10 copies at 10 random hosters, and if hosters shut down, the files are automatically copied to a new hoster. So there’s no way you’ll lose the files. Hosters can receive MMR tokens for providing their disk space to users of the Memority platform to store their data.
Above all Memority platform is the interface for creating applications by third-party developers and percentage for miners. Third-party developers will be able to implement their ideas and create their own applications, using the infrastructure of Memority, and receiving MMR tokens for this. Miners receive rewards in the form of MMR tokens for supporting the working capacity of blockchain (only 10,000 or more token owners can become miners). Memority use Proof of Authority mining, so there is no need for large computing resources 5% of payments for data storage are distributed among miners.
The most interesting group of beneficiaries from marketing and business point of view is data owners. According to Statista, during the May 2017 survey, 61% of respondents aged 18 to 29 years stated that they were using cloud storage services for their data, while 42% of respondents aged 30 to 59 years and 18% of respondents aged 60 years and older said the same. This statistics shows that the most active group of users by age is students and graduates. We can suppose that this age interval is also true for Memority’s potential hosters.
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
19.1K 
19.1K 
19.1K 
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
"
https://medium.com/@LanceUlanoff/google-photos-made-a-sad-task-easy-and-im-grateful-6d779cb0e3c0?source=search_post---------222,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Ulanoff
Jan 31, 2018·5 min read
I get it. Technology is consuming us. It’s turning our children into unhappy zombies, our social discourse into a swamp and may have helped Russia rig our elections.
It’s also wonderful and often sublime.
When my father-in-law passed away, we wanted to celebrate his memory with photos. Most of my digital photos are stored on a 1 terabyte network…
"
https://blog.gruntwork.io/cloud-nuke-how-we-reduced-our-aws-bill-by-85-f3aced4e5876?source=search_post---------223,"At Gruntwork, we write lots of automated tests for the code in our Infrastructure as Code Library. Most of these are integration tests that deploy our code into a real AWS account, verify the code works as expected, and then clean up after themselves. Sometimes, the cleanup doesn’t work when the tests run into errors or hit timeout limits.
In the early days, these leftover idle resources didn’t matter much. But by the start of 2018, our AWS bill was up to $2000/month. For a company of just 3 people (at the time), and no cloud-hosted product, this was outrageously expensive.
To solve this, we set out to build a tool that periodically goes through our AWS account and deletes all idle resources. We started out by poring through the AWS Billing Dashboard to identify the resources costing us the most money and then built out the tool to support deleting those resources. We had to be sure that we didn’t actively delete any resources while a test was running, so we designed the tool so it could delete only resources older than a specified time period (e.g. older than 24h).
We call this tool cloud-nuke. It’s an open source, cross-platform CLI application released under the Apache 2.0 License. Here’s an example of how you can use cloud-nuke to destroy all the (supported) resources in your AWS account that are more than 1 hour old:
This will delete all Auto Scaling Groups, (unprotected) EC2 Instances, Launch Configurations, Load Balancers, EBS Volumes, AMIs, Snapshots and Elastic IPs (more resources coming soon!). There’s also an extra option to exclude resources in certain regions:
We ran cloud-nuke on a nightly basis and after a few weeks of usage, in March, we saw that our bill had increased by 75% — whoops! After some digging into why, we figured out that there were two reasons: first, the team had grown to 5 people. Second, we realized that timing played an important role.
After further analyzing our spending, we realized that our testing fell into two categories:
Since we were running cloud-nuke on a nightly basis, that meant that all the automated tests could leave resources hanging around and costing us money for as long as 23 hours before being deleted. But how to delete those resources without affecting users doing manual testing? We fixed this by creating separate AWS accounts for manual testing and automated testing. We then run cloud-nuke every 3 hours in the automated testing account and every 24 hours in the manual testing account. This cut our monthly spending in half.
We were excited at the progress and wanted to see just how far we could push it. We noticed that while the spending on compute resources like EC2 instances had greatly reduced, our monthly CloudWatch requests spend was close to $500. This was particularly surprising because none of our tests made a large number of requests to the CloudWatch API.
What followed was a few days of searching through CloudTrail logs to figure out the source of the requests. After a while, we reached out to AWS support and they pointed us to an IAM role belonging to DataDog. Apparently, for DataDog to perform its primary monitoring functions it needed to make a lot of requests to the CloudWatch API — which it did very quietly. Since the integration was for a previous experiment that was no longer being actively pursued, we cut it out.
For the first time, in June, we got a bill that was less than $1000 and we’ve been spending only $500 on average since then.
There are a few main takeaways for how to reduce the cost of your testing environments:
We’re constantly looking for more ways to reduce cost as the products we build continue to evolve. In the meantime, cloud-nuke is open source and available to be used in your own cloud environment. Here are some features we’d love to add in the future (PRs are welcome!):
Take it for a spin and let us know what you think!
Get your DevOps superpowers at Gruntwork.io.
The Gruntwork Blog
1.6K 
8
Thanks to Yevgeniy Brikman. 
1.6K claps
1.6K 
8
Written by
Microsoft MVP | .NET Contributor | C# Enthusiast | Mildly Uninteresting
The Gruntwork Blog
Written by
Microsoft MVP | .NET Contributor | C# Enthusiast | Mildly Uninteresting
The Gruntwork Blog
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://onezero.medium.com/flickrs-big-change-proves-you-can-t-trust-online-services-e34af51ed4a?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Ulanoff
Nov 21, 2018·4 min read
I need a digital moving van.
I have 60 days to relocate more than 2,100 images from their home on Flickr to another safe digital haven. The once popular photo-sharing destination…
"
https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718?source=search_post---------225,"There are currently no responses for this story.
Be the first to respond.
Have you seen your application get stuck or fail to respond to health check requests, and you can’t find any explanation? It might be because of the CPU quota limit. We will explain more here.
TL;DR:We would highly recommend removing CPU Limits in Kubernetes (or Disable CFS quota in Kublet) if you are using a kernel version with CFS quota bug unpatched. There is a serious, known CFS bug in the kernel that causes un-necessary throttling and stalls.
At Omio, we are 100% Kubernetes. All our stateful and stateless workloads run completely on Kubernetes (hosted using Google’s Kubernetes Engine). Since the last 6 months, we’ve been seeing random stalls. Applications stuck or failing to respond to health checks, broken network connections and so on. This sent us down a deep rabbit hole.
This article covers the following topics.
Kubernetes (abbreviated as k8s) is pretty much a de-facto standard in the infrastructure world now. It is a container orchestrator.
In the past, we used to create artifacts such as Java JARs/WARs or Python Eggs or Executables, and throw them across the wall for someone to run them on servers. But to run them, there is more work — application runtime (Java/Python) has to be installed, appropriate files inappropriate places, specific OSes and so on. It takes a lot of configuration management, and is a frequent source of pain between developers and sysadmins. Containers change that. Here, the artifact is a Container image. Imagine it as a fat executable with not only your program, but also the complete runtime (Java/Python/…), necessary files and packages pre-installed & ready to run. This can be shipped and run on a variety of servers without any further customized installations needed.
Containers also run in their own sandboxed environment. They have their own virtual network adapter, their own restricted filesystem, their own process hierarchy, their own CPU and memory limits, etc. This is a kernel feature called namespaces.
Kubernetes is a Container orchestrator. You give it a pool of machines. Then you tell it: “Hey kubernetes — run 10 instances of my container image with 2 cpus and 3GB RAM each, and keep it running!”. Kubernetes orchestrates the rest. It will run them wherever it finds free CPU capacity, restart them if they are unhealthy, do a rolling update if we change the versions, and so on.
Essentially, Kubernetes abstracts away the concept of machines, and makes all of them a single deployment target.
OK, we understand what Containers and Kubernetes are. We also see that, multiple containers can fit inside the same machine.
This is like flat sharing. You take some big flats (machines/nodes) and share it with multiple, diverse tenants (containers). Kubernetes is our rental broker. But how does it keep all those tenants from squabbling with each other? What if one of them takes over the bathroom for half a day? ;)
This is where request and limit come into picture. CPU “Request” is just for scheduling purposes. It’s like the container’s wishlist, used mainly to find the best node suitable for it. Whereas CPU “Limit” is the rental contract. Once we find a node for the container, it absolutely cannot go over the limit.
And here is where the problem arises…
Kubernetes uses kernel throttling to implement CPU limit. If an application goes above the limit, it gets throttled (aka fewer CPU cycles). Memory requests and limits, on the other hand, are implemented differently, and it’s easier to detect. You only need to check if your pod’s last restart status is OOMKilled. But CPU throttling is not easy to identify, because k8s only exposes usage metrics and not cgroup related metrics.
For the sake of simplicity, let’s discuss how it organized in a four-core machine.
The k8s uses a cgroup to control the resource allocation(for both memory and CPU ). It has a hierarchy model and can only use the resource allocated to the parent. The details are stored in a virtual filesystem (/sys/fs/cgroup). In the case of CPU it’s /sys/fs/cgroup/cpu,cpuacct/*.
The k8s uses cpu.share file to allocate the CPU resources. In this case, the root cgroup inherits 4096 CPU shares, which are 100% of available CPU power(1 core = 1024; this is fixed value). The root cgroup allocate its share proportionally based on children’s cpu.share and they do the same with their children and so on. In typical Kubernetes nodes, there are three cgroup under the root cgroup, namely system.slice, user.slice, and kubepods. The first two are used to allocate the resource for critical system workloads and non-k8s user space programs. The last one, kubepods is created by k8s to allocate the resource to pods.
If you look at the above graph, you can see that first and second cgroups have 1024 share each, and the kubepod has 4096. Now, you may be thinking that there is only 4096 CPU share available in the root, but the total of children’s shares exceeds that value (6144). The answer to this question is, this value is logical, and the Linux scheduler (CFS) uses this value to allocate the CPU proportionally. In this case, the first two cgroups get 680 (16.6% of 4096) each, and kubepod gets the remaining 2736. But in idle case, the first two cgroup would not be using all allocated resources. The scheduler has a mechanism to avoid the wastage of unused CPU shares. Scheduler releases the unused CPU to the global pool so that it can allocate to the cgroups that are demanding for more CPU power(it does in batches to avoid the accounting penalty). The same workflow will be applied to all grandchildren as well.
This mechanism will make sure that CPU power is shared fairly, and no one can steal the CPU from others.
Even though the k8s config for Limit and Requests looks similar, the implementation is entirely different; this is the most misguiding and less documented part.
The k8s uses CFS’s quota mechanism to implement the limit. The config for the limit is configured in two files cfs_period_us and cfs_quota_us(next to cpu.share) under the cgroup directory.
Unlike cpu.share, the quota is based on time period and not based on available CPU power. cfs_period_us is used to define the time period, it’s always 100000us (100ms). k8s has an option to allow to change this value but still alpha and feature gated. The scheduler uses this time period to reset the used quota. The second file, cfs_quota_us is used to denote the allowed quota in the quota period.
Please note that it also configured in us unit. Quota can exceed the quota period. Which means you can configure quota more than 100ms.
Let’s discuss two scenarios on 16 core machines (Omio’s most common machine type).
Let’s say you have configured 2 core as CPU limit; the k8s will translate this to 200ms. That means the container can use a maximum of 200ms CPU time without getting throttled.
And here starts all misunderstanding. As I said above, the allowed quota is 200ms, which means if you are running ten parallel threads on 12 core machine (see the second figure) where all other pods are idle, your quota will exceed the limit in 20ms (i.e. 10 * 20ms = 200ms), and all threads running under that pod will get throttled for next 80ms (stop the world). To make the situation worse, the scheduler has a bug that is causing unnecessary throttling and prevents the container from reaching the allowed quota.
Just login to the pod and run cat /sys/fs/cgroup/cpu/cpu.stat.
We end up with a high throttle rate on multiple applications — up to 50% more than what we assumed the limits were set for!
This cascades as various errors — Readiness probe failures, Container stalls, Network disconnections and timeouts within service calls — all in all leading to reduced latency and increased error rates.
Simple. We disabled CPU limits until the latest kernel with bugfix was deployed across all our clusters.
Immediately, we found a huge reduction in error rates (HTTP 5xx) of our services:
We said at the beginning of this article:
This is like flat sharing. Kubernetes is our rental broker. But how does it keep all those tenants from squabbling with each other? What if one of them takes over the bathroom for half a day? ;)
This is the catch. We risk some containers hogging up all CPUs in a machine. If you have a good application stack in place (e.g. proper JVM tuning, Go tuning, Node VM tuning) — then this is not a problem, you can live with this for a long time. But if you have applications that are either poorly optimized, or simply not optimized (FROM java:latest) — then results can backfire. At Omio we have automated base Dockerfiles with sane defaults for our primary language stacks, so this was not an issue for us.
Please do monitor USE (Utilization, Saturation and Errors) metrics, API latencies and error rates, and make sure your results match expectations.
This was a wild ride and discovery. The following resources helped us a lot in understanding:
Did you encounter similar issues or want to share your experiences with throttling in containerized production environments? Let us know in the Comments below!
If you liked this article and want to work on similar challenges at scale, why not consider joining forces :)
A glimpse into how we’re engineering the future of travel
1.4K 
14
1.4K claps
1.4K 
14
Engineers and data scientists dedicated to building the future of hassle-free travel
Written by
SRE Engineer @ omio.com
Engineers and data scientists dedicated to building the future of hassle-free travel
"
https://medium.com/hackernoon/aws-vs-digitalocean-which-cloud-server-is-better-1386499a6664?source=search_post---------226,"There are currently no responses for this story.
Be the first to respond.
Sixth annual State of the Cloud Survey conducted by RightScale on the latest cloud computing trends shows that AWS continues to lead in public cloud adoption.
The cloud is usually used to refer to a few servers associated with the web that can be contracted as a part of a product or software application service. Cloud-based services can incorporate web hosting, data sharing, and software use.
‘The cloud’ can likewise refer to distributed computing, where a few servers are connected to share the load. This connection means that as opposed to using one single powerful server, complex procedures can be distributed over different smaller servers
In a cloud, there are many distributed resources acting as one. This makes the cloud very tolerant of errors, due to the distribution of data. Use of the cloud tends to lessen the creation of different versions of files, due to shared access to records and data.
DigitalOcean and AWS are cloud service platforms that offer database storage, computer power among other functionalities. DigitalOcean versus AWS is as named a David versus Goliath story with a twist. The brave upstart, DigitalOcean, faces a setup behemoth. Like David, DigitalOcean has a technique that plays to its strengths while staying away from a fight with Amazon. But this isn’t a battle until the very end. Amazon and AWS address the necessities of various groups of audiences and realizing what each does well will empower you to pick between them.
DigitalOcean (spelled as a single word; “Digital Ocean” was a 90’s-era producer of wireless communications gadgets) is a new cloud hosting supplier. Launched in 2011, DigitalOcean concentrates solely on developers’ needs. The organization currently has 9 data centers positioned in San Francisco, Singapore, Amsterdam, New York and London.
DigitalOcean concentrates on three key selling points to stand out: simplicity, pricing, and high-performance virtual servers. They zero in on giving developers an easy and quick way to set up affordable Linux instances which they call droplets. DigitalOcean supports most of the modern Linux distros; Ubuntu, Fedora, Debian, and CentOS. It is straightforward to set up several applications on their droplets e.g. Ruby on Rails, LAMP, Ghost, Docker or stack.
DigitalOcean’s pricing is the most affordable among all cloud providers. Pricing starts at $0.007/hr or $5/mo and they provide an easy transition between the hourly and monthly tariffs. Their most popular package, called a droplet costs $0.015/hr or $10/mo while providing 1 core processor, 1Gm memory, 30GB SSD disk and 2TB transfer. On AWS the closest to this is a package known as t2.small instance which goes for $0.026/hr, doubling the cost of a droplet on DigitalOcean. One more benefit is that DigitalOcean doesn’t have hidden charges for extra services like more traffic or fixed IP addresses.
DigitalOcean is known for providing very high-performance servers. Their network speed is 1Gbps; all hard disks are SSD and an incredible start-up time of only 55 secs. DigitalOcean nodes are placed at the top by independent performance tests, way far above Amazon machines.
DigitalOcean has a massive stockpile of documentation accessible for its administration, and since it is as yet a moderately basic VPS host, there is generally not much of a need for support. So, they don’t offer help by telephone and response times by email can be slow, likely on account of the sheer number of clients they are supporting.
Looking at performance vs price, DigitalOcean had a notable edge over most of its rivals for years, but that notch has since been shut as everyone followed their lead and revised their network infrastructure while lowering pricing on entry level packages. DigitalOcean is a favorite among developers due to the super fast setup times user friendliness of their platform.
Finally, DigitalOcean prides itself with a simple, user-friendly setup. Targeting developers only, providing Linux virtual machines and DNS management. It lacks hosted databases, configuration management, analytics, load balancing among others. DigitalOcean proudly markets themselves as a bare-bones IaaS provider for Linux developers.
Amazon’s AWS is the market’s leader by far; It is estimated that Amazon has as much computing muscle as the next 11 rivals on the list combined. They offer an umbrella of various IaaS and PaaS solutions. The most celebrated of them all is the EC2 IaaS solution. Other services offered by AWS include load balancing, storage, content delivery, databases, networking and content delivery, deployment and configuration management and application development platforms. They own the largest data centers in the world located strategically in 9 regions around the world.
It is evident that DigitalOcean cannot compete with Amazon’s AWS concerning features. The only area where DigitalOcean can compete is against the EC2, but even here DigitalOcean’s capacity is about 1% that of the EC2.
Amazon also has a robust database of help documentation, but with the vast size of service offering available in the portal, most clients will need to engage with support at some point or another. They have a huge support team that can accommodate this, but support is not included with all packages. Technical assistance charges can take up to 10% of your monthly expenditure, which can add up for larger organizations.
If you are already of DigitalOcean, you should congratulate yourself for making the smarter choice. If you’re on AWS, running a couple of ECM virtual machines, with high costs of bandwidth each month, it might be deserving to switch and take advantage of the free bundled bandwidth.
It is clear that the need for developer centric cloud is rising. The intent towards a ‘NoOps’ environment is evident. However, the answer to which cloud server is better solely depends on your project requirements and compliance.
Written by Dmitry Budko
Want to learn more? Check out here
#BlackLivesMatter
1.6K 
12
1.6K claps
1.6K 
12
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Collection of posts from those who build Dashbouquet https://dashbouquet.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://levelup.gitconnected.com/your-infrastructure-as-code-cloudformation-vs-terraform-34ec5fb5f044?source=search_post---------227,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Imagine if all of your infrastructure configurations from AWS, Azure or Google could be replicated faster and more accurately than you could click. This is infrastructure as code.
The insane benefits of using infrastructure as code are:
I am going to be taking a look at both Terraform & CloudFormation. I used Terraform extensively at Localz but for the sake of learning I decided to get my hands dirty with CloudFormation.
Terraform is the communities answer to infrastructure as code. It is open source with over 12k stars, is cloud agnostic supporting hundreds of providers. It is often less verbose than CloudFormation & has a great module system.
CloudFormation is developed and maintained by AWS, it is very tightly integrated and only supports AWS. The tight integration lends itself to having a great UI, being able to view all your stacks for a given account is great. It enables cross stack referencing with ease, which is also a massive win for modularity and breaking down that monolith.
Terraform’s most used command feature init and plan. Since Terraform is cloud agnostic you need to explicitly say where you will store your state file, this is a part of the init command. Running a Terraform plan is used to create an execution plan, against your state file and existing infrastructure.
CloudFormation has a similar process where you create & execute change sets. You will use a combination of create-change-set , execute-change-set and deploy. create-change-set is akin to Terraform’s plan whilst execute-change-set is Terraform’s apply & deploy rolls both commands into one.
The main difference is you will be looking at and resolving issues through CloudFormation’s UI, which is easy to use, even for me 😃.
Both Terraform's plan & CloudFormation’s create-change-set is a great place to have a manual check, in your continuous integration pipeline.
I found CloudFormations API’s to be a little confused. I believe create-change-set followed by a execute-change-set used to be the de facto but now deploy exists to replace them both. But in some cases especially deploying to production, you would want to follow good practice and inspect the execution plan before you executing it.
I thought in this case you would want to use a create-change-set followed by a execute-change-set but that is annoying as create-change-set explicitly needs to know whether you are creating or updating stack. So this would cause heartache in your continuous integration pipelines. But after further inspection, I believe you run a deploy with a --no-execute-change-set flag, and then re run deploy without the flag. I’m still unsure of the recommended approach.
I ran a simple experiment where both tools created a simple EC2 instance. I deleted both instances and tried to run both deployment steps again.
Hidden within Terraform’s plan is a refresh which is used to reconcile your state file, with the real world infrastructure. This means Terraform was able to detect the deletion & boot up a new instance. CloudFormation has no such reconciliation and depends on the existing stack. Thus it was adamant nothing had changed. The only fix for CloudFormation was to rename the EC2 resource.
I wouldn’t rely on this as a silver bullet for reconciliation and wouldn’t ever recommend manually playing with any stacks, that are managed by any tool. It will only lead to heartbreak 💔.
Intrinsic functions is one of my favourite parts of CloudFormation. Terraform does have a lot more functions than CloudFormation, but I prefer the syntax that CloudFormation uses, the lack of quotations, dollar signs & curly braces can make for some more readable code.
I also found the occasional shortcomings of Terraform’s interpolation in certain circumstances.
Conditional deployments are an important part of any deployment, you want to be deploying from the same stack whether its production or development. But sometimes production just requires extra resources, or vice versa.
Only CloudFormation supports conditionals explicitly, you can tag each resource with a conditional flag. Where as Terraform requires you to make use of their (awesome) count parameter and ternary conditional.
The count parameter is unique to Terraform, so CloudFormation would require more repeated code.
One of CloudFormations most powerful features is being able to so easily cross stack reference. This makes it extremely easy to break up the infrastructure monolith!
In the above example, the Networking Stack exports its DbSubnet1, as ${branch}-DbSubnet1. Which in my EC2 Stack I simply reference that Subnet using the Fn::ImportValue: !Sub ${branch}-DbSubnet1 . CloudFormation is also smart enough to know that the EC2 stack relies on the Networking Stack and will refuse to delete it until its dependencies are gone.
Exclusive to Terraform modules are a very powerful way to break down your Terraform deployments. A Terraform module is essentially a collection of Terraform files & resources that define a set piece of infrastructure. It allows for inputs & outputs. To use the module, you simply pass in your inputs as variables and under the hood Terraform will pull in the module and execute the Terraform code as usual.
Above is an example, where I define many instances in any region by simply using a reusable module. Without this module, I would need to define each instance and the associated Terraform code required to set up an instance in a new region.
The reason why this is not the same as a CloudFormation stack cross reference, is that the Terraform state file would still contain all of the state. Where as with CloudFormation, the state files would be split between each stack making it more manageable.
CloudFormation uses nested stacks to accomplish the same task. The issue with nested stacks are that if a child stack fails the entire stack will. This makes for deployment pains & nightmares. Approaching CloudFormation using a layered cake approach and cross stack referencing is the way to go.
CloudFormation also features CloudFormation init, which is AWS’s way of taking your user data script and turning it into state based configuration. It is extremely powerful and shouldn’t be overlooked. It allows you to run stack updates against instances and reconcile the state. Where as previously using user data you would have to terminate and create a new instance.
Maybe I was never very good at Terraform, but I have been preferring the way CloudFormations handles errors. The CloudFormation UI is really good and gives a good overview of your stack. I found that with Terraform errors, while they pointed you in the right direction it was still hard to gain an overall idea of what had happened.
You would think CloudFormation would support every AWS feature in existence, but unfortunately you are wrong. Because open source is the best, Terraform seems to have more AWS features.
Take for example, RDS with IAM auth, this feature has been released for over a year and is still in the CloudFormation backlog. Where as Terraform had a contributor gazoakley, make a simple PR and Terraform supports RDS IAM auth well before CloudFormation.
Another more important example, there is no Key Secrets Manager CloudFormation support. Where as terraform has its own resources for secrets manager since April.
We can’t forget that Terraform is cloud agnostic, a big win. But I wouldn’t take it too literally as from what I have seen cloud providers are pretty good at sinking their tiny hooks into you through features, or specific codes like ARNs.
But because Terraform is cloud agnostic, it can really be applied to anything that has state and an API. For example Kong API Gateway can be managed with Terraform. Thus you can apply your skills you’ve learnt else where in your stack!
I enjoy both Terraform & CloudFormation as they are both a great step in the right direction.
CloudFormation has a great UI for both debugging and general overview of everything thats happening. Cross referencing is powerful & makes separating stacks easy, whilst I personally really enjoy the syntax. The lack of support is my real gripe, as it’s not open source the thought of having to use custom resources or shell scripts to get certain features makes me cringe. 😢
Terraform is great because of it’s vibrant community of open sources, it’s simple module paradigm & it’s cloud agnostic. Terraform can be hard to debug through the cli and often you end up with a large monolith repo that includes all of your infrastructure. You can break it down into modules, but it doesn’t quite have the same separation of concerns you can get with CloudFormation.
Thanks for reading! If you enjoyed it be sure to give it a clap and check out both Terraform & CloudFormation!
Coding tutorials and news.
933 
7
A monthly summary of the best stories shared in Level Up Coding Take a look.
933 claps
933 
7
Written by
Vibes AWS. Software Engineer
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Vibes AWS. Software Engineer
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/automation-generation/build-a-day-trading-algorithm-and-run-it-in-the-cloud-for-free-805450150668?source=search_post---------228,"There are currently no responses for this story.
Be the first to respond.
Commission-free stock trading on a free Google Cloud Platform instance, step-by-step.
When I first started learning about how easy it’s become to take control of my money in the market, day trading jumped out at me as the thing to do. Of course, if it were really as easy as it sounds, everyone would be doing it. Finding a profitable strategy takes time and work, and sticking to it in turbulent market conditions takes discipline.
It’d be a lot easier to take the room for human error — and a whole lot of stress — out of the equation if we could have a computer execute the strategy for us. Just a few years ago, there were a lot of hurdles to doing that, even if you had the programming know-how. Most brokerages that the average person could access charged commission fees, which would eat away the profits of a day-trading strategy. Finding any sort of API access among those that offered commission-free solutions was a challenge to me.
In this article, I am using Alpaca’s commission-free trading API with their premium data service Polygon. (Please note that, according to their docs, you’ll need to sign up for a brokerage account in order to access the premium data feed used here.) I’ll provide a day-trading script that leverages this premium data for a little technical analysis, and I’ll go over what it does and how to run it yourself.
First off, go ahead and get the script from GitHub with this command:
Now, you can open it up in your favorite text editor and follow along. Note that near the top of the file, there are placeholders for your API information — your key ID, your secret key, and the URL you want to connect to. You can get all that information from the Alpaca dashboard.
Replace the placeholder strings with your own information, and the script is ready to run. But before we let it touch even your simulated account’s (entirely make-believe) money, let’s go over what it does. (If you’re more interested in how to get it running on GCP than what it’s doing, skip ahead to the next section.)
Broadly, this is a momentum-based algorithm. We’ll not trade for the first fifteen minutes after the market opens, because those are always pretty hectic. Between the fifteenth minute and the first hour, though, we’ll look for stocks that have increased at least 4% from their close on the previous day. If they’ve done that and they meet some other criteria, we’ll buy them, and we’ll hold them until they either rise high enough (meeting our price target) or fall too low (meeting our ‘stop’ level.)
You’ll notice that below the connection information in the code, there are some additional variables that can be configured. These can be tweaked easily to best suit your needs for the algorithm. There are thousands of stocks available to trade, but not all of them are suitable for a strategy like this.
We filter down the list by looking for a few things — we want a relatively low share price, but not one that’s so low that it behaves more like a penny stock. We also want to be sure that the stock is liquid enough that we’ll get our orders filled. We make sure that the dollar volume of the stock was at least min_last_dv on the previous trading day.
The default_stop and risk parameters are important to making sure that our algorithm stays within acceptable limits. Risk is what percent of our portfolio we’ll allocate to any given position. Since we sell when we hit the stop loss, the amount of cash from our portfolio at risk on a trade is default_stop * risk * account_balance .
I won’t go over how we get our initialization data here — if you want, you can take a look at the code and check out Polygon’s documentation on their ‘ticker’ data. What’s a little more interesting is the fact that we can also stream data in real time from Polygon. (This is also done in a recently-published “HFT-ish” example, another Alpaca day trading algorithm that trades much more frequently than this one and tries to profit from tiny order book imbalances.)
Using Alpaca’s Python SDK, we connect to three types of streaming channels. The first is trade_updates, which is simply a connection to Alpaca on which we can hear updates on our orders as they happen. We’ll use this to make sure we’re not submitting multiple open orders at once for a stock and to see whether or not our orders get filled.
The other two channels are A.<symbol> and AM.<symbol> . For each stock that we’re going to watch, we subscribe to those channels to receive updates from Polygon about the price and volume of the stock. The A channel updates every second, whereas the AM channel updates every minute. We aggregate the information from the A channel ourselves so we can do up-to-the-second calculations, but we consider AM to be the source of truth, replacing whatever we’ve aggregated with what comes through that channel. While we might get away with only watching A and relying on our own aggregation, trusting AM gives us a little extra resilience to hiccups in the connection and such.
Once we’ve added the incoming data to our aggregate, if we haven’t already ordered shares of a stock, we check to see if it looks like a good buy. We define a “good buy” as something with a positive, growing MACD that’s been trading at a decent volume and is up over 4% from yesterday’s close so far today. We also want to make sure that it’s maintained its momentum after the open, so we look to see that the price is higher than its highest point during the first fifteen minutes after the market to open. We hope that these stocks will continue to rise in value as the day goes on.
If we have a position in a stock, we also check with each bar that comes in for that stock if it’s time to sell. We sell when the stock has reached either our target price or our stop loss, or if the MACD suggests that the security is losing its momentum and it’s fallen back to our cost basis. Ideally, enough stocks hit the target price we set that we can recover the losses from those that hit the stop loss, with some extra profits on top.
At the end of the trading day, we liquidate any remaining positions we’ve opened at market price. The use of market orders is generally not ideal, but they are used in this case because the potential cost of holding overnight is greater than we were willing to risk on the position. Ideally, we have already liquidated our shares based on our defined stop losses and target prices, but this allows us to catch anything that sneaks by those by trading flat.
If you scroll down past the bottom of the long run() method, you’ll see how we check to see when the market will be opening and closing using the Alpaca Calendar API endpoint. Using this means that, if you like, you can set up a Cron job to run the script at the same time every day without having to worry about market holidays or late opens causing issues. Many people prefer to run their scripts manually, but it’s nice to have the option to just let it run on its own.
At this point, if you’ve plugged in your own API keys, you could just run python algo.py and be off to the races, watching it buy and sell stocks as its signals are triggered through the Alpaca dashboard. But you might not want to leave your own machine running all day — or maybe you’re just worried about a cat bumping the power button during market hours.
Fortunately, it’s easy to get a machine in the cloud — away from your cat — set up with Google Cloud Platform’s free tier. At the time of writing, they’re even offering several hundred dollars in free credit for trial accounts, so you don’t need to worry too much about running into minor incidental costs. Once you’ve signed up for a GCP account, head to the GCP console and create a project.
Once you’ve created a project, go to the sidebar and navigate to Compute Engine > VM Instances. In this panel, you can see any instances you’ve created. That might be blank for now, but let’s go ahead and make one. Hit Create and fill out your configuration like this.
Hit Create at the bottom, and after a short bit of watching a circle spin, you’ll have a machine up and running, visible in the VM Instances panel. It’ll look something like this:
You can click Open in browser window to get a shell to the machine. Go ahead and do that now — we’ll need it in a minute. But before we type anything into the terminal, we need to get our script onto the machine. (You might know of some ways to do that through the terminal — if so, go for it! Or, you can follow along to do it through the GCP console interface.)
In the sidebar, scroll to Storage > Browser. On the Storage page, go ahead and hit Create Bucket. You’ll get a prompt like this:
We’re not too concerned about the storage cost, since we’ll be storing much less than 1 GB and only sharing it with our one instance. (And we have a few hundred dollars in credit anyhow, if you’ve signed up for the trial.) Once your bucket is created, it’s easy to put your files into it.
Upload the requirements.txt and algo.py files you checked out from the GitHub repository and plugged your Alpaca API credentials into. We’re almost done! All that’s left is to get the files onto our instance. In the browser terminal you’ve opened, you can type gsutil cp gs://<your-bucket-name>/* . to download your files onto the VM.
Now that the script is on the cloud instance, we just need to run it. Python is already installed, but to make requirement management easy, go ahead and download and install pip. In the VM terminal, do this:
And then install the algorithm’s dependencies:
And that’s it! Now, when you’re ready to get the algorithm running, you can just type python3 algo.py into the VM’s terminal and watch it get to work. (At this point, you can also delete the bucket you created, if you’re worried about a few pennies in storage fees.) Now that you’re on your way to being a GCP pro; feel free to play around with the Python code in the script and see what works best for you! I hope this guide has been helpful, and good luck!
Technology and services are offered by AlpacaDB, Inc. Brokerage services are provided by Alpaca Securities LLC (alpaca.markets), member FINRA/SIPC. Alpaca Securities LLC is a wholly-owned subsidiary of AlpacaDB, Inc.
You can find us @AlpacaHQ, if you use twitter.
News and thought leadership on the changing landscape of…
1K 
18
1K claps
1K 
18
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Written by

News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
"
https://faun.pub/this-is-how-i-reduced-my-cloudfront-bills-by-80-a7b0dfb24128?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
I applied the same pieces of advice to reduce my CloudFront usage costs:
"
https://medium.com/adobetech/three-principles-of-api-first-design-fa6666d9f694?source=search_post---------230,"There are currently no responses for this story.
Be the first to respond.
In my conversations with developers who are looking to embrace cloud-native development and openness, one question frequently comes up: what exactly does “API First” mean? All too often, “API First” can come to mean “Yeah, APIs are important, but they’re not essential”.
There are three principles of API First Design:
1. Your API is the first user interface of your application
2. Your API comes first, then the implementation
3. Your API is described (and maybe even self-descriptive)
Let’s take a look what each of these principles mean.
API first thinking means that your API is the first interface of your applications. This means that the people developing against your API are your users, and your API needs to be designed with those users in mind.
Your API is how your product exposes its functionality, so if there is functionality in your product not covered by an API, it can’t be covered by a graphical user interface, command line interface, or voice interface either, effectively making the functionality invisible.
Finally, as your API is the first and most important way to access and interact with your product, the API needs to be managed and designed deliberately. Just as you spend time to design your graphical user interface, invest time to design your API: what it exposes, what it does, and how it grows.
Once you realize that your API is an interface that deserves attention of its own, you begin to realize that the API has — and should have — a life of its own.
Your implementation will change as your application evolves and you optimize, refactor and grow the functionality. Your API, however, should not change frequently, but instead grow slowly and deliberately.
Your implementation will change frequently, your API should not.
Here’s another way to think about this approach: If your API is the surface area of your product; the functionality is its volume. Doubling the functionality will only grow your API surface by 25%.
It’s important to think of API evolution in terms of growth and increasing flexibility. Graceful API evolution is additive in terms of functionality, and subtractive in terms of requirements. While change is inevitable, planning for a graceful API evolution is a good way to minimize changes that break things. For example: required input may become optional, but not the other way around.
Treating your API as independent from the implementation, even if that’s harder, allows you to decouple development of API and implementation. Your API becomes a contract and specification to the implementation, instead of just being a thin veneer on top of your implementation.
The third principle of API First Design is about descriptiveness. In order to be used, your API needs to be easily understood by people that have not been involved in its creation. That means documentation.
Usable API documentation is an essential prerequisite to making it consumable by humans. As robots and AI aren’t taking over programming anytime soon, this makes it essential.
When it comes to documentation for APIs, structured documentation beats unstructured documentation. Following a standard pattern for URLs, resource types, request methods, headers, request parameters, and response formats will make it easier to explore and understand functionality, and reduces surprises when your API grows.
Speaking of surprises: in your API design, try to minimize surprises and follow established standards and best practices wherever possible. When it’s impossible to do so, document the deviation so that your API consumers won’t waste hours chasing a bug introduced by an API that is working just a bit differently than expected.
There is one thing that makes even the best documented APIs stand out: APIs that are self-descriptive by using hypermedia constructs like links that allow the discovery of other API resources. This gives you a great way to close the loop from design, over implementation, to documentation.
Follow the Adobe Tech Blog for more developer stories and resources, and check out Adobe Developers on Twitter for the latest news and developer products.
News, updates, and thoughts related to Adobe, developers…
791 
3
Thanks to Rachel Luxemburg. 
791 claps
791 
3
Written by
Principal at Adobe
News, updates, and thoughts related to Adobe, developers, and technology.
Written by
Principal at Adobe
News, updates, and thoughts related to Adobe, developers, and technology.
"
https://itnext.io/keep-you-kubernetes-cluster-balanced-the-secret-to-high-availability-17edf60d9cb7?source=search_post---------231,"As you probably know, a Kubernetes cluster is made of master and worker nodes. The scheduler is a component in a master node, which is responsible for deciding which worker node should run a given pod.
Scheduling is a complex task and like any optimisation problem you will always find a scenario in which the result may seem sub-optimal to the human eye.
Nonetheless, the default scheduler does a pretty decent job. It follows common sense strategies such as avoiding to schedule multiple replicas of a pod on the same node or avoiding overloading a node while others remain under allocated.
I am not familiar with the code behind the default scheduler but my observations have taught me that balancing node workload takes priority over preventing duplicates running on the same node.
This means that you may find yourself in a situation where all replicas of a pod are on the same node.
Therefore, if the node goes down — and you should assume that it will — availability will be impacted.
Interestingly, Kubernetes usually works by strictly enforcing some sort of state. For example, if you say you want three pods A , Kubernetes will imediately spawn new pod(s) should that number go below three. This is not the case for scheduling. Kubernetes will try to to spread your pods intelligently when it creates them but will not proactively enforce a good spread afterwards.
Scheduling happens at pod creation only.
Running multiple replicas of a pod will not guarantee high availability but it cannot be achieved without it. For obvious reasons, you should never have all the replicas of a pod on the same node.
High Availability is not an absolute concept; you can never be 100% highly available. How can your application be available if all the nodes go down simultaneously?
So you generally need to choose a level of availability you are comfortable with. For example, if you are running three nodes in three separate availability zones, you may choose to be resilient to a single node failure. Losing two nodes might bring your application down but the odds of loosing two data centres in separate availability zones are low.
The bottom line is that there is no universal approach; only you can know what works for your business and the level of risk you deem acceptable.
Let’s have a look at a scenario where scheduling can jeopardise availability.
Let’s say we start in an ideal state of scheduling. We have four services A , B , C , and D , each scaled to run three replicas.
If Node 3 were to die, the master node(s) will notice that we only have two pods of each service. The master node(s) will immediately reschedule one of each on the remaining healthy nodes. This could lead to:
After some time (typically a few minutes), your cloud provider will notice that a node is down and will spin another one. When that node is ready we end up with a cluster like that:
The cluster is unbalanced but availability is not lost, yet… If we lose any of the nodes we still have at least one replica of each pod running.
However, let’s see what happens when we start soliciting the scheduler by releasing a new version of service A . This is typically done with a rolling deployment.
Kubernetes will start a new pod for service A , wait for the pod to be ready, and kill one of the old pods. It continues doing so until all the old pods have been replaced.
So, the scheduler is asked to place a new A v2 pod — the only place it can do so is Node 3. So far so good. However, where should it put the other two? As mentioned above the scheduler tries to avoid having duplicates on the same node but in that case Node 3 is massively under-utilised compared to the other nodes. So there is a high chance that all the new A v2 pods will be scheduled on Node 3, which would lead to this situation:
And now we have a problem; all the A pods are on a single node. If Node 3 were to fail again we loose availability for service A .
The whole process is described by this magnificent GIF:
One way to bring back order is by doing a bit of chaos engineering. The scheduler can only fix a balance problem when pods are created. So let’s get killing to force creation of new pods!
Enters the descheduler. This project runs as a Kubernetes Job that aims at killing pods when it thinks the cluster is unbalanced. You can run it once or as a Cron Job to run it periodically.
The installation is pretty straightforward and well explained on the GitHub page.
You can use a variety of strategies to delete pods, which are defined in a Config Map. Let’s review two of them.
Using the RemoveDuplicatesstrategy, the Descheduler will find identical pods on a node. It will then kill some of them, hoping the scheduler will schedule the pods on another node.
Note that it wouldn’t work with the scenario outlined above. This is because we still haven’t resolved the workload balance problem.
However, the LowNodeUtilization strategy can help with that:
You must define what it means for a node to be under and over utilised. In this example, if a node is below 20% CPU utilisation and below 20% memory utilisation and has less than 20 pods it is considered under-utilised. If a node is above 50% CPU utilisation or above 50% memory utilisation or has more than 50 pods it will be considered over-utilised.
For this strategy to work, the descheduler will need to find at least one under-utilised node and one over-utilised node.
As usual, there are no universal utilisation parameters that will work well everywhere. You need to tweak them and find out what works best for you.
If you are really confident in your availability you could run this job every hour. Otherwise you could run it at night when impact on traffic is low. In the latter case, it might be useful to run it a few times in a row. For example, you could run it everyday at 3am, 3:15am and 3:30am.
In conclusion, remember that High Availability is not something you will ever achieve without actively testing for it. If you don’t kill your nodes and test for disaster scenario your system will experience downtime.
Keeping your cluster balanced will not guarantee High Availability but it will certainly help you towards that goal.
If it becomes necessary, you can also try advanced scheduling techniques. You can find information about this here and here. It is also worth having a look at pod affinity and anti-affinity.
https://twitter.com/cesarTronLozai
ITNEXT is a platform for IT developers & software engineers…
633 
7
633 claps
633 
7
Written by
Head of engineering. Java, Scala, Haskell. Works with Kubernetes. Follow me on Twitter @cesarTronLozai
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Head of engineering. Java, Scala, Haskell. Works with Kubernetes. Follow me on Twitter @cesarTronLozai
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://koukia.ca/keep-calm-and-use-azure-application-insights-ff38e04a43fd?source=search_post---------232,"If you have been following me here, I started writing about Azure Application insights 4 or 5 years ago on my blog when it first came out and I was really impressed by its power even then, but now after several years, I think it has made a lot more improvements so it’s time for another closer look and some of the new features it has.
"
https://codeburst.io/kubernetes-ingress-simply-visually-explained-d9cad44e4419?source=search_post---------233,NA
https://medium.com/google-cloud/top-13-google-cloud-reference-architectures-c8a697546505?source=search_post---------234,"There are currently no responses for this story.
Be the first to respond.
👋 Hi Cloud Devs!!Last year I created #13DaysOfGCP mini series on Twitter which you all loved. So, here I compiled 13 more common Google Cloud reference architectures. If you were not able to catch it, or if you missed a few days, here I bring to you the summary!
Those the 13 hand picked Google Cloud architectures. I am thankful and really humbled by your participation, engagement and topic suggestions that made this series so much fun, not just for me but also for the rest of the Google Cloud Community members!
Thanks again and 👋 until later 🙂
Google Cloud community articles and blogs
294 
2
294 claps
294 
2
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/avmconsulting-blog/api-management-for-micro-services-micro-apis-5940f09fb638?source=search_post---------235,"There are currently no responses for this story.
Be the first to respond.
Micro-services is a very hot topic these days. Almost all cloud infrastructure providers now support building and deploying micro-services. Some have their own proprietary tools and some use open source tools. However, despite the label or the name, they have given for the tool or the service, all of these have the same core functionality and operate almost in the same way. For the interest of this article, I’ll be using AWS tools to build up the solution architecture.
The context here is, that we have a set of micro-services that exposes API endpoints, and we need to properly expose these to client applications. Basically, we have an API as a collection of micro-APIs. This is so simple, right? Well, I would say Yes and No. We are talking about production-ready APIs here, and there few concerns when we expose an API in production. I am not going to go into detail about API management here, but I’ll brief few
From the above concerns, the first concern “Unified interface to connect” is the main interest of this article. When we are building an API as a collection of micro-APIs we need a way to expose the endpoints exposed by each service to the client applications properly. And as I see the best option would be to expose a single view to the client applications aggregating all endpoints from each miro-API (micro-service). Because the client doesn’t care that we are using micro-services, and all it needs is less complexity for consuming the API.
Also, the above concerns don’t add much business value to the application but they are best practices for deploying APIs in production. And it’s always a good practice to use a managed service that would provide these features.
Amazon API Gateway is a fully managed service that has support for all the concerns mentioned above. It has the support to connect a variety of services at the back and provide a unified RESTful API at the front. By the time I am writing this article it supports the following integration types.
You can get up-to-date more information on the above from their documentation here. I am not going into details on how to build micro-services using the above integration types. If you need more information on these you can find them here.
Okay so let’s assume we have multiple API Gateway deployments (Micro-services) ready and we need to expose them as a single interface to the client applications. For this purpose, we are going to use the Amazon API Gateway’s support for “Custom Domain Names”. To use this feature, we need to use two other services also. They are,
Before I explain the steps here, Let’s look at the solution architecture of what we are going to build. The diagram below shows an overview of how the above services connect.
I think the above diagram pretty much explains what we need to do. But I would briefly explain the steps. Like I said I assume that the micro-services deployed with API Gateway are in place already.
<Domain Name>/<Service Name>/<Version>/*
I have added a screenshot of the API Gateway custom domain mapping along with base path mappings for you to get an idea of what to expect at last.
You can get more details about setting up custom domain names in the Amazon API gateway from their documentation mentioned below.
docs.aws.amazon.com
So now you have a unified single view of the API exposed via a structured URL for your consumer applications. I am not going into details about how the other API publishing concerns mentioned above can be implemented using API Gateway since it’s not the interest of this article.
There are lot more other features supported by Amazon API Gateway like API Keys, Usage Plans. And you can find more details on these from the following link.
aws.amazon.com
Hope the information here is helpful. Don’t hesitate to leave comments if you have any.
👋 Join us today !!
️Follow us on LinkedIn, Twitter, Facebook, and Instagram
If this post was helpful, please click the clap 👏 button below a few times to show your support! ⬇
AVM Consulting — Clear strategy for your cloud
340 
1
340 claps
340 
1
AVM Consulting — Clear strategy for your cloud
Written by
AWS Certified Solutions Architect. More details https://www.linkedin.com/in/asankanissanka/
AVM Consulting — Clear strategy for your cloud
"
https://medium.com/@virgil.utopia/the-silent-extinction-of-iot-startups-767c08773c9a?source=search_post---------236,"Sign in
There are currently no responses for this story.
Be the first to respond.
Virgil's Utopia
Feb 18, 2019·5 min read
It wasn’t even a good thermostat. It was a mediocre one that served a single purpose: to allow me to set the temperature when I was out of town through a smartphone app. Mind you, it was a pretty bad app too. But for all its shortcomings the little thermostat worked continuously for more than 3 years. It recorded the temperature, turned on the heater at the right time to keep things comfy and made nice daily reports about the usage. It even saved me money on my gas bill. Then in December of 2018, as I was preparing to board a plane, I tried to order it to heat the house. We all try to be as green as possible, so I keep the house cold while I’m away and it takes quite a few hours to bring it up to a livable temperature.
Only the app wasn’t responding. For hours all I got was a blank screen every time I opened it. Of course, I arrived to a cold house, but to my surprise the thermostat was still working on the last setting I left it on before traveling.
It was clear that he device was still fine, but the cloud connection had been cut. The website of the producer was gone. In the following days I discovered a letter of apology from the company warning that they may have to shutdown due to financial difficulties. It had been sent a week before my trouble began. The letter had been sorted into Spam.But none of these elements change the simple fact that I bought a device that stopped working when the company that made it… just died. It wasn’t a service that the company stopped providing. It wasn’t that they chose to end support due to going out of business. The product I bought stopped functioning as expected due to how their producer operated. It is this small and seemingly meaningless event that should worry the Internet of Things (IoT) startup scene. As this happens to more and more people, it will cause a silent extinction.
By this point in this article, everyone tech savvy is probably already feeling amused by this completely predictable series of events. Obviously investing in a device from a tech startup is risky and surely IoT devices are themselves an even bigger risk. Bear with me.
A lot has been written about how insecure the Internet of Things is and how it’s leading us down a bad path. There’s even a successful twitter tag #internetofshit that details the failures and misguided nature of IoT projects.
But this article is about long-term effects. IoT devices are not going away, because having your thermometer, shoes and thermostat connected to the internet is a huge value proposition. The analytics and control they offer is invaluable to both users and companies. Paradoxically this is also where the problem is. You see, we’ve grown accustomed to the startup mode of innovation: iterate fast, fail often and learn from your mistakes. And that works well in software, but as minor annoyances and bricked devices start to pile up, users will become wary. It’s one thing to sign up for a free account online only to have the company behind it fail in a few years, it’s a completely different thing to wake up to a cold house every few years and having to urgently replace whatever IoT failed this time.
On one hand it’s a problem with the business model. Even though the cloud part of IoT is cheap, some companies stop being able to afford that 1$/year/device to keep the things internetting. Only few companies dare try running ads, mostly because of privacy fears. Some resort to subscriptions but that limits growth, and if there’s one thing startups depend on, it’s fast growth. So what’s the solution? Startups can just eat the low costs, grow fast and sell the treasure hoard of value they’ve accumulated to a big company when they exit… right? Maybe. That’s great for the first wave but 90% of startups fail. That’s 90% of people in cold houses, whose self-lacing sneakers malfunction.
IoT startups failures are leaving behind many users who’ve been burned harder than early adopters of other startups. When it’s time to get a replacement, do you think they’ll pick up the one from the plucky startup with neat features? Or will they pick the less feature packed device from a big player, just so they won’t have to suffer through another cold night? This problem is insidious. There won’t be a mass of voices saying “next time I’m buying a big brand”. It’ll just be many individual cases of picking something else.
This will start as a slump but will manifest in the long term as a lack of innovation. Each new wave of IoT startups will be trusted less while big players will take over the space with bland offerings. This will not be a crash. It will just be give the whole industry the distinct aroma of sadness you can only get from finely crushed dreams.
Like any good apocalyptic article, I’d like to list of a series of unlikely solutions. The first is the standard advocacy for open source. If an IoT device is based on open source components they can benefit from increased security and the possibility of continued development long after the initial creators no longer maintain it. Of course, this isn’t without a set of disadvantages, but other authors have talked about this better than I could.
Another option would be to design the system to fail gracefully in the event it’s orphaned. The IoT device could continue to function on its own but one has to keep in mind that any IoT device left online without security updates might soon fall prey to botnets and other cyberspace predators. This is where open source would again be a tremendous boon.
My last proposal is a little unorthodox in the sense that it makes an appeal to the higher powers. The big players know that the reduction of innovative startups isn’t a good thing. Less innovation means less options even for them. That’s why the tech giants can create a garden in which IoT startups can flourish. The cloud hosts could provide middlewares that are run and maintained by the large hosting company. If IoT communication and security are provided by the cloud host instead of the startup we’re already in better territory than we were before. The caveat is that this idea implies creating standards and that’s never easy. Amazon and Google have taken steps in this direction with their smarthome initiatives so there is hope. Although some may argue that depending on a bigger fish is also dangerous.
What do you think? Even if you’re in the bah-humbug camp of IoT I’d like to read your constructive opinions. Tweet me or comment on medium.
Senior researcher and strategist specialized in artificial intelligence and blockchain but working with all innovative technologies.
483 
10
483 
483 
10
Senior researcher and strategist specialized in artificial intelligence and blockchain but working with all innovative technologies.
"
https://medium.com/@talhaocakci/how-to-pass-aws-solutions-architect-professional-exam-87bebfdae86f?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
Talha Ocakçı
Sep 9, 2018·9 min read
First things first: Passing the exam does not mean you are a solutions architect. Second, the real target is to be a solutions architect, not a plain certificate holder. So, a certificate may be required but not sufficient for real job.
If we agree on it, we may talk about the details of both exam and AWS environment and how AWS certification exam evaluates you. By the way, I have prepared for the associate exam with this course. I highly recommend it.
AWS simply evaluates if you may imitate a network structure with real machines, cables, routers etc in a cloud environment on virtual machines. Also your capability to for protecting the network structure from internal mistakes and external assaults while minimizing the costs and preventing repeating operations and automating the tasks.
Associate exam simply evaluates your familiarity with
2. Responsibilities of the AWS customers and AWS itself.
3. Usage of AWS-specific services.
Learning the first two items is great for both on-prem networks and all of the cloud suppliers. Second and third items are AWS-specific and may be different on other cloud suppliers and may not exist for on-prem networks. While learning the AWS specific services, I have learnt so many things related to software programming. Thus, paying attention to the internal details of these services will open your eyes, I think. For instance I enjoyed while reading abut DynamoDB billing algorithm, some algorithms in Kinesis, usage of Elastic Network Interfaces, IAM roles usage and much more things.
Professional exam evaluates whether you really used what you demonstrated that you know in associate exam. Also many questions about on-prem and cloud network connections are asked. Many questions make you select 2 or 3 possibly correct options out of 5 or 6 options. You need to select the options with minimum cost, minimum effort or maximum security.
The most important AWS items are VPC and EC2 since they are the backbone for any type of application. We construct everything upon them. You must be comfortable on these topics:
Serving your applications inside a virtual private cloud network (VPC). You need to set up several subnets for your application servers, DB servers etc… And you need to make them communicate with each other with route tables while preventing other VPC subnet connections or external connections with security groups or network control access lists.
Protecting your network from outside assaults: When to use security groups, when to use IAM roles or users, when to use network access control lists. Managing inbound and outbound connections and stateful/stateless rules are important.
Connecting your VPC to internet with Internet Gateway or NAT servers. You need to be able to prefer IGW to NAT instances, for instance. You need to know the reasons. Also you need to know when and why NAT instances may be bottleneck.
One of the most important reasons of using cloud platforms is being able to use new virtual machines and using them as cluster for horizontal scaling.
AWS uses auto scaling groups and EC2 instances (spot, reserved and on-demand) for horizontal scalability.
Certification exam evaluates whether you can
Scale down when the CPU or RAM load decreases on virtual machines for minimizing the costs
Ignore the CPU bursts and prevent unnecessary scale up operations. This is crucial. Some DDOS attacks may cause several scale up operations that will burst the costs. You need to make sure if a scale up is necessary or not by using warm up thresholds.
Use the correct type of load balancers and configure them for multi AZ (availability zone) load distribution.
Each EC2 instance may get data from different sources. EBS (Elastic Block Storage) and EFS (Elastic File System) can be thought as hard disks of the virtual machines. Data is stored as blocks.
Certification exam evaluates whether you can:
S3 (simple storage service) is different from EBS and EFS. S3 stores data as objects. That means each file is an object in S3. You need to know how to
DynamoDB is the NoSql solution and used for key, value pairs. Values are mostly JSON objects.
RDS is the managed service for storing relational data, simply, an RDBMS system such as Aurora (MySQL fork), Oracle is installed on a virtual machine and most of the operations like automatic backups and configuration management is done by AWS.
You may simply create an EC2 instance and install MongoDB or Mysql, but it requires to have some experience to optimize the configurations. If you are not experienced enough, using DynamoDB or RDS might be a solution. But consider the higher prices. As a solution architect you need to lower the costs, don’t forget!
You need to know below for passing the exam:
CloudFront is CDN (content delivery network) of Amazon and used for replicating and caching your dynamic or static content to the regions that are accessed frequently for quicker delivery times.
You need to know
Adjusting the cache times
Using Origin Access Identity when the origin is S3 object
This is possibly the most important topic when you work with the clients with existing network infrastructure on on-premise machines. Possibly, you will need to migrate the infrastructure gradually to AWS or maybe you will never be able to migrate some parts to AWS. That’s why you need to connect existing infrastructure with the AWS environment.
This is really a hard task for software developers with no network maintenance experience. At least, it was really hard for me.
You need a really deep understanding of DirectConnect besides VPC, and bastion hosts.
Please read this for an introduction: https://aws.amazon.com/tr/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/
Route53 is the DNS management service of AWS and used for DNS records of your system. Route53 is simply used for routing the traffic to your environment according to the DNS records. That’s why you need to know A records, ALIAS records, where to use them and their pricing in AWS.
When you need to manage the environment above region level, you will deal with Route53 records.
For instance when the EC2 instances on a region fails completely, you need to reroute the traffic to another AWS region. This is done with a combination of Route53 health checks and DNS fail-overs.
Tracking who is doing what in you AWS environment is so important. For instance when an EC2 instance is terminated with an AWS user, you need to figure out the responsible person and then take some precautions for not letting it again. You may find the responsible person by CloudTrail and then act accordingly. For instance you may first add termination protection to the EC2 instances first and then let only the trusted people by creating IAM roles with required permissions and assigning them to trusted people.
AWS checks your ability with several use cases in the certification exam.
CloudWatch is a service for watching the system metrics and setting up alerts according to these metrics. Yo may watch CPU, RAM, network usage easily.
For instance you may set up an alert when the CPU usage is above 90% and alert an autoscaling group for scaling up or you may just inform a user with dropping a message to SNS. (Simple Notification Service)
Also, we may tag some resources for managing the permissions. For instance we may track the cost of some EC2 instances and load balancers with custom tags on them. For instance you may track production, test and development costs by using tags.
Creating your infrastructure automatically is a must have for reducing operation costs. When you define every item and their relations with a metadata, reconstructing it would be so quick and easy. In AWS we use CloudFormation for this.
There will be more than 5 questions about CloudFormation. So allocate some serious time for CloudFormation usage.
AWS has lots of other services. Let me just summarize the ones you will probably encounter in exam. The most important one is Kinesis since data streaming and analytics is so much important in modern applications.
Second one is SWF (Simple Workflow Service). SWF is used for managing, scaling background jobs with each other and human intervention.
Other ones are SNS (Simple Notification Service) and SQS (Simple Queue Service).
As a solutions architect, our primary concern is security and low cost. For reducing the costs we may consolidate our accounts. For instance you may consolidate the costs of different departments of your company so that you may get the extra discounts according to usage volume by joining the volumes together. That’s why you need to know how to set up consolidated accounts.
Also you need to track the usage volumes of each EC2 instance with CloudWatch and change instance type if you don’t fully leverage the capacity of the machines.
Lambda functions
Amazon tries to demonstrate serverless architectures. So, certification exam allocates about 5 questions for Lambda functions, pricing of it and connecting lambda functions with SQS or Kinesis.
Originally published at: http://www.buildtalents.com/how-to-pass-aws-solutions-architect-professional-exam/
Yazılarımda bazı şeyleri yanlış anlatıyor olabilirim. Kesin yanlış anlatıyorumdur.
See all (390)
285 
2

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
285 claps
285 
2
Yazılarımda bazı şeyleri yanlış anlatıyor olabilirim. Kesin yanlış anlatıyorumdur.
About
Write
Help
Legal
Get the Medium app
"
https://eng.lifion.com/going-cloud-native-2dc748c0fbcf?source=search_post---------238,"It may come as no surprise for any software engineer to hear this, but managed services offered by today’s cloud providers render significant benefits to engineering organizations. These cloud providers have developed several different solutions to help organizations scale both vertically and/or horizontally across the globe, while in several cases, being at a lower cost to any institution managing their own infrastructure. On top of lower cost and overall ease of scalability, cloud providers guarantee several out-of-the-box freebies that would normally need to be discussed, addressed, and actively monitored if an organization managed their servers solely on their own.
While the benefits are clear, the plethora of Cloud-Native database options makes it difficult to select the right one for your organization’s use case. Here at Lifion by ADP we’re building a global scale platform with a massive suite of products. We use multiple types of data stores dependent on the data and its use case in our platform, such as key-value, relational, graph, document stores, etc.... As we continue to scale our platform and our customer base we found ourselves with a business need to move to a Cloud-Native solution for a core data store that is heavily and frequently used whenever end-users interact with the system. We strive to be thoughtful and systematic about our decisions and designs, especially when changing such a critical piece of our architecture. This meant carefully identifying our requirements, enumerating our possible solution options, and making sure our approach to selecting the correct one was diligent, strategic, and evidence-based. We’re firm believers in using “the right tool for the job” when it comes to building our technology stack. Given we strategically use Amazon Web Services for hosting our platform we intentionally did not consider alternative non-AWS cloud-native options. In this article, my team, who’s responsible for the Metadata Engine and Architecture of our platform, shares how we went about this exercise and explains our findings.
After having several stakeholder discussions and identifying areas that could be improved upon within our current paradigm, we came up with the following overarching requirements.
The team identified a strategy to consistently test across the board with a goal of benchmarking only read speeds of JSON payloads per data store. We developed a proof of concept, and ran those equivalent tests against each database conforming to the following parameters:
Based on the results of our benchmarking strategy outlined above, we compiled a pros and cons list for each solution, which we’ve captured below. Expanded graphs with full document sizes and all requests, specified in our benchmarking strategy above, can be seen in “For The Curious” below. The graphs in “Final Outcome” are zoomed for ease of visualization.
https://aws.amazon.com/s3/
No, Amazon S3 does not meet our requirements, due to its slowness and variability of response time. However, we have several internal networking layers to control traffic in our VPCs, which could have potentially hurt response times during our testing. Since it can give the most stable cache by its high durability and availability, it is possible it could address our use case in conjunction with another caching layer.
https://aws.amazon.com/rds/aurora/details/mysql-details/
Yes, Aurora (MySQL) does meet our requirements, in most cases. There is a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities, etc. however we may not need all of these features for our use case.
https://aws.amazon.com/rds/aurora/details/postgresql-details/
Yes, Aurora (PostgreSQL) does meet our requirements. Amazon Aurora (PostgreSQL), like the MySQL implementation, has a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities and so on, but again we may not need all of these features for our use case.
https://aws.amazon.com/elasticache/redis/
Yes, ElastiCache for Redis meets our requirements. ElastiCache For Redis does not persist documents by default, however, automatic daily backups can be enabled. This automatic backup stores snapshots within S3, up to 35 days. With Redis, we can get extremely fast response times, making this one of the best options for our use case.
https://aws.amazon.com/elasticache/memcached/
Yes, ElastiCache for Memcached meets our requirements in all the same ways in which Redis does. The Memcached implementation is missing several out-of-the-box features that the Redis implementation offers. Some features missing include snapshots, replications, lua scripting and advanced data structure support.
Below are additional graphs as measured during our tests: color indicates JSON payload size, Y-Axis measures time, and X-Axis represents number of requests. Each graph is the expanded version of those graphs highlighted above. Again, the following benchmarks only tested the read speeds of JSON payloads per data store.
Based on our tests/learnings from developing our proof of concept, we concluded that S3 is too slow for our use case, confirming our theory that S3 fits better for latency-insensitive, high-reliability needs such as for file storage or serving. Both implementations of Aurora seem like good choices because they provide persistent storage by default, and may be good options for some other use cases we have internally. However, we may not need to pay the extra cost of persistent storage and large scale transactions. Upon further investigation we also found with the Aurora options we’d need to run a larger instance size than we would if using ElastiCache, which would cost considerably more. If you could avoid paying extra cost, on top of using a faster solution, wouldn’t you? Hello, ElastiCache.
Once we had narrowed down to ElastiCache for our approach the next question we had to ask ourselves was which implementation? This is a question we debated internally, and it ultimately boiled down to the following differences we saw between Redis and Memcached. With that said, here are some of the differences we identified that were key to our use case between Redis and Memcached:
ElastiCache for Memcached
ElastiCache for Redis
Knowing the result of our findings would become a very important piece of the Lifion platform undoubtedly fueled us to take several extra steps in our due diligence, while enjoying the impact of our work. Keep in mind these findings were pivotal for our specific use case, and might not hold true in all scenarios, as most questions posed within Computer Science are rebutted with: “it depends”.
Plain and simple, our final decision boiled down to our global scalability requirements. Built-in replication, as well as snapshots in cases of disaster recovery, were what eliminated the Memcached implementation. Sure, we could have implemented our own solutions to fill the void, but why not take advantage of already optimized systems? Our needs are global, and having these optimized solutions provided by Amazon were truly the differentiator. Not to mention, Redis read speeds, based on most of our benchmarks, were also slightly faster. So, at the end of the day, it’s all about choosing the “right tool for the right job”.
Credited Platform Engineers
All things tech at Lifion by ADP
4.1K 
Thanks to Tom Rogers and Aubrie-Ann Jones. 
4.1K claps
4.1K 
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
"
https://medium.com/@dearsikandarkhan/files-copying-between-aws-ec2-and-local-d07ed205eefa?source=search_post---------239,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sikandar Khan
Feb 5, 2018·2 min read
scp -i /directory/to/abc.pem /your/local/file/to/copy user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file
Note: You need to make sure that the user “user” has the permission to write in the target directory. In this example, if ~/path/to/file was created by user “user”, it should be fine.
scp -i /directory/to/abc.pem user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file /your/local/directory/files/to/download
#Hack 1: While downloading file from EC2, download folder by archiving it. zip -r squash.zip /your/ec2/directory/
#Hack 2 : You can download all archived files from ec2 to just by below command.
scp -i /directory/to/abc.pem user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:~/* /your/local/directory/files/to/download
Happy exploration.
Head of Engineering || Cloud and Distributed Systems Specialist
419 
4
419 claps
419 
4
Head of Engineering || Cloud and Distributed Systems Specialist
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/techmagic/serverless-vs-docker-what-to-choose-in-2019-80cb80f4b680?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Let’s start with the fact that cloud computing is growing exponentially. Businesses are continually migrating from traditional data centres and inefficient physical servers to innovative cloud technologies and microservices architectures. What are the key factors driving businesses to build or migrate their architectures to the cloud? Based on a recent survey we highlighted 4 benefits of cloud computing that are considered to be vital for most businesses.
Today there is an increasing number of different cloud services that enable developing more cost-efficient applications with higher performance and more effortless scalability such as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Function as a Service (FaaS) and Software as a Service (SaaS). If you’ve been looking for a full-service framework solution for your business needs, then it’s likely that you’ve come across the concepts of Serverless and Docker Containers, that represent FaaS and PaaS models, respectively. Many experts now consider these solutions as the best practices in cloud computing as they offer simplicity and flexibility in application development and deployment. However, they have some unique differences that must be taken into consideration, and today we are going to cover it.
To start with, it’s worth saying that both — Serverless and Docker Containers point out an architecture that is designed for future changes, and for leveraging the latest tech innovations in cloud computing. While many people often talk about Serverless Computing vs Docker Containers, the two have very little in common. That is because both technologies aren’t the same thing and serve a different purpose. First, let’s go over some common points:
Although Serverless is more innovative technology than Docker Containers, they both have their disadvantages and of course, benefits that make them both useful and relevant. So let’s review the two.
Serverless allows you to build and run applications and services without provisioning, scaling, and managing any servers. You can build them for virtually any type of application or backend service, and everything required to run and scale your application with high availability is handled for you. Let’s have a look at the benefits and drawbacks of Serverless.
The four major services that are being utilized for the Serverless orchestration are Amazon Step Functions, Azure Durable Functions, Google Cloud Dataflow, and IBM Composer.
Here are a few companies and startups that use Serverless right now: Netflix, Codepen, PhotoVogue, Autodesk, SQQUID, Droplr, AbstractAI.
To get more information about Serverless, refer here:
Docker is a containerization platform that packages your application and all its dependencies together in the form of a docker container to ensure that your application works seamlessly in any environment.
The most popular tools that are being utilized for Docker containers orchestration are Kubernetes, Openshift, Docker Swarm.
Here are a few companies and startups that use Docker right now: PayPal, Visa, Shopify, 20th Century Fox, Twitter, Hasura, Weaveworks, Tigera.
To get more information about Docker, refer here:
Are Serverless and Docker the competing platforms? Hardly. They are mutually supporting parts of the dynamic world of cloud computing. Both services used to develop microservices but work for different needs. If you want to reduce application management and don’t care about the architecture, then Serverless is the best option. If you want to deploy an application on specified system architecture with having control over it, then Docker containers are the best option. So when comparing Serverless vs Docker, it comes down to choosing what is better for your particular needs.
TechMagic works a lot with Serverless architectures utilizing JavaScript stack and AWS or Google infrastructure. We are a certified AWS Consulting Partner and an Official Serverless Dev Partner.
To receive more information about TechMagic’s cloud computing capabilities and services, contact us at hello@techmagic.co or through the contact form.
techmagic.co
All about JavaScript, AWS, and Serverless in one place.
316 
5
316 claps
316 
5
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
"
https://medium.com/%E6%A2%97-%E7%A7%91%E6%8A%80/%E5%A5%BD%E6%A3%92%E6%A3%92%E7%9A%84%E9%A8%B0%E8%A8%8A%E9%9B%B2-%E5%AE%A2%E6%88%B6%E8%B3%87%E6%96%99%E5%85%A8%E6%8E%9B-%E7%84%A1%E6%B3%95%E5%BE%A9%E5%8E%9F-%E5%8F%AA%E8%B3%A013%E8%90%AC%E4%BA%BA%E6%B0%91%E5%B9%A3-5670dc4bbc44?source=search_post---------241,"There are currently no responses for this story.
Be the first to respond.
雲端服務有風險，尤其是某些公司的服務風險更大，你還是多幾個異地備份吧。
這年頭的新創，多半都把服務和各種資料放在雲端，但某些雲端服務一但出包，可能就會把公司搞到倒。
像這個例子：
"
https://scientya.com/a-beginners-guide-to-the-basics-of-what-cloud-computing-is-about-e8b3b7f25a30?source=search_post---------242,"Forget about definitions and technical explanations. Not everyone is a highly skilled DevOps Engineer and Cloud Architect. Salesforce CEO and Chairman Marc Benioff brings it straight to the point. Cloud computing is a better way to run your business according to him. The possibility of improving your business is something which is appealing to every decision maker across the globe. Who thought that the cloud would one day be considered as the digitization tool par excellence as n’cloud.swiss Founder and Chairman André Matter emphasizes again and again. The year 2000 will always be remembered as the milenium of the more and more oustanding pace of technological progress. New and old technologies are as close together as never before. The almost uncontrollable increase in human knowledge leads to endless incremental innovations. The hunt for the next big thing seems to be never ending.
Even though cloud computing still plays that role, one might think that it is being pushed aside by the likes of artificial intelligence and blockchain. But wait… most new technologies are distinguished by a huge number of data and intelligence. In other words, “the many disparate servers which are part of cloud technology hold the data which an AI can access and use to make decisions and learn things like how to hold a conversation. But as the AI learns this, it can impart this new data back to the cloud, which can thus help other AIs learn as well” as noticed Gary Eastwood from the IDG Contributor Network. Same goes for blockchain and other data intensive technology. Cloud computing is not only the digitization tool par excellence, it is omnipresent and plays undoubtedly a key role in today’s technological progress.
Providing IT resources and on-demand applications over the Internet at usage-based prices
“Simply put, cloud computing is the delivery of computing services — servers, storage, databases, networking, software, analytics and more — over the Internet (“the cloud”). Companies offering these computing services are called cloud providers and typically charge for cloud computing services based on usage, similar to how you are billed for water or electricity at home.” (Microsoft Azure)
Whether you run apps that share photos with millions of mobile users or support critical business operations in your organization, the cloud is a technology providing quick access to flexible and cost-effective IT resources. When it comes to cloud computing, you do not have to invest in hardware in advance or spend a lot of time managing it. Instead, you can provide the exact type and size of computing resources you need to implement your latest breakthrough idea or operate your IT department. You can access as many resources as you need almost immediately by paying only for what you use. Cloud computing provides an easy way to access servers, storage, databases and a full range of application services over the Internet. Cloud providers such as Amazon Web Services, Microsoft Azure, Google Cloud Platform or “Swiss made” n’cloud.swiss operate and manage the network-attached hardware needed for these application services, providing and using the resources you need through a web application.
Advantages of cloud computing
The cloud has become a technology that influences everyone’s daily life. The adoption of solutions and services in the cloud present a number of advantages and benefits, among others:
Types of cloud services: IaaS, PaaS, SaaS
Cloud computing consists of three main types, commonly referred to as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Choosing the right cloud computing type consists in knowing your needs to achieve an optimal level of control with worrying about unnecessary tasks. Microsoft defines these types as follows:
Infrastructure-as-a-service (IaaS): The most basic category of cloud computing services. With IaaS, you rent IT infrastructure — servers and virtual machines (VMs), storage, networks, operating systems — from a cloud provider on a pay-as-you-go basis.
Platform as a service (PaaS): Platform-as-a-service (PaaS) refers to cloud computing services that supply an on-demand environment for developing, testing, delivering and managing software applications. PaaS is designed to make it easier for developers to quickly create web or mobile apps, without worrying about setting up or managing the underlying infrastructure of servers, storage, network and databases needed for development.
Software as a service (SaaS): Software-as-a-service (SaaS) is a method for delivering software applications over the Internet, on demand and typically on a subscription basis. With SaaS, cloud providers host and manage the software application and underlying infrastructure and handle any maintenance, like software upgrades and security patching. Users connect to the application over the Internet, usually with a web browser on their phone, tablet or PC.
Cloud deployments: public, private, hybrid
There are three different ways to deploy cloud computing resources. These are public cloud, private cloud and hybrid cloud.
Public clouds are owned and operated by a third-party cloud service provider, and deliver computing resources like servers and storage over the Internet using a web browser. Today’s leading cloud providers Amazon AWS or Microsoft Azure are examples of a public cloud.
A private or on-prem cloud refers to cloud computing resources used in-house and exclusively by a single business or organisation. The particularity here is that a private cloud can be physically located on the company’s on-site datacenter.
A combination of both public and private clouds leads to what we call hybrid cloud. The advantage here is that a hybrid cloud allows data and applications to be shared between them. By allowing data and applications to move between private and public clouds, customers enjoy greater flexibility and more deployment options.
After which criteria do customers decide for a cloud provider?
Simply put, customers do not decide for one single cloud provider. Many companies pursue a multi cloud strategy to maintain the ability and flexibility to select different cloud services from different providers. Single multi-cloud infrastructure vendors like n’cloud.swiss are rare. Praised as a Swiss and European alternative to the major cloud providers, n’cloud.swiss is a cloud platform running in one of the world’s most secure data centers in Switzerland. The idea is to enable customers to design a cloud according to their specific requirements with the same product, either as a service model, an on-prem version in existing IT environments or as a hybrid variant. In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n’cloud.swiss application catalogue. Within the latter, n’cloud.swiss offers more than 142 applications from 30 different IT categories “free and ready to go” as well as the opportunity to upload also other development applications and tools easily. In addition, personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award this Swiss cloud platform a unique selling point and a competitive advantage.
Overview of n’cloud.swiss products und services
UNMANAGED PRODUCTS
MANAGED PRODUCTS
A cloud solution in minutesTry n’cloud.swiss for free1. Create an n’cloud.swiss account2. Start a virtual machine3. Join the world of n’cloud
The digital world publication
2.8K 
3
2.8K claps
2.8K 
3
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
"
https://medium.com/h-o-l-o/title-elemental-chat-the-first-holochain-p2p-app-released-for-hosts-3e9f44bdcaca?source=search_post---------243,"There are currently no responses for this story.
Be the first to respond.
When we first began this project, we shared our vision for an Internet of peer-to-peer web applications that would give users the power to direct their own online experiences. We connected with thousands of people also inspired by the possibility of removing centralising powers built into the infrastructure so that humanity might vastly expand its creative and collaborative potential.
The launch of Elemental Chat in our Alpha test certainly doesn’t get all that done. But it is a real step towards realising that promise. Elemental Chat running on HoloPorts has no central database. Each person who is running the app signs their messages to their own chain and then automatically posts them to the shared database that is hosted by the other users of the application. This is the start of something. This isn’t simply the seed of an idea. The deep roots of Holochain and Holo have fully sprouted and are now growing and strengthening.
This rapid recent growth is demonstrated in the timeline of events that led us to the current release.
The speed at which we have been able to move since the launch of Holochain RSM is indicative of how quickly and easily others will also be able to bring new P2P apps to market. So let’s dig into what Elemental Chat is, what it is not and what it lets us do moving forward.
Elemental Chat is simply a proof of concept application. What we mean by that is that it doesn’t have all the bells and whistles of popular messaging apps on the market today. You can create channels and you can send messages. You can also query the application network to see how many nodes are connected and how many people have the application open in a browser.
What you can’t do is limit what channels you see or follow, or send messages to only one person. You also can’t have reply threads, and in fact, the UI doesn’t even showcase all of the best features in Holochain. In this super-basic testing app, you see everything and can differentiate little which means a less than ideal user experience.
None of those wonderful user-friendly features are included with Elemental Chat, and that is intentional. We wanted an uncomplicated app with only a few features to avoid spending too much of our time testing and debugging a chat application, when our focus is really to build the underlying infrastructure that comprises the Holo distributed hosting platform.
Paul explained a few of the reasons why we chose Elemental Chat to release as the first distributed app for Holo quite nicely in the December 24 Dev Pulse:
“First, we thought it would be more fun to test; second, we knew it would push against all of Holochain’s capabilities in ways that would show up any bugs; and third, we wanted something that would show us where the performance bottlenecks were.”
But another important reason for us is because chat functionality illustrates the difference and potential of Holochain as compared to blockchain.
Releasing a chat app — even a toy app like Elemental Chat — demonstrates unequivocally that Holochain is designed for building classes of applications that could never really work on a blockchain. Think about apps where things like scalability, and speedy transactions or interactions are crucial for basic functionality. The vast majority of web apps that we use in our lives simply do not require the global consensus of a blockchain and in fact could be hampered if we try to force them into consensus models. Chat exemplifies this difference to us in the most natural of ways.
Imagine being at a party with 25 to 100 of your closest friends with people in small groups around a room chatting. Many different conversations are happening in the room simultaneously. You might be able to hear parts of many of them, but you are likely paying attention to one or two of them more than others. Each other person in the room is most likely doing the same — that is they are listening to the conversation they are having — not to the general humm of conversation in the entire space. Because of that, each person has their own unique experience of what is being said. I doubt anyone at that party would say there was something wrong with the way people were communicating. In fact — that very normal in-person multi-sensory type of setting is exactly what many of us have been missing this past year due to the pandemic.
So let’s talk about how a typical blockchain might try to model the communication at that party. One person (Alice) might say something. Another person (Bob) would need to process it and clarify to everyone else what was said and would record it and ensure that each other person understood it the exact same way. The first person (Alice) could not say something again until the first thing she said was processed by every person in the room. Some sort of order would need to be created for who was speaking because each statement would need to get placed in the correct order and shared with every other person in the room for the conversation to be deemed valid.
This sounds ludicrous — because it is. Consensus models are irrelevant for many human needs and when it comes to collaboration they are often extremely inefficient. With blockchains that inefficiency is evidenced in multiple ways but perhaps most explicitly by the extreme consumption of energy and creation of waste used to power the networks.
The reality is many applications only need to do two things really well: enable ease of coordination and reach eventual consistency.
Let’s take a look now at how Holochain’s natural patterns approach would model that same party. As you speak with a small subset of your friends, your messages are saved to your local chain. Your friends that are participating in your conversation would receive an immediate signal and hear your message. After someone’s message is saved to their local chain it is also saved to the group database or DHT. After a short while, your part of the shared database would get updated with messages from other conversations happening elsewhere in the party — and eventually you’d have access to all the gossip messages from the party. Sounds like a great and memorable party!
The use of natural patterns to model digital interaction is something that makes Holochain different — it’s why Holochain enables applications that can truly scale and why it’s ideal for collaboration. Holochain ensures agency for each user but does not require imposing a singular perspective.
So we can see why Holochain is great for modeling a chat app, but many were expecting us to release the app that will run Holo’s cryptocurrency HoloFuel. People often ask “How can HoloFuel work without consensus?”
Imagine a bunch of parents in a community create a babysitting time swap system and have a phone app where I can credit you for three hours of babysitting, and my babysitting balance goes down by three hours when I do. Some people have provided more babysitting than they’ve received and they’d show positive balances of hours. Other folks who have received more babysitting than they’ve provided, would have a negative balance of hours.
If Alice needs to transfer some credits to Bob, do we really need global consensus? Do we need to know the state of every account in the system? No. Only Bob’s and Alice’s accounts are changing from this transaction, and they are the only authorities to make changes to their accounts. They can sign a single identical transaction to each of their chains (a transaction that shows Alice spending credits and Bob receiving them), and when they publish it to the rest of the network, others on the network will validate the transaction to make sure it follows the shared agreements. It is basically that simple. (Well it’s not quite that simple, but for a better understanding of how Holochain works, check out this twelve minute video.)
This model for currency is more like peer-to-peer accounting of value, where your chain holds the history of your transactions. Not everyone is transacting in hours of babysitting but the principles of the model work exactly the same way when you replace babysitting with distributed cloud hosting as the service, and it will work that way for food, energy, and many other applications as well.
The release of Elemental Chat for hosts is an important step forward on the journey. For us though, the next infrastructure milestone is the one we’ll be doing backflips about. That is when non-hosts — regular web users — will actually get the experience of creating an account and logging into Elemental Chat that is hosted on a hosts’ HoloPort. We are already beginning the testing cycle for that milestone.
In the coming weeks, you can count on us to support more and more hosts to connect their ports to the Holo network and to begin chatting in Elemental Chat. We will be assessing how well the performance scales and listening to what folks have to say about it.
We will most likely make a few small changes to the chat app as we prepare for the wider release to web users. We will continue to evolve the testing framework so that it can be used more generally by hosted apps, not just applications we directly install on HPOS.
As we shared in the previous article, we are also moving forward on several of the other milestones that come after Hosted Elemental Chat. We may even be releasing the Host Console work simultaneously, but we are not promising that yet. After that, we’ll be focusing on the HoloFuel application.
What isn’t visible from these diagrams, however, is the work on Holochain that is continuing to evolve and which is critical for both Holo and community application developers who are building apps in our ecosystem.
Holochain has been steadily improving usability of the framework. When we first announced RSM, though it was a vast improvement technically, we hadn’t started releasing to crates, nor stabilized the Holochain development kit (HDK). We have recently shipped a raft of breaking changes to the HDK which should be fairly stable now. We’ve also ‘nixified’ Holochain to make it easier to install and ensure identical installations among collaborators. You can stay current with the latest tested version via Holochain.love.
In the coming weeks, there will be additional improvements to Holochain. Following the changeover of the database engine, we’ll be implementing sharding and other key network scaling features before we move on to more security and identity management features. All of these are foundations necessary for the Beta release of Holo.
So, I will leave you with yet another shout out to all of those who are supporting this amazing work coming into being. Kudos to the dev teams for bringing this release across the finish line. Kudos to the community-facing teams who are supporting hosts getting connected to the network. Kudos to all the app devs in the community pulling for new features and demonstrating how natural patterns work in Holochain. Kudos to all the admins and volunteers who share their knowledge with newcomers to the project.
In gratitude,Mary
Post-blockchain technology, value-stable cryptocurrency…
1K 
1K claps
1K 
Written by
Holo is a distributed cloud hosting platform for peer-to-peer Holochain apps (hApps); building a better web. Powered by @Holochain
Post-blockchain technology, value-stable cryptocurrency, and peer-to-peer hosting of scalable, distributed apps. https://holo.host
Written by
Holo is a distributed cloud hosting platform for peer-to-peer Holochain apps (hApps); building a better web. Powered by @Holochain
Post-blockchain technology, value-stable cryptocurrency, and peer-to-peer hosting of scalable, distributed apps. https://holo.host
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://debugger.medium.com/i-joined-google-one-and-i-bet-you-will-too-9ffc8eca9324?source=search_post---------244,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Ulanoff
Sep 28, 2021·3 min read
I fought and fought. In truth, I deleted and deleted in a futile attempt to maintain enough free Google Cloud Storage space to continue receiving Gmail.
They were a continued source of stress, the messages informing me that I’d eaten up 97% of my Gmail storage space, and I’d been seeing them for at least a…
"
https://medium.com/google-cloud/google-cloud-load-balancer-setup-tweaking-and-observations-c12d704e6d52?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Load Balancer (GCLB) is a software defined globally distributed load balancing service. It enables GCP users to distribute applications across the world and scale compute up and down with very little configuration and cost. It allows for 0 to 1 million requests per second (rps) with no pre-warming. Pricing starts at $.025/hr or $0.6/day or $18/month for global anycast (single IP) with autoscaling. It’s low cost and insanely powerful technology available for anyone to test today for free.
In this article I will test and demonstrate some of the product features highlighted on https://cloud.google.com/load-balancing/
There are a few deployment types for the GCP load balancer service.Global external:1. HTTP(S) — Public only, single or multi-region2. SSL proxy, TCP Proxy — Public only, single or multi-regionRegional external:3. Network — Public, TCP/UDP, single regionRegional internal:4. Network — Internal only, TCP/UDP. Can be used in-conjunction with #1 HTTP GCLB.
In this article we will test #1 HTTP public load balancing.
The GCLB can help improve end user latency with the power of Google’s huge network. Check this article here for more detail measuring and comparing latency with different GCLB deployment types.
Health checks, backends, and front ends are the core elements of the GCLB. Focus on these if you are looking to setup load balancing quickly.
Health checksThere are 2 types of health checks you should be aware of. #1 is required by the load balancer and #2 is extra to ensure health of instance groups (autohealing).1. Used by the load balancer that ensures that requests are only sent to instances that are up and running2. Used by instance groups to recreate instances that are unhealthy by your set criteria. Ex: HTTP health check port 80; if webserver fails, instance is terminated and recreated.
Health checks are setup within the Compute Engine of the UI as they are used in more than one place in GCP. You can run health checks on instance groups without the load balancer as well. If the healthcheck notices an instance is not healthy it will terminate and recreate.
The GCLB will manage instance health for you and the instance group will manage autoscaling so make sure you have both configured and tuned per your requirements. You can keep them basic; HTTP port 80, if the web server or instance fails the instance group will terminate and recreate a new one.
Backend Services These are the services that direct incoming traffic to instance groups. The backend service for a single load balancer can contain a number of backends or buckets. You can have 1 backend service going to multiple instance groups. Each backend is a really a link to the instance groups you wish to distribute traffic between.
So say you plan to have 3 instance groups (US, EMEA, ASIA) setup behind the HTTP(S) load balancer, they are configured 1 backend service. Here is how the back end service looks configured with 3 instance groups (backends) using the health TCP health check we created earlier.
Host and path rulesThis is where you can send subdomains or different hosts to different back ends. Example, if I had eu.mydomain.com I could send those requests directly to my autocomplete-demo-eu instance group. Host and path rules are automatically configured for * to send to the backend service you create so you do not have to worry about configuring these if you are doing basic http load balancing.
FrontendsThe front end is your virtual IP (VIP) or called anycast IP in GCP. One front end can service multiple regions (backends). In most cases you would want a static or reserved IP and not the default ephemeral. This way you can easily point an a-record on your Cloud DNS zone file to your load balancer IP.
That’s it for configuration of the GCP load balancer. Easy huh? After you review and update configuration in a few minutes you’ll have traffic going through your front end to your instance groups.
Health checksThe portal provides a ⚠️ suggestion after a health check is added to an instance group. The warning provided is that you may want to increase the check interval and unhealthy threshold as shown below. This is working to help you reduce the amount of false positives that may come with services under heavy load. Consider changing your check interval from 2s to 10s, and unhealthy threshold from 2 attempts to 6 attempts if you are just getting started.
Initial delayThis is a setting on the instance groups configuration that specifies the delay after an instance has been replaced for the health check to run again. Default is 300 sec which is 5 minutes. Hopefully your instances are light enough to boot in or around a 1 minute. If you think you may need more time for your application to come online after initial boot by the instance group autoscale action, then increase the initial delay.
RoutingIncoming client requests are sent to to the region closest to the use assuming the region has capacity. If more than one zone is configured with backends, traffic is distributed across each zone. Check my behavior observations below for more details. Within the zone requests are distributed via round robin. You can override round robin in zone by configuring affinity.
AffinityTypically the LB is going to route new requests to any instance and traffic from one connection is going to route to the same instance. Say you want to set stickiness to make sure all connections from one client go to the same instance. Configure session affinity to client IP. You can also set by cookie. The GCLB sends a cookie on the first client request and future incoming requests with that cookie will be sent to the same instance.
Scaling instances back or maintenanceEasy, just drop an instance group’s number of instances. GCE takes all instances in that group offline and you are no longer charged. Now you can scale back or perform maintenance, maybe change an instance template to a new version of software in a region. Take a region out of service by removing the instance group from the load balancer back end to perform maintenance, test, or upgrade software. Then re-add that group and push changes across other regions.
Minimum sizingI noticed the GCLB performed best with 3–4 minimum instances if using a single instance group as a backend. There is a slight lag for the LB to pickup the new instance information so its probably not the best idea to run a backend with 1 instance group with 2 instances at a very minimum. Multiple instance groups should be fine with 1–2 instances, just be aware and test the slight lag with the GCLB to pickup new instances added to groups. Tweak the initial delay threshold if you need to run a small amount of instances.
IPv6 SupportYou an attach a IPv6 Ip to a GCLB and have the same type of routing globally as you would with IPv4. One strategy would be to configure the GCLB with an IPv6 address to handle all IPv6 traffic. Just create an additional forwarding rule with the IPv6 address. Then you can associate both IPv6 and IPv4 with the same load balancer and back end instances. More on IPv6 support here.
The GCLB is a managed service and intended to make intelligent decisions on routing and traffic shaping for you. Below are a few specific service behaviors observed for basic http load balancing.
I used Apache Bench for connection testing from a US and EU instance. Apache Bench (ab) is included in the apache2 package. More info on Apache Bench testing here.
Location based routing and traffic load distributionFirst test was sending 10,000 requests with 100 concurrent from an ab-tester US instance and the GCLB routed directly to my us-east1 instance group. So the GCLB routing works great ✔️.
Increasing the requests from 10,000 to 100,000 from an ab-tester in EU this time, initially routed to the europe-west1 instance group (blue), then distributed across all 3 instance groups to better handle the load. At the same time my instance group in EU scaled up to 7 instances to accept the bulk of the traffic. Intelligent load balancing with minimal configuration and seamless autoscaling works great ✔️.
About 30 minutes after the 100,000 query spike had long passed and the instance group started to stabilize and scale down gracefully.
Scale resources up and down with intelligent autoscaling ✔️.
I was going to stress test the GCLB up to 1 million rps just to see it perform with my own eyes. After doing more testing in excess of 500,000 rps instances I found the same behavior was demonstrated: flawlessly routing and balancing of traffic across regions. I had quickly gained trust in the service and saved myself the testing costs. Saved money by trusting the platform. ✔️
FWIW, here is the diagram of the HTTP load balancing example put together showing some of the elements covered in this post and where each element lives.
If you wish to learn more about the GCLB please check out the Google Cloud Next ’17 session given by Prajakta Joshi, PM for the networking team on GCP here. She does an excellent job explaining the services background and features in detail.
Special thanks to Prajakta and Jens for providing feedback on this article.
Thanks for reading and have fun with GCP!
Google Cloud community articles and blogs
217 
6
217 claps
217 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/what-to-do-when-you-dont-get-any-medal-in-a-kaggle-competition-b54cc433da3?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Andrew Lukyanenko
Apr 11, 2020·7 min read
Be like gradient descent — learn from the errors!
Several weeks ago one more Kaggle Competition has ended — Bengali.AI Handwritten Grapheme Classification.
Bengali is the 5th most spoken language in the world. This challenge hoped to improve on approaches to Bengali recognition. Its alphabet has 49 letters and 18 diacritics…
"
https://medium.datadriveninvestor.com/3-simplest-advice-to-begin-cloud-career-in-9-hours-7735ffe15c4b?source=search_post---------247,"There are currently no responses for this story.
Be the first to respond.
Quickest way to begin with Cloud in a weekend (and it’s free)
#SatyenKumar
"
https://medium.com/efficient-u/3-tips-to-organize-files-on-your-mac-69df3d8eb62a?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
Finder Tags — Use Them
We all have a “friend” that stores all their files on their desktop.
“I have a system — it works for me,” they say.
It doesn’t work for them.
These are the same people who probably have thousands of unread emails, and ask you to email Word docs back and forth to each other so you can each make changes…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@wisdomgoody/%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%A5%E0%B9%88%E0%B8%99-kong-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-konga-%E0%B8%81%E0%B8%B1%E0%B8%99-%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7%E0%B8%88%E0%B8%B0%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%A7%E0%B9%88%E0%B8%B2-kong-%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B9%84%E0%B8%94%E0%B9%89%E0%B8%A1%E0%B8%B5%E0%B8%94%E0%B8%B5%E0%B9%81%E0%B8%84%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-api-gateway-%E0%B8%97%E0%B8%B1%E0%B9%88%E0%B8%A7%E0%B9%86%E0%B9%84%E0%B8%9B-%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88-2-8475f2fe8b40?source=search_post---------249,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ploy Thanasornsawan
Aug 12, 2019·6 min read
จากบทความตอนที่ 1 :การตั้งค่า ที่เล่าเรื่อง Kong กับ Konga คืออะไร รวมถึงการใช้ portainer และ set up konga เริ่มต้นเรื่องการสร้าง service, route และ consumer ใครยังไม่ได้อ่านและยังไม่รู้ว่า Kong คืออะไร สามารถไปอ่านได้ที่ลิงค์นี้
ขอเกริ่นเริ่มต้นว่า ในบล็อคนี้เราจะพูดถึง plugin ตัวไหนบ้าง
1.Authentication คือการยืนยันตัวตนเมื่อเข้าใช้งาน ซึ่งอันนี้เราจะเอามาใช้ก่อนเข้าใช้งาน service นั้นๆ ซึ่งจากบทความตอนที่ 1 เราได้มีตั้งค่า credentials ไปสองแบบคือแบบ Basic Auth และ API Key คราวนี้เราจะมาลอง enable plugin authentication ให้กับ route ของเรากัน โดยตอนนี้เรามี service 2 ตัวคือnginx -> จะให้ใช้ plugin basic-auth webHttpd -> จะให้ใช้ plugin key auth
เปิด konga ขึ้นมา http://konga:1337 คลิ๊กไปที่ Tab services -> name service(nginx)-> routes -> name route(nginx) -> plugins -> ADD PLUGIN -> basic-auth -> ADD PLUGIN ตรงนี้ของ basic-auth สามารถ letf blank all แล้วกด ADD PLUGIN มันจะไปดึงการตั้งค่า username, password ที่เราทำไว้ใน consumer มาให้
ทีนี้มาทดสอบ service nginx กันว่ามี basic-auth มารึยังโดย route nginx คือ /test
ทีนี้มาลอง add plugin authentication ประเภท api-key ให้กับ service webHttpd กัน คลิ๊กไปที่ Tab services -> name service(webHttpd)-> routes -> name route(webHttpd) -> plugins -> ADD PLUGIN -> key auth -> ADD PLUGIN โดยอันนี้สิ่งสำคัญในการกรอกมีแค่ชื่อ key ในที่นี้ขอตั้งชื่อว่า X-API
มาลอง test service route webHttpd กัน โดย path ของ webHttpd คือ /users
มันจะแจ้งเตือนว่า เราไม่มี API Key ให้มัน เพราะงั้นถ้าจะทดสอบก็ไปโหลด postman แล้วใส่ชื่อ key ว่า X-API และ value key ให้เอาจาก API KEY ที่สร้างไว้ใน consumer credential นำค่า key, value มาไว้ใน header แล้วลองส่ง request ด้วย GET
เดี๋ยวเราค่อยๆสังเกต header เวลาตอบกลับหลัง add plugin แต่ละตัวเข้าไปกัน
อย่างอันนี้จะมี Header 11 อันบอกว่า server ปลายทางที่ request ไปเป็น Apache และส่งผ่าน Kong version 1.2.1
2.Security ในที่นี้เราจะมาลองตัวง่ายๆ คือ cors แล้ว cors คืออะไร cors เป็นเหมือนข้อกำหนดในการขอข้อมูลข้ามระหว่างโดเมนระหว่างเบราเซอร์กับเซิฟเวอร์ ในที่นี้เราสามารถกำหนดให้ทุกเบราเซอร์เลยขอข้อมูลได้โดยกำหนดเป็น *
พอมาดูใน postman หลังกดส่ง request ไปอีกรอบจะเห็นว่ามี header เพิ่มมาอีกตัว เป็น Acccess-Control-Allow-Origin เป็น * คือให้ร้องขอข้อมูลได้จากทุกโดเมน
3.Traffic control อันนี้เราจะมาลองเล่น plugin rate-limiting เราสามารถกำหนดได้ว่าใน 1 second ให้มี request ได้กี่ครั้งหรือภายใน 1 ชั่วโมงหรือต่อวัน ซึ่งการกำหนดแบบนี้จะช่วยป้องกัน DDos (HTTP Flood) การส่ง request เข้ามายัง server เป็นจำนวนมากจน server ตอบกลับไม่ทันแล้ว server ตาย อันนี้ถ้าเกิดใครอยากลองใช้ command curl ในการ enable plugin ให้กับ route แทน konga ก็ได้
แล้วเราลอง refresh konga ดู จะเห็นว่ามี plugin rate-limit เพิ่มเข้ามา ถึงแม้ว่าเราจะไม่ได้เพิ่ม plugin ผ่านการกดบนหน้าเว็บ konga โดยตรง
ทีนี้ลองกดส่ง request ใน postman แล้วมาสังเกต header ที่เปลี่ยนไปกัน อันนี้ใน command เรากำหนดให้สามารถส่ง request ได้ 10000 ครั้ง ต่อชั่วโมงจาก X-RateLimit-Limit-Hour พอเรากดส่งอีกรอบตัวเลข 9999 ใน X-RateLimit-Remaining-Hour เป็น 9998 สามารถไปลองเล่นเองได้
4.Serveless คือเจ้าตัว kong สามารถไปเรียกใช้ serveless ของเจ้าอื่นได้ อย่าง AWS Lambda หรือ Azure Function ได้ ซึงโดยปกติแล้ว ถ้าใช้ AWS จริงๆ AWS เองก็มี AWS API Gateway เหมือนกัน แต่ข้อเสียคือเวลาจะใช้ API Gateway ตัวนี้ มันจะต้องเชื่อมกับทุกอย่างที่เป็น product aws หมด แต่ถ้า Kong คืออิสระจะไปเชื่อมกับใครก็ได้ ในตัวอย่างนี้เราจะใช้ kong ไปเรียก AWS Lambda กัน ส่วนความหมายของ serveless ทุกคนสามารถไปอ่านเพิ่มเติมได้จากลิงค์นี้
ซึ่งขั้นแรกทุกคนอาจจะต้องมีบัญชีของ AWS ก่อน ทางไปสมัคร ลิงค์นี้ แล้วก็มาที่ AWS Lambda กดสร้างฟังก์ชั่นในที่นี้่ตั้งชื่อว่า getRandomMessage
ส่วนตัวพลอยเรียน AWS จาก udemy คอร์สนี้ ซึ่งฟังก์ชั่น getRandomMessage อันนี้ก็ถือเป็นตัวอย่างหนึ่งในคอร์สนี้ด้วย (ไม่ได้ค่าโฆษณาใดๆ 555+ แต่มันเรียนเข้าใจ) ซึ่งเขาจะมีสอนเรื่องการสร้าง account IAM ด้วยแล้วโหลดไฟล์ aws credential มาเก็บไว้ เอาไว้ตั้งค่าเชื่อมต่อสำหรับ deploy code เชื่อมต่อกับ aws ซึ่ง aws credential จะถูกนำมาใช้ใน config ของ kong ด้วย
ทีนี้เราลองมาทดสอบใน postman กัน จะเห็นว่าหน้า index ของ webHttpd ที่เป็น It works เปลี่ยนเป็นข้อความที่มาจาก AWS Lambda เรียบร้อยแล้ว
มาสังเกต header หลังเพิ่ม plugin เชื่อมต่อกับ serveless ของ AWS กัน
5.Analytics & Monitoring ของ kong สามารถเลือกใช้ได้หลายรูปแบบมาก อันนี้จะขอยกตัวอย่างการใช้ Prometheus โดยคราวนี้จะลอง add plugin เข้าไปที่ตัว consumer แทน route เหมือนทุกๆครั้ง
อันนี้ตอนกด ADD PLUGIN เราไม่ได้ config ค่าเชื่อมต่ออะไรบน kong แต่เราจะไป config บน Prometheus แทนโดยการลง docker Prometheus แล้วเพิ่ม target
อ้างอิงจากบทความตอนที่ 1 สำหรับคนที่เครื่องเป็น windows แล้วต้องลง virtualbox สำหรับเล่น docker เราได้มีสร้าง vistualbox ของ prometheus ไว้แล้ว
เริ่มแรก ssh เข้าไปใน virtualbox เข้าเราก่อนด้วยคำสั่งdocker-machine ssh prometheusdocker pull prom/prometheusdocker run -d -p 9090:9090 prom/prometheus
ทีนี้มาแก้ config target ใน prometheus ให้ชี้ไปที่ kong admin โดยเข้าไปที่ path /etc/prometheus แล้วทำการแก้ไขไฟล์ prometheus.yml เมื่อแก้ไขเสร็จให้ทำการ restart container อีกที
ในรูปคือทำการเพิ่ม job_name และ static_configs ซึ่ง IP ตรงนี้เอา IP Kong ของตัวเองมาใส่นะ ไม่ใช่ IP ของตัวอย่างในบล็อคนี้
จากนั้นลองเปิดเบราเซอร์ promtheus ขึ้นมา จากตอนที่ลองสร้าง virtualbox ของตัวอย่างนี้ได้ IP อยู่ที่ 192.168.99.111 ส่วน port คือ 9090 ถ้าคลิ๊กที่เมนู status แล้วเลือก target ทุกคนควรจะได้แบบในรูป
พอกดมาที่เมนู graph จะเห็นว่ามี dropdown ข้อมูลของ kong ให้สามารถดึงข้อมูลมาโชว์ได้
ผลลัพธ์กราฟที่ได้
6.Tranformation อันนี้จะเป็นเรื่องการดัดแปลง request และ response ซึ่ง kong ก็มีให้เลือกหลายอย่างตามรูปเหมือนเดิม แต่ถ้าจะเอาชัดๆ เราเล่นที่ตัว response ดีกว่า พอยิง request ไปแล้วดู response ที่เปลี่ยนไป เช่น header เพิ่มขึ้น
ในที่นี้ add header ชื่อ name , value=ploy ตามรูป
จากนั้นมา test กับ postman กัน…
7.Logging เรื่องการเก็บ logs นี่มีประโยชน์มาก ทุก service ใช้คำว่าต้องมี !! เอาไว้เช็คความผิดปกติที่เกิดขึ้นกับ server แบบต้นทางถูกส่งมาจากใคร ใช้ method อะไร GET หรือ POST แล้วถ้า POST นี่มีการส่ง parameter แปลกๆมายัง server รึป่าว เรามาดูกันว่า Kong สามารถใช้ plugin logging กับอะไรได้บ้าง
จะขอยกตัวอย่างใช้เป็น File log นะเพราะใช้งานง่ายดี 555+ File log จะเป็นการเก็บไฟล์อยู่ใน docker ของ kong เอง ใส่แค่ path ให้ plugin รู้ว่าต้องไปบันทึกลงไหน
มาเช็คไฟล์ logs ของเรากัน
ที่ทำโฟกัสแดงไว้ที่ user-agent คือจะบอกว่า logs มันมีประโยชน์มากในการหาว่าต้นทางเป็นใคร แล้วก็อีกจุดที่โฟกัสเพื่อจะโยงเรื่องต่อไปคือ latencies ความหมายคือความล่าช้าของอินเทอร์เน็ต ซึ่งในรูป request:2312 เราจะมาทำให้มันเร็วขึ้นกัน
kong มีอีกปลั๊กอินที่น่าสนใจคือ proxy cache ความหมายของมันคือ ประมาณว่าในกรณีปกติ ถ้าไม่ใช้ proxy cache ถึงเราจะเรียก service ผ่าน kong แต่ kong ก็ต้องไป request ขอข้อมูลจาก server service นั้นใหม่ทุกครั้ง แต่ถ้าเกิดใช้ cache proxy ที่ kong คือ kong จะทำการจำให้ว่า web service ตัวนี้มีอยู่ในระบบที่ kong ดูแลอยู่หรือไม่ ถ้ามีอยู่แล้วก็จะไปดึงข้อมูลชุด request จาก cache มา ตอบกลับทำให้ไวขึ้น ไม่ต้องไปเรียก request ใหม่จาก server service นั้น ในขณะเดียวกันก็ทำการเช็คว่า service นั้นมีการปรับเปลี่ยนรูปแบบ request ด้วยไหม แบบอาจจะต้องใส่ header api-key เพิ่ม แต่ก่อนไม่มี ในแคชก็ไม่มีข้อมูล มันก็จะไปเรียกข้อมูลใหม่มาเก็บในแคชแล้วค่อยตอบกลับไปหาผู้รับ
ใช้คำสั่ง curl ในการ enable plugin proxy cache
หน้าตา plugin บน konga ใน route ของ webHttpd ก็จะประมาณนี้
ทีนี้เรามาลอง test ผ่าน postman กันอีกที แล้วสังเกต logs ที่เกิดขึ้น
จะเห็นว่า latencies หลังเพิ่ม proxy cache ลดลงคือเร็วขึ้นจาก 2312 เป็น 1461
passion coding and develop my world
See all (926)
183 
1
183 claps
183 
1
passion coding and develop my world
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/jandi-blog-tw/saas-cloud-2a0c332e1b95?source=search_post---------250,"There are currently no responses for this story.
Be the first to respond.
由於現在有太多新名詞被翻出來「重用」，以及台灣人普遍對於「資訊服務」似乎抱著一種懷疑不信任的態度，所以特別撰寫這篇短文，用最簡單的方式來介紹「雲端服務」，破除企業用戶與一般使用者的常見迷思！
「雲端」雖然是近幾年才火紅的炫詞，但其實當網際網路出現時，雲端就已經算是存在了，但為何我們「現在」才大談雲端運算與服務呢？因為：
雲端服務是多種「技術」與「商業需求」結合的必然結果，缺一不可。
就像你現在不可能回頭用 2.5G（85.6 kb）的行動上網了，幾乎所有人都已經是 4G（75 Mbps）技術的用戶了。（差距大約是 1000 倍）
如果我們的技術停留在 2.5G 那就不可能有直播與影音串流服務，因為速度不夠；如果社會上沒有這個需求，那麼直播與影音串流也不會出現。
因此，在網路基礎設施整體提升、移動化辦公，有網路的地方就有商業的趨勢下，讓部分企業市場開始意識到「在雲服務的協助下，才能讓運作效能提升數個層級」。
從最初的 CRM（客戶關係管理）、銷售管理、客服管理、專案管理、通訊協作類、進銷存貨、財務類……等等，幾乎包含了企業市場「衣食住行」的所有需求。
簡單來說，「雲」只不過是個想像出來的抽象形容詞，關鍵還是「技術與需求」，一句話總結：
雲端服務（Cloud Service）是結合雲端運算（Cloud Computing）、雲端儲存（Cloud Storage）、網路連線，與商業需求的新時代網際網路服務。
「這是套裝買斷軟體的終結」
因為現在的雲端技術，已經可以讓使用者在體驗上，達到與過去安裝在 Windows 或 macOS 作業系統上的應用軟體一樣順暢，甚至更容易維護（軟體安裝與版本更新），只要上網登入網站，就能跨平台跨裝置操作。
無論是技術面或者成本效益，雲端服務都已經來到了最佳的時間點。
我能理解，部分企業與使用者，因為過去習慣了套裝軟體與傳統的工作思維，即便想引入雲端服務，卻有著一些疑惑。
沒關係！就讓我試著點出常見的迷思，讓你們不用找資料找得這麼累！
大部分的雲端服務是採取訂閱制付費，無論 Office 365（微軟也從過去套裝買斷軟體進入雲端訂閱服務），Adobe Creative Cloud，還是企業通訊協作工具 JANDI 都是每月付費的模式，從帳面來看，每個月多了一筆支出？
但是，實際上這根本是請了一個超廉價的超強員工！因為透過「訂閱式服務」你將省去維修與更新費用，雲端服務確保你永遠都在最穩定、最快速的最新版本。
只要連上服務，就直接幫你更新維護，公司不需要一段時間就要請個工程師到公司進行維護，訂閱式付費，用不到一個員工的價格，解決一個部門才能解決的問題，甚至讓軟體保持在最新、最高效，最穩定的情況下，也不用擔心買到了盜版軟體。
另一方面，訂閱式也大量減少閒置資源，大部分的 SaaS 服務是按照「用量」收費。這概念就像是如果你一個月用不到 10GB 的流量，就不必去買上網吃到飽。付費的機制會跟著公司的用量而彈性決定，不必再一次買斷買了一堆，然後吃不下，這才是浪費。
看過剛剛的 Windows 勒索病毒 例子，你不覺得放在你自己的電腦上，或者公司的伺服器更不安全嗎？
市面上的雲端服務工具，大多是用「世界知名且可靠」的資料儲存中心（Amazon Web Services、Microsoft Azure、Google Cloud Platform；例如 JANDI 是用亞馬遜的資料儲存中心，進行加密與永久儲存。）
也就是說，除非這三間公司（Google、微軟、亞馬遜）被攻破或倒閉，不然你的資料是不會有人能看到與讀取的。（但如果真的發生了，相信造成的動盪會比上次的金融海嘯更慘。）
基於現在的網路基礎設施，只要你的電腦不是 Windows 98，都是能順暢運行的（X）
基於瀏覽器的主要好處是「跨裝置、跨平台」，你甚至可以用 iPhone 等智慧型裝置進行操作。
多數的雲端服務也有提供手機 APP 與電腦應用程式的版本，因此不論是不同的電腦作業系統，還是手機裝置的不同，連上服務時，所有公司同仁看到的畫面與內容都會是一致的。
這個問題問的很好，我們在第一點提到了「訂閱式的雲端服務大量降低了企業用戶的維護成本」，所以雲端服務公司的員工組成，比起以往要有大量的業務人員，現在則是更多的工程師與設計師。
然而，因為網路市場的競爭特別激烈，能存活兩年以上的雲端服務，在操作上多半簡單易懂。
另外，除了必須將使用者的體驗設計到非常容易使用之外，提供「即時的線上客服」也成為了雲端公司的必備元素。目的是讓企業用戶，不必在騰出時間請人到公司，「引入試用、教育訓練、購買訂閱、維護更新」，四個流程都能在線上完成！
當然，也有較佛心的雲端服務公司提供了「線下的教育訓練與諮詢服務」，因此「服務商的在地化」也成為企業端選擇服務時的關鍵之一。因為我們不會希望在詢問客服時還要寫英文吧？或者要找真人詢問建議時，他這麼回了一句：
「不好意思，你的所在地區不提供教育訓練。」
破除 4 個常見迷思之後，讓我提供幾個「雲端工作情境」給你參考：
擁有許多分公司或分店的組織，異地協作非常困難。但擁有了雲端思維後，工作與溝通變簡單了，不用設置很多主機，資料就能即時同步，不會再因為紙本紀錄下的版本錯亂而疲憊煩躁，雲端技術能讓大家像是在同一地工作，即時同步地瞭解每個分店的狀況。
客戶需要知道訂單的生產狀況、是否已經出貨、物品維修情形……等等。對企業來說，傳統的方式（電話或傳真）提供客戶所需資訊，是非常耗費成本的，而且請一堆業務的話，人力與管理成本更是昂貴。
再來，在資訊系統尚未完善的公司，業務人員不一定能在接到電話後，立刻幫客戶解決問題，可能需要到生產線上詢問作業人員。
若有好的雲端系統，客戶能隨時連上系統取得想要的資訊，公司內部也能即時同步檔案與文件，也因為雲端服務的保密性與資料的永久儲存，保障了資訊安全。（拜託不要用 LINE 了，廣告與朋友的干擾，以及資訊安全問題，並不適合企業內工作使用。）
雲端工作思維，是資訊時代下的必然產物，你不跟上，就是落後了。不只是節省人力與資訊維護成本，甚至創造更多的產能與時間。
就是訂閱了一個超強的人工智能，而不是一堆大幅增加你管理成本的員工。
如果你也有上述困擾，想徹底為團隊進行健康檢查，希望自己能駕馭雲端工作術，提升團隊效能跟執行默契，推薦這場「體檢工作坊」給你！除了可以透過實際操作體驗雲端工作術，還能進行互動式提問，進行客製諮詢，讓你成為「雲端工作思維」，快樂工作者的一員！
「點此分享」此篇到臉書，給更多需要的朋友。
JANDI 帶給你全新的工作體驗，讓你用習慣的溝通方式，一站完成繁忙的工作！這裡是 JANDI…
835 
Thanks to 曹凱閔 (KM Tsao). 
JADNI Taiwan 的 Medium 訂閱。 主站已搬遷至 blog.jandi.com/tw/ ，歡迎參考。 Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
835 claps
835 
Written by
《雷蒙三十》 Podcast 主持人，分享高效工作與品質生活的一人公司故事｜18 歲印度旅行，22 歲成為外商遠距工作者，24 歲結婚到北京互聯網做產品和運營，26 歲回到台灣和老婆一起帶著電腦到處工作，懷著任性熱愛生活。｜▲ 最新內容在個人網站：raymondhouch.com/blog
JANDI 帶給你全新的工作體驗，讓你用習慣的溝通方式，一站完成繁忙的工作！這裡是 JANDI 提供國內外最新工作生產力的內容部落格，讓我們一起更快、更樂，成為快樂的工作者！
Written by
《雷蒙三十》 Podcast 主持人，分享高效工作與品質生活的一人公司故事｜18 歲印度旅行，22 歲成為外商遠距工作者，24 歲結婚到北京互聯網做產品和運營，26 歲回到台灣和老婆一起帶著電腦到處工作，懷著任性熱愛生活。｜▲ 最新內容在個人網站：raymondhouch.com/blog
JANDI 帶給你全新的工作體驗，讓你用習慣的溝通方式，一站完成繁忙的工作！這裡是 JANDI 提供國內外最新工作生產力的內容部落格，讓我們一起更快、更樂，成為快樂的工作者！
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GorillaStack/aws-instance-scheduler-vs-gorillastack-250584e73016?source=search_post---------251,"Sign in
There are currently no responses for this story.
Be the first to respond.
GorillaStack
Apr 16, 2018·4 min read
A lot of organisations come to us for help after they have spent weeks wrestling with AWS Instance scheduler. Other users who have never tried to grapple with the 40 page instructional document ask us why they shouldn’t use it instead of GorillaStack.
The AWS Instance Scheduler is a solution written by the AWS Solutions team, based off an old blog post outlining how to write your own instance scheduler. That guide and this solution provide users with a roll your own instance scheduler. We will outline the key points of difference between the AWS Instance Scheduler and the GorillaStack Rules Engine.
The first and most obvious difference between GorillaStack and the AWS Instance Scheduler is the difference in the feature set. GorillaStack’s Rules Engine can be triggered on multiple triggers (not just schedules) and can perform a far greater set of actions beyond scheduling instances, around preparation for disaster recovery, snapshot creation/retention, patching, auto scaling management and DynamoDB scaling, to name just a few. GorillaStack also completes the feedback loop by providing an Event Log (with audit information and execution history) as well as our Engine Room (providing ROI and savings tracking to report back to the business).
Many teams that we speak with are time poor and lack the human resources they need for projects that deliver genuine value to their business. Managed solutions have become increasingly popular over the last few years, as businesses realise the critical importance of maintaining focus on business objectives.
While the AWS Instance Scheduler seems straightforward to deploy and use, there are hidden complexities in the nature of its implementation, configuration and maintenance. These problems are compounded in enterprises, with complex, large, multi-account environments and distinct teams with different requirements.
GorillaStack provides enterprise-grade automation software as a managed service, with all the features that large businesses require (SAML, role based access control, audit history on rule configuration) and all the features that the end users want (notifications, actions, customisation).
GorillaStack has been designed for large, dynamic teams and continues to be developed by their feedback. Some examples of significant features differences that make a difference to users:
Another area where GorillaStack’s Rules Engine really shines is how it manages granular targeting of resources. Users implementing the AWS Instance Scheduler need to make decisions in implementation about whether to implement a cross region or cross account flavour. If configured for multiple accounts, all schedules will always apply to all accounts and all regions. In GorillaStack on the other hand, for each rule, the user has the option to select which regions and accounts at a per rule basis, so all accounts and all regions is an option, but isn’t mandated.
Within the AWS Instance Scheduler deployment, the user is required to specify a particular Resource Tag Key, which will be used to consider Resource Tag values to match against each schedule configured. This means that every resource to be targeted can only be identified on the presence of a single tag.
In GorillaStack on the other hand, we provide the notion of TagGroups. Users specify a combination of Resource Tag Key:Value pairs and matching strategies (case sensitive, case insensitive or using regular expressions). The user can then combine these using a boolean expression to define how to match against resources at runtime. This gives the ability to cut into specific subsets of resources at a far more granular level.
GorillaStack serves small businesses and startups all the way through to some of the largest private enterprises and government organisations in the world. The common strand that runs through each of our customers is their focus on innovation and progress. The best practicing organisations recognize the importance of enabling their teams to focus on the core work that drives towards a business’s overall strategy whilst allowing the undifferentiated heavy lifting to be taken care of by tools that were specifically designed for the job at hand.
Originally published at www.gorillastack.com on April 16, 2018.
AWS Cost Management — Be a DevOps Hero with our automated cost optimisation software for Amazon Web Services
1.3K 
1.3K 
1.3K 
AWS Cost Management — Be a DevOps Hero with our automated cost optimisation software for Amazon Web Services
"
https://medium.com/@containerum/how-to-deploy-wordpress-and-mysql-on-kubernetes-bda9a3fdd2d5?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Containerum
Sep 11, 2018·6 min read
by Dmitry Krasnov
There are hardly many people who have not heard of WordPress — arguably the most popular CMS for websites and blogs. It is available as a docker image (over 10 million pulls on DockerHub), and by running it on Kubernetes you can build a reliable and scalable website platform.
In this tutorial I will show you how to deploy WordPress and a MySQL database using K8s. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.
What are they? PersistentVolume (PV) resources are used to manage durable storage in a cluster. PersistentVolumes can also be used with other storage types like NFS. A PersistentVolumeClaim (PVC) is a request for storage by a user. Refer to the Kubernetes documentation for an exhaustive overview of supported PersistentVolumes.
In this manual we will go through several steps:
We will use NFS server to store WordPress and MySQL data. Let’s prepare it first.
Open ports TCP:111, UDP: 111, TCP:2049, UDP:2049
Now we will share the NFS directory over the private subnet of Kubernetes:
You need to change “172.31.32.0/24” to your private cluster subnet.
Create a backup directory for mysql & wordpress files for volumes:
Create a Secret for MySQL Password
A Secret is an object that stores a piece of sensitive data like a password or key. Each item in a secret must be base64 encoded. Let’s create a secret for admin use. Encode the password (in our case — admin):
You will get the encoded password: YWRtaW4=
Create a secret.yml file for MySQL and Wordpress that will be mapped as an Environment Variable as follows:
Don’t forget to add your password to the secret.yml above.
Then run:
$ kubectl create -f secret.yml
Deploy Persistent Volume for WordPress & MySQL
Create PV files and change the IP address of the NFS server you are using.
Now run the following command to create a PV:
Deploy PersistentVolumeClaim(PVC)
PVC is a request for storage that can at some point become available, bound to some actual PV.
Create a PVC for wordpress with the following content:
And then run:
$ kubectl create -f pvc-wordpress.yml
Do the same for MySQL:
Run:
Great, now let’s deploy the apps!
Here’s the mysql-deploy.yml for MySQL service and deployment:
The file consists of 2 separate configs:
The Service part maps MySQL’s port 3306 and makes it available for all containers with the labels app:wordpress & tier:mysql.
The Deployment part declares the creation strategy and specs of our MySQL container:
Now create the deployment and service:
The process above is pretty much the same for WordPress. Here’s the wordpress-deploy.yml:
Again, the file consists of two configs:
Service maps port 80 of the container to the node’s external IP:Port for all containers with the labels app:wordpress & tier:frontend
Deployment declares the creation spec of our WordPress container:
Here’s what we do next:
To access WordPress list the services and navigate to the External IP:Port , in our case that would be EXTERNAL_IP:32723.
Done! Now you can create your own blog or website in the WordPress web panel.
You can check if everything works fine by using the following commands:
On NFS Server
Check NFS server and you will see your data under /mysql & /html
After installing Wordpress try to delete one pod and it will mount data from /html automatically:
We have deployed a wordpress with MySQL, Persistent volumes, and NFS on Kubernetes. The main benefit of this stack is flexibility since it allows you to implement practically any type of workflow. This workflow can be extended or complexified depending on your development needs.
Thanks for reading! Feel free to leave your feedback. Don’t forget to follow us on Twitter and join our Telegram chat to stay tuned!
Containerum Platform is an open source project for managing applications in Kubernetes available on GitHub. We are currently looking for community feedback, and invite everyone to test the platform! You can submit an issue, or just support the project by giving it a ⭐. Let’s make cloud management easier together!
Containerum Platform for managing applications in Kubernetes.
See all (13)
265 
6
265 claps
265 
6
Containerum Platform for managing applications in Kubernetes.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/how-gclouds-billing-issue-caused-a-business-shutdown-for-one-day-948962ecc50e?source=search_post---------253,NA
https://codeburst.io/why-syncing-to-the-cloud-is-not-always-the-right-solution-8b76cb64039d?source=search_post---------254,NA
https://blog.heptio.com/perspective-on-multi-cloud-part-1-of-3-6396caf522b5?source=search_post---------255,"Multi-cloud keeps coming up in conversation these days. In this series I will (hopefully) provide useful perspective on the topic of cloud providers, the utility of decoupled architectures, being smart about cloud provider lock-in, and the tension that exists between private and public cloud deployments.
I don’t claim prescience here, but having been quite active in building a both a public cloud offering (Google Compute Engine), an open source abstraction on infrastructure (Kubernetes), and more recently working with customers running on a broad cross section of infrastructure technologies. I hope you will find my perspective useful.
Since this is a pretty broad topic I will write this series in three parts: (1) some thoughts on the cloud providers and the role of technologies like Kubernetes in multi-cloud deployment, (2) the gap that exists between public cloud and on-premises cloud platforms and (3) (this will be a little more technical) some thoughts around federation, multi-cloud architecture and the roles cloud providers may take.
These posts will be grounded in a set of observations on the current ecosystem, and offer some friendly advice (actual mileage will vary 🙂). Let’s get going:
Observation 1: The public cloud isn’t a one horse race. Microsoft and Google are running. Hard.
Amazon has done some truly remarkable work. They defined a genre and despite the obvious advantages of their emerging competitors (Microsoft and Google) have executed superbly. Today they have a clear lead in this space and are the horse to beat. Every re:Invent conference showcases an impressive array of new services and the pace of innovation is palpable. Despite the remarkable and obvious execution prowess that Amazon has demonstrated I am nevertheless bullish on the futures of both Microsoft and Google. This isn’t about Amazon’s lack of execution, they have done legitimately wonderful work. It is recognizing and understand the emerging strength of their competitors. Google and Microsoft have strong pre-existing leverage-able assets that they bring to this segment that should be understood.
Microsoft is executing well. Both in the overall quality of their cloud offering, and in their ability to sell it to people. It has been startling to see the overall improvement in the quality of their underlying infrastructure in a relatively short period of time. If you looked at Azure a year ago and decided to pass on the infrastructure, I encourage you to take another look; it is a fundamentally different platform than it was 1 year ago.
Beyond the trajectory of their service quality, they have shown a strong pragmatism with respect to open source software. This is both refreshing and obviously necessary for them to emerge as a true force in cloud. They have obviously moved past a ‘Windows only’ world. Azure’s position with everything from operating system to orchestration technology points to an ‘accept all comers accepted’ world view. If you need an example of this, think about how unlikely it would have been 5 years ago to see Microsoft join the Linux foundation at the platinum sponsor level.
I look at the trajectory Microsoft is on, and having a decent sense of their relentlessness as a company (I worked there for over a decade), and can unequivocally say they play ‘for keeps’. Keep an eye on them.
Google is the dark horse in this race. They don’t have the sales and go-to-market muscle of Microsoft (yet), but the underlying technology is legitimately good. When you look at their suite of offerings, their network stands out first and is quite unique. Everything transits on their global private network. The core compute offering is also surprisingly good. Beyond the more mundane services they are creating new classes of technology that other cloud providers will struggle to compete with. Look at their Spanner service — using a global array of atomic clocks they have created a highly scalable, globally consistent relational database. I cannot overstate how technically hard this is to accomplish. You will get some of the way there with mundane open source technologies, but in the end without the atomic clocks you will miss the magic. Google’s services offerings are massively differentiated vs what anyone else can deliver. No one else can do this.
Their machine learning offering is also head and shoulder’s above anything else commercially available in the industry from what I have seen: nothing even comes close when you factor in the TPU acceleration of Tensor Flow. (Side note: Amazon and Microsoft invested in FGPA offerings that will likely considerably accelerate things, but there is definitely something to be said for purpose built silicon that is built around the open source base technology.)
First point of friendly advice: Even if you are not ready to go multi-cloud, it makes sense to ‘hedge’ on being able to benefit from the services and capabilities other cloud providers offer (if not now, certainly in the future). Think about open source technologies like Kubernetes and Docker as a way to decouple your developer workflows and apps from your cloud provider.
Observation 2: Nothing creates vendor lock-in like provider specific service dependencies, but that is okay as long as you are being smart about it.
A lot of folks argue that the key point of lock-in is data gravity. Once your data is in one cloud it is impractical to move it. There is some truth to this, but it is technically easy to solve. Amazon demonstrated at the recent re:Invent that there is nothing in the world that can move data more quickly than a semi truck full of hard drives. While those trucks only make one way trips for now, it is reasonable to assume that competitive and legislative pressure could change this.
In my mind the key point of lock-in isn’t data locality. It is cloud provider unique services. Now I am in no ways suggesting that enterprise be afraid of cloud provider services. There are some really useful capabilities that you will only get when you buy into a service. Just recognize that detangling your infrastructure from deep service dependencies will likely be even more difficult than getting your data out of a cloud provider’s offering.
Efforts like the open service broker API working group are tackling the challenge of creating a structured way to map services into a cluster environment. Definitely keep an eye on this work that takes the Cloud Foundry service broker as a working baseline and generalizes it for broader platforms.
Second point of friendly advice: Think about using formal service abstractions that allow more control over which services are made available to your engineers. This space is nascent but will be an important point of control in the future.
Having said all of this, there are times when you really should look at cloud provided management services. Amazon RDS is a good way to manage a database. If you want to go somewhere else you can; it isn’t that intrinsically different than any normal MySQL implementation. Be judicious about betting on services that don’t have an OSS alternative, but recognize that there are some legitimately amazing cloud specific services. Just understand what dependencies you are taking.
Third point of friendly advice: Be judicious about what dependencies you take, and approach each specific service dependency on an ROI basis. Focus on hosted services based on open source software technology; these will not lock you in.
Fourth point of friendly advice: Think about building your own reusable services for your own organizations and delivering them into your environments vs betting on the cloud providers services. (More about this in part ii of this series)
Up next, the difference between public and private cloud…
Heptio
63 
1
Thanks to Joe Beda. 
63 claps
63 
1
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
"
https://medium.com/videocoin/the-cloud-business-is-built-on-live-streaming-3c739e11acf?source=search_post---------256,"There are currently no responses for this story.
Be the first to respond.
What came first, the cloud business or live streaming? Live streaming and the cloud network are in a symbiotic relationship. One will not expand without the other growing. However, the increasing demand for video streaming has grown exponentially in the past five years while cloud business has reflected a more stable growth curve.
Live streaming content was once limited to the traditional television broadcast network. With the expansion of the cloud network, it is possible to stream video in a quality that is very close to traditional broadcast television. The increasing demand for live streaming will require the cloud business to expand at a rapid rate to meet this demand.
Video streaming on the internet takes up approximately 80% of the bandwidth of the cloud network today. Live streaming video requires even more bandwidth than standard, non-real time, streaming video.
Live streaming requires more bandwidth due to a process called transcoding. The variety of screens consumers view content is increasing by the day, but different types of devices need different codecs for the streaming content to be optimized for each device.
The process of transcoding requires a lot of bandwidth on the cloud network. However, live streaming requires simultaneously capturing the video and transcoding the video to many different types of codecs. This process requires much more bandwidth than just transcoding pre-recorded content one codec at a time.
The global video streaming market size is predicted to reach 124.57 billion by 2025. This projected growth is due to a few different factors.
The first is the growing adoption of smartphones and other media streaming devices globally. The second factor is the extensive range of high-speed Internet technologies such as 3G, 4G, and the new 5G, has resulted in increasing the growth of data consumption over the cloud network.
Finally, with so much content readily available on the internet, the most engaging form of content is video. As companies compete for consumer engagement, they will increase their video output.
Live streaming is in higher demand compared to pre-recorded content with the younger generation. Generation Z is more engaged with exciting live content rather than pre-recorded content. Further, live video is more engaging to users because anything can happen — it’s exciting to witness video in real-time. 1 in 5 videos on Facebook are through Facebook Live and on average, watched 3x longer than pre-recorded content.
In an increasingly more digital world, the desire for authenticity and human connection has not escaped consumers. Heavily edited images and messaging have made consumers crave more organic experiences with content. Live streaming seems to be the answer to this void in the current consumer experience.
The cloud business will have to grow and expand to accommodate the inevitable growth of live streaming. While the cloud network came first to help the growth of the internet, increased live streaming is going to expand the internet and cloud business.
Igniting Innovation
275 
1
275 claps
275 
1
Written by
Creators of VIVID, the next generation NFT publishing platform that allows anyone to create, manage, and sell multimedia NFTs.
Powering next generation video apps built by you.
Written by
Creators of VIVID, the next generation NFT publishing platform that allows anyone to create, manage, and sell multimedia NFTs.
Powering next generation video apps built by you.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://salon.thefamily.co/northflank-is-doing-everything-a-startup-shouldnt-do-b54b8c6a4a51?source=search_post---------257,"It’s super exciting when there’s a rule that you know is important in most every case, and then you see it being joyfully broken by an entrepreneur.
For example, no one’s going to advise you to never meet your cofounder in the real world. But the founders of Northflank met in real life for the first time at The Family’s onboarding weekend.
It’s never a good idea to spend 3 years developing a complete, robust, tech-heavy solution without having any clients. Unless, of course, you’re in the top 0.1% of coders in the world.
Northflank is that counterintuitive company that’s going to take everyone by surprise, and it’s going to be a building block that 100% of startups around the world are going to need.
So what does Northflank do? Solves an old problem that the era of cloud computing never really fixed: the need for continuous integration, delivery and deployment. And it turns cloud providers into service providers like all others, letting you switch between them according to your needs.
Whether you need to optimize for price, latency, or uptime, Northflank lets you do it.
And in a world where the rapidity with which you can scale is critical, it’s time to no longer need that one person on the team who wrote a deployment script in some obscure language, and now they’re the one who’s entirely responsible for it :-)
Northflank is one of the archetypes that I love: two hackers, nice, honest, who are full of light, and who know how to learn what they need to learn.
Remember the name, and go discover what they’ve built: https://northflank.com/exclusive-access
Education, Leverage & Capital for European Founders
726 
726 claps
726 
Written by
Co-Founder & Partner @_TheFamily - Brain (& Meat) Lover
Education, Leverage & Capital for European Founders
Written by
Co-Founder & Partner @_TheFamily - Brain (& Meat) Lover
Education, Leverage & Capital for European Founders
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pcmag-access/what-is-icloud-apples-cloud-based-subscription-service-explained-b88dfd5a98b7?source=search_post---------258,"There are currently no responses for this story.
Be the first to respond.
ICloud+ kicks in extra Apple cloud storage as well as security and privacy features. Here’s how it works.
By Lance Whitney
ICloud has long been Apple’s online service for backing up, syncing, and sharing your online files. With the release of iOS/ iPadOS 15, Apple has unveiled an expansive upgrade to iCloud’s paid subscription, dubbed iCloud+, that adds exclusive features aimed at enhancing your security and privacy online.
These additions include Private Relay, Hide My Email, Custom Email Domain, and HomeKit Secure Video. The plan is accessible from your iPhone, iPad, Apple Watch, Mac, and Apple TV, and can be shared with as many as five family members.
Apple’s use of the term iCloud to describe a variety of apps and services is confusing. So what’s the difference? First, iCloud is Apple’s standard file backup and syncing service. You’re able to back up and synchronize your photos, email, contacts, calendars, notes, reminders, messages, and other content online. The basic version of iCloud offers 5GB of online storage for free.
iCloud+, meanwhile, is the paid upgrade that becomes available once you update to iOS 15 or iPadOS 15. It includes tiers of 50GB, 200GB, and 2TB of iCloud storage, plus several enhanced features that are not accessible through the free 5GB plan.
Apple’s online storage site is called iCloud Drive, which is where your backed up and synced content resides. You can also upload, download, and share files directly with iCloud Drive.
The biggest new addition to iCloud is Private Relay, Apple’s take on a VPN aimed at keeping your internet activities private and protected when using Safari. However, it differs from traditional VPN services by replacing your IP address with one from a range of anonymous addresses based on your general region. Your internet traffic is sent through two separate hops, or relays, by two separate companies, so no single entity (not even Apple) can monitor your online activity.
Hide My Email is an improved version of the existing Sign in with Apple feature that lets you use an anonymous Apple ID to register with certain apps and websites. Whereas Sign in with Apple works only with supported apps and websites, Hide My Email allows you to use a random email address for any website or online form to cut down on spam.
Designed for anyone who already has their own personal domain name, Custom Email Domain allows you to send and receive iCloud Mail using your own domain name, instead of an icloud.com address. iCloud Mail supports as many as five personal domains with up to three email addresses per domain.
iCloud+ also has more options for those who use HomeKit Secure Video. Depending on your subscription, you get 50GB of iCloud storage and support for recording video from one HomeKit Secure Video camera, 200GB of storage and support for up to five HomeKit Secure Video cameras, or 2TB of iCloud storage and support for unlimited HomeKit Secure Video cameras.
Don’t know what subscription tier you have? After upgrading to iOS/iPadOS 15, you can check your plan or upgrade to a paid version on your iPhone or iPad. Open Settings > [your name] > iCloud. If the Storage section at the top says iCloud+, you’re good to go. If it says iCloud, then you’re on the free plan.
To upgrade to a paid subscription, tap Manage Storage, then select Change Storage Plan. Choose one of the available three plans—50GB for 99 cents a month, 200GB for $2.99 a month, or 2TB for $9.99 a month—then select Upgrade to iCloud+ to activate the new plan. iCloud+ is also included with all three Apple One subscriptions.
You can enable iCloud Private Relay under Settings > [your name] > iCloud > Private Relay (Beta). Turn on the switch next to Private Relay (Beta). From there, tap the IP Address Location option.
There are two choices. Tap Maintain General Location to use an IP address based on your overall location so that you can still see local content when you use Safari. Choose Country and Time Zone to use an address based on a much larger and broader location derived from your country and time zone.
You can now browse the web on Safari. If a site supports Private Relay, everything should work normally. Websites that do not may show content for the wrong region, add an extra step for logging in, or not work at all.
If you run into trouble with Private Relay, or want to turn it off, disable the feature completely or just for certain networks. For turning it off, go back to Settings > [your name] > iCloud > Private Relay (Beta). Turn off the switch next to Private Relay (Beta).
To turn it off for certain Wi-Fi networks, go to Settings > Wi-Fi and tap the More button next to a specific Wi-Fi network. Turn off the switch next to iCloud Private Relay.
You can also turn off Private Relay for any cellular connection. Go to Settings > Cellular > Cellular Data Options. Turn off the switch next iCloud Private Relay.
To use the Hide My Email feature, go to Settings > [your name] > iCloud > Hide My Email. You can also go to your iCloud settings page and select the Manage button for Hide My Email.
The Hide My Email page shows any random addresses you have already used with the Sign in with Apple feature. Tap an address that you wish to use again as a way to hide your real email address, then confirm that it is forwarding to your actual email address.
You can also tap Create New Address to generate a new email. A new random address will automatically be generated with the icloud.com domain name. If you’re okay with the address, tap Continue. Otherwise, choose Use Different Address and another email will be created.
Once an address is selected, you can then enter a label to help you remember this address. Tap Done at the All Set screen. Now the next time you need to create an online account or fill out a form, you can enter this random address.
Any emails sent to this account will be forwarded to your actual address. But the good news is that you can deactivate the address if you’re receiving too much spam or other unwanted messages. Just go back to Settings > [your name] > iCloud > Hide My Email. Select the random address the mail is being forward from, then tap Deactivate Email address. Tap Deactivate to confirm your request.
You can set up a custom email domain from the iCloud settings page. Select the Manage button under Custom Email Domain. A window pops up asking whether you want to use the domain only for you or for your entire family.
Next, enter the domain name you want to use with iCloud then follow the required steps. You must enter the email addresses that you use with the domain.
The records with your domain registrar must also be updated, then verify that your domain and addresses have been set up with iCloud mail.
Your iCloud+ subscription and benefits can be shared with other family members. On your iPhone or iPad, go to Settings > [your name] > iCloud > Manage Storage > Share with Family.
Tap Share Storage Plan at the next screen, then select Send Invitation to compose a message that will be sent out to your family members. As each person accepts the invitation, they are added to your iCloud family.
Originally published at https://www.pcmag.com.
PC Magazine: redefining technology news and reviews since…
102 
102 claps
102 
PC Magazine: redefining technology news and reviews since 1982.
Written by

PC Magazine: redefining technology news and reviews since 1982.
"
https://towardsaws.com/a-comprehensive-guide-to-cloud-computing-with-aws-9904fd3d99fc?source=search_post---------259,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kisan Tamang
Nov 28, 2020·4 min read
According to Research and market, the cloud computing industry is to grow from $371.4B in 2020 to $832.1B by 2025 at the rate of 17.5%. The growth is unstoppable and the demand for certified professionals has increased more than ever.
The cloud computing services cover a vast range of options — from computing power, databases, networking to storage — typically over the internet…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/4-ways-to-get-google-cloud-credits-c4b7256ff862?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 24, 2020·4 min read
Google Cloud credits are an incentive offered by Google that help you get started on Google’s Cloud Platform for free. Like Amazon and Microsoft, Google is trying to make it easy and in some cases free to get started using their Cloud Platform or certain services on their platform that they believe are “sticky” — which is beneficial if you’d like to try the services out for personal use or for a proof-of-concept. There is both a spend and a time limit for Google’s free credits, but then they also offer “always free” products that do not count against the free credit and can be used forever, or until Google decides to pull the plug, with usage limits.
The most basic way to use Google Cloud products is the Google Cloud Free Tier. This extended free trial gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts:
The Google Cloud 12-month free trial and $300 credit is for new customers/trialers. Be sure to check through the full list of eligibility requirements on Google’s website. (No cryptomining — sorry!)
Before you start spinning up machines, be sure to note the following limitations:
Your free trial ends when 12 months have elapsed since you signed up and/or you have spent your $300 in Google Cloud credit. When you use resources covered by Always Free during your free trial period, those resources are not charged against your free trial credit.
At the end of the Free Trial you either begin paying or you lose your services and data, it’s pretty black and white, and you can upgrade at any time during your Free Trial with any remaining credits being applied against your bill.
The Always Free program is essentially the “next step” of free usage after a trial. These offerings provide limited access to many Google Cloud resources. The resources are usually provided at monthly intervals, and they are not credits — they do not accumulate or roll over from one interval to the next, it’s use it or lose it. The Always Free is a regular part of your Google Cloud account, unlike the Free Trial.
Not all Google Cloud services offer resources as part of Always Free program. For a full list of the services and usage limits please see here — a few of the more popular services include Compute Engine, Cloud Storage, Cloud Functions, Google Kubernetes Engine (GKE), Big Query and more. Be sure to check the usage limits before spinning up resources, as usage above the Always Free tier will be billed at standard rates.
Google is motivated to get startups to build their infrastructure on Google Cloud while they’re still early stage, to gain long-term customers. If you work for an early-stage startup, reach out to your accelerator, incubator, or VC about Google Cloud credit. You can get up too $100,000 in credit — but it will come at the price of a large percentage of equity.
Options that don’t require you to give up equity include Founder Friendly Labs, StartX if you happen.
Google offers several options for students, teachers, and researchers to get up and running with Google Cloud.
There are also several offerings related to making education accessible without associated credits. See more on the Google Cloud Education page.
Various vendors that are Google Cloud partners run occasional promotions, typically in the form of a credit greater than $300 for the Google Cloud Free Trial, although we’ve also seen straight credits offered. For example, CloudFlare offers a credit program for app developers.
Also check out events that might offer credit — for example, TechStars startup weekends offers $3,000 in Google Cloud credits for attendees. Smaller awards of a few hundred dollars can be found through meetups and other events.
Google Cloud Credits do offer people and companies a way to get started quickly, and the Always Free program is a unique way to entice users to try different services at no cost, albeit in a limited way. Be sure to check out the limitations before you get started, and have fun!
Further reading:
Originally published at www.parkmycloud.com on February 18, 2020.
CEO of ParkMyCloud
133 
1
133 
133 
1
CEO of ParkMyCloud
"
https://levelup.gitconnected.com/top-6-django-compatible-hosting-services-ee9e6bedbfab?source=search_post---------261,"Finding the best Django hosting to suit all of your requirements might be quite a big deal, but it’s definitely worth the effort. Moreover, it’s also worth trying different options and seeing what will be the best fit for you. Some engineers prefer using paid and highly functional servers, while others find more benefits in (relatively) free services, and both options have their own benefits. We have chosen the top 6 small, mid, and large-scale Django hosting services with various functions that certainly deserve consideration.
AWS is one of the most elaborate Django hostings. What really stands AWS out from the others is its flexibility and functionality. It’s possible to configure the application for practically any needs using its highly adjustable construction kit and apply the server for any project and for any purpose. It’s also a great choice for those who run lots of lengthy projects.
With AWS you will cover such aspects as data storage, content delivery, database management, networking, load balancing, and auto-scaling. This list will be insufficient if analytics, mobile development, testing, developer and management tools with security services are also omitted.
Even though this Django hosting is predominantly aimed at processing Amazon clients (as far as they have the corresponding infrastructure to run it), there are no direct restrictions for other users. The bottom line is that this is the most suitable tool for web professionals and those companies who may require truly wide functionality. Also, as Amazon spans around 60 entities untied by the Global infrastructure, it works particularly well for big enterprises with offices in different countries.
Among the many users at mid-to-small scale, AWS can also boast Atlassian, Expedia, Vodafone, Siemens, Philips, and has many more up its sleeve.
Pros:
Cons:
The AWS pricing is not an easy thing.
Their official website offers a free trial with special offers for beginners. This may be an alluring proposal to have a go, but pay attention that once you exceed the chosen tier they are going to charge you according to official pay-as-you-go rates.
There is also an option to go for non-expiring free tier AWS solutions. These, however, have perks when you begin gathering momentum due to limited storage volumes.
To know the exact (or, more likely, the approximate) amount you are about to pay after the end of the trial period, you should use the calculator placed on the “Pricing” page of their website.
Even though AWS might seem way too tangled at first glance, it is still worth thorough consideration for those who are willing and aiming to make progress in this industry. To make things easier, you may take a look at Terraform (a tool for implementation culture infrastructure as code.) Also, using guides might be a starting point for AWS’s deeper understanding.
Azure by Microsoft is a cloud-based platform with the main functions being a content delivery network, media services, web apps, API apps, and so on. As a whole, this service might resemble DigitalOcean in Amazon’s interpretation. It also embodies all the necessary toolkit to help host your Django website. This is crucial for busy websites, especially those that receive billions of monthly requests.
The list of Azure’s case studies contains such companies as HP, Asos, Adobe, IHG, and Airbus, among others.
Pros:
Cons:
djangostars.com
While Microsoft Azure ensures it’s five times less expensive than AWS hosting services, it definitely concedes to Amazon Services in terms of complexity, the absence of certain features and even price. This, however, does not hinder big enterprises running their businesses if they find it cost-effective enough. Instead, you may find it beneficial especially if you use other Microsoft products.
The service is, obviously, run by Google and is an elaborate product with a lot of tools, starting from services to compute, store, run big data analytics, machine learning, and much more. They are specifically focused on meeting the needs of large enterprises akin to Spotify, Lush, Johnson-Johnson, Coca-Cola, Sony music, Wix, etc. An alternative function cloud storage for people and companies that need to keep their documents securely in place off-site.
Pros:
Cons:
Using other Google services, looking for generous offers to host a Django app, and being fine with paying for costly yet terrific google support assistance are the main reasons why you should give it a shot.
The pricing is comparatively low with $0,020 per GB monthly for the Regional class and $0,007 per GB monthly once you go for the Coldline class.
Let’s get right to it: even if free Django hosting services did exist, you would rarely opt for them since the number of features, their quality, and lack of support would hardly amaze or fit you. However, some Django hosting providers ensure free trial periods, which are equally advantageous for clients and hosts.
djangostars.com
djangostars.com
Hetzner is a Germany-based organization with data centers in Germany and Finland. When visiting their website, the first thing to strike one’s eye is its old web design, but this does not prevent the company from offering a wide range of services, such as SSL certificates, dedicated servers, vServers, unlimited bandwidth, storage box, domain registration, and others. Overall, Hetzner is a wise solution for those who understand the ropes of hosts’ specs and thus know what exactly they want to get from them.
Hetzner is widely used by Leoni, Autodoc, Showmax, Bitdefender, Outdooractive, etc.
Pros:
Cons:
One of the most attractive parts of Hetzner’s is its affordability. It might turn to be truly cheap to go with Django hosting. It allows the use of web hosting with Python starting from the level 9 (cx41), though. Here is the list of approximate fees as follows:
DigitalOcean is no less than a decent competitor to Google Cloud Platform, Amazon Web Service and Azure. This cloud service platform is a reasonably good hosting provider in terms of simplicity, fast load times (with SSD storage and its 8 datacenters) and security. It also provides an intuitive interface and straightforward setup. Primarily, it’s equally good for newbies and experienced businesses with basic needs.
Lytham Labs, QuoDeck, Fanout, Accern, TaskRabbit, and numerous other organizations have been happily using DigitalOcean services for years.
Pros:
Cons:
Overall, and covering multiple aspects, DigitalOcean doesn’t have obvious shortcomings. It’s both good in terms of pricing and coverage.
Starting from $5 for monthly subscription up to $960 depending on its configurations.
To sum it up, it’s probably the best solution for Linux developers according to price and effectiveness, regardless of the fact that it doesn’t offer any analytics, configuration management, or hosted databases.
Here you won’t find a lot of frills and fancies, but Heroku still possesses PaaS architecture, Python, Ruby, Java and PHP support, plus add-ons and databases. It features app metrics, code and data rollback, real-time insights, GitHub Integration, and more. What stands Heroku out from the others is that it provides an easy way to setup and configure elements, which raises engagement and productivity. Best for small and medium-sized companies.
Pros:
Cons:
All the paid plans follow the pay-as-you-go model.
By and large, Heroku is extremely good to help you kickstart from the beginning but might impede at more advanced levels. It’s probably the best choice if you are aiming at simply launching a product.
It is a big deal to pick the perfect hosting service for yourself. Through trials and errors, you can, however, come to terms with the most suitable kind for your specific needs.
So, if you are yet to find your own best hosting for Django, let’s take a look at the essentials of a robust host:
These features are not all completely standard, yet they might give you hint if this or that host is worth trying. Overall, we recommend you to take a closer look at these Django compatible providers and pick those that most suit your business size, aim, background, and personal preferences.
In taking stock of these Python & Django compatible web hosting services, it is vital to emphasize that they tap potential at different levels. While you can go with Heroku and DigitalOcean in the middle or even entry-level, Hetzner might need you to be savvier. The same goes for AWS, Azure, and Google Cloud Services: with a wide range of features and hefty fees, these are better for bigger companies with advanced needs.
* VPS prices taken for Germany as each service works in this region
*Comparison of CPU performed for minimal rates
This article about django hosting services is written by Denis Podlesniy — Backend Developer at Django Stars.
gitconnected.com
Coding tutorials and news.
908 
Thanks to hackerhodl. 
908 claps
908 
Written by
Guides and recommendations on how to transform ideas into digital products people want to use.
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Guides and recommendations on how to transform ideas into digital products people want to use.
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.signals.network/july-2018-update-fruits-of-our-labor-6f208f6b0d39?source=search_post---------262,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Sometimes it’s a blessing to be a developer. These last few sweltering weeks, we’ve been lurking in server rooms, blasting industrial air-con, and doing our absolute best not to catch even a glimpse of natural light. While everyone else has succumbed to heatstroke, we’ve been steadily progressing, cool as cucumbers. There’s a lot of ground to cover in this update, from infrastructure to team, app to roadmap, so let’s get to it!
SafeDX servers ready for testing
On July 10th we met with our partners Quantasoft and SafeDX, one of Foxconn’s data centers. SafeDX gave us a full tour of their huge server rooms and our infrastructure for the servers has been finalized, currently being tested. We already received access and deployed the first of many strategies on them, carrying out some performance and integration tests. This marks a huge step forward, as we will be able to run the first full capacity stress-tests of our platform, with the necessary power to process thousands of algorithmic strategies simultaneously — no small feat!
Signals has grown
You may have seen that we have been actively looking for new developers over the last few months. We are glad to confirm we have recently hired one more backend developer and three new frontend developers, but there is still one open position to strengthen the backend team. If you’re interested you can apply here: https://goo.gl/forms/FE3BofHVXpWsrLfC3
This also poses a good opportunity to introduce you one of the core members of our team, who joined Signals few months ago: Chief Software Architect Viktor Borza, who brings with him a broad skill set that’s proving to be exactly what we needed. His experience has been very much welcomed, and in the short time he’s been with us he’s proved his leadership skills time and again. Viktor is mainly responsible for the architecture of the platform and the new Signals Framework , which will be used by developers while creating new custom strategies.
As a means of further introduction, here’s a rundown of Viktor’s background:
Platform Redesign
The Signals platform aims to be a place where the most advanced developers can coexist with regular traders, and that demands a great UI/UX design which is user friendly for everyone. This is one of our main focuses at the moment so we are working hard to redesign the website and the platform. It’s been coming along nicely, so we thought we should share a screenshot of the new look. As always, feedback is welcome!
Upcoming Milestones
We mentioned in our previous blogpost that our roadmap is undergoing some revision, to account for infrastructure upgrades and a growing team. While there will be a dedicated blog post covering this, we would like to introduce you to some of the details which we have been working on over the last months.
The new version of the Signals platform will include the new Data and Indicator Marketplaces. The former will feature a list of ticker data from Poloniex and other exchanges, while the latter will come with multiple Signals technical analysis indicators. Both marketplaces will also include examples for usage. Furthermore, as we work to improve the already existing Strategy Marketplace, we will also launch the My Strategies feature.
In My Strategies you will be able to code your own strategies using the new, public version of the Signals Framework, and our in-app editor. So far, we’ve had 650 developers register for early access, and expect to see many more in the coming months — to sign up, register using our designated form. Backtesting of strategies will also be available, bringing with it the ability to change indicators directly by amending their code, and granting access to performance metrics and chart visualizations. Once a strategy is backtested, you can then deploy it and start receiving notifications.
One more exciting new development we are working on is the first version of our Mobile App, where you will be able to receive notifications from all the strategies you are following, wherever you are. It will give you a detailed overview of each strategy and will be able to alert you via a push notification whenever there is a particular signal on any of your selected strategies. Again, we’ll be introducing this more fully in a dedicated post when the time is right.
That just about wraps things up for this month’s newsletter, so we hope you’re as excited as we are to see things come together. There’s no doubt that we’ve secured some of the best infrastructure we could’ve hoped for, and our dev team has been doing a fantastic job. We couldn’t have anticipated a more productive summer and we hope you feel the same. Get in touch and let us know your thoughts, we are really keen to make Signals the trading platform you’ve been dreaming of!
From day trader to data scientist — Signals lets you…
257 
257 claps
257 
Written by
Signals is a futuristic marketplace where you can discover, create and monetize cryptocurrency trading strategies driven by machine learning.
From day trader to data scientist — Signals lets you create, test and monetize automated trading bots. Access trusted strategies with no technical knowledge
Written by
Signals is a futuristic marketplace where you can discover, create and monetize cryptocurrency trading strategies driven by machine learning.
From day trader to data scientist — Signals lets you create, test and monetize automated trading bots. Access trusted strategies with no technical knowledge
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/zero-trust-with-reverse-proxy-c744d8956f21?source=search_post---------263,"There are currently no responses for this story.
Be the first to respond.
A reverse proxy stands in front of your data, services, or virtual machines, catching requests from anywhere in the world and carefully checking each one to see if it is allowed.
In order to decide (yes or no) the proxy will look at who and what.
This issue of GCP Comics presents an example of accessing some rather confidential data from an airplane, and uses that airplane as a metaphor to explain what the proxy is doing.
Reverse proxies work as part of the load balancing step when requests are made to web apps or services, and they can be thought of as another element of the network infrastructure that helps route requests to the right place. If the request is invalid, either because it is from an unauthorized person or an unsafe device, then the proxy may deny the request.
Why might the proxy say no to my request?
Looking at the device originating the request, the proxy could deny access due to:
Resources
For more on proxies and Zero Trust, check out the following resources:
Want more GCP Comics? Visit gcpcomics.com & follow us on medium pvergadia & max-saltonstall, and on Twitter at @pvergadia and @maxsaltonstall and to not miss the next issue!
Google Cloud community articles and blogs
49 
1
49 claps
49 
1
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technology-hits/100-gb-free-cloud-storage-for-you-1b0ddb1fb6fd?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
We have been familiar with Google drive's cloud-based storage solution. I believe all Gmail users store their important data and pictures in Google drives because it has a synchronization feature that saves our…
"
https://medium.com/@harshithdwivedi/how-disabling-external-ips-helped-us-cut-down-over-80-of-our-cloud-dataflow-costs-259d25aebe74?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Harshit Dwivedi
Aug 29, 2019·4 min read
Tl;dr : Set --usePublicIps=false in the execution parameter on your Dataflow pipeline.
For some context, we’re making a real-time data aggregation pipeline which guarantees that you get your web/app user data made available to you in near-real time (5–6 secs).
Think of it like Google Analytics, but on steroids!
Our entire infrastructure is built on Google Cloud Platform and we are using the following products within the GCP family :
To give you a scale of things we’re trying to do, here’s a screenshot outlining the average traffic we handle from a single property of our friends over at Z1Media :
The challenge here is obviously to keep the throughput and reliability of the system as high as possible while keeping the costs to a minimum.
We recently switched to BigQuery loads instead of streaming data directly into BigQuery which helped us cut down the costs by a fraction of 30% per day as Loading Files into BigQuery is free and we kept the data loading frequency to a bare minimum so that the data is near-realtime in nature.
Enabling BigQuery File Loads requires you to write the data to a Google Cloud Storage bucket where BQ can load the data from. To see if this would incur us any costs, we quickly looked at the docs to see the ingress/egress price for GCS.
Turns out that the Ingress/Egress data to and from Google Cloud Storage within products in the same region was Free and lucky for us, both Cloud Dataflow and BigQuery were in the same region of Tokyo (yay!).
Within a day of this new setup in place, we were overjoyed by the cost reduction until we saw a new pricing metric being added to our billing (which was more than what streaming inserts used to cost us).
Googling for what Carrier Peering means gave us no meaningful results either; the official docs mentioned that the Carrier Peering was to be used when you wanted to access G-suit products from within Google Cloud, which is something we weren’t doing.
On searching for questions on Stackoverflow and reddit, we revisited the docs for compute engine which stated that this ingress/egress pricing was free and a minute info caught our attention.
Egress to Google products (such as YouTube, Maps, Drive), whether from a VM in GCP with an external IP address or an internal IP address
Internal IP address was something we assumed that our dataflow would be using since we nowhere specified it to use an external IP address; and since we didn’t want our data to be available to anyone else apart from BigQuery we didn’t need to have an external IP in the first place!
Turns out that by default, Dataflow will enable external IPs and you need to pass a flag --usePublicIps=false while executing your Dataflow pipeline that disables it if you want to do so.
But just passing the flag isn’t enough as our pipeline failed immediately when we tried to start it with the following message :
Workflow failed. Causes: Subnetwork ‘’ on project z1media network ‘default’ in region asia-northeast1 does not have Private Google Access, which is required for usage of private IP addresses by the Dataflow workers.
Turns out that by default, accessing other Google Cloud services via internal IP is not allowed; so to fix this; we went to VPC network from the Cloud Dashboard and selected asia-northeast1 since that was where all of our products were situated in.
From within there, we simple enabled the option to allow “Private Google Access” and that was it!
And that was it!
We ran our pipeline after making these changes and everything was back to normal without affecting our workflow.
If you are working at a high growth company and want your data made available to you as soon as it’s created; take a look at https://roobits.com/ and we might be what you are looking for!
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Has an *approximate* knowledge of many things. https://aftershoot.co
312 
1
312 claps
312 
1
Has an *approximate* knowledge of many things. https://aftershoot.co
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/windmill-engineering/production-is-for-cattle-but-development-is-for-pets-5f4ecba11f7d?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
Kubernetes makes it easy to manage herds of cattle: lots of servers running in production.
Today we're announcing a simple, open-source tool pets. pets makes it easy to manage herds of cats: lots of servers running on your machine that you want to keep a close eye on for local development.
pets is for the cloud-service developer who has multiple servers that they run for day-to-day feature work. Maybe the servers run as bare processes. Maybe they run in containers. Or in minikube. Or in a remote Kubernetes cluster.
We should be able to express the constellation of servers independently of how we start them. A Petsfile is like a Makefile for expressing how servers start and fit together. This lets us switch back and forth quickly between servers running locally and servers running in the cloud.
The simplest example of a Petsfile is when you have two servers in a single repo. This example uses the Go tools, but pets works with any programming language.
Then run
pets will automatically start frontend and all of its dependencies!
pets keeps track of which servers are currently running. You can list them with pets list :
Or tear them down with pets down :
If you have servers in multiple repos that you want to run with pets, that’s OK too!
Then run
The load function will use go get to fetch the repo. You can also use load with a relative path to point to another repo on your disk, if you expect the repo to be already checked out.
Lastly, let’s talk about how this works when you want to run a frontend talking to a backend server in Kubernetes. This is when pets starts to shine.
Then, from the frontend repo, run:
Now you have a local frontend talking to a cloud-based backend. As you run more and more services, pets helps you to run them in increasingly complicated combinations.
For more documentation, check out pets on Github. Have fun!
Kubernetes for Prod, Tilt for Dev
241 
241 claps
241 
Kubernetes for Prod, Tilt for Dev https://tilt.dev/
Written by
Software Engineer. Trying new things @tilt_dev. Formerly @Medium, @Google. Yay Brooklyn.
Kubernetes for Prod, Tilt for Dev https://tilt.dev/
"
https://medium.com/javarevisited/7-free-courses-to-learn-google-cloud-platform-for-beginners-cbb260fbd8e4?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to learn Google Cloud Platform in 2021 and looking for some free online Google Cloud training courses, tutorials, and learning materials to start your GCP journey then you have come to the right place.
Earlier, I have shared the best Google Cloud Platform courses, and today, I am going to share free online courses from Udemy Youtube, and Coursera which you can join to learn Google Cloud platform.
If you don’t know, Google Cloud Platform is the massive Cloud platform for Google which is one of the three biggest public cloud platforms along with AWS and Microsoft Azure.
There is a huge demand for Google Cloud Professionals but there are not many skilled professionals in the market, hence more and more people are learning about Google Cloud and getting certified.
This article contains free Google cloud courses from sites like Udemy, Coursera, and Pluralsight which you can use to learn Google cloud from scratch. These Cloud services are providing great help to the technical world easing up the tasks, remote access, and security features. People are taking their business online, and the market is growing at a steady rate.
We have to keep up with the trend if you’re a business enthusiast, you know how much importance our words hold. To provide you with knowledge of the cloud and how you can work with them we have picked up a few courses that you can access for free and make your way in the market with new technology available. We have tried to provide you all the free Google cloud courses that can satisfy your curiosity and help you attain your goal whether you’re a developer or just looking to gain some Cloud computing skills.
We specifically focused on the part to provide you with the best kind of teaching that doesn’t waste much of your time and can make learning easy for you.
By the way, if you don’t mind spending few bucks on learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the best online courses to learn GCP concepts from scratch.
udemy.com
Here is the list of the best free courses that you can access online to learn the Google Cloud Platform or GCP. These courses not only cover the platform details but also get you familiar with essential GCP services and features.
This is one of the best available courses on Udemy. To begin this course you need to have little knowledge of IT. Since the Google cloud platform is the fastest growing cloud service in the world, people are taking their business online and enhancing them with services the cloud has to provide. If you are looking to be in the developer section of the industry this course will help you a lot. Also, big data concepts and machine learning concepts are introduced recently. Here are things you will learn in this course:
This course focuses on providing you with a deep understanding of google cloud concepts and working. You’ll be learning a lot about its applications in different segments of this course.
Here is the link to join this free GCP course — Google Cloud Platform Concepts
This is one of the best free courses to learn Google Cloud Platform and it’s offered by none other than Google Cloud on Coursera. You can access the course to learn it all from their official website as well. They have sincerely categorized and provided concept-wise learning, distributed among topics. All the new technology and its functioning can be accessed on the official website if you have a learner id. This course is also part of multiple Coursera specializations like Developing Application with Google Cloud and completing this course counts towards getting the certification. Here are things you will learn in this course:
All the topics are covered with examples. It is a theoretical way to teach so you have to be patient while you learn and apply concepts. Doubts can be easily addressed in the query section.
Also, real-time practice is going to provide you a detailed understanding of cloud services and functions. You can interact with other users following the same course as well. There are other premium courses suggested by Google as well.
Here is the link to join this Coursera course — Google Cloud Platform Fundamentals: Core Infrastructure
Overall a fantastic free Coursera course to learn key GCP concepts and services like Google App Engine, Google Compute Engine, Google Cloud Storage, Google Kubernetes Engine, Google Cloud SQL, and BigQuery, etc.
This is another awesome free Udemy course to learn about Google Cloud Platform from scratch. To start this course you should have a little experience with coding or software development.
This is an introductory course to get started on Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands o
You can also use this free course to prepare for highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc. Here are things you will learn in this free Google Cloud course:
For all those people who are looking to gain application development skills in this course focus on that too.
Also, the basic knowledge of the cloud would provide you a better understanding. You will be getting a lot of theory and hands-on to prepare you for a bigger platform.
Here is the link to join this free course — Google Cloud Fundamentals 101
Google Cloud has many courses on Pluralsight and this is one of them. PluralSight uses effective tutorial learning to provide critical cloud skills. Even they’ll teach you how to operate on hybrid as well as in multi-cloud computing.
You’ll be able to arrange and understand the way digital information works in the cloud. Pluralsight has teamed up with Google to provide you with the essential skillset to make your career in the cloud.  This course designed for AWS professionals like AWS developers, solution architects, and sysops administrators who are already familiar with essential cloud computing concepts and have prior experience with AWS. Here are things covered in this course:
They start from basic and the course takes you through all the important topics that need to be learned.
Here is the link to join this course — Google Cloud Fundamentals for AWS Professionals
Google has mentioned the Pluralsight courses on its official website for cloud computing as well. This course is personally recommended by Google and since the website doesn’t support free access for a longer time, make sure you get to the learning as soon as possible. Alternatively, you can also use Pluralsight 10-day-free-trail to access this course for FREE.
pluralsight.pxf.io
This is a list of small online tutorials from a Youtube Playlist to learn about Google Cloud Platform from Google Tech itself. Learning Google Cloud Platform! GCP can be overwhelming at first, but these small tutorials will help you get started.
You will watch a short video to understand the difference between Google Cloud Platform’s (GCP) free tier and free trial and understand how they can help you test GCP or use it for development purposes at no or little cost.
There are many short videos to learn about essential services and Google Cloud Platform concepts like compute, storage, networking, security, pricing, and many more.
If you like to learn from short videos then I highly recommend this free Youtube playlist from Google Tech. You can watch them right here or on Youtube.
This is another awesome course to learn about Google Cloud Platform and it's available on Pluralsight. This course will provide an overview of the platform and a framework for diving deeper.
While the Google Cloud Platform is a more recent offering than some of its competitors, it draws on years of experience running Google’s massive internal infrastructure and exposes a streamlined set of solution-focused capabilities to help you build great systems.
Created by Howard Dierking this course will first teach the core building blocks of the platform. Next, you’ll explore the characteristics that differentiate Google’s offering from other cloud platforms.
Finally, you’ll learn the common application architectural patterns. By the end of this course, you will be able to understand how the areas fit together and provide starting points for deeper exploration.
Here is the link to join this course — Google Cloud Platform Fundamentals
By the way, this course is not exactly free as you would need a Pluralsight membership which costs around $29 per month or $299 per year. Alternatively, you can use their 10-day-free-trial to watch this course for FREE.
pluralsight.pxf.io
This is one of the best free Udemy courses available online to learn Google Cloud Platform concepts, plus learners have provided it near-perfect ratings to learn. One thing that would be beneficial is that you don’t need to learn anything to begin this course, you can even start right now. If you are willing to learn the basics of google cloud and cloud computing this is the course for you. Here are things covered in this course:
This course has a goal to provide you with a simple conceptual introduction to cloud services and computing. There is not much technical information to remember, this course will help you to have the perspective if you are starting from scratch. Plus if you choose to learn from this course you’re going to get Linux Academy’s Hands-On lab and maybe you’ll get flashcards.
Here is the link to join this course — Google Cloud Concepts
That’s all about the best free courses to learn Google Cloud Platform or GCP in 2021. Google cloud is the fastest growing cloud service in the world. Hope you found these courses useful to help you learn the new technology and implementation. We would recommend you to visit each course personally to have a better insight.
Other Cloud Computing Articles you may like
Thanks for reading this article so far. If you like these free Google Cloud Platform online courses for beginners then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you don’t mind spending few bucks for learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the better courses to learn GCP concepts from scratch.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
164 
164 claps
164 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@datapath_io/aws-direct-connect-vs-vpn-vs-direct-connect-gateway-97900cdf7d04?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
Datapath.io
Mar 27, 2018·3 min read
I recently wrote about the AWS Direct Connect Gateway. The AWS Direct Connect Gateway is a new addition to the AWS connectivity space, which already includes AWS Direct Connect and a Managed VPN service. In this blog post we will explore all three and take a look at the different use-cases that they are aimed at.
AWS Direct Connect is a service aimed at allowing enterprise customers easy access to their AWS environment. Enterprises can leverage the AWS Direct Connect to establish private connectivity to the AWS global network from their data centers, office locations or co-location environments.
AWS Direct Connect supports two bandwidth levels: 1 G and 10 G. higher bandwidth levels can be provisioned by having multiple 10 G interfaces connected in tandem.
Lower bandwidth levels of 50 M, 100 M, 200 M, 300 M, 400 M and 500 M can only be provisioned through an AWS partner supporting AWS Direct Connect.
There are two aspects of Direct Connect pricing: the per hour port fee and the data transfer pricing. Port fees depend on the port speed selected.
Data transfer pricing is split into two heads: data transfer in and data transfer out. Data transfer in is free in for all port fees and direct connect locations.
Data transfer out is priced differently depending on AWS region and the direct connect location. Case in point data transfer out from us east-1 to CoreSite DE1, Denver, CO is priced at $0.020/GB, where as data transfer out from AWS Singapore to the same site is prices at $0.090/GB.
AWS Direct Connect can be used as a replacement for a VPN connection over the public internet, to connect customer networks with AWS. The Direct Connect is likely to provide a more reliable level of performance however it is significantly more expensive as compared to a VPN.
As mentioned earlier, VPNs can also be leveraged to connect on-premise networks or office locations with AWS. VPNs on AWS come in three flavours: hardware only, software only and a mix of hardware/software.
The hardware only VPN uses a hardware VPN device to connect the virtual private gateway on the AWS end to a customer VPN gateway on the customers end, via IPsec VPN tuneels.
Hardware only VPNs include both the AWS managed AWS VPN solution and the AWS VPN CloudHub. The AWS managed VPN solution can be deployed inc cases where there is only one customer network to be connected to.
Cloudhub comes into play where multiple networks have to be connected to AWS. CloudHub is arranged in a classic hub and spoke topology where all traffic flows through a central hub VPC.
The managed VPN solution is charged on the basis of VPN connection hours. A VPN connection hour counts as every hour that the VPN connection is up and running. Each VPN connection hour is charged at $0.05. this holds true for all AWS regions except the Tokyo region which is priced at $0.048.
Software only VPNs can also be provisioned to manage both ends of the VPN network. VPN appliances that run on EC2 instances are used to create VPN connections between the remote network and the AWS VPC.
AWS VPN while being a lower cost option for connectivity between AWS and on-premise networks, can be limited by the amount of bandwidth it can pass.
AWS Direct Connect gateway is a relatively new service from AWS. Direct Connect allowed AWS users to connect their AWS environment to AWS. However connecting from a single Direct Connect location to multiple AWS VPCs wasn’t so straight forward.
AWS Direct Connect gateway is aimed at making it easier to connect from a single Direct Connect location to multiple AWS regions or VPCs.
The Direct Connect Gateway is connected to multiple AWS VPCs in different AWS regions via Virtual private Gateways. The Direct Connect Gateway is in turn connected to the Direct Connect via a virtual private interface. This allows multiple VPCs to be connected to the customer network via one virtual private interface.
https://datapath.io/resources/blog/aws-direct-connect-vs-vpn-vs-direct-connect-gateway/
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
51 
51 
51 
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
"
https://medium.com/thipwriteblog/%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%A0%E0%B8%97%E0%B8%82%E0%B8%AD%E0%B8%87-cloud-computing-service-%E0%B8%A1%E0%B8%B5%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%87-544e2c6a8c85?source=search_post---------269,"There are currently no responses for this story.
Be the first to respond.
ในปัจจุบันมีผู้ให้บริการคลาวด์ หรือที่เรียกกันว่า Cloud Provider อยู่มากกว่า 20 เจ้า แต่ที่เราคุ้นหูคุ้นตากันดีก็คงเป็นสามเจ้ายักษ์ใหญ่ ที่ติดอันดับเป็น “Big Three” ของวงการคลาวด์ นั่นก็คือ Amazon Web Service (AWS), Google Cloud Platform (GCP) และ Microsoft Azure ซึ่งแต่ละเจ้าก็จะให้บริการคลาวด์ที่แตกต่างกันไป
อ้อ! ขอย้ำว่า นี่เรากำลังพูดถึง “ประเภทบริการคลาวด์” ไม่ใช่ “ประเภทของคลาวด์” นะ
ทีนี้เรามาทำความรู้จักประเภทของ Cloud Service กันดีกว่า ซึ่งบางครั้งพวกฝรั่งเขาก็เรียกกันว่า Cloud Computing Stack เผื่อใครเอา keyword ไปใช้ research ต่อ โดยทั่วไปจะแบ่งประเภทบริการคลาวด์ออกเป็น 3+1 ประเภท
ทำไมต้องบวกหนึ่ง?ก็เพราะว่า.. ถ้าเอาแบบหลักๆ จริงๆ ที่คนพูดถึงบ่อยๆ จะมีแค่ 3 ประเภทแรก ส่วนข้อสี่ที่บวกเพิ่มมานั้นเหมือนมันเป็นตัวย่อยแยกออกมาทีหลังนั่นเอง
Step ถัดไป เพื่อให้สมองเรียบเรียงเรื่องยากให้เป็นเรื่องง่าย เราต้องสร้างความคุ้นเคยกับชื่อ types ทั้งหมดก่อน โดยที่ยังไม่ต้องสนใจว่าแต่ละชื่อมันคืออะไร เริ่มค่ะ!!
อ่ะ! ก็ยังดูยาวๆ จำยากๆ อยู่ดีใช่มะ งั้นดูปาก thip นะคะ แล้วท่องตามวนไปค่ะ…
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
Infra — Platform — Software — Function
บริการทั้งหมดนี้ ผู้ให้บริการคลาวด์บางเจ้า อาจจะเปิดทุก service ครอบคลุมทั้งสามสี่อย่างนี้ หรือบางเจ้าก็จะให้บริการแค่บางประเภท อันนี้ก็แล้วแต่เรา ว่าจะเลือกใช้บริการอะไร ของเจ้าไหน
ต่อไป สายย่อจะขอบรีฟใจความสำคัญสั้นๆ ของคลาวด์แต่ละประเภท ให้พอมองภาพออก
โดย cloud provider แต่ละเจ้าก็จะมี products ยิบย่อยและเยอะมากกก ชื่อผลิตภัณฑ์แต่ละเจ้าก็จะแตกต่างกัน ตัวอย่างเช่น Web Server ของ AWS จะชื่อ “EC2” แต่ถ้าเป็นของ Microsoft Azure จะชื่อ “App Service” ประมาณนี้
สำหรับใครที่อ่านจบแล้ว แต่รู้สึกยังจำอะไรไม่ได้เท่าไหร่ เราแนะนำให้กลับมาอ่านวนไปวันละ 1 รอบ แล้วเลือกจำทำความเข้าใจแค่ครั้งละ 1 service
แต่ถ้าใครอ่านรอบเดียวแล้วรู้เรื่องงง นั่นแปลว่าเราบรีฟได้ดีมากกก555+ ไม่ต้องอ่านซ้ำแล้วก็ด่ะ แต่ก่อนออกไปกด clapsss ให้เรารู้หน่อย จะได้หาอะไรมาบรีฟบ่อยๆ เนอะ!
อ่านบทความที่เกี่ยวข้อง
medium.com
Programming, Technology, Work Life, Finance, Storyteller, Lifestyle, Content Creator, Podcast
32 
Some rights reserved

32 claps
32 
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
"
https://blog.devgenius.io/what-is-multi-cloud-bfc7624e13c3?source=search_post---------270,"We’re only a couple of months away from the new year, which means it’s time to start looking ahead to the tech trends that will dominate the software industry in 2022. As the new year approaches, we want to help you get familiar with upcoming trends so you can be prepared and start taking your skills to the next level.
Today, we’ll discuss multi-cloud. Multi-cloud is a cloud computing model that leverages two or more cloud platforms, allowing you to take advantage of the resources different cloud providers offer. Multi-cloud can help organizations lower their cloud cost, increase resiliency and flexibility, and much more. Gartner estimates that over 75% of cloud customers will adopt a multi-cloud strategy by the end of 2022. With its increase in popularity, it’s an important concept to know!
We’ll cover:
The multi-cloud cloud computing model leverages two or more cloud platforms. Instead of relying on a single cloud provider, the multi-cloud strategy relies on different cloud providers to take advantage of the various cloud services those providers offer. Multi-cloud can refer to combinations of software as a service (SaaS), infrastructure as a service (IaaS), and platform as a service (PaaS) models. Commonly used cloud providers include Google Cloud Platform (GCP), Amazon Web Services (AWS), Microsoft Azure, IBM Cloud, and VMware.
Different cloud providers offer different cloud resources, such as cloud storage, machine learning, big data analytics, serverless computing, databases, and more. Most businesses that move to the cloud implement some form of a multi-cloud model. A multi-cloud solution is a solution that’s transferable across many different cloud infrastructures. These solutions typically leverage cloud-native technologies and help manage workloads across many different clouds.
What is a cloud-native technology?
Cloud-native is an approach to app development that leverages the cloud computing delivery model. Cloud-native technologies use tools like containerization, service meshes, declarative APIs, and microservices to allow you to build, deploy, and manage high-impact applications. According to the Cloud Native Computing Foundation (CNCF), these technologies “empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds.”
Multi-cloud management is a set of procedures and tools we can use to manage and secure various applications across multiple clouds. Multi-cloud management platforms can be difficult to implement. It can be hard to consistently maintain cloud security across different platforms, deploy apps across various environments, and visualize information from various clouds on a single interface. An effective multi-cloud management solution does the following:
There are many advantages to adopting a multi-cloud strategy. Let’s take a look at some of the top benefits:
The hybrid cloud approach is often confused with the multi-cloud approach, but the two have key differences. A hybrid cloud environment uses both public cloud and private cloud. This allows you to maintain the security of private data within an on-premises cloud solution or within a private cloud. Hybrid cloud solutions leverage private data centers while taking advantage of the resources available from public cloud providers.
Multi-cloud doesn’t require the use of a private cloud or coordinated operations to work between the different cloud environments. That being said, a multi-cloud infrastructure can implement hybridization, which is sometimes called hybrid multi-cloud. This means that they use both public and private cloud, and these different cloud environments have some level of coordination between the two, allowing you to work within a single cloud IT infrastructure. A hybrid multi-cloud environment allows you to easily integrate Agile and DevOps best practices, securely and consistently deploy and scale data across various environments, and fully control your workloads.
Public cloud is a cloud deployment model where resources are hosted by a cloud service provider and shared across multiple organizations. Public clouds allow organizations to scale their resources without adding more physical resources, and only use cloud resources when necessary. This also means that the organization saves money, because the organization is only paying for resources when they need them, and they aren’t spending money on as much physical hardware or software packages.
Private cloud is a cloud deployment model where resources are hosted by an organization’s own infrastructure. The private cloud is typically hosted at either the organization’s own data center or at a third-party facility. This environment is usually physically secured and protected. The organization using the cloud service usually manages the private cloud, including performing maintenance, upgrades, software and cloud data management, and more. Private clouds allow organizations to have the same level of control and safety as a normal on-premises environment at a lower cost.
Container orchestration technologies, such as Kubernetes, play a major role in an effective multi-cloud architecture. Multi-cloud can be difficult to implement because the more clouds you have, the more difficult it is to consistently manage them. Kubernetes can help reduce some of the complications of implementing a multi-cloud architecture.
A major complication in multi-cloud is provisioning. With Kubernetes, you can host all of your workloads on it and use the same configurations for all of your clouds in your multi-cloud architecture. If you organize your workloads to run in Kubernetes, you only need to configure your monitoring tools to monitor Kubernetes, which takes a lot of the stress out of monitoring your entire infrastructure. Kubernetes also helps strengthen security because you can standardize configurations to reduce the chance of oversights. Kubernetes also has its own security features that you can leverage to secure your infrastructure.
Congrats on taking your first steps with multi-cloud! The multi-cloud model will continue to increase in popularity, as the cloud continues to take over the business landscape. Adopting a multi-cloud architecture has many benefits and use cases, allowing better business continuity, disaster recovery, flexibility, and resiliency. Combining containerization technology, like Kubernetes, with multi-cloud allows you to move your contained apps between different clouds and still have full functionality. It’s a key piece of multi-cloud deployment.There’s still so much more to learn about the cloud. Some recommended topics to cover next include:
Coding, Tutorials, News, UX, UI and much more related to development
80 
2
80 claps
80 
2
Written by
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
Coding, Tutorials, News, UX, UI and much more related to development
Written by
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
Coding, Tutorials, News, UX, UI and much more related to development
"
https://koukia.ca/introduction-to-azure-event-grid-87b058408c70?source=search_post---------271,"Azure Event Grid is a managed event routing platform which enables you to react in real-time to changes that are happening in your applications hosted in Azure or any Azure resources that you own.
Today to get notified of a stage change in a resource, such as a new record in a database, or when someone creates a Virtual Machine…
"
https://towardsdatascience.com/a-performance-benchmark-of-google-automl-vision-using-fashion-mnist-a9bf8fc1c74f?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sebastian Heinz
Aug 21, 2018·14 min read
Google AutoML Vision is a state-of-the-art cloud service from Google that is able to build deep learning models for image recognition completely fully automated and from scratch. In this post, Google AutoML Vision is used to build an image classification model on the Zalando Fashion-MNIST dataset, a recent variant of the classical MNIST dataset, which is considered to be more difficult to learn for ML models, compared to digit MNIST.
During the benchmark, both AutoML Vision training modes, “free” (0 $, limited to 1 hour computing time) and “paid” (approx. 480 $, 24 hours computing time) were used and evaluated:
Thereby, the free AutoML model achieved a macro AUC of 96.4% and an accuracy score of 88.9% on the test set at a computing time of approx. 30 minutes (early stopping). The paid AutoML model achieved a macro AUC of 98.5% on the test set with an accuracy score of 93.9%.
Recently, there is a growing interest in automated machine learning solutions. Products like H2O Driverless AI or DataRobot, just to name a few, aim at corporate customers and continue to make their way into professional data science teams and environments. For many use cases, AutoML solutions can significantly speed up time-2-model cycles and therefore allow for faster iteration and deployment of models (and actually start saving / making money in production).
Automated machine learning solutions will transform the data science and ML landscape substantially in the next 3–5 years. Thereby, many ML models or applications that nowadays require respective human input or expertise will likely be partly or fully automated by AI / ML models themselves. Likely, this will also yield a decline in overall demand for “classical” data science profiles in favor of more engineering and operations related data science roles that bring models into production.
A recent example of the rapid advancements in automated machine learning this is the development of deep learning image recognition models. Not too long ago, building an image classifier was a very challenging task that only few people were acutally capable of doing. Due to computational, methodological and software advances, barriers have been dramatically lowered to the point where you can build your first deep learning model with Keras in 10 lines of Python code and getting “okayish” results.
Undoubtly, there will still be many ML applications and cases that cannot be (fully) automated in the near future. Those cases will likely be more complex because basic ML tasks, such as fitting a classifier to a simple dataset, can and will easily be automated by machines.
At this point, first attempts in moving into the direction of machine learning automation are made. Google as well as other companies are investing in AutoML research and product development. One of the first professional automated ML products on the market is Google AutoML Vision.
Google AutoML Vision (at this point in beta) is Google’s cloud service for automated machine learning for image classification tasks. Using AutoML Vision, you can train and evaluate deep learning models without any knowledge of coding, neural networks or whatsoever.
AutoML Vision operates in the Google Cloud and can be used either based on a graphical user interface or via, REST, command line or Python. AutoML Vision implements strategies from Neural Architecture Search (NAS), currently a scientific field of high interest in deep learning research. NAS is based on the idea that another model, typically a neural network or reinforcement learning model, is designing the architecture of the neural network that aims to solve the machine learning task. Cornerstones in NAS research were the paper from Zoph et at. (2017) as well as Pham et al. (2018). The latter has also been implemented in the Python package autokeras (currently in pre-release phase) and makes neural architecture search feasible on desktop computers with a single GPU opposed to 500 GPUs used in Zoph et al.
The idea that an algorithm is able to discover architectures of a neural network seems very promising, however is still kind of limited due to computational contraints (I hope you don’t mind that I consider a 500–1000 GPU cluster as as computational contraint). But how good does neural architecture search actually work in a pre-market-ready product?
In the following section, Google AutoML vision is used to build an image recognition model based on the Fashion-MNIST dataset.
The Fashion-MNIST dataset is supposed to serve as a “drop-in replacement” for the traditional MNIST dataset and has been open-sourced by Europe’s online fashion giant Zalando’s research department (check the Fashion-MNIST GitHub repo and the Zalando reseach website). It contains 60,000 training and 10,000 test images of 10 different clothing categories (tops, pants, shoes etc.). Just like in MNIST, each image is a 28×28 grayscale image. It shares the same image size and structure of training and test images. Below are some examples from the dataset:
The makers of Fashion-MNIST argue, that nowadays the traditional MNIST dataset is a too simple task to solve — even simple convolutional neural networks achieve >99% accuracy on the test set whereas classical ML algorithms easily score >97%. For this and other reasons, Fashion-MNIST was created.
The Fashion-MNIST repo contains helper functions for loading the data as well as some scripts for benchmarking and testing your models. Also, there’s a neat visualization of an ebmedding of the data on the repo. After cloning, you can import the Fashion-MNIST data using a simple Python function (check the code in the next section) and start to build your model.
AutoML offers two ways of data ingestion: (1) upload a zip file that contains the training images in different folders, corresponding to the respective labels or (2) upload a CSV file that contains the Goolge cloud storage (GS) filepaths, labels and optionally the data partition for training, validation and test set. I decided to go with the CSV file because you can define the data partition (flag names are TRAIN, VALIDATION and TEST) in order to keep control over the experiment. Below is the required structure of the CSV file that needs to be uploaded to AutoML Vision (without the header!).
Just like MNIST, Fashion-MNIST data contains the pixel values of the respective images. To actually upload image files, I developed a short python script that takes care of the image creation, export and upload to GCP. The script iterates over each row of the Fashion-MNIST dataset, exports the image and uploads it into a Google Cloud storage bucket.
The function load_mnist is from the Fashion-MNIST repository and imports the training and test arrays into Python. After importing the training set, 10,000 examples are sampled and sotored as validation data using train_test_split from sklean.model_selection. The training, validation and test arrays are then stacked into X_data in order to have a single object for iteration. A placeholder DataFrame is initialized to store the required information (partition, filepath and label), required by AutoML Vision. storage from google.cloud connects to GCP using a service account json file (which I will, of course, not share here). Finally, the main process takes place, iterating over X_data, generating an image for each row, saving it to disk, uploading it to GCP and deleting the image since it is no longer needed. Lastly, I uploaded the exported CSV file into the Google Cloud storage bucket of the project.
AutoML Vision is currently in Beta, which means that you have to apply before trying it out. Since me and my colleagues are currently exploring the usage of automated machine learning in a computer vision project for one of our customers, I already have access to AutoML Vision through the GCP console.
The start screen looks pretty unspectacular at this point. You can start by clicking on “Get started with AutoML” or read the documentation, which is pretty basic so far but informative, especially when you’re not familiar with basic machine learning concepts such as train-test-splits, overfitting, prcision / recall etc.
After you started, Google AutoML takes you to the dataset dialog, which is the first step on the road to the final AutoML model. So far, nothing to report here. Later, you will find here all of your imported datasets.
After hitting “+ NEW DATASET” AutoML takes you to the “Create dataset” dialog. As mentioned before, new datasets can be added using two different methods, shown in the next image.
I’ve already uploaded the images from my computer as well as the CSV file containing the GS filepaths, partition information as well as the corresponding labels into the GS bucket. In order to add the dataset to AutoML Vision you must specify the filepath to the CSV file that contains the image GS-filepaths etc.
In the “Create dataset” dialog, you can also enable multi-label classification, if you have multiple labels per image, which is also a very helpful feature. After hitting “CREATE DATASET”, AutoML iterates over the provided file names and builds the dataset for modeling. What exactly is does, is neither visible nor documented. This import process may take a while, so it is showing you the funky “Knight Rider” progress bar.
After the import is finished, you will recieve an email from GCP, informing you that the import of the dataset is completed. I find this helpful because you don’t have to keep the browser window open and stare at the progress bar all the time.
The email looks a bit weird, but hey, it’s still beta…
Back to AutoML. The first thing you see after building your dataset are the imported images. In this example, the images are a bit pixelated because they are only 28×28 in resolution. You can navigate through the different labels using the nav bar on the left side and also manually add labels so far unlabeled images. Furthermore, you can request a human labeling service if you do not have any labels that come with your images. Additionally, you can create new labels if you need to add a category etc.
Now let’s get serious. After going to the “TRAIN” dialog, AutoML informs you about the frequency distribution of your labels. It recommends a minimum count of $n=100$ labels per class (which I find quite low). Also, it seems that it shows you the frequencies of the whole dataset (train, validation and test together). A grouped frquency plot by data partition would be more informative at this point, in my opinion.
A click on “start training” takes you to a popup window where you can define the model name and the allocate a training budget (computing time / money) you are willing to invest. You have the choice between “1 compute hour”, whis is free for 10 models every month, or “24 compute hours (higher quality)” that comes with a price tag of approx. 480 $ (1 hour of AutoML computing costs 20 $. Hovever, if the architecture search converges at an earlier point, you will only pay the amount of computing time that has been consumed so far, which I find reasonable and fair. Lastly, there is also the option to define a custom training time, e.g. 5 hours.
In this experiment, I tried both, the “free” version of AutoML but I also went “all-in” and seleced the 24 hours option to achieve the best model possible (“paid model”). Let’s see, what you can expect from a 480 $ cutting edge AutoML solution. After hitting “START TRAINING” the familiar Knight Rider screen appears telling you, that you can close the browser window and let AutoML do the rest. Naise.
First, let’s start with the free model. It took approx. 30mins of training and seemed to have converged a solution very quickly. I am not sure, what exactly AutoML does when it evaluates convergence criteria but it seems to be different between the free and paid model, because the free model converged already around 30 minutes of computing and the paid model did not.
The overall model metrics of the free model look pretty decent. An average precision of 96.4% on the testset at a macro class 1 presision of 90.9% and a recall of 87.7%. The current accuracy benchmark on the Fashion-MNIST dataset is at 96.7% (WRN40–4 8.9M parameters) followed by 96.3% (WRN-28–10 + Random Erasing) while the accuracy of the low budget model is only at 89.0%. So the free AutoML model is pretty far away from the current Fashion-MNIST benchmark. Below, you’ll find the screenshot of the free model’s metrics.
The model metrics of the paid model look significantly better. It achieved an average precision of 98.5% on the testset at a macro class 1 presision of 95.0% and a recall of 92.8% as well as an accuracy score of 93.9%. Those results are close to the current benchmark, however, not so close as I hoped. Below, you’ll find the screenshot of the paid model’s metrics.
The “EVALUATE” tab also shows you further detailed metrics such as precision / recall curves as well as sliders for classification cutoffs that impact the model metrics respectively. At the bottom of the page you’ll find the confusion matrix with relative freuqencies of correct and misclassified examples. Furthermore, you can check images of false positives and negatives per class (which is very helpful, if you want to understand why and when your model is doing something wrong). Overall, the model evaluation functionalities are limited but user friendly. As a more profound user, of course, I would like to see more advanced features but considering the target group and the status of development I think it is pretty decent.
After fitting and evaluating your model you can use several methods to predict new images. First, you can use the AutoML user interface to upload new images from your local machine. This is a great way for unexperienced users to apply their model to new images and get predictions. For advanced users and developers, AutoML vision exposes the model through an API on the GCP while taking care of all the technical infrastructure in the background. A simple Python script shows the basic usage of the API:
As a third method, it is also possible to curl the API in the command line, if you want to go full nerdcore. I think, the automated API exposure is a great feature because it lets you integrate your model in all kinds of scripts and applications. Furthermore, Google takes care of all the nitty-gritty things that come into play when you want to scale the model to hundrets or thousands of API requests simultaneously in a production environment.
In a nutshell, even the free model achieved pretty good results on the test set, given that the actual amount of time invested in the model was only a fraction of time it would have taken to build the model manually. The paid model achieved significantly better results, however at a cost note of 480 $. Obviously, the paid service is targeted at data science professionals and companies.
AutoML Vision is only a part of a set of new AutoML applications that come to the Google Cloud (check these announcements from Google Next 18), further shaping the positioning of the platform in the direction of machine learning and AI.
In my personal opinion, I am confident that automated machine learning solutions will continue to make their way into professional data science projects and applications. With automated machine learning, you can (1) build baseline models for benchmarking your custom solutions, (2) iterate use cases and data products faster and (3) get quicker to the point, when you actually start to make money with your data — in production.
If you have any comments or questions on my story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice. Follow me on LinkedIn or Twitter, if you want to stay in touch.
Make sure, you also check the awesome STATWORX Blog for more interesting data science, ML and AI content straight from the our office in Frankfurt, Germany!
If you’re interested in more quality content like this, join my mailing list, constantly bringing you new data science, machine learning and AI reads and treats from me and my team right into your inbox!
I hope you liked my story, I really enjoyed writing it. Thank you for your time!
Originally published at www.statworx.com on August 20, 2018.
CEO @ STATWORX. Doing data science, stats and ML for over a decade. Food, wine and cocktail enthusiast. Check our website: https://www.statworx.com
44 
1
44 
44 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/devopsturkiye/react-native-mobil-translate-uygulamas%C4%B1-a4d4f13f4937?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Abdulkerim Karaman
Sep 9, 2019·3 min read
Bir çok bulut servis sağlayıcı firma (Google, AWS, Yandex v.s.) yapay zeka destekli dil çeviri servisi sunmaktadır. Sağlayıcılar belli bir istek sayısına kadar ücretsiz olarak hizmet vermekle birlikte ücretli paketlerde sunulmaktadır. Bu makalemizde örnek bir mobil çeviri uygulaması ile bu servisleri nasıl kullanacağımızı inceleyeceğiz.
Bu örneğimizde Yandex’in sunmuş olduğu çeviri servisini kullanacağız. Bu servis arka tarafta istatistiksel makine çevirisi ve yapay makine çevirisi kullanmaktadır.
İstatistiksel Makine Çevirisi:
Aynı anlama sahip, ancak farklı dillerde yazılmış yüzbinlerce paralel metni karşılaştırarak bir çeviri modeli oluşturur. Eşleşen kelimeler karşılaştırma sırasında çıkarılır ve bir matriste saklanır. Örneğin, sistem “köpek” ve “dog” un birbirlerinin muhtemel çevirileri olduğuna karar verir ve bu bilgiyi saklar. Bir dil modeli oluşturmak için sistem tek bir dilde metinleri analiz eder ve kullanılan tüm kelime ve cümleleri listeler. Her kelimeye ve cümleye, dildeki istatistiksel sıklığını tanımlayan kendi sayısal kimliği verilmiştir (ne sıklıkla kullanıldığı).
Yapay Makine Çevirisi:
İstatistiksel yaklaşıma benzer şekilde, sinir ağı aynı zamanda bir dizi paralel metni analiz eder, içlerinde kalıp bulmayı öğrenir ve kullanılan tüm sözcük ve cümlelerin listesini çıkarır.
Bununla beraber, istatistiksel yaklaşım gibi basit tanımlayıcıları kullanmak yerine, sinirsel makine çevirisi kelime gömme adı verilen methodu kullanır. Her sözcük için sözcüksel ve anlamsal özelliklerini tanımlayan sayılardan oluşan bir vektör oluşturulur.
Çeviri için kelimeleri ve cümleleri ayırmak yerine sinir ağı, her bir kaynak cümleyi bir bütün olarak tercüme eder cümlede her kelime birkaç yüz numara uzunluğunda bir vektör ile eşleştirilir. Cümle bir vektör uzayına dönüştürülür. Bu vektör uzayı, kelimelerin anlamlarını ve ilişkilerinin belirlenmesini sağlar.
Bu kadar bilgiden sonra uygulama kısmına geçelim.
Öncelikle https://translate.yandex.com/developers/keys adresine giderek “create a new key” seçerek yeni bir key oluşturalım. (Ücretsiz olarak belli bir istek sayısına kadar kullanmanıza imkan veriyor.)
Key’i oluşturduktan sonra örnek bir istek yapalım. Detaylı bilgi için https://tech.yandex.com/translate/doc/dg/concepts/about-docpage/ adresindeki dökümanı inceleyebilirsiniz.
Aşağıdaki örnek request’i kullanabilirsiniz.
Sonuca göz atalım:
Başarılı bir şekilde çalıştı. Şimdi yeni bir mobil uygulama oluşturalım. (React Native)
App.js dosyasını aşağıdaki şekilde değiştirelim.
Kısaca bahsetmek gerekirse. Kullanıcının çeviri yapacağı metni girmesi için bir adet TextInput ve Translate butonu ekledik. Kullanıcı translate butonuna tıkladığında TextInput’ dan aldığımız değeri servise gönderiyoruz ve dönen değeri ekrana yazdırıyoruz.
Uygulama içinde React Hooks ile gelen useState kullanılmıştır. React Hooks ile ilgili ayrıntılı bilgiye aşağıdaki makaleden ulaşabilirsiniz.
medium.com
Evet uygulamamızı test ediyoruz ve sorunsuz şekilde çalıştığını gözlemliyoruz.
Repo:
https://github.com/akaramanapp/MobileTranslateApp
Faydalı olması dileğiyle. Hoşçakalın.
Full Stack Developer
67 
67 
67 
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
"
https://medium.com/@pv.safronov/how-i-built-a-web-service-in-appengine-over-the-weekend-6a90192c4699?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pavel Safronov
Oct 11, 2021·8 min read
10 years ago, when I had just started my career, there was almost no automation available. To build a simple service, you would almost certainly end up purchasing a VPS, configuring Nginx, installing all the necessary packages, thinking about security, installing and configuring databases, etc. What I did over this weekend still looks like magic to me. Spin-up service with caches, distributed queues, balancers, etc., is a matter of few minutes these days.
Today I’ll tell you about the service I built over the last weekend. No rocket science here. The service is quite small. But still, it serves as proof that one can create such a service in just 2 days. That means that you can put your ideas into code with a certain level of motivation and spare time, and one of the modern cloud services will take care of the rest.
But enough on that for now, let’s jump to the implementation details.
A long time ago, I had this idea to optimize my study plan by picking the Leetcode problem from the topic I didn’t touch before. In this article I described a manual approach for that. Now I want to automate things even more, so I wanted to create a web service that everybody can use.
Specs for the service:
If you’re impatient, you can check the result here https://avid-life-90820.appspot.com/.
The source code is available here https://github.com/prius/grind-helper
For every problem, I like to follow this framework:
This step is about making it work. For that, I’ve built a flask app locally. This app makes a series of requests to leetcode and constructs the statistics I need on each request.
From this stage, we were ready to move to the next step since I knew for sure I had all the necessary libraries, and those libraries provided all the information I needed.
In particular:
Now to make the solution usable, I had to overcome the following issues:
So for that code to serve the request, I had to come up with some database or cache that lets me avoid fan out to the requests to leetcode API on every user request. That is when Google AppEngine comes to play, and you start realizing how cool that is.
It took me some time to figure out how to use Google Cloud. I’d say I’ve spent good several hours figuring out which services I need and how actually to work with them. But this time investment is definitely worth it. A long time ago, I spent months of my time learning how balancers, reverse proxies, caches, firewalls, etc., work. Now it is just a matter of figuring out which button to press in the UI or which python library to use.
I started with using Cloud Run. The main problem for me was where to store the cached results for the data from leetcode. Both Postgres database and Memcache service were like $40 per month, which is not worth it for a pet project. Later I realized that if you run an app in the AppEngine, it provides you a shared Memcache installation for free. This installation offers you no guarantees at all, but again, for a pet project, that’s fine. And the best part is that to use Memcache, you just have to import a python library. That’s it. No jokes. Literally, import the library and do memcache.set(key, val) straight away. Quite impressive IMO.
The final solution comprises of the following components:
And all of that is available to you out of the box. Could you imagine anything like that 10 years ago?
The workflow for the worker is the following:
Workflow for the user-facing part:
In the end, you’ll have a result that looks like this:
I had to validate the user input. Because having such a form exposed to the public is a great source of all kinds of code injections. So before publishing the service, I’ve made a validator for the form to ensure all the information I use from there is safe to use.
Another reason to split service to the “worker” and “user-facing” part was to prevent external users from accessing the sensitive data and issuing calls directly to my leetcode account. It is not like a lot can be done having my leetcode token, but still.
Of course, it is never enough security. But there are always tradeoffs. I decided I made enough for this simple service to feel comfortable publishing it to the Internet.
When it all works, and I am almost ready to publish my service, there are a couple of small steps left.
First of all, I don’t want the service to look ugly (as it does with plain HTML). I remember there was a Twitter bootstrap that does the job for you once imported. But by now, it looks a bit outdated to my taste. So I asked a front-end engineer friend of mine what is popular these days. He recommended me Material-UI, another UI based on Google’s guidelines. I’m quite happy with what I’ve got. One more time this weekend, Google made my life much easier.
Once I did that, I also had to resolve the titles of the tags and problems correctly. It took me a while to add some code to populate the cache with this data. Now you can see human-readable names and titles instead of just Leetcode’s internal identifiers.
There is still a huge room for optimization here. So far, I have just made requests to the Memcache from the user-facing app asynchronous. Because for each user request, I have to make like 4000 Memcache calls, which produces a lot of IO. Of course, this still sounds crazy, even if it is asynchronous. I need to aggregate the results in cache and store them to resolve them in batches. Making so many IO requests per one user request for such a simple application is no good.
So the “make it fast” phase is still ahead. However, for the current state, when the service has just one user (myself), it is not worth putting so much effort into it.
After my service finished, I felt that Google does a lot to facilitate life not only for ordinary people but also for developers.
It is not a marketing post and not a post praising Google, though. Should I choose some other cloud provider, like Amazon or Yandex, I’m pretty sure I would have had a similar experience because those folks are competing with each other intensively. It is more about how the development process was 10 years ago and how it is now. Back then, to create a similar service with similar infrastructure, I would spend like 2 weeks, I guess. And some things I would never be able to achieve at all (like secure key management, load balancers (cause I need access to BGP for that), etc.). Today it is so simple. Just click a couple of buttons, and here you go.
Still, I believe that this knowledge I acquired when I had to work on a much lower level of abstraction helps me to be more effective now. I know how all that stuff works under the hood, so I know exactly what I’ll get when I press those buttons and how to use it. It saves time. But anyway, I’m happy for the young developers who don’t need to mess around with all that low-level crap and can spend their time actually doing something useful.
On a sad note, what I didn’t really like is that when you use all that magic provided by Google, you make your service not portable. Using Google libraries is fun and easy, but it’ll be painful if you decide to switch to another cloud. If you are thinking about scaling your service, you have to implement adapters for all the external calls to migrate in the future. I’m sure there are such adapters in open-source already, but I have a little expertise here.
And finally, the lesson I learned this weekend is that if you have some idea, you can just sit for a couple of days and implement it instead of ruminating on this idea for months. It is so easy today, just import some libraries and write some code. Cloud providers will take care of the rest.
Facebook SRE (ex-Yandex) github.com/prius
127 
127 
127 
Facebook SRE (ex-Yandex) github.com/prius
"
https://medium.com/@seema.singh/reading-and-writing-files-with-google-colaboratory-f0c234683946?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
Seema Singh
Apr 8, 2020·2 min read
Google Colaboratory or Google colab is a great free cloud service which allows us to leverage cloud resources like GPU’s and TPU’s for executing heavy ML models. Along with GPU’s and TPU’s it also provides a support to install various other libraries on the cloud, resolving the installation issues of local environment.
This service replicates the Jupyter Notebook on cloud and can be used in the exact way how it’s used in the local environment except some basic differences like reading and writing files on cloud.
The only differentiating factor between Google Colab and and other cloud services is the charges. Google Colab provides GPUs/TPU’s which are absolutely free of cost unlike other cloud services
Simple example to read and write csv file with Google Colab:
Step 1: Create a new notebook
Step 2: Rename the Jupyter Notebook and code the required logic
Following example is scraping the COVID19 data from ‘https://www.mohfw.gov.in/'
Step 3: Save the csv to Google Drive
We need to mount the drive for saving csv to Google Drive.
Step 4: Read the csv from Google Drive
This is one of the methods of reading and writing with Google Colab. There are lots of other ways for carrying out the same task.
Thank you for reading!.
Data Scientist
See all (111)
103 
6
103 claps
103 
6
Data Scientist
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/penn-engineering/taking-penn-engineering-technology-to-market-a724f368e45b?source=search_post---------276,"There are currently no responses for this story.
Be the first to respond.
By Lida Tunesi
Boon Thau Loo, professor in the Department of Computer and Information Science (CIS) and associate dean of Master’s and Professional Programs, acknowledges that there is a divide between the worlds of academia and industry.
“As a professor, you can hardly publish a research paper on an idea that is useful for one narrow application,” Loo says. “You have to think about general purpose abstractions or platforms. But when you establish a startup, that mindset changes. With limited resources, you shouldn’t be trying to boil the ocean. You have to find one compelling product that you can put in the hands of a customer.”
By collaborating on the CIS department’s first-ever spinoff, a startup called Netsil, Loo and his lab members gained the skills to cross that chasm.
Netsil’s visual interface makes it easy to monitor the performance and activity of a cloud application. This type of monitoring is growing increasingly important since engineers now commonly build web apps from separate modules. One app can rely on hundreds of smaller services that need to seamlessly communicate with one another. These dispersed services can be complicated to keep track of, so Netsil provides a way to map out an app’s networked pieces and make sure they are operating correctly and in sync with one another.
However, the idea behind Netsil was not always obvious; this kind of monitoring has only recently become necessary.
The project started taking shape in 2012 when Loo’s NetDB@Penn research group created a technology called Scalanytics, which was an offshoot of his bigger research agenda on declarative networking. In declarative networking, network administrators “declare” the intended behavior of their networks using a simplified programming language. A smart compiler then automatically infers the rest of the actual code, freeing up the operator from manually writing complex programs that are prone to human error.
Scalanytics is a toolkit for building a high-performance network monitoring system using these declarative networking principles. To encompass different needs, like checking for performance problems or potential security issues, the team made Scalanytics modular and customizable.
“It lets you build monitoring functionality in an easy way, like building with LEGO blocks,” Loo says.
The team felt that Scalanytics had enough potential to bring it out of the lab and turn it into a product they could sell. Using a National Science Foundation Small Business Innovation Research grant, Loo took a year of sabbatical leave in early 2014 to form a startup around the Scalanytics technology with students from his research team, brothers Harjot and Tanveer Gill, and sisters Cam and Anh Nguyen. Their company would later become Netsil.
Soon enough, the group hit a snag. They learned that a good piece of technology does not necessarily equal a marketable commodity, and realized they would have to shift their thinking and find a niche for Scalanytics.
“What we thought was a product was really just a piece of technology.” Loo says. “There were hundreds of ways we could have applied the technology, so where do we start? A lot of what we did in the initial days was figuring out the right product-market fit. You can only this figure out by leaving the research lab and engaging real users to solve their problems. We had a couple of false starts. I strongly credit my students for figuring this out.”
Pieces started to fall into place for Netsil as the industry saw a rise in the practice of building software from small components with individual purposes, rather than all in one piece. These components are called microservices. One microservice might be a firewall checking for security intrusions, another might be a data analytics package that figures out where customer traffic is coming from. Each microservice is independent of the others and they communicate via API calls.
“Microservice architecture is a way of taking a complex cloud service, and breaking in into smaller, modular software components,” Loo says. “This architecture makes it easy to deploy new or modify existing cloud services.”
Ten or fifteen years ago, Loo says, when users visited a website, they might have hit the web server and its one database, but that was it. These days web apps are much more complex and might even consist of hundreds of software components. To complicate things even more, some of those components can be sourced from third-party vendors.
Microservices have some advantages. Since they are independent of one another, developers can manage, scale, fix, or replace a microservice without affecting the rest of the software. Some of these traits make them very well-suited for cloud applications, and even for running traditional hardware network appliances such as network routers and switches as software modules in the cloud. Microservices for one app can even be written in different programming languages, providing some flexibility.
However, these modular pieces also come with their own set of problems. For one, the more microservices you have, the more challenging it is to keep track of them and to follow the communication between them. This is where Netsil found its calling. The product creates a visual representation of the microservices that make up an application, and of how they are talking to each other.
“We went from a very general technology to creating what Netsil calls the ‘Google Maps of microservices,’” Loo says.
This map makes it easier to pinpoint the source of problems or, ideally, to spot and fix potential issues before they happen. This in turn improves the experience of the app’s users by preventing crashes and downtime.
The technology also circumvents another issue that arises with this type of web service architecture. The code behind a microservice is sometimes “opaque,” meaning a systems administrator deploying the service cannot see its inner workings. So rather than struggling to monitor each individual microservice, Netsil looks at the bigger, network-scale picture. It then reverse-engineers that information to deduce what’s going on at the smaller levels and map it out.
“Netsil taps network traffic to make sense of what’s happening within the application,” Loo says.
Because of this approach, Netsil is also non-invasive. Many times, if a programmer wants to watch over a piece of code as it works, he or she must add new lines of code with instructions for monitoring. Netsil’s technology avoids this.
“It’s like when you go to see the doctor,” Loo says. “They don’t need to cut open your stomach to know why you’re sick.”
After Loo returned to Penn, the Netsil team, which has now grown to include another Penn CIS graduate, Kevin Lu, moved from Philadelphia to San Francisco. It was acquired by Nutanix, a publicly listed company that provides businesses “hyperconverged infrastructure,” or the ability to manage multiple cloud resources within a single control point.
Loo says his foray into the world of startups gave him deeper insights into his own work.
“I have a better understanding of near-term projects that industry is best suited to do, as opposed to fundamental research that has a much longer time horizon,” he says. “It is the latter that universities should focus on. These days, I start new research projects by thinking hard about why anyone would care five to ten years down the road.”
The experience at Netsil also gave him new tools at Penn. “I attribute my ability to run our master’s programs to the organizational skills I picked up while working on this venture,” Loo says. “I would not be able to do my job as effectively if it weren’t for this experience.”
Loo also views the tech transfer experience as an important opportunity for research mentorship. “It is very satisfying to see my former advisees and students take on leadership roles in the company, taking it to great heights. Student development is a reason why many of us become professors.”
University of Pennsylvania’s School of Engineering and…
167 
167 claps
167 
University of Pennsylvania’s School of Engineering and Applied Science
Written by

University of Pennsylvania’s School of Engineering and Applied Science
"
https://medium.com/shecodeafrica/understanding-the-cloud-286d5630d2f?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
While surfing the Internet or having a discussion with a technical colleague or friend, you may have at some point or the other heard about the word “Cloud / Cloud computing” and it’s association to storage, In your curious state, you try to find out it’s meaning but really all that Technical grammar in the explanation never gets to you
What Exactly Is The Cloud?
Your typical technical blog will describe it as “ a computing-infrastructure and software model for enabling ubiquitous access to shared pools of configurable resources (such as computer networks, servers, storage, applications and services),which can be rapidly provisioned with minimal management effort, often over the Internet.”
No, Really, What Exactly Is The Cloud?
To break all that down, you first need to understand the Cloud isn’t a physical thing (neither is it the cloud up in the sky) you get to use like a safe.
“The cloud” is simply all of the things you can access remotely on the internet as an individual. The saying “something’s being stored in the cloud” simply means it’s being stored on a server or multiple servers built by another individual in a particular location rather than your normal PC’s hard drive or Phone’s storage.
We make use of the cloud in some of our daily activities but have little or no idea we’re really doing that. Here are a few examples of how we use the cloud:
Some of the popular cloud storage services available used to host data online include: Google Cloud, Amazon web services(AWS) owned by Amazon which has been around since 2006 and is used by a host of popular sites including Instagram and Pinterest
How Big Is The Cloud?
Unlike our local storage on our PC’s or Phones which have a storage limit, the storage limit for a particular cloud service is limitless, no one knows the amount of space that can be created but according to an infographic, the cloud can store up to 1 Exabyte
How Safe Is The Cloud?
With growing concerns in personal information being compromised, many individuals get worried about making use of them, but Some Companies like Google are tackling that with their data encryption for paid cloud storage users.
Benefits Of working with The Cloud?
There are immense benefits associated with working with the cloud, the biggest being how it allows you store and access data(large or small) remotely from any location without having to worry about storage space. It also allows large companies run efficiently and cost-effective while scaling up or down on data size:
We celebrate and empower women in Technology across Africa…
95 
95 claps
95 
We celebrate and empower women in Technology across Africa by telling their stories, empowering them through technical programs & activities while also helping them share their knowledge and ideas through articles.
Written by
Software Developer | Developer Relations and community Expert | DEI Consultant
We celebrate and empower women in Technology across Africa by telling their stories, empowering them through technical programs & activities while also helping them share their knowledge and ideas through articles.
"
https://blog.inten.to/measuring-api-developer-experience-91b5c19645b0?source=search_post---------278,NA
https://medium.com/the-vue-storefront-journal/storefront-cloud-is-out-ab1da9cc1115?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
The last few months were a real rush. We worked silently (stealth mode) on a PaaS solution based on Vue Storefront. We wanted to have it ready before Meet Magento New York. Now, it’s ready!
Vue Storefront got pretty solid
Meet Magento New York, 2017 was the place of the first public Vue Storefront project announcement. Now, with 3.3K stars on Github, a few dozen live-sites, over 800 devs in the community and 80 active contributors, the project has proved its value.
It started and will be continued as an Open Source project with a monthly release cycle. We’re pushing it forward faster than ever — working mostly on quality, not quantity with each new release.
We’re really grateful to our Community for helping to make this happen. We’ve had a think — we believe we can make it even better.
The solution integrated with Storefront Cloud
The Vue Storefront goals were to improve the Customer and Developer’s experience. Developer’s love our tech stack and the simplicity of working with the platform, and customers appreciate the speed and user experience it provides.
Vue Storefront works great as a framework, however, there are still dev-ops, and maintenance resources required to make it happen at scale. We decided that this is the missing piece of the puzzle, and we can provide it, resulting in a one-stop integrated service.
Storefront Cloud is a mobile-first eCommerce platform that helps you build an engaging user-experience across devices.
“The Progressive Web App (PWA) and Accelerated Mobile Page (AMP) standards — introduced by Google, and currently getting traction, can be a game changer in this situation. A native-like user experience, and page loading times < 1s can increase conversion rates by 20–50% and more.” — Patryk Piątek, Co-founder of Storefront Cloud
The Storefront Cloud platform provides users with a complex mobile and desktop eCommerce frontend that is connected with your existing backend system.
It’s based on Vue Storefront. This allows you to use PWA and AMP technologies, which make the experience faster and smoother for the client, especially on mobile devices. Moreover, with the integrated Open Loyalty platform, merchants are provided with a whole set of tools for re-engaging users and managing all touchpoints (offline and online).
The scalability is unlimited — and the scale is tailored by the Cloud Team to client’s traffic and sales needs. The Storefront Cloud stack has been built on Kubernetes. It’s based on micro-services, headless architecture using Amazon Web Services (like Cloudfront), Fastly, ElasticSearch and other cloud services to provide the required scalability and speed.
No changes to your existing eCommerce are required. Storefront Cloud users are provided with full access to Vue Storefront and Open Loyalty solutions for preparing a customized user experience tailored to industry and customer habits.
All Storefront Cloud users will receive access to the Storefront Cloud Help Center and a set of Command Line tools for managing the Kubernetes cluster, as well as the code deployments and staging environments.
Batteries Included
We’re providing Storefront Cloud users with a full set of tools required to manage the infrastructure and deployments.
You can check our Command Line tool on Github and read the Developer’s docs.
Beta Access Program
The service is currently available in the closed Beta Access Program for our partners and agencies. Please contact us to get an invitation and to test Storefront Cloud!
The official journal of Vue Storefront community.
278 
1
278 claps
278 
1
Written by

The official journal of Vue Storefront community. The latest news on meet-ups, contributions, releases, and the community surrounding Vue Storefront — the top open source solution for progressive web apps dedicated to mobile-first eCommerce. (www.vuestorefront.io)
Written by

The official journal of Vue Storefront community. The latest news on meet-ups, contributions, releases, and the community surrounding Vue Storefront — the top open source solution for progressive web apps dedicated to mobile-first eCommerce. (www.vuestorefront.io)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/predict/5-technologies-that-can-revolutionize-our-everyday-life-in-2021-a466e699817b?source=search_post---------280,"There are currently no responses for this story.
Be the first to respond.
From Hybrid Cloud Services to 5G and from Quantum Computers to self-driving vehicles. In 2021 there will be no surprising inventions but for the first time things already invented will have a widespread and extended practical application.
Artificial intelligence, 5G, the Internet of things, self-driving cars, quantum computers are all things already…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/keep-your-cluster-secrets-up-to-date-with-manifold-kubernetes-8d483fd4cc1?source=search_post---------281,"There are currently no responses for this story.
Be the first to respond.
At Manifold, we’re dedicated to helping developers find, organize, and connect the best services to their applications. We recognize that developers need efficient workflows, which means making sure Manifold works effortlessly with the most popular tools and platforms. Today we are pleased to announce Manifold for Kubernetes. This new integration makes it easier for developers who use Kubernetes to deploy configuration and secrets exactly where they need it.
How will our Kubernetes integration help? If you’re a developer using Kubernetes, your stack is likely composed of several first party and third party services. And traditionally you would rely on repetitive configuration files or error prone `kubectl` commands, inevitably causing services to be misconfigured.
Our custom resource definition facilitates cloud native delivery of your Manifold-managed resources and secrets exactly where you need them (even in multiple locations). With Manifold connected to your Kubernetes cluster you won’t worry about deployed secrets being out of date again. Kubernetes will continuously source the most up to date values directly from Manifold.
Stay tuned for further integrations launching throughout the month of August. You can keep up to date by visiting our new Integrations page. You can share your ideas for what we should integrate with next to hello@manifold.co.
You can start using our Kubernetes integration today by following the Getting Started Guide available in our integrations documentation.
If you want to dig into the internals of our Kubernetes integration, check out the repo on Github at https://github.com/manifoldco/kubernetes-credentials. There you will find the source code, details on how to contribute, as well as an avenue for logging and bugs or feature requests. And if you like the project, throw us a star ⭐️!
github.com
Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.
It groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community.
Source: kubernetes.io
We’re interested in what you’re building! Whether you’re using Kubernetes yet or not, send us a message, to hello@manifold.co, with how you’re using Manifold for and what we could be doing better.
We're determined to make it easy for developers to use the…
383 
383 claps
383 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/deploy-any-resource-with-manifold-terraform-a199b765b3d?source=search_post---------282,"There are currently no responses for this story.
Be the first to respond.
At Manifold, we’re dedicated to helping developers find, organize, and connect the best services to their applications. We recognize that developers need efficient workflows, which means making sure Manifold works effortlessly with the most popular tools and platforms. Today we are pleased to announce the Manifold Terraform provider. This new integration makes it easier for developers who use Terraform to deploy configuration and secrets exactly where they need it.
How will our Terraform integration help? If you’re a developer using Terraform, your stack is likely composed of first and third party services, and organizing these is a headache. Inevitably you find your configuration files and deploy scripts riddled with secrets and application config.
The Manifold Terraform provider enables you to read the data of your provisioned resources. With access to your resources we’ve streamlined delivery of secrets and configuration to the appropriate applications in your stack, or other providers. No more hard-coding secrets in your Terraform config files.
Stay tuned for further integrations launching throughout the month of August. You can keep up to date by visiting our new Integrations page. You can share your ideas for what we should integrate with next to hello@manifold.co.
You can start using our Terraform provider today by following the Getting Started Guide available in our integrations documentation.
If you want to dig into the internals of our Terraform integration, check out the repo on Github at https://github.com/manifoldco/terraform-provider-manifold. There you will find the source code, details on how to contribute, as well as an avenue for logging and bugs or feature requests. And if you like the project, throw us a star ⭐️!
github.com
Having your infrastructure as code allows you to feel confident that your deployments always have the correct shape and configuration.
You start by creating configuration files which describe to Terraform the components needed to run a single application or your entire infrastructure. Terraform generates an execution plan describing what it will do to reach the described state for you (create a load balancer or compute instance, ship an application tarball, add configuration or secrets, and more).
We’re interested in what you’re building! Whether you’re using Terraform yet or not, send us a message, to hello@manifold.co, with how you’re using Manifold for and what we could be doing better.
We're determined to make it easy for developers to use the…
514 
514 claps
514 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devops-dudes/5-differences-between-eks-and-openshift-36838d9fb856?source=search_post---------283,"There are currently no responses for this story.
Be the first to respond.
Recently I got the chance to finally play around with EKS, stand up a control plane and worker nodes and actually deploy an application to it. Last year I deployed my first application to OpenShift. Here are some of the key differences between the two:
"
https://medium.com/@datapath_io/7-things-you-need-to-know-about-aws-direct-connect-gateway-7deb1d6c7b85?source=search_post---------284,"Sign in
There are currently no responses for this story.
Be the first to respond.
Datapath.io
Mar 27, 2018·3 min read
AWS recently announced the Direct Connect Gateway. It is a super useful service aimed at making it easier to set up dedicated connectivity between Direct Connect and globally distributed VPCs. Below we take a look at what it is and what you can do with it.
AWS Direct Connect Gateway is a service built on top of the AWS Direct Connect. It allows AWS Direct Connect users to connect multiple VPCs in the same or different AWS regions to their Direct Connect connection.
AWS Direct Connect was born out of the need for establishing dedicated connectivity to AWS from on-premise or enterprise data centres.
However, connecting from a single Direct Connect location to multiple AWS regions wasn’t so straightforward.
The Direct Connect Gateway makes this easier and reduces management over head in the process.
With the AWS Direct Connect Gateway creating connections from a single Direct Connect to multiple VPCs in different AWS regions is pretty straight forward.
One use-case which it does not address is Inter Region VPC peering i.e. creating direct VPC to VPC connectivity across AWS regions. But more on that later.
The Direct Connect Gateway sits between the Direct Connect locations and customer VPCs.
VPCs distributed across regions are connected to the Direct Connect Gateway via a Virtual private Gateway.
Direct Connect Gateways are in turn connected to AWS Direct Connect connection over a virtual private interface.
Before the introduction of the AWS DirectConnect Gateway AWS users could only provision local VIFs. This meant that there was no way to connect multiple AWS regions or VPCs to a remote Direct Connect connection.
Connecting multiple AWS VPCs to the same Direct Connect Gateway becomes possible with the AWS Direct Connect Gateway. The Direct Connect Gateway can be connected to multiple VGWs at the same time. It also connects to the AWS Direct Connect through a single Virtual Private Interface.
What had to be done with multiple VIFs before, can now be done using a single VIF, with the AWS Direct Connect Gateway.
As Jeff Bar points out in this blog post, this leads to significant reductions in administrative overhead as well as reducing the load on network devices.
With this new announcement AWS has made it super easy to setup connectivity between both local and remote VPCs and the Direct Connect. However, considering that this is the first iteration of the product it still has some shortcomings:
Datapath.io’s Inter Region Connect allows to setup secure and dedicated connectivity between VPCs in different AWS regions. Customers can also provision full-mesh IPsec VPN on AWS and between VPCs.
https://datapath.io/resources/blog/7-things-need-know-aws-direct-connect-gateway/
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
18 
1
18 
18 
1
Cloud to Cloud Network - All your multi-cloud applications connected via a secure and dedicated private network https://datapath.io/
"
https://blog.qtum.org/qtum-partners-with-google-cloud-to-launch-suite-of-developer-tools-91ef68090a4?source=search_post---------285,"Qtum has unveiled a full suite of blockchain developer tools in partnership with Google Cloud. These free-to-use tools, are designed to give developers and non-technical users alike, a simple and cost-effective way of launching nodes and building on the Qtum blockchain.
“Google Cloud is the perfect partner to help us make the blockchain ecosystem simpler and more intuitive. Where launching a node was once an intensive and complex process, Qtum’s new developer suite introduces helpful shortcuts and tools to make it faster and easier. With a more accessible technology, we hope to open up and expand the Qtum community to include people with a broader range of experience — from experts to the everyday user.” — said Miguel Palencia, Qtum CIO.
The suite of tools launched on Google Cloud are available through the Qtum compute engine. The compute engine lets anyone launch a full developer environment on Qtum and begin developing or staking in a matter of seconds, free of cost. Previously, developers had to source the necessary tools themselves to create a full Qtum node or decentralized applications (dApps). With Qtum, users now have access to the suite of tools necessary to build dApps, launch a full node, make a fork, or begin staking on the Qtum blockchain on Google Cloud. When the Qtum source code is updated, Google Cloud will automatically update the code everywhere, saving developers the need to manually re-download in order to remain on the latest version.
The Qtum developer toolkit includes Qtum Core, a Solidity Compiler, Qmix IDE, Solar (smart contracts deployment tool), Qt-dev libraries, and all other necessary libraries and tools to develop dApps.
To access the Qtum blockchain developer tool, visit this link.
Website: https://qtum.org/Telegram Group: t.me/qtumofficialTwitter: https://twitter.com/QtumOfficialFacebook: https://www.facebook.com/QtumOfficial/
Qtum — Defining the Blockchain Economy
242 
242 claps
242 
Written by

Qtum — Defining the Blockchain Economy
Written by

Qtum — Defining the Blockchain Economy
"
https://medium.com/devops-world/aws-iam-7b8f1b10602b?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
www.101daysofdevops.com
Course Link:
www.101daysofdevops.com
YouTube link:
www.youtube.com
Identity and Access Management(IAM) is used to manage AWS
and it provide access/access-permissions to AWS resources(such as EC2,S3..)
If we notice at the right hand side at the top of console it says Global i.e creating a user/groups/roles will apply to all regions
To create a new user,Just click on Users on the left navbar
By default any new IAM account created with NO access to any AWS services(non-explicit deny)
Always follow the best practice and for daily work try to use a account with least privilege(i.e non root user)
IAM Policies: A policy is a document that formally states one or more permissions.For eg: IAM provides some pre-built policy templates to assign to users and groups
Default policy is explicitly deny which will override any explicitly allow policy
Let take a look at these policies
AdministratorAccess
We can create our own custom policy using policy generator or written from scratch
So Custom Policy where everything denies for EC2 resources
Below is the simulation I run where I created a test user who has only Amazon S3 read only access
Now let me run the simulation,as you can see it’s a nice way to test your policies
Everything you need to know about…
15 
15 claps
15 
Written by
AWS Community Builder, Ex-Redhat, Author, Blogger, YouTuber, RHCA, RHCDS, RHCE, Docker Certified,4XAWS, CCNA, MCP, Certified Jenkins, Terraform Certified, 1XGCP
Everything you need to know about Linux,Docker,AWS,Python,Continuous-Integration and Deployment,GIT,Puppet,Kubernetes,DataScience,Frontend-development
Written by
AWS Community Builder, Ex-Redhat, Author, Blogger, YouTuber, RHCA, RHCDS, RHCE, Docker Certified,4XAWS, CCNA, MCP, Certified Jenkins, Terraform Certified, 1XGCP
Everything you need to know about Linux,Docker,AWS,Python,Continuous-Integration and Deployment,GIT,Puppet,Kubernetes,DataScience,Frontend-development
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ashish_fagna/aws-services-every-developer-should-be-aware-of-f7c48aaa854f?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ashish
Feb 3, 2020·33 min read
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.
Amazon CloudSearch makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application.
Amazon Elasticsearch Service is a fully managed service that makes it easy for you to deploy, secure, and run Elasticsearch cost effectively at scale. The service provides support for open source Elasticsearch APIs, managed Kibana, integration with Logstash and other AWS services, and built-in alerting and SQL querying. Amazon Elasticsearch Service lets you pay only for what you use.
Amazon EMR is a big data platform for processing vast amounts of data quickly (Easily Run and Scale Apache Spark, Hadoop, HBase, Presto, Hive, and other Big Data Frameworks) and cost-effectively at scale. EMR gives analytical teams the engines and elasticity to run Petabyte-scale analysis for a fraction of the cost of traditional on-premises clusters.
Amazon Kinesis makes it easy to collect, process, analyze video and data real-time, streaming data so you can get timely insights and react quickly to new information.
Amazon MSK is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications.
Amazon Redshift is a fully managed petabyte-scale data warehouse service in the cloud.
Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization. QuickSight lets you easily create and publish interactive dashboards that include ML Insights. With our Pay-per-Session pricing, QuickSight allows you to give everyone access to the data they need, while only paying for what you use.
AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud.
AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premise data sources, at specified intervals.
AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console.
AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis.
AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications.
Amazon EventBridge is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. EventBridge delivers a stream of real-time data from event sources, such as Zendesk, Datadog, or Pagerduty, and routes that data to targets like AWS Lambda.
Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. Amazon MQ reduces your operational load by managing the provisioning, setup, and maintenance of ActiveMQ, a popular open-source message broker.
Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging.
Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work.
AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.
Sumerian allows you to easily create and run browser-based 3D, augmented reality (AR), and virtual reality (VR) applications.
AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.
AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.
The AWS Cost & Usage Report contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, and reservations (e.g., Amazon EC2 Reserved Instances (RIs)).
The Reserved Instance Utilization and Coverage reports are available out-of-the-box in AWS Cost Explorer. It allows users to manage and monitor their instance reservations.
Savings Plans is a flexible pricing model that provides savings of up to 72% on your AWS compute usage. This pricing model offers lower prices on Amazon EC2 instances usage, regardless of instance family, size, OS, tenancy or AWS Region, and also applies to AWS Fargate usage.
AWS Blockchain allows you to build scalable blockchain networks and ledger applications for your business.
Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log ‎owned by a central trusted authority. Amazon QLDB tracks each and every application data change and maintains a complete and verifiable history of changes over time.
Alexa for Business is a service that enables organizations and employees to use Alexa to get more work done. With Alexa for Business, employees can use Alexa as their intelligent assistant to be more productive in meeting rooms, at their desks, and even with the Alexa devices they already use at home or on the go.
Amazon Chime is a communications service that lets you meet, chat, and place business calls inside and outside your organisation, all using a single application.
Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device.
Amazon WorkMail is a secure, managed business email and calendar service with support for existing desktop and mobile email client applications. Amazon WorkMail gives users the ability to seamlessly access their email, contacts, and calendars using the client application of their choice.
Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.
Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to the conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.
Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.
Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service ECS is a highly secure, reliable, and scalable way to run containers.
Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. Customers such as Intel, Snap, Intuit, GoDaddy, and Autodesk trust EKS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.
Virtual servers, storage, databases, and networking for a low, predictable price. Lightsail is ideal for simpler workloads, quick deployments, and getting started on AWS. It’s designed to help you start small, and then scale as you grow.
Fully managed batch processing at any scale. AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted.
AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. One can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.
AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications.
AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. With Lambda, you can run code for virtually any type of application or backend service — all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability.
AWS Outposts is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience. AWS Outposts is ideal for workloads that require low latency access to on-premises systems, local data processing, or local data storage.
The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architecture in powerful new ways.
AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS developers can deploy their applications to Wavelength Zones,
VMware Cloud on AWS is an integrated cloud offering jointly developed by AWS and VMware delivering a highly scalable, secure, and innovative service that allows organizations to seamlessly migrate and extend their on-premises VMware vSphere-based environments to the AWS Cloud running on next-generation EC2 and other AWS infrastructure.
Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost. Over 10 years ago, Amazon’s retail business needed a contact center that would give our customers personal, dynamic, and natural experiences.
Amazon Pinpoint is a marketing and analytics service hosted on the Amazon Web Services (AWS) public cloud that enables an organization to engage with and track metrics related to its application end users. With Amazon Pinpoint, an AWS user can measure customer engagement and generate analytical data from its applications. A marketing team can then use this insight to strengthen campaigns.
Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails.
Contact Lens for Amazon Connect is a set of machine learning (ML) capabilities integrated into Amazon Connect. With Contact Lens for Amazon Connect, contact center supervisors can better understand the sentiment, trends, and compliance risks of customer conversations to effectively train agents, replicate successful interactions, and identify crucial company and product feedback.
Amazon Corretto is a no-cost, multiplatform, production-ready distribution of the Open Java Development Kit (OpenJDK). Corretto comes with long-term support that will include performance enhancements and security fixes.
An open-source software development framework for defining your cloud infrastructure in code and provisioning it through AWS CloudFormation.
AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal.
AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.
AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. CodeCommit eliminates the need to operate your own source control system or worry about scaling its infrastructure.
AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.
AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.
AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place.
The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.
AWS Device Farm is an application testing service that lets you improve the quality of your web and mobile apps by testing them across an extensive range of desktop browsers and real mobile devices; without having to provision and manage any testing infrastructure.
AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors.
Amazon AppStream 2.0 is a fully managed application streaming service. You centrally manage your desktop applications on AppStream 2.0 and securely deliver them to any computer. You can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure.
Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device.
Amazon WorkLink is a fully managed service that lets you provide your employees with secure, easy access to your internal corporate websites and web apps using their mobile phones.
Amazon WorkSpaces is a managed, secure Desktop-as-a-Service (DaaS) solution. You can use Amazon WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe.
Amazon GameLift is a fully managed service for deploying, operating, and scaling your session-based multiplayer game servers in the cloud.
Amazon Lumberyard is a cross-platform video game development engine that connects to the Amazon Web Services (AWS) public cloud. Amazon Lumberyard is based on technology from AWS, CryEngine game software, the Twitch social video game platform and the Double Helix game studio.
AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core can support billions of devices and trillions of messages, and can process and route those messages to AWS endpoints and to other devices reliably and securely.
Amazon FreeRTOS (a:FreeRTOS) is an open source operating system for microcontrollers that makes small, low-power edge devices easy to program, deploy, secure, connect, and manage.
AWS IoT Greengrass seamlessly extends AWS to edge devices so they can act locally on the data they generate, while still using the cloud for management, analytics, and durable storage.
AWS IoT 1-Click is a service that enables simple devices to trigger AWS Lambda functions that can execute an action. AWS IoT 1-Click supported devices enable you to easily perform actions such as notifying technical support, tracking assets, and replenishing goods or services.
AWS IoT Analytics is a fully-managed service that makes it easy to run and operationalize sophisticated analytics on massive volumes of IoT data without having to worry about the cost and complexity typically required to build an IoT analytics platform.
The AWS IoT Button is a programmable button based on the Amazon Dash Button hardware. This simple Wi-Fi device is easy to configure and designed for developers to get started with AWS IoT Core, AWS Lambda, Amazon DynamoDB, Amazon SNS, and many other Amazon Web Services without writing device-specific code.
AWS IoT Device Defender is a fully managed service that helps you secure your fleet of IoT devices. AWS IoT Device Defender continuously audits your IoT configurations to make sure that they aren’t deviating from security best practices. A configuration is a set of technical controls you set to help keep information secure when devices are communicating with each other and the cloud.
AWS IoT Device Management makes it easy to securely register, organize, monitor, and remotely manage IoT devices at scale. With AWS IoT Device Management, you can register your connected devices individually or in bulk, and easily manage permissions so that devices remain secure.
AWS IoT Events is a fully managed service that makes it easy to detect and respond to events from IoT sensors and applications. Events are patterns of data identifying more complicated circumstances than expected, such as changes in equipment when a belt is stuck or motion detectors using movement signals to activate lights and security cameras.
AWS IoT SiteWise is a managed service that makes it easy to collect, store, organize and monitor data from industrial equipment at scale. You can easily monitor equipment across your industrial facilities to identify waste, such as breakdown of equipment and processes, production inefficiencies, and defects in products.
AWS IoT Things Graph is a service that makes it easy to visually connect different devices and web services to build IoT applications.
AWS Partner Device Catalog is catalog where you can find devices and hardware to help you explore, build, and go to market with your IoT solutions. You can search for and find hardware that works with AWS, including development kits and embedded systems to build new devices, as well as off-the-shelf-devices such as gateways, edge servers, sensors, and cameras for immediate IoT project integration.
Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.
Amazon Augmented AI (Amazon A2I) makes it easy to build the workflows required for human review of ML predictions.
Amazon CodeGuru is a machine learning service for automated code reviews and application performance recommendations. It helps you find the most expensive lines of code that hurt application performance and keep you up all night troubleshooting, then gives you specific recommendations to fix or improve your code.
Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text. No machine learning experience required.
Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances or Amazon ECS tasks to reduce the cost of running deep learning inference by up to 75%. Amazon Elastic Inference supports TensorFlow, Apache MXNet, and ONNX models, with more frameworks coming soon.
Amazon Forecast uses machine learning to combine time series data with additional variables to build forecasts. Amazon Forecast requires no machine learning experience to get started. You only need to provide historical data, plus any additional data that you believe may impact your forecasts.
Amazon Fraud Detector is a fully managed service that makes it easy to identify potentially fraudulent online activities such as online payment fraud and the creation of fake accounts.
Amazon Kendra is a highly accurate and easy to use enterprise search service that’s powered by machine learning. Kendra delivers powerful natural language search capabilities to your websites and applications so your end users can more easily find the information they need within the vast amount of content spread across your company.
Amazon Lex is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions.
Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications. With Amazon Personalize, you provide an activity stream from your application — clicks, page views, signups, purchases, and so forth — as well as an inventory of the items you want to recommend, such as articles, products, videos, or music
Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly’s Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize speech that sounds like a human voice.
Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content. It also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.
Amazon SageMaker Ground Truth helps you build highly accurate training datasets for machine learning quickly. SageMaker Ground Truth offers easy access to labelers through Amazon Mechanical Turk and provides them with built-in workflows and interfaces for common labeling tasks. Y
Amazon Textract is a service that automatically extracts text and data from scanned documents. Amazon Textract goes beyond simple optical character recognition (OCR) to also identify the contents of fields in forms and information stored in tables.
Amazon Transcribe makes it easy for developers to add speech-to-text capability to their applications. Amazon Transcribe uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately. Amazon Transcribe can be used to transcribe customer service calls, to automate closed captioning and subtitling, and to generate metadata for media assets to create a fully searchable archive.
The AWS Deep Learning AMIs provide machine learning practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale. You can quickly launch Amazon EC2 instances pre-installed with popular deep learning frameworks and interfaces such as TensorFlow, PyTorch, Apache MXNet, Chainer, Gluon, Horovod, and Keras to train sophisticated, custom AI models, experiment with new algorithms, or to learn new skills and techniques.
AWS Deep Learning Containers (AWS DL Containers) are Docker images pre-installed with deep learning frameworks to make it easy to deploy custom machine learning (ML) environments quickly by letting you skip the complicated process of building and optimizing your environments from scratch. AWS DL Containers support TensorFlow, PyTorch, and Apache MXNet.
AWS DeepComposer is the world’s first musical keyboard powered by machine learning to enable developers of all skill levels to learn Generative AI while creating original music outputs. DeepComposer consists of a USB keyboard that connects to the developer’s computer, and the DeepComposer service, accessed through the AWS Management Console.
AWS DeepLens is a deep learning-enabled video camera. It is integrated with the Amazon Machine Learning ecosystem and can perform local inference against deployed models provisioned from the AWS Cloud.
AWS Inferentia is a high performance machine learning inference chip, custom designed by AWS: its purpose is to deliver cost effective, low latency predictions at scale. Inferentia is present in Amazon EC2 inf1 instances
Apache MXNet is a fast and scalable training and inference framework with an easy-to-use, concise API for machine learning. You can get started with MxNet on AWS with a fully-managed experience using Amazon SageMaker, a platform to build, train, and deploy machine learning models at scale.
TensorFlow™ enables developers to quickly and easily get started with deep learning in the cloud. The framework has broad support in the industry and has become a popular choice for deep learning research and application development, particularly in areas such as computer vision, natural language understanding and speech translation. You can get started on AWS with a fully-managed TensorFlow experience with Amazon SageMaker, a platform to build, train, and deploy machine learning models at scale.
Gluon Time Series (GluonTS), an Apache MXNet-based toolkit for time series analysis using the Gluon API. We are excited to give researchers and practitioners working with time series data access to this toolkit, which we have built for our own needs as applied scientists working on real-world industrial time series problems both at Amazon and on behalf of our customers.
Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.
AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes.
AWS Chatbot is an interactive agent that makes it easy to monitor and interact with your AWS resources in your Slack channels and Amazon Chime chat rooms.
AWS CloudFormation provides a common language for you to model and provision AWS and third party application resources in your cloud environment.
AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.
AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics.
AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.
AWS Control Tower provides the easiest way to set up and govern a new, secure, multi-account AWS environment based on best practices established through AWS’ experience working with thousands of enterprises as they move to the cloud.
The Console Mobile Application allows AWS customers to monitor resources through a dedicated dashboard and view configuration details, metrics, and alarms for select AWS services. The Dashboard provides permitted users with a single view a resource’s status, with real-time data on Amazon CloudWatch, Personal Health Dashboard, and AWS Billing and Cost Management.
AWS License Manager makes it easier to manage your software licenses from software vendors such as Microsoft, SAP, Oracle, and IBM across AWS and on-premises environments. AWS License Manager lets administrators create customized licensing rules that emulate the terms of their licensing agreements, and then enforces these rules when an instance of EC2 gets launched.
The AWS Management Console brings the unmatched breadth and depth of AWS right to your computer or mobile phone with a secure, easy-to-access, web-based portal.
AWS Managed Services (AMS) operates AWS on your behalf, providing a secure and compliant AWS Landing Zone, a proven enterprise operating model, on-going cost optimization, and day-to-day infrastructure management.
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.
AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts.
AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you. Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.
AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures.
AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources.
AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.
The AWS Well-Architected Tool helps you review the state of your workloads and compares them to the latest AWS architecture best practices.
Amazon Elastic Transcoder is media transcoding in the cloud. It is designed to be a highly scalable, easy to use and a cost effective way for developers and businesses to convert (or “transcode”) media files from their source format into versions that will playback on devices like smartphones, tablets and PCs.
Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices.
AWS Elemental MediaConnect is a high-quality transport service for live video. Today, broadcasters and content owners rely on satellite networks or fiber connections to send their high-value content into the cloud or to transmit it to partners for distribution.
AWS Elemental MediaConvert is a file-based video transcoding service with broadcast-grade features. It allows you to easily create video-on-demand (VOD) content for broadcast and multiscreen delivery at scale.
AWS Elemental MediaLive is a broadcast-grade live video processing service. It lets you create high-quality video streams for delivery to broadcast televisions and internet-connected multiscreen devices, like connected TVs, tablets, smart phones, and set-top boxes.
AWS Elemental MediaPackage reliably prepares and protects your video for delivery over the Internet. From a single video input, AWS Elemental MediaPackage creates video streams formatted to play on connected TVs, mobile phones, computers, tablets, and game consoles.
AWS Elemental MediaStore is an AWS storage service optimized for media. It gives you the performance, consistency, and low latency required to deliver live streaming video content. AWS Elemental MediaStore acts as the origin store in your video workflow.
AWS Elemental MediaTailor lets video providers insert individually targeted advertising into their video streams without sacrificing broadcast-level quality-of-service.
AWS Elemental Appliances and Software solutions bring advanced video processing and delivery technologies into your data center, co-location space, or on-premises facility.
AWS Migration Hub provides a single location to track the progress of application migrations across multiple AWS and partner solutions. Using Migration Hub allows you to choose the AWS and partner migration tools that best fit your needs, while providing visibility into the status of migrations across your portfolio of applications.
AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers. Planning data center migrations can involve thousands of workloads that are often deeply interdependent.
AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.
AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server.
AWS Server Migration Service (SMS) is an agentless service which makes it easier and faster for you to migrate thousands of on-premises workloads to AWS. AWS SMS allows you to automate, schedule, and track incremental replications of live server volumes, making it easier for you to coordinate large-scale server migrations.
AWS Transfer for SFTP is a fully managed service that enables the transfer of files directly into and out of Amazon S3 using the Secure File Transfer Protocol (SFTP) — also known as Secure Shell (SSH) File Transfer Protocol.
Cloud migration does not need to be a complex, time consuming, or costly endeavor. CloudEndure Migration simplifies, expedites, and reduces the cost of cloud migration by offering a highly automated lift-and-shift solution.
AWS Amplify is a development platform for building secure, scalable mobile and web applications. It makes it easy for you to authenticate users, securely store data and user metadata, authorize selective access to data, integrate machine learning, analyze application metrics, and execute server-side code.
Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door” for applications to access data, business logic, or functionality from your backend services.
Amazon Pinpoint is a marketing and analytics service hosted on the Amazon Web Services (AWS) public cloud that enables an organization to engage with and track metrics related to its application end users.
With Amazon Pinpoint, an AWS user can measure customer engagement and generate analytical data from its applications.
AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.
AWS Device Farm is an application testing service that lets you improve the quality of your web and mobile apps by testing them across an extensive range of desktop browsers and real mobile devices; without having to provision and manage any testing infrastructure.
Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define.
Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.
Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers an extremely reliable and cost effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1. Amazon Route 53 is fully compliant with IPv6 as well.
AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.
AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure.
AWS Cloud Map is a cloud resource discovery service. With Cloud Map, you can define custom names for your application resources, and it maintains the updated location of these dynamically changing resources.
AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS.
AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users.
AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway.
Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.
Amazon Braket is a fully managed service that helps you get started with quantum computing by providing a development environment to explore and design quantum algorithms, test them on simulated quantum computers, and run them on your choice of different quantum hardware technologies.
AWS RoboMaker enables a robot to stream data, navigate, communicate, comprehend, and learn. RoboMaker provides an IDE, simulation service, fleet management capabilities, and seamless integration with various Amazon and AWS services to empower customers to innovate and provide best-of-class robotic solutions.
AWS Ground Station is a fully managed service that lets you control satellite communications, process data, and scale your operations without having to worry about building or managing your own ground station infrastructure.
AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.
Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.
Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. Amazon Detective automatically collects log data from your AWS resources and uses machine learning to easily conduct faster and more efficient security investigations.
Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time consuming for security teams to continuously analyze event log data for potential threats.
Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.
Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.
AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.
AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys using FIPS 140–2 Level 3 validated HSMs.
AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud.
AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.
AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.
AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM.
AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.
AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners.
AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.
AWS Single Sign-On (SSO) makes it easy to centrally manage access to multiple AWS accounts and business applications and provide users with single sign-on access to all their assigned accounts and applications from one place. With AWS SSO, you can easily manage SSO access and user permissions to all of your accounts in AWS Organizations centrally.
AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.
Amazon Elastic Block Store (EBS) is an easy to use, high performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction intensive workloads at any scale.
Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.
Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA).
Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS.
Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.
AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services in the cloud as well as on premises using the AWS Storage Gateway. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems, and AWS Storage Gateway volumes.
The Snow family of services offers a number of physical devices and capacity points, including some with built-in compute capabilities. These services help physically transport up to Exabytes of data into and out of AWS. The Snow family of services are owned and managed by AWS and integrate with AWS security, monitoring, storage management and computing capabilities.
AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Customers use Storage Gateway to simplify storage management and reduce costs for key hybrid cloud storage use cases.
CloudEndure Disaster Recovery continuously replicates your machines (including operating system, system state configuration, databases, applications, and files) into a low-cost staging area in your target AWS account and preferred Region. In the case of a disaster, you can instruct CloudEndure Disaster Recovery to automatically launch thousands of your machines in their fully provisioned state in minutes.
Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases.
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It’s a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.
Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads.
Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores.
Amazon Managed Apache Cassandra Service is a scalable, highly available, and managed Apache Cassandra–compatible database service. With Amazon Managed Cassandra Service, you can run your Cassandra workloads on AWS using the same Cassandra application code and developer tools that you use today.
Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.
Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log ‎owned by a central trusted authority. Amazon QLDB tracks each and every application data change and maintains a complete and verifiable history of changes over time.
Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.
Amazon Relational Database Service (RDS) on VMware lets you deploy managed databases in on-premises VMware environments using the Amazon RDS technology enjoyed by hundreds of thousands of AWS customers. Amazon RDS provides cost-efficient and resizable capacity while automating time-consuming administration tasks including infrastructure provisioning, database setup, patching, and backups, freeing you to focus on your applications.
Amazon Redshift is an Internet hosting service and data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations.
Amazon Timestream is a fast, scalable, fully managed time series database service for IoT and operational applications that makes it easy to store and analyze trillions of events per day at 1/10th the cost of relational databases. Driven by the rise of IoT devices, IT systems, and smart industrial machines, time-series data — data that measures how things change over time — is one of the fastest growing data types.
AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases.
Software Developer. Machine Learning, Artificial Intelligence Learner.
5 
5 
5 
Software Developer. Machine Learning, Artificial Intelligence Learner.
"
https://blog.bluzelle.com/bluzelle-now-available-on-heroku-cd462dbc21dc?source=search_post---------288,"We’re thrilled to announce that we’ve reached another achievement with the integration of Bluzelle Beta into Salesforce owned Heroku. Heroku is a Platform-as-a-Service for developers. It makes it easy for developers to get their core infrastructure needs so that the developer can focus on just building their applications.
Heroku is one of the first cloud platforms to emerge — beginning development in 2007. It supports multiple languages, same ones Bluzelle does, such as Java, Node.js, Python. Seeing the future potential of Heroku, the company was bought by Salesforce in 2010 for $212 million. It’s now become one of Salesforce’s biggest areas of growth.
With it’s large suite of integrated services, which includes data storage, files storage, devops and much more, Heroku has become a go-to platform when a developer wants to get their product out fast. Now Heroku is not only used by developers, it also has specific tools for enterprises.
Our database is not just for blockchain projects but for all software developers. With all the data breaches happening (see what happened with Marriot and a 500M data breach), every application needs the security, integrity and reliability provided by decentralized solutions.
Now that we are available on Heroku, we’re able to reach software developers in an easier way. This opens up to more early users and getting valuable feedback on the product to keep improving it. On Heroku, developers are able to deploy Bluzelle’s database solution in their applications at the touch of a button.
With the upcoming release of Bluzelle Bernoulli, this was a great time to get into Heroku and have new developers use our product. In addition to Heroku, we’re also identifying other distribution platforms that go after developers. By being on these well recognized platforms, it gives further validation to what our team is building and what our community has supported.
Head over to the add-on page, check out the sample app we’ve designed to get a handle on how it works and start leveraging the magic of swarm distribution in your applications!
We love hearing from our users, so don’t hesitate to reach out with any feedback or queries through the usual channels.
Bluzelle is a decentralized storage network for the creator economy.
401 
1
401 claps
401 
1
Written by
CEO Bluzelle. WEF Tech Pioneer 2017. Futurist, Writer for Fast Company, Forbes, Coindesk. “The Revolution Will Not Be Televised.”
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
Written by
CEO Bluzelle. WEF Tech Pioneer 2017. Futurist, Writer for Fast Company, Forbes, Coindesk. “The Revolution Will Not Be Televised.”
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
"
https://medium.com/swlh/the-cloud-renaissance-of-microsoft-d06ee1b4e12f?source=search_post---------289,"There are currently no responses for this story.
Be the first to respond.
We’ve all seen the story one time too many of the tech companies that lose touch with reality. The companies that built success on the back of a vision but with that success came arrogance. Companies like Palm and AOL that created products and services that were met with critical acclaim, only to fall into irrelevance as the new upstart comes up and dethrones them with fresh ideas. It is with this track record of inevitable…
"
https://blog.clairvoyantsoft.com/common-aws-cost-saving-measures-63dd5f9d2057?source=search_post---------290,"Amazon Web Services (AWS) provides a variety of valuable services that many organizations can utilize to run their workloads. From Elastic Beanstalk (to run scalable Web Applications) to Elastic MapReduce (to run scalable Big Data workloads) to spinning up simple…
"
https://medium.com/dazn-tech/understanding-the-scaling-behaviour-of-dynamodb-ondemand-tables-126004269e70?source=search_post---------291,"There are currently no responses for this story.
Be the first to respond.
Update 15/03/2019: Thanks to Zac Charles who pointed me to this new page in the DynamoDB docs. It explains how the OnDemand capacity mode works. Turns out you DON’T need to pre-warm a table. You just need to create the table with the desired peak throughput (Provisioned), and then change it to OnDemand. After you change the table to OnDemand it’ll still have the same throughput.
To confirm this behaviour, I ran a few more experiments:
The result confirms the aforementioned behaviour.
Back when AWS announced DynamoDB AutoScaling in 2017, I took it for a spin and found a number of problems with how it works.
At re:invent 2018, AWS also announced DynamoDB OnDemand. DynamoDB OnDemand tables have different scaling behaviour, which promises to be far superior to DynamoDB AutoScaling.
Talking with a few of my friends at AWS, the consensus is that “AWS customers no longer need to worry about scaling DynamoDB”! So I ran a number of experiments to help me understand how OnDemand tables deal with sudden spikes in traffic.
Let’s assume that a newly created table can handle X amount of read/write throughput before it needs to scale. We can find out the value of X by:
To help me quickly find the threshold, I used Step Functions to test different values of Y (i.e. peak throughput): 100, 200, 300, 500, 700, 900, 1100, 1300, 1500, 1750, 2000, 2500, 3000, 5000, 7000, and 9000.
For every value of Y up to 3000, there were no throttled writes. But I was not able to perform more than 4000 writes/s.
The conclusion is that, a newly created table can handle 4000 writes/s without any throttling.
From the announcement post by Danilo, one paragraph piqued my curiosity in particular.
Is the adaptive behaviour similar to DynamoDB AutoScaling and calculate the next threshold based on the previous peak Consumed Capacity? I explained the problem with this approach in my previous post – the threshold should be based on the throughput you wanted to execute (consumed + throttled), not just what you succeeded with (consumed).
With this question in mind, I reran the experiment for peak traffic of 9000 writes/s. As you can see from the diagrams below, the table’s threshold has been raised to around 7000 writes/s.
The good news is that, the table scaled to support throughput higher than the previous peak Consumed Capacity. The bad news is that it didn’t adapt to our desired throughput of 9000 writes/s so we still experienced some throttling.
I ran another experiment to see how quickly a new table can scale to 40,000 writes/s and the steps it took.
For this experiment, I had to use a different Step Functions state machine. Using DynamoDB BatchWrites, I was able to achieve around 10,000 writes/s from a 1GB Lambda function. So to generate 40,000 writes/s I needed to run multiple instance of this function in parallel.
The new state machine looks like this.
As you can see from the result, it took roughly one hour to scale the table to meet our throughput demand of 40,000 writes/s.
Afterwards, we’ll be able to achieve 40,000 writes/s against this table without any problems.
This behaviour opens up an interesting possibility.
Most DynamoDB table would never experience a spike of more than 4000 writes/s. For those that do, you can also “pre-warm” the table to your target peak throughput. After pre-warming the table, it would be able to handle your desired peak throughput.
What if the traffic ramps up to 40,000 writes/s steadily instead? I ran a series of experiments to find out.
In this case, the traffic increases steady over the course of an hour until it reached 40,000 writes/s. As you can see from the results below, the rate of progression is too fast for the OnDemand table to adapt to on the fly. As a result we incurred a lot of throttled write events along the way.
Rerunning the experiment and slowing up the rate of progression shows improved results. But we still experienced a lot of throttling.
As we slowed down the rate the throughput goes up, we experienced less and less throttling along the way. One would assume that there is a rate of progression, Z, where the throughput increases slowly enough that there will be no throttling.
A lot have been made about the fact that OnDemand tables are more expensive per request. A rough estimate suggests it’s around 5–6 times more expensive per request compared to a provisioned table.
While that is the case, my experience with DynamoDB is that most tables are poorly utilized. A quick look at our DynamoDB tables in production paints a familiar picture. Despite the occasional spikes, tables are using less than 10% of the provisioned throughput on average.
And it’s not hard to find even worse offenders.
The bottom line is, we are terrible at managing DynamoDB throughputs! There are so much waste in reserved throughputs that, even with a hefty premium per request for OnDemand tables we will still end up with a huge saving overall.
As a side, all these experiments cost us a total of $271.61 for over 217 million write requests.
In the grand scheme of things, this is insignificant. The cost of the engineering time (my time!) to conduct, analyze and summarise the results of these experiments is far greater.
But even beyond that, the value of lessons we learnt is far greater. We now have the confidence to use OnDemand tables for anything that has a peak throughput of less than 4000 per second. Which means:
The productivity saving this simple experiment creates for us makes the AWS cost look like peanuts! The moral of the story? Be curious, ask questions, experiment, and share your learnings with the community.
In summary:
Originally published at theburningmonk.com on March 9, 2019.
Revolutionising the sport industry
94 
94 claps
94 
Written by
AWS Serverless Hero. Independent Consultant https://theburningmonk.com/hire-me. Author of https://productionreadyserverless.com. Speaker. Trainer. Blogger.
Revolutionising the sport industry
Written by
AWS Serverless Hero. Independent Consultant https://theburningmonk.com/hire-me. Author of https://productionreadyserverless.com. Speaker. Trainer. Blogger.
Revolutionising the sport industry
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@womenwhocode/a-cloud-guru-partners-with-wwcode-to-advance-more-women-into-cloud-computing-e089ffa919dc?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Women Who Code
Dec 13, 2017·2 min read
The Disruptive Cloud Computing Education Tech Startup to Provide Training Resources and Scholarships to Members of World’s Largest Women in Tech Nonprofit
San Francisco — October 3rd, 2017 — A Cloud Guru and Women Who Code (WWCode) announce a partnership to provide more women access to Amazon Web Services (AWS) education and certification training, which offers pathways into high-demand careers in cloud computing.
The partnership supports women in technology by providing opportunities for participation in A Cloud Guru events and scholarships for online training. Over $25,000 in scholarships were awarded to WWCode to attend A Cloud Guru’s premier cloud computing conference, ServerlessConf, being held October 8 -11 in NYC. As an official WWCode training partner, A Cloud Guru is also providing members full and partial educational scholarships to their leading online cloud computing education and AWS certification training.
Drew Firment, Managing Partner at A Cloud Guru, spoke about the partnership, “The technology industry needs more women leaders of tomorrow. With so many cloud computing job openings, the time is now to level the playing field in tech. By empowering others to achieve their career goals, WWCode is having a dramatic impact on the effort to improve diversity and inclusion.”
Women Who Code’s Vice President of Partnerships and Growth, Jennifer Tacheff said, “The generosity and support of our sponsors and partners is vital to the work that Women Who Code is doing. It allows us to make opportunities available to our members, provide services that they might not otherwise have access to, and continue to celebrate and inspire them to change the world.”
About A Cloud Guru
A Cloud Guru delivers cloud computing training to over 250,000 students across 160 countries and 6 continents. Our platform was developed by experts in building scalable cloud solutions that don’t rely on servers or traditional infrastructure. To learn more about A Cloud Guru, visit https://acloud.guru or like us on Facebook: https://www.facebook.com/acloudguru or follow us on Twitter @acloudguru.
MEDIA: Danielle Bechtel — danielle@acloud.guru | 804.263.3852 | https://acloud.guru
Originally published at www.womenwhocode.com.
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
239 
239 
239 
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
"
https://medium.com/@jaychapel/5-free-google-cloud-training-resources-9410766667bc?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 17, 2019·4 min read
If you’re looking to break into the cloud computing space, there’s an abundance of resources out there, including free Google Cloud training. If you know where to look, open source learning is a great way to get familiar with different cloud service providers. Combined with our previous blog posts on free training resources for AWS and Azure, you’ll be well on your way to expanding your cloud expertise and finding your own niche. No matter where you are in the learning process, there’s a training resource for every experience level and learning type — get started now with our list of 5 free Google Cloud training resources:
For free, hands-on training there’s no better place to start than with Google Cloud Platform itself. GCP’s free tier option is a no-brainer thanks to its offerings:
And for help with navigating the platform as you use it, check out GCP’s documentation for a full overview, comparisons, tutorials, and more.
On the Google Cloud training page, you’ll find plenty of classes to get technical skills and learn best practices for using the platform. Among those options, they have also teamed up with Coursera, an online learning platform founded by Stanford professors, to offer courses online so you can “skill up from anywhere.”
Coursera includes a number of free courses, and until 1/1/19, you can sign up and get your first month free on any select Google Cloud Specialization. Courses include topics in Machine Learning, Architecting, Data Engineering, Developing Applications, and the list goes on.
In conjunction with Coursera, Google Cloud offers hands-on training with specialized labs available via Qwiklabs, a learning lab environment for developers. Choose a “quest” from their catalog to get started with 50+ hands-on labs from beginner to expert level, where you’ll learn new skills in a GCP environment and earn cloud badges along the way. Get started with GCP Essentials and work your way into more advanced, niche topics like Managing Cloud Infrastructure with Terraform, Machine Learning APIs, IoT in Google Cloud, and so on.
You can’t go wrong with YouTube. An endless amount of free videos offers an abundance of Google Cloud training for those of you who prefer to watch the movie instead of reading the book (you know who you are). Some of the most popular YouTube channels for free Google Cloud training include:
While other resources keep you learning with hands-on training, tutorials, and certification prep, blogs keep your mind flowing with new insights, ideas, and the latest on all things cloud computing. Google Cloud and Qwiklab have blogs of their own, perfect for supplemented reading with their trainings. But for a more well-rounded blog with content on other service providers, check out Cloud Academy. We also cover Google Cloud on the ParkMyCloud blog — check out this guide to Google Cloud machine types, an explanation of sustained use discounts, and introduction to resource-based pricing. And be sure to subscribe to relevant discussion forums such as r/googlecloud on Reddit and the GCP Slack.
As it becomes clear that cloud computing is here to stay, free training resources only continue to emerge. We picked the 5 above for their reliability, variety, quality, and range of information. Whether you’re new to Google Cloud or consider yourself an expert, these resources will expand your knowledge and keep you up to date with what’s latest in the platform.
More Free Training Resources:
Originally published at www.parkmycloud.com on October 23, 2018.
CEO of ParkMyCloud
32 
32 
32 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/google-cloud-platform-vs-aws-is-the-answer-obvious-maybe-not-c85623f7d86e?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 12, 2019·7 min read
Google Cloud Platform vs AWS: what’s the deal? A while back, we also asked the same question about Azure vs AWS. After the release of the latest earnings reports a few weeks ago from AWS, Azure, and GCP, it’s clear that Microsoft is continuing to see growth, Amazon is maintaining a steady lead, and Google is stepping in. Now that Google Cloud Platform has solidly secured a spot among the “big three” cloud providers, we think it’s time to take a closer look and see how the underdog matches up to the rest of the competition.
As they’ve been known to do, Amazon, Google, and Microsoft all released their recent quarterly earnings around the same time the same day. At first glance, the headlines tell it all:
The obvious conclusion is that AWS continues to dominate in the cloud war. With all major cloud providers reporting earnings around the same time, we have an ideal opportunity to examine the numbers and determine if there’s more to the story. Here’s what the quarterly earning reports tell us:
You can see here that while Google is the smallest out of the “big three” providers, they have shown the most growth — from Q1 2018 to Q1 2019, Google Cloud has seen growth of 83%. While they still have a ways to go before surpassing AWS and Microsoft, they are moving quickly in the right direction as Canalys reported they were the fasted growing cloud-infrastructure vendor in the last year.
It’s also important to note that Google is just getting started. Also making headlines was an increase in new hires, adding 6,450 in the last quarter, and most of them going to positions in their cloud sector. Google’s headcount now stands at over 114,000 employees in total.
The Obvious: Google is not surpassing AWS
When it comes to Google Cloud Platform vs AWS, we have a clear winner. Amazon continues to have the advantage as the biggest and most successful cloud provider in the market. While AWS is growing at a smaller rate now than both Google Cloud and Azure, Amazon still holds the largest market share of all three. AWS is the clear competitor to beat as they are the first and most successful cloud provider to date, with the widest range of services, and a strong familiarity among developers.
The Less Obvious: Google is actually gaining more ground
While it’s easy to write off Google Cloud Platform, AWS is not untouchable. AWS has already solidified itself in the cloud market, but with the new features and partnerships, Google Cloud is proving to be a force to be reckoned with.
We know that AWS is at the forefront of cloud providers today, but that doesn’t mean Google Cloud is very far behind. AWS is now just one of the three major cloud providers — with two more (IBM and Alibaba) gaining more popularity as well. Google Cloud Platform has more in store for its cloud business in 2020.
A big step for google was announced earlier this year at Google Cloud’s conference — Google Cloud Next — the CEO of Google Cloud announced that they would be coming out with a retail platform to directly compete with Amazon, called Google Cloud for Retail. What ‘s different about their product? For starters, they are partnering with companies such as Kohl’s, Target, Bed Bath & Beyond, Shopify, etc. — these retailers are known for being direct competition with Amazon. In addition to that, this will be the first time that Google Cloud has had an AI product that is designed to address a business process for a specific vertical. Google doesn’t appear to be stopping at just retail — Thomas Kurian said they are planning to build capabilities to assist companies in specialized industries, ex: healthcare, manufacturing, media, and more.
Google’s stock continues to rise. With nearly 6,450 new hires added to the headcount, a vast majority of them being cloud-related jobs, it’s clear that Google is serious about expanding its role in the cloud market. In April of this year, Google reported that 103,459 now work there. Google CFO Ruth Porat said, “Cloud has continued to be the primary driver of headcount.”
Google Cloud’s new CEO, Thomas Kurian, understands that Google is lagging behind the other two cloud giants, and plans to close that gap in the next two years by growing sales headcount.
Deals have been made with major retailer Kohl’s department store, and payments processor giant, PayPal. Google CEO Sundar Pichai lists the cloud platform as one of the top three priorities for the company, confirming that they will continue expanding their cloud sales headcount.
In the past few months, Pichai added his thoughts on why he believes the Google Cloud Platform is on a set path for strong growth. He credits their success to customer confidence in Google’s impressive technology and a leader in machine learning, naming the company’s open-source software TensorFlow as a prime example. Another key component to growth is strategic partnerships, such as the deal with Cisco that is driving co-innovation in the cloud with both products benefiting from each other’s features, as well as teaming up with VMware and Pivotal.
Driving Google’s growth is also the fact that the cloud market itself is growing so rapidly. The move to the cloud has prompted large enterprises to use multiple cloud providers in building their applications. Companies such as Home Depot Inc. and Target Corp. rely on different cloud vendors to manage their multi-cloud environments.
Home Depot, in particular, uses both Azure and Google Cloud Platform, and a spokesman for the home improvement retailer explains why that was intentional: “Our philosophy here is to be cloud-agnostic, as much as we can.” this philosophy goes to show that as long as there is more than one major cloud provider in the mix, enterprises will continue trying, comparing, and adopting more than one cloud at a time — making way for Google Cloud to gain more ground.
Multi-cloud environments have become increasingly popular due because companies enjoy the advantage of the cloud’s global reach, scalability, and flexibility. Google Cloud has been the most avid supporter of multi-cloud out of the three major providers. Earlier this year at Google Cloud Next, they announced the launch of Anthos, a new managed service offering for hybrid and multi-cloud environments to give enterprises operational consistency. They do this by running quickly on any existing hardware, leverage open APIs and give developers the freedom to modernize. There’s also Google Cloud Composer, which is a fully managed workflow orchestration service built on Apache Airflow that allows users to monitor, schedule and manage workflows across hybrid and multi-cloud environments.
Google Cloud Platform vs AWS is only one of the battles to consider in the ongoing cloud war. The truth is, market performance is only one factor in choosing the best cloud provider. As we always say, the specific needs of your business are what will ultimately drive your decision.
What we do know: the public cloud market is not just growing — it’s booming. Referring back to our Azure vs AWS comparison — the basic questions still remain the same when it comes to choosing the best cloud provider:
Right now AWS is certainly in the lead among major cloud providers, but for how long? We will continue to track and compare cloud providers as earnings are reported, offers are increased, and price options grow and change. To be continued in 2020…
Originally published at www.parkmycloud.com on December 5, 2019.
CEO of ParkMyCloud
21 
1
21 
21 
1
CEO of ParkMyCloud
"
https://medium.com/linode-cube/choose-your-rdbms-flavor-mysql-or-a-better-clone-48dacb52a081?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
A relational database management system, otherwise known as RDBMS, is a huge part of what happens in the cloud. This data management system is based on the relational model. It is one of the primary database models in widespread use today.
Relational databases in general are structured to recognize relations among stored items of information. They are easy to extend (i.e., add a new data category) without having to modify all existing applications. Each relational database is structured around a set of tables containing data crammed into predefined categories.
This structure is essential for relational databases. Each table is an element of a relation to some other data. It holds one or more data categories in columns. Each row uses an unique data instance for the categories defined by the columns.
Perhaps the most highly recognizable relational database product is Oracle’s MySQL. It offers one of the most comprehensive and advanced feature sets found in data management tools. It provides users with some of the highest levels of scalability, security, reliability, and uptime.
But despite MySQL’s popularity as the RDBMS frontrunner, it is not the only player in the relational cloud database game. Several impressive alternatives exist, such as Postgre SQL, SQLite, and MariaDB.
Of course, no single database product is the perfect option for every business need. This overview highlights the differences, so you can better decide which product is your ideal choice.
A popular open-source RDBMS, PostreSQL is more commonly known as Postgres. This database is designed by engineers for engineers. That suggests its hardcore reputation for reliability and accuracy.
PostgreSQL is ACID-compliant (Atomicity, Consistency, Isolation, Durability) and transactional. This means database transactions guarantee validity even in the event of errors or power failures.
This free, open-source community-driven software is overseen by the PostgreSQL Global Development Group headed by Bruce Momjian. The PostgreSQL community is largely regarded as world’s largest independent open-source development community.
Postgres emphasizes extensibility and standards compliance. It has solid reliability in securely storing data and returning it in response to requests from other software applications. It handles workloads ranging from small, single-machine applications to large, data-warehousing operations with many concurrent users.
Linux distributions typically have PostgreSQL available in supplied packages. The Mac OS X has PostgreSQL server as its standard default database in the server edition and includes PostgreSQL client tools in the Mac desktop edition.
PostgreSQL has numerous top-drawer features that make it a stand-out alternative to MySQL. These include both updatable and materialized views, triggers, and foreign keys. It also supports functions and stored procedures, and provides several expandability options.
I spoke with Momjian to discuss an earlier release and the potential for its Global Development Group evolving into a business or commercial entity for PostgreSQL. He recognizes Postgres’s real power and contribution as an influential software community is in remaining open source. That stature results in encouraging anyone to contribute patches, make commercial products based on it, and offer consulting for training on using it.
That preference for nonbusiness activity makes Postgres a serious alternative to the likes of Oracle and Microsoft. It is unlikely that the Postgres community will ever be waylaid by conflicts between decisions based on sales targets and those based on product dependability.
SQLite is a lightweight RDBMS. It does not require its own process, its own clustering, or its own user management the way other RDBMS do.
Let’s clear up three potentially confusing factors that separate SQLite from other databases in this category.
First, SQL is a query language that drives relational databases. Building on that, SQLite is an embeddable relational database management system.
Second, unlike other relational databases, SQLite does not support stored procedures.
Third, SQLite is file-based, unlike other databases which are server-based.
What makes SQLite lightweight while retaining its relational power is its use of a compact in-process library. For example, even with all its features enabled, the library size can be less than 500 kibibyte (KiB). If optional features are omitted, the size of the SQLite library can drop below 300KiB.
This library implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain. That makes the database free to use for any purpose, whether commercial or private.
You also can configure SQLite to run in a minimal stack space (think 4KiB) and very little heap, as in 100KiB. This makes SQLite a popular database engine choice on memory-constrained gear. While more memory is better than less, SQLite does not suffer degraded performance in low-memory environments.
Another key factor for selecting SQLite is the full-time attention to maintaining the database despite its free, nonmoney-making community structure. An international team of developers works on SQLite’s code base full time.
Database longevity counts big time here. The developers keep expanding SQLite’s capabilities and enhancing its reliability and performance. But they also keep maintaining backwards compatibility with the published interface spec, SQL syntax, and database file format.
Free support for SQLite is available from public support forums. But you have two other options that are not free.
One is purchasing an annual maintenance subscription for $1,500 annually that entitles you to private, expert advice from the developers via email. Or two, you can obtain more direct and time-sensitive technical support via high-priority email and phone directly from the SQLite’s developers. You can also add guaranteed response time for an additional cost. These enhanced support packages range from $8,000 to $35,000 per year.
MariaDB is a fully functional, open-source transactional database solution for modern application development and enterprise use cases. It offers a comprehensive package of technology and services, including feature-rich releases of MariaDB Server and MariaDB MaxScale. The combination of features does a good job of closing the gap between open-source functionality and proprietary offerings.
The original developers of MySQL created MariaDB as a fork of the MySQL database after Oracle acquired MySQL in 2008. This is a very significant development path if you are concerned about compatibility and functionality. It also gives you the confidence to use MariaDB as a drop-in, fully backward-compatible replacement for MySQL.
In fact, MariaDB has gone down a more progressive development path. It offers more DB engines than MySQL itself. For example, these standard engines are included: MyISAM, BLACKHOLE, CSV, MEMORY, ARCHIVE, and MERGE storage engines.
Plus, MariaDB includes 11 additional DB engines. These include: ColumnStore, MyRocks, Aria, FederatedX, SphinxSE, TokuDB, CONNECT, SEQUENCE, Spider, and Cassandra.
MariaDB also has a hefty catalog of optimizer enhancements compared to MySQL. For my money, MariaDB has far more features and enhancements than the leading contenders.
MariaDB’s backward compatible is one of its strongest selling points as a binary drop-in replacement for MySQL. For instance, data and table definition files (.frm) are binary compatible. All filenames, binaries, paths, ports, and sockets are the same. All client APIs and protocols are identical.
Two more salient features are 1. all MySQL connectors work as-is with MariaDB, and 2. the MySQL-client package works with the MariaDB server.
Typically, open-source relational databases share many common features and tools. In looking for your ideal RDBMS solution, you might find that a decision comes down to a scant difference or two that make one RDBMS better meet your needs.
Think of the choice as evaluating between a commercial “brand name” software option or a generic, “new and improved” platform. No doubt, cost can especially be a determining factor.
An open-source relational database might give you a business edge. A free database can be better than proprietary options. You will no doubt spend less in support costs adopting PostgreSQL, SQLite or MariaDB than if you buy a commercial product.
Get familiar with all three. Apply due diligence and requisite deliberation, then pick your favorite RDBMS flavor.
Please feel free to share below any comments, questions or insights about your experience with relational database management systems. And if you found this blog useful, consider sharing it through social media.
About the blogger: Jack M. Germain is a veteran IT journalist whose outstanding IT work can be found regularly in ECT New Network’s LinuxInsider, and other outlets like TechNewsDirectory. Jack’s reporting has spanned four decades and his breadth of It experience is unmatched. And while his views and reports are solely his and don’t necessarily reflect those of Linode, we are grateful for his contributions. He can be followed on Google+.
We’re covering everything from tech news and industry…
58 
1
58 claps
58 
1
We’re covering everything from tech news and industry happenings to event recaps and general tips.
Written by
Cloud Hosting for You. Sign up today and take control of your own server! Contact us via ticket or email for all support inquiries: https://www.linode.com/contact
We’re covering everything from tech news and industry happenings to event recaps and general tips.
"
https://medium.com/voximplant/nextgen-cpaas-whats-next-big-things-f9aafe23a4fa?source=search_post---------296,"There are currently no responses for this story.
Be the first to respond.
Alexey Aylarov at APIdays Amsterdam
So first of all we can all meet that CPaaS was rather successful thing so far. That market is growing. Companies trying to find their competitive advantages. And we can actually see that a CPaaS in general was rather successful because of a number of factors.
First of all, the high point of CPaaS is largely due to the flourishing of the “new enterprise.” When companies like Uber and Lyft became unicorns, everyone suddenly realized that startups could go to the next level by using cloud communication platforms. When the market began to understand this, demand for CPaaS grew, as cloud solutions enable you to use “box solutions” very quickly to turn a profit.
Second, it should be remembered that CPaaS platforms have always been targeted to developers. And every modern startup always has developers available whom can easily use CPaaS.
Third, clouds are clouds, which means service availability worldwide, scalability, and increased capacity on demand, all without headaches.
And finally, most platforms offer the principle of pay-as-you-go, when you have to pay only for what you use.
The first thing to mention here is Serverless, which has taken the convenience of CPaaS to a new level. Serverless doesn’t mean the absence of servers entirely, but their absence on the client side. In terms of computing resources, this is the same as pay-as-you-go, because the fees are charged according to the load on the computing provider. Another benefit of serverless is that customers have direct access to the platform’s runtime, which leads to less delays and increased reliability.
Another trend are visual flow editors. This is one of the steps towards the business audience, which (most often) cannot code, but can collect the logic of the bot/call center in a visual editor. Approaches to implementation vary slightly (see Smartcalls from Voximplant, Studio from Twilio, FlowBuilder from MessageBird, etc.), but the essence is similar — the customer uses visual blocks instead of code varying their location and connections between them. Some of these editors still allow you to use the code as an advanced feature, for example, our Smartcalls, but the story is a bit different.
Finally, there is the cloud IDE. Of course, this can hardly be compared to a IDEA for now, but easily to the VS Code. If CPaaS gives a developer a powerful tool to work with the code, this developer will most likely be very satisfied. The cloud IDE with a convenient debugger, smart autocomplete, code highlighting, custom styles, tabs, etc. working quickly within the web-interface gets platform extra bonus points for its flexibility.
But the story wouldn’t be complete…
…if it weren’t for AI. Machine learning gives new degrees of freedom to communication platforms, namely:
Speech recognition and synthesis can be developed independently but it is very time-consuming. You can turn to big players like Google or Amazon — their models already recognize human speech very well, as well as imitate it (nod to WaveNet).
Natural Language Understanding/Processing is now the hottest topic in the world of communications. If a business decision is based on NLU, then, as a variant, there is a synthesis of speech: a person answers a call, his/her speech is transliterated, this text is given back to the robot and the robot, in order to react, selects the text of the answer, which again must be synthesized. Automation of this process is available with Google Dialogflow, IBM Watson, and Amazon Lex, just to name a few.
When a call center operator communicates with a customer, it is possible to analyze speech in the background and provide the operator with additional information so that he/she does not waste his/her time. For example, a customer may ask where the nearest ATM is — the system will recognize the question and display the answer on the operator’s screen so the operator can simply read the answer instead of asking the customer to wait.
Almost everyone is interested in this, but it is the most difficult area of CPaaS at the moment because speech has several cultural differences and there are multiple ways to say one thing. Nowadays, many companies analyze emotions using text. There are some solutions, but they cannot be said to be successful, because the analysis of only the text will not go far; obviously, emotions are not only what is said, but how. Therefore, a convincing analysis of emotions in real time is a question for the (near) future.
Everybody knows about noise reduction — when you talk on the phone, a trained model “removes” background noise so that the other person can only hear you. Sometimes, the speaker’s voice suffers because the models are not always able to distinguish between the frequencies related to the background and those related to the voice but on the whole, it works fairly well. Modern smartphones blur the background with the help of AI. This approach, within the framework of video calls, will also be in demand — imagine not having to look for the perfect background, because AI can wash away any environment. Though why “imagine” — Skype already has such functionality.
Analyzing the video stream or videos helps you understand what’s in the frame. This is a very resource-intensive task so it is best handled by those who have a lot of computing power — Google, Microsoft, and other major players.
This includes more than just data classification and segmentation. Imagine if you had tens of thousands of call records that needed to be translated into text and then searched. It is much more effective if AI goes through these records and distributes them into groups (such as sales calls and warranty calls), which can then reveal whether or not the operator followed standard customer service protocols (i.e. how the person behaved, what were the emotions, etc.). You can extract as much information as you like from such a data set with the help of machine learning.
It’s a special case but it’s also a good example: we implemented the definition of an answering machine in our platform. Now, the platform is able to recognize answering machines in Russian. We have trained the model on many calls and it is able to distinguish between a live person and a recorded message. Conventional methods of detection are not very effective (for example, by audio signal), but AI has helped us to achieve accuracy up to 99%, while recognition takes only two seconds.
Machine learning requires a lot of resources, not just computational power, but also people with special skills — data scientists, for example, who create and configure training models, and know what data is needed and when. Such people are not easy to find and their work is expensive. They are in great demand among major players, and competing with Google to recruit is hard but still possible. Therefore, instead of competing with the giants, it is better to collaborate with them. Most CPaaS-players piggy back on the achievements of large companies. On one hand, this means that the larger partner manages the costs of other players and can set or change the rates for recognition and synthesis of speech (remember WaveNet from Google). On the other hand, you use the solutions of the larger player and they suddenly decide to change the rates, then you have to do the same thing which may not please your users. There’s also the additional concern of sending data to this giant which, for some businesses, poses a problem of privacy and security protocols. It is always possible, however, not to depend only on one partner, but use solutions of several with similar functionality. Finally, such cooperation is convenient and beneficial for CPaaS-players.
New technologies on the horizon that will affect communications in the same way that WebRTC did in its day are 5G and AV1.
5G is designed to bring to life the principle of “always online” — this is the ultimate goal, but this will not happen overnight. With the advent of this technology, CPaaS will have even more opportunities, because even those who have not used mobile data transmission before will start to do so in the IoT. The communication infrastructure will change, and with it, the usual telecommunications businesses as well.
AV1 video codec will also be useful for CPaaS, as it is free of charge, so you will not have to worry about licenses. The free codec, which is more efficient than H.265 and available to everyone, will also change the world of communications.
The future is unfolding before our eyes, and Voximplant is not just watching what is going on, but also participating in the process.
Stories about telecom industry for web and mobile…
357 
357 claps
357 
Stories about telecom industry for web and mobile developers.
Written by
CEO/Co-founder VoxImplant/Zingaya
Stories about telecom industry for web and mobile developers.
"
https://medium.com/manifoldco/announcing-manifold-2-0-c574ada28320?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
Today we’re excited to introduce you to Manifold 2.0.
3x the services, an all-new discovery experience, dashboard refresh and a streamlined developer experience make Manifold the easiest way to build cloud native applications.
You can explore all the new features here, or read-on for an accelerated introduction.
The ecosystem of managed developer tools and services is growing, fast — Gartner predicts global IaaS revenue to grow by 35% this year alone!
With more than 3x the services on Manifold to choose from, there’s always something new and exciting to try. Maybe it will inspire you to introduce an entirely new feature, save you time, make your application more performant or reliable— the challenge is keeping up.
That’s why we’re working to revolutionize how you discover, explore and experiment with services.
It starts with a brand new discovery experience on Manifold’s homepage. There you’ll find services bundled into collections and supported by patterns, tutorials and stories because your search doesn’t end when you find a new service. You need to understand what the service does, how it works and how to integrate it.
The Manifold dashboard has been completely redesigned and rebuilt for speed. With the addition of Manifold Teams, we’ve added a new sidebar to help you organize your projects and change contexts with ease.
Start building your application with Manifold and save time integrating and deploying cloud native applications.
We're determined to make it easy for developers to use the…
344 
344 claps
344 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-vs-alibaba-cloud-pricing-a-comparison-of-compute-options-c626d83487cc?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 9, 2018·5 min read
When discussing cloud providers and pricing, it’s common to compare within the ‘big three’ (AWS, Azure, Google Cloud), but what about Alibaba cloud pricing? After all, they’ve been regarded as the #4 cloud provider (at least in terms of revenue) and they’re rapidly growing in the cloud space. They’ve already made a name for themselves in China, and as they continue growing, the time is ripe for a cloud cost comparison.
Let’s explore Alibaba cloud pricing for its Elastic Compute Service (ECS) in comparison to a more familiar service offering — AWS EC2.
When it comes to cloud service pricing comparisons, a quick glance won’t do it. There are different services, pricing models, and other offerings mixed in between each provider, making it hard to do a casual comparison.The same can be said with Alibaba, as finding pricing alone gets complicated, and can be found in different places and at different rates.
Search for Alibaba pricing here, which we will call the “Alibaba Cloud” site, and you’ll get one rate. But if you go ahead and sign up for an account, and you’re ready to buy an instance, you might end up here or here, which we can call the “Aliyun” site — the original name of Alibaba’s cloud arm.
Confusing? yes. So here’s what both sites tell us:
And that’s the extent of their similarities. If you want actual pricing, instance availability and types, you need to sign up for an Alibaba account to start some deep digging. If you already know that you prefer PAYG, the Alibaba public site lists PAYG pricing for all available instance types (although it doesn’t seem consistent with that you’ll find in the actual purchasing console).
Instance types on the Alibaba Cloud site are broken down between “Entry Level” and “Enterprise,” with various options under each category.
Entry Level Instances
However, if you log into the Alibaba management console and get to the Aliyun side of the site, entry level and enterprise is not a thing. Instead, you’re given the top-tier options of “Basic Purchase” vs “Advanced Purchase”.
Basic Purchase
Here’s a price comparison for Alibaba vs AWS “t” instance prices in various regions. The AWS prices show the hourly PAYG pricing, multiplied by an average 730 hour month.
AWS prices are higher, but the instances are PAYG, meaning you can stop them when not in use. This strategy is common for t2 instances used in a dev-test environment and could save you over 73%. To compete with comparatively lower Alibaba prices, those are the kinds of savings you need.
From the “Advanced” part of the Aliyun purchasing site, there’s a lot more options, including PAYG instances. For simplicity, the scope here is limited to a couple of instance types, comparing some m5 and i3 instances with Alibaba equivalents. PAYG pricing is listed where offered.
What about auto scaling? That’s one reason I got PAYG pricing from another source. According to Alibaba, “all ECS instances that Auto Scaling automatically creates, or manually adds, to a scaling group will be charged according to their instance types. Note that you will still be charged for Pay-As-You-Go instances even after you stop them.”
But one thing you can do is manually add subscription-based instances to an auto scaling group, then configure them to be not removed as you scale down. Generally speaking, the full price of AWS Linux instances over a 1-month period is up to 35% higher than what you pay with a 1-month Alibaba subscription. For an AWS Windows instance over a 1-month period, it’s anywhere from 9–25% higher than an Alibaba subscription.
Alibaba is less expensive than AWS for the one-month subscription — that’s a fact. But for longer-lived instances, AWS Reserved Instances are more affordable, being about 40–75% less expensive than AWS PAYG, and less than some or possibly all Alibaba monthly subscriptions. AWS RI’s are also more easily applicable for auto scaling than a monthly subscription.
For non-production instances that you can turn off when not in use, PAYG is more affordable for both cloud providers, and ParkMyCloud can automate the scheduling. The only problem with Alibaba will be finding instances types that you can use with the PAYG option.
Original post: https://www.parkmycloud.com/blog/aws-vs-alibaba-cloud-pricing/
CEO of ParkMyCloud
52 
1
52 
52 
1
CEO of ParkMyCloud
"
https://medium.com/t-t-software-solution/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-azure-app-service-%E0%B8%9F%E0%B8%A3%E0%B8%B5-%E0%B9%83%E0%B8%99-10-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-e3a5c25ef749?source=search_post---------299,"There are currently no responses for this story.
Be the first to respond.
สำหรับคนที่สนใจทดลองใช้ Azure Cloud Service น่ะครับผม
ตัวอย่างนี้เป็นแบบ Paas น่ะครับ โดยผมจะจำลองการสร้าง web application และ web api แบบ ง่ายๆผ่าน Azure ครับผม
ขั้นตอน
2. เสร็จแล้วระบบจะนำเราไปที่ my.visualstudio.com
3. ข้อดีของเว็ปนี้คือ3.1 ฟรี Azure 300$ 1 ปี (25$ ต่อเดือน)3.2 ฟรี Pluralsight (เว็ป Learning Centre) 3 เดือน (มี้เนื้อหาดีๆเยอะมากๆครับ)3.3 ฟรี Azure App Service (กดเล่นได้เรื่อยๆ แต่เล่นได้ครั้งล่ะ 1ชั่วโมง)
4. ให้ทดลองเลือก Tool >> Azure App Service >> Use it free
5. จะมีให้เราทดลองเลือก App ได้หลายประเภท, ลองเลือก Web App
6. เลือก MVC Template
7. เลือก Account ที่เราจะใช้ เสร็จแล้วรอสักพักให้ Azure สร้าง Web ให้เรา ซึ่งถ้าเสร็จแล้วจะได้หน้าตาตามภาพ
8. สิ่งที่เราจะไ้คือ
8.1 Link สำหรับ Web ที่พึ่งสร้าง
8.2 Link สำหรับทดลองเข้าใช้งาน Azure
8.3 Edit Code ผ่าน Visual Studio Code ได้ online เลย
8.4 Download sample code ลงมาได้
8.5 Download Azure Publish Profile เพื่อเอาไว้ deploy web ผ่าน Visual Studio ได้จากเครื่องของเรา
8.6 Git
9. ตัวอย่างหน้าเว็ป
10. ตัวอย่าง Azure Portal
11. ตัวอย่าง Edit code online
12. ผม ทดลองสร้าง Web API เพิ่มน่ะครับ โดยกด Previous แล้ว ระบบจะลบ web app ออกไปก่อน
13. เลือก API APP
14. เลือก API To Do List
15. หน้า Web API
16. กด using the Swagger UI, จะได้หน้าตาของ Swagger UI พร้ม list ของ Web API สวยงามน่าใช้เลยครับ ^ ^
17. ทดลอง เรียก service GET /api/ToDoList
18. เท่านี้เราก็จะได้ web api แบบบง่ายๆไว้ทดลองใช้ได้ครับผม
นี้เป็นบทความแรกของผม ใน Medium ผิดพลาดยังไงต้องขออภัยมา ณ ทีนี้ด้วยน่ะครับ
นายป้องกัน
https://www.tt-ss.net/
26 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
26 claps
26 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@highwayman1991/cloud-computing-in-a-modern-world-distributed-cloud-computing-by-hivenet-f6f2f96294d5?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
Алексей Филатов
Sep 19, 2019·4 min read
The Internet and information technology play a great role in modern human’s life. Today we can observe a new era in the development of social media where we deal with a variety of electronic devices, big storage capacities and high speed of information response. There is an important innovative in this world of new technologies which is called cloud computing. It can bring absolutely new advantages to people from different professional groups.
Cloud computing represents a system which serves as an internet-based information centre. The customers can get an access to all kind of files and software safely through some different devices. This opportunity doesn’t depend on the location of customers: they can use it from any place on Earth where it possible to have an access to Wi-Fi.
Cloud computing is an excellent instrument for organizations and individuals seeking an easy way to store and access media from one device to another. It enables people to run software programs without installing and develop and test programs without necessarily having servers. And it has a wide list of advantages for business projects. Let’s consider them in a more detailed way.
There are several positive features in the usage of cloud computing in business. Here are some of them.
Global access. Thanks to a remote server, employees will be able to share files and calendars with ease. Such opportunities help to improve planning and time management within your company.
Comfortable data sharing. Thanks to a remote cluster of servers, you won’t get lost important information. If there will be mistakes with your concrete files or all of your data, it will be able to be sent to another device.
Economic solutions. Cloud computing allows you to avoid additional expenses for licensing fees, data storage costs, payments for software updates and salary for special managers.
Safety and security. Databases in cloud computing are difficult to penetrate for hackers. And developers strive to find new techniques to protect the system from hackers better.
Large storage. In comparison with a personal computer, cloud computing gives your company an opportunity to store much more information.
Device independence. You can get an access to cloud computing software everywhere and anywhere. You just need to have a device and an access to Wi-Fi.
HiveNet represents a Distributed Cloud Computing Network. It connects devices from different countries in order to perform valuable computing tasks. HiveNet helps customers and computer owners to come into contacts with each other.
HiveNet is responsible for task assignment and validation and fair payment distribution. Computer owners offer computing power to customers and receive payments from them. Let’s see what advantages different sides in this project have.
Computer owners. Usual people can earn during using computer. Some profit will be got for the fulfillment of simple tasks by your computer’s power. You just need a device with an internet connection. So, this is a real perspective way to get passive income that doesn’t require special abilities from you. And you can get profit even when you sleep.
Customers. HiveNet allows to use already available computers, so there is no need for companies to spend money on new devices, housing, etc. That’s why this instrument is much cheaper in comparison with usual cloud computing.
Crypto traders. People from this group will have nice short- and long-term opportunities thanks to using HiveCoin. These tokens are used for the payments for computing power within the HiveNet.
Nature. Thanks to the increase of the usage of available devices, there will be no need to build many new ones. That’s why there will be less electronic garbage. And it will be better for environment.
The project team consists of specialists in different fields. You can get to know more about them here.
Presale of tokens will be started on 24th September, so you will have a chance to purchase perspective HiveCoins for good value. You will be able to buy tokens for the cost 0.06 USD/HNT from 24th September to 7th October. Then the price will have risen gradually. With the start of IEO / public sale, tokens will be sold for the cost 0.08 USD/HNT.
Here are the links to resources where you will be able to know more about the project:
Website: https://www.hivenet.cloud/
White Paper and Light Paper: https://www.hivenet.cloud/resources
Facebook: https://www.facebook.com/HiveNetCloud
Twitter: https://twitter.com/HiveNetCloud
LinkedIn: https://www.linkedin.com/company/hivenetcloud
Medium: https://medium.com/official-hivenet-blog
Youtube: https://www.youtube.com/c/HiveNet
Telegram: https://t.me/hivenet
The article was written and published by Arcadio707 (https://bitcointalk.org/index.php?action=profile;u=2426003)
Crypto enthusiast
213 
213 
213 
Crypto enthusiast
"
https://towardsdatascience.com/automate-and-supercharge-google-colab-with-javascript-9f494d98489d?source=search_post---------301,"Sign in
There are currently no responses for this story.
Be the first to respond.
M Khorasani
Apr 14, 2021·6 min read
Cloud computing is quickly becoming the weapon of choice for data scientists and firms alike, for all the right reasons too. With the resources that you are afforded and the granularly small…
"
https://medium.com/backplane/extinguish-the-routing-tire-fire-f6ba0b4ec7b3?source=search_post---------302,"There are currently no responses for this story.
Be the first to respond.
How infrastructure evolves, and how Backplane can help.
In our last post, we introduced one of the key technologies that makes it possible for Backplane to help you untangle the network that runs your web architecture. This time, we’re going to walk you through how a typical web architecture evolves, and how you can avoid a whole lot of pain by using Backplane instead.
Sign up now to get started for free.
All web architectures start out the same way — with a whole lot of good intentions. As business needs take over, time becomes an issue, that round of funding sneaks up on you, etc., a lot can go wrong. Here’s how it usually goes:
Backplane is designed to be your reliable, consistent interface to help you manage your infrastructure. Get health checks, intelligent / secure routing with end-to-end TLS, mutual authentication between data centers, load balancing, and a growing list of advanced middleware like: OAuth flow handling, geo headers, ip rules, and more! All handled close to your users without having to write a single line of code. Backplane requires no extra infrastructure, is secure by design, and is trusted by experts.
Where you used to have a fractured set of services with different configurations and control planes, you now have one interface to help you untangle your network: Backplane. Less code means fewer bugs, and fewer tire fires.
Let us help you untangle your network — click here to sign up!
backplane
39 
39 claps
39 
Written by
Founder \\ @backplaneio. https://www.backplane.io.
backplane
Written by
Founder \\ @backplaneio. https://www.backplane.io.
backplane
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/apis-and-digital-transformation/we-wanna-be-like-the-big-tech-companies-cd0ea295de31?source=search_post---------303,"There are currently no responses for this story.
Be the first to respond.
By Jim Haar
In my work consulting with enterprises, I often hear business leaders say something along the lines of, “We wanna be like Netflix or Amazon or Google or [Big Technology Company X.]” This can be a misguided way of strategizing digital transformation.
Not so long ago, for example, I was with several executives from a major U.S. auto insurance provider. The lead executive commented, “We want to be like the big tech companies.”
The insurance provider has a significant business in the United States, and its leaders were concerned that in 10 years (give or take), that business as we know it may be gone. The company’s industry and economic trend forecasters (and it has the money to buy the best) believed that the rise of autonomous vehicles, ridesharing services, and other transportation disruptions may portend a significant decline in the traditional owned vehicle transport business model.
There is some evidence of this already as younger people are buying fewer cars, especially in urban areas. It’s still somewhat difficult to pinpoint how much of the shift is due to technology, lingering impacts of the Great Recession on millennials’ ability to make big purchases, migrations toward urban centers with mature public transportation infrastructure or all of the above (and perhaps more) — but the insurance provider’s leaders had seen enough to know that if they weren’t prepared to change, they’d likely be in for trouble. But what did that change look like? Where did it start?
The insurance provider had both a vision and a fear. Its leaders worried that some digital native company would launch “i-insurance,” and overnight, the provider would be sidelined, just as traditional taxi cabs and short-term accommodations have been impacted by digital upstarts.
What’s curious here is that this insurance company recognized that the threat could come from anywhere — maybe a ridesharing company, maybe a larger tech player, maybe some fintech that comes out of nowhere. So why the focus on emulating the biggest tech companies in particular?
Saying that a company should be more like these vastly complicated companies is a little akin to buying into a myth or an idea. Yes, large technology companies may have practices that other companies could benefit from adopting — but without some specificity of vision, some plan for evolving that particular business, attempts to “be like [Company X]” are just so much flailing around. Without the vision, it’s not a strategy — not a blueprint to act or react.
The insurance executives had formed many interesting visions of what insurance might look like in the future. One hypothetical difference from today, for example, is that the insurance burden may shift from the vehicle owner (or vehicle manufacturer or vehicle control software provider) to the passenger for a particular trip.
Suppose, for example, that a woman named Sally doesn’t own a car and needs to go to the hardware store. She uses an app to hail an autonomous vehicle. The app knows her current location and destination, of course — but it also knows a variety of other things that are useful not only for navigation but also for insurance.
It can not only map a path but also consider road conditions. It knows traffic, weather, and time of day. It has access to accident histories for potential routes. It knows how accessible emergency services are. It has a basic financial profile for Sally, and it knows what kind of vehicle is coming and what the safety ratings on that vehicle are. In essence, the app can get everything it needs to build an insurability underwriting profile for that trip. The app then sends that profile out for reverse auction to a handful of insurers and gets back bids. Maybe the bids are 8¢, 11¢, 6¢, 13¢, and 9¢. Sally’s app selects the 9¢ bid because she set a preference in her profile for higher-rated insurers. Or maybe her app knows that the 9¢ insurer is her bank and she gets a little perk. Many variations on the core idea are possible.
But how do ideas like this become something a company actually sells or produces? How might an industry incumbent gain the ability to launch new services with the ease that big digitally-native companies do?
The insurance company executives were frustrated because they knew the company had a lot to offer. It had decades of experience — and data — underwriting and insuring vehicles and drivers. It has an excellent brand and reputation. But what it didn’t have was a way to get started on its vision.
Cloud competency or any other broad technical benchmark observed in “big tech companies” may be a piece, to be sure — but there’s a lot more to it than that. If a company is just migrating data to the cloud because storage is cheap, that’s not going to accomplish a lot. If, on the other hand, the company has identified data and functions that are valuable and is using the cloud to scale them for new services, that may accomplish more. But the company still needs more than just “cloud;” that data and those functions, wherever they are hosted, need to be accessible to developers (and their apps) via application programming interfaces, or APIs. If the company isn’t competent with APIs, the ways it can capitalize on “cloud agility” may be limited.
For many companies, digital transformation is about taking whatever makes a company’s value proposition unique — its data, its proprietary functions and services, its user base and brand equity, and so on — and positioning it to constantly evolve as user behavior changes. Data that has been used only within the four walls of the enterprise may be transformative as part of an ecosystem play in which external developers leverage the data to create new services in new markets. An enterprise accustomed to doing everything itself may find that it reaches a bigger audience by piggybacking off a larger platform. A company that locked down access to digital assets may find it moves faster by breaking the silos, sharing data, and moving from monolithic teams to smaller, more granular groups.
That’s the discussion the insurance executives needed to have. When the problem is complex, businesses must see both the holistic and specific pictures — the forest and the trees. This isn’t about trying to ape a big technology company or buy into a single broad technology — it’s about identifying what makes an enterprise valuable, forming a vision for how that value can evolve in a world of digital platform economics and rapidly-changing customer preferences, and then tuning operations and technology investments to make that vision possible.
[Looking for more insights about digital transformation? Check out Apigee’s new ebook The Digital Transformation Journey, and explore Apigee Compass to get tailored recommendations for your company’s digital journey.]
APIs are the de-facto standard for building and connecting…
55 
55 claps
55 
APIs are the de-facto standard for building and connecting modern applications. They connect applications to one another and to the data and services that power them - enabling businesses to combine software for new products.
Written by
The cross-cloud API platform. Delivering the products that make every business a digital business.
APIs are the de-facto standard for building and connecting modern applications. They connect applications to one another and to the data and services that power them - enabling businesses to combine software for new products.
"
https://medium.com/@everisbrasil/institui%C3%A7%C3%B5es-financeiras-saiba-como-se-tornar-pr%C3%B3ximo-ouvir-e-saber-as-reais-necessidades-dos-e97ff28ce37d?source=search_post---------304,"Sign in
There are currently no responses for this story.
Be the first to respond.
everis Brasil
Sep 18, 2020·3 min read
Você já parou para pensar a última vez que atualizou seus dados de contato nas instituições financeiras que utiliza? Qual foi a última vez que ligou para seu gerente de conta ou que precisou ir ao banco para realizar alguma atividade financeira? Estamos tão acostumados ao modelo digital que é natural encontrar, seja em uma roda de amigos ou até mesmo na família, alguém reclamando sobre o atendimento ou até mesmo sobre a percepção que o banco possui sobre você, sem ter a visão correta sobre as suas principais necessidades.
Em quase todos os lugares, encontramos relatos de reclamações de Instituições Financeiras, as queixas mais frequentes estão ligadas a distância entre os consultores (gerentes) e a realidade atual de seus clientes. Nesses casos podemos identificar gaps em contatos telefônicos sem informações financeiras exatas, dados pessoais desatualizados e total incerteza sobre as necessidades reais dos contratantes, acarretando em ofertas de produtos que não interessam e/ou não se adequam ao momento atual do cliente.
Mas, pensando como uma Instituição Financeira, quais são os desafios para se tornar próximo, ouvir e entender as necessidades reais dos clientes? Cada dia mais, esse é um requisito essencial para os consumidores, pensar em sua experiência de maneira macro, desde o momento em que ele acessa um aplicativo ou se dirige até uma agência. Saber agir com ações inteligentes no momento certo e de qualquer dispositivo, gerando engajamento em tempo real é a chave para conseguir uma boa experiência com o consumidor. Um exemplo, seria utilizar a plataforma de Financial Service Cloud, da Salesforce, onde você poderia utilizar uma comunidade de cliente privado para conectar o consultor diretamente com os clientes.
No passado, você precisaria contratar ferramentas de patrimônios simples e incapazes de atender de forma personalizada as necessidades totais de uma instituição financeira, ou, talvez contratar desenvolvedores locais para construir sistemas caseiros com mínima capacidade de atender as necessidades em longo prazo.
Para que o cliente se sinta próximo das instituições, o contato personalizado e de forma individual, encurta o caminho entre o banco e seus clientes, demonstrando o valor que cada um representa para a instituição.Com os dados dos clientes reunidos de forma aprimorada, os consultores serão empoderados de recursos fáceis e ágeis no atendimento personalizado e exclusivo para cada um dos consumidores, em tempo real e de qualquer dispositivo.
Imagine, ser alertado pela plataforma sobre momentos financeiros específicos dos clientes, para assim poder oferecer os melhores produtos, aumentando a produtividade dos negócios. Com o Financial Service Cloud é possível de forma simples e nativa.
Juntar a experiência com a personalização de cada cliente faz com que a instituição tenha a visão total sobre o seu perfil. Esse é um recurso interessante quando olhamos para os objetivos de cada cliente, que poderão ser captados e administrados facilmente, para promover a adesão de investimentos ideais e de futuros bons negócios, tornando o relacionamento do banco com o cliente em um relacionamento pessoal.
Outra possibilidade utilizando a ferramenta Financial Services Cloud, é utilizar o conceito de Household, para que todas as pessoas relacionadas ao clientes sejam registradas e possam ser alcançadas por negócios e ofertas, criando uma espécie de “árvore” de relacionamentos sistêmica de forma nativa e fácil. Além de poder contar com todos os dados de ativos adquiridos dos clientes, como cartões e investimentos realizados.
Independentemente do tamanho da Instituição Financeira, seja ela um banco tradicional ou digital, essas são as 3 prioridades que o setor bancário precisa ter em mente quando se refere ao seu consumidor para conquistar, fidelizar e tornar a experiência do consumidor completa e satisfeita.
Exponential intelligence for exponential companies
15 
15 
15 
Exponential intelligence for exponential companies
"
https://medium.com/level-up-web/clickhelp-as-a-dita-alternative-96f795bd3d41?source=search_post---------305,"There are currently no responses for this story.
Be the first to respond.
by ClickHelp — professional help authoring tool
DITA is short for The Darwin Information Typing Architecture. It comprises the principles of specialization and inheritance that are much like the Darwinian theory. “Information typing” means the classification of information into topics, not “typing” as in keyboarding :)
“Architecture” is a hierarchy of elements or adaptable set of structures. It provides vertical headroom (new applications) and lateral extension (specialization into new types) for information.
“DITA is an XML standard for authoring, generating, and providing technical information.”
The idea of creating such a data model belongs to IBM, which needed more effective reuse of content in products’ documentation. So the technical publications department presented DITA in 2001 for internal use. In 2004 DITA was donated to the OASIS standards organization. The latest version was released in 2016, it’s called DITA V1.3 Errata 01.
DITA consists of a set of design principles that help to create and manage content separately from formatting. If you want to understand how it works, you must understand how DITA uses topics, maps, and output formats. You create your content in DITA topics, apply DITA maps to define which topics move into which deliverables, then handle those maps to DITA output formats to produce your final deliverables.
A topic is the basic content unit of DITA. It is understood separately and used in different contexts. A topic should be brief to distinguish a single subject or answer a single question; at the same time, it must make sense and be written as an independent item. It has a fixed structure, which helps topics be consistent even when written by different authors. DITA 1.0 and 1.1 provides three basic topic types:
DITA 1.2 includes additional topic types (specialized for learning plans, overviews, summaries, and assessments).
Confused already? Yes, it’s not so simple. And DITA introduces a lot of items that should be well-understood for the efficient application of this approach in action. Let’s continue to introspect in other elements to understand what issues are addressed with the help of DITA for the technical documentation creation process.
A map is a list of indications to a set of some particular topics. It determines topics to incorporate into the deliverables. The map also establishes the order and hierarchy of the topics and ensures browsing, such as TOC and cross-topic links in the final deliverable. Maps can be created at any time, even before the topics’ development. Additionally, maps can create multiple deliverables from a single set of topics. Typically, more than half of topics are used in multiple maps. You should remember not to create a dependency between topics, they should be context-neutral. Because in case of some changes in such dependent topics, your reader may go on the wrong path. You can also use a single map for multiple deliverables. Maps can point not only to topics but also to other maps. Using maps gives you flexibility but can significantly complicate the understanding of the process. In cases you need only two or three output variants with the set of topics that are 90% concurrent, the creation of separate maps for each output can bring in extra overhead.
Output formats allow producing various deliverables suitable for different purposes (posting to the web, printing, etc.) from a content. By default, the DITA-OT provides output formats for:
Also, you can develop your own output format. It can be modified to match the templates you developed in the tools you used before, for example, Microsoft Word. But it’s not an easy task to do.
Output formats can control the arrangement of your deliverables since DITA uses semantic tagging rather than format tagging. Semantic tagging helps to indicate the nature of the content. Each output format has the corresponding settings for the target file type. It means that you define how and in what form each element must be included in the output, based on the type of such element: topic, main content, link element, numbered list, etc.
The most evident advantages of using DITA are the following:
So, basically, you create some topic just once, and DITA helps you use it in multiple deliverables for different purposes. And if you have multiple outputs with overlapping content, it is convenient to use DITA.
However, every story has two sides. And here’s the other side of DITA’s:
You must decide whether you really need such a standard in your work. If you do not update your documents regularly or reuse the same content in various combinations, maybe you can do just fine without DITA.
The next issue is the price: the shift to DITA demands a significant infusion of resources — CMSs can be expensive to implement with DITA. It requires experts to manage the transition. You’ll probably spend a lot of time developing your information architecture rather than writing the docs for your company. In addition to the reliable CMS itself, that you need to buy. This is about an investment of tens of thousands of dollars and 12–18 months of hard work, in some cases, even more. Moreover, staffing is quite a challenge. Not so many people know DITA, that’s why their salaries are high. If put together all these points, DITA may prove to be not so cost-effective. Usually, it’s a bundle for small and medium businesses.
Another problem is acceptance: documentation managers must decide on what to rewrite, reuse, convert, and neglect. It’s not an easy task if you have several managers: each has a certain set of documents that they consider important, so the settling of this issue may raise a conflict, which is time-consuming and may delay the transition considerably.
One more important point is training since DITA is not so simple as one might think. It takes time and money. Some writers like to keep with the traditions and do not like to leave their comfort zone, so they protest against the process. And a technical writer must distract a lot to understand DITA, it’s not as intuitive as they put it.
Now, when DITA is not just a word for you, and you understand the core concepts, let me tell you about your other option. If you need to make a professional approach to user documentation, you can use ClickHelp. This online documentation tool addresses the same issues much easier.
ClickHelp is a cloud-based tool with no need of installing anything. You can sign up and get your own ready-made portal in the vendor’s cloud. It protects access for authors with SSO, or you can use simple login and password. ClickHelp works in a web browser from any place in the world. You can easily run a review process for your content, publish, and update your user guides quickly.
Let’s have a look at professional technical writing functions that ClickHelp offers and how they cover DITA use cases:
“With ClickHelp, you get a ready-made documentation site with the teamwork functions and documentation hosting.”
Implementation of such standards as DITA is a long-range project that consumes time, money, and human resources. And grasping at new trends because everyone is following them is not the right way to go. Instead, maybe you need to take time, look around, and find another alternative for your documents to be structured. ClickHelp might be just what fits you best — content reuse and dynamic output are paired with the ease of use for better documentation delivery. Give it a try, and you will see that DITA is not the only approach to professional documentation writing.
Good luck with your technical writing!ClickHelp Team Author, host and deliver documentation across platforms and devices
Originally published at https://clickhelp.com.
Stories for technical writers, web developers and web…
243 
243 claps
243 
Stories for technical writers, web developers and web designers. It's time to level up your skills!
Written by
ClickHelp - Professional Online Technical Writing Tool. Check it out: https://clickhelp.com/online-documentation-tool/
Stories for technical writers, web developers and web designers. It's time to level up your skills!
"
https://medium.com/@jaychapel/aws-vs-alibaba-cloud-pricing-a-comparison-of-compute-options-c7ea928fd9c9?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jan 29, 2020·9 min read
As cloud users continue to use Alibaba Cloud, extending its global presence, we’ll review a comparison of AWS vs Alibaba Cloud pricing. Commonly recognized as the #4 cloud provider (from a revenue perspective anyway), Alibaba is one of the fastest-growing companies in the space today.
Alibaba has been getting a lot of attention lately, given its rapid growth, and making headlines after the release of their latest quarterly revenue and full fiscal year 2019 reports. Alibaba is at the top of the market in Asia, and dominating in China with cloud revenue up 66% year-over-year. While Alibaba is in the top 5 CSPs worldwide, they still have a lot of plans for the future to maintain this growth and continue to move up.
The company said it is focused on high-value security, analytics, and artificial intelligence tools and “rationalizing our offerings of commodity products and services.” With an annual revenue run rate of $4.5 billion, it is clear that Alibaba Cloud intends to compete globally with AWS and other major cloud providers.
However, on a global scale, AWS continues to dominate the market. In the latest quarter, Amazon reported Amazon Web Services (AWS) sales of $7.7 billion, compared to $5.44 billion at this time last year. AWS revenue grew 41% in the first quarter — at this time last year, that number was 49%.
ParkMyCloud supports Alibaba Cloud and AWS, and with that, let us focus on pricing and cost savings — our forte. In this blog, we dive a bit into the pricing of Alibaba Elastic Compute Service (ECS), compare it with that of the AWS EC2 service and whether Alibaba Cloud computing can offer better value than AWS.
Elastic Compute Service (ECS) and Elastic Cloud Compute (EC2), respectively, are the standard compute services offered by Alibaba Cloud and AWS.
Both cloud computing services provide the same core features:
The major differences between Alibaba Cloud ECS and AWS EC2 are that Alibaba Cloud provides a wider range of instance families and that AWS offers more regions globally.
Finding actual pricing for comparison purposes can be a bit complicated, as the prices are listed in a couple of different places and do not quite exactly match up since pricing varies between different instance types, and no instances from the two companies are identical. If one searches for Alibaba pricing, one ends up here, which I am going to call the “Alibaba Cloud” site. However, when you actually get an account and want to purchase an instance, you can up here or here, both of which I will call the “Aliyun” site. [Note that you may not be able to see the Aliyun sites without signing up for an account and actually logging-in.]
Aliyun (literally translated “Ali Cloud”) was the original name of the company, and the name was changed to Alibaba Cloud in July 2017. Unsurprisingly, the Aliyun name has stuck around on the actual operational guts of the company, reflecting that it is probably hard-coded all over the place, both internally and externally with customers. (Supernor’s 3rd Conjecture: Engineering can never keep up with Marketing.)
Both sites show that like the other major cloud providers, Alibaba’s pricing model includes a Pay-As-You-Go (PAYG) offering, with per-second billing. Note, however, that in order to save money on stopped instances, one must specifically enable a “No fees for stopped instances” feature. Luckily, this is a global one-time setting for instances operating under all Pay-As-You-Go VPC instances, and you can set it and forget it. Unlike AWS, this feature is not available for any instances with local disks (this and other aspects of the description lead me to believe that Alibaba instances tend to be “sticky” to the underlying hardware instance). On AWS, local disks are described as ephemeral and are simply deallocated when they are not in use. Like AWS, Alibaba Cloud system/data disks continue to accrue costs even when an instance is stopped.
Both sites also show that Alibaba also has a one-month prepaid Subscription model. Based on a review of the pricing listed for the us-east-1 region on the Alibaba Cloud site, the monthly subscription discount reflects a substantial 30–60% discount compared to the cost of a PAYG instance that is left up for a full month. For a non-production environment that may only need to be up during normal business hours (say, 9 hours per day, weekdays only), one can easily see that it may be more cost-effective to go with the PAYG pricing, and use the ParkMyCloud service to shut the instances down during off-hours, saving 73%.
But this is where the similarities between the sites end. For actual pricing, instance availability, and even the actual instance types, one really needs to dive into a live Alibaba account. In particular, if PAYG is your preference, note that the Alibaba public site appears to have PAYG pricing listed for all of their available instance types, which is not consistent with what I found in the actual purchasing console.
The Alibaba Cloud site breaks down the instance types into “Entry Level” and “Enterprise”, listing numerous instance types under both categories. All of the Entry Level instance types are described as “Shared Performance”, which appears to mean the underlying hardware resources are shared amongst multiple instances in a potentially unpredictable way, or as described by Alibaba: “Their computing performance may be unstable, but the cost is relatively low” — an entertaining description to say the least. I did find these instance types on the internal purchasing site, but did not delve any further with them, as they do not offer a point of reference for our AWS vs. Alibaba Cloud pricing comparison. They may be an interesting path for additional investigation for non-production instance types where unstable computing performance may be OK in exchange for a lower price.
That said…after logging in to the Alibaba management console, reaching the Aliyun side of the website, there is no mention of Entry Level vs Enterprise. Instead, we see the top-level options of “Basic Purchase” vs “Advanced Purchase”. Under Basic Purchase, there are four “t5” instance types. The t5 types appear to directly correspond to the first four AWS t2 instance types, in terms of building up CPU credits.
These four instance types do not appear to support the PAYG pricing model. Pricing is only offered on a monthly subscription basis. A 1-year purchase plan is also offered, but the math shows this is just the monthly price x12. It is important to note that the Aliyun site itself has issues, as it lists the t5 instance types in all of the Alibaba regions, but I was unable to purchase any of them in the us-east-1 region — “The configuration for the instance you are creating is currently not supported in this zone.” (A purchase in us-west-1, slightly more expensive, was fine).
The following shows a price comparison for Alibaba vs AWS for “t” instance prices in a number of regions. The AWS prices reflect the hourly PAYG pricing, multiplied by an average 730 hour month. I was not able to get pricing for any AWS China region, so the Alibaba pricing is provided for reference.
While the AWS prices are higher, the AWS instances are PAYG, and thus could be stopped when not being used, common for t2 instances used in a dev-test environment, and potentially saving over 73%. One can easily see that this kind of savings is needed to compete with the comparatively low Alibaba prices. I do have to wonder what is up with that Windows pricing in China….does Microsoft know about this??
Looking at the “Advanced” side of the Aliyun purchasing site, we get a lot more options, including Pay-As-You-Go instances. To keep the comparison simple, I am going to limit the scope here to a couple of instance types, trying to compare a couple m5 and i3 instances with their Alibaba equivalents. I will list PAYG pricing where offered.
In this table, the listed monthly AWS prices reflect the hourly pay-as-you-go price, multiplied by an average 730 hour month.
The italicized/grey numbers under Alibaba indicate PAYG numbers that had to be pulled from the public-facing website, as the instance type was not available for PAYG purchase on the internal site. From a review of the various options on the internal Aliyun site, it appears the PAYG option is not actually offered for very many standalone instance types on Alibaba…
The main reason I pulled in the PAYG prices from the second source was for auto scaling, which is normally charged at PAYG prices. In Alibaba, “all ECS instances that Auto Scaling automatically creates, or manually adds to a scaling group will be charged according to their instance types. Note that you will still be charged for Pay-As-You-Go instances even after you stop them.” It is possible, however, to manually add subscription-based instances to an auto scaling group, and configure them to be not removed when the group scales-down.
In general, the full price of the AWS Linux instances over a month is 22–35% higher than of an Alibaba 1-month subscription. A full price AWS Windows instance over a month is 9–25% higher than that of an Alibaba subscription. (And once again, it appears Windows licensing fees are not a factor in China.)
When it comes to Alibaba Cloud pricing vs AWS, Alibaba Cloud is trying to attract business and expand their global footprint by offering special promotions typically consisting of free trials, specially priced starter packages, and time-limited discounts on premium services. In many cases, taking advantage of these promotions could be useful in order to save money, but so is AWS.
Amazon also has their fair share of money-saving offerings as well. AWS announced the release of AWS Savings Plans — a new system for getting a discount on committed usage for EC2.
There are two kinds of Savings Plan:
You can now purchase reserved instances that, rather than going into effect immediately, are scheduled for future purchase.
Now, when planned correctly, you can avoid lapsing on Reserved Instance coverage for your workloads by scheduling a new reservation purchase to go into effect as soon as the previous one expires. The furthest in advance you can schedule a purchase is three years, which is also the longest RI term available.
However, AWS RI purchases have few limitations, they can be queued for regional Reserved Instances, but not zonal Reserved Instances. Regional RIs are the broader option as they cover any availability zone in a region, while zonal RIs are for a specific availability zone and actually reserve capacity as well.
Alibaba definitely comes out as less expensive in this AWS vs Alibaba cloud pricing comparison — the one-month subscription has a definite impact. However, for longer-lived instances, AWS Reserved Instances will certainly be less expensive, running about 40–75% less expensive than AWS PAYG, and thus less than some if not all of the Alibaba monthly subscriptions. AWS RI’s are also more easily applicable to auto scaling groups than a monthly subscription instance.
For non-production instances that can be shut down when not in use, PAYG is less expensive for both cloud providers, where ParkMyCloud can help you schedule the downtime. The difficulty with Alibaba will actually be finding instances types that can actually be purchased with the PAYG option.
Originally published at www.parkmycloud.com on January 9, 2020.
CEO of ParkMyCloud
21 
1
21 
21 
1
CEO of ParkMyCloud
"
https://medium.com/syncedreview/google-cloud-next-new-hybrid-cloud-anthos-partnerships-data-centres-41468c0fc957?source=search_post---------307,"There are currently no responses for this story.
Be the first to respond.
Artificial intelligence and machine learning have become powerful tools for tech giants looking to build the optimal cloud service platform. To catch up with rivals Amazon Web Services and Microsoft Azure, Google is accelerating the development of its own cloud services. At the annual Google Cloud Next conference yesterday in San Francisco the company unveiled a handful of new AI-based services and partnerships for its smart analytics and machine learning tools.
This is the first Google Cloud Next conference for new Google Cloud CEO Thomas Kurian. A veteran who was previously Oracle’s president of product development, Kurian told The Wall Street Journal his plan is to borrow strategies from Oracle and expand Google Cloud’s sales team to help the company take the lead in the cloud competition.
Anthos — The hybrid cloud management product based on Kubernetes
In his keynote speech Kurian introduced Anthos, a hybrid cloud management product based on Kubernetes that allows customers to run managed container services on multiple cloud or hybrid cloud deployments. Customers can move old applications and run them on Google’s cloud without modifying the original code. In her speech, Google Cloud Director of Product Management Jennifer Lin said “With Anthos, you can run anywhere.”
Lin also revealed a beta service called Anthos Migrate — a technology that can auto-migrate and modernize customers’ infrastructures to original virtual machines or applications. Her demo showed how Anthos Migrate can simplify modernization and move virtual machines operating either on-premises or on cloud providers such as AWS and Azure into containers on the Google Kubernete Engine. Anthos Migrate will be available in beta soon.
Google partners with seven open-source companies
Under Kurian’s leadership, Google Cloud took an aggressive step to challenge leading cloud vendor Amazon Web Services by announcing partnerships with seven open-source, software-centric companies — Confluent, DataStax, Elastic, InfluxData, MongoDB, Neo4j, and Redis Labs.
Earlier this year, shares of Elastic and MongoDB fell after AWS introduced a free Elasticsearch code library and MonogBD-like database software. Google provided some relief yesterday, pledging to provide fully managed services based on Elastic, MongoDB, and its other five new partners’ flagship softwares and databases, which will be seamlessly integrated with the Google Cloud Platform.
In a Redis Labs press announcement, Google Cloud Corporate Vice President Kevin Ichhpurani said “We share Redis Labs’ vision and commitment to open source-centric innovation and we’re excited to deepen our partnership on behalf of customers. Bringing Redis Enterprise to GCP is beneficial to customers looking for a seamless, fully managed, cloud native service that they can rely on for their critical workloads.”
New data centres in Seoul and Salt Lake City
Google Cloud also announced it is building data centres in Seoul, South Korea, and Salt Lake City, Utah, expanding its cloud computing infrastructure to 13 countries across 19 different time zones. Google Cloud’s new data centre in Osaka, Japan, will come online in the next few weeks, while another in Jakarta, Indonesia, will be put into use in early 2020.
Google Cloud Next 2019 runs to April 11 at the Moscone Center in San Francisco.
Journalist: Fangyu Cai & Tony Peng | Editor: Michael Sarazen
2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon. Apply for Insight Partner Program to get a complimentary full PDF report.
Follow us on Twitter @Synced_Global for daily AI news!
We know you don’t want to miss any stories. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.
We produce professional, authoritative, and…
62 
62 claps
62 
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/using-heroku-to-deploy-an-angular-app-5be838ef1de7?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dieter Jordens
Dec 17, 2019·5 min read
A good programmer sometimes likes to sacrifice a few evenings of their free time to get acquainted with the technology they’ve never worked with before. It often stops there because they have a busy job, children, or a fun hobby. They don’t want to spend another night deploying their application when they can have a walk or enjoy a refreshing pint…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@seragizozel/tips-and-tricks-for-aws-cloud-practitioner-certification-exam-9f37fe73b46?source=search_post---------309,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sera Giz Ozel
Dec 30, 2020·6 min read
Today, AWS is the most popular cloud service provider and in terms of market share, being way ahead of Google Cloud and Microsoft Azure, which are its two closest competitors.
The AWS Certified Cloud Practitioner exam tries to measure the knowledge and understanding of attendees in AWS Cloud environment. There are many different types of AWS cloud certification depending on specific usage, however, AWS cloud Practitioner can be considered as baseline.
Of course, if you have hands-on experience in AWS cloud, (or in other cloud environments) you may reduce the preparation time for the exam. However, if you don’t have any experience, don’t worry, you may need more time to understand the environment and for sure you will get some hands-on experience when preparing for the exam.
1- Download the exam guide: This is provided by AWS on their website. It will provide you topics that you need to cover and the format of the exam.
2- Sample Questions: There are 10 sample questions with their answers provided by AWS for free. When I first check the questions before starting to study for the exam, I thought it will be a very tough exam, however, after finishing my study I checked the questions again and found them very easy. So when you first check the questions, if you think it is difficult, please don’t worry, because after studying well and check the questions again, you will feel the same as me and be surprised how much you learned during that preparation.
3- Online/Offline Courses: You can benefit from many online courses. AWS provides online and classroom courses on its website. Also, you can find many good online courses that you can use for preparation.
First of all, I watched AWS Certified Cloud Practitioner Training 2020 — Full Course by Andrew Brown from ExamPro which I found very useful because of a clear explanation of important concepts and also Mr.Brown shows how each system works by sharing his screen. I repeated the steps by watching his video, created a free account to play with AWS Cloud services in order to get a better understanding. Also, you don’t need to subscribe to watch this video, because it is from YouTube (free).
At that point, also taking notes is very important. If you don’t have a long time experience with AWS Cloud, you might start to get confused with the names of the AWS Services, some services have very similar names with different scopes. My tips: write down the names with an explanation of them with at least 2–3 sentences to make sure you will remember when you come back and look at your notes. ( I like to write it down when I study about something, maybe still a bit traditional :), I had around 10 pages of notes at the end of my study)
4- Practice Exam Questions : It is very hard to find similar questions as a real exam. When I check user-comments of some example question sets or some online courses, some people were saying those questions are harder than the exam and for others, it was the opposite. I can tell the resources that I share in this blog are very good sources to prepare for the exam and I have encountered very similar or sometimes same questions.
→ After intensively searching for the best question sets by checking the user-comments, I purchased Neal David’s 500 Practice Exam Questions from Udemy. Each question includes a detailed explanation of the answer. I enhanced my notes by checking the answers of every question. At the end of each exam (which includes 60 questions), I was checking for each concept and services the White Papers of AWS in order to extend my knowledge (mainly this White Paper).
→ In addition to that, here I found 260 sample questions (written as 520 but actually it is duplicated) which are very similar to the real exam. Just please carefully review the answers because there are many wrong answer keys, therefore please also consider comments for each question.
As a summary, I created and used an AWS Cloud account by watching Andrew Brown’s video, then checked Neal David’s 500 Practice Exam Questions, took some notes at the same time by checking White Papers of AWS, and before the exam I reviewed these questions on the internet.
In the real exam, 95% of the topics were the same as what I covered with videos and practice questions. However, the questions were more tricky in terms of grammar and multiple choices. Please make sure that you read the questions well.
For example in the questions, they could ask ‘If a company wants to reach their AWS cloud environment 7/24 hours and be protected from electric shot-downs, what shouldn’t they implement?’ rather than asking ‘What should they implement..’.
The exam can be taken at a testing center or home. After you register on AWS Certification website, (Exam Code: CLF-C01) you can choose the certification exam and date/time schedule from available timeslots.
I chose to take the online exam. Before taking the exam, I suggest you to prepare your exam area according to the instructions that are provided by the AWS like no phone, no watch, closed room, etc..
After you install the required programs on your computer, during the exam you will not able to do anything with your computer, except looking and answering the questions. Instructor will connect with you at the beginning of the exam with voice and checking your camera. He/She may want you to show the room, your wrists and the table, etc.. The instructor might not watch you all the exam time but your camera will record all activities. Also during the exam, another instructor can connect to your session and asks you to show around again.
After you complete the exam, you will immediately see PASS/FAIL information according to your number of correct answers. However, you will not be able to see your score and your certification/badge, because after the exam, your camera record will be examined by the instructor to make sure that you were respecting the rules. Then within one week, you will receive your certification and badge with more detailed information regarding your exam result.
Additional Resources:
If you also looking for additional resources, these websites cover some good quality training with an explanation of their advantages and disadvantages.
Good Luck!
Data Science and all related topics..
40 
2
40 claps
40 
2
Data Science and all related topics..
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/elrondnetwork/elrond-partners-with-ankr-and-enables-one-click-deployment-for-selected-battle-of-nodes-validators-fe1c563fc382?source=search_post---------310,"There are currently no responses for this story.
Be the first to respond.
Elrond and Ankr have entered into an agreement which enables Elrond Nodes to be easily deployed and hosted on the blockchain cloud infrastructure provider’s platform. The collaboration aims to synchronize the capabilities of the Ankr platform with the specific needs of operating Elrond infrastructure.
The end-goal is to provide node operators with all the needed features to run a Validator at maximum capacity, in an intuitive and easy to set-up environment which can be paid for in cryptocurrency.
“The drive to be a part of the new economy, and contribute to the internet-scale blockchain’s security & efficiency should not be met with technical obstacles. We are happy to soon be able to offer our supporters the means to set up & run an Elrond node with Ankr’s intuitive platform.” said Beniamin Mincu, Elrond CEO
To demonstrate the out of the box capabilities of Ankr and the remarkable progress already done to accommodate specific Elrond Validator needs, several Ankr-powered nodes will enter the fray in our incentivized testnet event.
As such, 15 distinguished non-technical members of the Elrond community will receive exclusive access to Elrond Ankr nodes and use them to join Battle of Nodes. They will do their best to climb the leaderboards, boosted by the always-on infrastructure, do battle with all the tools at their disposal and will report back with their findings.
“Competitions are exciting because they give us the chance to shine and show off our capabilities. We’re happy to take on the challenge of providing tools for Battle of Nodes and look forward to be able to host mainnet Elrond nodes in our data centers across the globe” said Chandler Song, Ankr CEO
The intuitive and easy to use interface of the Ankr platform meets the needs of non-technical node operators and at the same time provides a high-level of security out of the box. Notably, remote SSH access to the underlying operating system is not enabled.
The two partners will utilize this final stage of the testnet to add more functionalities to the Ankr solution, which is specifically aimed at the less technical experienced user, without compromising the existing high-level of security.
We’re excited to share the progress of the work so far and are looking forward to enabling Elrond node operators access to Ankr’s hosting options that feature 1-click deployment, intuitive interface and cryptocurrency payment.
Please visit our partner’s platform for more information regarding the innovative blockchain cloud infrastructure offering. https://app.ankr.com/
Elrond is a new blockchain architecture, designed from scratch to bring a 1000-fold cumulative improvement in throughput and execution speed. To achieve this, Elrond introduces two key innovations: a novel Adaptive State Sharding mechanism, and a Secure Proof of Stake (PoS) algorithm, enabling linear scalability with a fast, efficient, and secure consensus mechanism. Thus, Elrond can process upwards of 10,000 transactions per second (TPS), with 5-second latency, and negligible cost, attempting to become the backbone of a permissionless, borderless, globally accessible internet economy.
elrond.com
Ankr is building an infrastructure platform and marketplace for Web3-stack deployment.
The blockchain infrastructure enables resource providers and end-users to easily connect to blockchain technologies and DeFi applications.
Our customized node deployment and API solutions with strong focus on user experience drastically lower the entry barrier for everyday people, enterprises and developers to contribute to the blockchain ecosystem.
ankr.com
A scalable value transfer protocol for the digital economy
208 
1
208 claps
208 
1
A scalable value transfer protocol for the digital economy
Written by
CEO, Elrond Network
A scalable value transfer protocol for the digital economy
"
https://blog.markgrowth.com/ecommerce-web-hosting-803c2b2aaeba?source=search_post---------311,"It wouldn’t be wrong to say that Ecommerce has not only changed the way we do business, but it has also changed the way we live our lives. About 22 years ago, when legendary sitcom Friends was just starting to get popular and the world was considerably less interconnected, the idea of starting your own business was met with close-to-zero enthusiasm as compared to now.
It was then in 1995, Amazon surfaced the market and since then the landscape of global retail industry has never been the same. Things changed so dramatically that it is difficult to imagine a life before Amazon now.
With Amazon, came the boom in ecommerce sector that enabled many SMEs to establish themselves and even compete with the bigger fishes. As the years rolled on, ecommerce industry transformed into a humongous money-making juggernaut that has already contributed billions of dollars to the US economy.
Without a surprise, the promising potential and recurring success stories of businesses in the ecommerce industry prompted more entrepreneurs to try their luck in this growing space. Therefore, it is now more important than ever to educate them about the important things that they might disregard.
To begin with, the first and foremost important task is to select a suitable web host for your online store before your ecommerce business takes off; it’s an important decision and deserves a lot of thought. The choice of web hosting you make holds a direct impact on the way your website is delivered to visitors.
There are mainly three types of web hosting services available; Shared, Dedicated and Cloud. All of the hosting services have their share of pros and cons. By the end of this article, you’ll have a better understanding and clarity in terms of choosing a web host for your online store.
The best thing about Shared hosting is that it is economical compared to Dedicated and Cloud hosting. You can pay as low as $3/month and that’s all the positives you can expect from shared hosting. The biggest con of shared hosting is exactly what its name suggests; you have to share server with other websites. That brings with itself numerous concerns including the risk of downtime, slow page load time and poor security. All of these factors can severely damage user experience, which might compel customers to never visit your website again.
Moreover, if you have plans to scale up in future, you would need to migrate your website to a dedicated/cloud server, to ensure the continual optimal performance of your website. Therefore, it is wiser to start with a dedicated or a cloud server in the first place. My suggestion: don’t go with shared hosting for your online store.
Dedicated servers are the complete opposite of Shared hosting. Having a dedicated server means that you won’t have to share it with other websites. In other words, you don’t have to worry about slow page load time, downtime, and security shortcomings.
However the downside of dedicated servers is that they are extremely expensive and a single server can easily exceed $100 if you opt for “better” features. Additionally, you might need to hire a specialist technician for Dedicated Hosting to look after the server’s technicalities.
Cloud Hosting is relatively newer when compared with Shared and Dedicated Hosting. It is a cluster of hundreds of individual servers that work together to make it look like a single giant server. And as the business grows, the hosting infrastructure providers such as AWS and DigitalOcean can simply add more hardware to make an even larger cloud.
The main advantages of Cloud Hosting is that it follows a Pay-As-You-Go method. You only pay for the resources you consume, unlike dedicated servers. Similarly, you don’t have to share your servers with other websites, unlike shared hosting. Cloud Hosting is swiftly making inroads in the industry of web hosting and it’s gaining support every passing day.
Having put out all the pretty sides, there is however, one con attached to Cloud Hosting; the deployment and management of a website on cloud server. It can be difficult, especially if you aren’t well versed with the technicalities. The wisest thing to do in that case is to hire the services of Managed Cloud Hosting Providers — a service that takes care of server-related technicalities, while you center your energy on the core operations of your business.
A website’s uptime is basically the time when its operational and it doesn’t have to be elaborated further that all websites require high uptime from their web hosts. As an online store owner you have to make sure that your website is operational 24 hours a day, 7 days a week and 365 days a year. Any downtime can not only affect your website’s profitability, but it can also affect visitors’ user experience. Moreover, if your online store has a business model that involves investors, then a single downtime may result in lack of confidence from investors.
In order to keep your website operational for every minute of the year, you would want to hire services of a web host that provides 24/7 monitoring of your server’s operations. There should be on-site technicians skilled enough to fix any problem at any hour of the day, as well as take care of routine upgrades.
Here’s an example of what could go wrong if you don’t have 24/7 monitoring: Imagine a website that goes down on a Friday evening. Now, since your host doesn’t provide 24/7 monitoring, you will have to wait for the next business day to get your website running. That is Monday! That is two days of non-functionality of your website. Imagine all the profit that you would have to forego.
Ideally, a good hosting service should be able to continuously deliver an uptime of 99.8% or better each month. Or simply put, your website should only have a downtime of less than 90 minutes per month. While making your selection you ought to take into account all types of downtime, including network outages and scheduled maintenance for server’s software upgrades.
When you run an ecommerce website, it is imperative to keep your customers’ data secure. Sensitive information such as credit card details should be encrypted before it is sent from a customer’s web browser to your online store’s checkout form.
That way, your customers won’t be at risk if someone tries to hack into the data. Installing a Secure Sockets Layer (SSL) Certificate on your site supports encryption and also authenticates your business identity to your customers.
Majority hosts tend to install the certificate for you and assign a dedicated IP address at an exchange of a few extra dollars. So while closing in on a web host for your ecommerce site, ensure that your host grants you a SSL certificate and a dedicated IP. There are some cloud-hosting platforms such as Cloudways, that offers SSL certificate for Free. So make sure that you do your research well.
Price is a tricky prospect. If you are a startup, you would want to get a web host that is affordable and cheap. That might prompt you to go for shared web hosting, which would clearly be a mistake.
So while you are looking at options, here’s a reality check: You get what you pay for! It is likely that a company that charges less than $2/month would not offer you features such a Free SSL or 24x7 monitoring and non-outsourced support. Attractive features like that are going to cost you.
So when you are looking at your hosting options, compare the features instead of prices. Figure out the important features for your business and let go the ones you can live without. Once you have shortlisted the web hosts that meet your criteria, only then you should start comparing the price. Really simple.
Since you are hosting an ecommerce website, I’d recommend that you keep RAM, CPU power and storage space in mind. Moreover, you would need additional features such as backup, auto-scaling, 1-click cloning and a staging area. Therefore, make sure that your choice of web host has all the relevant tech specs.
It must be noted that inexpensive hosting plans cannot be expected to have the RAM, processing power, and disk space to serve all these needs. Additionally, you are likely to end up working more on reducing your store’s downtime and dealing with its load issues.
If you have a question or any confusions, you should not hesitate to call your shortlisted web host providers. Ask questions. Share with them your vision of the site. Tell them about what you feel your online store’s needs would be. Satisfy yourself before you make a choice. Try not to assume that your web host will take your online store as seriously as you do. It’s your site, you call the shots.
This might be the most important feature that you should look for in your web host. If your website goes down, can you get all the relevant help from your web host? The customer support is the real litmus test that separates a good web hosts from mediocre ones.
You web host should have a dedicated staff for customer support and if things go south, the customer support should hook you up with a real, live person on the phone who can find out what you did wrong and how it can be fixed. Before making a decision, you should try to find out as much about the customer support as possible. Try to find out different ways in which you can contact them at the time of need — email, toll-free phone, chat, and so on. See if they are staffed 24/7 or if they outsource support?
When you are done with research, you will find that all hosts are not equal when it comes to price and technical specifications. Some bank on their support crew, and some view customer support as an afterthought. It’s advisable to stay clear of the latter.
Web hosting companies provide a lot of features that distinguish them from one another. The trick is to find the one that suits your needs the most.
User Experience is perhaps the most essential ingredient that leads to popularity and profitability. If your ecommerce website’s user experience is shaky, you might not end up struggling. Therefore, choose a web host that provides necessary features to support the user experience that you have envisioned.
At the end of the day, you have to realize that you can only reach for the moon if your first step is right; or in this case, if you have the perfect web host. If you want to thrive in the competitive market, you have to choose the most suitable web hosting provider, otherwise, you are on the back foot from the very start. A position you might want to avoid.
Everything Marketing and Growth Hacking.
107 
1
107 claps
107 
1
Written by
Managed hosting platform for popular PHP based apps on AWS, GCE, DigitalOcean, Linode, Vultr and Kyup. https://www.cloudways.com/en/
Everything Marketing and Growth Hacking.
Written by
Managed hosting platform for popular PHP based apps on AWS, GCE, DigitalOcean, Linode, Vultr and Kyup. https://www.cloudways.com/en/
Everything Marketing and Growth Hacking.
"
https://medium.com/mobileforgood/7-top-tech-trends-of-2018-33a123cec23?source=search_post---------312,"There are currently no responses for this story.
Be the first to respond.
It’s been a year of tech growth, with Africa in the spotlight. Here’s what we’ve been talking about all year:
The big Cloud providers are also opening their first data centers in Africa. Microsoft will be opening Azure regions in Cape Town and Johannesburg “in 2018” — although it’s looking like that will slip to 2019. And Amazon has announced that in 2020 they will be opening an AWS region in Cape Town. The availability of world class cloud services in South Africa will greatly benefit organisations that have data sovereignty requirements. In the future we would like to see more data centers like these in other parts of the continent.
At Praekelt.org we’ve started experimenting with alternative languages that are purpose-built for problems that feature asynchronicity. We’re big fans of Elixir and have written about our uses of it and how it compares to Python. As more of the volume of messages we handle pass through more advanced messaging platforms such as WhatsApp, we’re able to build messaging services with more powerful features and new types of media. As these services grow, it’s important that our services scale and remain very responsive. Languages such as Elixir can help with this, and our new WhatsApp for Social Good platform, Turn, is built with Elixir.
Container and so-called “Cloud Native” technologies continued to grow at an explosive rate in 2018. The annual KubeCon + CloudNativeCon North America conference doubled in size from 4,000 to 8,000 attendees. For the first time, a single API for managing containerised workloads is available from all 3 major cloud providers. All provide a managed Kubernetes service that provides a common interface to compute, networking, and storage resources.
The rise of Artificial Intelligence, Machine Learning, and Data Science seems unstoppable. JetBrains’ 2018 Data Science Survey provides a good overview of trends in tech in the data science space. At PyConZA this year, there was an entire conference track dedicated to Data Science and it was given the largest stage and the closing keynote. And Google announced an AI research lab in Ghana.
Still, ethical questions around the use of AI technologies remain persistent. In April, Google employees protested against the company’s involvement in Project Maven. Amazon faced backlash for selling its Rekognition service to government agencies such as ICE. And most recently the British police forces trialed facial recognition tech in London. We’re continuing to look at these issues as we grow our Data Science team.
In Africa alone, the number of startups operating in the agritech market grew 110 percent over the past two years, with more than $19 million invested in the sector during that period, according to a recent report by Disrupt Africa entitled “Agrinnovating for Africa: Exploring the African Agri-Tech Startup Ecosystem Report 2018”. From drone technology and artificial intelligence to mobile apps that assist farmers across the continent, Africans are embracing tech to enhance production and ensure a more profitable future while maintaining food security.
IP, or internet protocol, messaging services, like Facebook Messenger and WhatsApp, have changed the global messaging space — an industry that boasts 2 trillion messages a year. WhatsApp reaches nearly 2 billion users, and Facebook Messenger nearly 1.5 billion. Allowing for a higher degree of engagement, including the ability to send rich media and images, and a substantial reach in emerging markets, IP messaging is quickly become the preferred mode of communication over SMS. We are tapping into this in our health projects.
“Digital health should be like a hospital bed”. I heard this at a health conference earlier this year and it resonated throughout all the talks and panels I went to this year. It no longer seems like integrating technology is even an option in healthcare, but an essential part of the consumer journey. At the Node Digital Medicine conference, Matthew Farkash from Blueprint Health talked about the importance of culture, mindset, and process in terms of integrating technology into health care. He drove home the need to be specialized but also integrated — that new products must also sell integration. All of this is key for technology to thrive in the healthcare space.
We’re looking forward to keeping ahead of these trends in 2019!
Contributed by Jamie Hewland, Ambika Samarthya-Howard, & Sewagodimo🎈
Sharing stories and experiences to help leverage the power…
60 
60 claps
60 
Written by
We use technology to solve some of the world's largest social problems. Follow our curated magazine MobileForGood. www.praekelt.org.
Sharing stories and experiences to help leverage the power of mobile to improve people's wellbeing
Written by
We use technology to solve some of the world's largest social problems. Follow our curated magazine MobileForGood. www.praekelt.org.
Sharing stories and experiences to help leverage the power of mobile to improve people's wellbeing
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/comparing-the-speed-of-vm-creation-and-ssh-access-of-cloud-providers-ddf5a54faaa4?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Jun 7, 2016·7 min read
07 June 2016
This blog post provides updated figures for the data published in my article last year (Part 1). For those unfamiliar with the article, the objective of my piece is to measure two processes: the speed of VM creation and the time it takes to access the Secure Shell (SSH) key between Cloud 66 cloud vendors (AWS, DigitalOcean, Linode, Vexxhost, Microsoft Azure, Cloud A, Packet, Rackspace and Google Cloud). Additionally, it reviews both of these processes on a year-on-year basis, as a comparative benchmark report.
The article is based on Cloud 66 internal data collections via our Birdseye tool. The data included in this article was collected between March 2015 and June 2016, compared to my Part 1 article, where the data included was collected between January 2014 and March 2015.
From a speed perspective, the clear leader in creating VM and SSH access is Google Cloud. This provider takes on average 39"" seconds to complete both processes. Google Cloud is closely followed by Vexxhost (43""s), Linode (53""s) and AWS (61""s)
The rest of the cloud vendors had slower average results — for example, Cloud A takes 1'04"" seconds, DigitalOcean 1'39"" seconds, Rackspace 1'49"" seconds, and Microsoft Azure takes 2'35"" seconds to fire up a box and let you in with SSH. Rounding up the score on the slower end was Packet with 9'51"" seconds, noting that they were the only bare metal cloud provider in the mix.
Note: Since the last article about ‘Speed of VM creation and SSH access’ Cloud 66 has integrated with new cloud providers: Cloud A, a Canadian cloud provider and Packet, who offer bare metal cloud services. Both of these clouds will be included in this article for data capture purposes, but outside the context of providing a year-on-year comparison.
Table 1: The X axis represents the time in seconds and Y axis represents the cloud provider.
The data shows that Google Cloud is more than 4 times faster than Microsoft Azure or Rackspace in completing both processes. Table 1 does not include Packet, as it’s a bare-metal only cloud service and therefore naturally has longer processing times than virtual cloud counterparts.
Next, I evaluated the speed of VM creation separately from the time it took for the first SSH access to the server for each cloud vendor.
First, let’s look at the speed of VM creation
Table 2: A snapshot chart for VM creation. The X axis represents the time from March 2015-June 2016 and the Y axis the speed in seconds.
If we compare the total speed results (the time of creating a VM and the time it takes to grant the first SSH access) with just the speed results of the VM creation alone, we can notice a strong positive correlation. Google Cloud is the fastest and Microsoft Azure is the slowest amongst virtual clouds, followed by bare-metal cloud provider, Packet with 9'50"" seconds.
The time of the VM creation amongst virtual clouds varies from 15"" seconds to up to 2'35"" seconds depending on the cloud vendor. For instance, Google Cloud is 5 times faster than DigitalOcean and 9 times faster than Microsoft Azure in the VM creation process. The top three clouds with the fastest VM creation speed are Google Cloud (15""s), AWS (23""s) and Cloud A (37""s).
Now let’s check the time it takes for the first SSH access
Table 3: SSH access snapshot. The X axis represents the time from March 2015-June 2016 and the Y axis the speed in seconds.
It’s generally faster to establish the first SSH access than the time it takes to create a VM. Most cloud vendors take under 30"" seconds to allow the first SSH access. If we look at Vexxhost for example, it takes 43 seconds to create a VM but only 2"" seconds for the first SSH access. This gives Vexxhost an overall speed of 47"" seconds, placing them second amongst our cloud providers.
Although Google Cloud and AWS are the speed leaders in VM creation, they’re the slowest in granting SSH access for the first time out of all cloud vendors. However, this does not influence the overall result.
Lastly, although Packet takes 9'50"" seconds overall, it is the fastest cloud taking only 1 second for SSH access to occur.
Cloud vendors at Cloud 66 cover different regions in the world, therefore making it difficult to compare them like for like. Therefore, I’ll tackle regional comparisons by reviewing the following geographies: Europe, Asia and Oceania, and the Americas to find out which cloud provider works best in which region.
EUROPE
Six out of nine Cloud 66 cloud providers have data centres in Europe, including AWS, Microsoft Azure, Rackspace, Linode, Google Cloud and DigitalOcean.
From all of the Google Cloud regions, the West Europe-1-b region is the fastest, completing the VM creation and SSH access in 30"" seconds. This is closely followed by AWS Europe with 59"" seconds.
Linode is the fastest cloud vendor in the London region (1'03"" seconds). Other alternatives are DigitalOcean (1'17"" seconds) and Rackspace (1'52"" seconds).
DigitalOcean is present in the Amsterdam region, with the speed of VM creation and SSH access taking anywhere between 1'22"" seconds, to up to 1'47"" seconds.
Frankfurt was added as a new region by DigitalOcean (1'20"" seconds) and Linode (1'50"" seconds) at the end of last year. DigitalOcean came up the fastest in the region.
*Note: Packet just announced a new data centre in Amsterdam.
ASIA AND OCEANIA
Six out of nine Cloud 66 cloud providers have data centres in Asia and Oceania, including AWS, Microsoft Azure, Rackspace, Linode, Google Cloud and DigitalOcean.
Out of all Google Cloud regions, the Asian region has the slowest speed results (between 36"" to 46"" seconds). Nevertheless, Google Cloud is still a leader in the Asia region.
In Japan, the fastest cloud provider in AWS (Tokyo: 1'01"" second), followed by Linode (Tokyo: 1'38"" seconds) and Microsoft Azure (2'27"" seconds). For Microsoft Azure, Japan is the second fastest region they provide service for.
In Singapore, again AWS is the speed leader with 1'04"" seconds, followed by Linode (1'20"" seconds) and DigitalOcean (1'29"" seconds).
The fastest cloud provider in Australia is AWS (Sydney: 1'04"" seconds), then Rackspace (Sydney: 1'51"" seconds) and lastly Microsoft Azure with 2 data centres: Australia East (2'27"" seconds) and Australia South-East (2'28"" seconds).
AMERICAS
All nine cloud providers cover the Americas region, with the main focus on US, Canada and a small presence in South America.
US:
In the US Central region, Google Cloud is able to create VM and open SSH access from between 31"" and 38"" seconds, whereas Microsoft Azure is nearly 5 times slower (2'36"" seconds).
AWS covers the US East and West regions and completes both processes from between 53"" seconds to a minute. The US- East-1-b is the fastest region services by Google Cloud (30"" seconds).
DigitalOcean’s speed varies from between 1'27"" seconds to 2'26"" seconds in the New York region. Linode VM creation and SSH access is done in approximately a minute. You can also use bare-metal provider Packet in New York (9'51"" seconds).
Rackspace provides the quickest VM creation and SSH access in North Virginia (1'13"" seconds), followed by the Chicago region (2'03"" seconds), Dallas (2'09"" seconds).
Canada:
Cloud 66 partners with two Canadian cloud providers Cloud A and Vexxhost. Cloud A takes 1'03"" seconds to create VM and SSH access, whereas Vexxhost takes between 45' seconds to a minute. DigitalOcean also has one data center in Toronto that takes 1'35' seconds processing time.
South America:
AWS covers the South America East region, where the speed of VM creation and SSH access is 1'01"" second.
Table 4: The year-on-year cloud performance comparison. The X axis represents time in seconds, the Y axis represents cloud providers. The light grey bars represent the period from January 2014 — March 2015, and the dark grey bars represent the period from March 2015 — June 2016.
The rank of cloud providers hasn’t changed since last year and Google Cloud is still the fastest and Microsoft Azure is the slowest performance-wise. Interestingly, the majority of cloud providers are slower than the previous year by approximately 15"" seconds for Azure, 10"" seconds for AWS, DigitalOcean and Google Cloud. Linode and Vexxhost are quicker by up to 5"" seconds and there’s a significant speed improvement by approximately 20"" seconds from Rackspace.
Packet and Cloud A are not included in this table, as they were only added last year, and therefore we don’t have data covering the 2014/15 period.
The content of this blog post is based on the Cloud 66 internal data collected via our Birdseye metrics tool. Please feel free to contact us if you have any questions or any additional data which could be added to the data points in this post.
If you have any questions or any additional data, don’t hesitate to get in touch.
Originally published at blog.cloud66.com on June 7, 2016.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
10 
10 claps
10 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/ec2-instance-types-comparison-and-how-to-remember-them-e0a7eb34b5de?source=search_post---------314,"There are currently no responses for this story.
Be the first to respond.
AWS offers a range of EC2 instance types optimized for various purposes. It’s great that they provide so much variety, but of course, it means one more thing that you have to learn.
We broke this down in a new video, which also compares EC2 purchasing options. Check it out here:
Or, read on for a look into each instance type. Remember that within each type, you’ll still need to choose instance sizes for your specific needs. Additionally, older generations within each instance types are available for purchase — for example, c5 is the latest “c” instance, but c4 and c3 are still available — but as the newer types tend to perform better at a cheaper price, you’ll only want to use the older types if you have an AMI or other dependency.
This image shows a quick summary of what we’ll cover:
These general purpose EC2 instance types are a good place to start, particularly if you’re not sure what type to use. There are two general purpose types.
The t2 family is a burstable instance type. If you have an application that needs to run with some basic CPU and memory usage, you can choose t2. It also works well if you have an application that gets used sometimes but not others. When the resource is idle, you’ll generate CPU credit, which you’ll utilize when the resource is used. It’s a cheaper option that’s useful for things that come and go a lot, such as websites or development environments.
We’ll also add a mnemonic to help you remember the purpose of each instance type.
Mnemonic: t is for tiny or turbo.
The m5 instance type is similar, but for more consistent workloads. It has a nice balance of CPU, memory, and disk. It’s not hard to see why almost half of EC2 workloads are on “m” instances.
There’s also an m5d option, which uses solid state drives (SSD) for the instance storage.
Mnemonic: m is for main choice or happy medium.
The c5 instance type has a high ratio of compute/CPU versus memory. If you have a compute-intensive application — maybe scientific modelling, intensive machine learning, or multiplayer gaming — these instances are a good choice. There is also the c5d option, which is SSD-backed.
Mnemonic: c is for compute (at least that one’s easy!)
The r4 instance family is memory-optimized, which you might use for in-memory databases, real-time processing of unstructured big data, or Hadoop/Spark clusters. You can think of it as a kind of midpoint between the m5 and the x1e.
Mnemonic: r is for RAM.
The x1e family has a much higher ratio of memory, so this is a good choice if you have a full in-memory application or a big data processing engine like Apache Spark or Presto.
Mnemonic: x is for xtreme, as in “xtreme RAM” seems to be generally accepted, but we think this is a bit weak. If you have any suggestions, comment below.
If you need GPUs on your instances, p3 instances are a good choice. They are useful for video editing, and AWS also lists use cases of “computational fluid dynamics, computational finance, seismic analysis, speech recognition, autonomous vehicles” — so it’s fairly specialized.
Mnemonic: p is for pictures (graphics).
The h1 type is HDD backed, with a balance of compute and memory. You might use it for distributed file systems, network file systems, or data processing applications.
Mnemonic: h is for HDD.
The i3 instance type is similar to h1, but it is SSD backed, so if you need an NVMe drive, choose this type. Use it for NoSQL databases, in-memory databases, Elasticsearch, and more.
Mnemonic: i is for IOPS.
d2 instances have an even higher ratio of disk to CPU and memory, which makes them a good fit for Massively Parallel Processing (MPP), MapReduce and Hadoop distributed computing, and similar applications.
Mnemonic: d is for dense.
As AWS has continued to add options to EC2, there are now EC2 instance types for almost any application. If you have comparison questions around pricing, run them through the AWS monthly calculator. And if you don’t know, then generally starting with t2 or m5 is the way to go.
Originally published at www.parkmycloud.com on July 24, 2018.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
10 
10 claps
10 
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-aws-firecracker-makes-containers-and-serverless-more-efficient-27e7df165062?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 26, 2019·3 min read
AWS Firecracker was announced at AWS re:Invent in November 2018 as a new AWS open source virtualization technology. The technology is purpose-built for creating and managing secure, multi-tenant container and function-based services. It was described by the AWS Chief Evangelist Jeff Barr as “what a virtual machine would look like if it was designed for today’s world of containers and functions.”
Firecracker is a Virtual Machine Manager (VMM) exclusively designed for running transient and short-lived processes. In other words, it helps to optimize the running of functions and serverless workloads. It’s also an important new component in the emerging world of serverless technologies and is used to enhance the backend implementation of Lambda and Fargate. Firecracker helps deliver the speed of containers combined with the security of VMs. If you use Lambda or Fargate, you’re already receiving the benefits of Firecracker. However, if you run/orchestrate a large volume of containers, you should take a look at this service with optimization in mind.
AWS can realize the economic benefits of Firecracker by creating what they call “microVMs”, which allows them to spread serverless workloads around multiple servers thus getting a greater ROI from its investment in the servers behind serverless. In terms of customer benefit, using Firecracker enables these new microVMs to launch in 125 milliseconds or less, compared to the seconds (or longer) it can take to launch a container or spin up a traditional virtual machine. In a world where thousands of VMs can be spun up and down to tackle a specific workload, this will constitute a significant savings. And remember, these are fully fledged micro virtual machines, not just containers.The micro VM’s themselves are worth a closer look as each includes an in-process rate limiter to optimize shared network and storage resources. As a result, one server can support thousands of microVMs with widely varying processor and memory configurations.\
There is also the enhanced security and workload isolation only available from Kernel-based Virtual Machine (KVMs) — more secure than containers, which are less isolated. One particularly valuable security feature is that Firecracker is statically linked, which means all the libraries it needs to run are included in its executable code. This makes new Firecracker environments safer by eliminating outside libraries. Altogether, this offering and the combination of efficiency, security and speed created quite the buzz at the AWS re:Invent launch.
There are a few caveats related to the still novel aspects of the technology. In particular, compared to alternatives, such as containers or Hyper-V VMs, it is prudent to confine to non-production workloads as the technology is still new and needs to be more fully battle-tested for production use.
However, as confidence, adoption, and experience grow in the use of serverless technologies it certainly seems like Firecracker can offer a popular new method for provisioning compute resources and will likely help bridge the current gap between VMs and containers.
Originally published at www.parkmycloud.com on May 23, 2019.
CEO of ParkMyCloud
See all (317)
18 
18 claps
18 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/statuscode/the-top-10-serverless-links-of-2017-%EF%B8%8F-11d3b9ffa15c?source=search_post---------316,"There are currently no responses for this story.
Be the first to respond.
2017 saw the arrival of Statuscode’s latest newsletter Serverless Status. Curated by Raymond Camden, the newsletter covers the latest updates on serverless architectures and paradigms.
Here’s a run through of what subscribers of Serverless Status were reading in 2017.
Rafal Gancarz
A roundup of key Serverless computing platforms, tools and frameworks available.
Featured in May’s Issue 4
yaron haviv
Nuclio is a framework, written in Go, for high-performance Serverless real-time event and data processing.
Featured in October’s Issue 26
Mark Boyd
“Startups are building next generation stacks combining GraphQL, cloud-based storage, and edge service providers to deliver their sites. The next goal? To make the whole architecture model serverless.”
Featured in September’s Issue 20
Matt Billock
A performance and usability comparison of three function-as-a-service providers: AWS Lambda, Google Cloud Functions and Microsoft Azure Functions.
Featured in August’s Issue 15
Amazon
A practical five module online workshop from Amazon that focuses on AWS Lambda, unsurprisingly, and covers areas like user management and RESTful APIs.
Featured in July’s Issue 11
Randall Degges
A well made argument that webhooks are more often the right way to go for situations where customers are extending upon your service.
Featured in October’s Issue 24
Hacker News
A Hacker News thread with over 100 comments answering the question: ‘How was your experience with AWS Lambda (Amazon’s serverless platform) in production?’
Featured in June’s Issue 8
Peter Sbarski
A cool calculator to provide estimates for Serverless usage across Lambda, Azure Functions, Google Cloud Functions, and OpenWhisk.
Featured in June’s Issue 9
John Chapin
Featured in July’s Issue 10
Node.js YouTube Channel
A walkthrough of one company’s entire Serverless stack, with a heavy focus on the ‘why’.
Featured in May’s Issue 2
Many thanks for reading! If you enjoyed this roundup of 2017 in Serverless be sure to give it a clap. 👏
If you would like free weekly updates on the latest Serverless news you can subscribe here to Serverless Status or follow our Serverless Twitter account.
Keeping developers informed.
89 
89 claps
89 
Written by
Editorial Coordinator at Cooper Press
Keeping developers informed.
Written by
Editorial Coordinator at Cooper Press
Keeping developers informed.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/the-abc-of-devops-implementation-with-dockers-and-containerization-d62eb8895fff?source=search_post---------317,"There are currently no responses for this story.
Be the first to respond.
DevOps is a rage in the IT industry. As per Wikipedia’s definition, DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops), which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. DevOps, The primary reason for the popularity of DevOps, is that it allows enterprises to develop and improve products at a quicker pace than traditional software development methods.
As our ever-changing work environment is becoming more fast-paced, the demand for faster delivery and fixes in the software development market is on the rise. Thus, the need for the production of high-quality output in a short period with limited post-production errors gave birth to DevOps.
As we have discussed on one of our blog the importance of shifting to a DevOps way of software development, we now change the conversation to containerization, which is an accessible technology that is frequently being used to make the implementation of DevOps smoother and more convenient. Containerization is a technology that makes it easier to follow the DevOps practice. But what exactly is containerization? Let’s find out!
Containerization is the process of packaging an application along with its required libraries, frameworks, and configuration files together so that it can be run in various computing environments efficiently. In simpler terms, containerization is the encapsulation of an application and its required environment.
It has lately been gaining lots of traction as it overcomes the challenges that stem from running virtual machines. A virtual machine emulates an entire operating system inside the host operating system. It requires a fixed percentage of hardware allocation that goes into running all the processes of an operating system. And this, therefore, leads to unnecessary wastage of computing resources due to significant overhead.
www.datadriveninvestor.com
Also, setting up a virtual machine takes time, and so does the process of setting up a particular application in every virtual machine. This results in a significant amount of time and effort being taken up in just setting up the environment. Containerization, popularized by the open-source project ‘Docker,’ circumvents these problems and provides increased portability by packaging all the required dependencies in a portable image file along with the software.
Let us dive deeper into containerization, its benefits, how it works, ways of choosing the tool for containerization, and how it trumps the usage of virtual machines (VMs).
Some popular container providers are:
Docker has become a popular term in the IT industry, and rightly so. Docker can be defined as an open-source software platform which offers a simplified way of building, testing, securing, and deploying applications within containers. Docker encourages software developers to collaborate with cloud, Linux, and Windows operating systems for easy and faster delivery of services.
Docker is a platform that provides containerization. It allows for the packaging of an application and its dependencies into a container, thereby, helping to ease the development and accelerate the deployment of the software. It helps maximize output by doing away with the need to replicate the local environment on each machine on which the solution is supposed to be tested, thus saving valuable time and effort that would go into the furthering of the progress.
Docker file can be quickly transferred and tested among the workers. The process of container image management is also made simple by Docker and is quickly revolutionizing the way we develop and test applications at scale.
Let’s find out why containers are slowly becoming an integral part of the standard DevOps architecture.
Docker has popularized the concept of containerization. Applications in Docker containers have the capability of being able to run on multiple operating systems and cloud environments such as Amazon ECS and many more. Hence, there is no technology or vendor lock-in.
Let us understand the need for implementing DevOps with containerization.
Initially, software development, testing, deployment, and the supervising required were undertaken one after another in phases, where completion of one phase would lead to the beginning of another.
DevOps and Docker image management technologies, like AWS ECR, have made it easy for software developers to perform IT operations, share software, and collaborate, and enhance productivity. Apart from encouraging developers to work together, they are successful in eliminating the conflict of different work environments that affected the application previously. To put it simply, containers, being dynamic in nature, allow IT professionals to build, test, and deploy pipelines without any complexities while, at the same time, bridging the gap between infrastructure and operating system distributions, which sums up the DevOps culture.
Containers benefit software developers in the following ways:
Elucidated below are the steps to be followed to implement containerization successfully using Docker:
Several companies are opting for containerization for the various benefits it entails. Here’s a list of advantages you will enjoy by using containerization technology:
Containerization packages the application along with its environmental dependencies, which ensures that an application developed in one environment works in another. This helps developers and testers work collaboratively on the application, which is precisely what DevOps culture is all about.
Containers can be run on multiple cloud platforms like GCS, Amazon ECS (Elastic Container Service), Amazon DevOps Server.
Containers offer easy portability. A container image can be deployed to a new system easily, which can then be shared in the form of a file.
As environments are packaged into isolated containers, they can be scaled up faster, which is extremely helpful for a distributed application.
In the VM system, the bare-metal server has a different host OS from the VM. On the contrary, in containers, the Docker image can utilize the kernel of the host OS of the bare-metal physical server. Therefore, containers are comparatively more work-efficient than VMs.
Containerization makes maximum utilization of computing resources like memory and CPU, and utilize far fewer resources than VMs.
With the quick spinning of apps, the delivery takes place in less time, making the platform convenient for performing more development of systems. The machine does not need to restart to change resources.
With the help of automated scaling of containers, CPU usage, and machine memory optimization can be done, considering the current load. And unlike the scaling of Virtual Machines, the machine does not need to be restarted to modify the resource limit.
As containers provide process isolation, maintaining the security of applications becomes a lot more convenient to handle.
Containerization is advantageous in terms of supporting multiple containers on a singular infrastructure. So, despite investing in tools, CPU, memory, and storage, it is still a cost-effective solution for many enterprises.
A complete DevOps workflow, with containers implemented, can be advantageous for the software development team in the following ways:
A Virtual Machine has the capability to run more than one instance of multiple OS’s on a host machine without overlapping. The host system allows the guest OS to run as a single entity. A docker container does not burden the system as much as a virtual machine, as running an OS requires extra resources, which can reduce the efficiency of the machine.
Docker containers do not tax the system and use only the minimum amount of resources required to run the solution without the need to emulate an entire OS. Since fewer resources are needed to run the Docker application, it can allow for a larger number of applications to run on the same hardware, thereby cutting costs.
However, it reduces the isolation that VMs provide. It also increases homogeneity because if an application runs on Docker on one system, then it will run without any hiccups on Docker on other systems as well.
Both containers and VMs have the virtualization mechanism. But for containers, the virtualization of the Operating System takes place, while in the latter, the virtualization of the hardware takes place.
VMs show limited performance, while the compact and dynamic containers with Docker show advanced performance.
VMs require more memory, and therefore have more overhead, making them computationally heavy as compared to Docker containers.
Some of the commonly-used Docker terminologies are as followed:
A service is created with Docker, and then it is packaged into a container image. A Docker image is a virtual representation of the service and its dependencies.
An instance of the image is used to create a container that is made to run on the Docker host. The image is then stored in a registry. A registry is needed for deployment to production orchestrators. Docker Hub is used to store it in its public registry at a framework level. An image, along with its dependencies, is then deployed into one’s choice of environment. It is important to note that some companies also offer private registries.
A business organization can also create their private registry to store Docker images. Private registries are provided if images are confidential, and the organization wants limited latency between an image and the environment where it is deployed.
Docker image containers or applications can run locally on Windows and Linux. This is achieved simply by the Docker engine interfacing with the operating system directly, making use of the system’s resources.
For managing clustering and composition, Docker provides Docker Compose, which aids in running multiple container applications without overlapping each other. Developers further connect all the Docker hosts to a single virtual host through the Docker Swarm Mode. After this, the Docker Swarm is used to scale the applications to a number of hosts.
Thanks to Docker Containers, developers have access to the components of a container, like application and dependencies. The developers also own the framework of the application. Multiple containers on a singular platform, and depending on each other, are called Deployment Manifest. In the meantime, however, the professionals can pay more attention to choosing the right environment for deploying, scaling, and monitoring. Docker helps in limiting the chances of errors, that can occur during transferring of applications.
After the completion of the local deployment, they are further sent to code repository like Git repository. The Docker file in the code repository is used to build Continuous Integration (CI) pipelines that extract the base container images and build Docker images.
In the DevOps mechanism, the developers work on the transferring of files to multiple environments, while the managerial professionals look after the environment to check defects and send feedback to the developers.
It is always a good idea to anticipate the future and prepare for scalability post deciding upon the requirements of a project. With time, the project gets more complex, and therefore, it is necessary to implement large scale automation and offer faster delivery.
Containerized environments, being dense and complex, require proper handling. In this context, PaaS solutions can be adopted by software developers to focus more on coding. There are multiple choices when it comes to selecting the most convenient platform that offers better and advanced services. Hence, determining the right platform for an organization based on its application is quite taxing.
To make it easy for you, we’ve laid down some of the parameters to be considered before choosing the best platform for containerization:
For smooth performance, it is important to hand-pick a platform that can be adjusted or altered easily and automated depending on the nature of the requirements.
Being mostly proprietary in nature, PaaS solution vendors have the tendency to lock you into one infrastructure.
Choose a platform that has a wide range of in-built tools along with third-party integrated technologies for encouraging the developer to make way for further innovation.
While choosing the right platform, it is crucial to find one which supports private, public, and hybrid cloud deployments, to cope with the new changes.
As it is natural to pick a containerization platform that can support long-term commitments, it is important to know what pricing model is offered. There are plenty of platforms that offer different pricing models at different scales of operations.
Another crucial aspect to keep in mind is that containerization does not happen overnight. The professionals need to invest their time in restructuring the architectural infrastructure. They should be encouraged to run micro-services.
To shift from the traditional structure, large applications need to be broken down into small parts that are further distributed into multiple connected containers. It is recommended, therefore, to hire experts who can put in the required efforts towards finding a convenient solution to handle both Virtual Machines and containers on a singular platform, as making an organization completely dependent on containers takes time.
When it comes to modernization, legacy IT apps should not be ignored. With the help of containerization, IT professionals can reap the benefits of these classic apps for proper utilization of investment in legacy frameworks.
Make the most of containerization by running more than one application on container platforms. Invest in new applications at minimal cost and modify each platform by making it friendly for both currents as well as legacy apps.
As a containerized environment has the capability to change quicker than the traditional environment, it has some significant security risks. The agility can benefit the developers by offering fast access. However, it will fail in its task if the required level of security is not ensured.
A major one, encountered while dealing with containers, is that handling container templates packaged by third-party or untrusted sources can be very risky. It’s, therefore, better to verify a publicly available template before using it.
An organization needs to enhance and integrate its security processes for the hassle-free development and delivery of apps and services. With the modernization of platforms and applications, security should be a significant priority for an enterprise.
To keep pace with the ever-changing IT industry, the professionals should keep on striving for better, and therefore, utilize new tools available in the market to enhance security.
This marks the conclusion of Part 2! In Part 3, we’ll talk about the Key DevOps tools & Implementation strategy of DevOps.
empowerment through data, knowledge, and expertise.
144 
144 claps
144 
Written by
We are a digital product development company and your guide on the digital transformation journey.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
We are a digital product development company and your guide on the digital transformation journey.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@pandeyparul/h2o-ai-hybrid-cloud-democratizing-ai-for-every-person-and-every-organization-8ebe770f15e8?source=search_post---------318,"Sign in
There are currently no responses for this story.
Be the first to respond.
Parul Pandey
Apr 17, 2021·6 min read
Harnessing the true potential of AI by enabling every employee, customer, and citizen with sophisticated AI technology and easy-to-use AI applications.
Democratization is an essential step in the development of AI, and AutoML technologies lie at the heart of it. AutoML tools have played a pivotal role in transforming the way we consume and understand data. Given the impact that data and predictive analytics can have in addressing critical problems, it becomes imperative to make the power of AI available to a wide variety of users in an organization. This is essential to address day-to-day needs and deliver greater insights. At H2O.ai, we have outlined a six stepped approach to help enterprises make this happen.
At the core of this approach lies automation, explainability, flexibility, and scalability. Having a carefully carved out strategy is imperative to the success of AI adoption in an enterprise. At H2O.ai, we're here to help you get started and determine how H2O AI Hybrid Cloud can solve your organization's specific challenges.
H2O AI Hybrid Cloud is an end-to-end platform that enables organizations to rapidly build world-class AI models and applications. H2O hybrid cloud harnesses the true potential of AI by enabling every employee, customer, and citizen with sophisticated AI technology and easy-to-use AI applications. There are no user limitations. Infact, it has been developed with a mission to empower everyone in the organization to innovate and use AI to make better decisions, streamline operations, reduce risk, and personalize customer experiences. H2O AI Hybrid Cloud integrates with all major clouds, operating systems, and Python packages, enabling easy integration to virtually any existing data science stack.
To understand the usefulness of the H2O AI Hybrid cloud, let’s look at its various components in detail with the help of an example. To put things into perspective, let’s look at the customer churn application which has been hosted on the H2O AI AppStore.
The customer churn application is an easy-to-use front-end for real-time predictions of a machine learning churn model. The goal of this application is to empower the call center representatives so they can make more informed decisions when talking to their customers. Whenever a customer calls a call center, the app displays in real-time the information about that customer i.e., whether or not they’re likely to churn and why. This would provide necessary information to the call center representative to understand the state of mind of the customer, beforehand.
This demo application is built on the Kaggle Customer Churn dataset and is hosted on the H2O AI AppStore. The dataset provides important details about each customer i.e how much they are paying for service, their demographic details, and also who’s left the company and who has not. The predictive model driving this application is built using Driverless AI and then productionalized with MLOps to be consumed by the application. The following section will walk through the end-to-end process in a bit more detail.
The customer churn and all the other applications are powered by state-of-the-art automated machine learning from H2O-3 and Driverless AI.
H2O’s AutoML supports tabular, time series, text, audio, image, and video data all in one platform. There is support for all the popular ML libraries, including H2O-3, XGBoost, Tensorflow, and PyTorch, as well as over 100 pre-coded experiments and recipes. The AutoML automatically selects the best algorithm, tunes the model, and provides data scientists and developers with a model leaderboard to continually challenge and test the champion models.
Here is a video that goes through the entire process of creating a predictive model in Driverless AI using a publicly available churn dataset. At the end of the entire experiment, Driverless AI delivers the single best model or an ensemble of models depending on the use case.
As can be seen in the demo above, Driverless AI saves our data scientists a lot of time which can then be used for other tasks like explaining the model predictions to stakeholders. Explainability is a vital component of AI, and we shall cover it in the next section.
H2O.ai has built one of the most extensive suites of capabilities for reviewing the machine learning models after they have been developed, such as Shapley Values, K-Lime, Surrogate Decision Trees, Reason Codes, Partial Dependency Plots, Disparate Impact Analysis, Exportable Rules-Based Systems, and more.
The machine learning interpretability(MLI) module equips the data scientists to understand how the model makes decisions under the hood, allowing them to better trust the results. For instance, the Shapley values can show the contribution of each feature towards the overall predictions. On the other hand, the partial dependency plots(PDP), allow the user to see how predictions would change if one specific feature changed. For instance, in the churn example demonstrated below, the average prediction goes from unlikely to churn to likely to churn as the payment amount increases..
H2O.ai has developed a rich ecosystem of MLOps and model management capabilities to get models into production faster and keep them there. H2O AI Hybrid Cloud offers complete capabilities to deploy, monitor, test, explain, challenge, and experiment with real-time models in production. H2O’s MLOps technology enables users to watch in real-time how data and predictions are changing as well as monitor alerts and risk flags as they occur.
H2O Wave is an open-source Python development framework that makes it fast and easy for data scientists, machine learning engineers, and software developers to develop real-time interactive AI apps with sophisticated visualizations. H2O Wave accelerates development with a wide variety of user-interface components and charts, including dashboard templates, dialogs, themes, widgets, and many more.
The H2O AI AppStore is a front-end for users to browse, use and share AI apps to solve their specific use cases. The AppStore consists of data science best practice apps curated by the H2O Kaggle GrandMasters team, and the industry vertical apps built by the makers at H2O.ai.
The AI hybrid cloud runs internal to a company allowing each organization to have their own app store with the apps most important to them. The engineers and data scientists of an organization can build their own applications specific to their needs and requirements.
In this article, we went into detail over the capabilities of the H2O Hybrid Cloud via a use case. We built a model using Driverless AI, looked at how MLOps can host productionalized models for us, and then used these models within a wave application. As creators of technology, our focus has always been to create something that is both efficient, easy to use as well as intuitive. At H2O.ai, we tend to focus our design principles on the human benefit of AI, explainability, and trust and this is clearly reflected in every aspect of the H2O hybrid cloud.
Originally published here.
Data Science @H2O.ai | Working at the intersection of product, community and developer advocacy.
11 
11 
11 
Data Science @H2O.ai | Working at the intersection of product, community and developer advocacy.
"
https://pub.towardsai.net/how-i-passed-my-associate-cloud-engineering-exam-in-three-months-90b85e6c3da1?source=search_post---------319,"I took my Associate Cloud Engineering exam recently, and I passed on my first try. I prepared for three months, and before…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.devgenius.io/nokia-moves-data-centers-to-google-cloud-70dd0458ee84?source=search_post---------320,"There are currently no responses for this story.
Be the first to respond.
Finland based telecommunications company, Nokia announced that the company signed a 5-year strategy agreement with Google under which Nokia will move its on-premises IT infrastructure to Google Cloud. Nokia has started moving its global data centres, servers and software applications onto Google Cloud, a process that’s expected to continue for the next 18 months to 24 months. Nokia expects complete migration by Fall…
"
https://medium.com/@alibaba-cloud/big-data-application-case-study-technical-architecture-of-a-big-data-platform-fb430686a2c7?source=search_post---------321,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 23, 2017·11 min read
Abstract: How should we design the architecture of a big data platform? Are there any good use cases for this architecture? This article studies the case of OpSmart Technology to elaborate on the business and data architecture of Internet of Things for enterprises, as well as considerations during the technology selection process.
How should we build the architecture of a big data platform? Are there any good use cases for this architecture? This article studies the case of OpSmart Technology to elaborate on the business and data architecture of the Internet of Things for enterprises, as well as considerations during the technology selection process.
Based on the ""Internet + big data + airport"" model, OpSmart Technology provides wireless network connectivity services on-the-go to 640 million users every year. As the business expanded, OpSmart technology faced the challenge of increasing amounts of data. To cope with this, OpSmart Technology took the lead to build an industry-leading big data platform in 2016 with Alibaba Cloud products.
Below are some tips shared by OpSmart Technology’s big data platform architect:
Business architecture
OpSmart Technology's business architecture is shown in the figure above. Our primary business model is to collect data through our own devices, explore value in the data, and then apply the data to our business.
On the data collection layer, we founded the first official Wi-Fi brand for airports in China, ""Airport-Free-WiFi"", covering 25 hub airports and 39 hub high-speed rail stations nationwide and providing wireless network services on-the-go to 640 million people each year. We also have the nation's largest Wi-Fi network for driving schools and our driving school Wi-Fi network is expected to cover 1,500-plus driving schools by the end of 2017. We are also the Wi-Fi provider of China's four major auto shows (Beijing, Shanghai, Guangzhou, and Chengdu) to serve more than 1.2 million people. In addition, we are also running the Wi-Fi network for 2,000-plus gas stations and 600-plus automobile 4S (sales, spare parts, service, survey) stores across the country.
On the data application layer, we connected online and offline behavioral data for user profiling to provide more efficient and precise advertisement targeting including SSP, DSP, DMP and RTB. We also worked with the Ministry of Public Security to eliminate public network security threats.
OpSmart Technology's big data and advertising platforms also offer technical capabilities for enterprises to help them establish their own big data platforms and improve their operation management efficiency with a wealth of quantitative data.
We abstracted our data architecture, which contains a number of themes as shown in the figure. The subject in the figure can be understood as users, and the object can be understood as things. The subject and object are connected through various forms. Such connections are established in time and space and are completed through computer and telecommunication networks. The subject has its own reflection in the connection network, which can be understood as a virtual identity (Avatars). The object also has its own reflection in the connection network, such as the Wikipedia description of a topic, or a commercialized product or service. These reflections are then packaged by advertisements as an advertising image. All these are object mirrors. The interaction between the subject and the object is actually the interaction between the subject image and the object image, and such interactions leave traces in both time and space.
The individual and group characteristics of the subject and object, as well as the subject-object relationships, all constitute big data. Through in-depth mining and learning, this information will give birth to powerful insights and have immeasurable value to businesses.
The data sizes of OpSmart Technology in the subject domain and the interaction domain are as follows:
Next let's move to our ideas about technology selection. I think that there is no best technical architecture, only the most appropriate architecture for our applications. Successful IT planning means starting from the business structure and providing the most appropriate technical architecture for each specific business scenario.
First, let’s take a look at our functional requirements. Take our advertising business for example. Our goal is to handle 10 billion messages a day. The requirements for big data capabilities are as follows:
Let’s assume that the record size is 2 KB and 70 PB of physical capacity is required to accommodate the data. We can then infer based on the query range requirements that the offline computing processing duration is 24 hours and online calculation duration is 10 minutes.
· We hope to outsource the infrastructure installation and O&M through the cloud platform.
· Big data technology is changing, and we hope component versions can be updated quickly.
· The external business environment is changing rapidly and it is hoped that computing resources can be dynamically increased or decreased to save costs.
· We hope to acquire professional security services at a lower cost.
· We anticipate more use of open-source components to facilitate overall output.
We finally settled on Alibaba Cloud, especially its E-MapReduce products, after comprehensive inspection of domestic cloud service providers. The cluster is ready shortly after purchase, and Hive, Spark, HBase, and other open-source big data components are available immediately.  First, we had to select the data storage engine.
We evaluated the performance and prices of each option taking storing 25 TB of data as the benchmark. We can see from the figure that, for offline analysis, you can consider the Hive on OSS mode to use open-source components to store the data of the past year. For online analysis scenarios, using HBase to store data of the last three months can guarantee high cost effectiveness. This solution also enables joint queries in multiple tables, but the SQL query responses are situational and SQL queries with different degrees of complexity may have different response times. If you want a consistent response time, consider the index-based solution, that is, Log Service. However, the shortcoming of using Log Service is the lack of joint queries in multiple tables. If you want to use open-source components, build ELK on the ECS on your own.
Next we chose the query engine. We used a benchmark SQL to facilitate horizontal comparisons of response time. The benchmark SQL statement is shown in the figure below:
Based on the finding, we concluded that using HBase-based Phoenix for interactive queries delivered a satisfactory response cycle.
That's it for the technology selection part. Now let's look at the big data platform technical architecture.
The figure shows the overview of the technical architecture of the big data platform. In the figure, almost all of the services and features are implemented through Alibaba Cloud products, and the development test environment is also based on Alibaba Cloud's ECS. We can see from the figure that we do not need to worry about the data room power supply, network, virtualization, hard disk maintenance, and other infrastructure problems. By utilizing the cloud platform, we can put more focus on our own business.
The specific Alibaba Cloud products used in our architecture are summarized as follows:
Alibaba Cloud's E-MapReduce is the core product of our big data platform, which covers Hive, Spark, HBase, Storm, and other core open-source components in the big data field, as well as industry-leading query engines such as Phoenix and Presto. The Zeppelin, Hue, and other interactive components are also out-of-the-box software.  E-MapReduce has frequent new version releases and its components are also constantly updated. But purchased E-MapReduce is not conveniently upgradable. To update in a timely manner, we chose a monthly subscription rather than an annual subscription. After the monthly resources expire, we directly purchase new resources to upgrade them, and the old resources will automatically be destroyed if not renewed. Alibaba's E-MapReduce supports increasing the number of nodes but does not allow reductions. Following the above rolling mode, we can also adjust the cluster size and various configurations at any time.  The above-mentioned rolling mode is feasible for the computing cluster. But what about data storage? The machines used for E-MapReduce all have high configurations and will be a waste if used only to store data. Data can be stored in the OSS and loaded with Hive. However, you still need to store data on E-MapReduce to use HBase. Once you put the data on E-MapReduce, the cluster cannot be destroyed at will. Therefore, we separated the data cluster and computing cluster so that the computing cluster can be destroyed and upgraded at any time, while the data cluster is guaranteed to stably provide services over the long term. These two clusters have different configurations. The computing cluster uses an SSD to achieve faster processing, while the data cluster (HBase) uses ultra cloud disks to achieve a larger capacity.  Then in what scenarios is the pay-as-you-go option used? According to our calculation, if the computing duration is longer than seven days, it would be more cost-effective to purchase monthly subscription clusters directly. Pay-as-you-go clusters can be used for temporary bursts of computing tasks.
Ticket service is the most attractive reason for us to choose Alibaba's cloud services. Our O&M teams often encounter complicated issues requiring urgent solutions. The team members can then conveniently open a ticket to ask the Alibaba's engineers for help. The process of communication on the issue also allows us to learn new things. We have learned a lot from Alibaba Cloud engineers.
Based on the technical overview, the software design in our technical architecture is as follows:
Some of our implementations are summarized as follows:
Server Load Balancer We activated external network access for many ECS servers to facilitate management, but the actual usage rate is not high. The cost of external network bandwidth took up a large part of the ECS cost. Now we have disabled the external bandwidth for all ECS servers and route the traffic through the Server Load Balancer. The Server Load Balancer's external bandwidth is shared among all ECS servers. Requests to ports of all apps including the SSH ones are forwarded by the Server Load Balancer. The bandwidth of the Server Load Balancer is unlimited. The speed is faster and the cost is lower. We think using Server Load Balancer this way was clever.
ECS Our business environment changes fast. Some machines that are useful today may become useless tomorrow. We adopted the monthly subscription + automatic renewal model to increase or decrease machines at any time to scale the configuration.
ONS This is Alibaba's Log Service which is called MQ inside Alibaba. It has a fast response, with high throughput. It can be applied to highly real-time scenarios such as real-time bidding.
Log Service The Log Service contains LogTail, LogStore, LogHub, LogShipper and LogSearch services. Among them, the LogShipper feature is very helpful as it automatically sends collected logs to the OSS so that you can directly load the data using Hive. However, this feature currently only supports the JSON format and Parquet.
Spark Despite the fact that official examples and Alibaba Cloud documentations are based on Scala, we chose Java for Spark app development, as it is more convenient for our development team. If Java 8 is supported, the functional programming, especially Lambda expressions, will be very close to Scala in terms of performance. As per our advice, Alibaba Cloud's new version of E-MapReduce already supports Java 8.  It is worth noting that it does not matter if your data is stored in MaxCompute. E-MapReduce provides the SparkSQL service to enable seamless access to data in MaxCompute. MaxCompute users can also join the Spark ecosystem.
Storm E-MapReduce currently provides the Storm component. If you require this component, you have two options: consume data in the Log Service, or install Kafka on E-MapReduce following the pilot operations to support adding nodes.
OSS Object Storage Service (OSS) is used for storage and achieves the separation of computing and storage by combining with E-MapReduce.
Zeppelin This is really great. With it, business specialists can use HiveQL, SparkSQL, Phoenix, or Presto in a web form to perform exploratory and interactive queries of data, without programming or SSH logon. In addition, the query history can be saved and a simple bar chart or pie chart can also be generated. Our DMP engineers no longer have to write code overnight just to implement a statistical query of a specific figure, and business specialists can get things done independently.
Phoenix HBase itself is a NoSQL database, and structured query is its weakness. We have a lot of OLAP requirements to deliver interactive results. Our original practice was to create our own HBase secondary indexes and perform jump queries for non-primary key fields. Later we found that Phoenix has had this feature ready for us on E-MapReduce. The HBase index table generated by its index mechanism is just the index table we originally created manually. So we fully switched to Phoenix for interactive queries. The default query timeout value of Phoenix on the old version of E-MapReduce is one minute, which was too short for us. But if we change the parameter, we have to reboot the service. As per our advice, Phoenix in the new version of E-MapReduce now has a default timeout value of half an hour.
Batch calculation: LogTail + LogHub + LogShipper + OSS + Hive + SparkSQL Batch calculation focuses on data collection. LogTail configures the collection rules, LogShipper automatically delivers the data to the OSS, Hive directly loads the data to form a data warehouse, and SparkSQL enables direct query of data in Hive on the Zeppelin interface. The entire ETL process is very smooth, with almost no coding effort required.
Interactive calculation: LogTail + LogHub + Storm + HBase + Phoenix For OLAP services with more stringent response time requirements, we can build an OLAP database with HBase as the center. In order to shorten the available data cycle, we can open a separate channel. We use LogTail to collect data and synchronize the data in LogHub to Storm to use Storm to convert data and write data to the HBase, and then we use Phoenix for queries on the Zeppelin interface.
Real-time calculation: Servlet + ONS + Spark Streaming + Redis For real-time bidding and other real-time computing businesses, we can take full advantage of ONS's ultra-fast response (within 1 ms) and high concurrency features, use Spark Streaming for computing and finally store the data in Redis.
Spark 2.0 was released, Hadoop 3.0 released Alpha, and HBase 2.0 released SNAPSHOT. Many features in these components are highly anticipated. We will pay close attention to E-MapReduce new releases of Alibaba Cloud, in hopes to try out the new open-source components soon.
To learn more about Alibaba Cloud E-MapReduce, click: https://www.alibabacloud.com/product/e-mapreduce
Author: Ai Jia, graduated from the Software Engineering major of Tsinghua University. Previously in Accenture and IBM, Ai is now a big data platform architect in OpSmart Technology.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
17 
17 claps
17 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/how-to-make-your-aws-cloudwatch-data-work-for-you-to-reduce-costs-1783b41dd8f3?source=search_post---------322,"There are currently no responses for this story.
Be the first to respond.
AWS CloudWatch is Amazon Web Services’ primary monitoring tool for your cloud environment. Whether you are aware of it or not, if you use AWS, your data is being collected in CloudWatch, on more metrics than you probably know what to do with. With a small amount of effort, however, you can make this data work for you to reduce costs, automatically.
First of all, is AWS CloudWatch collecting data about your utilization? Almost certainly, the answer is yes. As an example, here are some of the metrics that are collected by default for EC2:
Full list here.
Like many AWS services, AWS CloudWatch has a free tier that covers the needs of many applications. You’ll need to pay more for custom metrics; extra dashboards, alarms, and logs; and custom events.
Also worth keeping in mind is that data is kept historically, based on the data resolution. Data points with a period of 60 seconds are kept for 15 days, although shorter periods are kept for as short as 3 hours, and longer for up to 15 months.
What to do with all this data? First, set up your AWS CloudWatch dashboard(s)and create alerts on the metrics that are important to you. The next step is to use this data for automated optimization of your environment.
Most organizations using public cloud are wasting thousands or even hundreds of thousands of dollars on cloud resources they’re not actually using. Even if you’re aware of overspend, you may not think you have the time or bandwidth to address the issue. With automation, integrating cost control into your daily processes can be straightforward.
One ParkMyCloud customer, Kurt Brochu of Sysco Foods, once told us, “To me, the magic is that the platform empowers the end user to make decisions for the betterment of the business.” His team has achieved a lifetime ROI of 1400% using ParkMyCloud. AWS data and ParkMyCloud’s automation capabilities empower his users to identify what spend is necessary, and what can be optimized.
AWS data and automation capabilities empower end users to identify what spend is necessary, and what can be optimized.https://bit.ly/2X4U5ze @ParkMyCloud
Learn how — by hearing from Kurt directly!
The webinar will give you an understanding of:
Reserve Your Spot Now
Hope to see you there!
Originally published at www.parkmycloud.com on June 11, 2019.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
28 
28 claps
28 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/dadi/from-zookeeper-to-consul-3ae5338b1181?source=search_post---------323,"There are currently no responses for this story.
Be the first to respond.
The DADI decentralized network consists of three main layers:
• Stargates: authoritative DNS and security layer• Gateways: application gateway layer• Hosts: application host layer
The services that make up these layers need to be able to discover each other, make decisions based on service metadata, and collectively manage the state of the network as a whole.
The DADI network started life with Apache ZooKeeper at its heart, making use of its hierarchical tree based key value storage. ZooKeeper is a centralized key-value storage service trusted by the likes of Rackspace, Yahoo and eBay. Services would create ephemeral “ZNodes”, or key value pairs, in directories relating to their state. For example, a Gateway that is waiting for more Hosts would sit in `gateways/awaiting_hosts` and Hosts looking for a Gateway would iterate over this directory, attempting to connect to an available Gateway.
If a service disconnected, its associated key value pairs would disappear. If the service’s state changed, the previous key value pair would be removed (`hosts/awaiting_gateway/<uuid>`), and a new key value pair would be created (`hosts/live/<uuid>`).
In addition to the location of the key value pairs representing state (`<service>/<state>/<uuid>`), each key value pair would also hold service metadata, consisting of information such as geolocation, user applications, and public IP address etc.
This worked well at a low request level, but while ZooKeeper is known for its fast read operations, it’s not so pacy when it comes to write operations. When scaling out to thousands of services where those services need to change their states, update their metadata and discover other services on a regular basis, the performance needed just couldn’t be guaranteed.
It’s worth noting that we also encountered a number of glitches in ZooKeeper which were extremely hard to investigate owing to the lack of management tools available.
We worked with a number of alternatives to ZooKeeper, the best of which were [etcd](https://en.wikipedia.org/wiki/Container_Linux_by_CoreOS#ETCD) from CoreOS and [Consul](https://en.wikipedia.org/wiki/Consul) from Hashicorp. While etcd offers a lot of the features required for the DADI network, Consul was the stand out choice.
Service discovery is conspicuous by its absence in ZooKeeper. Picture this for a moment: you have Stargates running across every continent on Earth, Gateways running in every region, Hosts that number in the hundreds of thousands dotting the globe like streetlights, and among this living network, innumerable websites & web services running in harmony.
You type `www.cat.blog` into your browser and press return. Your machine attempts to resolve this domain name to an IP, and receives a list of IPs for the authoritative DNS servers — our Stargate layer — which it then queries for the DNS record for `www.cat.blog`.
At this point, the Stargate service needs to return a single IP address for a Gateway. It needs to be able to quickly and efficiently determine which Gateways are running the `cat-blog` application, and retrieve enough information to make an intelligent decision as to which IP address to return.
Using ZooKeeper would require either an iterative approach to reading Gateway key value pairs or a more intelligent data structure layered on top of this. DADI Stargate would need to iterate through each Gateway entry, checking application status and geographic metadata amongst other things. This isn’t a solution that will scale well.
Consul has service discovery built in. Each service, whether it is Stargate, Gateway or Host, will register itself with Consul. The metadata will be stored in a key value pair (`gateways/<uuid>`) and its state will be associated with the service registration in the form of a _tag_, which can be used later when querying the service registry.
So let’s see how Stargate would find the correct Gateway. We’ve typed `www.cat.blog` into our browser and pressed return. Our machine has queried a Stargate for an IP. The Stargate now queries Consul for all Gateway services that are running the `cat-blog` application. Consul returns a number of Gateways, all of which have Hosts connected that are running the `cat-blog` application.
DADI Stargate then intelligently determines the optimal Gateway and returns its IP. Our machine now connects to that Gateway and asks for `www.cat.blog`, a request which is then picked up by a Host connected to that Gateway, processed, verified and passed back to our machine.
As part of the due dilligence on the technologies reviewed for use at this level in the DADI network, we ran a high number of performance tests designed to simulate heavy network activity. Primarily we tested key value read/write operations: a key value pair would be written to, read and written to again, all the way from 0 to 100, ensuring the value had written before it was iterated.
We tested ZooKeeper running a single instance on a small machine based in Paris. We ran Consul both as a standalone server, as well as a group of a server and a client on a LAN network, again on small 2 core amd64 machines. To simulate real world network conditions, test applications ran over a WAN connection from the UK.
The first set of tests were on an empty server. Average atomic write/read transactions took ~77ms for Consul, and ~150ms for ZooKeeper.
Next, we added network activity. 100 test services reading and writing key value data for ZooKeeper; changing ZNode locations to simulate state change for Consul; registration of services with differing states; and finally, random disconnections.
An average atomic write/read transaction took ~97ms for Consul under these conditions, and ~201ms for ZooKeeper. Interestingly, even when we tested through Consul clients rather than direct connections to Consul servers, the transactions were still more performant than ZooKeeper.
We added more network activity: 250 very noisy test services. Consul completed atomic write/read transactions in ~115ms. ZooKeeper in ~309ms.
Finally, we added 10 very intensive services, completing back to back read/write operations and endless updates, all running concurrently. Consul completed write/read transactions in ~129ms. ZooKeeper in ~396ms.
Throughout the tests, Consul remained responsive to queries and its host machine relatively untaxed. With ZooKeeper we saw intensive CPU usage and the occassional spike. With Consul we also saw less network traffic overall, both with direct and indirect tests.
Read/write operations were consistently twice as fast under Consul, with service discovery lookups seemingly unaffected by the amount of network activity.
We’re working on switching out ZooKeeper for Consul. We know that Consul provides better performance for what the DADI network needs, and critically it has built in service discovery. We’ve already built a proof of concept with Consul, and we’ll be implementing this for real over the coming days & weeks.
As part of this work, we are changing how we handle states and are introducing finite state machines to better help manage these. We’ll be writing more about that soon.
Do you have any questions about how we’re using Consul? Why not pop along to the first of our new fortnightly AMAs on Telegram, Discord and Reddit?
Written by Adam K Dean, a Senior Engineer at DADI, working on the DADI network.
Faster. Greener. More Secure. The future of the cloud is Edge.
59 
59 claps
59 
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@LindaVivah/what-is-cloud-computing-in-60-seconds-7a4e468d8649?source=search_post---------324,"Sign in
There are currently no responses for this story.
Be the first to respond.
Linda Vivah
Jan 25, 2020·2 min read
Ready to learn what cloud computing is in 60 seconds? Let’s Go!
SLIDE 1: What is Cloud Computing?
SLIDE 2: Advantages of Cloud Computing
SLIDE 3: 3 Types of Cloud Computing
SLIDE 4: Some of the Leading Infrastructure Cloud Providers
** These 60 second lessons are originally shared on my Instagram
Hope this was helpful! Stay tuned for more mini lessons posted both on IG & Medium.
Happy Coding!
Web Developer | Tech & Lifestyle Blogger | Founder CodingCrystals.com | IG @LindaVivah
See all (176)
23 
23 claps
23 
Web Developer | Tech & Lifestyle Blogger | Founder CodingCrystals.com | IG @LindaVivah
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-cloud/year-2021-in-awesome-cloud-%EF%B8%8F-dd14a33e1ec2?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
This is an email from 📰 Awesome Cloud News ☁️, a newsletter by Awesome Cloud.
📅 Yearly News from Awesome Cloud 📰
Hey there!
How have you been? I know it has been a while. I was working on how to make Awesome Cloud better for you.
This is the first newsletter from my Medium publication, Awesome Cloud.
Awesome Cloud newsletters are intended to help cloud enthusiasts by providing up-to-date AWS services overview…
"
https://medium.com/technoetics/easiest-way-to-automate-image-upload-to-cloudinary-using-nodejs-5014b7cb629f?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
Cloudinary is a cloud service that offers a solution to a project’s entire image management pipeline by which we can easily upload images & videos to cloud. It offers comprehensive APIs and administration capabilities and is easy to integrate with any mobile or web application.
In today’s article we are going to write a simple nodejs script to automatically upload images to cloudinary cdn when someone adds a new image to a folder on local machine or remote server.Run “npm init” command in a folder and following command to install required npm modules:
Let us start by spinning up a simple express server:
We will be using chokidar npm module to listen for changes in image source folder to check if new files are added anywhere inside the folder. You can find an excellent tutorial explaining it’s usage in detail with all available options here: Tutorial.
We set up chokidar to listen for changes as follows:
Here we first specify folderpath in which we are going to add files on line 4. Then we set up watcher to watch files in folder on next line. Watcher has multiple event listeners on which appropriate callbacks are fired. It will first add all existing files in folder in watcher. In order to prevent uploading of existing files to cdn we first set scanComplete flag to false. When chokidar has finished scanning all files it fires ready event. In that case we can set scanComplete flag to true so that we can upload the next incoming files to cdn. Whenever a new file is added anywhere inside the source folder the add event is fired and since initial scan is now complete we can now add new files to cdn.
Now let us focus on setting up upload to cloudinary cdn. First we need to set up access to cloudinary cdn:
You will get the cloudname,api_key and api_secret from dashboard of cloudinary cdn. Create an account if you don’t have one and go to dashboard to get all the required credentials. Click on reveal to get api_secret in the dashboard tab.
Next we will upload files to cloudinary cdn when chokidar watcher detects addition of new file to filepath.
When file is added we first split path in individual names using split method in line 4.
For example if filepath is ‘/home/saurabh/Pictures/Apps/imgcat1/img1.jpg’ we get pathArray as [‘’,’home’,’saurabh’,’Pictures’,’Apps’,’imgcat1',’img1.jpg’]
Now I download images from chrome due to which it first creates a temporary “.crdownload” file while file download is in progress. In order to prevent uploading of temp files to cdn we have added an if condition in the next line. Once filedownload is complete I prefer to upload the file in a specific remote folder in cloudinary cdn rather than uploading files directly to base path. We first get destinationfolder name which might be imgcat1(second last element in pathArray variable) in the above example and destfileName which is last element in pathArray variable.
Next we call uploader method from cloudinary module to upload file from path variable that chokidar watcher returns. The second parameter in upload function is a json containing extra parameters which customizing upload to cdn. You can get detailed list of upload option here: Upload Api Reference
We first specify that we will upload the image inside destfolder at cloudinary cdn. Please create appropriate folder on the cdn by going to cloudinary dashboard, clicking on media library and creating new folder to avoid any unwanted issues. Next we set “use_filename” flag to true so that filesnames are not assigned random names on cdn and specify appropriate tags to have easier indexing and usage of images in future.
You can find the complete source code for the script here: GithubGist
Bonus Tip:
Tip #1: Even if you don’t use cloudinary as your cdn you can still use chokidar module for variety of purposes like updating file/content metadata when new files are uploaded into server or run some simple automation tasks based on your needs.
Tip #2: You can also upload video files to cloudinary cdn and if video files using same upload function by following documentation here: Video Upload Api Reference
Connect Deeper:
In the next article I will probably try to cover creation of progressive web app using reactjs and node/firebase backend. If you would like to get notified about upcoming articles then you can follow our facebook page: Technoetics
Codeclassifiers is a leading technology blog which…
9 
9 claps
9 
Codeclassifiers is a leading technology blog which obsessively covers apps, tutorials, designs, software development and technology articles.
Written by
Blogger,web and hybrid applications developer
Codeclassifiers is a leading technology blog which obsessively covers apps, tutorials, designs, software development and technology articles.
"
https://medium.com/luxor/sia-decentralized-cloud-storage-7de576542497?source=search_post---------327,"There are currently no responses for this story.
Be the first to respond.
We are bullish on Sia’s business model and that’s why we at Luxor Mining have dedicated one of our mining pools to supporting the network. In addition, we have been contributing back a percentage of our mining pool fees to the community to help it develop further. To check out more about Luxor’s Sia Mining Pool click here.
Sia is a decentralized data storage platform powered by blockchain technology. Sia connects people “hosts” that have underutilized hard drive capacity with customers “renters” in search of data storage. In essence, Sia is the Uber of data storage.
The blockchain technology delivers a competitive advantage for Sia, making it more secure and lower cost than traditional cloud storage providers. Sia has created a marketplace for data that provides higher latency, more throughput, less downtime and more security.
Below is a breakdown of what we think makes Sia such a valuable service:
One of the most important purchase criteria for renters is the price of data storage. Currently, renters purchase data storage from a number of providers such as Amazon, Microsoft, Dropbox, Google, etc. These services offer prices that are far above the price of Sia. On average, Sia’s storage costs 70% less than existing cloud storage providers.
Sia is able to offer such a low price point due to their low costs. This is fueled by blockchain technology and competition on the network.
It’s no secret that blockchain technology has the potential to dramatically reduce costs. That is why big companies such as Google and Goldman Sachs (amongst many others) have invested in the technology. By providing a cloud storage marketplace on the cloud, Sia is able to dramatically reduce its costs, which results in a lower price point for consumers.
The business model has built in competition between data storage hosts which reduces the price even further. On the network hosts show their geography, speed, latency, price and uptime. Renters have the ability to select the hosts that are best suited for their situation. This competition causes downward pressure on price and upward pressure on quality.
Arguably more important than price is data security. Renters need to be certain that their data will not be lost, tampered with or stolen.
This is a problem for the existing cloud storage providers. The threat of data being destroyed is more probable than the public is led to believe. For example Amazon only has a few data storage centers in the world. This means that if these were all destroyed then so to would the data. In the unlikely case that America was going to be attacked, targeting data centers would be an effective place to start. Even if all the facilities are not attacked, a simple outage can cause other companies to go with it. For example when Amazon experienced an outage last year in one of its data centers a number of companies also temporarily went down including Slack and Business Insider. It’s dangerous to rely so much on one company and a few data centers. An additional (although less likely) threat to existing cloud storage providers is stolen or tampered data.The public is becoming increasingly suspicious of large tech companies and hence is worried about their data. Cyber security attacks from third-parties are also a threat even with technology companies investing heavily to prevent it.
Ultimately Sia delivers a strong solution to data security issues. Sia is more secure than any of the existing providers as no one event can trigger a loss of data. The security is built from the following concepts:
Sia uses the Reed-Solomon Redundancy to ensure the security of data. Each piece of data is stored on 30 devices located around the world. Only 10 of the 30 devices need to be online in order to store the file. If every host has a 90% reliability (which is lower than the actual reliability) than the chance of losing the data is 1 in 1,000,000,000,000,000. Because these 30 servers are located around the world and on different servers the chances of lost data are extremely low. No random event can trigger a loss of data.
Sia encrypts and distributes renters’ files across a decentralized network. The files are encrypted on the renters machine. Each separate piece of data has a separate password so it can’t be deduplicated. Renters control their private encryption keys and have ownership of their data. No outside third party can access or control renters data, unlike traditional cloud storage providers.
The File Contract is an application of Sia that is build off of Smart Contract technology. Basically, the hosts do not get paid until they prove (at the end of the period) that they did hold on to the data. When the contract is formed both the renter and the host puts money into the smart contract. The reason the host also puts money into the contract is to provide additional incentive to fulfill their duties in the contract. If they break the contract they will not only lose out on revenue but also their deposit. The blockchain enforces the smart contract automatically, meaning there is no need for trust of a third-party or between the host and renter.
The way the data is stored is by using the Merkle Tree method. Each file is split into 64 Byte pieces, then each piece is hashed, and then the adjacent hashes are combined until one piece remains. The final hash in the Merkle root. The host must prove that they have one of the 64 Byte segment of the data (randomly selected). This ensures that the host does in fact have the data.
To hold hosts accountable they are monitored often to make sure they are indeed providing the services they advertised. They are checked for uptime, latency, throughput and price. Overall, hosts are around 90–95% reliable.
Sia provides a unique solution to a problem facing many users of cloud storage. Under the existing industry renters pay a high price point for a service with questionable security. With Sia renters are able to store their data for cheap and with better security.
Ultimately, we expect this business to to thrive and hope to continue to support it in the future!
For additional information on Sia check out their Whitepaper here.
Questions about Luxor?
Follow us on Twitter and tweet to us! We’re happy to help: https://twitter.com/LuxorTechTeam
We’re also on Discord. Ping us there at https://discord.gg/5qPRbDS
Luxor has spent the past 2 years building North America’s…
215 
215 claps
215 
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
Written by
Luxor is a Bitcoin mining pool and full-stack crypto mining company. The financialization of hashrate starts with us. https://www.luxor.tech/
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
"
https://faun.pub/how-to-approach-the-challenges-of-devops-in-large-organizations-b1a31e0562a6?source=search_post---------328,"There are currently no responses for this story.
Be the first to respond.
Implementing DevOps practices in small organizations seems like standard practice, but what if you’re trying to utilize DevOps in large organizations? Trying to modernize workflows can be a challenge for any company, but there are different challenges, risks, and benefits for bigger companies. Let’s take a look at how enterprises might approach a DevOps transformation through a few of the core tenants of DevOps.
There are a few different forms of feedback that come with DevOps: automated feedback about specific code (typically through unit and integration testing software), personal feedback from other team members, consumer feedback from customers using your product, and cross-team feedback throughout the organization. Startups and small companies may find it easier to have open lines of communication between individual team members as well as across teams.
Large organizations will need to make a conscious effort to keep team communication open, On the other hand, they will have more resources available (both money and employees) to field customer and in-house feedback about individual services or larger products. They may also be able to better purchase and implement automated testing and CI/CD tools, which leads to…
One of the biggest tech benefits to a DevOps approach is automating away the manual tasks that bog down critical projects. Large organizations often have the time, money, and people to set up automated tools, like CI/CD pipelines, unit and integration test suites, and config management systems. The biggest challenge in the enterprise world is trying to make everyone happy.
One approach is to standardize on a single tool for each purpose, such as Jenkins or Chef. This can enable your IT staff to specialize in those tools, but may make some users unhappy with being forced into a tool they may not prefer. The alternative is to allow each team or business unit to use their own preferred software, but this can turn into a “toolset hell” with a mashup of every combination of applications within your organization. Each approach has its pros and cons, and often comes down to a management decision.
Having individual teams that handle their part of the puzzle and nothing else is the biggest hurdle that enterprises face when trying to apply DevOps principles. The combination of ‘dev’ and ‘ops’ (and other disciplines, like ‘sec’ and ‘fin’) is naturally split out in a large organization, so recombining them can be a huge undertaking. Then again, that gap is exactly the problem the DevOps approach seeks to solve.
Some companies solve this by having a separate team that handles the cross-team support and communication. Other companies break down these silos by enabling employees to seamlessly migrate between teams depending on the project or application. The more “devopsy” method is to utilize ChatOps and centralized documentation repositories for open communication and collaboration, which can help break down unify the distinct teams.
The idea of holistic thinking tends to come easier to larger organizations, as successful enterprises typically have a system in place for “big picture” thinking, either through a management or product team, or through a cross-functional committee. That said, communication of this vision down to the employees, along with communication up to that management team, is crucial for enabling outside-the-box thinking to get past any roadblocks and hurdles that are in the way of creating and deploying the end product. Sometimes, the hardest part is convincing programmers that not everything needs to be solved with code!
Some folks think that DevOps only applies to startups and small companies, but we’re seeing more and more teams benefit from implementing DevOps in large organizations. The benefits of the above DevOps principles are numerous, but frequently come with a different set of challenges based on your organizational size. Once you are aware of those challenges and have a plan to overcome them, you can start to transform your enterprise to a DevOps shop.
Originally published at www.parkmycloud.com on July 17, 2018.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
58 
58 claps
58 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/@jaychapel/what-is-google-cloud-anthos-promise-for-hybrid-multi-cloud-environments-d5d62cba103c?source=search_post---------329,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 2, 2019·4 min read
Earlier this year at the Google Cloud Next event, Google announced the launch of its new managed service offering for multi-cloud environments, Google Cloud Anthos.
The benefits of public cloud, like cost savings and higher levels of productivity, are often presented as an “all or nothing” choice to enterprises. However, with this offering, Google is acknowledging that multi-cloud environments are the reality as organizations see the value of expanding their cloud platform portfolios. Anthos is Google’s answer to the challenges enterprises face when adopting cloud solutions alongside their on-prem environments. It aims to enable customers to evolve into a hybrid and multi-cloud environment to take advantage of scalability, flexibility, and global reach. In the theory of “write once, run anywhere”, Anthos also promises to give developers the ability to build once and run apps anywhere on their multi-cloud environments.
Google Cloud Anthos is based on the Cloud Services Platform that Google introduced last year. Google’s vision is to integrate the family of cloud services together.
Anthos is generally available on both Google Cloud Platform (GCP) with Google Kubernetes Engine (GKE) and data centers with GKE On-Prem. So how does Google aim to deliver on the multi-cloud promise? It embraces open-source technology standards to let you build, manage and run modern hybrid applications on existing on-prem environments or in public cloud. Moreover, Anthos offers a flexible way to shift workloads from third-party clouds, such as Amazon Web Services (AWS) and Microsoft Azure to GCP and vice-versa. This allows users not to worry about getting locked-in to a provider.
As a 100% software solution, Anthos gives businesses operational consistency by running quickly on any existing hardware. Anthos leverages open APIs, giving developers the freedom to modernize. And, it automatically updates with the latest feature updates and security patches, because is based on GKE.
Google also introduced Migrate for Anthos at Cloud Next, which automates the process of migrating virtual machines (VM) to a container in GKE, regardless of whether the VM is set up on-prem or in the cloud lets users convert workloads directly into containers in GKE. Migrate for Anthos makes the workload portability less difficult both technically and in terms of developer skills when migrating.
Though most digital transformations are a mix of different strategies, for the workloads that will benefit the most, containers, migrating with Anthos will deliver a fast, smooth path to modernization according to Migrate for Anthos Beta.
Another piece of the offering is Anthos Config Management, which lets users streamline confirmation so they can create multi-cluster policies out of the box, set and enforce secure role-based access controls, resource quotas, and create namespaces. The capability to automate policy and security also works with their open-source independent service for microservices, Istio.
The management platform also lets users create common configurations for all administrative policies that apply to their Kubernetes clusters both on-prem and cloud. Users can define and enforce configurations globally, validate configurations with the built-in validator that reviews every line of code before it gets to the repository, and actively monitors them.
Google Cloud is expanding its Anthos platform with Anthos Service Mesh and Cloud Run for Anthos serverless capabilities, announced last week and currently in beta.
The first is Anthos Service Mesh, which is built on Istio APIs, is designed to connect, secure, monitor and manage microservice running in containerized environments, all through a single administrative dashboard that tracks the application’s traffic. This new service is aimed to improve the developer experience by making it easier to manage and troubleshoot the complexities of the multi-cloud environment.
Another update Google introduced was Cloud Run for Anthos. This managed service for serverless computing allows users to easily run stateless workloads on a fully managed Anthos environment without having to manage those cloud resources. It only charges for access when the application needs resources. Cloud Run for Anthos can run workloads on Google Cloud on on-premises and is limited to Google’s Cloud Platform (GCP) only.
Both AWS and Azure have hybrid cloud offerings but are not the same, mostly for one single reason.
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility, in the same operating idea as Anthos, using the same AWS APIs, tools, and infrastructure across on-prem and the AWS cloud to deliver a seamless and consistent for an AWS hybrid experience.
As an extension of Azure to consistently build and run hybrid applications across their cloud and on-prem environments, Azure Stack delivers a solution for workloads wherever they reside and gives them access to connect to Azure Stack for cloud services.
As you can see, the main difference is that both AWS Outposts and Azure Stack are limited to combining on-premises infrastructure and the respective cloud provider itself, with no support for other cloud providers, unlike Anthos. Google Cloud Anthos manages hybrid multi-cloud environments, not just hybrid cloud environments, making it a unique offering for multi-cloud environment users.
Originally published at www.parkmycloud.com on September 24, 2019.
CEO of ParkMyCloud
18 
18 
18 
CEO of ParkMyCloud
"
https://medium.com/serverlessguru/mistakes-to-sidestep-when-going-for-a-serverless-design-264a40d89d0d?source=search_post---------330,"There are currently no responses for this story.
Be the first to respond.
Moving on from legacy monolithic coding patterns to a distributed micro-service design approach can get tricky. You can’t possibly account for all complications that might happen, but arming yourself with basic knowledge can significantly help your serverless journey. Let’s explore certain prime areas of focus when moving to serverless.
"
https://medium.com/@auth0/auth0-architecture-running-in-multiple-cloud-providers-and-regions-b2f1c6625788?source=search_post---------331,"Sign in
There are currently no responses for this story.
Be the first to respond.
Auth0
Aug 15, 2018·10 min read
Auth0 provides authentication, authorization, and single sign-on services for apps of any type (mobile, web, native) on any stack. Authentication is critical for the vast majority of apps. We designed Auth0 from the beginning so that it could run anywhere: on our cloud, on your cloud, or even on your own private infrastructure.
In this post, we’ll talk more about our public SaaS deployments and provide a brief introduction to the infrastructure behind auth0.com and the strategies we use to keep it up and running with high availability. This is an updated version of the 2014 High Scalability post about Auth0’s architecture.
A lot has changed since then in Auth0. These are some of the highlights:
The core service is composed of different layers:
In 2014 we used a multi-cloud architecture (using Azure and AWS, with some extra resources on Google Cloud) and that served us well for years. As our usage (and load) rapidly increased, we found ourselves relying on AWS resources more and more.
At first, we switched our primary region in our environment to be in AWS, keeping Azure as failover. As we began using more AWS resources like Kinesis and SQS, we started having trouble keeping the same feature set in both providers. As our need to move (and scale) faster grew, we opted to keep supporting Azure with a limited feature parity: if everything went down on AWS, we could still support core authentication features using the Azure clusters, but not much of the new stuff we had been developing.
After some bad outages in 2016, we decided to finally converge on AWS. We stopped all efforts related to keeping the services and automation platform-independent and instead focused on:
“Writing better automation let us grow from partially automated environments doing ~300 logins per second to fully automated environments doing more than ~3.4 thousand logins per second”
TWEET THIS
Let’s take a look at our US environment architecture, for instance. We have this general structure:
And this is the structure inside a single AZ:
In this case, we use two AWS regions: us-west-2 (our primary) and us-west-1 (our failover). Under normal circumstances, all requests will go to us-west-2, being served by three separate availability zones.
This is how we achieve high availability: all services (including databases) have running instances on every availability zone (AZ). If one AZ is down due to a data center failure, we still have two AZs to serve requests from. If the entire region is down or having errors, we can update Route53 to failover to us-west-1 and resume operations.
“We achieve high availability by running all services instances on every AWS availability zone”
TWEET THIS
We have different maturity levels for service failover: some services, like user search v2 (that builds a cache on Elasticsearch) might work but with slightly stale data; still, core functionality keeps working as expected.
In the data layer, we use:
We exercise failover at least once per year, and we have playbooks and automation to help new infrastructure engineers get up to speed on how to do it and what are the implications.
Our deployments are usually triggered by a Jenkins node; depending on the service we either use Puppet, SaltStack, and/or Ansible to update individual or groups of nodes, or we update our AMIs and create new autoscaling groups for immutable deployments. We have different types of deployments for new and old services, and this has been shown to be largely ineffective as we need to maintain automation, docs, and monitoring for something that should be unified.
We are currently rolling out blue/green deployments for some of the core services, and we intend to implement the same for every core and supporting service.
Besides unit test coverage on every project, we have multiple functional test suites that run in every environment; we run it on a staging environment before we deploy to production, and we run them again in production after finishing a deployment to ensure that everything works.
The highlights:
“Besides unit test coverage on every project, we have multiple functional test suites that run in every environment: staging before deploying to production and again in production after finishing deployment.”
TWEET THIS
Until 2017 we ran our own, custom-built CDN using NGINX, Varnish, and EC2 nodes in multiple regions. Since then, we transitioned to CloudFront, which has given us several benefits that include:
There are a few downsides, like the fact that we need to run Lambdas to perform some configurations (like adding custom headers to PDF files and things like that). Still, the upsides definitely make up for that.
One of the features we provide is the ability to run custom code as part of the login transaction, either via authentication rules or custom database connections. These features are powered by Extend, an extensibility platform that grew out of Auth0 and is now being used by other companies as well. With Extend, our customers can write anything they want in those scripts and rules, allowing them to extend profiles, normalize attributes, send notifications, and much more.
We have Extend clusters specifically for Auth0; they use a combination of EC2 auto-scaling groups, Docker containers, and custom proxies to handle requests from our tenants, processing thousands of requests per second and responding fast to variations of load. For more details about how this is built and run, check out this post on how to build your own serverless platform.
We use a combination of different tools for monitoring and debugging issues:
The vast majority of our alerts come from CloudWatch and DataDog.
We tend to configure CloudWatch alarms via TerraForm, and the main monitors we keep on CloudWatch are:
CloudWatch is the best tool for alarms based on AWS-generated metrics (like ones from load balancers or autoscaling groups). CloudWatch alerts usually go to PagerDuty, and from PagerDuty to Slack/phones.
DataDog is a service we use to store and act on time-series metrics. We send metrics from Linux boxes, AWS resources, off-the-shelf services (like NGINX or MongoDB), and also custom services we have built (like our Management API).
We have many DataDog monitors. A few examples:
As you can see from the examples above, we have monitors on low-level metrics (like disk space) and high-level metrics (like MongoDB replica-set change, which alerts us if there was a change in the primary node definition, for example). We do much more and have some pretty sophisticated monitors around some services.
DataDog alerts are pretty flexible in their outputs and we usually send them all to Slack, sending to PagerDuty only those who should “wake people up” (like spikes of errors, or most things that we are sure that are having an effect on customers).
For logging we use Kibana and SumoLogic; we are using SumoLogic to record audit trails and many AWS-generated logs, and we use Kibana to store application logs from our own services and other “off-the-shelf” services like NGINX and MongoDB.
Our platform evolved quite a bit in order to handle the extra load and the huge variety of use cases that are important to our customers, but we still have more room for optimizations.
Not only our platform grew, but our engineering organization increased in size: we have many new teams building their own services and are in need of automation, tooling, and guidance around scalability. With that in mind, these are the initiatives in place for us to scale not only our platform but also our engineering practice:
Auth0, a global leader in Identity-as-a-Service (IDaaS), provides thousands of enterprise customers with a Universal Identity Platform for their web, mobile, IoT, and internal applications. Its extensible platform seamlessly authenticates and secures more than 1.5B logins per month, making it loved by developers and trusted by global enterprises. The company’s U.S. headquarters in Bellevue, WA, and additional offices in Buenos Aires, London, Tokyo, and Sydney, support its customers that are located in 70+ countries.
For more information, visit https://auth0.com or follow @auth0 on Twitter.
Originally published at auth0.com.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
See all (400)
41 
41 claps
41 
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/manifoldco/manifold-for-teams-share-your-whole-stack-on-any-cloud-with-one-workflow-2386a5779a37?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
At Manifold, we’re dedicated to helping developers find, organize, and connect the best services to their applications. We recognize that developers are naturally collaborative and need a way to organize and visualize how they share their projects, which means making sure Manifold works effortlessly with teams.
We would like to introduce you to Manifold for Teams as the easiest way to share and organize Manifold-managed resources and secrets.
Building applications is collaborative, and when relying on cloud services for core functionality you need to be able to share access to those services. The problem is each service comes with its own flavour of access control and collaboration, and you’re stuck hunting down several invoices at the end of each month.
With Manifold for Teams you receive one bill at the end of the month, and share access to all of your resources and secrets with a single invite. Gain peace of mind through granting users access to only what they need, whether that’s running your application with credentials or being able to purchase and configure new products.
Reserve your team’s namespace by visiting your Manifold Marketplace dashboard. Then head over to our documentation and check out our getting started guide for teams, or read how to use teams in the command line.
We're determined to make it easy for developers to use the…
107 
107 claps
107 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/top-10-cloud-consulting-companies-worldwide-351a68e8bbc6?source=search_post---------333,"There are currently no responses for this story.
Be the first to respond.
Most of the startups nowadays are cloud-native and don’t have any challenges aside from the cost of operational expenses. However, cloud transition can be a daunting perspective for a long-standing company with a branched out legacy infrastructure.
Every cloud service provider offers consulting with moving your infrastructure to them. They also have multiple certified partners, who design and implement custom cloud solutions based on AWS, GCP, MS Azure, DigitalOcean, and other vendors. However, such services usually come at a hefty sum and might result in possible vendor lock-in. This is why multiple cloud consulting agencies offer their services in performing the audit of the existing infrastructure, delivering the roadmap to transition and accomplishing the designated milestones.
The only issue (and a major one at that) is that some companies might fail to deliver on their promises. That’s why it’s wise to shortlist the candidates based on feedback of their previous customers. Below is the list of top 10 cloud consulting companies, formed based on customer reviews on independent business rating agency Clutch from Washington, DC. The list is given in no particular order.
Below we describe them in some more details, highlighting their staff size, core areas of expertise, average cheque, minimum project budgets and other important details.
Altoros is an international IT & cloud consulting enterprise headquartered at Sunnyvale, CA, with R&D centers in the US, Belarus and Argentina. The company offers nearly 2 decades of experience and services of 300+ seasoned software engineers and cloud architects to help their customers increase operational efficiency and shorten the time-to-market for new products. The company charges $100-$149/hr and their minimum project budget is $25,000+.
ServerCentral is a cloud and dedicated data center services provider from Chicago, IL founded in 2000. The company hosts and manages all types of cloud and legacy infrastructure solutions for entrepreneurs, startups, organisations and Fortune500 enterprises alike. If the apps have to be hosted outside the US, the company engages the assistance of multiple trusted SLA-covered datacenters to ensure top-grade connectivity for your products and services. The company fields multiple teams of 50+ IT talents, who charge $100-$149/hr for their services and begin working on projects with budgets as low as $5,000+.
Aplana is an international IT services corporation headquartered in Huntingdon Valley, PA with R&D offices in Moscow, Manchester, UK and Offenbach am Meine, Germany. The company was founded in 1999 as a part of Europe’s then largest IT services provider, I.T. Group and has since then became Gold Partner of Microsoft, IBM and Oracle, serving global industry leaders in transportation, logistics, finances, hospitality and leisure and other spheres of economy. The company fields 500+ seasoned software development and cloud systems engineers, who charge $50-$99/hr and take on the projects with minimum budget of $25,000+.
Trigent is the enterprise software, web & mobile development and cloud services provider founded 1995 and headquartered in Southborough, MA with 6 R&D centers in India. Trigent provides a variety of services, including managed cloud transformation, enterprise digital transformation, cloud development and product engineering among others and serves companies from startups to world-leading Fortune500 enterprises. The team of 250+ IT experts charges $25-$49/hr and works on projects as small as $10,000+.
Eleks is one of the global IT outsourcing and consulting leaders from Lviv, Ukraine. Since 1991 the company grew to more than 1,100 employees in offices in the US, Japan, and across Eastern Europe. Eleks serves customers of all sizes, from startups to world-moving innovation leaders who make billions in income yearly. Their core areas of expertise are cloud consulting and SI, custom software development, cyber security and AI implementation. The hourly fee is $25–49/hr and the minimum project size is $50,000+.
BlueWolf is an IBM company, a global cloud consulting company and SalesForce integrator founded in 2000 and headquartered in New York, NY. Their main area of expertise is augmenting the customer’s systems and workflows with innovative cloud, AI and ERP solutions. The company serves industry leaders and leading market players in automotive, mobile, finances, transportation, marketing and a bunch of other industries. Bluewolf houses several thousands of IT talents in more than 20 R&D hubs across the globe, charges $150-$199/hr and engages in projects with at least $50,000+ budget.
PSL Corp. headquartered in Medellin, Colombia is the leading Latin American IT outsourcing company of 2017–2018, the global leader in Java outsourcing for a couple of years, the reliable nearshore partner of industry-leading enterprises for decades. The company was established in 1986, and through their 32 years of experience they won a reputation of reliable IT services integrator, innovator and powerful success driver for their customers. Their areas of expertise include cloud infrastructure management and optimization, Big Data and Fast data solutions, AI and ML product integration, full-scale enterprise software development. The company houses nearly a thousand of IT professionals, charges $25-$49/hr and accepts the projects with at least $75,000+ budget.
IT Svit is a Managed Services Provider from Kharkiv, Ukraine. The company concentrates the effort on full-scale startup services and MVP development, developing and integrating Big Data solutions and Machine Learning tools, executing cloud transition and digital transformation projects of any scope, providing full-stack software development and automated QA.
IT Svit is recognized by Clutch as one of the leaders of IT outsourcing market in Ukraine 2017–2018 and are listed among top 10 Managed Service Providers worldwide. The company of 50+ software developers and DevOps engineers operates since 2005, charges $25-$49/hr, and accepts projects with budgets as small as $5,000+.
SADA Systems is a cloud consulting and IT solutions provider from Los Angeles, CA, who prides itself as a continuous global leader in innovations that create transformational value. The company is a Gold partner of Microsoft, Google Cloud and Facebook and has a ton of awards and accolades as an innovation driver and a great place to work. The company’s core areas of expertise include cloud consulting and SI, productivity solutions consulting and integration, custom software development and providing managed IT services. The team of 50+ software engineers operates since 2000, charges $150-$199/hr and works on projects with at least $25,000+ budgets.
TechMD is an award-winning IT solutions provider from Los-Angeles, CA passionate about providing enterprise-grade scalability, performance and security to small-to-medium businesses. Their core expertise areas include cloud consulting and SI, cybersecurity, strategic development consulting and managed IT services. The team operates since 2003, houses 50+ IT veterans, charges $150-$199/hr and works on projects with budgets as low as $5,000.
All of the companies above are providing their services for years and decades, have won multiple awards and accolades and have established themselves as reliable IT services and cloud consulting partners. Their experience of working with industry leaders and fledgling startups alike enables them to find working solutions for any types of challenges and any project scopes. Surely, one of them will fit your project requirements and budget and will help you reach the business goals with maximum return on investment!
empowerment through data, knowledge, and expertise.
16 
16 claps
16 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
DevOps & Big Data lover
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/fbdevclagos/how-to-manage-heroku-logs-with-timber-io-483600579d38?source=search_post---------334,"There are currently no responses for this story.
Be the first to respond.
Logs are collections of time-stamped events collected from the line by line steps of all of your app’s running processes, system components, backing services and so on. Managing logs can be time-consuming, annoying and frustrating, yet very useful in debugging. For instance, with logging, you can get the step by step process of an HTTP request and also be able to trace where the error comes from.
Logs are probably the most frequently used data source developers rely on for debugging — both during development and in production. Useful logs can provide the developer with tremendous help when trying to understand the steps it took before it got to the error point or the steps taken.
Heroku has a nice default well-structured logging system called Logplex that routes log streams from diverse sources into a single channel, providing the foundation for a truly comprehensive logging.
Heroku has four categories of logs and they are:
The Heroku logging system is basically designed for collecting and routing logs messages. It is not designed for long-term storage of logs and also has limits. By default, it keeps the last 1,500 lines of log messages and these expire after 1 week.
So if you would like to keep more than 1,500 lines or for longer-term storage, search, alerting, filtering and other processing, you can drain your logs to a drain service provided by one of the many logging Add-on Providers on Heroku of which Timber.io is one.
Timber.io is a cloud-based logging platform, that has support for Python, Ruby, Elixir, and Node apps. It automatically structures your log data with its open source packages, makes the lines readable, adds valuable context, lets you easily create graphs, get alerts from your logs, filter, search and do some other processing on your logs.
With Timber.io, you can keep your Heroku logs for up to six months, get real-time log tailing without delay, search and filter logs by a user, request ID, or any other context.
Setting up Timber on your Heroku app is extremely easy, all you need is a few clicks and one line command to get everything done.
You need to register on Timber.io to start using it on your Heroku app. So go create an account.
Now visit this page to create a new organization and follow the below steps:
2. On the next page, you can invite team members with their email and assign a role. Enter the email of a member, assign a role and click on Invite. When you are done, click on Next to continue
3. In this page, we are going to create a project, set environment, language, and platform. Enter the project name, you can set the environment to either of the available environment (production, staging, other), select the language of your application, select Heroku and click on Create.
4. In the next page, you’ll be given an API Key, so take note of that.
To set up Timber on Heroku, you need to add a Timber drain to Heroku. In your shell, run the command example below:
If Timber is added successfully to your Heroku app, you would see an output similar to this below:
And you’ve successfully installed Timber on your Heroku app.
Quite a number of tools are available in the Timber Console, that you can use to manage your Heroku logs.
Search the logs you need faster with Timber Console powerful searching. It follows the Lucene style syntax with slight differences, conducive for searching logs.
With this tool, you can easily solve customer issues. This allows you to filter the logs to a specific user. It uses context, particularly the context.userdocument to segment logs by a specific user.
This lets you quickly seek your logs in respect to time. That is, it helps you filter your logs based on the time range given.
This is a powerful tool that lets you view all properties, information of the incoming request, including the parameters sent. This information tends to be useful during debugging.
The HTTP request tracing tool enables you to to segment your logs by request ID, making it very easy to view logs for specific HTTP requests. It lets you see only events related to that request and uses context, especially the HTTP context document, to segment logs.
This tool helps you quickly filter your logs based on popular tags, error levels and so on. It helps you navigate and explore your logs quickly and easily.
Debugging is a stressful thing and logs help a lot in tracing steps previously taken before encountering an error. Being in control of your logs can make that easy for you. Though there are many other logging platforms you can go for, I think Timber is one of the best.
If you enjoyed this article, kindly clap 👏🔥
A Tech Community united in knowledge sharing.
254 
254 claps
254 
Written by
Software Engineer. Subscribe to my YouTube channel: http://bit.ly/oyetoketoby
A Tech Community united in knowledge sharing. Visit fb.com/groups/devclagos
Written by
Software Engineer. Subscribe to my YouTube channel: http://bit.ly/oyetoketoby
A Tech Community united in knowledge sharing. Visit fb.com/groups/devclagos
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thipwriteblog/6-%E0%B8%AA%E0%B8%B4%E0%B9%88%E0%B8%87%E0%B8%97%E0%B8%B5%E0%B9%88-alibaba-cloud-%E0%B9%84%E0%B8%94%E0%B9%89%E0%B9%80%E0%B8%9B%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%9A%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2%E0%B8%9C%E0%B8%B9%E0%B9%89%E0%B9%83%E0%B8%AB%E0%B9%89%E0%B8%9A%E0%B8%A3%E0%B8%B4%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B8%A5%E0%B8%B2%E0%B8%A7%E0%B8%94%E0%B9%8C%E0%B9%80%E0%B8%88%E0%B9%89%E0%B8%B2%E0%B8%AD%E0%B8%B7%E0%B9%88%E0%B8%99-7645bae63ebb?source=search_post---------335,"There are currently no responses for this story.
Be the first to respond.
พึ่งไปอ่านเจอบทความที่พูดถึง Alibaba Cloud ผู้ให้บริการคลาวด์เจ้าใหม่ที่เล่นใหญ่ จนสามยักษ์ใหญ่อย่าง “Big Three” (AWS , Azure , GCP) อาจสะเทือนได้ โดยเขาวิเคราะห์ถึง 6 สิ่งที่อาลีบาบาคลาวด์ได้เปรียบกว่าผู้ให้บริการคลาวด์เจ้าอื่น ไหนๆก็อ่านผ่านตามาแล้ว เลยมาแปะให้อ่านกันที่นี่ด้วย
ภาพรวมฟังดูน่าตื่นตาตื่นใจ และด้วยความได้เปรียบในหลายๆ ด้าน ก็ดูจะเกินต้านทานเหมือนกันสำหรับ cloud ในโซนเอเชีย แต่ส่วนตัวแล้วยังไม่คุ้นกับอาลีบาบาคลาวด์เท่าไหร่ หากเทียบกับสามเจ้าที่ได้ยินชื่อคุ้นหูมากกว่า ก็เลยเป็นที่มาว่าทำไมเราถึงสนใจที่จะถอดบทความนี้มาแปะไว้ วันนี้ได้ทำความรู้จักกันคร่าวๆแล้ว ไว้มีโอกาสต้องลองไปวิ่งเล่นในก้อนเมฆ Alibaba ดูบ้างละ
source : www.infoworld.com
Programming, Technology, Work Life, Finance, Storyteller, Lifestyle, Content Creator, Podcast
7 
Some rights reserved

7 claps
7 
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-cloud-governance-models-c664a97e2489?source=search_post---------336,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 4, 2020·4 min read
The deliverability of cloud governance models has improved as public cloud usage continues to grow and mature. These models allow large enterprises to tier and scale their AWS Accounts, Azure Subscriptions and Google Projects across hundreds and thousands of cloud users and services. When we first started talking to customers 5+ years ago, mostly AWS users at the time, they often had a single AWS account for their entire organization and required third-party tools to manage usage and costs by project, line of business or application owner. But now, the “Big 3” cloud providers offer an array of ways for even the largest Fortune 500 enterprises to set up, run and manage their use of the dizzying volume of cloud services.
The main way cloud providers allow cloud administrators to manage and grant access to their services is by leveraging Identity and Access Management (IAM) and providing options for roles and policies that govern both access and usage. IAM lets you grant granular access to specific AWS, Azure and/or Google Cloud resources and helps prevent access to other resources. IAM lets you adopt the security principle of least privilege, where you grant only necessary permissions to access specific resources like VM’s, Databases, Storage, Containers, etc.. With IAM, you manage access control by defining who (identity) has what access (role) for which resource.
In ParkMyCloud, we apply this with Teams and Roles. Admins can create Teams (equivalent to Projects, Applications, or Lines of Business) and can invite a Team Lead to manage that PMC Team, and they can in turn grant users access and set permissions for them, which can then by automated based on policies, usually by leveraging tags but you can use other metadata as well.
What if you want more flexibility with the cloud providers to both manage user access and to more tightly align your cloud services and usage to your organizational structure, projects and applications? Each of the major providers has designed ways for large enterprises to implement a hierarchical usage of cloud users and services that probably can look very similar to that enterprises organization chart. (If you can understand their jargon.)
We dug into AWS, Azure and Google and this is what we found:
Amazon Web Services (AWS)
Microsoft Azure
Google Cloud
Tips for implementing Cloud Governance Models:
The cloud providers have done a pretty good job of documenting their roles, policies and hierarchies and creating a graphical representation of their current hierarchical structures cloud governance models. Of course, none of them use the same terminology — I mean, why would you, too easy, right? (And why does Google rank a ‘Folder’ above a ‘Project’? )
With these options available to you, your cloud operations team can make sure to use this to your advantage when planning new resources, accounts, and use cases within your organization. Let us know your thoughts and if you use any of these models to improve your cloud usage.
Originally published at www.parkmycloud.com on September 1, 2020.
CEO of ParkMyCloud
15 
15 
15 
CEO of ParkMyCloud
"
https://medium.com/@timfrankandersen/a-world-class-toolbox-for-remote-working-d0753bdcfa66?source=search_post---------337,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tim Frank Andersen
Mar 19, 2020·4 min read
In a time where we are all - more or less - grounded due to the Coronavirus,it´s time to learn new skills and to upgrade on everything it takes to become a professional remote worker. Freelancers have already refined these skills to perfection, so now it´s up to the rest of us to draw on their expertise and experience and to build the right toolset for ourselves.
Having worked in the digital area since 1995, I have been through a ton of tools and solutions, but now that everything is cloud-based and available as subscription services, it has never been easier. Pretty much all tools are available in a free-to-try version, so you can figure out which ones work best for you, your role and job situation.
I have collected all the tools that I find relevant and necessary as part of a full-grown toolbox. I have added links to their website and a price indication (they all come with different subscription possibilities, so I have chosen the one, that I find most relevant as a starting point) to make it as easy as possible for you to get an overview and to get started.
Of course, it adds up economically, so choose wisely and maybe start with the 5 most important solutions you don´t already have or use.
Please let me know what you think in the comments below and please feel free to add tools that I have either forgot or just don´t know about.
Enjoy.
Great for webinars or online meetings. Support breakouts, group sessions, one2one conversationsPrice indication:$15/month/host, up to 100 participants
Video meetings, one2one chats, screen sharingFree to use
Great danish developed Webinar tool for corporationsPrice indication:$199/month for the whole company
Online document storage, document sharing, document collaboration, backupPrice indication:Dropbox: $120year for 2 TbGoogle Drive: free up to 15 GbiCloud: $1/mont for 50Gb
Online brainstorming and collaborationPrice indication:Mural: $12/month/userMiro: $8/month pr. User
Collaborative mind mapping, note-taking and idea developmentPrice indication:€8.25/month/user for multi user version
Tool for meeting management and interactive group brainstorming. Template-basedPrice indication:€32.5/month/facilitator
Tool for inspirational mood boards and idea collectionFree to use
Crowd involvement and interactionMake it possible to create quizzes or vote and share your opinion in the meeting or during your presentationPrice indication:$10/month, unlimited questions pr. Presentation
Q&A tool and polling during online townhalls or webinarsPrice indication:€600/year max. 5 polls pr. event
Run innovation challenges, manage employee ideasPrice indication:€349/month, up to 200 participants
Link-based distribution of large files (up to 2gb for free)Free to use
Create dynamic interactive online corporate learning experiencesPrice indication:€195/month, up to 30 users
A platform for large online conferences, seminars, and eventsIncluding registration, reception, mainstage, networking, one 2 one networking sessions, and online boothsNo price indicated
Daily team communication and quick messagingPrice indication:€ 6.25/month/user
Collaboration platform for teams incl. online meetings, chats, document sharingTeams only work if you are using Microsoft office 365Price indication:$12.50/month/user
Keeping track of who is awake and available to call and who is asleep in a large international collaboration team. Post activities, tailor the timetable to the team, etc.Pricing:$119/year, up to 15 people
Work management platformSupporting teams, projects, goals and daily tasksPrice indication:€ 11/month/user
Kanban based Project collaborationPrice indication:Trello: $10/month/userMonday: price not indicated
Interactive learning platformPrice indication:$80/year for one class
Learning games and online quizzesPrice indication:$10/month pr. Host, up to 20 players pr. Game
With all these services, which require you to log in with a name and a password, it can be a real challenge to remember and control all your passwords. These two password services allow you to create a vault where you can store all your passwords, and then only use one to access your services online while still keeping your logins safe.Price indication:Lastpass: $3/month/user1password: $4/month/user
Ps. If you work in UX or design. Here is a great curated list of tools for that specific area:
Serial Entrepreneur and gadget freak. Co-founder and Chairman at Liveshopper.net. For more info: timfrankandersen.com
See all (166)
57 
57 claps
57 
Serial Entrepreneur and gadget freak. Co-founder and Chairman at Liveshopper.net. For more info: timfrankandersen.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/aws-for-machine-learning-part-1-a4731872086a?source=search_post---------338,"There are currently no responses for this story.
Be the first to respond.
1. Cloud Computing.2. What is AWS?3. AWS for machine learning
Before the concept of cloud computing came into the picture back then even if a website needs to be hosted companies had to buy huge servers and maintain them. It was a huge cost and inefficient workforce diversion for the companies which wanted to focus on the actual task at hand rather than the maintaining of these servers.
Some other companies saw this as an opportunity which went ahead and bought these huge servers and had a huge collection of servers and rented them out to other companies.
It is a win-win for everyone since it is cheaper and easier for the companies that wanted to focus on their application/product rather than maintaining these servers. This is how Cloud as a service started
We all use electricity, how do we pay for this we pay according to the number of units used. We don't worry about how it is generated?, How it is transported?, How it is maintained? right!!!
Similar to that we rent a server on the cloud, we pay for the amount of server used. The cost will usually depend on the size of the server and the amount of time it is being used.
If a computing service is provided to you in this format it is called cloud as a service
The service has basically three types.1. IaaS: Infrastructure as a service2. PaaS: Platform As A Service3. SaaS: Software As A Service
SaaS —Gmail would be one of the best examples. Where our only concern is to write the email and send it out we do not worry about anything else like what is server goes down where it is stored etc.
PaaS — Imagine if you are the owner company of Netflix/Facebook. Then PaaS will be used where your application Facebook/Netflix is hosted with the data like videos or images. Rest everything is taken care of by the service provider.
IaaS —It is made of highly scalable and automated compute resources. IaaS is fully self-service for accessing and monitoring computers, networking, storage, and other services. Examples:- DigitalOcean, Amazon Web Services (AWS), Cisco Metacloud, Microsoft Azure, Google Compute Engine (GCE).
Usually, Larger companies may prefer to retain complete control over their applications and infrastructure, but they want to purchase only what they actually consume or need to use IaaS.
Name some large enterprises in your mind all of them are into providing cloud services. Why?? This fun fact will provide you the answer. In 2020 AWS is responsible for more than 63% of the entire company’s operating profits for the year. And mind you it is one of the largest, if not the largest companies in the world.
Popular Cloud Service Providers:
So now you must have some idea on why I chose to explain AWS instead of the other service providers
More references:
Make sure to follow me on medium, linkedin, twitter, Instagram to get more updates. And also if you liked this article make sure to give a clap and share it.
Join our WhatsApp community here.
Get smarter at building your thing. Join The Startup’s +750K followers.
37 
Thanks to Elliot Gunn. 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
37 claps
37 
Written by
Data Scientist @IndianMoney.com
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
Data Scientist @IndianMoney.com
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/savedroid/what-we-do-to-make-sure-that-our-software-runs-correctly-and-the-data-of-our-users-are-protected-f8fd5a0a05e3?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
Nowadays, the internet is perceived as an unsafe place. For how long do you need to exercise your brain in order to remember issues related to viruses and malware, phishing scams and/or breaches in data protection regulations in the last year? 30 seconds? 1 minute? Well, if you are still thinking, does Facebook ring any bells?
However, the first sentence of this article represents a too simplistic perspective. If we were to describe the internet in an also simple way, we would say it is a place to store information. Just like we store objects in drawers. Imagine that you have a very confidential information and you want to protect it. Then, element logic tells you not to leave this document on your desk at work or in an open space and not to tell anybody where you placed it. You rather put it inside of the drawer, lock it, and keep it to yourself. Since nobody knows it is there, nobody will be looking for it. From this example we can withdraw the conclusion that it is up to us, whether we are safe on the internet or not.
To make sure our software is safe, we use Amazon Web Services — AWS, which is not confined by any specific location. It’s a cloud based platform that offers to store the data in a space on the internet. We decided to use this Amazon service and resign from a land-based server due to a few very important reasons.
First of all, the cloud is more flexible. That means that the computing power is almost unlimited there. In other words, memory and space get either bigger or smaller, according to our needs. The cloud is ductile and works like a Play-Doh, which helps us to keep our apps optimized. Whereas, capability of a land-based server is very fixed and limited.
Another important aspect about the cloud is, that it is not based on a specific geographical location. That means if, for some unknown and unexpected reason, suddenly there was no electricity in the whole of Frankfurt, it would not affect our apps at all. It is very unlikely to have no access to the data stored on the cloud, while with land-based servers this scenario is possible. Not only one might lose the access to data, but also the data itself due to some reason might get damaged or even erased, when it comes to land-based servers.
What makes the cloud a very secure storage place is its infrastructure. It enables our software architecture to be based on microservices, which can be improved easily — focussing on simplicity rather than complexity. There is a very simple rule we are following here: Do not put all of your eggs on one basket. Let’s imagine an app as a Lego castle, which consists of many Lego parts. Each part is responsible for something different. Like one brick only for logging in, another brick for logging out, and yet additional ones for choosing the Wishes and for choosing the Smooves. That makes any action separated. AWS CloudWatch is automatically monitoring the software, therefore, once a challenge appears, we can very quickly isolate and fix it, without influencing the other bricks. This is called “separation of concerns”. The bricks, however, are connected to each other, some of them communicate and all together create a very smart organism, controlled of course by a human.
This enables an infrastructure which is easily extensible. A new button or a new functionality can very easily be added to or removed from the microservices.
This explains what our backend developers are responsible for. However, what you see — two apps and three webpages — is a job of our frontend developers. This simple picture illustrates exactly how it looks in reality ;)
Well, it is always nice to know, how the software you are using works. Second of all, it is important for users not to have any gaps in the app, which would make them wait longer for an action to be processed. The cloud prevents exactly this kind of situations.
Another major advantage of AWS is very good data protection practices. We are the customer of Amazon, which owns this cloud. Every action taken by a user is marked by a token, which could be a sequence of symbols, letter or numbers. Hence, it is completely anonymous. Every following action will require another exclusive token. It goes on and on. Thanks to that, the data stays secure. We would like to also stress the fact, that we fully respect GDPR.
Last but not least, our AI is also operating within the cloud. Thus, being divided into microservices. This idea of share-out is reflected in the portfolios of our users. Storing the assets that are split makes them safer. Now, not only we provide for you the service of storing your savings safely in different cryptocurrencies, but also our AI will tell you which combination is the most optimal for you at the moment. It couldn’t be easier! Answer yourself a few simple questions: would you be able to do it on your own, and would you have the time and a will to do it?
We want to bring to the market a product that would be useful, unique and will serve the users in the best way possible. We established the software, which is smart, intelligent, secure and we want to offer it you. Why not try it out then?
Let us know in the comments, if this article was helpful for you to understand how our software works, and if the idea of keeping your eggs in different baskets seems appealing to you!
savedroid’s Team
It takes people of all makeups to create apps that reshape…
100 
100 claps
100 
It takes people of all makeups to create apps that reshape experiences, and boost you to reach your goals. That’s why we mix data, intuition, and passion. We’re some of the best at what we do. And want you to be too. These are stories from our team.
Written by
Fintech rocking the world of crypto savings. www.savedroid.com
It takes people of all makeups to create apps that reshape experiences, and boost you to reach your goals. That’s why we mix data, intuition, and passion. We’re some of the best at what we do. And want you to be too. These are stories from our team.
"
https://medium.com/@womenwhocode/aws-security-guide-7-best-practices-to-avoid-security-risks-64f5ecb5ef20?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Women Who Code
Jan 10, 2019·7 min read
There are many different benefits when engaging with Amazon Web Services as your cloud platform either as a standalone provider or as a multi-cloud, hybrid computing environment. AWS comes with a level of flexibility and agility in its service and infrastructure which makes it possible for an organization’s network to be innovative, responsive and open to quick change.
However, AWS comes with own set of challenges as well. While Amazon provides its subscribers with a wide variety of security features, organizations often do not use them to its fullest. While cloud security can be overwhelming at times, robust cybersecurity is still possible to achieve as long as potential pitfalls and best practices are kept in mind.
Cloud resources are mostly transient. This makes it a challenge to keep a log of assets. According to a recent research, the usual lifespan of a natural cloud resource is a little over a couple of hours.
Additionally, companies use environments that make use of different cloud accounts across geographical regions. This encourages scattered visibility and makes identifying risks increasingly tricky. What you cannot see, you cannot secure.
Ensure you are using a cloud security solution that gives visibility into the number of type of resources you use across geographies and accounts in a single dashboard. These can include load balancers, virtual machines, users, security groups, etc. A better understanding of your environment allows for the implementation of more minute policies and inhibits risk.
Root accounts are capable of doing the most damage when unauthorized individuals get access to them. Often time administrators miss disabling API root access, providing a doorway for malicious users. Here’s a quote from the AWS docs.
We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks.
Most of the time, AWS root accounts should not be accessed by anyone, including top admins. These should not be shared with users or applications. It is imperative to protect root accounts via multi-factor authentication and should be accessed as less as possible.
As mentioned earlier, misplaced and stolen credentials are among the leading causes of security breaches. AWS IAM can manage all user groups and accounts with detailed policies and permissions.
However, administrators regularly assign overly necessary access to different AWS resources. This not only allows users to modify and gain access to resources they shouldn’t get but also allows malicious users to cause additional harm if these accounts are breached.
Similar to any other permission-based systems, the configuration of IAM should also be compliant with the underlying rule of least privilege. This translates to any groups or users should only get permissions as required to perform their regular jobs or tasks.
Often, IAM access keys are not rotated. This leads to weaknesses in the IAM’s ability to keep user groups and account secure and allows attackers a larger window to gain access to them. Additionally, rotating IAM, access keys ensures that old keys are not used to get access to critical services.
Rotating or changing your access keys a minimum of once a quarter is a good idea. In case users have the right level of access, they should be able to rotate their access keys themselves. Here’s an example policy that shows how you might create a policy that allows IAM users to rotate their own access keys:
{ “Version”: “2012–10–17”, “Statement”: [ { “Effect”: “Allow”, “Action”: [ “iam:ListUsers”, “iam:GetAccountPasswordPolicy” ], “Resource”: “*” }, { “Effect”: “Allow”, “Action”: [ “iam:*AccessKey*”, “iam:ChangePassword”, “iam:GetUser”, “iam:*ServiceSpecificCredential*”, “iam:*SigningCertificate*” ], “Resource”: [“arn:aws:iam::*:user/${aws:username}”] } ]
}
Leading cloud security incidents are down to stolen or misplaced credentials. Unfortunately, it is quite common to notice access credentials for public cloud environments on the web. A recent example would be the Uber breach. Organizations must prioritize a way to detect and close off these account compromises.
Robust password policies, coupled with multi-factor authentication (MFA) needs to be enforced within AWS environments. AWS suggests allowing MFA for all accounts that currently have console passwords. Users need to determine which accounts have pre-existing MFA. Then under IAM, ‘MFA Device’ needs to be checked for all users. Devices such as smartphones can also be used as an additional authentication device.
As mentioned earlier, misplaced and stolen credentials are among the leading causes of security breaches. AWS IAM can manage all user groups and accounts with detailed policies and permissions.
However, administrators regularly assign overly necessary access to different AWS resources. This not only allows users to modify and gain access to resources they shouldn’t get but also allows malicious users to cause additional harm if these accounts are breached.
Let’s take an example. User A who is new to IAM wants to create a new IAM user with privileges to manage EC2 instances for their AWS infrastructure. However, since User A is new, they are not sure what to put in the role’s access policy. To meet the immediate requirements, he/she attaches the PowerUserAccess policy that looks like this:
{ “Version”: “2012–10–17”, “Statement”: [ { “Effect”: “Allow”, “NotAction”: “iam:*”, “Resource”: “*” } ]
}
This is highly not recommended because any breach of this particular account will breach the entire AWS infrastructure for that particular company.
Similar to any other permission-based systems, the configuration of IAM should also be compliant with the underlying rule of least privilege. This translates to any groups or users should only get permissions as required to perform their regular jobs or tasks.
AWS offers more information about the policies that were last accessed under the Access Advisor tab. User A can scope down these application-specific permissions to get rid of permissions that are not used.
It is the responsibility of the organization or a dedicated team within the organization to ensure all of the latest security patches are applied to hosts within the cloud environment the organization is engaging with.
Traditional network vulnerability scanners are active on on-premise networks, but not so much when it comes to vulnerabilities in the cloud environment. This is one of the factors that needs to be taken into consideration while setting up your cloud migration strategy.
Ensure that the host(s) your organization uses is up to date when it comes to patches. All necessary fixes are released and applied by your OEM vendors. Third-party tools can assist with mapping the data originating from the host’s vulnerability updates. A prime example would be Amazon Inspector.
Alternatively, you can also manually perform updates as follows. The steps below are intended for use with Amazon Linux.
You can see all active screens by running
[ec2-user ~]$ screen -lsThere is a screen on:17793.pts-0.ip-12–34–56–78(Detached)
1 Socket in /var/run/screen/S-ec2-user.
You can then grab the process ID from the previous command and use it to connect to a detached screen as follows:
2. Run the yum update command to run the updates. Furthermore, you can add the — security flag to install just the security updates.
If you need to update multiple EC2 instances, consider using EC2 run command update to manage your Linux instances.
Security groups can be imagined akin to firewalls that control and monitor traffic to AWS environments. Admins again are at fault at often assigning this security to a range of IP addresses that tend to be broader than what is necessary.
Research by a reputed cloud analyst shows about 85% of resources that are associated with security groups have no restriction over outbound traffic whatsoever. An uncomfortably large number of companies were not adhering to network security related best practices and also had risky or misconfigurations.
Best practices in the industry rule that any outbound access needs to be restricted to inhibit data exfiltration during a security breach or any accidental data loss.
IP ranges should be limited when assigning them to security groups. This should be done in a way that though all systems network smoothly, systems are not left open unnecessarily. If you use 0.0.0.0/0, you enable all IPv4 addresses to access your instance using HTTP or HTTPS. To restrict access, enter a specific IP address or range of addresses.
It might sound like a challenge to keep your AWS cloud service secure from vulnerabilities. Many organizations commit mistakes which exposes their network and data at risk primarily due to poor configuration, lack of adequate planning and vulnerability detection and scanning methods.
Thanks to Amazon, there are some useful security measures that organizations can use to control the security of their environment. Unfortunately, most do not. Some of these include IAM, AWS CloudTrail and permissions on cloud resources which can be configured in a manner to suit the organization’s requirements.
Originally published at www.womenwhocode.com.
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
27 
27 
27 
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
"
https://blog.cloudboost.io/how-to-prepare-for-the-hybrid-cloud-a-checklist-6f3f4ff562c1?source=search_post---------341,"This is a guest story by Gilad Maayan, a technology writer having 20 years’ experience in developer tools and IT infrastructure.
As more enterprises look to leverage the benefits of cloud computing, the hybrid cloud has emerged as a popular option. Let’s clarify exactly what defines each type of cloud service in modern IT architecture:
In the hybrid cloud, public and private cloud services are connected using encryption technology over a wide area network. The hybrid cloud’s popularity is evident in the fact that 73 percent of surveyed organizations are pursuing a hybrid cloud strategy.
Flexibility: enterprises can leverage the scalability of public cloud services to store growing data volumes or run applications with high workloads, all while maintaining the governance required for sensitive data by restricting it to the private cloud.
Easy Scalability: upgrading on-premise systems to incorporate growing workloads or more data is costly and time-consuming. The hybrid cloud solutions offer effortless scalability by simply provisioning more public cloud resources (or in the case with serverless computing — scale automatically), and the provider takes care of the rest.
Agility: when heavy application usage arises for mission-critical apps, organizations can quickly leverage the public cloud for more computing resources and switch some workloads to the public systems, meaning less downtime, better performance, and fewer outages.
Cost-effective: the ability to dynamically adjust the amount of capacity you use in the public or private environment can lead to impressive cost-effectiveness. Many public cloud providers offer a pay-per-use pricing structure, providing even more cost benefits.
Now that you’re clear on what the hybrid cloud is and what it offers, let’s go through a checklist of some important points you need to consider when preparing for a transition to hybrid cloud infrastructure.
When preparing to implement a hybrid cloud setup, you’ll avail of the services of one or more public cloud providers. It’s a good idea here not to just dive in and choose one of the more popular names like AWS or Microsoft Azure, without performing appropriate due diligence and research.
After all, moving to the cloud, even though it has the potential for many benefits, still represents a significant investment of time and resources for your company. Your choice of public cloud provider is a decision you need to get right, ultimately by thoroughly assessing the services offered by each provider and deciphering how well those services align with your planned use of the public cloud.
Perhaps you want to solely use the public cloud for application workloads, in which case you’ll want something like AWS EC2 or Microsoft Azure. The key is to recognize that there are a slew of options out there. Thoroughly researching the alignment of available public cloud services with your business goals will help you choose the right cloud provider that gives you the performance, reliability, and availability you need.
It’s always prudent to have a disaster recovery plan in place that clearly documents the processes, policies and tools you plan to use in the event of a natural or human-induced catastrophe.
Downtime costs enterprises an estimated $100,000 per hour, so it’s clearly a real priority to cope with any unexpected outages promptly and minimize downtime, particularly for those mission-critical applications that are central to your infrastructure.
As part of a hybrid cloud movement, cloud backup solutions are also essential. If you choose to use the public cloud to store data, you need to realize that cloud providers can experience outages too. This is where cloud backup solutions can come in useful. Top cloud vendors offer built-in backup and recovery solutions, such as Azure’s site recovery solution or AWS’s disaster recovery services, while third-party vendors (such as N2WS) offer managed disaster recovery and expanded features on top of the default DR offerings.
By backing up data in the cloud to some sort of backup or disaster recovery service you ensure that you are prepared for the worst-case scenarios with your hybrid cloud implementation.
It’s important to have a full understanding of your compliance requirements before the transition to the cloud. If any sensitive data will enter into public cloud systems, you must ensure the cloud provider has the required certifications to comply with any government policies pertaining to sensitive information you handle.
The Health Insurance Portability and Accountability Act (HIPAA) in 2013 extended the compliance rules to businesses that store protected information on behalf of their clients. This means that for any sensitive health information, there’s shared accountability between your public cloud provider and your business. Appropriate investigation is necessary to make sure your cloud provider complies with HIPAA rules.
The Payment Card Industry Security Standards Council published a revised set of guidelines in 2013 in relation to cloud environments, specifying that, “there is shared responsibility between the cloud service provider (CSP) and its clients.” when protecting credit card information.
In summary, you at least need to check your cloud provider’s website for a specific compliance section, and require proof that its systems have been independently verified as complying with relevant regulations for data in your industry.
Any successful hybrid cloud environment relies on the low-latency connection between public and private cloud systems for everything to function smoothly. This all comes down to efficient network management; the last thing you need is to go to the effort of implementing a hybrid cloud set up only to encounter bandwidth issues and network bottlenecks.
An option often considered is updating Internet lines for increased speed, but this can quickly become costly. A private networking option such as a virtual private network can provide the importance you need for a dedicated connection between private and public clouds. You could also boost public cloud reliability by availing of a direct connection service, which are offered by both Azure and AWS.
The whole point of using a hybrid cloud set up is to leverage the scalability and convenience of the public cloud to reduce the burden on on-premise systems and reduce the cost of your IT infrastructure. On this note, it makes sense to plan properly and balance workloads for applications. You need to decide which apps it makes sense to host on public versus private clouds.
The best advice in this regard is to perhaps segment your applications first into those requiring the most control, monitoring, and performance management; these apps can be hosted in your private cloud. Other considerations include the need to keep information confidential and the data volume or scale of the data produced by your apps. Additionally, match the business use case with the most appropriate solution. For example, Microsoft Azure would be best used for running a .NET-based application because it naturally has an advantage with such apps.
Instead of doing everything at once, first identify any processes, applications or tools you could easily migrate to a public cloud system. These initial workloads can act as pilot projects, giving your company the chance to learn a lot about the intricacies and nuances of smaller migrations before tackling a larger migration to a public cloud provider.
Also, with each migration phase, check its impact on applications before making the move. This way you can figure out which applications need updating before or during the migration process.
The ability to orchestrate workloads and data between public and private clouds can provide impressive cost savings, however, you need to carefully budget for your hybrid cloud IT investment. Provisioning a private cloud infrastructure requires significant upfront investment in software and possibly hardware. However, heavy reliance on public cloud resources can lead to large monthly usage bills. The key is finding the right balance that fits your company.
One of the main draws of a hybrid cloud setup is how easy it is to extend your IT capabilities by simply provisioning more resources from the public provider when your storage or computing needs grow. However, private clouds are scalable too, so the right scaling strategy is important.
You need to decide on an appropriate scaling strategy that includes factors such as anticipated costs, the sensitivity of data, and the control you need over the applications that you scale. These factors will ultimately dictate whether you scale out particular data or apps to the public cloud or private cloud.
Security is obviously imperative in any IT setup, and in the hybrid cloud, it can be tricky to get right. There is a need to bridge security between private/on-premises systems and public cloud systems into a cohesive whole.
The most important thing is to identify security needs early and put a consistent security policy into operation in advance of migrating to a hybrid cloud setup. The cloud computing security policy should consider network security (encryption), unprotected APIs, risk assessments, regulatory compliance, data redundancy, and proper authentication.
The checklist outlined here has overviewed some of the most important things you should be thinking about to prepare for your organization’s move towards implementing a hybrid cloud IT infrastructure. Putting it all together, the cloud can seem like a complicated thing that isn’t worth the effort or the complexity. However, the benefits of more agility, increased scalability, and much higher flexibility are worth it in the long run. Furthermore, and even more tellingly because cost is nearly always the key constraint for a company’s IT infrastructure, the hybrid cloud leads to cost savings of between 5 percent and 30 percent for enterprises.
Gilad is the CEO and Founder of Agile SEO, a digital marketing agency focused on SaaS and technology clients. He has done strategic consulting, content marketing, and SEO/SEM for over 150 technology companies including Zend, Oracle, Electric Cloud, JFrog and Check Point. Together with his team, he’s helped numerous tech startups move from zero to tens of thousands of users, and driven double to triple digit growth in conversion and revenue for established software businesses.
Want to write an article for our blog? Read our requirements and guidelines to become a contributor.
Liked the story? Clap us so more people can find it! 👏Originally published at AltexSoft’s blog: “How to Prepare for the Hybrid Cloud: A Checklist”
The Realtime JavaScript Backend.
14 
14 claps
14 
Written by
Being a Technology & Solution Consulting company, AltexSoft co-builds technology products to help companies accelerate growth.
The Realtime JavaScript Backend.
Written by
Being a Technology & Solution Consulting company, AltexSoft co-builds technology products to help companies accelerate growth.
The Realtime JavaScript Backend.
"
https://medium.com/dadi/ama-recap-october-6th-2018-a722f2caa739?source=search_post---------342,"There are currently no responses for this story.
Be the first to respond.
Where are we on collaborating with virtual masternode hosting websites like WireHive or using VPS for nodes?
Joseph DenneEfforts on this front have progressed significantly. We have a build process for all major architectures in place, and on the back of that working nodes for all of the major VPS providers. (Yes, this includes AWS, Digital Ocean et al.) We will be providing how-to articles in due course, but note that our primary focus is in the building of a network outside of the data centre environment, so this is a relatively low priority for us.
Please explain the procedure for POS staking of tokens. It seems like DADI will take possession of all the tokens. What steps are being taken to ensure the security of tokens and is there some insurance cover for stakers if something happens to the smart contract?
James LambieWe’ll be asking node holders to provide an Ethereum wallet address on their account page within my.dadi.cloud. Once we have that, we will provide the address for a smart contract that will receive the PoS. A transaction history will be available to show that we have received the PoS.
As with all Ethereum smart contracts, the PoS smart contract will be publicly accessible. The contract will be audited by a third party to ensure no vulnerabilities. The immutability of the blockchain guarantees stability and availability for all published contracts so the risk of something happening to the contract is pretty low.
Any partnerships with IoT industry planned or in the works?
James LambieWe have some well progressed discussions with multiple IoT providers, there will be more information on this in due course. Watch this space!
Joseph DenneTo note, because of the sensitivity of this space we really can’t talk about this in any detail, but there are several conversations open and ongoing.
Who won the DADI Foundation Challenge Award?
Joseph DenneThe Foundation will be announcing this themselves in the coming weeks. We’re very happy with the level and quality of the applications received — over 100 comprehensive applications from diverse set of organisations all over the world. I couldn’t be more pleased with the start that the DADI Foundation has had. Jennifer and her team are doing amazing work.
Will DADI consider making Gateways that the community can just buy instead of trying to build one ourselves?
James LambieYes, we love this idea. We’re planning on making elements of the network backbone rentable, either on a whole or part ownership basis. The idea is still on the drawing board, but we’ll be progressing it in earnest as we move in to 2019.
With so many other blockchain companies going into CDN space, how does DADI intend to differentiate given that it’s not yet well known?
Joseph DenneWell as far as I know we’re the only viable — as in live with production traffic — decentralized CDN in the world. So, there’s that. We also have market leading functionality. DADI CDN isn’t just about content distribution, it’s about media manipulation. You can use it to dynamically resize and treat your images and other assets in real time, based on the nature of request from your end user. For example, you can serve more compressed images to users on mobile connections. Want to see what it can? You can check out a live demo here: https://docs.dadi.cloud/sandbox/dadi-cdn
Finally, it’s important to state that DADI is much, much more than just a CDN. First and foremost, we’re a network. And DADI CDN is but one decentralized app running on that network. There are many more to come.
Can/will you be releasing progress reports available to the community? For example, quarterly updates on events attended, partners made, advisors joined, developments achieved, things that changed in those months.
Joseph DenneMea culpa 😳I started this in Q4 last year, but missed releasing an updating in Q2 and Q3 2018. I’m not sure why, but will put it down to quite how busy we’ve been! (So much good stuff being built here!) I’ll get the missed quarters written up ASAP, and will then continue to publish updates quarterly. If I miss any again, please shout in my general direction.
Any other advantages for network participants besides earnings that could be highlighted in future campaigns?
James LambieEarnings is the big one of course, but we are also looking at initiatives including a carbon offset programme for contributing devices, or partnerships which will facilitate part or full payment for new devices that connect to the network over a contracted period.
Follow up: Sorry, don’t follow. You mean the person is reimbursed for their device?
James LambieTo be clear, the carbon offset programme will use revenue created by the network to deliver green initiatives (for example, planting trees) to offset device usage.
I understand that Gateways and Stargates on-boarding are not due until Q3 2019 at the earliest. What options are there for me as a holder of over 800k tokens in the immediate future besides running over 100 Hosts? What plans does the team have for people like us to discourage us from reducing stake?
Joseph DenneYou can reach out to us directly. We’re onboarding with early partners now, and intend to extend this in the coming couple of months to include larger token holders. Exactly what this looks like is TBC, but I would assume that it will take the form of buying in to a Gateway within the network backbone (through the attribution of your POS).
What contingencies are there for Founding Node owners if the device develops an issue or a component gets damage? Will we be able to get replacements from the market or need to contact the team?
James LambieThe team is here to support you and we’ll do our best to assist members of our Founding Node community. It’s possible that we may have replacement components available at cost, but I guess it depends on the nature of the problem. We’ll be including a Returns address when sending out the Founding Nodes so in the worst case you could send it back to us in London and we’ll see what we can do.
Does DADI work with market markers for steady volume on trading exchanges? I see the volume levels have been impeccably steady over time.
Joseph DenneYes, we work with one of the most reputable market-making businesses to provide liquidity across key exchanges. We’d be happy to share more information on this relationship but will need to speak to our partner first, which I’ll do next week.
Follow up: Will there be any DADI OTC options in the future, maybe through this leading market maker? I know a team with significant interest.
Joseph DenneYes. We can facilitate this now and would be happy to make introductions.
What other reasons make DADI a better choice instead of Amazon AWS besides lower cost and decentralized node geolocation? Is DADI as secure and stable as Amazon AWS and how you can prove it?
Joseph DenneIsn’t a lower price point and decentralized — thus faster — delivery enough?! Well, in addition to this DADI’s web services provide the fastest route to market for a digital product, making it the lowest TCO (total cost of ownership) of any competing stack.
I’m interested in what makes you think that AWS is inherently secure and stable… it’s not. It’s only as secure and stable as the application that you deploy to it. And yes, DADI competes on these fronts. The proof is in the pudding: use it and see for yourself.
James LambieAlso, the usability of the interfaces provided by AWS for every single one of their products is appalling. DADI software has been designed for ease of use and happy users.
Paul ReganAlso, AWS exists to buy Mr. Bezos yachts and jets. DADI exists to share the wealth created by digital services. And it’s greener too. 😄
If my network runs with a VPN, is there any disadvantages/advantages to this? For example, can I VPN from Europe to US to have my node operate in another country that may see higher traffic? And would this impact the performance, or will the node be aware and act differently?
Arthur MingardWe’d suggest adding bypassing rules to your VPN in order to connect within the correct region. The Host application is smart enough to reduce the number of hops, connecting to a nearby Gateway even when running through your VPN, but to get most out of it, it’s best to use a local VPN.
When will DADI begin to advertise in earnest?
Joseph DenneTBC. Our channels strategy is focused on partnerships in the first stages: with agencies, with OEMs and with brands. Once the network reaches scale we will begin direct customer sales, which will include advertising.
What kind of devices are you going to start selling in the DADI Shop from January 2019?
Joseph DenneOh, how we’d love to answer this in detail now, but we can’t talk about it yet! Suffice to say we have a number of new devices in development with our industrial design partner Blond, each of which build on the Founding Node concept — the first should be in market during Q1 next year and be of a larger capacity. (That sound you can hear in the background is everyone telling me to stop talking! More details after Christmas. Promise.)
Is the mainnet still scheduled for Q4 and what percentage of existing clients have moved over to the DADI network so far, based on the upgrade agreements the team mentioned earlier?
Arthur MingardThe mainnet is live (to be pedantic 🙂) but yes, we remain on track for ‘Constellation’ late in Q4, which is where clients will be able to start using CDN without our hand-holding. In the meantime, we are working with three existing and three new clients to onboard products to the network. We cannot be more specific out of respect to those we are working with but will be making announcements in due course.
Will there be a platform release at some stage that will allow contributors to connect with each other to pool resources and share in node returns?
Arthur MingardThere are currently a few models we’re working with to allow shared ownership and thus revenue sharing, being careful to restrict the potential excessive dominance from large/irresponsible providers. In terms of resource pooling, there are no benefits to sharing hardware resources, as a single machine, even low powered IoT devices, are capable of contribution. The network will rely as much on spare computational power as dedicated resources, reducing the barrier for entry. If your computer uptime meets the minimum contribution duration, you’re good to go.
Is it recommended (for privacy/security reasons) to separate the founding node in/from the private home network? If so, will you instruct inexperienced people somehow how to do this (top level instruction)?
Arthur MingardFounding nodes will remain self-updating for all major updates, security patches and fixes. All measures are taken to ensure that there is no risk both to and from your home network to the Host application.
DADI Store is planned to be network-ready in Q1 2019, how soon will the community be able to test a user-ready beta version?
James LambieWe hope to have a beta version available for testing by the wider community by the end of Q4.
When will the global map on dadi.cloudshow live data from the network?
Arthur MingardThis will come shortly after constellation goes live, in line with the release of the network telemetry suite.
The team has been working together for about 5 years now which is commendable. With a finished product out now and network scaling commencing, where do you see DADI in the next two years in terms of the technology, the network and token value?
Joseph DenneI’ve been working with my co-founders for 20 years, if you can believe it. Mad. I don’t like to speculate on token value as $DADI is a utility (although it’s bonkers that the token price is so low given that many top 100 tokens don’t have a working product, have little to no traction and in many cases are not even registered businesses and as a result are unbanked!), but we plan to have clients in the hundreds and contributors in the thousands. Our network will have been through many development iterations and we’ll have our full proposed suite of applications ready for client use, as per our recently updated roadmap: https://dadi.cloud/en/roadmap/. Our marketplace will be established, too, which means third party applications — including those of our partners — will be in full swing.
I also suspect there will be a number of people wondering why they didn’t sign up to contribute two years ago when they first had the chance.
With regard to the iOS and Android apps, how far along is development? Do they affect performance, battery life and are they invasive to the device?
Arthur MingardWe have a proof of concept for iOS, meaning we know that Host will run in an app environment. There’s a lot of work to do here in relation to performance impact testing, but we’re off to a great start.
How do you intend to deliver updates safely to the nodes on the network without disrupting them or causing undue downtime?
Arthur MingardThe update process is load balanced. When an update is deployed, the rollout process takes into account the current application load, introducing redundancy when required, before commencing a staggered blue/green deployment process.
Is there a possibility we’ll see ‘DADI Compatible’ labels on Philips light bulb packages in the future? Is this how DADI sees it? What would be the advantage for Philips and its customers to have a DADI compatible light bulb?
James LambieSetting Philips aside as the example for a moment, yes. We see a huge opportunity for partners to include DADI technology in connected devices and bring with it myriad possibilities for new revenue or customer subsidy, for example.
As participants in the DADI network, will node owners sign an agreement?
Joseph DenneThere will be terms & conditions attached to provision of the POS, which itself is a prerequisite to attaching a node to the network. So, in short: yes.
Is DADI interested in selling (anonymized) sensordata to databrokers/datahandlers like DataBrokerDAO or Streamr?
Arthur mingardThe DADI network is built to support a wide range of applications, and as we progress through the roadmap the possibilities increase exponentially. Streamr, DataBrokerDAO and other realtime data streaming applications will most certainly be deployable on the network, but we aren’t interested in selling user data. Privacy is important to us.
When Constellation launches at the end of the year, how many Hosts does the team expect to onboard and what will be the estimated network capacity?
Arthur MingardOn top of the 300 Founding nodes, we’ve provisioned a distributed backbone as part of our partnerships with a number of datacenter providers. As a result, there’s a lot of flexibility in the network capacity with comfortable room for growth.
Written by Bolaji Oyewole. Bolaji is the Community Manager at DADI.
Faster. Greener. More Secure. The future of the cloud is Edge.
57 
57 claps
57 
Upholding the founding principles of the Web through the democratisation of computational power.
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
"
https://medium.com/@nancyadams_73524/time-to-write-thank-you-notes-e866b32ade36?source=search_post---------343,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nancy Adams
Jan 11, 2018·4 min read
I love the people who send me presents. I love the people who don’t. I am thankful for the people who send me thank you notes. I am thankful for the people who don’t. I do like reading the thank you notes. I get to relive the gifting. I get to think about the person who wrote the note. I like getting a letter these days. Opening the envelope. Admiring or deciphering the writing. Wondering if the letter was written at the kitchen table? Dining room table? Desk? On a lap over a book?
My addresses are in my smart phone. I won’t use the other address books as they are 1) not mine and 2) seriously out of date. On the outside the three are not too different. Small, able to fit in a pocket or a purse or a palm,about the same width, 3 inches. My smart phone is the longest, at 5 inches. The book marked “Address” with an Art Deco font has a spiral edge. All three have the alphabet down the right side. The two older books you open up. The right edge of the page is almost completely cut away for ‘A’, not cut away at all for ‘Z.’ I can flip to the right letter of the alphabet by putting my thumb on the letter and opening the book up. The smart phone takes me to the page with a touch of the letter.
The old address books have a line for name, street and city. The spiral address book has a line for telephone numbers. The other old one just a suggestion at the top of each page. The plain book has one phone number: “Del 0277. The address is Buffalo. The spiral address book has one number as well, Lincoln 8430. People were not using phones to keep in touch, at least not these people, not then.
My address book too many items to list. Phone: home, work, iPhone, mobile, main, home fax, work fax, pager, other. Email: home, work, iCloud, other, add custom label. I can add ring tone, url. Address, birthday, date, related name social profile, instant message, notes, field, linked contacts. Off my brain goes into the wild blue yonder, or wherever the ‘cloud’ is. The old address books take me firmly to real places, Buffalo, Rochester, Erie, Franklinville. The spiral address book has lots of military addresses.
Both old ones have dates next to entries, written neatly on the left. The earliest date in the spiral book is 1939. The earliest one in the plain book is 1911. The next earliest is 1926. The plain book has some fountain pen entries, a rather ornate “Lee E Adams” makes me think this was my grandfather’s writing. Another early one in fountain pen is the name and address of his sister. Other entries are written more neatly in pen, and then later in pencil. I can see no evidence of any addresses erased, however. They look like the writing of a different person. On the left are dates like “1–4–34 here” for my great aunt in Washington.
When I put a new address in my phone address book, the old address disappears forever. No way to track the past of our peripatetic family. Only the ‘remember this? Where was that apartment? When did you move? And the people who have died, no little note “died Dec. 1, 1933” next to the address. I put a note in my electronic entry to prevent horrid gaffes.
Perhaps my grandmother took over my grandfather’s address book. There is an entry for “Mrs E.N. Day, 171 Central Avenue, Silver Creek. That was my grandmother’s home when I was little. My Dad and his brothers grew up in that house. Mrs E.N. Day, their great-aunt, came with the house when they moved in. She left in horror when my grandmother became pregnant with my Uncle. She said to my grandmother “Josephine, how could you!” My grandfather later said that if he had known that was the secret for getting rid of her he would have ‘done it’ a lot sooner, according to my Dad, who was ten years old at the time. Mrs. E.N. Day became a boarder on Parkside, a lovely street around the corner from the family home. There is a note that she died in Dec. 1933.
The old address books have business cards stuck in. Cards fell out: my Great Uncle Philip’s, with “ Dead Man’s Float Amber Dean” written on the back. I just googled it on my phone. It is a book in the “Crime Club Library. Published January 1, 1944.
Who were these people who moved around so much? Sometimes every year. I have never heard of most of them. The last entry is in 1944 in the plain book, 1947 in the spiral book. No wartime addresses in the plain book, no addresses for my Dad, my Uncle in the plain book. My Dad’s wartime address is precise in the spiral book: Lt. (j.g.) Lee Towne Adams, U.S.S. Razorback, % Fleet P.O., San Francisco , Cal. On the side the note Dec 1944. Dad made it back, not like 25% of the Lt’s on submarines. No note on the left “died 1944.” The Razorback made it back too. It had a Turkey address for a while before it moved to Little Rock Arkansas.
I will pull out a pen and paper for my thank you notes. I will get to think about the people who remembered me at Christmas. I will conjure them up for a minute, imagining them at their tables or walking down their streets. I will use the addresses in my phone, updated as if the past had never existed. I had better get to them before I forget what the presents were.
Journalist covering old news of the day
See all (618)
40 
40 claps
40 
Journalist covering old news of the day
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Fonality/nows-the-time-for-your-company-to-embrace-managed-it-services-2241f82b1e4?source=search_post---------344,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fonality
Jan 26, 2017·3 min read
Technology is rapidly improving, and as it does, modern professionals are quickly discovering new ways of doing business. To an extent, they’re still reliant on their companies’ phone systems, which enable them to call up customers and business partners all over the world, but they’re quickly embracing new innovations as well.
Cloud computing makes it easier than ever to manage and share information. Mobile devices add capability and flexibility to people’s daily lives. Text and voice chat are now possible across numerous platforms, which expedites things even further.
The only problem is that the more channels people have, the tougher it becomes for corporate IT managers to oversee all of them and prevent issues from cropping up. This is why now’s the time to consider the benefits of a managed services provider. If you’re having trouble coordinating all of your company’s communications, you can always just leave that job to the professionals.
Why managed IT is a must in 2017
There are plenty of companies out there that already turn to outside providers for managing their business phone systems, but it’s now time to go one step farther than that. According to CIO, numerous organizations are struggling with handling other aspects of IT, and they’re looking to improve.
Currently, only 60 percent of companies that consider their technology use “advanced” are using a managed service provider (MSP). The rest are dealing with IT solutions that are disjointed and siloed.
There are a few reasons they should consider switching over. One is simple efficiency — it’s far easier for employees to collaborate and work effectively when they can share information through a shared platform and avoid silos. Security and compliance are also factors. It’s tough to ensure that your data management is safe and law-abiding when you’re handling information in 10 different ways at the same time. Finally, cost savings and ROI are commonly cited as reasons to embrace MSPs.
Find the right provider for you
Once you’ve decided that an MSP is the right option for you, the next step is to find the provider that’s a perfect fit for your business.
This will depend on a few factors. One is the prior track record of the company in question. Ideally, you want to work with a partner that’s shown proficiency not just with business phone plans, but also with managing cloud data and other forms of communication and file storage.
You also want a provider that’s a good match for where you are and what you do. This means finding a company that’s proven to work effectively in your local area, for one thing. Having an understanding of the locality and the particular challenges it presents is important. You also want a company that is prepared to offer quick support if anything goes wrong. This means they need to have a presence nearby.
Our hybrid solution covers all the bases
Managing IT in the modern business world means tackling a whole host of challenges. You want a provider that’s ready for all of them. At Fonality, we believe we can be just that provider. We offer a hybrid solution that includes VoIP phone service as well as cloud solutions and much more. We aim to bring all your company’s communications under one centralised, convenient umbrella.
Everyone’s talking about the cloud these days, and rightfully so. But you also want your phone service and everything else to be in good hands, too. We’re up to that challenge at Fonality. We want to provide you with a total solution that help you manage your whole IT infrastructure and sleep easy.
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
23 
1
23 
23 
1
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
"
https://medium.com/@cuelogictech/event-driven-serverless-architecture-using-aws-lambda-6aef8d52ba80?source=search_post---------345,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cuelogic Technologies
Aug 22, 2018·5 min read
If we think about computing in the Cloud Computing age, our mind is quickly drawn towards containers and virtual machines. For instance, during the development of a production environment with both approaches, we think about the need for upgrading the container and patching the OS.
Amazon Web Services (AWS), at the end of the year 2014, announced a new service known as “Lambda” that enables us to focus on business logic rather than infrastructure. Lambda is a serverless compute service that executes your code that is available in the cloud in response to events and accomplishes the entire administration of the computer services automatically.
To better understand it, first, let’s understand what is serverless compute service and how does it work.
An architecture where the execution of code is entirely managed by the provider of cloud, instead of the conventional method of building applications and deploying them on servers. It means developers need not worry about maintaining, provisioning, and managing servers when deploying code.
Serverless architecture is an application that mainly depends on custom code running in ephemeral containers(Function as a Service) or on third-party services(Backend as a Service), the best-known vendor host of AWS Lambda. Serverless Framework is a single toolkit for deploying the serverless architectures to any provider. By moving most behavior to the front-end by using these ideas, such architectures eliminate the requirement for conventional ‘always on’ server system that is sitting behind an application.
Based on the circumstances, such systems can minimize complexity and operational cost significantly at the cost of the immaturity of supporting services, and vendor dependencies. Several largest tech companies are using serverless for powering their products. It is used in various industries including media, entertainment, CRM, and so on. The serverless architecture provides significant benefits regarding time-to-market, labor, and infrastructure cost.
Event-driven implies that a Lambda function is triggered during the occurrence of an event, so the application flow is mainly driven by events. Every AWS lambda function is an event consumer in this kind of architecture. This is because an event invokes them and they have the responsibility to process it.
1. Patterns for Building Event-Driven Serverless Architectures Using AWS Lambda
The patterns for building event-driven serverless AWS Lambda architecture are microservices, event-driven data processing, event workflows, application ecosystems, IoT and mobile applications, and web applications. The serverless code that is written with the help of FaaS can be utilized together with the code that is written in a conventional server style, known as microservices. The monolithic applications are segmented into smaller services in a microservices architecture so that you can build, scale, manage them independently. The benefits of AWS microservices architecture include quality(better scaling and optimizations), innovation(more maintainable code), and speed(faster deployment and development).
For serverless environments, one of the most common applications is to trigger actions after the occurrence of an event as I discussed above in the ‘Event-Driven’ section. In the serverless web applications, there can exist a combination of a running process that determines personal and contextual elements of the user to serve functionality and content that meets the needs of the user. In a serverless environment, the in-built IoT and mobile applications decide on what content must be provided to the user depending on their context.
In an application ecosystem, the workflows or applications are created in a serverless environment and draw on a combination of the products and functionalities of AWS. Recently, the release of AWS Step Functions is adding more sophistication to the serverless workflow possibilities. In step functions, the decision trees can be created, and they can be aligned with AWS products and Lambdas to perform branched workflow actions.
2. Challenges for Building Event-Driven Serverless Architectures Using AWS Lambda
Few challenges arising due to building event-driven serverless AWS Lambda architecture include issues arising with third-party API system, lack of operational tools, and architectural complexity. Security and vendor lock-in concerns, multitenancy problems, and vendor control are some of the issues arising with third-party API system.
The developers today are depending on vendors for monitoring and debugging tools. However, this may change as more and more organizations are moving towards AWS serverless architecture. Another challenge is the distributed computing architectures are time-consuming and complex to build. This applies to microservices architecture and serverless architecture in equal measure.
3. Benefits for Building Event-Driven Serverless Architectures Using AWS Lambda
The benefits of event-driven serverless architecture using AWS Lambda include easier operational management, faster innovation, and reduced operational costs. The serverless platform offers a clear separation between infrastructure applications and services running on top of the platform. The automatic scaling functionality of FaaS reduces the operational management overheads.
As AWS serverless architecture has minimized the system engineering problems in the underlying platform, the product engineers can innovate at a rapid pace. The human resource and infrastructure cost reduction is the advantage of serverless architecture as AWS Lambda bills you the time only when the function is called.
Also read: A Guide to Serverless Computing with AWS Lambda
The serverless movement has just begun, and you can expect the vendors of the cloud to heavily invest in enhancing the capabilities and feature set of serverless offerings. The monitoring and operational management of FaaS, DevOps teams, serverless frameworks will continue to mature. The agility and globalization demands, IoT proliferation, chat and voice bots, LessOps, serverless, containerization and a lot more trends are at play here. All of these will speed up the serverless adoption.
Many organizations are planning to migrate their production environments to serverless. For instance, Bustle(a media company) is using a serverless architecture to handle 52 million monthly visitors, and there are many more instances where serverless is being utilized. 2017 was very exciting for serverless, and this will continue to explode in 2018.
Originally Posted: Cuelogic Blog
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
11 
11 
11 
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
"
https://medium.com/@jaychapel/the-cloud-managed-services-market-is-growing-and-thats-good-for-msps-660bfe8cfd12?source=search_post---------346,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 15, 2019·3 min read
Lately, we have been talking to quite a few providers of cloud managed services that play in both the private and public cloud spaces. These conversations have centered around how cloud management needs are evolving as enterprises’ hybrid and multi-cloud needs have accelerated.
Most refer to this market as cloud managed services (for once, no acronym associated), and many of these managed service providers (MSPs) also sell migration services to bring customers from private to public cloud, and cloud services between Amazon Web Services (AWS), Microsoft Azure, and Google Compute Platform (GCP). So these MSPs can help you move your applications to the cloud, sell you the cloud services you’re using, and manage and optimize your cloud services. It’s a rapidly growing market with a lot of M&A activity as MSPs race to provide differentiated cloud managed services that enable them to help enterprises get to market faster, better, and cheaper.
The global cloud managed services market size is expected to reach USD 82.51 billion by 2025, according to a study conducted by Grand View Research, Inc. Enterprises are focusing on their primary business operations, which results in higher cloud managed services adoption. Business services, security services, network services, data center services, and mobility services are major categories in the cloud managed services market. Implementation of these services will help enterprises reduce IT and operations costs and will also enhance productivity of those enterprises.
Taking a step back, I had a look at Wikipedia to make sure we were all aligned on what managed services provider are and cloud management is (cloud managed services):
Cloud managed services enable organizations to augment competencies that they lack, or to replace functions or processes that incurred huge recurring costs. These services optimize recurring in-house IT costs, transform IT systems and automate business processes allowing enterprises to achieve their business objectives.
The “net net” is that MSPs providing managed cloud services enable enterprises to adopt and manage their cloud services more efficiently.
In March 2018 Gartner published a Magic Quadrant for Public Cloud Infrastructure Managed Service Providers if you're interested to see who they rank as the best of the best in when implementing and operating solutions on AWS, Azure, and GCP (note this includes multi-cloud but not hybrid cloud). Several large SI’s are on the list like Accenture, Capgemini, and Deloitte, along with newer born in the cloud pure play MSPs like 2ndWatch, Cloudreach, and REANcloud.
What’s interesting to us about this list is the recent M&A activity we have seen with many of these companies, here’s a few we were able to remember over a beer (shout out to Crooked Run Brewery in Sterling, VA):
As you can see, there is a clear bias towards buying “born in the cloud”, public cloud focused MSPs, as that’s where the lack of enterprise expertise lies, and of course the hyper growth is occurring as companies migrate from private to public cloud. Many of these providers started off supporting just AWS, and now need to or have begun supporting Azure and Google as well to support The “big 3” cloud service providers in this new, and emerging multi-cloud world.
MSPs that want to get into the cloud managed services game need to realize the pains are different in the public cloud, and that their focus needs to be on helping enterprises with security and governance, managing cloud spending, the lack of resources/expertise, and the ability to manage multi-cloud.
Originally published at www.parkmycloud.com on September 13, 2018.
CEO of ParkMyCloud
8 
8 
8 
CEO of ParkMyCloud
"
https://medium.com/angelhack/a-hackathon-guide-to-amazon-web-services-27a6c2dafd10?source=search_post---------347,"There are currently no responses for this story.
Be the first to respond.
For our 10th Global Hackathon Series, you’ll notice a few familiar faces, one of them being Amazon Web Services! They’re back, and sponsoring our Global Series and will be live and in-person at five of our hackathons. As always, we’re pumped to have them, their tech (and those free credits) on board!
Honestly, we’d be a little shocked if you’ve never heard of them. But just in case!
AWS is an on-demand delivery of IT resources via the Internet with pay-as-you-go pricing. They offer over 40 services that range across computing, storage, database, analytics, application, and deployment products that help organizations move faster, lower IT costs, and scale applications.
AWS basically provides you with the right resources to worry less on making things work and focus on what makes Hackathons so much fun!
Step 1: Head here ➡️ https://aws.amazon.com/start-now/
And check out their 10-minute tutorials!
Step 2: Check out their extensive documentation library! ➡️ https://aws.amazon.com/documentation/
Step 3: And their tools! ➡️ https://aws.amazon.com/tools/
Yes, yes we did. Grab your $200 in AWS Credits per person through AWS Activate.
Link: https://pages.awscloud.com/hack17angel.html
San Francisco — May 6–7
Hyderabad — July 25–26
Manhattan — June 10–11
Seattle — July 15–16
Silicon Valley — July 29–30
Insert promo code: AngelNews for discounted hackathon tickets!
We would love to hear about your hackathon experience, please reach out to aws-hackathons (at) amazon.com to share your story.
AngelHack ignites the passion of the world’s most vibrant…
20 
20 claps
20 
AngelHack ignites the passion of the world’s most vibrant community of code creators + change makers to invent the new and make change happen, together. Want to contribute? Email troy (at) angelhack (dot) com
Written by
AngelHack ignites the passion of the world's most vibrant community of code creators + change makers to invent the new and make change happen, together.
AngelHack ignites the passion of the world’s most vibrant community of code creators + change makers to invent the new and make change happen, together. Want to contribute? Email troy (at) angelhack (dot) com
"
https://koukia.ca/introduction-to-azure-webjobs-7c122ff05dcb?source=search_post---------348,"Azure WebJobs to me are same old Windows Services, but better! Hosted in the Azure cloud with tons of features around them, like debugging, scaling, scheduling, instrumentation and so on.
Azure Webjobs lets us run scripts or programs as background processes on App Service Web Apps. You can simply upload and run an executable file such as as cmd, bat, exe (.NET), ps1, sh, php, py, js and jar. These programs run…
"
https://medium.com/@jaychapel/the-multi-cloud-environment-in-2020-advantages-and-disadvantages-5a0ed2f2fec8?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 8, 2020·5 min read
Now more than ever, organizations have been implementing multi-cloud environments for their public cloud infrastructure.
We not only see this in our customers’ environments: a growing proportion use multiple cloud providers. Additionally, industry experts and analysts report the same. In early June, IDG released its 8th Cloud Computing Survey results where they broke down IT environments, multi-cloud and IT budgets by the numbers. This report also goes into both the upsides and downsides using multiple public clouds. Here’s what they found:
Other goals include:
Interestingly, within multi-cloud customers of ParkMyCloud, the majority are users of AWS and Google Cloud, or AWS and Azure; very few are users of Azure and Google Cloud. About 1% of customers have a presence in all three.
The study found that the likelihood of an organization using a multi-cloud environment depends on its size and industry. For instance, government, financial services and manufacturing organizations are less likely to stick to one cloud due to possible security concerns that come with using multiple clouds. IDG concluded that enterprises are more concerned with avoiding vendor lock-in while SMBs are more likely to make cost savings/optimization a priority (makes sense, the smaller the company, the more worried they are about finances).
Since multi-cloud has been a growing trend over the last few years, we thought it’d be interesting to take a look at why businesses are heading this direction with their infrastructure. More often than not, public cloud users and enterprises have adopted multi-cloud to meet their cloud computing needs. The following are a few advantages and typically the most common reasons users adopt multi-cloud.
While taking advantage of features and capabilities from different cloud providers can be a great way to get the most out of the benefits that cloud services can offer, if not used optimally, these strategies can also result in wasted time, money, and computing capacity. The reality is that these are sometimes only perceived advantages that never come to fruition.
As companies implement their multi-cloud environments, they are finding downsides. A staggering 94% of respondents — regardless of the number of clouds they use or size of their organization — find it hard to fully take advantage of their public cloud resources. The survey cited the biggest challenge is controlling cloud costs — users think they’ll be saving money but end up spending more. When organizations migrate to multi-cloud they think they will be cutting costs, but what they typically fail to account for is the growing cloud services and data as well as lack of visibility. For many organizations we talk to, multiple clouds are being used because different groups within the organization use different cloud providers, which makes for challenges in centralized control and management. Controlling these issues brings about another issue of increased costs due to the need of cloud management tools.
Some other challenges companies using multiple public clouds run into are:
Configuring and managing different CSPs requires deep expertise which makes it more of a pressing need to find employees that have the experience and capabilities to manage multiple clouds. This means that more staff are needed to manage multi-cloud environments confidentiality so it can be done in a way that is secure and highly available. The lack of skills and expertise for managing multiple clouds can become a major issue for organizations as their cloud environments won’t be managed efficiently. In order to try fix this issue, organizations are allocating a decent amount of their IT budget to cloud-specific roles with the hope that adding more specialization in this area can help improve efficiency.
The statistics on cloud computing show that companies not only use multiple clouds today, but they have plans to expand multi-cloud investments:
In a survey of nearly 551 IT people who are involved in the purchasing process for cloud computing, 55% of organizations currently use multiple public clouds.
Organizations using multiple cloud platforms say they will allocate more (35%) of their IT budget to cloud computing.
SMBs plan to include slightly more for cloud computing in their budgets (33%) compared to enterprises
As cloud costs remain a primary concern, especially for SMBs, it’s important organizations keep up with the latest cloud usage trends to manage spend and prevent waste. To keep costs in check for a multi-cloud, you can make things easier for your IT department and implement an optimization tool that can track usage and spend across different cloud providers.
For more insight on the rise of multi-cloud and hybrid cloud strategies, and to demonstrate the impact on cloud spend, check out the drain of wasted spend on IT budgets here.
Originally published at www.parkmycloud.com on July 2, 2020
CEO of ParkMyCloud
13 
13 
13 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/are-aws-reserved-instances-better-than-on-demand-f8bdb75ea679?source=search_post---------350,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 6, 2019·13 min read
We are frequently asked about the impact of instance scheduling on AWS Reserved Instances for EC2 and RDS. Scheduling On Demand instances as well as Reserved Instances (RIs) are both useful techniques for cost optimization, but they are polar opposites in terms of goals. RIs are all about getting a better price for RDS or EC2 instances that are running all the time. Scheduling is all about reducing costs by turning off instances when they are not in use.
How should a customer choose between buying an AWS Reserved Instance and applying scheduling to the instance? This is an important question, as usually the savings from scheduling can exceed the savings available from a Reserved Instance. Before we dive into that though, let’s get familiar with some critical RI nuances that can help you make the decision…and make the most of your RIs.
Note: versions of this article were originally published in 2015 and 2017. It has been completely re-analyzed, rewritten, and updated!
First of all, it’s worth clarifying that there is no functional difference between Amazon Reserved Instances and On Demand. It’s all in the billing. <rant> I wish I had a dollar for each time someone asked me (while looking at the AWS Console) “Which ones of these are the Reserved Instances?” </rant>. You cannot be sure which instances are using a reservation until you get your bill. You can make a good guess, based on what account you are looking at and what reservations you have purchased, but if you are using instance size flexibility with region-based RIs, it would be a pure guess. An instance reservation is not like a hotel reservation, where you end up with a specific room for the duration of your stay. That would be a Dedicated Host Reservation — a whole other beast with its own set of limitations.
Yep — there are 3600 seconds in an hour. I did the math. You may ask: why does that matter? It matters because of two things: per-second billing for EC2, and the fact that AWS RIs are billed in one hour chunks. More precisely, RIs are billed in one “clock-hour” chunks, where a clock-hour starts on the hour and up to the final second of the hour — like 04:00:00 to 04:59:59.
If you are running a number of instances that use per-second billing, and they go up or down during a (clock) hour, then multiple instances may be able to leverage the Reserved Instance pricing.
As paraphrased shamelessly from here, if you purchase one m5.large Reserved Instance and run four matching m5.large instances for 15 minutes each (900 seconds each) in the same hour, the total running time is one hour, which consumes the entire Reserved Instance allocation. This usage pattern is illustrated below.
That said, if multiple matching instances are running at the same time, the Reserved Instance billing benefit is applied to all of the instances, up to a maximum of 3600 seconds in a clock-hour. On-demand rates are used for any usage after that. This is illustrated below.
By the way, remember that per-second billing only applies to instances running an open-license OS like Amazon Linux or Ubuntu.
If your instance is running any flavor of Windows, Red Hat Linux, or any pay-by-the-hour image from the AWS Marketplace, that instance is billed by the hour, and any matching reservation will be consumed by the hour, even if the instance is not. Any overlapping instances will each need their own RI to get RI pricing.
Wait…what? Isn’t that kind-of an oxymoron? Elastic means it should be able to change with my usage…and aren’t reservations are a commitment to usage? Well, maybe…but some RIs are definitely more elastic (or at least more flexible) than others. Regional Reserved Instances can leverage “instance size flexibility.” This is the ability of a reservation to apply up or down to other instance types in the same family. Regional RIs are applied in priority order from the smallest instance in the region to the largest instance in the region. This makes these AWS reservations somewhat elastic in that if you buy reservations for smaller instance types than you normally use, then those reservations are more likely to be consumed, even if you rightsize an instance or replace one server with another of a different size.
The math of how an RI of one instance type can be applied to another instance type is governed by a “normalization factor”, described here. Putting the table in that link another way:
Some examples of how we can use this table:
The lesson here is that it is easiest to manage reserved instances if you buy the smallest instance size available within an instance family. In fact, as shown below this is the exact type of recommendation you are likely to receive from the AWS Cost Explorer.
So why do we mention AWS flexible reserved instances with relation to schedules? Recall that the allocation of RIs to instances is done by AWS each second within a clock-hour. Each reservation is consumed by instances in its family until all 3600 seconds have been allocated. You do not necessarily have to keep an instance up for the whole hour to use the reservation. You can still consume the whole RI, even if you are starting and stopping instances by schedule or within an auto-scaling group over the course of an hour.
Something so cool cannot be without limitations and gotchas. Here are a few we’ve noticed:
As usual, there are a number of different decisions that need to be made before this question can be answered. AWS RIs are available with two different levels of flexibility and several different terms of purchase.
In terms of flexibility, RIs can either be purchased as “standard” or “convertible.”
Both AWS Convertible RIs and Standard RIs offer:
There are a few options for terms of purchase that affect AWS RI pricing. RIs can be purchased for 1 or 3-year terms, and with no upfront, partial upfront, or all upfront payment available for each.
To get a better idea of the difference in the amount you’ll end up paying with each purchasing option, let’s take a look at some pricing scenarios for the general purpose m5.large instance in us-east-1 (US Virginia). When you look at these on the AWS RI price list, they are usually grouped by year terms and standard vs. convertible. Since you can already see it that way on AWS, and because time is the greatest unknown, I am going to give a bit of a different view, keeping the years together on the same comparison chart.
To benefit from the discount provided by a reservation while keeping contract terms to a minimum, look at the 1-year RIs:
You can see from this that within the Convertible vs. Standard tiers, there is not a lot of difference between the upfront cost payment options. Your upfront cash decision here can depend on your accounting process, perhaps accounting for the difference between CapEx and OpEx (Capital Expenditures and Operating Expenditures), or based on your current and prospective cash flow. The pricing between the Convertible vs. Standard options are so similar as to make you seriously consider if the benefit of flexibility in the AWS Reserved Instances convertible option outweighs the minor cost savings.
If are confident of your three-year usage plans, look at the 3-year RIs, shown here as the cost per year:
There is a bit more visible progression between the options here, and the cost savings difference between the 1-year graph and the 3-year graph are clearly seen. Still, the 1-year vs. 3-year decision may again come down to accounting practices.
Note that these graphs are just for one instance type in one location in about the middle of the overall performance pack. They do not express a true profile of all instance types/families in all locations, and you should do your own comparison before buying any RIs. That said….
FINALLY getting into the core question. Is it better to shut an instance down or buy a reserved instance? Let’s do the math and compare AWS On Demand vs. Reserved, with on/off scheduling applied to the On Demand option. Given all the different pricing options it can be a bit difficult to decide how to compare these. Let’s go with the no-upfront RI options and see where the RIs break even with scheduling.
Since the hours might be judge at this scale, here are the break-even hours in a table (hours rounded-up):
So what do these hours mean?
By way of comparison, here are a few common schedules and their hours in ParkMyCloud (with weeks converted to months on the basis of 52 weeks/12 months = 4.33 weeks/month):
Of the above, 7 AM — 7 PM Weekdays is the second most popular schedule in our entire system, and at 260 hours, easily is more cost-effective than the least expensive RI.
In that final schedule above, you need to run 16 hours per day, Monday-Friday before a 3-year RI starts to look like a better deal. If you have other matching instances on another schedule in that same region/AZ, you could then buy the RI anyway, and save even more money.
“But wait!” you say… “There are these things called Scheduled Reserved Instances – would they not offer me the best of both worlds?” Maybe, but note that to support Scheduled RIs, AWS has essentially set aside hardware for use as Scheduled RIs — this is a finite set of dedicated resources. On paper, a Scheduled RI can definitely save you a bit more money, but let’s look at the limitations.
When launched in 2016, Scheduled RIs were only available in “US East (N. Virginia), US West (Oregon), and Europe (Ireland) regions, with support for the C3, C4, M4, and R3 instance types.” Given that this has not changed, I am guessing the feature has not taken off as AWS had hoped. That said, if you have something that needs to run for 4 hours per day, daily, and can use an older instance family in a supported region…go for it! This may also be a good place to leverage AWS Spot instance types.
Deciding between different RI options starts with knowledge of how stable your usage is going to be in the coming years. Most organizations have a good understanding of at least a baseline level of usage, and some actually construct portfolios of reserved instance purchases, much like how you might balance a stock portfolio. For example, 3-year Standard Reserved Instances are for those applications that are stable and unlikely to change. Balance those with some 3-year Convertible Reserved Instances to help save money in the longer term but allow for some flexibility in use. Use 1-year RIs to account for shorter-term usage volatility, with again maybe a balance between Standard and Convertible where needed for those loads that might shift over the short term.
Just like with stocks, such portfolios require active management, awareness of what is being used and what is coming in both the short and long term. Companies with these types of RI management typically have dedicated cloud cost management staff, though RI management itself does not need to be a full-time job. One of our larger customers tells us they do a quarterly review of their RI portfolio, starting with using ParkMyCloud to schedule any instances they can, and then rebalancing their RIs and investing in more as needed. This is a best practice for any size company.
For production workloads that run 24×7 long-term, you REALLY should be buying RIs. For production workloads with an unknown future, or for non-production workloads (dev, test, QA, training, etc.), the answer is “probably not”. Be careful though, as often RIs are seen as a quick fix to save 30–50% on the cloud bill, but then other ways are found to save even more, and you end up with underutilized RIs. Before you buy a Standard RI, make sure you have evaluated your resources for underutilization and overprovisioning. Also consider the following cost-saving options:
The worst thing one can do is…nothing. Set aside some time each quarter to review EC2 and RDS instance utilization. Rightsize and schedule what you can — try a free trial of ParkMyCloud to see how we can help you with that. After that, anything left running 24×7 is a candidate for an RI. If you are risk-averse or time-crunched, stick with 1-year convertible RIs to save at least 36% on at least some of your resources. Then take all that money you saved and put it into other things that will bring greater value to your business.
Originally published at www.parkmycloud.com on August 15, 2019.
CEO of ParkMyCloud
13 
13 
13 
CEO of ParkMyCloud
"
https://medium.com/techmagic/is-your-product-well-architected-11b0ce583afc?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
What does it mean to be Well-Architected and why should you even consider it?
Well, let’s start from the very beginning.
The process of developing a product from scratch or migrating an existing one to the cloud can involve dozens of various services and plenty of resources, depending on your purposes. Once your company has architected the solution on AWS, your infrastructure can start getting more and more complex, causing you to overlook AWS innovations or cost-saving opportunities.
AWS Well-Architected Framework was established as a solution to this matter. This framework consists of a set of questions and principles across declared five pillars and helps clients clarify the advantages and disadvantages of different decisions made while developing their applications on AWS.
Let’s talk a bit more about those core pillars that define the direction of the key design areas of all architectures. Each of the five pillars describes a set of main design principles, methodologies, and decision points for developing different solutions on AWS.
One of the first questions companies ask when considering building their infrastructure on the cloud is — “Is it secure?” That’s why we put Security as a first “pillar” of AWS Well-Architected Framework.
Like all other cloud providers, AWS drives on a shared security model. That means that AWS is responsible for the security of the cloud, and at the same time, users are responsible for the security of their content and applications that utilize AWS services.
In accordance with Amazon’s AWS documentation,
“The Security pillar encompasses the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.”
If you have solid authorization and authentication management, automated responses to security events, protected infrastructure at all levels, and if you manage well-classified data with encryption, you will be provided with defense-in-depth of your infrastructure.
As one of the five pillars of the AWS Well-Architected Framework, Reliability is described as “the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.”
One of the biggest advantages of AWS is that it has been considered to be more reliable than data centers when it comes to failure and change control. The Reliability pillar accents on the three areas of concern: Foundations, Change Management, and Failure Management.
Cost Optimization is at the core of the AWS cloud practice. Ultimately, the ability to amplify business with lower infrastructure costs — is the stated commitment of the cloud. The Cost Optimization pillar of the AWS Well-Architected provides instructions on how to design, control, and react to business and technology conditions in order to optimize your AWS infrastructure so that you pay only for the required and used resources.
Cost Optimization is aimed to avoid needless expenses by recognizing and controlling where money is spent, providing the right resources, tracking cost and making sure scaling is executed cost-effectively.
The Cost Optimization pillar focuses on four areas of concern: Cost-Effective Resources; Matching Supply and Demand; Expenditure Awareness; and Optimizing Over Time.
As described in AWS documentation,
“The Performance Efficiency pillar focuses on the efficient use of computing resources to meet requirements and the maintenance of that efficiency as demand changes and technologies evolve.”
One of the key reasons businesses are building their products on the AWS cloud is providing a high-performance infrastructure that can easily adjust to constantly changing technology and market conditions.
You can take full advantage of the technical, financial, and business improvements the cloud offers with the Performance Efficiency pillar of the Well-Architected Framework. AWS has the entire set of tools to select, review and monitor your cloud infrastructure for ongoing development and optimization.
Operational Excellence pillar is focused on developing and maintaining AWS-based environments that are not only efficient but also scalable and effective across the board. It enables businesses to ensure that the cloud infrastructure can efficiently operate changes, react to events, and automate standard tasks and processes to successfully manage daily operations.
Operational Excellence in the cloud is composed of three areas: Prepare, Operate, Evolve.
Business operations play a significant role in how companies can transform their business due to cloud computing. The best expertise in developing robust, repeatable processes for managing your cloud infrastructure is covered in the Operational Excellence “pillar”.
By optimizing your architecture around these pillars, you’ll have an ability to:
As we know, the solidity of a whole building lies in its foundation. The same we can say about architecting technology solutions. The Well-Architected Framework is a truly beneficial resource and if you incorporate all the described pillars into your architecture, you will be able to develop a solid foundation of your environment enabling you to receive cost savings benefits while implementing AWS solutions.
TechMagic works closely with AWS to ensure that the framework can meet the individual requirements of our clients in accordance with their industry, as well as the company size and global reach of their businesses. In 2017 we became an AWS Certified Consulting Partner with narrow Serverless technological expertise.
If you are interested in receiving more information about TechMagic’s AWS capabilities and services, please contact us at hello@techmagic.co or through the contact form.
Check out our case study on AWS — Elements.cloud, a process knowledge web application.
Talking about AWS solutions, it’s also worth considering a new concept that allows you to focus on specific workload types from the well-architected perspective — Well-Architected Lens. We are going to carry out an in-depth overview of this topic in our further articles.
This article summarizes a more detailed AWS document “AWS Well-Architected Framework”.
All about JavaScript, AWS, and Serverless in one place.
10 
10 claps
10 
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/7-favorite-aws-training-resources-3167313159d?source=search_post---------352,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 12, 2020·5 min read
When it comes to AWS training resources, there’s no shortage of information out there. Considering the wide range of videos, tutorials, blogs, and more, it’s hard knowing where to look or how to begin. Finding the best resource depends on your learning style, your needs for AWS, and getting the most updated information available. Whether you’re just getting started in AWS or consider yourself an expert, there’s an abundance of resources for every learning level. With this in mind, we came up with our 7 favorite AWS training resources, sure to give you the tools you need to learn AWS:
What better way to learn that at your own pace? AWS self-paced labs give you hands-on learning in a live AWS environment, with AWS cloud services, and actual scenarios you would encounter in the cloud. There are two different ways to learn with these labs. You can either take an individual lab or follow a learning quest. Individual labs are intended to help users get familiar with an AWS service as quickly as 15 minutes. Learning quests guide you through a series of labs so you can master any AWS scenario at your own pace. Once completed, you will earn a badge that you can boast on your resume, LinkedIn, website, etc.
Whatever your experience level may be, there are plenty of different options offered. Among the recommended labs you’ll find an Introduction to Amazon Elastic Compute Cloud (EC2), and for more advanced users, a lab on Maintaining High Availability with Auto Scaling (for Linux).
Sometimes the best way to learn something is by jumping right in. With the AWS Free Tier, you can try AWS services for free. This is a great way to test out AWS for your business, or for the developers out there, to try services like AWS CodePipeLine, AWS Data Pipeline, and more. While you are still getting a hands-on opportunity to learn a number of AWS services, the only downside is that there are certain usage limits. You can track your usage with a billing alarm to avoid unwanted charges, or you can try ParkMyCloud and park your instances when they’re not in use so you get the most out of your free tier experience. In fact, ParkMyCloud started its journey by using AWS’s free tier!
AWS Documentation is like a virtual encyclopedia of tools, terms, training, and everything AWS. You’ll find case studies, tutorials, cloud computing basics, and so much more. This resource is a one-stop-shop for all of your AWS documentation needs, whether you’re a beginner or advanced user. No matter where you are in your AWS training journey, AWS documentation is always a useful reference and certainly deserves a spot in your bookmarks.
Additionally, you’ll find whitepapers that give users access to technical AWS content that is written by AWS and individuals from the AWS community, to help further your knowledge of their cloud. These whitepapers include things from technical guides, reference material, and architecture diagrams.
So far, we’ve gone straight to the source for 3 out of 7 of our favorite AWS training resources. Amazon really does a great job of providing hands-on training, tutorials, and documentation for users with a range of experience. However, YouTube opens up a whole new world of video training that includes contributions from not only Amazon, but other great resources as well. Besides the obvious Amazon Web Services channel, there are also popular and highly rated videos by Edureka, Simplilearn, Eli the Computer Guy, and more.
As cloud technology usage continues to expand and evolve, blogs are a great way to stay up to speed with AWS and the world of cloud computing. Of course, in addition to aws labs, a free-trial, extensive documentation, and their own YouTube channel, AWS also has their own blog. Since AWS actually has a number of blogs that vary by region and technology, we recommend that you start by following Jeff Barr — Chief Evangelist at Amazon Web Services, and primary contributor. Edureka was mentioned in our recommended YouTube channels, they also have a blog that covers plenty of AWS topics. The CloudThat blog is an excellent resource for AWS and all things cloud, and was co-founded by Bhaves Goswami — a former member of the AWS product development team. Additionally, AWS Insider is a great source for all things AWS. Here you’ll find blogs, webcasts, how-to, tips, tricks, news articles and even more hands-on guidance for working with AWS. If you prefer newsletters straight to your inbox, check out Last Week in AWS and Inside Cloud.
As public cloud computing continues to grow — and AWS continues to dominate the market — people have become increasingly interested in this CSP and what it has to offer. In the last 8–10 years, two massive learning platforms were developed, Coursera and Udemy. These platforms offer online AWS courses, specializations, training, and degrees. The abundance of courses that these platforms provide can help you learn all things AWS and give you a wide array of resources to help you train for different AWS certifications and degrees.
GitHub is a developer platform where users work together to review and host code, build software and manage projects. This platform has access to a number of materials that can help further your AWS training. In fact, here’s a great list of AWS training resources that can help you prepare for an Amazon Cloud certification. The great thing about this site is the collaboration among the users. The large number of people in this community brings together people from all different backgrounds so they are able to provide knowledge about their own specialties and experiences. With access to everything from ebooks, video courses, free lectures, and sample tests, posts like these can help you get on the right certification track.
There’s plenty of information out there when it comes to AWS training resources. We picked our 7 favorite resources for their reliability, quality, and range of information. Whether you’re new to AWS or consider yourself an expert, these resources are sure to help you find what you’re looking for.
Originally published at www.parkmycloud.com on February 6, 2020.
CEO of ParkMyCloud
17 
17 
17 
CEO of ParkMyCloud
"
https://medium.com/dadi/introducing-dadi-cloud-22c2de924761?source=search_post---------353,"There are currently no responses for this story.
Be the first to respond.
They say never launch anything on a Friday, but we’re minutes beyond a properly engaging AMA session and we can’t wait to show you what we’ve built. Here, then, is the refreshed https://dadi.cloud/.
The new site consolidates our legacy properties to provide a single central source for DADI technology.
This is the first version. Our iterative development process will see many updates to the site over the coming days, weeks, months and years. Ultimately it will be the home for DADI accounts: the place that you sign up for and use DADI web services; where you come to participate in the network as a Node; and where you can learn what the technology can do for you and your business.
The site is running on DADI technology end-to-end and is hosted on the DADI network. It’s currently on our testnet, and will be moved to the mainnet when it goes live next month.
You’ll notice we’ve arranged content into three sections: Network, Web services and Community. The first brings an overview of the DADI network, including a map of the testnet showing location of all hosts, plus how it works and detail of how to contribute when the time comes.
Web services gives an overview of each of the applications designed to be run on the network (and access to roll up your sleeves and get building with them right away), while community is where you’ll find support for DADI technology ranging from tutorials from our team to updates on new releases.
Have a good look around and let us know what you think. If there’s anything you’d like to see we can look at adding it to the roadmap for iteration.
Some housekeeping: user accounts from our old website are now redundant, so if you have any questions regarding token distribution (which you should have resolved by now!) drop us a line via support@dadi.co.
Written by Joseph Denne. Joseph is the Founder & CEO of DADI.
Faster. Greener. More Secure. The future of the cloud is Edge.
140 
140 claps
140 
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Written by
Peer-to-peer serverless infrastructure, powered by blockchain technology and built using the spare capacity all around us. The future of the cloud is Edge.
Upholding the founding principles of the Web through the democratisation of computational power.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/neo4j/week-11-importing-and-querying-kickstarter-projects-583b30f92e84?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
If you want to code along, sign up — or in — to https://dev.neo4j.com/aura, create your free database, and join us.
neo4j.com
Being avid Kickstarter backers ourselves (especially for board games and gadgets), that dataset was very compelling to us.
If you’d rather watch than read, here is the video of our live-stream:
We looked at the Kickstarter site overall — and at a particular project — to relate the data in the dataset with the actual site.
The dataset contains 375k Kickstarter projects from 2009 to 2017, including category, country, and pledge data.
Some questions we can ask after importing the datasets:
The data is available as a zip download with 2 CSVs — one with the data dictionary, the other with 375k rows of Kickstarter projects.
Columns:
To make it easier to import, I would usually upload the CSV file somewhere publicly to access it easily, like GitHub, Pastebin, or S3.
This time, we wanted to demonstrate how you can use Google Spreadsheets for that.
After uploading the 100k chunk of the file as a new sheet, you can choose “File → Publish to the Web” to publish a single sheet, e.g. as a CSV publicly.
Other tools and people can access just that data read-only, including our Neo4j instance.
Google Spreadsheets didn’t like importing the full 400k rows, so we split the file using xsv split -c 10000 split kickstarter.csv into smaller chunks (csvkit works too, or just head -10000 in any unix shell).
As the long URL is a bit unwieldy, we created a bit.ly shortlink for our needs: https://dev.neo4j.com/kickstarter
The data model has some interesting tidbits, especially around modeling the state.
We could model the state just as a property like in the CSV, but as there are only a few relevant states and it’s an important attribute of a project to distinguish them, we can also add labels to a project for:
That helps us to visualize the state of a project quickly and also sub-select the relevant projects easily.
We extract the subcategory and category and the country as separate nodes.
We could extract the pledge information into a separate node — e.g. for finance and security reasons — but we kept it in the project for simplicity.
After creating our blank AuraDB Free database, and opening Neo4j Browser we can get going.
We can look at the first few rows of the data:
Which gives us object/hash/dict/map representations of each row with the headers as key and the row data as values.
All values are strings, so if we want to convert them we need to do that manually.
We also check how many rows our dataset contains.
That returns 75k rows, which is a bit much for our AuraDB free instance (50k nodes, 175k rels).
So we need to cut it down, but because we want to have projects of all kinds of status, we can sub-select a single year to use, and that gives us roughly 22k rows to work with.
First, we create a bunch of constraints to ensure data uniqueness and speed up the import when looking up existing data:
We import the data incrementally, starting with the projects. We use MERGE here so that we can re-run the import without causing duplication.
We’ll add the pledge information later.
We see that it imported some 22k projects. By clicking on the (Project) pill in the sidebar we can quickly pull up a few projects in the visualization.
Now we want to turn the state information into labels. For just the three labels, we can run a single update statement each that adds a label of the right kind to the project node.
So if we now query projects from the sidebar we see their success even visually. In the stream, we discuss styling the colors by clicking on the pills on top of the visualization and then choosing a different color.
Importing the categories involves creating nodes for them and then connecting the subcategory to the project and the subcategory to the category. As we use MERGE here, it also ensures each relationship is created only once.
Please note that shared names of subcategories in this approach will be merged together into a single one.
If you don’t want this, you have to create the subcategories in the context of a category, as shown below.
We can now run a bunch of queries to see which categories exist, how many there are, and to get a visual.
Importing countries is pretty straightforward. Just create the node for a country if it doesn’t exist and connect it to the project.
Originally we had discussed extracting the backing information into separate nodes, but given the questions and the time we had we left that for a future exercise.
So we just find our projects again based on their id and set the few extra properties — remember to convert them to numeric values as needed.
If we want to explore our imported data model visually, we can use apoc.meta.graph procedure.
Which shows both the base data model but also our extra labels.
Now with the data in our graph we can start visualizing and querying it and answer our question.
Sad for the wolves :(
Professional chefs cooking at your home, e-bikes, art-guitars, 3d-printers… all expected but still pretty high pledges.
I think my highest was $750 for a laser cutter :)
We can show the category tree and expand a few subcategories to see their projects (and their state) visually in one picture.
Kinda expected. Let’s look at the successful ones.
Not that many in tech or art, but many comics and design projects, and music also has an almost 50% success rate.
We can calculate the totals and partials of failed and successful projects across the category tree and then sort by success rate.
Of course, this is skewed towards categories with fewer projects, as there is less competition and a few more successful projects have oversized impact.
Surprised to see Canada and Australia so high up there, compared to their population, but the U.S. is leading by an order of magnitude.
But if you actually want to be successful, being in New Zealand, Singapore, Ireland, or Sweden helps :)
With the data in the AuraDB Free database you can now do a lot of things.
Have fun and happy coding! You can find the data and details from the past few weeks at the github repository for the stream.
github.com
Developer Content around Graph Databases, Neo4j, Cypher…
6 
6 claps
6 
Written by
A software developer passionate about teaching and learning. Currently working with Neo4j, GraphQL, Kotlin, ML/AI, Micronaut, Spring, Kafka, and more.
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Written by
A software developer passionate about teaching and learning. Currently working with Neo4j, GraphQL, Kotlin, ML/AI, Micronaut, Spring, Kafka, and more.
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/the-rise-of-the-enterprise-cloud-manager-d9a29bc863c7?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
There is a growing job function among companies using public cloud: the Enterprise Cloud Manager. We did a study on ParkMyCloud users which showed that a growing proportion of them have “cloud” or the name of their cloud provider such as “AWS” in their job title. This indicates a growing degree of specialization for individuals who manage cloud infrastructure as demonstrated by their cloud computing job titles. And, in some companies, there is a dedicated role for cloud management — such as an Enterprise Cloud Manager.
The world of cloud management is constantly changing and becoming increasingly complex even for the best cloud manager. Recently, the increased adoption of hybrid and multi-cloud environments by organizations to take advantage of best-of-breed solutions, make it more confusing, expensive, and even harder to control. If someone is not fully versed in this field, they may not always know how to handle problems related to governance, security, and cost control. It is important to dedicate resources in your organization to cloud management and related cloud job roles. This chart from Gartner gives us a look at all the things that are involved in cloud management so we can better understand how many parts need to come together for it to run smoothly.
Having a role in your organization that is dedicated to cloud management allows others, who are not specialized in that field, to focus on their jobs, while also centralizing responsibility. With the help of an Enterprise Cloud Manager, responsibilities are delegated appropriately to ensure cloud environments are handled according to best practices in governance, security, and cost control.
After all, just because you adopt public cloud infrastructure does not mean you have addressed any governance or cost issues — which seems rather obvious when you consider that there are sub-industries created around addressing these problems, but you’d be surprised how often eager adopters assume the technology will do the work and forget that cloud management is not a technological but a human behavior problem.
And someone has to be there to bring the motivational bagels to the “you really need to turn your instances off” meeting.
Cohesively, businesses with a presence in the cloud, regardless of their size, should also consider adopting the functionalities of a Cloud Center of Excellence (CCoE) — which, if the resources are available, can be like an entire department of Enterprise Cloud Managers. Essentially, a CCoE brings together cross-functional teams to manage cloud strategy, governance, and best practices, and serve as cloud leaders for the entire organization.
The role of an Enterprise Cloud Manager or cloud center of excellence (or cloud operations center or cloud enablement team, whatever you want to call it) is to oversee cloud operations. They know all the ins and outs of cloud management so they are able to create processes for resource provisioning and services. Their focus is on optimizing their infrastructure which will help streamline all their cloud operations, improve productivity, and optimize cloud costs.
Moreover, the Enterprise Cloud Manager can systematize the foundation that creates a CCoE with some key guiding principles like the ones outlined by AWS Cloud Center of Excellence here.
With the Enterprise Cloud Manager leadership, DevOps, CloudOps, Infrastructure, and Finance teams within the CCoE can ensure that the organization’s diverse set of business units are using a common set of best practices to spearhead their cloud efforts while keeping balanced working relationships, operational efficiency, and innovative thinking needed to achieve organizational goals.
It’s worth noting that while descriptive, the “Enterprise Cloud Manager” title isn’t necessarily something widely adopted. We’ve run across folks with titles in Cloud Manager, Cloud Operations Manager, Cloud Project Manager, Cloud Infrastructure Manager, Cloud Delivery Manager, etc.
If you’re on the job hunt, we have a few other ideas for cloud and AWS jobs for you to check out.
With so much going on in this space, it isn’t possible to expect just one person or a team to manage all of this by hand — you need automation tools. The great thing is that these tools deliver tangible results that make automation a key component for successful enterprise cloud operations and work for companies of any size. Primary users can be people dedicated to this full time, such as an Enterprise Cloud Manager, as well as people managing cloud infrastructure on top of other responsibilities.
Why are these tools important? They provide two main things: visibility and action to act on those recommendations. (That is, unless you’re willing to let go of the steering wheel and let the platform make the decisions — but most folks aren’t, yet.) Customers that were once managing resources manually are now saving time and money by implementing an automation tool. Take a look at the automation tools that are set up through your cloud vendor, as well as third-party tools that are available for cost optimization and beyond. Setting up these tools for automation will lessen the need for routine check-ins and maintenance while ensuring your infrastructure is optimized.
To put it simply, if you have more than a handful of cloud instances: yes. If you’re small, it may be part of someone’s job description. If you’re large, it may be a center of excellence.
But if you want your organization to be well informed and up to date, then it is important that you have the organizational roles in place to oversee your cloud operations — an Enterprise Cloud Manager, CCoE and automation tools.
Originally published at www.parkmycloud.com on February 27, 2020.
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬.
To join our community Slack team chat 🗣️ read our weekly Faun topics 🗞️, and connect with the community 📣 click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
13 
13 claps
13 
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/syncedreview/eth-z%C3%BCrich-microsoft-study-demystifying-serverless-ml-training-b537a49c6ebc?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
Serverless computing is a new type of cloud-based computation infrastructure initially developed for web microservices and IoT applications. As it frees model developers from concerns regarding capacity planning, configuration, management, maintenance, operating and scaling of containers, VMs and physical servers, serverless computing has gained popularity with machine learning…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lmanchu/%E7%94%B1email%E7%9A%84%E6%95%85%E4%BA%8B%E7%9C%8B%E5%8D%80%E5%A1%8A%E9%8F%88%E7%9A%84%E7%99%BC%E5%B1%95-e2e6024f6279?source=search_post---------357,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lman’s murmur
Dec 18, 2018·5 min read
1998 年這部很經典的愛情喜劇片 You’ve got a mail，當時還很年輕一直很喜歡Meg Ryan ，加上從90年代初期就因為很熱愛這些跟電腦資訊有關的事物，對於這部片能夠把本來當時離一般人還很有距離的email用這麼簡單的故事串出來而覺得欣喜不已。
要先回想，當時在90年代中末期，Internet已經有比較多人知道是什麼，但主要就屬於因為WWW的崛起而知道Internet，另外一種就是因為都走傳統高中大學路線透過BBS而開始理解Internet，基本上要能夠開始使用Internet依然是個有門檻的行為，為了要開始使用Intenet你得先具備不少的電腦使用知識(不是常識)，接著還得透過各種麻煩的設備和連線方式才有辦法一窺Internet。
因此當年教人家上網也蔚為風潮，那種感覺就像是早年以為學電腦就是學倉頡輸入法一般，意思是使用電腦都是有門檻。對比起現在我家兩歲多小孩看著iPad就可以輕易的開始使用網路進而有能力找到自己喜歡的挖土機影片來看真的不可同日而語。
大概二十多年前，當時若你已經會使用Linux來架設各類伺服器，在Internet崛起初期，就已經可以靠這身本領來做生意賺錢，提供各種企業服務例如email server。我們來看看這老牌的網路基礎建設email已經縱橫了超過30年以上，從以前需要專業的工作站加上專業的底層軟體才能夠提供這樣的服務，在Linux, FreeBSD這一類OpenSource專案出現後，讓第一波有能力的工程師可以透過提供這些服務來賺錢，一下子這一類的服務提供商也如雨後春筍般在世界各地冒出，讓一般人都可以輕易的利用email來傳遞訊息。
直到2004年，Google發行了第一個邀請制的Gmail一時之間許多人以擁有Gmail為風尚，直到2007年全部開放無需特別邀請，直到現在Gmail說是全世界最流行的email恐怕無人敢說不是了。
我們回頭來思考一下，現在我們當然知道Gmail很成功，但來問自己，到底Gmail跟我們自建的email server差異到底在哪？
不理會後面幾點本來就是個奇怪理由和過時規矩的問題，作為email的基本功能能夠準確地把email傳遞到對方手中就是這服務的功能。那為何我們需要一個Gmail?
答案是
你所面對的問題其世界觀的規模不同
若你的公司永遠只是小小的，你的員工和利害關係人都在當地，那麼你確實只需要一個自己的email server就可以解決問題。
但如果你認為你的市場是國際並且客戶分佈廣，那麼你就會理解為何像Gmail這樣服務的重要。
有時候不是因為你不行，而是你沒跳出井看看世界到底多大！
而且也唯有利用類似Google所投資的基礎建設才有辦法建構出一個跨全球等級卻又能有良好體驗的Email。
仔細想想為了寄出你這封Email所付出的成本有多大。
透過Email和Gmail的歷史案例可以知道解決同一個問題為何需要不同的科技或者新的科技的理由，那麼在區塊鏈呢？
大多數人開始使用區塊鏈除了一般使用者從錢包和交易所加密貨幣開始，對於開發者和創業者扣除極少數極少數真的有能力做底層鏈以外，大多還是選擇從dAPP的開發開始，
在DappRadar中隨時可看到更新的各類dAPP使用者，你會發現實際使用數字其實是不高的，除了本身的國際行銷和獲取用戶能力以外，另外的關鍵問題根本在於目前的基礎建設是無法支撐這樣跨國級別的使用。
那就回到類似我們剛說的Email的問題，基礎建設若不到位，你做了再好的dAPP那麼也只能在某區域內局限性的使用，這部分還可以區分成你的dAPP是在公鏈還是私鏈上。
dAPP沒有不對，同樣也得面對跟過去科技一樣的挑戰，
這並非只是鼓吹dAPP然後單純的使用者體驗好不好就解決了，而是有更多基礎工程面的問題得去克服，其中還包含了許多軟硬整合的功夫在其中，你就開始想著為了讓你有所謂好的Gmail體驗，Google在各地花了多少心力蓋機房，並且在硬體伺服器端做了多少了創新才能讓你在上面開發只需要考慮使用者體驗來獲利。
並且爾後若你看到有宣稱做底層區塊鏈業者，你就看其是否有真的花心思在底層的開發和人力配置上也就可以知道是真是假！
當然也有個簡單的方式，若白皮書看起來都很厲害直接問該團隊他的老爸老媽叔叔公公是誰！這樣可能更決定了他的項目到底會不會成。
40 something, still having faith in Tech & trying to make some good impact to the world. Believe IoT & DLT can change the internet for good.
53 
53 
53 
40 something, still having faith in Tech & trying to make some good impact to the world. Believe IoT & DLT can change the internet for good.
"
https://medium.com/@jaychapel/microsofts-start-stop-vm-solution-vs-parkmycloud-5db83fad99a9?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Aug 28, 2019·6 min read
Users looking to save money on public cloud may be in the market for a start/stop VM solution. While it sounds simple, there is huge savings potential available simply by stopping VMs, typically on a schedule. The basic idea is that non-production instances don’t need to run 24×7, so by turning VMs off when they’re not needed, you can save money.
If you use Microsoft Azure, perhaps you’ve seen the Start/Stop VM solution in the Azure Marketplace. You may want this tool if you want to configure Azure to start/stop VMs for the weekend or on weekday nights. It may also serve as a way to avoid creating a stop VM powershell.
Users of Azure have taken advantage of this option to start/stop VMs during off-hours, but have found that it is lacking some key functionality that they require for their business. Let’s take a look at what this Start/Stop tool offers and what it lacks, then compare it to ParkMyCloud’s comprehensive offering.
Let’s take a look at Azure’s start/stop VM solution. The crux of this solution is the use of a few Azure services, specifically Automation and Log Analytics to schedule the VMs and Azure Monitor emails to let you know when a system was shut down or started. Both scheduling and keeping track of said schedules are important.
As far as the backbone of Azure services, the use of native tools within Azure can be useful if you’re already baked into the Azure ecosystem, but can be prohibitive to exploring other cloud options. You may only use Azure at the moment, but having the flexibility to use other public clouds in the future is a strong reason to use cloud-agnostic tools today.
Next, this solution costs money, but it’s not very easy to estimate the cost (but does that surprise you?). The total cost is based on the underlying services (Automation, Log Analytics, and Azure Monitor), which means it could be very cheap or very expensive depending on what else you use and how often you’re scheduling resources.
The schedules themselves can be based on time, but only for a single start and stop time — which is not practical for typical applications. The page claims it can be based on utilization, but in the initial setup there is no place to configure that. It also needs to be set up for 4 hours before it can show you any log or monitoring information.
The interface for setting up schedules and automation is not very user-friendly. It requires creating automation scripts that are either for stopping or starting only, and only have one time attached. This is tedious, and the single-time configuration makes it difficult to maximize off time and therefore savings.
To create new schedules, you have to create new scripts, which makes the interface confusing for those who aren’t used to the Azure portal. At the end of the setup, you’ll have at least a dozen new objects in your Azure subscription, which only grows if you have any significant number of VMs.
Users have noted numerous complaints in the solution’s reviews:
Luckily, there’s an easier option.
So if the Start/Stop VM Solution from Microsoft can start and stop Azure VMs, what more do you need? Well, we at ParkMyCloud have heard from customers (ranging from day-1 startups to Fortune 100 companies) that there are features necessary for a cloud cost optimization tool if it is going to get widespread adoption.
That’s why we created ParkMyCloud: to provide simple, straightforward cost optimization that provides rapid ROI while being easy to use. You can use ParkMyCloud to save money through Azure start/stop VM schedules for non-production resources that are not needed evenings and weekends, as well as RightSizing overprovisioned resources.
Here are some of the features ParkMyCloud has that are missing from the Microsoft tool:
As you can tell, the Start/Stop VM solution from Microsoft can be useful for very specific cases, but most customers will find it lacking the features they really need to make cloud cost savings a priority. ParkMyCloud offers these features at a low cost, so try out the free trial now to see how quickly you can cut your Azure cloud bill.
Related Reading:
Originally published at www.parkmycloud.com on August 5, 2019.
CEO of ParkMyCloud
24 
24 
24 
CEO of ParkMyCloud
"
https://koukia.ca/proactive-auto-heal-on-azure-app-services-8d136d7e894e?source=search_post---------359,"Azure App Services has had this Auto Heal feature for while when you could set it to make a decision when certain things happen like: When request count surpasses a certain limit or when requests are running slower usual or other situations. But to do that you needed to setup some rules and then when those rules were met, you could restart your application or things like that.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/multi-cloud-hybrid-cloud-and-cloud-spend-statistics-on-cloud-computing-ba4c194d2e10?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2019·4 min read
The latest statistics on cloud computing all point to multi-cloud and hybrid cloud as the reality for most companies. This is confirmed by what we see in our customers’ environments, as well as by what industry experts and analysts report. At last week’s CloudHealth Connect18 in Boston we heard from Dave Bartoletti, VP and Principal Analyst at Forrester Research, who broke down multi-cloud and hybrid cloud by the numbers:
More often than not, public cloud users and enterprises have adopted a multi-cloud or hybrid cloud strategy to meet their cloud computing needs. Taking advantage of features and capabilities from different cloud providers can be a great way to get the most out of the benefits that cloud services can offer, but if not used optimally, these strategies can also result in wasted time, money, and computing capacity.
The data is telling — but we won’t stop there. For more insight on the rise of multi-cloud and hybrid cloud strategies, and to demonstrate the impact on cloud spend (and waste) — we have compiled a few more statistics on cloud computing.
The statistics on cloud computing show that companies not only use multiple clouds today, but they have plans to expand multi- and hybrid cloud use in the future:
As enterprises’ cloud footprints expand, so too does their spending:
“Cloud is an inexpensive and easily accessible technology. People consume more, thereby spending more, and forget to control or limit their consumption. With ease of access, inevitably some resources get orphaned with no ownership; these continue to incur costs. Some resources are overprovisioned to provide extra capacity as a ‘just in case’ solution. Unexpected line items, such as bandwidth, are consumed. The IT department has limited visibility or control of these items.”
We’ve noticed some interesting patterns in the cloud platforms adopted by ParkMyCloud users as well, which highlight the multi-cloud trends discussed above as well as correlations between the types of companies that are attracted to each of the major public clouds. We observed:
Upon examining these statistics on cloud computing, it’s clear that multi-cloud and hybrid cloud approaches are not just the future, they’re the current state of affairs. While this offers plenty of advantages to organizations looking to benefit from different cloud capabilities, using more than one CSP complicates governance, cost optimization, and cloud management further as native CSP tools are not multi-cloud. As cloud costs remain a primary concern, it’s crucial for organizations to stay ahead with insight into cloud usage trends to manage spend (and prevent waste). To keep costs in check for a multi-cloud or hybrid cloud environment, optimization tools that can track usage and spend across different cloud providers are a CIO’s best friend.
Originally published at www.parkmycloud.com on September 18, 2018
CEO of ParkMyCloud
12 
12 
12 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/microsoft-azure-vm-types-comparison-e7d031b4b588?source=search_post---------361,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 10, 2019·5 min read
Microsoft Azure VM types come in a wide range optimized to meet various needs. Machine types are specialized, and vary by virtual CPU (vCPU), disk capability, and memory size, offering a number of options to match any workload.
With so many options available, finding the right machine type for your workload becomes confusing — which is why we’ve created this overview of Azure VM types (as we did before with EC2 instance types, and Google Cloud machine types). Note that while AWS EC2 instance types have names associated with their purpose, Azure instance type names are simply in a series from A to N.The chart below and written descriptions are a brief and easy reference, but remember that finding the right machine type for your workload will always depend on your needs.
General purpose VMs are suitable for balanced CPU and memory, making them a great option for testing and development, smaller to medium databases, and web servers with lower traffic:
The latest family of virtual machines stand out for data protection and code confidentiality. SGX technology and a 3.7GHz Intel XEON E-2176G Processor back these machines, and in conjunction with Intel Turbo Boost Technology, they can go up to 4.7 GHz.
A-series VMs have a CPU-to-memory ratio that works best for entry-level workloads, like those for development and testing. Sizing is throttled for consistent processor performance to run the instance.
Dv2 VMs boast powerful CPUs — roughly 35% faster than D-series VMs — and optimized memory, great for production workloads. With the same memory and disk configurations as the D-series, based upon either a 2.4 GHz or 2.3 GHz processor and Intel Boost Technology, they can go to up to 3.1 GHz.
With expanded memory and adjustments for disk and network limits, the Dv3 series Azure VM type offers the most value to general purpose workloads. Best for enterprise applications, relational databases, in-memory caching, and analytics.
Similar to the AWS t-series machine type family, B-series VMs are burstable and ideal for workloads that do not rely on full and continuous CPU performance. Customers can purchase a VM size that builds up credits when underutilized, and the accumulated credits can be used as bursts — spikes in compute power that allow for higher CPU performance when needed. Use cases for B-series VM types include development and testing, low-traffic web servers, small databases, micro services, and more.
With premium storage and a 2.4 or 2.3 GHz Intel Xeon processor that can achieve 3.5 GHz thanks to Intel Turbo Boost Technology 2.0, the Dsv3-series is best suited for most production workloads.
Compute optimized Azure VM types offer a high CPU-to-memory ratio. They’re suitable for medium traffic web servers, network appliances, batch processing, and application servers.
With a base core frequency of 2.7 GHz and a maximum single-core turbo frequency of 3.7 GHz, Fsv2 series VM types offer up to twice the performance boost for vector processing workloads. Not only do they offer great speed for any workload, the Fsv2 also offers the best value for its price based on the ratio of Azure Compute Unit (ACU) per vCPU.
F-series Azure VM types are great for workloads that require speed thanks to the 2.4 GHz Intel Xeon processor, reaching speeds up to 3.1 GHz with the Intel Turbo Boost Technology 2.0. The F-series is your best bet for fast CPUs but not so much when it comes to memory or temporary storage per vCPU. Analytics, gaming servers, web servers, and batch processing would work well with the F-series.
Memory optimized VM types are higher in memory as opposed to CPU, and best suited for relational database services, analytics, and larger caches.
Enterprise applications and large databases will benefit most from the M-series for having the most memory (up to 3.8 TiB) and the highest vCPU count (up to 128) of any VM in the cloud.
For applications that require fast vCPUs, reliable temporary storage, and demand more memory, the Dv2, G, and DSv2/GS series all fit the bill for enterprise applications. The Dv2 series offers a speed and power with a CPU about 34% faster than that of the D-series. Based on the 2.3 and 2.4 GHz Intel Xeon® processors and with Intel Turbo Boost Technology 2.0, they can reach up to 3.1 GHz. The Dv2-series also has the same memory and disk configurations as the D-series.
The Ev3 follows in the footsteps of the high memory VM sizes originating from the D/Dv2 families. This Azure VM types provides excellent value for general purpose workloads, boasting expanded memory (from 7 GiB/vCPU to 8 GiB/vCPU) with adjustments to disk and network limits per core basis in alignment with the move to hyperthreading.
For big data, SQL, and NoSQL databases, storage optimized VMs are the best type for their high disk throughput and IO.
VMs provide as much as 32 vCPUs with the Intel® Xeon® processor E5 v3 family. The Ls-series comes with the same CPU performance as the G/GS-Series and 8 GiB of memory per vCPU. This type works best applications requiring low latency, high throughput, and large local disk storage.
GPU VM types, specialized with single or multiple NVIDIA GPUs, work best for video editing and heavy graphics rendering — as in compute-intensive, graphics-intensive, and visualization workloads.
For the fastest and most powerful virtual machines, high performance compute is the best choice with optional high-throughput network interfaces (RDMA).
For the latest in high performance computing, the H-series Azure VM was built for handling batch workloads, analytics, molecular modeling, and fluid dynamics. These 8 and 16 vCPU VMs are built on the Intel Haswell E5–2667 V3 processor technology featuring DDR4 memory and SSD-based temporary storage.
And besides sizable CPU power, the H-series provides options for low latency RDMA networking with FDR InfiniBand and different memory configurations for supporting memory intensive compute requirements.
With six virtual machine types belonging to multiple families and coming in a range of sizes, how do you determine the right Azure VM type for your workload? The good news is that with this many options, you’re bound to find the right type to meet your computing needs — as long as you know what those needs are. With good insight into your workload, usage trends, and business needs, you’ll be able to find the Azure VM type that’s right for you.
Originally published at www.parkmycloud.com on October 16, 2018.
CEO of ParkMyCloud
See all (317)
16 
16 claps
16 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/fetch-ai/fetch-ai-grows-its-network-with-ankr-cloud-partnership-60d785a522b7?source=search_post---------362,"There are currently no responses for this story.
Be the first to respond.
Fetch.ai released its mainnet yesterday on 19 December 2019. Around 10% of its master nodes, those that take part in consensus, are up and running already. Fetch.ai will add more live nodes to the network over the coming weeks, with the aim to have all 200 master nodes running in the first few months of 2020.
Ankr will provide a node hosting application where users can easily run their Fetch.ai master node in just a couple of easy steps on the Ankr App Store. Ankr will soon publish our guide and tutorial on how to achieve this. Users will also be able to operate relay nodes. These do not take part in consensus, but are full nodes that assist with validation and can act as end-points and servers for connecting autonomous economic agents.
Chandler Song, CEO and Co-founder of Ankr said: “Fetch.ai are a unique proposition, aiming to address real-world problems that the real-world struggles to solve. This is what blockchain needs to accomplish eventual acceptance and adoption. Ankr are proud to be part of their journey and look forward to working alongside their team to support both their infrastructure and platform.”
“It’s a great pleasure to be working with Ankr and their team of experts to support the growth of our network,’’ commented Toby Simpson, co-founder of Fetch.ai. “This is a reliable, flexible and cost-effective way of operating Fetch.ai nodes and their focus on a great user experience ensures that it’s an entirely stress free process.”
Fetch.ai is a decentralized connectivity platform that enables devices to connect directly with digital agents delivering autonomous solutions to complex tasks.
We are accelerating research and deployment of emerging technologies, including blockchain and artificial intelligence. This enables us to build real world applications to provide solutions to everyday problems.
Fetch.ai is working to create ecostructures — ecosystem infrastructures — in which billions of devices can communicate seamlessly, autonomously and securely.
The platform simplifies the deployment of algorithms, IoT and complex systems. By using a collective learning AI/ML architecture, we enable devices and software agents to find each other, learn from each other and exchange value with each other, enhancing and optimizing solutions to problems through intelligent connectivity, machine learning and AI.
Fetch.ai master nodes (or Validator nodes) are computers that take part in consensus, securing the network and earning a reward for the formation of blocks over a time period. They operate and maintain the Fetch.ai ledger. The operators of the master nodes lock their FET tokens as a stake to gain entry to the Proof-of-Stake (PoS) consensus. This requires them to be online and submit valid blocks during the staking period, and rewards are earned for those following the protocol correctly.
All essential details, requirements, pricing and a step-by-step tutorial will be provided in the coming weeks, so that you can be among the first users to run the Fetch.ai master nodes, or your own relay-node end-point, once they are live.
The Ankr platform is connected to top-end specification servers, hosted globally from data centers owned by some of the largest companies on the planet, to provide the highest quality service to the Ankr platform.
At the moment, our data providers include ​DigitalOcean​, telecom giant T​elefónica​, BCP and more which are spread out over 5 continents, and we have many more world-class providers to come.
These data centers guarantee a tremendous global scope to our platform. While services such as ​Amazon​’s A​WS​ or ​Microsoft​ Azure​ offer only 20 global locations, Ankr can offer many times more, by virtue of each data center provider being a location in their own right, each with different global configurations. This results in the latency of your Ankr-hosted blockchain being minimized and the compute power maximized, meaning transactions will be smooth, efficient and lightning-fast.
Moreover, Ankr are significantly cheaper as public cloud providers, and our approach to cloud computing is notably eco-friendly since we use idle resources that would otherwise still be consuming power. By optimizing the use of existing hardware rather than adding to the global footprint, we present the most ecologically sound solution possible to the Cloud Computing market.
Ankr Website:​ https://www.ankr.com Twitter:​ https://twitter.com/ankr Telegram announcements:​ https://t.me/anrknetworkann Telegram chat:​ https://t.me/ankrnetwork
Fetch.ai
Website: https://fetch.ai/Twitter: https://twitter.com/Fetch_aiTelegram announcements:​ https://t.me/Fetch_AI_AnnouncementsTelegram chat: https://t.me/fetch_aiYouTube: https://www.youtube.com/c/FetchAIGitHub: https://github.com/fetchai
Artificial Intelligence for Blockchains
93 
93 claps
93 
Fetch.ai is building an open access, tokenized, decentralized machine learning network to enable smart infrastructure built around a decentralized digital economy
Written by
AI and digital economics company, based in Cambridge, UK
Fetch.ai is building an open access, tokenized, decentralized machine learning network to enable smart infrastructure built around a decentralized digital economy
"
https://towardsdatascience.com/the-state-of-ai-25899c7b5341?source=search_post---------363,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tim Frank Andersen
Nov 7, 2018·7 min read
AI is predicted to be the third era of computing and it is creeping into more and more of our daily routines. Now it´s also becoming cloud-based which means that even the most sophisticated techniques become available to everybody. So it´s about time to get started.
By Tim Frank Andersen, Chief Digital Officer at Charlie Tango
A few weeks ago, I attended Google Next in London — a conference with 16,000 participants, covering most of the new technology areas that are about to impact our world.
Here I had a very inspiring conversation with the owner of Dixit Algorizmi Gallery from Berlin about the future of art, while two AI-controlled robotic arms each drew their own different portrait of me. And that made me think, how far we had come in the development of machine-based intelligence.
Artificial Intelligence (AI) is a term that covers a range of different computational techniques ranging from machine learning, deep learning, neural networks to reinforcement learning, all simulating intelligent behaviour in computers.
The field of AI research dates back to the 50´s. Even when I studied to become an engineer some 30 years ago, I was taught how to make natural language processing algorithms and to code neural networks.
But a lot has happened since then. It seems like we are at the tipping point for AI - if we haven´t passed it already.
When it starts to work, we stop calling it AIA lot of daily routines are already affected by the use of some form of AI.
Every time you turn on your new iPhone, FaceID using AI scans your face and recognizes you in order to unlock your phone.
Every plane leaving the ground each day will be partly flown by an autopilot system.
Gone are the days where we navigated our way with a map, or even a TomTom navigation system. Now it´s all on our smartphones, intelligently rerouting our way to get us to the destination in the shortest time.
And try to think back to when your mailbox was cluttered with spam? The junk mails are still being sent, I promise you. But smart algorithms from either Google or Microsoft manage to delete 99% of these, so it´s no longer a big problem for us.
Fancy a new series on Netflix? Well, thanks to pattern recognition and collaborative filtering techniques, Netflix´s recommendation algorithm not only knows exactly which series you are most likely to prefer, they also know which unique images presenting the show will make you stay the longest.
And if you happen to use Google Photos, you know that you can now search your 20,000 or more pictures for items like sea turtles or green cars or even names of friends — in your own native language. And the app will automatically show you the desired result, without you having to tag each and every photo — it´s all being handled by AI-based image recognition.
So slowly, but steadily, AI is being engraved into our daily routines, making our life smarter, simpler and more effective without us noticing it. Because when it works, we just call it navigation, searching or recommendation, without thinking about the technology underneath.
Big data neededAll the examples above are heavily dependent on access to huge data sets. Because that´s how you train your algorithm. And this tends to favour companies with the most customers.
So, no wonder that Google — with 7 application each banking more than 1 billion active monthly users (Search, Gmail, Chrome, Android, Maps, YouTube, Google Play Store) and very soon more to come (Google Drive and Google Assistant) are regarded as the leading AI company in the world.
Huge data sets are the raw material needed to get Machine Learning to work. And Google has a lot of it.
Take their Autonomous Vehicle Project Waymo as an example. Since 2009, they have self-driven more than 16 million km on physical roads. But that is dwarfed by the 11 billion km driven in simulation. All this data combined has led them to a point where they are now allowed to launch commercial driverless car services in Arizona and California. This will happen by the end of 2018.
How can small companies compete with that? Luckily, Google has an answer to this. Google already owns Kaggle — the world’s largest community of data scientists and machine learners and a database of more than 11.000 public available datasets. Now they have launched an Open data marketplace called: Google Cloud Public Datasets Program. The plan is to make big data sets available for everybody, together with the necessary sets of tools in order to start working with AI in a cloud-based decentralized way.
What´s new?This is one of the reasons why we are about to take a giant leap forward. Commercial AI based on huge data sets are way too complicated and expensive for most companies to run on-premise.
Therefore, it is crucial that these services and opportunities become available as cloud-based — pay as you go — solutions. That is exactly what Google is launching with their Cloud AI Platform. The vision behind is to democratize the use of AI and to make it accessible and easy to get started for even small companies or individuals. You will find a list of pre-trained models ready to use and all the tools and APIs ready to get started.
The other groundbreaking reason why we are about to witness a giant leap forward is the use of specifically designed hardware for AI. Google has now launched the third generation of computer chips designed and optimized for AI assignments. They are called TPUs (Tensor Process Units) and they will speed up most AI jobs significantly. They are now also made available through the same cloud solution for everybody to use.
This is not done for altruistic reasons. Of course, there is a business model behind it. It´s service based — you get the first fix for free and, after that, you pay per usage. But it is still a more democratizing model than having to build your own data centers and AI installations on site.
Centralized - and edge based, at the same timeGoogle is not the only company developing AI-specific hardware. Apple’s newest generation of smartphones, which uses AI for things like face detection and portrait photography, boast the A12 chip where the built-in Neural Engine can handle 5 trillion operations per second!
The world´s leading semiconductor company, ARM, whose chipsets you will find in most smart devices, sensors and IoT products across the world, has launched project Trillium to enable a new era of ultra-efficient machine learning, where most of the computation will be done on the spot, in the device, so you don´t have to send massive amounts of real-time data to the cloud but only the digested interpretation.
This means that we see a two-way development of more centralized cloud-based AI service platforms and ecosystems based on huge data centers and very advanced tools. And at the same time a movement towards the edge, which allows for local and real-time data handling. Both developments are necessary if we want to unleash the full potential of AI in a commercial setup.
What´s the purpose?All this can be quite interesting if you are into technology, but that is exactly what it is — technology. So, what are the purposes behind it? What kind of business value are companies looking to obtain by using AI?
At the Google Next conference in London, CXOs from large companies like Airbus, ING, Metro AG, HSBC and E.ON flocked to the stage to explain how AI helped them with objectives like predictive maintenance, fraud prevention, crime investigation, sales forecasting and better customer service.
In the recently published report from Microsoft: “Artificial Intelligence in Europe”, the wide range of business purposes was divided into five categories:
with prediction (92%) and automation (88%) being the two most common in Denmark.
The report also showed that Danish companies (25 interviewed) are among the most advanced in Europe, with 96% stating that they are in the “piloting” AI stage or beyond.
So, if you are not already experimenting with how to gain business benefits out of using different AI techniques, it might be time to get started.
The other side of the coinThere are lots of issues to worry about.
This is an area that is still prematurely regulated. There are risks concerning data collection and data handling, not to mention the risk of information overload and false signals. Decisions made by algorithms are much less transparent than human-made decisions, which also pose a huge challenge when a company needs to explain their actions taken based on algorithmic guidance. Then there is the huge impact that the increased rollout of AI and automation will have on the job market.
And do we really want large international companies to host our data and run our intelligence?
Finally, I haven´t even touched on the fact that AI development in China and the rest of Asia is happening at lightning speed, almost under the radar. China, a country with 800 million internet users, heavy surveillance, no data regulation and a single point of decision, stated in their “Made in China 2025” plan that they aim at being the world leader in AI by 2030. And they are well underway: in 2017, 48% of total equity funding of AI start-ups globally came from China.
In other words, there is a global battle taking place as we speak. It will impact all of us and change the balance of power between countries and in industries within the next 5–10 years.
So, I strongly recommend focusing on this area and to start collecting relevant GDPR compliant customer data and to start experimenting with different AI techniques, no matter what business you are in.
Serial Entrepreneur and gadget freak. Co-founder and Chairman at Liveshopper.net. For more info: timfrankandersen.com
See all (166)
6 
Some rights reserved

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
6 claps
6 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chanakaudaya/microsoft-azure-cloud-networking-in-a-nutshell-b5ba95fe5e63?source=search_post---------364,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chanaka Fernando
Mar 13, 2020·5 min read
Microsoft Azure is one of the up and coming cloud service providers that can be used to host your applications in the cloud. It has various cloud services which can be categorized as
Regardless of what type of service you are going to utilize in the Azure cloud, you need to understand the networking model of the Azure to better utilize these services. This article discusses the various networking services available in Azure in a nutshell.
The users of the Azure cloud can utilize various network services available in azure to deliver the best experience to the consumers of their respective business applications and services. These network services can be divided into 4 main categories.
Depending on the place within the data path in which these services are applied, I have come up with the below figure which puts all these services into a single diagram as a nutshell representation of Azure cloud networking services.
As depicted in the above figure, azure networking services cover the full spectrum of connectivity between services spanning from service delivery, application security, intra/inter networking to the monitoring of the services. These services can be further detailed into the following services.
User experience is critical when using cloud services since these resources are hosted all over the internet. These services make sure that applications are delivered to consumers with better performance without any jitter.
These services provide the necessary infrastructure for securing the applications and services hosted in the cloud.
These are the services that are used to build the connectivity between components within the cloud as well as outside the cloud.
In the cloud, failure is inevitable and you should take necessary steps to monitor the failures and recover immediately. These services allow cloud users to monitor their applications and services.
https://docs.microsoft.com/en-us/azure/networking/networking-overview
📝 Save this story in Journal.
👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.
Engineer | Author | Speaker | Associate Director @ WSO2
See all (204)
8 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
8 claps
8 
Engineer | Author | Speaker | Associate Director @ WSO2
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-to-save-money-with-microsoft-azure-enterprise-agreements-71c60eb03b5c?source=search_post---------365,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 4, 2019·4 min read
As more large enterprises adopt Azure cloud, especially those that have traditionally used Microsoft tools, we have observed growing interested in Microsoft Azure Enterprise Agreements, commonly known as EAs. We thought it would be useful to understand more about Microsoft EA’s, how they work with Azure, and what they mean to both the enterprise and the ISV.
While you can create an Enterprise Agreement with Microsoft specifically for Azure, most companies using this option already have an EA in place for use of their software assets like Windows, Office, Sharepoint, System Center, etc. If you have an EA for other products, then you can simply add Azure to that existing agreement by making an upfront monetary commitment. You can then use eligible Azure cloud services throughout the year to meet the commitment. And you can pay for additional usage beyond the commitment, at the same rates. So, like any Enterprise License Agreement (ELA), including AWS’s EDP, you are committing to a contract term and volume to gain additional discounts.
According to Microsoft, the Enterprise Agreement is designed for organizations that want to license software and cloud services for a minimum three-year period. The Enterprise Agreement offers built-in savings ranging from 15 percent to 45 percent based on committed spend — and given how these commitments typically work, it is likely that the more you buy, the better your discount. The minimum listed commitment for an EA is 500 more users or devices for commercial companies (250 for public sector), and they specifically state this minimum does not apply to Server and Cloud Enrollment, an offering aimed at companies with EAs in place to help them standardize on Microsoft server and cloud technologies.
As it turns out, the Azure Enterprise commitment minimum is very low. You are required to make an upfront monetary commitment for each of the three years of the agreement, with a minimum order value of one “Monetary Commitment SKU” of $100 per month ($1,200/year). This low commitment make sense: once an enterprise is on a cloud platform, it’s sticky — land and expand is the name of the game for Azure, AWS, and Google. They expect infrastructure to grow significantly beyond the minimum, and just need to get a foot in the door. And of course, the starting point on the cloud is supposed to be much cheaper and flexible than on prem infrastructure.
There are certain Azure-specific EA benefits besides just price to entice users to move off of Pay-As-You-Go. You can create and manage multiple Azure subscriptions with a single EA. You can also roll up and manage all your subscriptions, giving you an enterprise view of how many resource minutes you’re using per subscription. In addition, you can assign subscription burn to accounting departments and cost centers so you can more easily manage budgets and see spend at various roll up levels.
EAs give you access to certain features that you’d otherwise be required to purchase separately. For example, an Azure EA gives you the option to purchase Azure Active Directory Premium, which will give you access to multi-factor authentication, 99.99% guaranteed uptime, and other features. Pay-As-You-Go only gives you access to the free version of Azure AD.
Besides getting the best pricing and discounts, what are some of the other added benefit an EA might provide to an enterprise:
Now, for vendors like ParkMyCloud, that need Azure pricing data to perform our service, how are we affected by the EA? Not adversely: the good news is that Microsoft makes EA pricing available through dedicated APIs and/or the Azure Price Sheet. We can match this information to a customer by using their Offer ID which defines their EA subscription and corresponding pricing (discounts).
Whether an Azure Enterprise Agreement makes sense for your organization is up to you to decide. Luckily, it’s not the only way to keep Azure costs in check. Here are a few others to explore:
Originally published at www.parkmycloud.com on September 26, 2019.
CEO of ParkMyCloud
13 
13 
13 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/aws-trusted-advisor-implies-the-existence-of-aws-doubted-advisor-f135719ca88a?source=search_post---------366,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 30, 2019·4 min read
AWS Trusted Advisor is a service that helps you understand if you are using your AWS services well. It does this by looking at 72 different best practices across 5 total categories, which include Cost Optimization, Performance, Security, Fault Tolerance, and Service Limits. All AWS users have access to 7 of those best practices, while Business Support and Enterprise Support customers have access to all items in all categories. Let’s dive in to each category to see what is there and what is missing.
A category that is near and dear to our hearts here at ParkMyCloud, the Cost Optimization category includes items related to the following services:
This list includes many of the services that are often the most expensive line items in an AWS account, but doesn’t take into account a large percentage of the AWS services available. Also, these recommendations only provide links to other AWS documentation that might help you solve the problem, as opposed to a service like ParkMyCloud that provides both the recommendations and ability to take the action of shutting down idle instances or resizing those instances for you.
This category caters more towards production instances, as it aims to make sure the performance of your applications is not hindered due to overutilization (as opposed to the Cost Savings category above, which is focused more on underutilization). This includes:
This category is one of the weakest in terms of services supported, so you may want to factor that in if you’re trying to make sure your production applications are performing well on alternative AWS services.
The security checks of AWS Trusted Advisor will look at the following items:
Security is a tough category to get right, as almost every one of these needs to be reviewed for your business needs. While this isn’t an exhaustive list of security considerations, it certainly helps your organization cover the basics and prevent some “I can’t believe we did that” moments.
One of the main benefits of the cloud that often gets overlooked is the use of distributed resources to increase fault tolerance for your services. These items in the fault tolerance category are focused on increasing the redundancy and availability of your applications. They include:
Overall, this turns out to be a great list of AWS services that can really make sure your production applications have minimal downtime and minimal latency. Additionally, some services like snapshots and versioning, help with recovering from problems in a timely fashion.
One of the hidden limitations that AWS puts on each account is a limit of how many resources you can spin up at any given time. This makes sense for AWS, so they don’t have new users unintentionally (or intentionally!) perform a DOS for other users. These service limits can be increased if you ask nicely, but this is one of the few places where you can actually see if you’re coming close. The services covered are:
While these checks and advice from AWS Trusted Advisor certainly help AWS users see ways to improve their usage of AWS, the lack of one-click-action makes these recommendations just that — recommendations. Someone still has to go verify the recommendations and take the actions, which means that in practice, a lot of this gets left as-is. That said, while I wouldn’t suggest upgrading your support just for Trusted Advisor, it certainly can provide value if you’re already on Business Support or Enterprise Support.
Originally published at www.parkmycloud.com on December 17, 2019.
CEO of ParkMyCloud
14 
14 
14 
CEO of ParkMyCloud
"
https://medium.com/geekculture/a-microsoft-azure-failure-has-exposed-data-from-thousands-of-customers-around-the-world-a72685ba68c2?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
A severe vulnerability in Microsoft Azure has exposed data from thousands of clients of the cloud computing service, including some of the…
"
https://medium.com/@jaychapel/aws-iam-user-vs-iam-role-for-secure-saas-cloud-management-34e8933e6287?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 5, 2019·9 min read
While going through our recent Cloud Cost Optimization Competency review with AWS, one of the things they asked us to do was remove the ability for customers to sign up for our service using AWS IAM User credentials. They loved the fact that we already supported AWS IAM Role credentials, but their concern was that AWS IAM User credentials could conceivably be stolen and used from outside AWS by anyone. (I say inconceivable, but hey, it is AWS.) This was a bit of a bitter pill to swallow, as some customers find IAM Users easier to understand and manage than IAM Roles. The #1 challenge of any SaaS cloud management platform like ours is customer onboarding, where every step in the process is one more hurdle to overcome.
While we could debate how difficult it would be to steal a customer cloud credential from our system, the key (pun intended) thing here is why is an IAM Role preferred over an IAM User?
Before answering that question, I think it is important to understand that an IAM Role is not a “role” in perhaps the traditional sense of Active Directory or LDAP. An AWS IAM Role is not something that is assigned to a “User” as a set of permissions — it is a set of capabilities that can be assumed by some other entity. Like putting on a hat, you only need it at certain times, and it is not like it is part of who you are. As AWS defines the difference in their FAQ:
An IAM user has permanent long-term credentials and is used to directly interact with AWS services. An IAM role does not have any credentials and cannot make direct requests to AWS services. IAM roles are meant to be assumed by authorized entities, such as IAM users, applications, or an AWS service such as EC2.
(The first line of that explanation alone has its own issues, but we will come back to that…)
The short answer for SaaS is that a customer IAM Role credential can only be used by servers running from within the SaaS provider’s AWS Account…and IAM User credentials can be used by anyone from anywhere. By constraining the potential origin of AWS API calls, a HUGE amount of risk is removed, and the ability to isolate and mitigate any issues is improved.
Software as a Service (SaaS) means different things to different vendors. Some vendors claim to be “SaaS” for their pre-built virtual machine images that you can run in your cloud. Maybe an intrusion detection system or a piece of a cloud management system. In my (truly humble) opinion this is not a SaaS — this is just another flavor of “on prem” (on-premise), where you are running someone’s software in your environment. Call it “in-cloud” if you do not want to call it “on-prem”, but it is not really SaaS, and it does not have the challenges you will experience with a “true” SaaS product — coming in from the outside. A core component of SaaS is that it is centrally hosted — outside your cloud. For an internal service, you might relax permissions and access mechanisms somewhat, as you have total control over data ingress/egress. A service running IN your network…where you have total control over data ingress/egress…is not the same as external access — the epitome of SaaS. Anyway: </soapbox>. (Or maybe </rant>depending on the tone you picked up along the way…)
The kind of SaaS I am focussing on for this blog is SaaS for cloud management, which can include cloud diagramming tools, configuration management tools, storage management+backup tools, or cost optimization tools like ParkMyCloud.
AWS has enabled SaaS for secure cloud management more than any other cloud provider. A bold statement, but let’s break that down a bit. We at ParkMyCloud help our customers optimize their expenses at all of the major cloud providers and so obviously all the providers allow for access from “outside”. Whether it is an Azure subscription, a GCP project, or an Alibaba account, these CSP’s are chiefly focussed on customer internal cross-domain access. I.e., the ability of the “parent” account to see and manage the “child” accounts. Management within an organization. But AWS truly acknowledges and embraces SaaS.
You could attribute my bold statement to an aficionado/fanboi notion of AWS having a bigger ecosystem vision, or more specifically that they simply have a better notion of how the Real World works, and how that has evolved in The Cloud. The fact is that companies buy IT products from other companies…and in the cloud that enables this thing called Software as a Service, or SaaS. All of the cloud providers have enabled SaaS for cloud access, but AWS has enabled SaaS for more secure cloud access.
So…where was I? Oh…right…Secure SaaS access.
OK, so AWS enables cross-account access. You can see this in the IAM Create Role screen in the AWS Console:
If your organization owns multiple AWS accounts (inside or outside of an AWS “organization”), cross-account access allows you to use a parent account to manage multiple child accounts. For SaaS, cross-account access allows a 3rd-party SaaS provider to see/manage/do stuff with/for your accounts.
Looking a little deeper into this screen, we see that cross-account access requires you to specify the target account for the access:
The cross-account role allows you to explicitly state which other AWS account can use this role. More specifically: which other AWS account can assume this role.
But there is an additional option here talking about requiring an “external ID”…what is that about?
Within multiple accounts in a single organization, this may allow you to differentiate between multiple roles between accounts….maybe granting certain permissions to your DevOps folks…other permissions to Accounting…and still other permissions to IT/network management.
If you are a security person, AWS has some very interesting discussions about the “confused deputy” problem mentioned on this screen. It discusses how a hostile 3rd party might guess the ARN used to leverage this IAM Role, and states that “AWS does not treat the external ID as a secret” — which is all totally true from the AWS side. But summing it up: cross-account IAM Roles’ external IDs do not protect you from insider attacks. For an outsider, the External ID is as secret as the SaaS provider makes it.
Looking at it from the external SaaS side, we get a bit of a different perspective. For SaaS, the External ID allows for multiple entry points…and/or a pre-shared secret. At ParkMyCloud (and probably most other SaaS providers) we only need one entry point, so we lean toward the pre-shared secret side of things. When we, and other security-conscious SaaS providers, ask for access, we request an account credential, explicitly giving our AWS account ID and an External ID that is unique for the customer. For example, in our UI, you will see our account ID and a customer-unique External ID:
If we look back at the definition of the AWS IAM Role, we see that IAM roles are meant to be assumed by authorized entities. For an entity to assume a role, that party has to be an AWS entity that has the AWS sts:AssumeRole permission for the account in which it lives. Breaking that down a bit, the sts component of this permission tells us this comes from the AWS Secure Token Services, which can handle whole chains of delegation of permissions. For ParkMyCloud, we grant our servers in AWS an IAM Role that has the sts:AssumeRole permission for our account. In turn, this allows our servers to use the customer account ID and external ID to request permission to “Assume” our limited-access role to manage a customer’s virtual machines.
From the security perspective, this means if a hostile party wanted to leverage SaaS to get access to a SaaS customer cloud account via an IAM Role, they would need to:
So….kind-of a short recipe of what is needed to hack a SaaS customer. (Yikes!) But this is where your access privileges come in. The access privileges granted via your IAM role determine the size of the “window” through which the SaaS provider (or the bad guys) can access your cloud account. A reputable SaaS provider (ahem) will keep this window as small as possible, commensurate with the Least Privilege needed to accomplish their mission.
Also — SaaS services are updated often enough that the service might have to be penetrated multiple times to maintain access to a customer environment.
Going back to the beginning, our quote from AWS stated “An IAM user has permanent long-term credentials and is used to directly interact with AWS services”. There are a couple frightening things here.
“Permanent long-term credentials” means that unless you have done something pretty cool with your AWS environment, that IAM User credential does not expire. An IAM User credential consists of a Key ID and Secret Access Key (an AWS-generated pre-shared secret) that are good until you delete them.
“…directly interact with AWS services” means that they do not have to be used from within your AWS account. Or from any other AWS account. Or from your continent, planet, galaxy, dimension, etc. That Key ID and Secret can be used by anyone and anywhere.
From the security perspective, this means if a hostile party wanted to leverage SaaS to get access to a SaaS customer cloud account via an IAM Role, they would need to:
So this list may seem only a little bit shorter, but the barriers to compromise are higher, and the opportunity for long-term compromise is MUCH longer. Any new protections or updates for the SaaS servers has no impact on an existing compromise. The horse has bolted, so shutting the barn door will not help at all.
The other cloud providers provide some variation of an access identifier and a pre-shared secret. Unlike AWS, both Azure and Google Cloud credentials can be created with expiration dates, somewhat limiting the window of exposure. Google does a great job of describing their process for Service Accounts here. In the Azure console, service accounts are found under Azure AD>App registrations>All apps>App details>Settings>Keys, and passwords can be set to expire in 1 year, 2 years, or never. I strongly recommend you set reminders someplace for these expiration dates, as it can be tricky to debug an expired service account password for SaaS.
For all providers you can also limit your exposure by setting a very limited access role for your SaaS accounts, as we describe in our other blog here.
Azure does give SaaS providers the ability to create secure “multi-tenant” apps that can be shared across multiple customers. However, the API’s for SaaS cloud management typically flow in the other direction, reaching into the customer environment, rather than the other way around.
Fortunately, when AWS “strongly recommended” that we should discontinue support for AWS IAM User-based permissions, we already supported an upgrade path, allowing our customer to migrate from IAM User to IAM Role without losing any account configuration (phew!). We have found some scenarios where IAM Role cannot be used — like between the AWS partitions of AWS global, AWS China, and the AWS US GovCloud. For GovCloud, we support ParkMyCloud SaaS by running another “instance” of ParkMyCloud from within GovCloud, where cross-account IAM Role is supported.
With the additional security protections provided for cross-account access, AWS IAM Role access is the clear winner for SaaS access, both within AWS and across all the various cloud providers.
Originally published at www.parkmycloud.com on April 18, 2019.
CEO of ParkMyCloud
13 
1
13 
13 
1
CEO of ParkMyCloud
"
https://medium.com/@unfoldlabs/covid19-diaries-cloud-cloudification-everywhere-783ddc8bc803?source=search_post---------369,"Sign in
There are currently no responses for this story.
Be the first to respond.
UnfoldLabs
May 1, 2020·10 min read
Cloud Computing has certainly been the buzzword of the 21st century, and nearly 90% of enterprises were already on the way to adopting the cloud. But no one saw the pandemic coming or a huge surge in cloudification becoming the most important aspect of life.
The onset of Covid19 has sent billions of people worldwide to be on the Shelter at Home mode, which has helped cloud adoption surge more than 200%. Grounded by an extended lock down, people are seeking newer solutions to routine life and existence tasks. Cloud computing services-based companies have suddenly become the backbone for almost all the operations around the world.
People are going online and using cloud solutions on a scale that was never imagined before. As of now, the usage of public cloud services sit on $354.6 Billion and more than $1.3 Trillion IT spending will be affected by the shift to the cloud by 2022.
The cloud companies are weathering the pandemic stress-test caused by the sudden spike in workload. There are three cloud providers who have won significantly during these trying times — AWS, Azure and Google Cloud.
Other cloud service providers like Alibaba, Oracle, IBM and Salesforce, have also seen a wider adoption of their cloud platforms. One major aspect to remember is that though there has been a surge — we have not heard a lot about the outages from any of the major cloud provider, which shows a couple of aspects:
Below are some interesting verticals that are using the cloud platforms to meet the extraordinary challenges of today.
Healthcare
Tele-Health was talked about as a technology solution for ages — but was not adopted due to patient privacy and security concerns. However, with social distancing, health care systems and physicians worldwide are racing to adopt virtualised treatment approaches that remove the need for physical meetings between patients and health providers.
Though, the cloudification surge in healthcare segment is expected to be pulled back post COVID19, most of the changes will stay for better.
Ecommerce
The pandemic has rapidly accelerated the transition to digital commerce. Online order volumes of full-assortment grocery merchants rose to 210.1%. Most retailers are relying on the cloud to operate especially as they face a surge in transactions.
Though it is certain that COVID19 has dramatically accelerated the digital disruption of grocery, we expect online grocery shopping will reduce 50% after lock down ends. However, the world will start to see retailers reduce capital spending for physical stores, and more investments for technology.
Online Learning
Over the past few months, the number of students affected by closures in 138 countries has nearly quadrupled to 1.37 Billion. In addition, more than 60.2 Million teachers are no longer in the classroom but video conferencing.
Today, the learners throughout the world can learn anywhere, anytime truly realizing the concept of digitization. Online Learning has become a daily habit now.
Entertainment
With 42 countries under lock down, there is a spike in internet consumption, mostly through multimedia streaming apps. The shelter-in-place orders across the globe, combined with the loss of live sports is accelerating the adoption of various streaming services like Netflix, Disney, Amazon Prime etc.
The weekly consumption of video on mobile devices, has grown from 36.4 Billion minutes to 58 Billion minutes. The average weekly minutes spent watching Netflix saw a 115% increase after lock downs were implemented.
The pandemic has spiked up the usage of entertainment and will stay like this way for ever.
Remote Work
There is a significant leap in video conferencing usage over the past few weeks, especially with Zoom. With so many people working remotely, many companies need video conferencing to conduct meetings. Just not work, people around the world are using apps like Zoom to connect for non-work purposes — online classes, family catch ups, and workout classes.
The cloud technology is helping all the services remain closely connected and progress across a geographic distance which otherwise would be impossible.
Though the pandemic has had a devastating impact on our economy and social life, it has ushered in a new era of urgent innovations around the world:
1. Apple and Google have designed contact tracing technology that uses Bluetooth signals to track if users have been in contact with anyone exposed to corona virus.
2. Hands free door openers — Sanitized door handles which almost act ‘life and death’ for the hospital staff is a life breather.
3. Drones with the help of Machine Language is checking on the fevers, coughing and sneezing behaviors of a person in a zone — Watch Video.
4. Machine Learning, Artificial intelligence and imaging solutions are being used to validate social distancing.
5. Emergency ventilators specially designed for the contagious environment — cleans the room of viral particles and supplies purified air to the patient.
6. DIY kit for automated bag valve mask ventilation which helped even US department of defense.
From printable protective face shields to disposable doorknob sleeves and an elbow-operated extension for lift buttons, the world has been on an invention spiral, with the cloud being the backbone of all solutions.
Not All is Okay as It Seems
The abrupt movement of millions of people online might have created a huge upsurge in cloud adoption but at the same time the pandemic has created a perfect storm for cyber attacks. With millions of people working in unfamiliar and less secure circumstances, massive cyber-attacks have begun.
Over the past month, there has been 41.5% spike in the number of devices exposed to remote desktop protocol via non-standard TCP ports which are quite harmful which is creating vulnerability to the serious BlueKeep flaw — attackers can exploit to bypass authentication and gain direct access to systems.
The general public is not aware that working from home — can be a similar threat profile as at an airport or a Starbucks because you just do not have the right protection as you have in the workplace. Hence, most of the attackers are fooling people with phony emails with malicious links or playing on sympathies with fake crowdfunding pages for people who have fallen ill.
Though the big players like Microsoft and Google are working tirelessly every day, designing intelligent solutions to negate the possible cyber-attacks, hackers are trying to shift their techniques to capitalize on fear by impersonating established entities like the WHO, CDC, etc. to get into the system.
After this prolonged period of crisis, the strong desire for renewal, there will be a tremendous burst of creative energy and economy-building.
With shelter in place — think of a world without the cloud. We would have had a lot of inefficiencies and lack of options to our day-to-day living. The cloud has for sure eliminated boredom and enhanced our lives of humanity while we are social distancing at home.
The fact is that the pandemic has clearly accelerated cloud migration and adoption. The steep rise in the usage of the cloud will have a positive impact on the world for humanity, business and industry. With cloud and cloudification being the order of the day during COVID19, it is hard to imagine a world without cloud technology and solutions.
Cloudification will not stop. We will continue to see a lot more innovation on the cloud and how we will interact with technology in the post-COVID19 era. I am confident that as a society and species, we will innovate our way out of the crisis, and the cloud will be the fundamental backbone and enabler for years to come.
The most important aspect of COVID19 is that it has taught us to be humble human beings. I have also been admiring the ways and means humanity is pulling together with humility and showing our resilience. I am inspired by the shared determination of the people to overcome the crisis, the optimism with which we have taken this with courage and positivism. As the Earth continues to breathe, Nature continues to bloom, Cloud & Cloudification is here to stay.
This post was written by Asokan Ashok, the CEO of UnfoldLabs. Ashok is an expert in driving customer insights into thriving businesses and commercializing products for scale. As a leading strategist in the technology industry, he is great at recommending strategies to address technology & market trends. Highly analytical and an industry visionary, Ashok is a sought after global high-tech industry thought leader and trusted strategic advisor by companies.
Entrepreneur. Inventor. Strategist. Visionary. Evangelist. Architect.
For any comments or discussions, please feel free to reach out to Ashok or UnfoldLabs at “marketing-at-unfoldlabs-dot-com”.
Innovative Technology Product/Services company. Makers of cool next-gen products. Guide to Mobile, BigData, Cloud, IoT, VR, Wearables, Telematics, 5G.
See all (80)
13 
13 claps
13 
Innovative Technology Product/Services company. Makers of cool next-gen products. Guide to Mobile, BigData, Cloud, IoT, VR, Wearables, Telematics, 5G.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/lotus-fruit/future-or-farce-the-enigma-of-cloud-gaming-d2fd75e673df?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Omar Zahran
Feb 23, 2020·8 min read
Full disclosure: I am nowhere near what anyone would call a gamer. Whenever a friend of mine would ask me about a new game that came out and if I have played it, the answer is always no. I generally play a few sports titles and some fighting or racing games and that’s about it. Despite not being fully invested in this, I find an interest in how gaming has evolved over the years. From the 8-bit days all the way to…
"
https://blog.devgenius.io/desktop-as-a-service-how-reliable-is-it-de8800553ebb?source=search_post---------371,"There are currently no responses for this story.
Be the first to respond.
Linux was first released in 1991. Since then, it has come a long way.
We all love Linux so much. It is everywhere and it has changed the whole open-source world. Most of the servers these days run Linux. In the…
"
https://levelup.gitconnected.com/simple-implementation-of-eureka-server-with-springboot-780b64e19735?source=search_post---------372,"Once the microservices are built and deployed, it needs to be located by the clients. When the services are deployed in a physical server, this is a fairly easy task, the host and port information or the endpoint itself could be stored in a configuration file. In an environment where the application is deployed in the cloud, the service instances will have dynamic locations which cannot be configured through a file system. To solve this problem, one would need a discovery tool to dynamically identify the service instances.
There are two types of service discovery — client-side and server-side.
In this article, we will implement the Client-side Discovery using Netflix Eureka.
2. Annotate the main class with @EnableEurekaServer, this would inform the service to act as a eureka server
3. In the property file — add the default port and disable the service from self-registering with eureka
Once these changes are made, build and run the application. Again, if you are on STS, then run the application as spring boot and if you are on Eclipse then run it as a java application.
When the service is up, access it at the default port given in the property file. In this case, it would be http://localhost:8761/
You would see the eureka service up and running. As we have not created any microservice that would be registered with the Eureka server, we do not see any information regarding the instances registered with Eureka.
We will see how to implement a eureka client in the next tutorial.
The entire code for this example is available on GitHub.
Coding tutorials and news.
18 
18 claps
18 
Written by
Author | Programmer | Area of Interest — Software Design & Architecture, Network Protocols & Communication, Space Technology | https://rubykv.com/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Author | Programmer | Area of Interest — Software Design & Architecture, Network Protocols & Communication, Space Technology | https://rubykv.com/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://medium.datadriveninvestor.com/cloud-economics-how-to-overcome-human-biases-to-save-money-4788c1593a17?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
It’s important for cloud customers to understand cloud economics. Cloud costs are dynamic — and hopefully, optimized. However, that’s not always the case. Since optimizing cloud infrastructure is a “technological problem”, there are a number of human biases at play that are not always accounted for.
Some articles you’ll find jump directly to the idea that “cloud economics” is a synonym for “saving money”. And while the economies of scale and infrastructure on demand mean that public cloud can save you money over traditional infrastructure, the two terms are not interchangeable.
www.datadriveninvestor.com
Shmuel Kliger (founder of our parent company, Turbonomic) explains in this video that cloud economics “is the ability to deliver IT in a scalable way with speed, agility, new consumption models, and most importantly, with a high level of elasticity.”
He further explains this idea in another video — that it’s microservices architecture taking the place of monolithic applications that allows this elasticity and rewrites the way cloud economics works.
The concepts described above are exciting — but before assuming these benefits of speed, agility, etc. will be gained naturally upon adopting any type of cloud technology, we need to remember the human context. Taken from the perspective of rational economics, cloud users should always choose the most optimized cloud infrastructure options. If you’ve ever seen a whiteboard diagram of the cloud infrastructure your company uses, or taken a peek at your organization’s cloud bill, you’ll know this is not the case.
To understand why, it’s beneficial to take a behavioral economics perspective. Through this lens, we can see that individuals and businesses are often not behaving in their own best interests, for a variety of reasons that will vary by the individual and the organization… and perhaps by the day.
Cost is particularly dependent on where you sit within an organization and the particular lens you look through. For example, the CFO might have a very different view from the engineering team. Here’s a great talk and Twitter thread on the cultural issues at play from cloud economist Corey Quinn.
Examples of cognitive biases impacting cloud cost decision making include:
(There are plenty more, but perhaps we’re falling prey to the bias and some of these decisions are perfectly rational.)
The point is that despite the automated buzz of AI and robotic process automation, the cloud doesn’t inherently manage itself to optimize costs. You need to do that.
Cloud providers’ management environments are confusing, and do not always encourage users to make good decisions. Luckily, the wind has started to blow the other way on this front, as cloud providers realize that providing cost optimization options provides a better user experience and keeps them more customers in the long run. We’ve started to see more options like Google’s Sustained Use discounts and AWS’s new Savings Plans that make it easier to reduce costs without impacting operations. However, it’s up to the customer to find, master, and implement these solutions — and to know when cloud native tools don’t do enough.
The good news is that being aware of natural tendencies that impact cost optimization is the first step to reducing costs.
First, determine what your goals are. What does “cost saving” mean to you? Does it mean reducing the overall bill by 20%? Does it mean being able to allocate every instance in your AWS account to a team or project so you can budget appropriately? Does it mean eliminating unused infrastructure?
No matter what your goal, you need to understand your cloud bill before you can take action to reduce costs. The best way to do this is with a thorough tagging strategy. All resources need to be tagged. Ideally, you will create a set of tags that is applied to every resource, such as team, environment, application, and expiration date. To enforce this, some organizations have policies to terminate non-compliant instances, effectively forcing users to add these essential tags.
Then, you can start to slice and dice by tag to understand what your resources are being used for, and where your money is going.
Once you have a better picture of the resources in your cloud environment, you can start to review opportunities to use pricing options such as Reserved Instances or Savings Plans; places to eliminate unneeded resources such as orphaned volumes and snapshots; schedule non-production resources to turn off outside of working hours; upgrade and resize instances; etc.
While engineering teams can do these reviews as part of their normal processes, many organizations choose to create a “cloud center of excellence” or a similar department, solely focused on cloud expertise and cost management. Sysco shared a great example of how this worked for them, with gamification and a healthy dose of bagels as motivating factors for users throughout the organization to get on board with the team’s mission.
On the flip side, there’s only so far food bribery can go. Since, as we’ve outlined in our cloud economics model, changing user behavior and habits is difficult, the best way to ensure change is by sidestepping the human element altogether. Those on/off schedules for dev and test environments? Automate them. Governance? Automate it. Resizing? Automate.
Originally published at www.parkmycloud.com on January 28, 2020.
empowerment through data, knowledge, and expertise.
93 
93 claps
93 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
CEO of ParkMyCloud
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/manifoldco/so-youre-ready-for-the-holidays-is-your-email-739816fc56b1?source=search_post---------374,"There are currently no responses for this story.
Be the first to respond.
Making an email stand out in the inbox is difficult enough, but with everyone competing for the same inbox real estate as your brand around the holidays that challenge skyrockets. Before you cave in to the pressure of increasing your email frequency, stop and take a deep breath. Here at Mailgun, we send billions of email every month, and that number only increases once we reach the Q4 holiday season. When it comes to sending, we like to help out when we can. We sat down and made a checklist to make sure that your sending habits and emails are wrapped up (and formatted correctly) for successful holiday sending this year.
First things first, how much do you actually plan on sending? Some senders don’t think about the influence a sudden increase in sending can have on things like reputations, engagement rates, and even billing. Before you send anything, it’s important to step back and go through the following measures (if they’re applicable):
No matter which provider you’re using to send out email, it’s important that all of these things are done ahead of time. Do your due diligence now instead of the night before and you’ll save yourself a world of stress.
We understand how tempting it is to dump all of your email replies back into a bucket like noreply.domain.com, but trust us when we say there is a better way. Proper inbound routing doesn’t have to be painful or hard, and it leads to better customer satisfaction.
A good rule of thumb is to have replies routed to either a support or customer experience team for handling issues like incorrect addresses, product returns, etc. The subdomains of these transactional email interactions should reflect the team they are connecting with like @support.domain.com or @customerexperience.domain.com.
Worried about the jumbled mess of code you get back from replies? A good inbound routing solution will take care of that mess for you. Mailgun’s inbound routing comes fully parsed and transcoded to UTF-8, making it a breeze to deal with customers. Poorly handled inbound routing leads to a lot of headaches for multiple departments, so be sure to go over what procedures you have in place before the start of the season, so you don’t have to dig in an abandoned email bucket later.
Your company might already have your email contacts sorted for the holiday season already, but whether you do or you don’t it’s the right time to look at your lists carefully.
Sit down with your marketing team and check all of the lists for engaged users first, this will help improve your deliverability rate to the inbox. Getting rid of unengaged recipients will also help limit your sending to honeypot accounts that can get you sent to spam or worse — a blacklist.
Next, run those addresses through an email validation tool that will let you know of any bogus or misspelled email addresses so they don’t tarnish your sending reputation. That will go a long way in cleaning your list ahead of the season (and new year!) Verifying your email addresses may feel like extra work, but those little once and awhile cleanups can massively improve your results.
Like in life, being prepared is the key to a stress-free holiday season; the same saying goes for your email sending. Whether or not you send marketing messages, sticking to some best practices on the technical side gives you a better chance of reaching your contacts in their inbox. If you do the work now, you save yourself the stress of making up for it later.
To learn more about Mailgun, contact them online, visit their website, or sign up for an account through Manifold! Happy Holiday Sending!
Special thanks to Dianna Perez and the rest of the Mailgun team for submitting this post for us to publish on their behalf as part of Manifold’s 12 Days of Services.
We're determined to make it easy for developers to use the…
147 
147 claps
147 
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-founder @ManifoldCo.
We're determined to make it easy for developers to use the cloud services they love.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cudos/cudos-provides-elrond-decentralized-hosting-for-dapps-%EF%B8%8F-6d5d664f3aff?source=search_post---------375,"There are currently no responses for this story.
Be the first to respond.
Elrond started an exciting collaboration with Cudos, a decentralized cloud computing network used by 300,000 businesses and individuals in over 145 countries.
The Cudos validators network connects on- and off-chain resources, bridging the immutability of blockchain networks with the power of computing resources provided by a distributed network. This allows developers to host applications on the Cudos network and connect with the underlying blockchain as needed.
Through cooperation with Elrond, applications running on the Cudos computing layer will be able to connect to the Elrond Network as a settlement layer. This will enable Elrond developers to consume Cudo Compute resources that will provide virtually any WebAssembly, container or virtual machine workload to their distribution network with over 300,000 users.
In addition, a portion of the Cudos token will be bridged to the Elrond Network as Elrond Standard Digital Token (ESDT) to participate in decentralized finance opportunities on Maiar Exchange. Elrond is exploring joint operations of infrastructure, such as Cudos validator nodes, that are relevant for the collaboration and the expansion of our ecosystems.
“True decentralization needs to happen both at the settlement layer, from centralized financial systems to the blockchain, and at the compute layer, from centralized data centers to distribute compute networks. Platforms such as Cudos can efficiently complement the Elrond Network to offer true end-to-end decentralization.” — Beniamin Mincu, CEO of Elrond Network
“We’re excited to connect to a great developer community that shares our passion for Rust. We look forward to exploring these synergies and provide Elrond developers with access to the fast and efficient Cudos distributed compute platform.” — Matt Hawkins, founder and CEO of Cudos
Elrond is an internet-scale blockchain, designed from scratch to bring a thousand-fold cumulative improvement in throughput and execution speed. To achieve this, Elrond introduced two key innovations: a novel adaptive state sharding mechanism and a secure proof-of-stake algorithm, enabling linear scalability with a fast, efficient and secure consensus mechanism. Thus, Elrond can process upwards of 15,000 transactions per second with six-second latency and negligible cost, attempting to become the backbone of a permissionless, borderless, globally accessible internet economy.
The Cudos Network is a layer-one blockchain and layer-two computation and oracle network that ensures decentralized, permissionless access to high-performance computing at scale and enables scaling of computing resources to 100,000 nodes. Once bridged onto Ethereum, Algorand, Polkadot and Cosmos, Cudos will enable scalable computing and layer-two oracles on all of the bridged blockchains.
Website | Twitter | Telegram | YouTube | Discord
Next Generation Cloud
5 
5 claps
5 
Next Generation Cloud
Written by
Next Generation Cloud
Next Generation Cloud
"
https://medium.com/@jaychapel/what-are-aws-scheduled-reserved-instances-and-do-they-save-money-fa68d5bc3026?source=search_post---------376,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jan 20, 2020·3 min read
A few years ago, AWS announced the release of their Scheduled Reserved Instances. These reserved instances are designed for workloads that recur on a daily, weekly, or monthly schedule, and are purchased for a one-year term. AWS says that Scheduled Reserved Instances provide a 5–10% savings over On-Demand instances used for this same purpose.
While we always appreciate ways to save on AWS, there are a few reasons that Scheduled Reserved Instances are unlikely to make a useful addition to your toolbox when compared to other cost-savings options available.
First of all, they have a decidedly limited use case, only for predictably scheduled operations that will go on for at least one year. Many companies would need to see a much higher savings rate than 10% with this year-long commitment when looking at AWS on demand vs reserved.
Secondly, they are inflexible. Once you set a schedule, you cannot change or override it, and the options to set schedules are limited to daily, weekly, or monthly recurrence on a set duration. Since one of the main benefits of cloud is the ultra-flexibility you get on short notice, this might be a deal-breaker by itself.
Additionally, as Beth Pariseau pointed in a TechTarget article, additional management overhead is required to manage every additional type of instance that a company leverages.
Note that Scheduled Reserved Instances are also limited by region — only available in US East (Northern Virginia), US West (Oregon), and Europe (Ireland) regions — and by instance type, currently supporting C3, C4, M4, and R3 instance types. This means that modern versions of those EC2 instance families, like M5, M5a, M5n, M6g, C5, or C5n, are not available for scheduling, so you’re using an older version of those EC2 instance types. While this might not matter much now, the list of usable instance types has not been updating along with the on-demand instance types, which means this problem will only get worse with time.
When compared to standard AWS reserved instance pricing, you’re not really getting the savings you’d expect from this kind of commitment. Reserved Instances typically save 30% for a convertible 1-year purchase, and can be about 60% for a standard 3-year purchase. This means that if you have an EC2 instance you need that would match the scheduled reserved instance but are using that same EC2 size for other workloads throughout the month, then the non-scheduled EC2 reserved instance pricing works much better with more savings.
For your recurring workloads, you can run instances only when you need them but maintain flexibility by using ParkMyCloud to schedule on/off times for On-Demand instances. By keeping workloads on just during business hours, you’ll save 65% using ParkMyCloud, with even more savings achievable if you need that instance even less throughout the month (and doesn’t even account for savings achieved through RightSizing). This beats any AWS RI pricing while maintaining flexibility for your organization. Keep this in mind while you are evaluating your reserved instance vs on demand decisions.
See the chart below for a full comparison of using Scheduled Reserved Instances vs. using ParkMyCloud.
This comparison shows that AWS Scheduled Reserved Instances are unlikely to be worth any effort or investigation by your cloud operations team. ParkMyCloud provides more benefits and much higher savings with more flexibility and less commitment. Even standard AWS EC2 Reserved Instance pricing and savings can give you more bang for your buck.
If you’ve got workloads and servers that don’t need to run very frequently throughout the month, but you need to ensure they can be spun up at a moment’s notice, then ParkMyCloud can help you save money and enable your users for maximum cloud efficiency. Give ParkMyCloud a try for yourself — start seeing savings today.
Originally published at www.parkmycloud.com on January 14, 2020.
CEO of ParkMyCloud
12 
12 
12 
CEO of ParkMyCloud
"
https://medium.com/@addevice/how-5g-and-cloud-based-will-shape-the-future-of-global-applications-8eeeff96f622?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
Addevice | Mobile Development
Apr 27, 2021·6 min read
The prospects of 5G and cloud-based applications were bright even before Covid-19. The pandemic only accelerated the transformation into a 5G and cloud-based application realm. We witness unprecedented growth in internet usage in addition to the expansion of remote work on a massive scale. The data processing and application development will be impacted by these processes. Here is how 5G and cloud-based will shape the future of global applications.
Let’s look at how technologies will shape the digital transformation in 2021. There are two trends that clearly shape the digital realm and these are Artificial Intelligence technologies and the Internet of Things.
Some 90% of new enterprises are eager to apply AI technology in their businesses by 2025, according to IDC. According to Research and Markets, AR/VR technology shall have rapid growth until 2025.
IDC also predicts that by 2025, nearly two-thirds of companies will be software producers with over 90% of apps deployed in cloud-native, 80% of code externally sourced. IDC also says that there will be 1.6 times more developers as compared to the current number.
Another trend will be the consolidation of 5 public cloud mega-platforms with 75% of the Iaas+Paas market share. IDC says SaaS vendors will become ‘’platforms’’ drastically.
It’s not a secret that latency is going to matter more and more. What is latency in computing? It’s the delay before a transfer of data begins following an instruction for its transfer. So, how long should it take for the data to transfer? Research says you should do that in less than several hundred milliseconds. This brings us to the 100 Millisecond Rule.
The term was coined by the Department of Defense. They found the time a user hits a keyboard until the time when a letter appears on the screen is 100 milliseconds. When you exceed 100 milliseconds, you cause a delay that is perceptible to a human being.
So, what is the lesson for the application developers? If 100 milliseconds is the time that action is conceived as instantaneous, they need to keep to 100 Millisecond Rule. Providing a user experience with low latency makes a difference. Studies show that it fosters retention and, finally, increases the conversion rate.
It’s unbelievable how much a person of the 21st century spends on the internet. You can see people on TikTok, Facebook, and Instagram on the transport and even on roads. But that’s not the whole story. Our home devices are already connected to the internet. IoT devices are widely gaining popularity. An average housewife takes care of her grilling chicken even when in a supermarket. 35 percent of consumers enhance their home connectivity with range extenders and Wi-Fi mesh networks already.
Does this tell you anything about the application development? We will witness a new infrastructure that will deliver local experiences to global audiences. We will see a new global platform with new capabilities.
We talked about the 100 Millisecond Rule. But 5G does more than that. Let’s talk about 1 millisecond(1ms) with 5G. Just compare with 4G that was 200 milliseconds. What implications will it have for apps and networks? We believe soon we will have the next generation of applications that will create real-time experiences with incredibly low latencies.
Many remember the times when we made a shift from dial-up to DSL. But now it’s time for new opportunities with 5G. It’s time for multi-user interactive applications and platforms. This will have a tremendous impact on the way people make money and spend their time. As Spencer Kimball says, this is a historic leap in network latency.
As we already mentioned, there will be 1.6 times more developers as compared to the current number. We expect to see multi-billion companies as the experience of Netflix and Uber is inspiring.
We say that new consumption patterns will empower developers. What does this mean in real life? First, we will see a proliferation of IoT devices. Next, companies will rush to provide managed services software and infrastructure-as-a-service. And this trend is already ongoing.
Just consider one simple forecast. The number of IoT devices will total 41 billion by 2027. By 2023, 70% of automobiles will be connected to the internet.
These will have implications for application development and the developers. We will witness an expansion and massive growth in the app development market with prospects for new software with new capabilities.
Most companies feel secure and on budget when they turn to cloud-based applications. However, this is not the case. Even with cloud apps, you need to have your cloud backup apps. To avoid unpleasant situations with data loss, you should apply one of the cloud backup apps that will copy your personal files to offsite cloud storage.
Why do that? Suppose there is a theft or fire or flood. You will be in danger of losing both your computer and its backup drive. To avoid massive loss of data, you need to turn to cloud backup apps. They protect your data with industry-standard encryption.
Here are some of the cloud backup services that you may want to use:
IDrive
+ Free disk-shipping data-transfer option
+ Uploads are fast
+ Back up mobile devices available
+ Rich syncing option
- No unlimited storage option.
Backblaze
+ Cheap, fast, easy, and simple
+Unlimited storage
- No multiple-computer plans
- Mobile apps could be better.
Acronis
+ Feature set is unique
+ Small system-performance impact
+ Modern, intuitive user interface
- Complicated pricing
- You may not use all features.
CrashPlan
+ Unlimited storage
+ Fast upload & download
+ Security option
- Expensive
- Requires a lot of resources during backups.
SpiderOak One
+ Backs up unlimited devices
+ Strong sharing and syncing features
+ Secure
- Steep learning curve
- Cramped, confusing user interface.
Carbonite Safe
+ Unlimited storage
+ Intuitive backup-flagging system
- Some features cost extra
- Slow upload speeds
- Mobile apps are no longer available.
Having said all this, let’s think about the future of application development. We are clear now that the public demands real-life experiences with low latency. And the 5G and cloud make that possible today. But there is one but. The application development is not going to be dominated by Facebooks and Googles only. Individual developers and small groups are going to have their say.
We expect to see super innovative apps and new cases that we have not dreamt of. The IoT market, virtual reality will create an environment that solutions out of the box will emerge. And these will shape the application development creating a promising future for new and innovative apps.
The prospects of application development are bright with 5G and cloud-based apps. There is no doubt about it. But don’t expect to see giant companies like Zoom or Facebook in the market only. Expect to see the unexpected. With prospects of virtual reality and cloud capabilities, new apps will come out that will shape the market in a new way.
#IT. #Mobile #App #Development. #iOS. #Flutter. https://addevice.io/
9 
9 
9 
#IT. #Mobile #App #Development. #iOS. #Flutter. https://addevice.io/
"
https://medium.com/@jaychapel/why-reserved-instance-pricing-needs-careful-evaluation-c7110fa90e58?source=search_post---------378,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 26, 2019·3 min read
Once or twice a year we like to take a look at what is going on in the world of reserved instance pricing. We review both the latest offerings and options put out by cloud providers, as well as how users are choosing to use Reserved Instances (AWS), Reserved VMs (Azure) and Committed Use (Google Cloud).
A good place to start when it comes to usage patterns and trends is the annual Rightscale (Flexera) State of Cloud Report. The 2019 report shows that current reservation usage stands at 47% for AWS, 23% for Azure and 10 percent of GCP. These are some interesting data when you view them alongside companies overall reporting that their number one cloud initiative for the coming year is optimizing their existing use of the cloud. All of these cloud providers have a major focus on pre-selling infrastructure via their reservations programs as this provides them with predictable revenue (something much loved by Wall St) plus also allows them to plan for and match supply with demand. In return for an upfront commitment they offer discounts of ‘up to 80%”, albeit much as your local furniture retailer has big saving headlines, these discount levels still warrant further investigation.
While working on an upcoming new feature release we began to dig a little deeper into the nature of current reserved instance pricing and discounts. From our research it appears that a real world discount level is in the 30%-50% range. To achieve some of the much higher level discounts you might see the cloud providers pushing, typically requires commitments of three years; being restricted to only certain regions; restrictions on OS types; and generally a willingness to commit to spending a few million dollars.
Reservation discounts, while not as volatile as spot instances, do change and need to be carefully monitored and analyzed. For example as of this writing, one of the more popular modern m5.large instance types in a US East Region costs $0.096 per hour when purchased on demand, but reduces to $0.037, a significant 62% saving. However, to secure such a discount requires a three-year commitment and prepayment in full up front. While the numbers of such organizations committing to contracts of this nature is not publicly known, it is likely that only the most confident of organizations with large cash reserves would be positioned to make a play like this.
Depending on the precise program used to purchase the reservations, there can be certain options to either convert specific instance families, instance types and OS’s for other types or even to resell the instances on a secondary exchange for a penalty fee of 12%, on AWS for example. Or to terminate the agreement for the same 12% fee on Azure. GCP’s Committed Use program seems to be the most stringent as there is no way to cancel the contract or resell pre-purchased instances, albeit Google does not offer a pre-purchase option.
As the challenge of optimizing cloud spend has slowly moved up the priority list to take the #1 slot, so has a maturation process taken place inside organizations when it comes to undertaking economic analysis and understanding the various tradeoffs. Some organizations are using tools to support such analysis, others are hiring consultants or using in house analytics resources. Whatever the approach in terms of analyzing an organization’s use of cloud, this typically requires looking at balancing the purchase of different types of reservations, spot instances or using on-demand infrastructure that is highly optimized through automation tools. Whatever the approach, the level of complexity in such analysis is certainly not reducing, and mistakes are common. However, the potential savings are significant if you achieve the right balance and is clearly something you should not ignore.
The relative balance between the different options to purchase and consume cloud services in many ways reflects the overall context within which organizations operate, their specific business models and broader macro issues such as the outlook for the overall economy. Understanding the breadth of options is key and although for most organizations, reservations are likely to be a key component it is worth digging into just how large the relative trade offs might be.
Originally published at www.parkmycloud.com on April 2, 2019.
CEO of ParkMyCloud
12 
12 
12 
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/how-to-analyze-google-cloud-committed-use-discounts-bf9eb587ccfb?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 15, 2019·6 min read
In this blog we will look at the Google Cloud Committed Use discount program for customers that are willing to “commit” to a certain level of usage of the GCP Compute Engine.
The Committed Use purchasing option is particularly useful if you are certain that you will be continually operating instances in a region and project over the course of a year or more. If your instance usage does not add up to a full month, you may instead want to look at the Google Cloud Sustained Use discounts, which we discussed in a previous blog.
The Google Cloud Committed Use discount program has many similarities to the AWS and Azure Reserved Instances (RI) programs, and a couple unique aspects as well. A comparison of the various cloud providers’ reservation programs is probably worth a blog in itself, so for now, let’s focus on the Google Cloud Committed Use discounts, and the best times and places to use them.
General Purpose Memory Optimized
Each Committed Use purchase must include a specific number of vCPUs and the amount of memory per vCPU. This combination of needing to commit to both a number of vCPUs and amount of Memory can make the purchase of a commitment a bit more complicated if you use a variety of machine types in your environment. The following table illustrates some GCP machine types and the amount of memory automatically provided per vCPU:
Machine TypeMemory per vCPUn1-standard3.75 GBn1-highmem6.50 GBn1-highcpu0.90 GBn1-ultramem14–24 GBcustom0.9–6.5 GB
While the vCPU aspect is fairly straightforward, the memory commitment to purchase requires a bit of thought. Since it is not based on a specific machine type (like AWS and Azure), you must decide just how much memory to sign-up for. If your set of machine types is homogeneous, this is easy — just match the vCPU/memory ratio to what you run. The good news here is that you are just buying a big blob of memory — you are not restricted to rigidly holding to some vCPU/memory ratio. The billing system will “use up” a chunk of memory for one instance and then move on to the next.
Looking at a specific example, the n1-standard-8 in the Oregon region that we discussed in the Sustained Usage Blog, we can see that the Committed Use discount does amount to some savings, but one must maintain a usage level throughout the month to fully consume the commitment.
Recall from the earlier blog that the base price of this instance type in the GCP Price list already assumes a Sustained Usage discount over a full month, and that the actual “list price” of the instance type is $277.40, and Sustained Usage provides up to a maximum of a 30% discount. With that as a basis, we can see that the net savings for the Committed Use discount over 1 year is 37%, and over 3 years, rises to 55%. This is close to the advertised discount of 57% in the GCP pricing information, which varies by region.
The break-even points in this graph are about 365 hours/month for a 3 year commitment, and 603 hours/month for a 1 year commitment. In other words, if you are sure you will be using a resource less than 365 hours/month over the course of a year, then you probably want to avoid purchasing a 3 year Commitment.
Because Commitments are assigned on a vCPU/RAM basis, you cannot simply point at a specific instance, and say THAT instance is assigned to my Committed Use discount. Allocation of commitments is handled when your bill is generated, and your discount is applied in a very specific order:
This sequence is generally good for the customer, in that it applies the Commitment to the more expensive instances first. For example, an n1-standard-4 instance in Northern Virginia normally costs $109.35. If an equivalent server was constructed as a Custom instance, it would cost $114.76.
For sole-tenant node groups, you are typically paying for an entire physical machine, and the Committed Use discount serves to offset the normal cost for that node. For a sole-tenant node group that is expected to be operating 7x24x365, it makes the most sense to buy Committed Use for the entire system, as you will be paying for the entire machine, regardless of how many instances are running on it.
Commitments are allocated over the course of each hour in a month, distributing the vCPUs and RAM to all of the instances that are operating in that hour. This means you cannot buy a Commitment for 1 vCPU and 3.75 GB of RAM, and run two n1-standard-1 instances for the first half of the month, and then nothing for the second half of the month, expecting it all to be covered by the Commitment. In this scenario, you would be charged for one month at the committed rate, and two weeks at the regular rate (subject to whatever Sustained Usage discount you might accumulate for the second instance).
Unlike AWS, where Reserved Instances are automatically shared across multiple linked accounts within an organization, GCP Commitments cannot be shared across projects within a billing account. For some companies, this can be a major decision point as to whether or not they commit to Commitments. Within the ParkMyCloud platform, we see customers with as many as 60 linked AWS accounts, all of which share in a pool of Reserved Instances. GCP customers do not have this flexibility with Commitments, being locked-in to the Project in which they were purchased. A number of our customers use AWS Accounts as a mechanism to track resources for teams and projects; GCP has Projects and Quotas for this purpose, and they are not quite as flexible for committed resource sharing. For a larger organization, this lack of sharing means each project needs to be much more careful about how they purchase Commitments.
Google Cloud Committed Use discounts definitely offer great savings for organizations that expect to maintain a certain level of usage of GCP and that expect to keep those resources within a stable set of regions and projects. Since GCP Commitments are assigned at the vCPU/Memory level, they provide excellent flexibility over machine-type-based assignments. With the right GCP usage profile over a year or more, purchase of Google Cloud Committed Use discounts is a no-brainer, especially since there are no up-front costs!
Originally published at www.parkmycloud.com on July 26, 2018.
CEO of ParkMyCloud
5 
5 
5 
CEO of ParkMyCloud
"
https://medium.com/webtutsplus/how-to-publish-your-spring-boot-app-to-the-cloud-97590666bf7b?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
We have created a simple authentication system in Java and Springboot, which you can see here.
"
https://medium.com/@odsc/migration-to-cloud-and-legacy-systems-challenges-and-opportunities-a2769712c1c1?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
ODSC - Open Data Science
Dec 11, 2020·4 min read
The emergence of cloud computing has brought about a real revolution in the way of storing and accessing data. Users don’t have to store their files on private servers anymore. Cloud computing is a virtual space that connects users all over the world. By using Cloud and legacy systems, companies can share resources, software, and data all over the Internet.
With Covid-19 accelerating virtual working trends, moving to the Cloud has become a baseline digital transformation imperative. It’s now more important than ever to collaborate and innovate on a unified platform that’s Cloud-native.
As organizations move to the Cloud, the need to address what to do with legacy systems that aren’t Cloud-native becomes a priority. More often than not, an organization’s target Cloud environment won’t support their legacy systems. As companies consider options for their legacy systems, they have a few including:
Regardless of the option selected, one can guarantee it will be costly — in time/effort and dollars. In fact, in a recent survey among CIOs, conducted by Logicalis, more than half spent 40% to 60% of their time managing legacy IT instead of focusing on or towards strategic activities. From this alone, we can deduct that legacy technology is a significant barrier to digital transformation and innovation.
Many companies continue using outdated systems with the old “if it’s not broke, why fix it?” adage. When actually there are quite a lot of reasons to modernize your legacy systems. The cost of running legacy software being the foremost reason, especially with free open-source alternatives available. Legacy systems also require a specific technical environment, including hardware. This will result in long term infrastructure maintenance spending remaining high, as compared to modern cloud-based solutions. This said, the fact is there are often hidden costs overlooked to maintaining legacy, and these include:
Security: legacy systems lack modern security features that can leave them less resistant to cyberattacks, harmful programs, and malware, which is only logical. If the software solution has been around for years, attackers have most likely had enough time to get familiar with the code and find its vulnerabilities.
Integration/Compliance: modern technologies are integration-ready by default and API vendors typically provide support for most of the programming languages and frameworks out of the box. However, connecting legacy software to a third-party tool or service often requires a significant amount of custom code.
Lost business opportunities: when maintaining legacy, you leave less room for innovation by the nature of your talent stagnating in the “same old same old” day-to-day use of technology. Instead of adopting new technologies and business models, you are stuck with your old software, letting new opportunities in your industry go unnoticed. How might this leave you open to competitive threats by those outperforming or looking to take over your market share?
This is why we advocate that by adopting open-source Apache Spark, there is no faster, easier way to migrate to the Cloud. When it comes to migration for cloud and legacy systems, your organization’s looking at millions of lines of code to convert to transition your current analytics models and workflows alongside your actual data migration. Frankly, only Apache Spark can handle these volumes in a timely, efficient manner. Also, when it comes to data science modernization, only Apache Spark is a comparable toolset to legacy SAS.
Even understanding Spark’s enabling capabilities, organizations still struggle to understand what to migrate when and how to the Cloud. We recommend a lift and reshape (replatform) as a first step, and then if necessary a refactoring to optimize your data structure. The most successful migrations we’ve noted start with migrating your code to a Cloud compatible technology as the first course of action. For example, converting SAS code to native PySpark code becomes the priority before optimizing your code or your analytics models. You’ll want to automate the code conversion process to ensure it happens on time to ensure your staff doesn’t continue to code in SAS and force a never-ending migration state. Once the code is Cloud compatible, the migration of your data from various sources to a target architecture that is unified and centralized, such as a data lake, can then be done more easily.
With your data now in the Cloud, you are able to retire the legacy workflows and modernize new processes in your faster and more performant environment. Spark delivers 100X the compute power, processing data faster than anything else on the market, making it the defacto option.
As you go through these steps, your people will be adopting the new toolsets, now in open source, and innovating and collaborating unlike ever before. When it once took your data scientists hours to run a model, it now can be done in seconds with Apache Spark. Imagine the innovation power behind the opportunity to leverage both structured and unstructured data in data modeling than now have no barriers between data domains.
To accelerate code migration with automation for cloud and legacy systems, organizations can benefit from a look at innovations such as Wise With Data’s SAS to PySpark automated migration solution. As the world’s only fully automated solution, SPROCKET is fast, simple, and accurate, producing native PySpark code at 95% automation. We’d love for you to see SPROCKET in action, contact us at hello@wisewithdata.com for a demo.
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
See all (95)
10 
10 claps
10 
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/tagging-best-practices-for-automated-cloud-governance-75f4ef29e853?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
Earlier this week we discussed ways to improve cloud automation through tagging. Today, I want to extend the conversation to look at how one ParkMyCloud user is applying tagging best practices to improve their cloud governance.
The company we talked to — they’re in media, so let’s call them MediaCorp — has about 10,000 employees, which means the Cloud Engineering team has several hundred cloud users to manage, with a combined 100+ AWS accounts and more than 5,000 active AWS resources. The only way they can maintain security and cost control in a cloud environment of this magnitude is through automated governance. Here’s how they do it.
MediaCorp has a strict policy: every AWS resource must have the same set of five tags attached to it:
How does MediaCorp ensure that all resources are tagged?
The key is to use automated rules to enforce that every resource has the five required tags — this is where ParkMyCloud’s policy engine comes into play. MediaCorp has a set of policies set up to check for the five tags. If a resource is missing any, the resource is immediately put on an “always parked” schedule and moved to a team (a way to group instances in ParkMyCloud) specifically for mistagged resources.
When this happens, the Cloud Engineering team gets an email and a Slack notification, so they can track down the creator of the offending resource and correct the process that created it.
Now the tags themselves come into play. MediaCorp uses their five-tag system for three main purposes:
Configuration management: as mentioned above, they use tags as the trigger for Chef cookbooks, and of course the same applies to Puppet Modules, or Ansible Playbooks.
CI/CD: MediaCorp uses Jenkins to provision cloud resources, so they use tags to associate build and deployment servers with their corresponding repository and build number, for both automated and manual development tasks.
Cost control: the “environment” tag determines what parking schedule is applied to each resource. Production resources run 24×7, of course, while “dev” or “test” resources are put on a schedule to park 7:00 PM — 7:00 AM and on weekends. (Users can always log in to override these schedules if needed.)
It may at first seem unnecessarily harsh to automatically park any resource that doesn’t have proper tags applied, but this process is what allows MediaCorp to keep a well-governed, cost-controlled infrastructure. You can always adapt their use case to your own needs by simply moving resources to another team and notifying that action is needed, without changing the state or schedule on the resource.
Either way, with a rigorous application of tagging best practices in place, you can automate governance and improve your workflows.
Originally published at www.parkmycloud.com on August 17, 2018.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
14 
14 claps
14 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/pharos-production/aws-elastic-beanstalk-part-1-cc0002825d54?source=search_post---------383,"There are currently no responses for this story.
Be the first to respond.
Give us a message if you’re interested in Blockchain and FinTech software development or just say Hi at Pharos Production Inc.
This time we will look at steps we require to depl0y Elastic Beanstalk app to AWS. We will not use default VPC(virtual private cloud) but will configure all required services from scratch.
Let’s create a new Virtual Private Cloud. Here we define VPC name and associate CIDR block — Classless Inter-Domain Routing. This is a network mask for all addresses inside VPC — 10.0.0.0/16. Addresses will look like 10.0.xxx.xxx/16. All instances in subnets will have CIDRs starting from 10.0.subnet.xxx/24 or 10.0.subnet.xxx/32. Also, you can use 172.31.0.0/16 instead of 10.0. Surely all names should contain something more informal rather than word “production”. But our current setup is just a playground. We don’t add IPv6 address resolution and leave Tenancy as default. You can change this later, Dedicated Tenancy should make your cloud work faster, surely for the additional cost.
New DHCP options set will be added automatically to assign IP addresses to all instances under current VPC.
Also, we should add DNS hostnames and DNS resolution to the VPC, otherwise we’re not able to connect from outside. Actually, this is not the best idea, but we will deploy the database into VPC subnetworks and we want to run migrations from a developer’s workstation. If you have any other plan how to migrate and seed data like running deployment node into VPC than you don’t need to give access from outside.
Now let’s add subnets to the VPC. We will create only public subnets for now. Here we define a new subnet into 1a Availability Zone with a CIDR block 10.0.64.0/20. That means we have 4K IP addresses available in the subnet. For sure this can cover all our needs.
In Subnet Actions menu we choose not to assign public IP addresses to instances. We want to access the only database from outside and not instances itself.
Another thing we should create is an Internet Gateway it possible to communicate with the internet.
Next, we should define a Routing Table. Here we’re attaching created internet gateway and VPC IP address. On subnet associations, we select all created subnetworks. Now all of them are linked to the router and able to connect with each other and with the internet too.
Also, we should define subnetwork ACL — to protect all traffic on a subnetwork’s layer. We have also Security Groups, they do the same but on instance layer. Another difference between ACL as SG is that you define in security group who can access the content, but in ACL you define who can’t. To simplify all our setup for now we define that everyone can come in and out of subnetwork. We should change this in the future. It’s a good practice to define rules with numbers like 100, 200, 300 etc. The last one is “other” case if any case has been succeeded. Later you can insert rules between 100 and 200 just by setting its value for example to 120.
Let’s add required Security Groups. We need 3 groups for — EC2 instances, Elastic Load Balancer, RDS.
All HTTPS traffic termination should be done in ELB, so inside the subnetwork all traffic is HTTP. Security group assigns its security on a per-instance basis. So we need to open port 80 for HTTP and 22 for SSH. Also, we can open ICMP port for the ping, but there is not much sense to do this. HTTP inbound rules are set to ELB security group. We don’t want to accept any other HTTP traffic. If you want to make instances communicate between each other, you should set EC2 security group to accept EC2 SG connections. An outbound rule is — open to all ports to an IP address.
ELB security group has an inbound rule set to HTTPS. This is the only one port we want to use on a connection. When we decide to use SSH to communicate with instances, we can set a public IP to the instance and connect directly to it. Outbound rules set to accept all
RDS security group has 3 rules, all 3 is for accepting connections on a Postgres port 5432. Here we specify source groups — it can get a connection from itself(means from all instances in RDS SG), from EC2 SG and from developer’s IP. Outbound rules set to accept all.
That’s all for the basic setup. Now VPC is ready to handle Elastic Beanstalk application and a database!
We start not from a database itself but from parameters setup. Let’s move from bottom to top of the menu.
We need to create a new Subnet Group. DB instance will operate in a group of subnetworks and we want to specify them. We want to use all subnets in a selected VPC.
Next, we create a Parameter Group. This is our future Postgres settings including everything you can tune in Postgres.
Now we’re ready to create a database. We make database separated from EB application to avoid database destruction in case we decide to completely remove EB environment but leave database alive.
In RDS console we select PostgreSQL as our database and production tier. We can select Dev/Test too, there is no difference.
All settings are quite straightforward. Here we selecting:
General Purpose (SSD)storage is suitable for a broad range of database workloads. Provides a baseline of 3 IOPS/GB and ability to burst to 3,000 IOPS.
Provisioned IOPS (SSD)storage is suitable for I/O-intensive database workloads. Provides flexibility to provision I/O ranging from 1,000 to 30,000 IOPS.
Magnetic storage may be used for small database workloads where data is accessed less frequently.
Fill settings with your favorite username and super-secured password. DB Instance Identifier will appear in the database DNS.
Also a new feature at RDS console, we check a price— supercool!
Network and Security. We select our VPC, subnet group, set database publicly accessible to make migrations from the development workstation. We have no preferences in the availability zone and select RDS Security Group.
Other settings are quite straightforward. Just don’t forget to set database name and DB Parameter Group here. If you don’t really want to back up your database, set retention period to 0.
That’s all for the basic setup. All other parameters like max connections, ACL settings etc you can set by yourself depends on your application and requirements. Now VPC is ready to handle an Elastic Beanstalk application!
End of Part 1. Next time we will look at how to set up and deploy basic EB application into this environment.
Thanks for reading!
Software Development Company.
92 
92 claps
92 
Software Development Company. Fintech and Blockchain. Enterprise Solutions.
Written by
CTO and Founder@ Pharos Production Inc. — Blockchain and Fintech Software Development Company.
Software Development Company. Fintech and Blockchain. Enterprise Solutions.
"
https://medium.com/@jaychapel/5-things-to-do-when-you-outgrow-the-aws-free-tier-ae68fe08ca76?source=search_post---------384,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 27, 2019·4 min read
The AWS free tier is a great way to get started using Amazon Web Services — it can be a great boost to individuals, startups, and small businesses. In fact, the AWS free tier was essential to getting ParkMyCloud off the ground when we launched. But of course, this program has limits on what you can use without being charged.
The AWS free tier is designed to give you the AWS experience without the cost, but that also comes with limitations on instance types, storage, hours, and how often you can call operations each month. Of course, all good things must come to an end. If you’ve outgrown the free tier option and are ready to experience the full benefits of AWS, there are a few things you can do to make sure you’re getting the most out of being a paying AWS customer.
The first thing to consider when your 12 months on forgoing the AWS free tier expire option is the most obvious difference — cost versus no cost. You’re paying for cloud services now, so ensure that you don’t pay more than you intend to.
Use AWS Budgets to create custom cost and usage budgets that notify you when you exceed (or are about to exceed) your budgeted amount. Track budgets by the month, quarter, or year, with custom start and end dates. You can also track costs by services, account, tags, and more, receiving alerts directly to your email or through the Simple Notification Service.
With AWS Budgets, you can also set custom utilization targets for reserved instances including Amazon EC2 instances, Amazon RDS, Amazon Redshift, and Amazon ElastiCache, receiving alerts whenever your usage drops below your set utilization target. To get started with creating and tracking budgets, start from the AWS Budgets dashboard or the Budgets API.
Next, you need to ensure that that budget is only going toward resources you actually need — so cost optimization should be a top priority. You might be overpaying by leaving instances running during non-production times, when you don’t need them. Scheduling stop/start times with automation is an easy way to integrate cost control outside of the AWS free tier.
Yet another caveat of cost optimization is right sizing. Besides making sure your instances are turned off when not in use, you should also make a practice of only using as much as you need at a given time, and that’s where right sizing comes into play. Size your workloads according to performance and capacity requirements, both initially and on an ongoing basis to ensure that resources do not end up underused or idle. AWS suggests that you use CloudWatch metrics to get a full view of your environment, and make a habit of right sizing once per month to keep the process smooth, ensure that you’re monitoring costs and keeping track of your billing and usage over time.
See a full list of cost traps to avoid in The Cloud Waste Checklist.
As your infrastructure grows, it’s important to manage your AWS resources with an effective tagging strategy. Tagging gives you the ability to attach custom metadata to instances, images, and more. Resources can be categorized by owner, purpose, or environment, helping you stay organized, improve visibility, and keep costs in check.
A good tagging strategy gives you a more accurate model for chargeback and showback and better insight in your usage and spend, but it’s up to you to enforce quality of tagging. Soft enforcement gives users notifications when policies are not followed, and hard enforcement automatically removes resources that are not tagged to align with company standard. According to AWS, organizations that use hard enforcement have a better time ensuring that the quality of tagging is enforced.
Learn more about tagging best practices.
Scheduling, right sizing, budget limits, and tagging are all methods of keeping costs optimized after you switch from the AWS free tier to a paid, full-service option. But what do all of these practices have in common? Governance. Clear policies and processes to keep usage, capacity requirements, and billing in check are all part of cloud and cost management, and should remain an ongoing priority as you continue using AWS or any cloud service provider.
For more information and how to plan governance after outgrowing the AWS free tier option, learn about how one software company automates governance.
Originally published at www.parkmycloud.com on August 23, 2018.
CEO of ParkMyCloud
27 
27 claps
27 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-best-aws-s3-and-dynamodb-courses-for-beginners-in-2021-a8a44b6066da?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to learn AWS S3 and DynamoDB in-depth and looking for the best online courses to start with then you have come to the right place. In the past, I have shared in-depth courses to learn Cloud Computing, AWS EC2, CloudFormation, and Quicksight, and today I’ll share the best courses to learn AWS S3 and DyanmoDB online.
These are some of the most essential AWS Services, and you should learn them if you are working in AWS on a day-to-day basis for preparing for in-demand AWS certifications like Cloud Practitioners, AWS Developer, AWS SysOps, and AWS Solution Architect exam. Good knowledge of AWS S3 and DynamoDB goes a long way in becoming an AWS Hero you always wanted to be.
Amazon AWS has powered thousands of people's websites and enterprise applications over the years by helping them with its various advanced infrastructure services ranging from cloud computing to file storage content delivery internet of things, and many more just to name a few.
One of the most used services are amazon AWS S3 and DynamoDB and both are considered as a database system but each one has its functionality the S3 is object storage used for storing files and images as well as web applications while DynamoDb is a NoSQL database service where you can store key values such as the people sign-in email and password and so on.
You will see in this article some courses that you can take from home and teach you how to use each service like how to use S3 in a file and image storage service in your project and DynamoDB as a database service. These online AWS courses have been created by experts and many AWS developers around the world. And many people have joined this course to learn these critical AWS services in depth. They are also very affordable and you can buy them on Udemy sales for just $9.99 which is almost free given the quality of these courses.
Without wasting any more of your time, here is the list of best online courses to learn AWS S3 and DyanmoDB. The list includes the best AWS S3 and Dynamo DB courses from Udemy, Pluralsight, and Coursera.
It also contains both beginner and intermediate level courses so that you can start from scratch and learn these key services in-depth
If you have some background in the IT industry or having a web application and you want a great service to deploy your work on a reliable hosting then Amazon AWS S3 is one of the best choices and this course is dedicated to teaching you these skills.  The course starts by teaching you the fundamentals such as how the data is stored inside S3 and the pricing and so on then moving to work with files in Amazon S3 such as uploading deleting and using buckets and protecting your data on the cloud and how to use the command-line interface.   You will learn in this course:
Here is the link to join this AWS S3 course — AWS Amazon S3 Mastery Bootcamp
For Javascript and Nod.js developers, this course is the best if you are intending to create a web application or any kind of services and websites that needs writing data to the databases and you want to upload your work on amazon AWS.  You will learn the basics of DynamoDb such as the basics of database relationships and data normalization the moving to advanced topics like designing tables as well as accelerate the DynamoDb performance and using the SDK to integrate the web and mobile application and more.   You will learn in this course:
Here is the link to join this DyanmoDB course — AWS DynamoDB
For anyone who has a web application or an online service app and wants to connect this application to Amazon Dynamodb then this short course is the right for you showing you a real-world example of how to achieve that goal.  Starting with the basics of Dynamodb like the data types and creating tables then moving to integrate Node.js with Amazon DynamoDb and some other languages such as python then you will create real-world Dynamodb web service and finally learning how to audit Dynamodb operations.   You will learn in this course:
Here is the link to join this course — Connecting Amazon DynamoDB to Your Application
This is the best Coursera course to learn Amazon DynamoDB, a key-value and document database that delivers single-digit millisecond performance at any scale. This is an intermediate level course and it is offered by none other than AWS, which means you will be learning from the source itself. This online training course from the Amazon web service will first teach you what is NoSQL databases and the challenges they solve.
After understanding NoSQL concepts you will dive deep into Amazon DynamoDB topics such as recovery, SDKs, partition keys, security and encryption, global tables, stateless applications, streams, and best practices. This course uses a combination of video-based lectures delivered by Amazon Web Services expert technical trainers, demos, and hands-on lab exercises, that you can run in your own AWS account. This combination of learning material enables you to build, deploy, and manage your own DynamoDB-powered application.
Here is the link to join this Coursera course — Amazon DynamoDB: Building NoSQL Database-Driven Applications
By the way, if you find Coursera certifications and courses useful, then, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it’s completely worth your money as you get unlimited certificates.
coursera.com
If you have seen many YouTube videos and tutorials but still not fully understanding how to use amazon AWS S3 properly for your application then this course Amazon s3 Mastery will help you achieve that goal and be comfortable using this service.  You will learn how to use the console and the buckets as well as creating different user levels and groups and how to use Cyberduck which an FTP service with Amazon S3 storage as well as CloudBerry to manage files and how to encrypt files and more.   You will learn in this course:
Here is the link to join this course — Amazon S3 Mastery
The last course in this article is intended to be for people with no prior experience in Amazon databases and will teach you from scratch how to use NoSQL for your projects with some interesting topics such as security and SDKs.  Starting with the database relationships how they work and the basics operation that you perform then moving to use the SDK and recovery as well as the security and encryption and private cloud global tables and more in this course.   You will learn in this course:
Here is the link to join this course — AWS Developer: Getting Started with DynamoDB
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount). I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
This is one of the best AWS course you can get online and fortunately its also available on Udemy, that means you can get it for just $9.9 on Udemy sale. This course will not just teach you Amazon S3 and DynamoDB in great detail but also mos of the core AWS services.
While this is more popular among the people preparing for the AWS solution architect exam, I find this course very good to learn and master AWS concepts and services.
The course is taught by Stephane Maarek, an AWS Hero and a 9 times AWS certified professional. If you want a single course to learn everything on AWS including S3 and DynamoDB then I highly recommend you to join this course.
Here is the link to join this course — Ultimate AWS Certified Solutions Architect Associate 2021
That’s all about the best online courses to learn AWS S3 and Dynamo DB, two of the most essential AWS services. The article has shown you some of the best courses to learn Amazon AWS S3 and Dynamodb and the best practices of each service as well as how to leverage the power and use those AWS services for your own projects. The knowledge you will learn in these courses will also be useful to pass popular AWS certifications like Cloud Practitioner, Developer Associate, SysOps Administrator, and AWS Solution Architect Associate.  Other Amazon Web Services Courses you may like
P. S. — If you are keen to learn Amazon Web Services and looking for a free online training course to start with then you can also check out these free AWS courses by Linux Academy on Udemy. This course is completely free and you just need an Udemy account to join this course.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
146 
146 claps
146 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/commencis/how-to-scale-your-digital-platform-while-minimizing-infrastructure-costs-with-aws-59e34e41376?source=search_post---------386,"There are currently no responses for this story.
Be the first to respond.
The leading Turkish online betting platform experiences high growth day by day expanding with new features and bet types. As its daily traffic fluctuates significantly, the company was in search of ways to reduce its server costs while improving the reliability and availability of its services.
The Challenge
The company has a variety of bet types for sport games and offers a safe, quick and enjoyable betting experience to its customers. Launched its first mobile application on 2006 and expanded along the upcoming years, today its online platforms experience one of the highest daily traffic in Turkey.
Being an online sports betting company, daily active users and traffic on its platforms fluctuate everyday depending on important sports events. Daily activity increases on weekends due to league matches while it remains lower during the weekdays when there is no significant sports event take place. While total number of sessions may decrease down to 400k on low days, it may increase up to 1.2 million on high days such as an important game night.
The company needed a solution to optimize its server cost while improving reliability and service availability on its high traffic platforms. Since online betting takes place 7/24 everyday, risks of potential outages put significant risks on company’s reliability as well as revenue generation.
Why Amazon Web Services
As Commencis, we decided that the best solution for a scalable, reliable and cost-optimized system was AWS cloud solution. Considering the fluctuations in traffic, the use of Autoscaling Group for EC2 was inevitable to respond to the demand on high days and reduce server costs on low days.
We have provided the solution with Up & Out scalability as you can see in the following diagram:
The Benefits
Today, our client can scale its mobile application elastically and automatically with a cost-optimized solution. New instances start immediately in accordance with traffic spikes on weekends. While DynamoDB is scaling up and down accordingly.
As a result of using AWS, the company is better prepared to expand its business and build out new product offerings. New products and services can be offered without traffic spike and overall system quality concerns. AWS also allows to adopt a continuous delivery model on their software deployment process and innovate faster.
Check out our AWS service capabilities and solutions:
https://aws.amazon.com/partners/find/partnerdetails/?n=commencis&id=001E000000U0VLBIA3
We help leading brands to grow and scale in digital…
53 
53 claps
53 
We help leading brands to grow and scale in digital, powered by our big data, analytics and cloud products.
Written by
Building purposeful digital transformation journeys and products for amazing brands.
We help leading brands to grow and scale in digital, powered by our big data, analytics and cloud products.
"
https://faun.pub/4-cloud-computing-jobs-to-check-out-if-you-want-to-break-into-the-space-1963df24ef20?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Lately, we’ve been thinking about cloud computing jobs and titles we’ve been seeing in the space. One of the great things about talking with ParkMyCloud users is that we get to talk to a variety of different people. That’s right — even though we’re laser-focused on cloud cost optimization, it turns out that can matter to a lot of different people in an organization. (And no wonder, given the size of wasted spend — that hits people’s’ buttons).
You know the cloud computing market is growing. You know that means new employment opportunities, and new niches in which to make yourself valuable. So what cloud computing jobs should you check out?
Cloud Operations. Cloud operations engineers, managers, and similar are the people we speak with most often at ParkMyCloud, and they are typically the cloud infrastructure experts in the organization. This is a great opportunity for sysadmins looking to work in newer technology.
If you’re interested in cloud operations, definitely work on certifications from AWS, Azure, Google, or your cloud provider of choice. Attend meetups and subscribe to industry blogs — the cloud providers innovate at a rapid pace, and the better you keep up with their products and solutions, the more competitive you’ll be.
See also: DevOps, cloud infrastructure, cloud architecture, and IT Operations.
Customer Success, cloud support, or other customer-facing job at a managed service provider (MSP). As we recently discussed, there’s a growing market of small IT providers focusing on hybrid cloud in the managed services space. The opportunities at MSPs aren’t limited to customer success, of course — just in the past week we’ve talked to people with the following titles at MSPs: Cloud Analyst, Cloud Engineer, Cloud Champion/Cloud Optimization Engineer, CTO, and Engagement Architect.
Also consider: pre-sales engineering at one of the many software providers in the cloud space.
Site Reliability Engineer. This title, invented by Google, is used for operations specialists who focus on keeping the lights on and the sites running. Job descriptions in this discipline tend to focus on people and processes rather than around the specific infrastructure or tools.
Cloud Financial Analyst. See also: cloud cost analyst, cloud financial administrator, IT billing analyst, and similar. Cloud computing jobs aren’t just for technical people — there is a growing field that allows experts to adapt financial skills to this hot market. As mentioned above, since the cloud cost problem is only going to grow, IT organizations need professionals in financial roles focused on cloud. Certifications from cloud providers can be a great way to stand out.
As the cloud market continues to grow and change, there will be new cloud computing job opportunities — and it can be difficult to predict what’s coming next. Just a few years ago, it was rare to meet someone running an entire cloud enablement team, but that’s becoming the norm at larger, tech-forward organizations. We also see a trend of companies narrowing in “DevOps” roles to have professionals focused on “CloudOps” specifically — as well as variations such as DevFinOps. And although some people hear “automation” and worry that their jobs will disappear, there will always be a need for someone to keep the automation engines running and optimized. We’ll be here.
Originally published at www.parkmycloud.com on September 21, 2018
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
5 
5 claps
5 
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/the-3-must-ask-questions-when-using-google-cloud-iam-7ffa44c8f2c6?source=search_post---------388,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 9, 2020·4 min read
Google Cloud IAM (Identity and Access Management) is the core component of Google Cloud that keeps you secure. By adopting the “principle of least privilege” methodology, you can work towards having your infrastructure be only accessible by those who need it. As your organization grows in size, the idea of keeping your IAM permissions correct can seem daunting, so here’s a checklist of what you should think about prior to changing permissions. This can also help you as you continuously enforce your access management.
Narrowing down the person or thing who will be accessing resources is the first step in granting IAM permissions. This can be one of several options, including:
Our biggest recommendation for this step is to keep this limited to as few identities as possible. While you may need to assign permissions to a larger group, it’s much safer to start with a smaller subset and add permissions as necessary over time. Consider whether this is an automated task or a real person using the access as well, since service accounts with distinct uses makes it easier to track and limit those accounts.
Google Cloud permissions often correspond directly with a specific Google Cloud REST API method. These permissions are named based on the GCP service, the specific resource, and the verb that is being allowed. For example, ParkMyCloud requires a permission named “compute.instances.start” in order to issue a start command to Google Compute Engine instances.
These permissions are not granted directly, but instead are included in a role that gets assigned to the identity you’ve chosen. There are three different types of roles:
Our recommendation for this step is to use a predefined role where possible, but don’t hesitate to use a custom role. The ParkMyCloud setup has a custom role that specifically lists the exact REST API commands that are used by the system. This ensures that there are no possible ways for our platform to do anything that you don’t intend. When following the “least privilege” methodology, you will find that custom roles are often used.
Once you’ve decided on the identity and the permissions, you’ll need to assign those permissions to a resource using a Cloud IAM policy. A resource can be very granular or very broad, including things like:
Each predefined role has a “lowest level” of resource that can be set. For example, the “App Engine Admin” role must be set at the project level, but the “Compute Load Balancer Admin” can be set at the compute instance level. You can always go higher up the resource hierarchy than the minimum. In the hierarchy, you have individual service resources, which all belong to a project, which can either be a part of a folder (in an organization) or directly a part of the organization.
Our recommendation, as with the Identity question, is to limit this to as few resources as possible. In practice, this might mean making a separate project to group together resources so you can assign a project-level role to an identity. Alternatively, you can just select a few resources within a project, or even an individual resource if possible.
These three questions provide the crucial decisions that you must make regarding Google Cloud IAM assignments. By thinking through these items, you can ensure that security is higher and risks are lower. For an example of how ParkMyCloud recommends a custom role assigned to a new service account in order to schedule and resize your VMs and databases, check out the documentation for ParkMyCloud GCP access, and sign up for a free trial today to get it connected securely to your environment.
Originally published at www.parkmycloud.com on June 4, 2020.
CEO of ParkMyCloud
15 
15 
15 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intel-tech/how-intel-products-address-cloud-service-providers-challenges-259bb91cc0be?source=search_post---------389,"There are currently no responses for this story.
Be the first to respond.
Author(s): Jacek Wysoczynski, Intel Senior Manager of Product Planning and Andrew Ruffin, Intel Strategy & Business Development Manager
In part 1 of our “Cloud Service Providers Challenges and Solutions” series we discussed the various challenges Cloud Service Providers (CSPs) and Data Center Operators are faced with day to day from costs to technically demanding applications to repairs and downtime. Now we will see how just a few of Intel’s products can help alleviate some of these issues.
Intel’s® new Optane™ solid state drives (SSDs) allow data center operators to gain the benefits of nonvolatile DIMMs (NVDIMMs) speed with the ease of upgrade and repairs of NAND SSDs, all in one system. And, as network speeds increase, Intel’s® Optane™ SSDs improve the situation further, by reducing overall operating costs and simplifying data center maintenance.
To overcome the limitations of both NVDIMM and NAND SSDs, Intel® created Optane™ SSDs. With Intel® Optane™ SSDs, data center operators can bypass the capacity limitations of NVDIMM, but at higher speeds than NAND SSDs(1). They will also see their operating costs lowered, and as network speeds increase, these savings will increase.
Intel® Optane™ SSDs have faster write speeds and lower latency compared to NAND SSDs. Intel® Optane™ SSDs writes data more similarly to RAM than to NAND SSDs, in that they do not require data to be written to 4K pages. By writing the data directly, instead of having to write changes to buffers, Intel® Optane™ SSDs can write data at speeds closer to those of RAM and NVDIMM compared to NAND SSDs.
Intel® Optane™ SSDs are also non-volatile, like SSDs, and unlike RAM, meaning it can be used as a power loss imminent (PLI) buffer. While NVDIMM can also be used as a PLI buffer, it takes the place of RAM on a server; Intel® Optane™ SSDs replace NAND SSDs instead. This allows servers to continue using RAM for processes running end user applications and move PLI writer buffer functionality to Intel® Optane™ SSDs.
Since Intel® Optane™ SSDs share the same form-factor and interface of NAND SSDs, they are also hot swappable like SSDs. They can be installed while the server is still on and in the rack, making upgrades and repairs much easier to perform than with NVDIMM. By using Intel® Optane™ SSDs instead of NVDIMM for PLI buffers, busy data centers have easier time performing necessary maintenance without causing costly interference to daily operations.
Since Intel® Optane™ SSDs are faster than NAND SSDs, fewer are needed to handle the increasing amount of incoming data on high-speed networks. For example, in order to handle a burst of up to 90% of available bandwidth on a 25 Gigabit Ethernet (GbE) network, a data center operator would require four PCIe Gen3 NAND SSDs (such as the 3200GB Intel® DC P4610 SSD) to handle incoming traffic as a PLI buffer. Alternatively, two 375GB Intel® Optane™ P4800X SSDs could handle the same workload.
As network and PCIe speeds increase, the gap in performance between NAND SSDs and Intel® Optane™ SSDs widens. On a 100GbE network, a data center operator would require 13 PCIe Gen3 NAND SSDs (such as the 3200GB Intel® D7-P5600 SSD) as a PLI buffer, or just three 400GB Intel® Optane™ P5800X SSDs.(2)
This reduction in number of devices immediately reduces purchase costs, but also results in cost efficiency in terms of system and resources required. For example, with fewer devices needed to handle incoming write data, more physical server space is available for bulk storage. This space saving could be used to reduce costs by reducing the number of servers needed to compute and store the same amount of information. If the number of servers is reduced, this would also reduce energy costs, as there are both fewer devices to power, and a lower amount of energy needed to cool the remaining servers. With fewer servers needed, data centers can be smaller, or expansions to data center size would not require additional space. Finally, fewer devices reduce workloads on technicians and IT staff, which can help reduce labor costs.
Intel® Optane™ SSDs also have significantly longer lifespans than NAND SSDs. The Drive Writes Per Day (DWPD) of NAND SSDs varies, but a mid-range NAND SSD is rated for three DWPD over five years(3). The Intel® Optane™ SSD DC P5800X Series, however, is rated for up to 100 DWPD over the same five years(4). The higher DWPD of an Intel® Optane™ SSD is a great fit for a PLI buffer, as data center operators can trust that the drive will not wear out during its lifetime. This also means that in servers with lower DWPD than listed, the Intel® Optane™ SSD can last far longer than a similarly sized NAND SSD, reducing operation and upkeep costs.
CSPs have to rise to meet the bigger demands being placed on them daily. The need to increase throughput in data centers while reducing costs means that CSPs must take every advantage available to them. With Intel® Optane™ SSDs, the demands of a data center can be met today with ease, and as the system grows, Intel® Optane™ SSDs grow with it.
(1) https://www.seagate.com/tech-insights/ssd-over-provisioning-benefits-master-ti/
(2)https://www.intel.com/content/www/us/en/products/docs/storage/distributed-storage-trends-white-paper.html
(3) https://www.intel.com/content/www/us/en/architecture-and-technology/optane-technology/delivering-new-levels-of-endurance-article-brief.html
(4) https://ark.intel.com/content/www/us/en/ark/products/201859/intel-optane-ssd-dc-p5800x-series-1-6tb-2-5in-pcie-x4-3d-xpoint.html
Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.
© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.
Intel‘s leading Technology Innovation blog on Medium. Follow for the latest tech innovation stories.
102 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
102 claps
102 
Written by
Intel news, views & events about global tech innovation.
The Intel Tech blog is designed to share the latest information on Intel’s technology innovations, performance, and technical leadership accomplishments.
Written by
Intel news, views & events about global tech innovation.
The Intel Tech blog is designed to share the latest information on Intel’s technology innovations, performance, and technical leadership accomplishments.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-to-manage-hybrid-multi-cloud-environments-with-google-cloud-composer-96e4352f242?source=search_post---------390,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Nov 6, 2019·3 min read
As we continue to evaluate ways to automate various aspects of software development, today we’ll take a look at Google Cloud Composer. This is a fully managed workflow orchestration service built on Apache Airflow that makes workflow management and creation simple and consistent.
The evolution of hybrid and multi-cloud environments continue to grow as enterprises want to take advantage of the cloud’s scalability, flexibility, and global reach. Of the three major providers, Google Cloud has been the most open to supporting this multi-cloud reality. For example, earlier this year, Google launched Anthos, a new managed service offering for hybrid and multi-cloud environments to give enterprises operational consistency by running quickly on any existing hardware, leverage open APIs and give developers the freedom to modernize. But, implementing the management of these environments can be either an invaluable proposition for your company or one to completely challenge your infrastructure instead — which brings us to Google’s solution, Cloud Composer.
With Cloud Composer, you can monitor, schedule and manage workflows across your hybrid and multi-cloud environment. Here is how:
Cloud Composer is ideal for hybrid and multi-cloud management because it’s built on Apache Airflow and operated with the Python programming language. Using open-source technology and the “no lock-in” approach and portability gives users the flexibility to create and deploy workflows seamlessly across clouds for a unified data environment.
Setting up your environment is quick and simple. Pipelines created with Cloud Composer will be configured as DAGs with easy integration for any required Python libraries, giving users of almost any level the ability to create and schedule their own workflows. With the built-in one-click deployment, you get instant and easy access to a range of connectors and graphical representations that show your workflow in action.
However, costs can be a drawback to making the most of your cloud environment when using Cloud Composer. Landing on specific costs for Cloud Composer can be hard to calculate, as Google measures the resources your deployments use and add the total cost of your Apache Airflow deployments onto your wider GCP bill.
Pricing for Cloud Composer is based on the size of a Cloud Composer environment and the duration the environment runs, so you pay for what you use, as measured by vCPU/hour, GB/month, and GB transferred/month. Google offers multiple pricing units for Cloud Composer because it uses several GCP products as building blocks. You can also use the Google Cloud Platform pricing calculator to estimate the cost of using Cloud Composer.
So, should you use Google Cloud Composer? Cloud Composer environments are meant to be long-running compute resources that are always online so that you can schedule repeating workflows whenever necessary. Unfortunately, since you can’t turn on and off a Cloud Composer environment; you can only create or destroy, it may not be right for every environment and could cost more than the advantages may be worth.
Originally published at www.parkmycloud.com on October 29, 2019.
CEO of ParkMyCloud
See all (317)
7 
7 claps
7 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/overprovisioning-always-on-resources-lead-to-26-6-billion-in-public-cloud-waste-expected-in-2021-da888ea68f74?source=search_post---------391,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 9, 2021·3 min read
Overprovisioning and leaving cloud resources on are two enormous sources of wasted spend.
Wasted spend drags down IT budgets — of particular importance as we enter 2021. The Flexera 2021 State of Tech Spend report found that the biggest change in key IT initiatives from 2020 to 2021 was in cost savings, with the percent of respondents ranking cost savings as a top initiative tripling year-over-year.
It’s important that this is being recognized. Based on data collected by Gartner, we estimate that wasted spend will exceed $26.6 billion this year.
Gartner estimates a total market spend of $304 billion on public cloud services end-user spending in 2021, as broken out in the table below. Their estimate for the proportion of that spent on Infrastructure as a Service (IaaS) is $65.3 billion. While wasted spend can be found in any area of cloud spend, customers tend to see the largest amount in these two areas, as well as finding it easiest to identify.
Cloud resources can be considered “idle” when they are running while not being used. For example, when development servers are left running overnight and on weekends when they’re not needed. Since compute resources are paid for by the minute or second, that’s a large portion of the week they’re being paid for but not used (and yes, this applies even if you have reservations.)
Our data shows that about 44% of compute spend is on non-production resources. If we estimate that non-production resources are only needed during a 40-hour work week, the other 128 hours (76%), the resources are sitting idle.
Applying that to the Gartner IaaS number, we estimate that up to $14.5 billion will be wasted on idle resources this year.
Overprovisioning occurs when a larger resource size is selected than is actually needed. There is a mindset of safety behind this, as of course, no one wants their applications to be under-resourced.
But the overprovisioning occurring is far beyond what is necessary, given the elasticity of the cloud. About 40% of instances are sized at least one size larger than needed for their workloads. The cost can be cut in half by reducing an instance by one size, while downsizing by two sizes saves 75%.
Many of our customers show a large percentage of their resources are oversized, but bringing this to a conservative estimate of 40% of resources oversized by one size, giving us a savings per resource of 50%, we estimate that up to $8.7 billion is wasted due to overprovisioning.
Another significant source of waste is orphaned volumes and snapshots. These are resources that have been detached from the infrastructure they were created to support, such as a volume detached from an instance or a snapshot with no volume attachment.
Our customers spend approximately 15% of their bills on storage, and we found that about 35% of that spend is on unattached volumes and snapshots. Applying that to the Gartner spending numbers, we estimate that up to $3.4 billion could be wasted this year on orphaned volumes and snapshots.
Altogether, this gives us an estimate of $26.6 billion to be wasted on unused cloud resources in 2021. This waste estimate is just based on the three prominent sources of cloud waste. It does not include wasted spend on Platform as a Service (PaaS), which makes up $55 billion in cloud spend according to Gartner’s estimates, nor from SaaS, unused reservation commitments, inefficient containerization, and other areas of the bill.
Attacking the three problem areas above is a great area to start for nearly all public cloud users. Here at ParkMyCloud, we’re on a mission to do just that. See how and try it out today, to do your part in reducing wasted cloud spend.
Originally published at www.parkmycloud.com on January 21, 2021.
CEO of ParkMyCloud
See all (317)
16 
16 claps
16 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-much-should-enterprises-worry-about-vendor-lock-in-in-public-cloud-5029bf40fffa?source=search_post---------392,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Sep 16, 2019·5 min read
One of the key drivers to a multi-cloud strategy is the fear of vendor lock-in. “Vendor lock-in” means that a customer is dependent on a particular vendor for products and services, and unable to use another vendor without substantial switching costs or operational impact. The vendor lock-in problem in cloud computing is the situation where customers are dependent (i.e. locked-in) on a single cloud service provider (CSP) technology implementation and cannot easily move to a different vendor without substantial costs or technical incompatibilities.
Before the cloud, IT was running in dedicated on-premises environments, requiring long-term capital investments and an array of software license commitments and never ending hardware refresh contracts. Based on that experience, it is understandable that a customer would be concerned about lock-in. Many large IT vendors like Oracle, IBM, HP, and Cisco would “lock” customers into 3–5–10 year Enterprise License Agreements (ELAs) or All You Can Eat (AYCE) hardware and software license agreements, promising huge discounts and greater buying power — but only for their products, of course. I used to sell these multi-year contracts. There is a common ground for sure, as the customer was locked-in to the vendor for years. But that was then and this is now. Is vendor lock-in really a concern for public cloud users?
Isn’t the point of cloud to provide organizations the agility to speed innovation and save costs by quickly scaling their infrastructure up and down? I mean, we get it — your servers, data, networking, user management, and much more are in the hands of one company, so the dependence on your CSP is huge. And if something goes wrong, it can be very detrimental to your business — your IT is in the cloud, and if you’re like us, your entire business is developed, built and run in the cloud. Most likely, some or all of your organization’s IT infrastructure where you are developing, building and running applications to power your business and generate revenue, is now off-premise, in the cloud. But although “lock-in” sounds scary, you are not stuck in the same way that you were with traditional hardware and software purchases.
Let’s talk about the realities of today’s public cloud-based world. Here are a couple of reasons why vendor lock-in isn’t as widespread a problem as you might think:
Now the cloud is not without risk, and when we talk to customers the primary vendor lock-in concerns we hear are related to moving to another cloud service provider IF something goes awry. You hope that this never has to happen, but it’s a possibility. The general risks include:
To minimize the risk of vendor lock-in, your applications should be built or migrated to be as flexible and loosely coupled as possible. Cloud application components should be loosely linked with the application components that interact with them. And, adopt a multi-cloud strategy.
Many companies are familiar with vendor lock-in from dealing with traditional enterprise companies mentioned above — you begin to use a service only to realize too late what the terms of that relationship brings with it in regards to cost and additional features and services. The same is not entirely true with selecting a cloud service provider like AWS, Azure, or Google Cloud. It’s difficult to avoid some form of dependence as you use the new capabilities and native tools of a given cloud platform. In our own case, we use AWS and we can’t just wake up tomorrow and use Azure or Google Cloud. However, it’s quite possible to set your business up to maintain enough freedom and mitigate risk, so you can feel good about your flexibility.
So how much should enterprises worry about vendor lock-in in public cloud? IMHO: they shouldn’t.
Originally published at www.parkmycloud.com on September 3, 2019.
CEO of ParkMyCloud
11 
11 
11 
CEO of ParkMyCloud
"
https://medium.com/@deepanshugahlaut/let-your-business-soar-high-with-cloud-computing-ad4ac9456613?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Deepanshu Gahlaut
Jun 6, 2015·4 min read
Implementation of cloud computing is gaining impetus day by day and changing the all aspects of businesses, whether large or small. According to Gartner’s top 10 strategic technology trends,
Cloud computing will affect the business decisions through 2015 and in the next three years.
Cloud computing has emerged as the most reliable solution for protecting the data and files from theft, intrusion, virus and phishing attacks, disaster that might occur in the future, and infected data drives. Moreover, this technology renders the businesses free from the worries of accessibility, maintaining regular data backups, hardware maintenance, and daily software updates.
Simply put, Cloud computing is the use of internet to compute i.e. store, manage and process data and getting maximum benefits from the distantly placed specialized servers and related services. The term “Cloud” means a location on Internet where you put all your data files, applications etc.
Cloud computing services can be grouped into the following three categories: PaaS, SaaS and IaaS
The demand of Cloud computing is increasing day by day for both personal as well as business purposes. Following reasons are responsible for the tremendous increase in the demand of this technology:
2. Global Accessibility: Cloud computing enables its users to access the data and information from anywhere on the globe and at any point of time that increases collaboration with your team members. You can access your data anywhere — home, office or even on the move. Thus you no longer remain confined to your office or personal computer. You just need and Internet connectivity to access the data from the Cloud.
3. Cost Reduction: Cloud computing lets you save the CAPEX (Capital Expenditure) and OPEX (Operating Expenditure). With this technology, you save a lot of money as you do not have to spend on purchasing infrastructure hardware and software, and also on managing the team of IT experts. Due to the savings which cloud computing yields in the long run, more and more businesses are shifting towards this technology in order to strengthen their economy.
4. Technical Efficiency: Maintaining the data and servers on-site, is very tough as compared to maintaining the same on the cloud. Cloud computing eases the task of documents and hardware management because it simplifies the application set-ups, provision of virtualized infrastructure and resources. For example, in case of managed VPS, your service provider is solely responsible for server support, maintenance and upgrades.
5. Disaster Recovery: Your crucial data is not affected by the physical conditions of an organization, man made mistakes or natural disaster because the data servers are based in different geographical locations. Cloud computing eliminates chances of expensive service stoppage and let you resume your business operations as they were before the disaster.
Cloud computing has a lot of benefits to offer. Those mentioned above are only a few important ones. In order to enjoy the benefits of this technology, you actually need to adopt this technology to save money and to make your business more flexible.
I write on SEO, content marketing, accounting, latest technologies, and social media.
See all (983)
5 
3
5 claps
5 
3
I write on SEO, content marketing, accounting, latest technologies, and social media.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-new-services-2019-whats-in-preview-a25a1752a746?source=search_post---------394,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 8, 2019·6 min read
As the cloud giant, Amazon Web Services (AWS) is constantly innovating — it’s no surprise that at any given time, there’s a list of AWS new services that will soon be released. We’ve put together a list of AWS new services that are currently in “preview” and are expected to be released later this year. Many of these were announced at AWS re:Invent 2018.
Interested in a list of all the services AWS offers? We actually just put that together too. See all 170 services here: AWS Services List.
This list is broken down by category, including Analytics, Blockchain, Compute, Database, Internet of Things, Machine Learning, and Security. While all are in preview right now, their release dates may vary.
Looking at these new services as a sample group, there are a few trends in the emerging technology. To no one’s surprise, there are a few blockchain and IoT announcements, both of which continue to be trending topics. Machine learning products also make up much of the list above — interestingly, with several releases of technology used to power Amazon.com.
Have you tried any of these new services? Let us know what you think of them!
Originally published at www.parkmycloud.com on April 23, 2019.
CEO of ParkMyCloud
60 
60 
60 
CEO of ParkMyCloud
"
https://faun.pub/how-to-use-9-cloud-devops-best-practices-for-cost-control-4cc160cbeef4?source=search_post---------395,"There are currently no responses for this story.
Be the first to respond.
Any organization with a functioning cloud DevOps practice will have some common core tenants. While those tenants are frequently applied to things like code delivery and security, a company that fails to apply those tenants to cost control are destined to have a runaway cloud bill (or at least have a series of upcoming meetings with the CFO). Here are some of those tenants, and how they apply to cost control:
One common excuse for wasted cloud spend is “well that other group has cloud waste too!” By aggressively targeting and eliminating cloud waste, you can set the tone for cost control within your team, which will spread throughout the rest of the organization. This also helps to get everyone thinking about the business, even if it doesn’t seem like wasting a few bucks here or there really matters (hint: it does).
By tearing down silos and sharing ideas and services, cost control can be a normal part of DevOps cloud practice instead of a forced decree that no one wants to take part in. Writing a script that is more generally applicable, or finding a tool that others can be invited to will cause others to save money and join in. You may also get ideas from others that you never thought of, without having to waste time or replicate work.
Having cost control as a central priority within your team means that you end up building it into your processes and software as you go. Attempting to control costs after-the-fact can be tough and can cause rewrites or rolling back instead of pressing forward. Also, tacked-on cost control is often less effective and saves less money than starting with it.
Integrating ideas and code from multiple teams with multiple codebases and processes can be daunting, which is why continually integrating as new commits happen is such a big step forward. Along the same lines, continually controlling costs during the integration phase means you can optimize your cloud spend by sharing resources, slimming down those resources, and shutting down resources until they are needed by the integration.
Continuous testing of software helps find bugs quickly and while developers are still working on those systems. Cost control during the testing phase can take multiple forms, including controlling the costs of those test servers, or doing continuous testing of the cost models and cost reduction strategies. New scripts and tools that are being used for cost control can also be tested during this phase.
Monitoring and reporting, like cost control, are often haphazardly tacked on to a software project instead of being a core component. For a lot of organizations, this means that costs aren’t actively being monitored and reported, which is what causes yelling from the Finance team when that cloud bill comes. By making everyone aware of how costs are trending and noting when huge spikes occur, you can keep those bills in check and help save yourself from those dreaded finance meetings.
Cloud cost control can contribute to better security practices. For example, shutting down Virtual Machines when they aren’t in use decreases the number of entry points for would-be hackers, and helps mitigate various attack strategies. Reducing your total number of virtual machines also makes it easier for your security teams to harden and monitor the machines that exist.
Auto-scaling resources are usually implemented by making services scale up automatically, while the “scaling down” part is an afterthought. It can be admittedly tricky to drain existing users and processes from under-utilized resources, but having lots of systems with low load is the leading cause of cloud waste. Additionally, having different scale patterns based on time of day, day of the week, and business need can be implemented, but requires thought and effort into this type of cost control.
Deploying your completed code to production can be exciting and terrifying at the same time. One factor that you need to consider is the size and cost of those production resources. Cost savings for those resources is usually different from the dev/test/QA resources, as they typically need to be on 24/7 and can’t have high latency or long spin-up times. However, there are some cost control measures, like pre-paying for instances or having accurate usage patterns for your elastic environments, that should be considered by your production teams.
As you can see, there are a lot of paths to lowering your cloud bill by using some common cloud DevOps tenants. By working these ideas into your teams and weaving it throughout your processes, you can save money and help lead others to do the same. Controlling these costs can lead to fewer headaches, more time, and more money for future projects, which is what we’re all aiming to achieve with DevOps.
Originally published at www.parkmycloud.com on January 24, 2019.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
12 
12 claps
12 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/@jaychapel/8-ways-to-improve-cloud-automation-through-tagging-9f6d1717ff91?source=search_post---------396,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 4, 2019·3 min read
Since the beginning of public cloud, users have been attempting to improve cloud automation. This can be driven by laziness, scale, organizational mandate, or some combination of those. Since the rise of DevOps practices and principles, this “automate everything” approach has become even more popular, as it’s one of the main pillars of DevOps. One of the ways you can help sort, filter, and automate your cloud environment is to utilize tags on your cloud resources.
In the cloud infrastructure world, tags are labels or identifiers that are attached to your instances. This is a way for you to provide custom metadata to accompany the existing metadata, such as instance family and size, region, VPC, IP information, and more. Tags are created as key/value pairs, although the value is optional if you just want to use the key. For instance, your key could be “Department” with a value of “Finance”, or you could have a key of just “Finance”.
There are 4 general tag categories, as laid out in the best practices from AWS:
In general, more tags are better, even if you aren’t actively using those tags just yet. Planning ahead for ways you might search through or group instances and resources can help save headaches down the line. You should also ensure that you standardize your tags by being consistent with the capitalization/spelling and limiting the scope of both the keys and the values for those keys. Using management and provisioning tools like Terraform or Ansible can automate and maintain your tagging standards.
Once you’ve got your tagging system implemented and your resources labeled properly, you can really dive into your cloud automation strategy. Many different automation tools can read these tags and utilize them, but here are a few ideas to help make your life better:
As your cloud use grows, implementing cloud automation will be a crucial piece of your infrastructure management. Utilizing tags not only helps with human sorting and searching, but also with automated tasks and scripts. If you’re not already tagging your systems, having a strategy on the tagging and the automation can save you both time and money.
Originally published at www.parkmycloud.com on August 14, 2018.
CEO of ParkMyCloud
9 
9 
9 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
