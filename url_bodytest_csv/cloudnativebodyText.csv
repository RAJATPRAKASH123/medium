story_url,bodyText
https://medium.com/@copyconstruct/monitoring-in-the-time-of-cloud-native-c87c7a5bfa3e?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Cindy Sridharan
Oct 4, 2017·41 min read
Note — Huge thanks to Jamie Wilkinson and Julius Volz, Google SREs present and past, for reading a draft of this and giving me invaluable suggestions. All mistakes and opinions however, are solely mine. This was, in essence, my Velocity 2017 talk.
The infrastructure space is in the midst of a paradigm-shifting change. The way organizations — from the smallest of startups to established companies — build and operate systems has evolved.
Containers, Kubernetes, microservices, service meshes, immutable infrastructure and serverless are all incredibly promising ideas which fundamentally change the way we run software. As more and more organizations move toward these paradigms, the systems we build have become more distributed and in the case of containerization, more ephemeral.
While we’re still at the stage of early adoption, with the failure modes of these new paradigms still being very nebulous and not widely advertised, these tools are only going to get increasingly better with time. Soon enough, if not already, we’ll be at that point where the network and underlying hardware failures have been robustly abstracted away from us, leaving us with the sole responsibility to ensure our application is good enough to piggy bank on top of the latest and greatest in networking and scheduling abstractions.
No amount of GIFEE (Google Infrastructure for Everyone Else) or industrial-grade service mesh is going to fix the software we write. Better resilience and failure-tolerant paradigms from off-the-shelf components now means that — assuming said off-the-shelf components have been configured correctly — most failures will arise from the application layer or from the complex interactions between different applications. Trusting Kubernetes and friends to do their job makes it more important than ever for us to focus on the vagaries of the performance characteristics of our application and business logic. We’re at a time when it has never been easier for application developers to focus on just making their service more robust and trust that if they do so, the open source software they are building on top of will pay the concomitant dividends.
In order to manoeuvre this brave new world successfully, gaining visibility into our services and infrastructure becomes more important than ever before to successfully understand, operate, maintain and evolve these.
Fortunately for us, a new crop of tools have emerged to help us rise to this challenge. While one might argue that these tools suffer from the selfsame problem they assist us in solving — viz, the tools themselves are every bit as nascent and emergent as the infrastructural paradigms they help us gain visibility into — strong community interest, community driven development and an open governance model do a lot to promote the sustainability and development of these tools.
In addition to a surge in open source tooling, commercial tooling modeled along the lines of Google, Facebook and Twitter’s internal tools have emerged to address the real need felt by the the early adopters of cloud native paradigms. Given how far both categories of tools have evolved in recent years, we now have a veritable smorgasbord of choices.
A plethora of tools at our disposal to adopt or buy, however, presents an entirely different problem — one of decision making.
How do we choose the best tool for our needs? How do we even begin to tell the difference between these tools when several of these tools more or less do the same thing? We might’ve heard that monitoring is dead and observability is all the rage now. Does that mean we stop “monitoring”? We hear a lot about the “three pillars of observability”, but what even is observability and why should we care? What really is the difference between logs and metrics, except where we send them to? We might’ve heard a lot of tracing — but how useful can tracing really be if it’s only just a log with some context? Is a metric just a log or trace that occurs too frequently for a backend system to store? Do we really need all three of them?
I’ve said this before in one of my posts, but it bears reiterating. It’s tempting, especially when enamored by a new piece of technology that promises the moon, to retrofit our problem space with the solution space of said technology, however minimal the intersection. Before buying or building a tool, it becomes important to evaluate the maximum utility it can provide for the unique set of engineering challenges specific teams face. In particular when it comes down to choosing something as critical as a monitoring stack, in order to be able to make better technological choices, it becomes absolutely necessary for us to first fully understand:
— the strengths and weaknesses of each category of tools — the problems they solve— the tradeoffs they make— their ease of adoption/integration into an existing infrastructure
Most importantly, it becomes important to make sure that we are solving the problems at hand, not using the solutions these new crop of tools provide. Starting over from scratch isn’t a luxury most of us enjoy and the most challenging part about modernizing one’s monitoring stack is iteratively evolving it. Iterative evolution — of refactoring, if you will — of one’s monitoring stack in turn presents a large number of challenges from both a technical as well as an organizational standpoint.
The goal of this post is to shed light on these technologies and primarily frame the discussion in the context of the problems that we will be solving and the tradeoffs we might be making. It’s important for me to state upfront that the main purpose of this post isn’t to provide catch-all answers or solutions or demos of specific tools. What I hope to achieve with this post is leave you with some ideas and hopefully some questions that you can try to answer as you design systems with the goal of bringing better visibility to them.
— What even is observability and how is it different from Monitoring? — An overview of the “three pillars of modern observability”: logging, metrics collection, and request tracing  — The pros and cons of each in terms of resource utilization, ease of use, ease of operation, and cost effectiveness — An honest look at the challenges involved in scaling all the three when used in conjunction  — What to monitor and how in a modern cloud native environment; what is better-suited to be aggregated as metrics versus being logged; how and when to use the data from all the three sources to derive actionable alerts and insightful analysis  — When it makes sense to augment the three aforementioned tools with additional tools
This post is titled Monitoring in the time of Cloud Native. I’ve been asked why I chose to call it monitoring and not observability. I was expecting more snark about the buzzword that’s actually in the title — Cloud Native — than the one conspicuous by its absence. I chose not to call it observability for this very same reason — two buzzwords was one too many for my liking.
In all seriousness, I do believe there’s a difference between the two. The reason I believe so is because the nature of failure is changing, the way our systems behave (or misbehave) as a whole is changing, the requirements these systems need to meet are changing, the guarantees these systems need to provide are changing. In order to rise to these challenges successfully, it becomes necessary to not just change the way we build and operate software, but also gain better visibility into our services, which in turn gives us a shorter feedback loop about the performance of our services in production, which in turn enables us to build better services. In order to craft this virtuous cycle, it becomes important to understand what’s observability and how it differs from Monitoring.
When I type “monitoring” into a search engine, the first two results that come up are the following:
— observe and check the progress or quality of (something) over a period of time; keep under systematic review.
— maintain regular surveillance over.
Monitoring, to me, connotes something that is inherently both failure as well as human centric. Let’s talk a bit more about this because this forms the bedrock of this post.
In the past, we might’ve first tested our application. This might’ve been followed by a QA cycle. Then we might’ve released our code, followed by “monitoring” it. Followed by leaving a lot to chance.
To be fair, I don’t believe this was how everyone has been managing software lifecycle, but it makes for a good caricature of what’s often considered “the old way”.
We “monitored” something because we expected something to behave a certain way. What’s worse, we expected something to fail in a very specific manner and wanted to keep tabs on this specific failure. An “explicit, predictable failure” centric approach to monitoring becomes a problem when the number of failure modes both increases and failure itself becomes more implicit.
As we adopt increasingly complex architectures, the number of “things that can go wrong” exponentially increases. We often hear that we live in an era when failure is the norm. The SRE book states that:
It turns out that past a certain point, however, increasing reliability is worse for a service (and its users) rather than better! Extreme reliability comes at a cost: maximizing stability limits how fast new features can be developed and how quickly products can be delivered to users, and dramatically increases their cost, which in turn reduces the number of features a team can afford to offer.
Our goal is to explicitly align the risk taken by a given service with the risk the business is willing to bear. We strive to make a service reliable enough, but no more reliable than it needs to be.
Opting in to the model of embracing failure entails designing our services to behave gracefully in the face of failure. In other words, this means turning hard, explicit failure modes into partial, implicit and soft failure modes. Failure modes that could be papered over with graceful degradation mechanisms like retries, timeouts, circuit breaking and rate limiting. Failure modes that can be tolerated owing to relaxed consistency guarantees with mechanisms like eventual consistency or aggressive multi-tiered caching. Failure modes that can be even triggered deliberately with load shedding in the event of increased load that has the potential to take down our service entirely, thereby operating in a degraded state.
But all of this comes at the cost of increased overall complexity and the buyer’s remorse often acutely experienced is the loss of ability to easily reason about systems.
Which brings me to the second characteristic of “monitoring” — in that it’s human centric. The reason we chose to “monitor” something was because we knew or suspected something could go wrong, and that when it did go wrong there were consequences. Real consequences. High severity consequences that needed to be remedied as soon as possible. Consequences that needed human intervention.
I’m not someone who believes that automating everything is a panacea, but the advent of platforms like Kubernetes means that several of the problems that human and failure centric monitoring tools of yore helped “monitor” are already solved. Health-checking, load balancing and taking failed services out of rotation and so forth are features these platforms provide for free. That’s their primary value prop.
With more of the traditional monitoring responsibilities being automated away, “monitoring” has become — or will soon be — less human centric. While none of these platforms will truly make a service impregnable to failure, if used correctly, they can help reduce the number of hard failures, leaving us as engineers to contend with the subtle, nebulous, unpredictable behaviors our system can exhibit. The sort of failures that are far less catastrophic but ever more numerous than before.
Which then begs the question — how do we design monitoring for such systems?
It really isn’t even so much about how to design monitoring for these systems, than how to design the systems themselves.
I’d argue that “monitoring” should still be both hard failure as well as human centric, even in this brave new world. The goal of “monitoring” hasn’t changed, even if the scope has shrunk drastically, and the challenge that now lies ahead of us is identifying and minimizing the bits of “monitoring” that still remain human centric. We need to design our systems such that only a small sliver of the overall failure domain is now of the hard, urgently human actionable sort.
But there’s a paradox. Minimizing the number of “hard, predictable” failure modes doesn’t in any way mean that the system itself as a whole is any simpler. In other words, even as infrastructure management becomes more automated and requiring less human elbow grease, application lifecycle management is becoming harder. As the number of hard failure modes shrink at the expense of a drastic rise in implicit failure modes and overall complexity, “monitoring” every failure explicitly becomes infeasible, and not to mention, quite unnecessary.
Observability, in my opinion, is really about being able to understand how a system is behaving in production. If “monitoring” is best suited to report the overall health of systems, “observability”, on the other hand, aims to provide highly granular insights into the behavior of systems along with rich context, perfect for providing visibility into implicit failure modes and on the fly generation of information required for debugging. Monitoring is being on the lookout for failures, which in turn requires us to be able to predict these failures proactively. An observable system is one that exposes enough data about itself so that generating information (finding answers to questions yet to be formulated) and easily accessing this information becomes simple.
For the uninitiated, blackbox monitoring refers to the category of monitoring derived by treating the system as a blackbox and examining it from the outside. While some believe that with more sophisticated tooling at our disposal blackbox monitoring is a thing of the past, I’d argue that blackbox monitoring still has its place, what with large parts of core business and infrastructural components being outsourced to third-party vendors. While the amount of control we might have over the performance of the vendor might be limited, having visibility into how services we own are impacted by the vagaries of outsourced components becomes exceedingly crucial insofar as it affects our system’s performance as a whole.
Even outside of third-party integrations, treating our own systems as blackboxes might still have some value, especially in a microservices environment where different services owned by different teams might be involved in servicing a request. In such cases, being able to communicate quantitatively about systems paves the way toward establishing SLOs for different services.
It seems pragmatic for individual teams to treat services owned by other teams as blackboxes. This enables individual teams to design better integrations with other systems owned by different teams based on the contracts they expose and guarantees they offer.
“Whitebox monitoring” refers to a category of “monitoring” based on the information derived from the internals of systems. Whitebox monitoring isn’t really a revolutionary idea anymore. Time series, logs and traces are all more in vogue than ever these days and have been for a few years.
So then. Is observability just whitebox monitoring by another name?
Well, not quite.
The difference between whitebox monitoring and observability really is the difference between data and information. The formal definition of information is:
Data are simply facts or figures — bits of information, but not information itself. When data are processed, interpreted, organized, structured or presented so as to make them meaningful or useful, they are called information. Information provides context for data.
The distinction between monitoring and observability isn’t just about if the data is being reported from the bowels of the system or if it’s collected via treating the system as a blackbox. The distinction, in my opinion, is more purpose-driven, than origin-driven. It’s not so much about where this data comes from than what we plan to do with it and how easily we can achieve this.
Whitebox monitoring is a fantastic source of data. Observability is our ability to easily and quickly find germane information from this data when we need it. Observability is more about what information we might require about the behavior of our system in production and whether we will be able to have access to this information. It doesn’t matter so much if this information is pre-processed or if it’s derived from the data on the fly. It’s also not about how we plan to process and use the raw data. This raw data we’re collecting could have various uses —
Both of these cases, I’d argue, fall under “monitoring”.
These are all different goals. We could optimize for some of these or maybe even all of these. Whitebox monitoring is an important component (possibly the most important component) that helps us achieve all of these aforementioned goals, but whitebox monitoring, per se, isn’t observability.
Different organizations might have different requirements for what falls under “monitoring”. For some, dependency analysis might be an active part of their “monitoring”. For others, security auditing might be an indispensable part of their Monitoring goals. As such, I see observability as a spectrum and something constantly evolving as a service evolves.
Another way of looking at what falls under “monitoring” as opposed to what’s “observability” is by differentiating what we do reactively as opposed to what we do proactively.
Again, this might be different for different organizations, but I think it’s important to differentiate between the two purposes. Proactively generating information from data because we feel we might need them at all times is different from generating information on the fly at the time of debugging or analysis from data proactively collected.
Yet another way of looking at this spectrum is to perhaps distinguish based on the Ops versus Dev responsibilities. I see “monitoring” as something that requires being on-call. Observability I see as something that mandatorily requires developer/software engineer participation.
Furthermore, it’s worth noting at there’s a cause/effect relationship at play here in the spectrum. A lot of times what people “monitor” at one layer (metrics like error rate and duration of request) are often the “symptoms”, with the cause being several layers down the spectrum.
Being able to troubleshoot a problem involves often starting with a symptom reported by a coarse-grained metric (increased error rate or response time) or a trace (service B is slow for certain types of requests from downstream service C) that provides a bird’s eye view of the problem and then iteratively drilling down to corroborate or invalidate our theories thereby reducing the search space at every iteration, until we finally reach the information needed to until we arrive at the root cause.
Which brings me to my next point about —
While having access to data becomes a requirement if we wish to derive information from it, observability isn’t just about data collection alone. Once we have the data, it becomes important to be able to get answers/information from this data easily.
While it’s true that raw data is more malleable than pre-processed information, deferring processing of information until we actually need it incurs other overheads, namely that of collection, storage and on-the-fly processing. While it might sound all very well in theory to state that implementation details don’t matter so long as we can get to our observability goals, how the data that is being gathered can be best processed and stored becomes a key practical consideration if we wish to achieve the dream of establishing and sustaining the virtuous cycle. Usability of data becomes a key concern as well, as does the barrier to data collection.
And lastly, I’d argue that the most overarching aspect of observability isn’t data collection or processing. Having data at our disposal alone doesn’t solve problems. Problem solving also involves the right amount of engineering intuition and domain experience to ask the right questions of the data to be able to get to the bottom of it. In fact, good observability isn’t possible without having good engineering intuition and domain knowledge, even if one had all the tools at one’s disposal. And that really is what the rest of this post aims to address, by hopefully giving you some food for thought in terms of how to build systems to make it possible to gain better insights from them.
A more concrete example would help us understand logs, metrics and traces better. Let us assume the architecture of our system or sub-system looks like the following:
A log is an immutable record of discrete events that happened over time. Some people take the view that events are distinct compared to logs, but I’d argue that for all intents and purposes they can be used interchangeably.
Event logs in general come in three forms:
1. Plaintext — A log record might take the form of free-form text. This is also the most common format of logs.2. Structured — Much evangelized and advocated for in recent days. Typically this is logs emitted in the JSON format.3. Binary — think logs in the Protobuf format, MySQL binlogs used for replication and point-in-time recovery, systemd journal logs, the pflogformat used by the BSD firewall pf which often serves as a frontend to tcpdump.
Logs, in particular, shine when it comes to providing valuable insight along with ample context into the long tail that averages and percentiles don’t surface. Coming back to the example we saw above, let us assume that all of these various services also emit logs at varying degrees of granularity. Some services might emit more log information per request than others. Looking at logs alone, our data landscape might look like the following:
The very first thing that jumps out to me when I look at the above diagram is abundance of data points. Recording anything and everything that might be of interest to us becomes incredibly useful when we are searching at a very fine level of granularity, but simply looking at this mass of data, it’s impossible to infer at a glance what the request lifecycle was or even which systems the request traversed through or even the overall health of any particular system. Sure, the data might be rich but without further processing, it’s pretty impenetrable.
What we require, in short, is information. The interesting aspect of information in the context of this discussion is what information we’re looking for? Do we want information about the lifecycle of a request? Or do we want information about the resource utilization of a specific service? Or do we want information about the health of a specific host? Or do we want information about why a specific service crashed? Or do we want information about the replication lag in a distributed key value store? Or are we looking for information about how long it took an eventually consistent system to converge? Or are we looking for information about GC pauses? Or are we trying to glean information about the symptoms or are we trying to find the root cause? There is, quite frankly, an endless amount of data points we can collect and an endless number of questions we can answer, from the most trivial to the most difficult.
Two very important pieces of information, however, pertains to the fate of requests throughout their lifecycle (which is usually short lived) and the fate of a system as a whole (measured over a duration that is orders of magnitudes longer than request lifecycles). I see both traces and metrics as an abstraction built on top of logs that pre-process and encode information along two orthogonal axes, one being request centric, the other being system centric.
A trace is a representation of a series of causally-related distributed events that encode the end-to-end request flow through a distributed system. A single trace can provide visibility into both the path traversed by a request as well as the structure of a request. The path of a request allows us to understand the services involved in the servicing of a request, and the structure of a trace helps one understand the junctures and effects of asynchrony in the execution of a request.
Albeit discussions around tracing pivot around their utility in a microservices environment, I think it’s fair to suggest that any sufficiently complex application that interacts with — or rather, contends for — resources such as the network or disk in a non-trivial manner can benefit from the benefits tracing can provide.
The basic idea behind tracing is straightforward — identify specific points in an application, proxy, framework, library, middleware and anything else that might lie in the path of execution of a request, instrument these points and have these coordinate with each other. These points are of particular interest since they represent forks in execution flow (OS thread or a green thread) or a hop or a fan out across network or process boundaries that a request might encounter in the course of its lifecycle.
Usually represented as a directed acyclic graph, they are used to identify the amount of work done at each layer while preserving causality using happens-before semantics. The way this is achieved is by adding instrumentation to specific points in code. When a request begins, it’s assigned a globally unique ID, which is then propagated throughout the request path, so that each point of instrumentation is able to insert or enrich metadata before passing the ID around to the next hop in the meandering flow of a request. When the execution flow reaches the instrumented point at one of these services, a record is emitted along with metadata. These records are usually asynchronously logged to disk before being submitted out of band to a collector, which then can reconstruct the flow of execution based on different records emitted by different parts of the system.
Collecting this information and reconstructing the flow of execution while preserving causality for retrospective analysis and troubleshooting enables one to understand the lifecycle of a request better. Most importantly, having an understanding of the entire request lifecycle makes it possible to debug requests spanning multiple services to pinpoint the source of increased response time or resource utilization. As such, traces largely help one understand the which and sometimes even the why — like which component of a system is even touched during the lifecycle of a request and is slowing the response?
The official definition of metrics is:
a set of numbers that give information about a particular process or activity.
Metrics are a numeric representation of our data and as such can fully harness the power of mathematical modeling and prediction to derive knowledge of the behavior of our system over intervals of time in the present and future— in other words, a time series. The official definition of time series :
a list of numbers relating to a particular activity, which is recorded at regular periods of time and then studied. Time series are typically used to study, for example, sales, orders, income, etc.
Metrics are just numbers measured over intervals of time, and numbers are optimized for storage, processing, compression and retrieval. As such, metrics enable longer retention of data as well as easier querying, which can in turn be used to build dashboards to reflect historical trends. Additionally, metrics better allow for gradual reduction of data resolution over time, so that after a certain period of time data can be aggregated into daily or weekly frequency.
One of the biggest drawback of historical time series databases has been the identification of metrics which didn’t lend itself very well toward exploratory analysis or filtering. The hierarchical metric model and the lack of tags or labels in systems like Graphite especially hurt in this regard. Modern monitoring systems like Prometheus represent every time series using a metric name as well as additional key-value pairs called labels.
This allows for a high degree of dimensionality in the data model. A metric is identified using both the metric name and the labels. Metrics in Prometheus are immutable; changing the name of the metric or adding or removing a label will result in a new time series. The actual data stored in the time-series is called a sample and it consists of two components — a float64 value and a millisecond precision timestamp.
Let’s evaluate each of the three in terms of three criteria before we see how we can leverage the strengths of each to craft a great observability experience:
— Ease of generation/instrumentation— Ease of processing — Ease of querying/searching— Quality of information— Cost Effectiveness
Logs are, by far, the easiest to generate since there is no initial processing involved. The fact that it is just a string or a blob of JSON makes it incredibly easy to represent any data we want to emit in the form of a log line. Most languages, application frameworks and libraries come with in built support for logging. Logs are also easy to instrument since adding a log line is quite as trivial as adding a print statement. Logs also perform really well in terms of surfacing highly granular information pregnant with rich local context that can be great for drill down analysis, so long as our search space is localized to a single service.
The utility of logs, unfortunately, ends right there. Everything else I’m going to tell you about logs is only going to be painful. While log generation might be easy, the performance idiosyncrasies of various popular logging libraries leave a lot to be desired. Most performant logging libraries allocate very little, if any, and are extremely fast. However, the default logging libraries of many languages and frameworks are not the cream of the crop, which means the application as a whole becomes susceptible to suboptimal performance due to the overhead of logging. Additionally, log messages can also be lost unless one uses a protocol like RELP to guarantee reliable delivery of messages. This becomes especially important if one is using log data for billing or payment purposes. Lastly, unless the logging library can dynamically sample logs, logging has the capability to adversely affect application performance as a whole. As someone mentioned on a Slack:
A fun thing I had seen while at [redacted] was that turning off most logging almost doubled performance on the instances we were running on because logs ate through AWS’ EC2 classic’s packet allocations like mad. It was interesting for us to discover that more than 50% of our performance would be lost to trying to control and monitor performance.
On the processing side, raw logs are almost always normalized, filtered and processed by a tool like Logstash, fluentd, Scribe or Heka before they’re persisted in a data store like Elasticsearch or BigQuery. If an application generates a large volume of logs, then the logs might require further buffering in a broker like Kafka before they can be processed by Logstash. Hosted solutions like BigQuery have quotas you cannot exceed. On the storage side, while Elasticsearch might be a fantastic search engine, there’s a real operational cost involved in running it. Even if your organization is staffed with a team of Operations engineers who are experts in operating ELK, there might be other drawbacks. Case in point — one of my friends was telling me about how he would often see a sharp downward slope in the graphs in Kibana, not because traffic to the service was dropping but because ELK couldn’t keep up with the indexing of the sheer volume of data being thrown at it. Even if log ingestion processing isn’t an issue with ELK, no one I know of seems to have fully figured out how to use Kibana’s UI, let alone enjoy using it.
While there is no dearth of hosted commercial offerings for log management, they are probably better known for their obscene pricing. The fact that a large number of organizations choose to outsource log management despite the cost is a testament to how operationally hard, expensive and fragile running it in-house is.
An antidote often proposed to the problem of the cost overhead of logging is to sample or to only log actionable data. But even when sampled aggressively, it requires us to make decisions a priori as to what might be actionable. As such, our ability to log “actionable” data is entirely contingent on our ability to be able to predict what will be actionable or what data might be needed in the future. While it’s true that better understanding of a system might allow us to make an educated guess as to what data now gathered can prove to be a veritable source of information in the future, potentially every line of code is point of failure and as such could become the source of a log line.
By and large, the biggest advantage of metrics based monitoring over logs is the fact that unlike log generation and storage, metrics transfer and storage has a constant overhead. Unlike logs, the cost of metrics doesn’t increase in lockstep with user traffic or any other system activity that could result in a sharp uptick in data.
What this means is that with metrics, an increase in traffic to an application will not incur a significant increase in disk utilization, processing complexity, speed of visualization and operational costs the way logs do. Metrics storage increases with the number of time series being captured (when more hosts/containers are spun up, or when new services get added or when existing services are instrumented more), but unlike statsd clients that send a UDP packet every time a metric is recorded to the statsd daemon (resulting in a directly proportional increase in the number of metrics being submitted to statsd compared to the traffic being reported on!), client libraries of systems like Prometheus aggregate time series samples in-process and submit them to the Prometheus server upon a successful scrape (which happens once every few seconds and can be configured).
Metrics, once collected, are also more malleable to mathematical, probabilistic and statistical transformations such as sampling, aggregation, summarization and correlation, which make it better suited to report the overall health of a system.
Metrics are also better suited to trigger alerts, since running queries against an in-memory time series database is far more efficient, not to mention more reliable, than running a query against a distributed system like ELK and then aggregating the results before deciding if an alert needs to be triggered. Of course, there are systems that strictly query only in-memory structured event data for alerting that might be a little less expensive than ELK, but the operational overhead of running large distributed in-memory databases, even if they were open source, isn’t something worth the trouble for most when there are far easier ways to derive equally actionable alerts. Metrics are akin to blackbox frontends of a system’s performance and as such are best suited to furnish this information.
The biggest drawback with both logs and metrics is that they are system scoped, making it hard to understand anything else other than what’s happening inside of a particular system. Sure, metrics can also be request scoped, but that entails a concomitant increase in label fanout which results in an increase in storage. While the new Prometheus storage engine has been optimized for high churn in time series, it’s also true that metrics aren’t the best suited for highly granular request scoped information. With logs, without fancy joins, a single log line or metric doesn’t give much information about what happened to a request across all components of a system. Together and when used optimally, logs and metrics give us complete omniscience into a silo, but nothing more. While these might be sufficient for understanding the performance and behavior of individual systems — both stateful and stateless — they come a cropper when it comes to understanding the lifetime of a request that traverses through multiple systems.
Traces
Tracing captures the lifetime of requests as they flow through the various components of a distributed system. The support for enriching the context that’s being propagated with additional key value pairs makes it possible to encode application specific metadata in the trace, which might give developers more debugging power.
The use cases of distributed tracing are myriad. While used primarily for inter service dependency analysis, distributed profiling and debugging steady-state problems, tracing can also help with chargeback and capacity planning.
Tracing is, by far, the hardest to retrofit into an existing infrastructure, owing to the fact that for tracing to be truly effective, every component in the path of a request needs to be modified to propagate tracing information. Depending on whom you ask, you’d either be told that gaps in the flow of a request doesn’t outweigh the cons or be told that these gaps are blind spots that make debugging harder.
We’ve been implementing a request tracing service for over a year and it’s not complete yet. The challenge with these type of tools is that, we need to add code around each span to truly understand what’s happening during the lifetime of our requests. The frustrating part is that if the code is not instrumented or header is not carrying the id, that code becomes a risky blind spot for operations.
The second problem with tracing instrumentation is that it’s not sufficient for developers to instrument their code. A large number of applications in the wild are built using open source frameworks or libraries which might require additional instrumentation. This becomes all the more challenging at places with polyglot architectures, since every language, framework and wire protocol with widely disparate concurrency patterns and guarantees need to cooperate. Indeed, tracing is most successfully deployed in organizations where there are a core set of languages and frameworks used uniformly across the company.
The cost of tracing isn’t quite as catastrophic as that of logging, mainly owing to the fact that traces are almost always sampled heavily to reduce runtime overhead as well as storage costs. Sampling decisions can be made:
— at the start of a request before any traces are generated— at the end once all participating systems have recorded the traces for the entire course of the request execution— midway through the request flow, when only downstream services would then report the trace
All approaches have their own pros and cons.
Given the aforementioned characteristics of logs, any talk about best practices for logging inherently embodies a tradeoff. There are a couple of approaches that I think can help alleviate the problem on log generation, processing, storage and analysis.
We either log everything that might be of interest and pay a processing and storage penalty, or we log selectively, knowing that we are sacrificing fidelity but making it possible to still have access to important data. Most talk around logging revolves around log levels, but rarely have I seen quotas imposed on the amount of log data a service can generate. While Logstash and friends do have plugins for throttling log ingestion, most of these filters are based on keys and certain thresholds, with throttling happening after the event has been generated.
If logging is provided as an internal service — and there are many companies where this is the case — then establishing service tiers with quotas and priorities can be a first step. Any user facing request or service gets assigned the highest priority, while infrastructural tasks or background jobs or anything that can tolerate a bounded delay are lower on the priority list.
With or without quotas, it becomes important to be able to dynamically sample logs, so that the rate of log generation can be adjusted on the fly to ease the burden on the log forwarding, processing and storage systems. In the words of the aforementioned acquaintance who saw a 50% boost by turning off logging on EC2:
The only thing it kind of convinced me to is the need for the ability to dynamically increase or decrease logging on a per-need basis. But the caveat there is always that if you don’t always run the full blown logging, eventually the system can’t cope to run with it enabled.
Data isn’t only ever used for application performance and debugging use cases. It also forms the source of all analytics data as well. This data is often of tremendous utility from a business intelligence perspective, and usually businesses are willing to pay for both the technology and the personnel required to make sense of this data in order to make better product decisions.
The interesting aspect to me here is that there are striking similarities between questions a business might want answered and questions we might want answered during debugging. For example, a question that might be of business importance is the following:
Filter to outlier countries from where users viewed this article fewer than 100 times in total.
Whereas, from a debugging perspective, the question might look more like:
Filter to outlier page loads that performed more than 100 database queries.
Or, show me only page loads from Indonesia that took more than 10 seconds to load.
While these aren’t similar queries from a technical perspective, the infrastructure required to perform these sort of analysis or answer these kinds of queries is largely the same.
What might of interest to the business might be the fact that:
User A viewed Product X.
Augmenting this data with some extra information might make it ripe for observability purposes:
User A viewed Product X and the page took 0.5s to loadUser A viewed Product X whose image was not served from cacheUser A viewed Product X and read review Z for which the response time for the API call was 300ms.
Both these queries are made possible by events. Events are essentially structured (optionally typed) key value pairs. Marrying business information along with information about the lifetime of the request (timers, durations and so forth) makes it possible to repurpose analytics tooling for observability purposes.
If you think about this, log processing neatly fits into the bill of Online Analytics Processing (OLAP). Information derived from OLAP systems is not very different compared to information derived for debugging or performance analysis or anomaly detection at the edge of the system. Most analytics pipelines use Kafka as an event bus. Sending enriched event data to Kafka allows one to search in real time over streams with KSQL, a streaming SQL engine for Kafka from the fine folks at Confluent.
KSQL supports a wide range of powerful stream processing operations including aggregations, joins, windowing, sessionization, and much more. The Kafka log is the core storage abstraction for streaming data, allowing same data that went into your offline data warehouse is to now be available for stream processing. Everything else is a streaming materialized view over the log, be it various databases, search indexes, or other data serving systems in the company. All data enrichment and ETL needed to create these derived views can now be done in a streaming fashion using KSQL. Monitoring, security, anomaly and threat detection, analytics, and response to failures can be done in real-time versus when it is too late. All this is available for just about anyone to use through a simple and familiar SQL interface to all your Kafka data: KSQL.
Enriching business events that go into Kafka anyway with additional timing and other metadata required for observability use cases can be helpful when repurposing existing stream processing infrastructures. A further benefit this pattern provides is that this data can be expired from the Kafka log regularly. Most event data required for debugging purposes are only valuable for a relatively short period of time after the event has been generated, unlike any business centric information that normally would’ve been evaluated and persisted by an ETL job. Of course, this makes most sense when Kafka already is an integral part of an organization. Introducing Kafka into a stack purely for real time log analytics is a bit of an overkill, especially in non-JVM shops without any significant JVM operational expertise.
The fact that logging still remains an unsolved problem makes me wish for an OpenLogging spec, in the vein of OpenTracing which serves as a shining example and a testament to the power of community driven development. A spec designed ground up for the cloud-native era that introduces a universal exposition as well as a propagation format. A spec that enshrines that logs must be structured events and codifies rules around dynamic sampling for high volume, low fidelity events. A spec that can be implemented as libraries in all major languages and supported by all major application frameworks and middleware. A spec that allows us to make the most of advances in stream processing. A spec that becomes the lingua franca logging format of all CNCF projects, especially Kubernetes.
Prometheus is much more than just the server. I see Prometheus as a set of standards and projects, with the server being just one part of a much greater whole.
Prometheus does a great job of codifying the exposition format for metrics, and I’d love to see this become the standard. While Prometheus doesn’t offer long term storage, the remote write feature that was added to Prometheus about a year ago allowed one to write Prometheus metrics to a custom remote storage engine like OpenTSDB or Graphite, effectively turning Prometheus into a write-through cache. With the recent introduction of the generic write backend, one can transport time-series from Prometheus over HTTP and Protobuf to any storage system like Kafka or Cassandra.
Remote reads, though, is slightly newer and I’ve only been seeing efforts coalesce into something meaningful in the last few months. InfluxDB now natively supports both Prometheus remote reads and writes. Remote reads allows Prometheus to read raw samples from a remote backend during query execution time and compute the results in the Prometheus server.
Furthermore, improvements to the Prometheus storage engine in the upcoming 2.0 release makes Prometheus all the more conducive to cloud-native workloads with vast churn in time-series names. The powerful query language of Prometheus coupled with the ability to define alerts using the same query language and enrich the alerts with templated annotations makes it perfect for all “monitoring purposes”.
With metrics, however, it’s important to be careful not to explode the label space. Labels should be so chosen so that it remains limited to a small set of attributes that can remain somewhat uniform. It also becomes important to resist the temptation to alert on everything. For alerting to be effective, it becomes salient to be able to identify a small set of hard failure modes of a system. Some believe that the ideal number of signals to be “monitored” is anywhere between 3–5, and definitely no more than 7–10. One of the common pain points that keeps cropping up in my conversations with friends is how noisy their “monitoring” is. Noisy monitoring leads to either metric data that’s never looked at — which in other words is a waste of storage space of the metrics server — or worse, false alerts leading to a severe case of alert fatigue.
While historically tracing has been difficult to implement, the rise of service meshes make integrating tracing functionality almost effortless. Lyft famously got tracing support for all of their applications without changing a single line of code by adopting the service mesh pattern. Service meshes help with the DRYing of observability by implementing tracing and stats collections at the mesh level, which allows one to treat individual services as blackboxes but still get incredible observability onto the mesh as a whole. Even with the caveat that the applications forming the mesh need to be able to forward headers to the next hop in the mesh, this pattern is incredibly useful for retrofitting tracing into existing infrastructures with the least amount of code change.
Exception trackers (I think of these as logs++) have come a long way in the last few years and provide a far superior UI than a plaintext file or blobs of JSON to inspect exceptions. Exception trackers also provide full tracebacks, local variables, inputs at every subroutine or method invocation call, frequency of occurrence of the error/exception and other metadata invaluable for debugging. Exception trackers aim to do one thing — track exceptions and application crashes — and they tend to do this really well. While they don’t eliminate the need for logs, exception trackers augment logs — if you’ll pardon the pun — exceptionally well.
Some new tools also help achieve visibility by treating the network packets as the source of truth and using packet capture to build the overall service topology. While this definitely has less overhead than instrumenting all application code throughout the stack, it’s primarily useful for analyzing network interactions between different components. While it cannot help with debugging issues with the asynchronous behavior of a multithreaded service or unexpected event loop stalls in a single threaded service, augmenting it with metrics or logs to better understand what’s happening inside a single service can help one gain enough visibility into the entire architecture.
Observability isn’t quite the same as monitoring. Observability connotes something more holistic and encompasses “monitoring”, application code instrumentation, proactive instrumentation for just-in-time debugging and a culture of more thorough understanding of various components of the system.
Observability means having the ability — and the confidence — to be able to build systems knowing that these systems can turn into a frankensystem in production. It’s about understanding that the software we’re building can be — and almost always is — broken (or prone to break soon) to varying degrees despite our best efforts. A good analogy between getting code working on one’s laptop or in CI to having code running in production would be the difference between swimming in an indoor pool versus swimming in choppy rivers full of piranhas. The feeling of being unable to fix one’s own service running in a foreign environment for the want to being able to debug isn’t acceptable, not if we want to pride ourselves on our uptime and quality of service.
I want to conclude this post with how I think software development and operation should happen in the time of cloud-native.
It’s important to understand that testing is a best effort verification of the correctness of a system as well as a best effort simulation of failure modes. Unit tests only ever test the behavior of a system against a specified set of inputs. Furthermore, tests are conducted in very controlled (often heavily mocked) environments. While the very few who do fuzz their code benefit from having their code tested against a set of randomly generated input, fuzzing can only comprehensively test against the set of inputs to one service. End-to-end testing might allow for some degree of holistic testing of the system and fault injection/chaos engineering might help us gain a reasonable degree of confidence about our system’s ability to withstand these failures, but complex systems fail in complex ways and there’s is no testing under the sun that enables one to predict every last vector that could contribute towards a failure.
Despite these shortcomings, testing is as important as ever. If nothing else, testing our code allows us to write better and more maintainable code. More importantly, research has proven that something as simple as “testing error handling code could have prevented 58% of catastrophic failures” in many distributed systems. The renaissance of tooling aimed to understand the behavior of our services in production does not obviate the need for testing.
Testing in production isn’t really a very new idea. Methodologies such as A/B testing, canary deployments, dark traffic testing (some call this shadowing) have been around for a while.
Being able to test in production however absolutely requires that the release can be halted and rolled back if the need arises. This in turn means that one can only test in production if one has a quick feedback loop about the behavior of the system one’s testing in production. It also means being on the lookout for changes to key performance indicators of the service. For an HTTP service this could mean attributes like error rate and latencies of key endpoints. For a user facing service, this could additionally mean a change in user engagement. Testing in production essentially means proactively “monitoring” the change in production. Which brings me to my next point.
Monitoring isn’t dead. Monitoring, in fact, is so important that I’d argue it occupies the pride of place in your observability spectrum.
In order to test in production, one needs good, effective monitoring. Monitoring that is both failure centric (in that we proactively monitor for changes to KPI’s) as well as human centric (we want the developer who pushed out the change to test in production to be alerted as soon as possible).
I chose to call this Tier I Monitoring, for the want of better word, since I believe these are table stakes. It’s the very minimum any service thats going to be in production needs to have. It’s what alerts are derived from and I believe that time-series metrics are the best suited for this purpose.
However, there are several other bits and pieces of information we might capture but not use for alerting. There’s a school of thought that all such information isn’t of much value and needs to be discarded. I, however, believe that this is the sort of information I often find myself requiring often enough that I want it presented to me in the form of a dashboard.
A good example of this sort of tier II monitoring would be this dashboard of GitLab’s which is aptly named fleet overview or this one which gives information about the running Go processes. I picked these examples because GitLab is famously known for its transparency and these are real, live production dashboards of a real company. I find analyzing these dashboards more interesting than cooking up toy examples for the purpose of a blog post.
While these metrics don’t particularly help with debugging of gremlins or problems we don’t even know exist, having such dashboards gives me a bird’s eye view of the system, which I find invaluable especially after a release, since it gives me extremely quick feedback about how known key metrics might have been impacted by the change, but weren’t severe enough to trigger an alert. Measuring heap usage for a potential memory leak would be a good example of such Tier II monitoring. I would really like to know if I pushed out a code that’s leaking memory, but I don’t consider it something I necessarily want to be alerted on.
Then there’s exploration, which I find useful to answer questions one could not have proactively thought about. This often involves querying of raw events or log data rich in context and is extremely powerful for surfacing answers we couldn’t have predicted beforehand.
The problem with all of the three approaches seen until now is that they require that we record information about our systems a priori. What this means is the the data we need is generated before we can derive any useful information from it.
Dynamic instrumentation techniques aren’t new. However, implementations like DTrace were primarily machine centric and mostly correlate events that remain confined to an address-space or specific machine. Recent academic research has married these ideas with some of the ideas pioneered by distributed tracing, allowing one to “to obtain an arbitrary metric at one point of the system,while selecting, filtering, and grouping by events meaningful at other parts of the system, even when crossing component or machine boundaries”.
The primary breakthrough the Pivot Tracing paper proposed is the baggage abstraction.
Baggage is a per-request container for tuples that is propagated alongside a request as it traverses thread, application and machine boundaries. Tuples follow the request’s execution path and therefore explicitly capture the happened-before relationship. Using baggage, Pivot Tracing efficiently evaluates happened-before joins in situ during the execution of a request.
The idea of baggage propagation has been incorporated into the OpenTracing spec, which now enables “arbitrary application data from a mobile app can make it, transparently, all the way into the depths of a storage system”. While this still isn’t quite the same as what the whitepaper describes, it still gets us one step closer to true end-to-end tracing and the ability to dynamically enrich tracing data for better visibility. Facebook’s Canopy further takes ideas pioneered by the Pivot Tracing paper and marries it with an underlying event model pioneered by Scuba, making exploration of data more dynamic than ever.
And finally there are the unknowables.
Things we can’t know about or don’t need to know about. Even if we have complete omniscience into our application and hardware performance, it’s simply not feasible — or required — to have complete visibility into the various layers of abstractions underneath the application layer. Think ARP packet losses, BGP announcements or recursive BGP lookups, OSFP states and all manner of other implementation details of abstractions we rely on without a second thought. We simply have to get comfortable with the fact that there are things we possibly cannot know about, and that it’s OK.
Which brings me to my final point —
Observability — in and of itself, and like most other things — isn’t particularly useful. The value derived from the observability of a system directly stems from the business value derived from that system.
For many, if not most, businesses, having a good alerting strategy and time-series based “monitoring” is probably all that’s required to be able to deliver on the business goals. For others, being able to debug needle-in-a-haystack type of problems might be what’s needed to generate the most business value.
Observability, as such, isn’t an absolute.
Pick your own observability target based on the requirements of your service.
@copyconstruct on Twitter. views expressed on this blog are solely mine, not those of present or past employers.
See all (30)
2.5K 
8
2.5K claps
2.5K 
8
@copyconstruct on Twitter. views expressed on this blog are solely mine, not those of present or past employers.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/walmartglobaltech/cloud-native-application-architecture-a84ddf378f82?source=search_post---------1,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Cloud native is an approach for building applications as micro-services and running them on a containerised and dynamically orchestrated platforms that fully exploits the advantages of the cloud computing model. Cloud-native is about how applications are created and deployed, not where. Such technologies empower organisations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. These applications are built from the ground up, designed as loosely coupled systems, optimised for cloud scale and performance, use managed services and take advantage of continuous delivery to achieve reliability and faster time to market. The overall objective is to improve speed, scalability and, finally, margin.
Speed — Companies of all sizes now see a strategic advantage in being able to move quickly and get ideas to market fast. By this, we mean moving from months to get an idea into production to days or even hours. Part of achieving this is a cultural shift within a business, transitioning from big bang projects to more incremental improvements. At its heart, a Cloud Native strategy is about handling technical risk. In the past, our standard approach to avoiding danger was to move slowly and carefully. The Cloud Native approach is about moving quickly by taking small, reversible and low-risk steps. Scalability — As businesses grow, it becomes strategically necessary to support more users, in more locations, with a broader range of devices, while maintaining responsiveness, managing costs and not falling overMargin — In the new world of cloud infrastructure, the strategic goal is to be to pay for additional resources only as needed — as new customers come online. Spending moves from up-front CAPEX (buying new machines in anticipation of success) to OPEX (paying for additional servers on-demand).
Cloud Native Computing Foundation is an open source software foundation housed in the Linux Foundation and includes big names such as Google, IBM, Intel, Box, Cisco, and VMware etc, dedicated to making cloud-native computing universal and sustainable. Cloud native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilisation. According to the FAQ on why CNCF is needed — Companies are realising that they need to be a software company, even if they are not in the software business. For example, Airbnb is revolutionising the hospitality industry and more traditional hotels are struggling to compete. Cloud native allows IT and software to move faster. Adopting cloud-native technologies and practices enables companies to create software in-house, allows business people to closely partner with IT people, keep up with competitors and deliver better services to their customers.
Microservice is an approach to develop a single application as a suite of small services, each running in their own process and communicating using lightweight protocols like HTTP. These services are built around business capabilities and are independently deployable by fully automated deployment machinery.
Each service of a cloud-native application is developed using the language and framework best suited for the functionality. Cloud-native applications are polyglot. Services use a variety of languages, runtimes and frameworks. For example, developers may build a real-time streaming service based on WebSockets, developed in Node.js, while choosing Python for building a machine learning based service and choosing spring-boot for exposing the REST APIs. The fine-grained approach to developing microservices lets them choose the best language and framework for a specific job.
Cloud-native services use lightweight APIs that are based on protocols such as representational state transfer (REST) to expose their functionality. Internal services communicate with each other using binary protocols like Thrift, Protobuff, GRPC etc for better performance
A cloud-native app stores its state in a database or some other external entity so instances can come and go. Any instance can process a request. They are not tied to the underlying infrastructure which allows the app to run in a highly distributed manner and still maintain its state independent of the elastic nature of the underlying infrastructure. From a scalability perspective, the architecture is as simple as just by adding commodity server nodes to the cluster, it should be possible to scale the application
According to Murphy’s law — “Anything that can fail will fail”. When we apply this to software systems, In a distributed system, failures will happen. Hardware can fail. The network can have transient failures. Rarely, an entire service or region may experience a disruption, but even those must be planned for. Resiliency is the ability of a system to recover from failures and continue to function. It’s not about avoiding failures, but responding to failures in a way that avoids downtime or data loss. The goal of resiliency is to return the application to a fully functioning state following a failure. Resiliency offers the following:
One of the main ways to make an application resilient is through redundancy. HA and DR are implemented using multi node clusters, Multi region deployments, data replication, no single point of failure, continuous monitoring etc.
Following are some of the strategies for implementing resiliency:
Testing for resiliency — Normally resiliency testing cannot be done the same way that you test application functionality (by running unit tests, integration tests and so on). Instead, you must test how the end-to-end workload performs under failure conditions which only occur intermittently. For example: inject failures by crashing processes, expired certificates, make dependent services unavailable etc. Frameworks like chaos monkey can be used for such chaos testing.
Containers make it possible to isolate applications into small, lightweight execution environments that share the operating system kernel. Typically measured in megabytes, containers use far fewer resources than virtual machines and start up almost immediately. Docker has become the standard for container technology. The biggest advantage they offer is portability.
Cloud-native applications are deployed using Kubernetes which is an open source platform designed for automating deployment, scaling, and management of containerised applications. Originally developed at Google, Kubernetes has become the operating system for deploying cloud-native applications. It is also one of the first few projects to get graduated at CNCF.
DevOps, the amalgamation of “development” and “operations describes the organisational structure, practices, and culture needed to enable rapid agile development and scalable, reliable operations. DevOps is about the culture, collaborative practices, and automation that aligns development and operations teams so they have a single mindset on improving customer experiences, responding faster to business needs, and ensuring that innovation is balanced with security and operational needs. Modern organisations believe in merging of development and operational people and responsibilities so that one DevOps team carries both responsibilities. In that way you just have one team who takes the responsibility of development, deployment and running the software in production.
Continuous integration (CI) and continuous delivery (CD) is a set of operating principles that enable application development teams to deliver code changes more frequently and reliably. The technical goal of CI is to establish a consistent and automated way to build, package, and test applications. With consistency in the integration process in place, teams are more likely to commit code changes more frequently, which leads to better collaboration and software quality.
Continuous delivery picks up where continuous integration ends. CD automates the delivery of applications to selected infrastructure environments. It picks up the package built by CI, deploys into multiple environments like Dev, QA, Performance, Staging runs various tests like integration tests, performance tests etc and finally deploys into production. Continuous delivery normally has few manual steps in the pipeline whereas continuous deployment is a fully automated pipeline which automates the complete process from code checkin to production deployment.
Cloud-native apps take advantage of the elasticity of the cloud by using increased resources during a use spike. If your cloud-based e-commerce app experiences a spike in use, you can have it set to use extra compute resources until the spike subsides and then turn off those resources. A cloud-native app can adjust to the increased resources and scale as needed.
We’re powering the next great retail disruption.
863 
7
863 claps
863 
7
Written by
Distinguished Engineer — Building the next generation Tech platform for Sams club
We’re powering the next great retail disruption. Learn more about us — https://www.linkedin.com/company/walmartglobaltech/
Written by
Distinguished Engineer — Building the next generation Tech platform for Sams club
We’re powering the next great retail disruption. Learn more about us — https://www.linkedin.com/company/walmartglobaltech/
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59?source=search_post---------2,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
By Allyson Gale and Ketan Umare
Today Lyft is excited to announce the open sourcing of Flyte, a structured programming and distributed processing platform for highly concurrent, scalable, and maintainable workflows. Flyte has been serving production model training and data processing at Lyft for over three years now, becoming the de-facto platform for teams like Pricing, Locations, Estimated Time of Arrivals (ETA), Mapping, Self-Driving (L5), and more. In fact, Flyte manages over 7,000 unique workflows at Lyft, totaling over 100,000 executions every month, 1 million tasks, and 10 million containers. In this post, we’ll introduce you to Flyte, overview the types of problems it solves, and provide examples for how to leverage it for your machine learning and data processing needs.
With data now being a primary asset for companies, executing large-scale compute jobs is critical to the business, but problematic from an operational standpoint. Scaling, monitoring, and managing compute clusters becomes a burden on each product team, slowing down iteration and subsequently product innovation. Moreover, these workflows often have complex data dependencies. Without platform abstraction, dependency management becomes untenable and makes collaboration and reuse across teams impossible.
Flyte’s mission is to increase development velocity for machine learning and data processing by abstracting this overhead. We make reliable, scalable, orchestrated compute a solved problem, allowing teams to focus on business logic rather than machines. Furthermore, we enable sharing and reuse across tenants so a problem only needs to be solved once. This is increasingly important as the lines between data and machine learning converge, including the roles of those who work on them.
To give you a better idea of how Flyte makes all this easy, here’s an overview of some of our key features:
Flyte frees you from wrangling infrastructure, allowing you to concentrate on business problems rather than machines. As a multi-tenant service, you work in your own, isolated repo and deploy and scale without affecting the rest of the platform. Your code is versioned, containerized with its dependencies, and every execution is reproducible.
All Flyte tasks and workflows have strongly typed inputs and outputs. This makes it possible to parameterize your workflows, have rich data lineage, and use cached versions of pre-computed artifacts. If, for example, you’re doing hyperparameter optimization, you can easily invoke different parameters with each run. Additionally, if the run invokes a task that was already computed from a previous execution, Flyte will smartly use the cached output, saving both time and money.
In the example above, we train an XGBoost model using the dataset provided here. The machine learning pipeline is constructed in Python and consist of the following four tasks, which align with a typical machine learning journey:
Note how each task is parameterized and strongly typed, making it easy to try different variants and use in combination with other tasks. Additionally, each of these tasks can be arbitrarily complex. With large datasets, for example, Spark is more preferable for data preparation and validation. Model training, however, can be done on a simple model coded in Python. Lastly, notice how we’re able to mark tasks as cacheable, which can drastically speed up runs and save money.
Below, we combine these tasks to create a Workflow (or “pipeline”). The Workflow links the tasks together and passes data between them using a Python based domain specific language (DSL).
Every entity in Flyte is immutable, with every change explicitly captured as a new version. This makes it easy and efficient for you to iterate, experiment and rollback your workflows. Furthermore, Flyte enables you to share these versioned tasks across workflows, speeding up your dev cycle by avoiding repetitive work across individuals and teams.
Workflows are often composed of heterogeneous steps. One step, for example, might use Spark to prepare data, while the next uses this data to train a deep learning model. Each step can be written in a different language and utilize different frameworks. Flyte supports heterogeneity by having container images bound to a task.
By extension, Flyte tasks can be arbitrarily complex. They can be anything from a single container execution, to a remote query in a hive cluster, to a distributed Spark execution. We also recognize that the best task for the job might be hosted elsewhere, so task extensibility can be leveraged to tie single-point solutions into Flyte and thus into your infrastructure. Specifically, we have two ways of extending tasks:
FlyteKit extensions: Allow contributors to provide rapid integrations with new services or systems.
Backend plugins: When fine-grained control is desirable on the execution semantics of a task, Flyte provides backend plugins. These can be used to create and manage Kubernetes resources, including CRDs like Spark-on-k8s, or any remote system like Amazon Sagemaker, Qubole, BigQuery, and more.
Flyte is built to power and accelerate machine learning and data orchestration at the scale required by modern products, companies, and applications. Together, Lyft and Flyte have grown to see the massive advantage a modern processing platform provides, and we hope that in open sourcing Flyte you too can reap the benefits. Learn more, get involved, and try it out by visiting www.flyte.org and checking out our Github at https://github.com/lyft/flyte.
Este artículo también está en español: eng-espanol.lyft.com
Stories from Lyft Engineering.
946 
5
Thanks to Ryan Lane and Mark Grover. 
946 claps
946 
5
Written by
Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making.
Stories from Lyft Engineering.
Written by
Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making.
Stories from Lyft Engineering.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf?source=search_post---------3,"Sign in
There are currently no responses for this story.
Be the first to respond.
adrian cockcroft
Aug 9, 2017·4 min read
Amazon Web Services recently joined the Cloud Native Computing Foundation, and I’m representing AWS as the CNCF board member, with Arun Gupta from our open source team coordinating technical engagement with projects and working groups. To explain what this is all about, I think it’s useful to look at what we mean by “cloud native,” and where the term came from.
Back in 2009, I was working at Netflix, and the engineering teams were figuring out some new application architecture patterns we would need to migrate to AWS. Some of us had learned how to automate deployments at scale from time spent working at eBay, Yahoo, and Google. We also learned new ideas from Werner Vogels and the AWS team. The result was a new set of fundamental assumptions that we baked into our architecture. In 2010, we started talking publicly about our cloud migration, and in 2012 we got the bulk of the platform released as a set of open source projects, collectively known as NetflixOSS.
While we didn’t invent most of these patterns, the fact that we gathered them together into an architecture, implemented it at scale, talked about it in public, and shared the code was influential in helping define what are often referred to as cloud native architectures.
Cloud native architectures take full advantage of on-demand delivery, global deployment, elasticity, and higher-level services. They enable huge improvements in developer productivity, business agility, scalability, availability, utilization, and cost savings.
On-demand delivery, taking minutes instead of weeks, is often the first reason that people move to cloud, but it doesn’t just reduce the deployment time for a traditional application: it also enables a new cloud native pattern of ephemeral and immutable deployments. In the old deployment model, where it takes weeks to get a resource, you’re going to hang on to it, order extra capacity in advance, and be reluctant to give it back, so you’ll figure out how to update it in place. The cloud native pattern, instead, is to bake instances or build containers, deploy many identical copies just as long as they are needed, shut them down when you are done, and create new images each time the code changes. NetflixOSS pioneered these concepts by baking Amazon Machine Images (AMIs). Docker subsequently used it as a core element of the container deployment model.
Deploying applications that span multiple datacenters is a relatively rare and complex-to-implement pattern, but cloud native architectures treat multi-zone and multi-region deployments as the default. To work effectively in this model, developers should have a good understanding of distributed systems concepts; a discussion of the “CAP Theorem” became a common interview topic at Netflix. Despite huge improvements in technology, the speed of light is a fundamental limit, so network latency, and in particular cross-regional latencies, are always going to be a constraint.
Cloud native architectures are scalable. When I first presented about Netflix’s use of AWS in 2010, we were running front end applications on a few thousand AWS instances, supporting about 16 million customers in the USA. Nowadays, Netflix is fully migrated to AWS, has over 100 million global customers, and is running on over 100,000 instances. The implementation details have changed over the years, but the architectural patterns are the same.
Over time, components of cloud native architectures move from being experimental, through competing implementations, to being well-defined external services. We’ve seen this evolution with databases, data science pipelines, container schedulers, and monitoring tools. This is one place where the Cloud Native Compute Foundation acts as a filter and aggregator. The Technical Oversight Committee of the CNCF reviews projects, incubates them, and adopts projects as they move from the experimental phase to the competing implementation phase. For customers who are trying to track a fast-moving and confusing world, it’s helpful to regard CNCF as a brand endorsement, for a loose collection of interesting projects. It’s a loose collection, rather than a single, integrated cloud native architecture, so there’s no particular endorsement of any one project over another, for members of CNCF, or for users of projects.
The CNCF currently hosts ten projects, and is incubating many more: Kubernetes for container orchestration, Prometheus for monitoring, Open Tracing for application flow monitoring, Fluentd for logging, Linkerd for service mesh, gRPC for remote procedure calls, CoreDNS for service discovery, Containerd and Rkt for container runtimes, and CNI for container native networking.
From the AWS perspective, we are interested in several CNCF projects and working groups. AWS were founding members of the Containerd project; we are excited about participating in the Containerd community, and have lots of ideas around how we can help our customers have a better experience. Our forthcoming ECS Task Networking capabilities are written as a CNI plugin, and we expect CNI to be the basis for all container-based networking on AWS. In addition, a recent CNCF survey reports that 63 percent of respondents host Kubernetes on Amazon EC2, and Arun is blogging about his experiences with several Kubernetes on AWS installers, starting with Kops. We have plans for more Kubernetes blog posts and code contributions, and think there are opportunities to propose existing and future AWS open source projects to be incubated by CNCF.
The charter of the open source team we are continuing to build at AWS is to engage with open source projects, communities, and foundations, as well as to help guide and encourage more contributions from AWS engineering. AWS is already a member of The Linux Foundation, which hosts CNCF, and we look forward to working with old and new friends on the shared goal to create and drive the adoption of a new computing paradigm optimized for modern distributed systems.
Please follow @AWSOpen to keep up to date on open source at AWS.
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
See all (448)
401 
4
Thanks to Zaheda. 
401 claps
401 
4
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
About
Write
Help
Legal
Get the Medium app
"
https://engineering.opsgenie.com/cloud-native-continuous-integration-and-delivery-tools-for-kubernetes-e6ea34d308c?source=search_post---------4,"Cloud Native is a new approach to build and run applications that can leverage the cloud computing delivery model. The objective is to improve speed and stability by optimizing the pipeline by leveraging Cloud Native model at each step, from coding to production, by making use of tools, such as containers and orchestrators, combined with additional tooling. As Cloud Native environments get more and more attraction, the tooling around these environments continue to evolve to fit the various needs.
A critical aspect of the modern development is Continuous Integration and Continuous Delivery. Ability to continuously push, test and deploy the code in lab, staging and production environments is an undeniable blessing for development. With the rise of containerization, the rapidness of this cycle has increased tremendously as both build and deployment stages continue to be containerized. However, the tooling for CI and CD must also keep the same pace to stay relevant in this crazy development world.
In this blog post, we will be introducing most promising CI and CD tools for Cloud-Native landscape or in other words pipeline tooling. We’ll also mention some other tools that are slightly Cloud Native compatible but promising.
First one in the list is Argo. Created by Applatix, Argo is one of the most Kubernetes compatible Cloud Native CI/CD solution, as it extends and makes use of Kubernetes APIs. Argo makes use of Custom Resource Definitions (CRD) where the pipeline itself is defined as Kubernetes objects. Custom controller listens to the changes on those objects and makes sure the pipelines are run properly. Using CRDs are the recommended way to extend Kubernetes functionality as it provides a standardized approach to define and watch the changes in arbitrary objects.
You can define pipelines as simple YAML files in Argo. Those pipelines can be used as CI, CD, Data Processing. There are many examples in the Argo repository. It also supports artifacts, templating and parameterization, variable substitution which makes pipelines reusable. As steps in a pipeline are run as Pods, you can make use of sidecar containers for services such as databases, volumes, secrets, config maps which might be helpful for defining your pipelines. Argo also features a nice UI where you can keep track of your runs.
Jenkins, formerly Hudson, is the lead open source automation software. Jenkins X is a new and opinionated Cloud Native CI/CD approach, which aims to require less intervention to pipeline configurations by providing strong defaults. It has a CLI, jx, which can create projects from templates, such as Spring Boot applications. Jenkins X also supports GitOps, where environment promotions are done via commits and environments created for each pull requests. It also comments on issues with feedback about the state of your code’s deployment. Under the hood, it uses Skaffold for image generation and CRDs of Kubernetes to manage the pipeline activity.
Although you can import existing projects, but Jenkins X seems a better fit for newly created projects, as it includes quick start templates and less documentation for importing existing projects. However it is not hard to add your own buildpacks by providing your own Jenkinsfile and Dockerfile. Jenkins X looks highly opinionated compared to regular Jenkins, which is intensely customizable. For instance, “jx import” command even creates a git repository in Github for your project, registers a Webhook to capture git push events and creates a Helm chart for your project. Although there are more than a thousand ways to accomplish the above with old Jenkins, Jenkins X seems to ease the operations for beginners and make a quick introduction to Cloud Native CI/CD but it’s default values and existing quick start templates might not be suitable for everyone, but as said before it is easy to extend it with buildpacks.
A new open source project by SAP, InfraBox is a continuous integration system based on Kubernetes. The project aims to be Cloud Native and seeks to utilize the resources of a cluster to avoid waste of resources. Unlike Argo, it does not make use of CRDs, but uses custom JSONs for defining a pipeline. InfraBox runs each step in Docker containers and allows building Docker images as well. There is also support for caching, timeouts, and secrets.
You can create dynamic pipelines, where steps are resolved while running a pipeline. As different from other solutions, InfraBox has a special support for tests, and it supports displaying them on its web UI as long as it is possible parse the output. One distinctive feature of InfraBox is ability to show not only test results but also things like code coverage, security check reports or metrics with its custom UI support.
As building images for Docker, required Docker for years, it has been a complication for most workflows. There are services such as Google Container Builder. However, these services might not be feasible for everyone. On the other hand, using regular Docker for building images requires root privileges and extended capabilities, while you might not be comfortable for granting such rights inside a container. As both the Docker image format and Registry API specification is already open source and some tools implement them to build containers without requiring Docker daemon to be run. Now, let’s list some promising ones;
Regular Jenkins, the one that most people are used to and the less opinionated one, also supports Kubernetes through the official plugin. Within the pipeline DSL, you can dynamically create pods and run your workflows inside the containers. It supports most of the features of Kubernetes, so you can make use of advanced features such as scheduling, secrets, liveness probes, which can be helpful to create isolated but robust CI and CD pipelines. The plugin can automatically auto scale the executors to your needs and according to your limits. No more waiting VMs to boot for simple Jenkins agents!
Gitlab supports a built-in container registry, Kubernetes support and even monitoring of your deployments inside the Kubernetes. Gitlab promotes a currently beta feature, Auto DevOps, which aims to handle a full cycle of a cloud-native application development. It’s CI builds, tests, asses the code quality and security for both code and container packages, and allows automatic deployment to Kubernetes. You also monitor the deployment with Prometheus and do manual review approvals, so that if you follow a git branch model, your pushes to that branch are mapped to a Deployment in your Kubernetes cluster. This helps you isolate your workflows from one another and utilize your Kubernetes to achieve a Cloud Native workflow. Although it seems complicated and currently in beta, this feature holds excellent promise for future. Gitlab itself can also be deployed to Kubernetes in a Cloud Native way too; it has a Helm chart in the development.
Concourse is a simpler automation system, which has a notion of resources, tasks, and jobs. You can use it for both CI and CD using YAML. Jobs depend on resources. Tasks inside a job make use of those resources, such as git repository source, or intermediate artifacts. Concourse can be deployed as a Helm chart in Kubernetes, and its builders make use of Docker images which makes proper task isolation. It utilizes RunC to run Docker images, without requiring Docker daemon itself. Currently, there are no autoscale mechanisms for the worker nodes as Concourse itself aims to be very simple, but that functionality can be easily added by making use of Horizontal Pod Autoscaler.
GoCD is a continuous delivery solution which can ease the modeling of complex workflows. It has a native support for Kubernetes, where all the infrastructure can be deployed with a Helm chart easily. Using GoCD, you can dynamically provision worker Pods by communicating with Kubernetes. It allows Docker in Docker and you can run customized images for different steps. There is also preliminary work for modeling the deployments as Kubernetes objects and track the state through GoCD UI easily.
An open source project by Netflix, Spinnaker, is a multi-cloud such as AWS, GCP, on-prem Kubernetes, continuous Continuous Delivery solution that handles the deployments of new codes in a controlled environment safely. It allows various deployment strategies, red/black (why not blue/green? Bonus points if you know), roll back on problems, approval, proper tearing down. Spinnaker can make use of LoadBalancer and liveness probe of deployments to ensure safe deployments. Although the same functionality can be achieved via standard Kubernetes Deployment updates even via kubectl, Spinnaker eases the management of multiple clusters and complex pipelines by allowing you to define integration tests on deployed clusters and rollback safely.
Another project from Google, Skaffold is a CLI tool for continuous development with utilizing local and remote Kubernetes clusters. It does not have a server-side component, and it can be safely used in non-interactive CI/CD systems. Skaffold in dev mode updates your application’s deployment regularly by detecting changes, pushing them to local cluster, and streaming the output. It only warns of errors, so the developer experiences a complete loop. You can also upload it to a remote Docker registry and run it within a Kubernetes cluster.
Weaveworks are originally known for creating Weave Net, a container SDN now available as both a Docker plugin and Kubernetes CNI option. Today, the company offers an extensive and eye-candy dashboard and a cloud offering, Weave Cloud. The Flux Operator for Kubernetes allows GitOps style CD & release management where operator fetches the changes from git push events and handles the deployments gracefully. It continously tries to match the declaritive configuration to the actualy deployment in Kubernetes. The cloud offering also has nice UI and Prometheus integration which aims to help your Continous Delivery pipelines.
In this blog post, we listed CI/CD tooling for Kubernetes. This is a growing interested area because as many companies try to adopt Cloud Native approaches in their development and delivery by utilizing containers, smart schedulers like Kubernetes, they also are in need of tools that work natively in environments such as Kubernetes and can build containers without interruption. As there are many new tools, and existing tools adopting the Cloud Native way, we hope to see more tools in the future.
If you liked this article, don’t forget to 👏 and follow OpsGenie Engineering!
OpsGenie is a cloud-based alerting and incident management service that could aggregate alerts from multiple IT monitoring systems and ensures the right people are notified at the right time, using e-mail, SMS, voice and mobile push notifications.
Interested in more?
engineering.opsgenie.com
engineering.opsgenie.com
Opsgenie is a cloud-based service for dev & ops teams…
833 
7
Thanks to Serhat Can. 
833 claps
833 
7
Written by
PhD Student @BilkentUniversity CS, on @OpsGenie SRE Team
Opsgenie is a cloud-based service for dev & ops teams, providing reliable alerts, on-call schedule management and escalations. OpsGenie integrates with monitoring tools & services, ensures the right people are notified.
Written by
PhD Student @BilkentUniversity CS, on @OpsGenie SRE Team
Opsgenie is a cloud-based service for dev & ops teams, providing reliable alerts, on-call schedule management and escalations. OpsGenie integrates with monitoring tools & services, ensures the right people are notified.
"
https://medium.com/kubeflow/kubeflow-1-0-cloud-native-ml-for-everyone-a3950202751?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Coauthors: Jeremy Lewi (Google), Josh Bottum (Arrikto), Elvira Dzhuraeva (Cisco), David Aronchick (Microsoft), Amy Unruh (Google), Animesh Singh (IBM), and Ellis Bigelow (Google).
On behalf of the entire community, we are proud to announce Kubeflow 1.0, our first major release. Kubeflow was open sourced at Kubecon USA in December 2017, and during the last two years the Kubeflow Project has grown beyond our wildest expectations. There are now hundreds of contributors from over 30 participating organizations.
Kubeflow’s goal is to make it easy for machine learning (ML) engineers and data scientists to leverage cloud assets (public or on-premise) for ML workloads. You can use Kubeflow on any Kubernetes-conformant cluster.
With 1.0, we are graduating a core set of stable applications needed to develop, build, train, and deploy models on Kubernetes efficiently. (Read more in Kubeflow’s versioning policies and application requirements for graduation.)
Graduating applications include:
Hear more about Kubeflow’s mission and 1.0 release in this interview with Kubeflow founder and core contributor Jeremy Lewi on the Kubernetes Podcast.
With Kubeflow 1.0, users can use Jupyter to develop models. They can then use Kubeflow tools like fairing (Kubeflow’s python SDK) to build containers and create Kubernetes resources to train their models. Once they have a model, they can use KFServing to create and deploy a server for inference.
Kubernetes is an amazing platform for leveraging infrastructure (whether on public cloud or on-premises), but deploying Kubernetes optimized for ML and integrated with your cloud is no easy task. With 1.0 we are providing a CLI and configuration files so you can deploy Kubeflow with one command:
In Kubeflow’s user surveys, data scientists have consistently expressed the importance of Jupyter notebooks. Further, they need the ability to integrate isolated Jupyter notebooks with the efficiencies of Kubernetes on Cloud to train larger models using GPUs and run multiple experiments in parallel. Kubeflow makes it easy to leverage Kubernetes for resource management and put the full power of your datacenter at the fingertips of your data scientist.
With Kubeflow, each data scientist or team can be given their own namespace in which to run their workloads. Namespaces provide security and resource isolation. Using Kubernetes resource quotas, platform administrators can easily limit how much resources an individual or team can consume to ensure fair scheduling.
After deploying Kubeflow, users can leverage Kubeflow’s central dashboard for launching notebooks:
In the Kubeflow UI users can easily launch new notebooks by choosing one of the pre-built docker images for Jupyter or entering the URL of a custom image. Next, users can set how many CPUs and GPUs to attach to their notebook. Notebooks can also include configuration and secrets parameters which simplify access to external repositories and databases.
Distributed training is the norm at Google (blog), and one of the most exciting and requested features for deep learning frameworks like TensorFlow and PyTorch.
When we started Kubeflow, one of our key motivations was to leverage Kubernetes to simplify distributed training. Kubeflow provides Kubernetes custom resources that make distributed training with TensorFlow and PyTorch simple. All a user needs to do is define a TFJob or PyTorch resource like the one illustrated below. The custom controller takes care of spinning up and managing all of the individual processes and configuring them to talk to one another:
To train high quality models, data scientists need to debug and monitor the training process with tools like Tensorboard. With Kubernetes and Kubeflow, userscan easily deploy TensorBoard on their Kubernetes cluster by creating YAML files like the ones below. When deploying TensorBoard on Kubeflow, users can take advantage of Kubeflow’s AuthN and AuthZ integration to securely access TensorBoard behind Kubeflow’s ingress on public clouds:
No need to `kubectl port-forward` to individual pods.
KFServing is a custom resource built on top of Knative for deploying and managing ML models. KFServing offers the following capabilities not provided by lower level primitives (e.g. Deployment):
Below is an example of a KFServing spec showing how a model can be deployed. All a user has to do is provide the URI of their model file using storageUri:
Check out the samples to learn how to use the above capabilities.
A model gathering dust in object storage isn’t doing your organization any good. To put ML to work, you typically need to incorporate that model into an application – whether it’s a web application, mobile app, or part of some backend reporting pipeline.
Frameworks like flask and bootstrap make it easy for data scientists to create rich, visually appealing web applications that put their models to work. Below is a screenshot of the UI we built for Kubeflow’s mnist example.
With Kubeflow, there is no need for data scientists to learn new concepts or platforms to deploy their applications, or to deal with ingress, networking certificates, etc. They can deploy their application just like TensorBoard; the only thing that changes is the Docker image and flags.
If this sounds like just what you are looking for we recommend:
1. Visiting our docs to learn how to deploy Kubeflow on your public or private cloud.
2. Walking through the mnist tutorial to try our core applications yourself.
There’s much more to Kubeflow than what we’ve covered in this blog post. In addition to the applications listed here, we have a number of applications under development:
In future releases we will be graduating these applications to 1.0.
All this would be nothing without feedback from and collaboration with our users. Some feedback from people using Kubeflow in production include:
“The Kubeflow 1.0 release is a significant milestone as it positions Kubeflow to be a viable ML Enterprise platform. Kubeflow 1.0 delivers material productivity enhancements for ML researchers.” — Jeff Fogarty, AVP ML / Cloud Engineer, US Bank
“Kubeflow’s data and model storage allows for smooth integration into CI/CD processes, allowing for a much faster and more agile delivery of machine learning models into applications.” — Laura Schornack, Shared Services Architect, Chase Commercial Bank
“With the launch of Kubeflow 1.0 we now have a feature complete end-to-end open source machine learning platform, allowing everyone from small teams to large unicorns like Gojek to run ML at scale.” — Willem Pienaar, Engineering Lead, Data Science Platform, GoJek
“Kubeflow provides a seamless interface to a great set of tools that together manages the complexity of ML workflows and encourages best practices. The Data Science and Machine Learning teams at Volvo Cars are able to iterate and deliver reproducible, production grade services with ease.”— Leonard Aukea, Volvo Cars
“With Kubeflow at the heart of our ML platform, our small company has been able to stack models in production to improve CR, find new customers, and present the right product to the right customer at the right time.” — Senior Director, One Technologies
“Kubeflow is helping GroupBy in standardizing ML workflows and simplifying very complicated deployments!” — Mohamed Elsaied, Machine Learning Team Lead, GroupBy
None of this would have been possible without the tens of organizations and hundreds of individuals that have been developing, testing, and evangelizing Kubeflow.
We could not have achieved our milestone without an incredibly active community. Please come aboard!
Thank you all so much — onward!
Official Kubeflow Blog.
484 
2
484 claps
484 
2
Written by
open source strategy @kubeflow, formerly @apollographql @docker & @newrelic. makes a mean frittata.
Official Kubeflow Blog.
Written by
open source strategy @kubeflow, formerly @apollographql @docker & @newrelic. makes a mean frittata.
Official Kubeflow Blog.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
Top highlight
If you are familiar with satellite imagery you’ve likely heard that we are entering a “golden era” of Earth Observation. It’s true! New satellites are generating Petabyte-scale publicly available archives of imagery at unprecedented rates, enabling new insights and fast global impacts.
This deluge of imagery is forcing scientists to reconsider traditional workflows of downloading thousands of image files to work on a personal computer. The alternative is a “Cloud Native” approach - forgo downloading data and operate on the data where it is stored on the Cloud. This approach has the major advantage of being able to utilize vastly scalable computing resources to crop, transform, and apply algorithms to imagery very quickly — Quickly enough to enable interactive analysis at full resolution over the entire globe.
For scientists this scalability and interactivity is fundamental to the process of discovery. A few years ago a seminal paper was published that quantified global deforestation using the entire Landsat archive using a Cloud Native approach (Hansen et. al. 2013). This paper was truly inspirational, demonstrating that questions of global scope are not limited to a select few with access to supercomputers. And in the last several years, Cloud Native tools for scientific research have been growing rapidly. In this article we will highlight a number of these tools, but focus on the Pangeo project: “A community platform for Big Data geoscience”.
We’ve developed an example Cloud Native quantitative analysis of Landsat 8 satellite imagery. What is special about this example is that the analysis is easily reproduced, scalable, and interactive: 100 Gigabytes of Landsat 8 images covering Washington State (representing the entire archive back to 2013–03–21) are found using NASA’s Common Metadata Repository (CMR). Then, using URLs instead of local file paths, the Normalized Difference Vegetation Index (NDVI), a simple landcover classification algorithm, is run in seconds on a Cloud-based cluster. Compare this to a traditional workflow, in which a scientist must wait hours or days to download and decompress 100 scenes from a USGS server, then run analysis locally.
One of the hallmark features of the Pangeo project is a community-developed JupyterHub instance running on Google Cloud with a preconfigured Python environment and Kubernetes cluster. This environment can be customized and launched with the click of a button using Binder, allowing anyone to run Python code interactively in a web browser. A more detailed blog post on the implementation of Pangeo’s Binder instance can be found can be found here. And you can interactively run the full Landsat example simply by clicking the button below!
One very special feature of the Landsat example is that computations are done in parallel and on-the-fly, made possible by several independently developed Python packages (xarray, rasterio, dask, holoviews) coming together with magical results! For example, take a look at the following screencast, which demonstrates dynamically computing NDVI for selected dates at a resolution suitable to the current zoom level.
You can also easily extract a time series for a particular pixel or patch. Interested in a different region, different index, or color scale? The example can easily be modified and run and code, graphs, and images saved to your local computer for future use.
One appealing feature of the Landsat example is that the a user needs only familiarity with Python, which has become one of the most pervasive programming languages in the scientific community. Parallel computation and memory management are taken care of by Dask behind the scenes. Rasterio and Xarray know how to pull down chunks of full resolution images, and Dask knows how to distribute computations that use those chunks. What’s more, if local memory is exceeded, data is written and read from disk, allowing for computations to be run without fear of “out of memory” errors.
However, this workflow would not be possible if the images weren’t stored in a format amenable to Cloud Native analysis. In the Earth Observation community there is a lot of excitement surrounding Cloud-Optimized Geotiffs (COGs) which are described in detail on https://www.cogeo.org, and well-advocated for in a series of blog posts by Chris Holmes (start here). In brief, COGs are Geotiff files that have internally organized overviews and image tiles and support HTTP range requests (enabling downloading of specific tiles rather than the full file). COGs are also nice because they work normally in GIS software such as QGIS. Strictly speaking the Landsat 8 images on Google Cloud are not in the COG format because they do not include built in overviews, but critically HTTP range requests still work.
Our Landsat example is not necessarily optimized in terms of computational efficiency. One simple way to speed up the analysis would be to work with “Analysis Ready Data”: At a basic level, images with the same dimensions that are aligned to the same coordinate grid, such that chunks are uniform and retrieved efficiently. The USGS has created such an archive for Landsat 8, but it is not available on a public Cloud.
For now, the reality is that most Earth Observation data is not stored on the Cloud, and of the data that is, much of it is not in a format amenable to Cloud Native workflows. There are innovative solutions using on-demand format conversion, such as the amazing GOES-16 data staging tool created by Element 84. Nevertheless, we hope that as NASA moves public archives to AWS, Cloud Native formats will be used and will lead to rapid and exciting new discoveries!
The Landsat 8 analysis described here might remind you of Google Earth Engine (GEE). GEE is a tremendous community resource and has a fantastic user interface. But what if you are not accustomed to programing in Javascript? Or, what if you want to run computations on the archive of Sentinel-1 SLC data stored only on Amazon Web Services (AWS)? Or a custom Terabyte-scale dataset on your own server?
There are many “platforms” currently under development that are designed to harness the power of commercial Cloud compute resources for scalable and fast analysis of Earth Observation data: Raster Foundry, EOBrowser, GBDX Notebooks to name a few. What distinguishes Pangeo from these platforms is that Pangeo is based purely on general purpose, open source, community based tools. Since these tools are designed well, they combine easily into something that is greater than the sum of the parts. It’s important to acknowledge that while the constituent tools are open and free, running analyses on the commercial Cloud is not. This is why some platforms charge hefty subscription fees to use their services. For now, Pangeo is generously supported by grants from the National Science Foundation and NASA which include credits on Google Cloud Platform .
In order to ensure flexibility and long term sustainability of Cloud Native tools, it is important to focus on Cloud-agnostic tools and recipes. Pangeo is tackling this issue by streamlining deployment with multiple Cloud providers. There are other great efforts on this front, generally spearheaded by academic groups. For example, OpenEO is providing “A Common, Open Source between Earth Observation Data Infrastructures and Front-End Applications”, see here for an excellent description of why this effort is so timely. And it’s worth noting that the Earth Sciences are not the only academic discipline confronting the issue of reproducing large scale analyses on Cloud infrastructure: For example, REANA comes from particle physics analyses and “… helps researchers to structure their input data, analysis code, containerised environments and computational workflows so that the analysis can be instantiated and run on remote compute clouds.”
Large archives of public satellite data on the Cloud can be a tremendous resource for the scientific community. They can also seem at first like being gifted the proverbial white elephant — how can researchers manage and conduct reproducible research with 45Tb of Landsat data? Fortunately, tools are emerging that enable researchers to make the most of Earth Observation data.
Pangeo is an exciting resource for Earth Observation because it is a collection of free and open cutting-edge computational resources geared toward Earth Scientists. It is also a collaborative community of scientists and developers who are eager to discuss these resources and continue to advance them. We hope this article and example analysis have piqued your interest in the Pangeo project. If you’d like to get involved, please visit the Pangeo website, or make direct contributions on the project GitHub repository.
This blog post and the Landsat analysis example was a team effort with important contributions from Daniel Rothenberg, Matthew Rocklin, Ryan Abernathey, Joe Hamman, Rich Signell, and Rob Fatland. Landsat data is made publicly available by U.S. Geological Survey.
The Pangeo project is currently funded through grants from the National Science Foundation and the National Aeronautics and Space Administration (NASA) . Google provides compute credits on Google Cloud Platform.
A community platform for big data geoscience
442 
1
Thanks to Joe Hamman and Ryan Abernathey. 
442 claps
442 
1
Written by
Research geophysicist at University of Washington eScience Institute
A community platform for big data geoscience
Written by
Research geophysicist at University of Washington eScience Institute
A community platform for big data geoscience
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/cloud-native-java-vs-golang-2a72c0531b05?source=search_post---------7,"There are currently no responses for this story.
Be the first to respond.
Java once-famous motto: “Write once and run everywhere” is pretty much obsolete these days, the only place we want to run code is inside a container. Where a “Just in time” compiler does not make any sense.
For this reason, probably, the Java ecosystem is in the midst of its transformation in order to become better suited for the cloud. Oracle’s GraalVm allows compiling byte code to Linux executables (ELF) and Rad…
"
https://medium.com/hackernoon/welcome-to-cloud-native-development-with-aws-cloud9-aws-codestar-c7b6536afba8?source=search_post---------8,"There are currently no responses for this story.
Be the first to respond.
I have been experimenting with AWS Cloud9 since Werner Vogels announced it a few weeks ago at AWS re:Invent 2017 (keynote video here).
This article is the paraphrased version of my talk AWS Cloud9 & CodeStar for Serverless Apps, given at the AWS User Group in Milan on Jan 10th.
I am going to skip the “Serverless Disclaimer” section of my deck.If you are not familiar with Serverless, please have a look here, or here, or here, or here, or here, or here, or here.
If you are familiar with Serverless and you don’t like it, you may still enjoy this article and the benefits of Cloud9 and CodeStar. Just make sure you mentally replace “FaaS” with “Container”, and “SAM” with “CloudFormation” :)
AWS Cloud 9 is a “cloud IDE” for writing, running, and debugging code.
I’d start saying that most IDEs are fantastic tools to boost your productivity and the quality of your code if you can use them and invested a few months/years in learning them properly.
That being said, some IDEs offer more advanced and useful features than others (please don’t take it personally, unless you code in Word or Notepad).
AWS acquired Cloud9 in July 2016, which has now been rebranded as AWS Cloud9. Even though it looked brilliant on Werner’s browser, my first impression during the keynote was something along the lines of “Why should I pay to write code?”, immediately followed by “Does that mean I cannot code when I’m offline?”. Like most software engineers, I’ve used many IDEs in the last ten years, for free, and I’m used to coding a lot while I’m traveling.
Apparently, I was not alone, and many developers asked the same questions during my presentation. So let me briefly recap my arguments.
Regarding the cost, I believe it’s pretty much negligible for most organizations that already use AWS heavily (less than $2 per month for a t2.micro environment, 8 hours a day, 20 days a month). And without considering AWS Free Tier and automatic cost-saving settings (hibernation after 30min).
The “no coding offline” drawback is much harder to defend, but let me try.
Unless you are going for a hardcore debugging session over a well-established project, can you really code for more than 30min when you are offline? Can you git clone or git pull anything useful? Can you npm install or pip install the modules you need? Can you mock or ignore all the third-party services and APIs your application consumes? Can you avoid googling around your favorite framework’s documentation?
Sure, you could prepare for a 12h flight and download/install everything you need in advance. But how often does that happen? Simply put, I’ve seen the best developers and engineers give up and take a break when the network goes down.
On the other hand, AWS Cloud9 offers you a better alternative for when your machine gives up :) I could throw my dev machine out of the window right now, switch to my colleague’s notebook, login into my AWS Account and keep working on the very same Cloud9 session (which is saved and stored server-side). That means you could as well use a much cheaper machine such as a Chromebook or a tablet (or a phone?). Well, you could be 100% operational using the random machine of an internet cafe, or your grandmother’s computer :)
Of course, there are always exceptions, and I’ll make sure I’m ready to use my local IDE when Cloud9 is not an option. In the meantime, I hope AWS will work on some sort of client-side offline support (and maybe turn Cloud9 into a progressive web app?).
I think AWS Cloud9 solves a bunch of problems for the many organizations currently trying to set up elaborate stacks of tools on every developer’s machine, especially if the team is heterogeneous and/or distributed.
Let’s recap some of its features:
I took the following screenshot during a live-debugging session of a very simple Lambda Function (note: I also spent 3 minutes customising theme & layout, according to my taste and needs).
I do have a few wishes for AWS Cloud9, and I’ve shared a few of them on Twitter (tweets below).
Let me discuss a few of them:
AWS CodeStar (aka Code-*) is a sort of “catch-all service” for the ever-expanding suite of tools for developers.It is a free service that lets you manage and link together services such as CodeCommit, CodeBuild, CodePipeline, CodeDeploy, Lambda, EC2, Elastic Beanstalk, CloudFormation, Cloud9, etc.
One of my 2018 new year resolutions is to use more memes in my decks (until someone decides to stop me, for some reason), so here’s how I presented some of the pain points that CodeStar can solve.
Data-driven parenthesis: I can statistically confirm that the JIRA meme generated 42% more laughs than all others combined.
CodeStar may not be the best fit for every project/organization, especially the most experienced and advanced ones, but it definitely provides some very good defaults to get started with. It’s worth noting that CodeStar is 100% free, and you only pay for the resources it will spin up on your behalf.
Let’s recap its features:
CodeStar can look like magic if you’ve never played with CodePipeline and CodeBuild, but unfortunately it’s not perfect yet. I’ve shared a few “wishes” on Twitter too (tweets below), and here’s a quick recap of what I’ve found.
Cloud9 and CodeStar are pretty cool services on their own, and I was excited to see how they’ve been integrated. Or, better, how Cloud9 has been integrated into CodeStar.
You can associate AWS Cloud9 Environments to your CodeStar project natively. In case multiple developers are working on the same project, you can create and assign a Cloud9 Environment to each developer (eventually, they’ll collaborate and invite each other, if needed).
Once you open Cloud9, you’ll find your IAM credentials integrated with git (which does require some work) and your CodeCommit repository already cloned for you.
Unfortunately, this magic doesn’t happen if you choose GitHub (for now?).
As a couple of friends and colleagues pointed out, it’s not such a critical or technical complex integration, in the sense that you could have taken care of it yourself (as you’d do on your local machine). But I think it’s a great way to streamline the development experience and reduce the margin for error, especially when you work on multiple projects and multiple accounts.
For example, most developers make great use of AWS profiles when working on their local machine, and some of them also manage to remember which profile can do what, in which account, etc. With CodeStar+Cloud9 you won’t care anymore about profiles or local credentials since every Cloud9 environment is bound to a specific project and account. Also, since CI/CD is enabled by default, most of the time you will just write code, test with sam-local and git push 💛
Of course, you may also have a generic Cloud9 Environment (i.e. not related to a specific project) and use it with multiple profiles to manage unique resources or prototype new stuff.
I decided to conclude my presentation with a brief parenthesis about AWS SAM, which got a few mentions and therefore deserves some context.
** Serverless alert **
SAM stands for Serverless Application Model, and it’s an open specification whose goal is to offer a standard way to define serverless applications.
Technically speaking, it’s a CloudFormation Transform named AWS::Serverless that will convert special Serverless resources such as AWS::Serverless::Function into standard CloudFormation syntax.
You can think of Transforms as a way to augment the expressiveness of CloudFormation templates so that you can define complex resources and their relationships in a much more concise way.
If you are familiar with other tools such as the Serverless Framework, you’ll notice that the syntax is quite similar (there is even a plugin to convert your templates to SAM).
In fact, you can deploy SAM templates with AWS SAM Local, a CLI tool for local development written in Go and officially released by AWS.
You can use AWS SAM Local to test your Lambda Functions locally and emulate API Gateway endpoints too. The CLI tool is available by default on every Cloud9 EC2 Environment, and the UI already supports some of its functionalities.
I have only one wish for AWS SAM: I would love to see more transparency and documentation related to the AWS::Serverless Transform.
Apr 2018 Update: AWS SAM is now open-source on GitHub!
And since I like dreaming, why not allowing custom CloudFormation Transforms too? I am almost ready to bet they are implemented with some kind of Lambda hook, and I can’t even start to imagine how many great things the community might be able to develop and share that way.
I hope you learned something new about AWS Cloud9 and CodeStar (please don’t confuse them and create weird hybrids such as “CloudStar” as I did a few times). I would recommend building a simple prototype or a sample project on CodeStar asap. You can get started here!
If you got this far, you probably enjoyed the article or feel like sharing your thoughts. Either way, don’t forget to recommend & share, and please do not hesitate to give feedback & share your ideas =)
#BlackLivesMatter
963 
7
963 claps
963 
7
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Italian | Musician | Traveler | Technical Evangelist @ AWS https://aws.amazon.com/developer/community/evangelists/alex-casalboni/ Opinions are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/aws-enterprise-collection/cloud-native-or-lift-and-shift-99970053b25b?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
“There is always tension between the possibilities we aspire to and our wounded memories and past mistakes.” — Sean Brady
I talk to a lot of executives who are debating different migration approaches for the applications in their IT portfolio. While there’s no one-size-fits-all answer to this question, we spend a good deal of time building migration plans with enterprises using a rubric that takes into account their objectives, the age/architecture of their applications, and their constraints. The goal is to help them bucket the applications in their portfolio into one of 6 Migration Strategies.
In some cases, the choices are obvious. We see a lot of organizations migrating their back-office technology and end-user computing applications to an as-a-service model (“re-purchasing” toward vendors like Salesforce and Workday); a number of organizations will look for opportunities to retire systems that are no longer in use; and some organizations will choose to later revisit systems that they don’t feel they have the appetite or capabilities to migrate yet (i.e. the Mainframe, though You Can Migrate Your Mainframe to the Cloud).
In other cases, the approach isn’t so obvious. In my previous post, I touched on the tension between re-architecting and rehosting (a.k.a. “lift-and-shift”). I’ve heard a lot of executives — including myself, before I learned better — suggest that they’re only moving to the cloud if they “do it right,” which usually means migrating to a cloud-native architecture. Other executives are biased toward a rehosting strategy because they have a compelling reason to migrate quickly (for example, a data center lease expiry), want to avoid a costly refresh cycle, or simply need a quick budget win, which tends to be in the neighborhood of 30% when you’re honest about your on-premises TCO.
Somewhere in the middle of rehosting and re-architecting is what we call re-platforming, where you’re not spending the time on a complete re-architecture, but, rather, making some adjustments to take advantage of cloud-native features or otherwise optimize the application. This common middle ground includes right-sizing instances using realistic capacity scenarios that can easily be scaled up rather than overbought, or moving from a pay-for product like WebLogic to an open-source alternative like Tomcat.
So which approach is more often right for your organization?
Without talking to you about your specific opportunities and constraints (which I’m happy to do; just drop me a note), it’s hard to give a definitive answer, but I can highlight a few anecdotes that should help shape your perspective.
The first is a quote from Yury Izrailevsky’s blog. Yury is the Vice President of Cloud and Platform Engineering at Netflix, and is a well-respected thought leader in our industry.
“Our journey to the cloud at Netflix began in August of 2008, when we experienced a major database corruption and for three days could not ship DVDs to our members. That is when we realized that we had to move away from vertically scaled single points of failure, like relational databases in our datacenter, towards highly reliable, horizontally scalable, distributed systems in the cloud. We chose Amazon Web Services (AWS) as our cloud provider because it provided us with the greatest scale and the broadest set of services and features. The majority of our systems, including all customer-facing services, had been migrated to the cloud prior to 2015. Since then, we’ve been taking the time necessary to figure out a secure and durable cloud path for our billing infrastructure as well as all aspects of our customer and employee data management. We are happy to report that in early January, 2016, after seven years of diligent effort, we have finally completed our cloud migration and shut down the last remaining data center bits used by our streaming service!
Given the obvious benefits of the cloud, why did it take us a full seven years to complete the migration? The truth is, moving to the cloud was a lot of hard work, and we had to make a number of difficult choices along the way. Arguably, the easiest way to move to the cloud is to forklift all of the systems, unchanged, out of the data center and drop them in AWS. But in doing so, you end up moving all the problems and limitations of the data center along with it. Instead, we chose the cloud-native approach, rebuilding virtually all of our technology and fundamentally changing the way we operate the company … Many new systems had to be built, and new skills learned. It took time and effort to transform Netflix into a cloud-native company, but it put us in a much better position to continue to grow and become a global TV network.”
Yury’s experience is both instructive and inspirational, and I’m certain that Netflix’s re-architecting approach was right for them.
But most enterprises aren’t Netflix, and many will have different drivers for their migration.
When I was the CIO at Dow Jones several years ago, we initially subscribed to the ivory tower attitude that everything we migrated needed to be re-architected, and we had a relentless focus on automation and cloud-native features. That worked fine until we had to vacate one of our data centers in less than 2 months. We re-hosted most of what was in that data center into AWS, and sprinkled in a little re-platforming where we could to make some small optimizations but still meet our time constraint. One could argue that we would not have been able to do this migration that quickly if we didn’t already have the experience leading up to it, but no one could argue with the results. We reduced our costs by more than 25%. This experience led to a business case to save or reallocate more than $100 million in costs across all of News Corp (our parent company) by migrating 75% of our applications to the cloud as we consolidated 56 data centers into 6.
GE Oil & Gas rehosted hundreds of applications to the cloud as part of a major digital overhaul. In the process, they reduced their TCO by 52%. Ben Cabanas, one of GE’s most forward-thinking technology executives, told me a story that was similar to mine — they initially thought they’d re-architect everything, but soon realized that would take too long, and that they could learn and save a lot by rehosting first.
One of my favorite pun-intended quotes comes from Nike’s Global CIO, Jim Scholefield, who told us that “Sometimes, I tell the team to just move it.”
Cynics might say that rehosting is simply “your mess for less,” but I think there’s more to it than that. I’d boil the advantage of rehosting down to 2 key points (I’m sure there are others; please write about them and we’ll post your story…) —
First, rehosting takes a lot less time, particularly when automated, and typically yields a TCO savings in the neighborhood of 30%. As you learn from experience, you’ll be able to increase that savings through simple replatforming techniques, like instance right-sizing and open source alternatives. Your mileage on the savings may vary, depending on your internal IT costs and how honest you are about them.
Second, it becomes easier to re-architect and constantly reinvent your applications once they’re running in the cloud. This is partly because of the obvious toolchain integration, and partly because your people will learn an awful lot about what cloud-native architectures should look like through rehosting. One customer we worked with rehosted one of its primary customer-facing applications in a few months to achieve a 30% TCO reduction, then re-architected to a serverless architecture to gain another 80% TCO reduction!
Re-architecting takes longer, but it can be a very effective way for an enterprise to re-boot its culture and, if your application is a good product-market fit, can lead to a healthy ROI. Most importantly, however, re-architecting can set the stage for years and years of continual reinvention that boosts business performance in even the most competitive markets.
While I still believe there’s no one-size-fits-all answer, I’d summarize by suggesting that you look to re-architect the applications where you know you need to add business capabilities that a cloud-native architecture can help you achieve (performance, scalability, globality, moving to a DevOps or agile model), and that you look to rehost or re-platform the steady-state applications that you aren’t otherwise going to repurchase, retire, or revisit. Either migration path paves the way for constant reinvention.
What’s your experience been?
Keep building,- Stephenorbans@amazon.com@stephenorbanRead My Book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT
Note: “Reinvention” is the fourth (and never-ending) stage of adoption I’m writing about in the Journey to Cloud-First series. The first stage is “Project,” the second stage is “Foundation,” and the third is “Migration.” This series follows the best practices I’ve outlined in An E-Book of Cloud Best Practices for Your Enterprise. Stay tuned for more posts in this series.
Both of these series are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT.
Tales of AWS in the Enterprise
226 
2
226 claps
226 
2
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Electronic_Arts/announcing-project-atlas-a-vision-for-a-cloud-native-gaming-future-d58ef77f74db?source=search_post---------10,"Sign in
There are currently no responses for this story.
Be the first to respond.
Electronic Arts
Oct 30, 2018·15 min read
By Ken Moss, Chief Technology Officer at Electronic Arts
I think I have the best job in the world. Growing up, I spent my spare time and spare change at my neighborhood arcade. That’s also where I found a great group of friends. At 13, I programmed my first video game. At 15, I sold my first game on a cassette tape on a Commodore 64. Four years ago, I joined Electronic Arts as the CTO, and two years ago, I took over leadership of our Frostbite engine. My job is to create the technology that powers the games that…
"
https://eng.lifion.com/going-cloud-native-2dc748c0fbcf?source=search_post---------11,"It may come as no surprise for any software engineer to hear this, but managed services offered by today’s cloud providers render significant benefits to engineering organizations. These cloud providers have developed several different solutions to help organizations scale both vertically and/or horizontally across the globe, while in several cases, being at a lower cost to any institution managing their own infrastructure. On top of lower cost and overall ease of scalability, cloud providers guarantee several out-of-the-box freebies that would normally need to be discussed, addressed, and actively monitored if an organization managed their servers solely on their own.
While the benefits are clear, the plethora of Cloud-Native database options makes it difficult to select the right one for your organization’s use case. Here at Lifion by ADP we’re building a global scale platform with a massive suite of products. We use multiple types of data stores dependent on the data and its use case in our platform, such as key-value, relational, graph, document stores, etc.... As we continue to scale our platform and our customer base we found ourselves with a business need to move to a Cloud-Native solution for a core data store that is heavily and frequently used whenever end-users interact with the system. We strive to be thoughtful and systematic about our decisions and designs, especially when changing such a critical piece of our architecture. This meant carefully identifying our requirements, enumerating our possible solution options, and making sure our approach to selecting the correct one was diligent, strategic, and evidence-based. We’re firm believers in using “the right tool for the job” when it comes to building our technology stack. Given we strategically use Amazon Web Services for hosting our platform we intentionally did not consider alternative non-AWS cloud-native options. In this article, my team, who’s responsible for the Metadata Engine and Architecture of our platform, shares how we went about this exercise and explains our findings.
After having several stakeholder discussions and identifying areas that could be improved upon within our current paradigm, we came up with the following overarching requirements.
The team identified a strategy to consistently test across the board with a goal of benchmarking only read speeds of JSON payloads per data store. We developed a proof of concept, and ran those equivalent tests against each database conforming to the following parameters:
Based on the results of our benchmarking strategy outlined above, we compiled a pros and cons list for each solution, which we’ve captured below. Expanded graphs with full document sizes and all requests, specified in our benchmarking strategy above, can be seen in “For The Curious” below. The graphs in “Final Outcome” are zoomed for ease of visualization.
https://aws.amazon.com/s3/
No, Amazon S3 does not meet our requirements, due to its slowness and variability of response time. However, we have several internal networking layers to control traffic in our VPCs, which could have potentially hurt response times during our testing. Since it can give the most stable cache by its high durability and availability, it is possible it could address our use case in conjunction with another caching layer.
https://aws.amazon.com/rds/aurora/details/mysql-details/
Yes, Aurora (MySQL) does meet our requirements, in most cases. There is a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities, etc. however we may not need all of these features for our use case.
https://aws.amazon.com/rds/aurora/details/postgresql-details/
Yes, Aurora (PostgreSQL) does meet our requirements. Amazon Aurora (PostgreSQL), like the MySQL implementation, has a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities and so on, but again we may not need all of these features for our use case.
https://aws.amazon.com/elasticache/redis/
Yes, ElastiCache for Redis meets our requirements. ElastiCache For Redis does not persist documents by default, however, automatic daily backups can be enabled. This automatic backup stores snapshots within S3, up to 35 days. With Redis, we can get extremely fast response times, making this one of the best options for our use case.
https://aws.amazon.com/elasticache/memcached/
Yes, ElastiCache for Memcached meets our requirements in all the same ways in which Redis does. The Memcached implementation is missing several out-of-the-box features that the Redis implementation offers. Some features missing include snapshots, replications, lua scripting and advanced data structure support.
Below are additional graphs as measured during our tests: color indicates JSON payload size, Y-Axis measures time, and X-Axis represents number of requests. Each graph is the expanded version of those graphs highlighted above. Again, the following benchmarks only tested the read speeds of JSON payloads per data store.
Based on our tests/learnings from developing our proof of concept, we concluded that S3 is too slow for our use case, confirming our theory that S3 fits better for latency-insensitive, high-reliability needs such as for file storage or serving. Both implementations of Aurora seem like good choices because they provide persistent storage by default, and may be good options for some other use cases we have internally. However, we may not need to pay the extra cost of persistent storage and large scale transactions. Upon further investigation we also found with the Aurora options we’d need to run a larger instance size than we would if using ElastiCache, which would cost considerably more. If you could avoid paying extra cost, on top of using a faster solution, wouldn’t you? Hello, ElastiCache.
Once we had narrowed down to ElastiCache for our approach the next question we had to ask ourselves was which implementation? This is a question we debated internally, and it ultimately boiled down to the following differences we saw between Redis and Memcached. With that said, here are some of the differences we identified that were key to our use case between Redis and Memcached:
ElastiCache for Memcached
ElastiCache for Redis
Knowing the result of our findings would become a very important piece of the Lifion platform undoubtedly fueled us to take several extra steps in our due diligence, while enjoying the impact of our work. Keep in mind these findings were pivotal for our specific use case, and might not hold true in all scenarios, as most questions posed within Computer Science are rebutted with: “it depends”.
Plain and simple, our final decision boiled down to our global scalability requirements. Built-in replication, as well as snapshots in cases of disaster recovery, were what eliminated the Memcached implementation. Sure, we could have implemented our own solutions to fill the void, but why not take advantage of already optimized systems? Our needs are global, and having these optimized solutions provided by Amazon were truly the differentiator. Not to mention, Redis read speeds, based on most of our benchmarks, were also slightly faster. So, at the end of the day, it’s all about choosing the “right tool for the right job”.
Credited Platform Engineers
All things tech at Lifion by ADP
4.1K 
Thanks to Tom Rogers and Aubrie-Ann Jones. 
4.1K claps
4.1K 
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
"
https://itnext.io/introduction-to-quarkus-cloud-native-java-apps-e205ae702762?source=search_post---------12,"This is your last free member-only story this month. Sign up for Medium and get an extra one
Java and the JVM are still the most popular programming languages but for Serverless and Cloud Native Microservices, its usage is declining due to the heavy memory footprint and slow boot time required for short lived containers; this is now about to change thanks to Quarkus.
As I get more into DevOps,Containers and Serverless; I find myself writing my containerized code in lightweight containers or FaaS using Python or JavaScript. Java it is just too slow to boot up to use in a Serverless framework and for microservices JavaScript or Python have faster boot times and smaller container size than Java making them more efficient.
Java is more than 20 years old, the world back them was vastly different from what is now. The JVM solved a huge problem and allowed us to write code once and run it in multiple platforms and operating systems. With Containers we can now package our apps, libs and OS resources into a single container that can run anywhere. The JVM portability is now less relevant. Back in the day, the extra overhead was a cost that we were willing to pay for portability, but not anymore. Now we want fast, low latency and reactive applications that are always available. Containers and container orchestration tools like Kubernetes provide this independently of the programming language.
As companies migrate to microservices, they take their Spring Java services, bundle them into a fat jars, add the JDK and run it in a Linux based container. This solution works but you have to manage heavy weight containers that are 500MB in size and take 10 to 30 seconds to be available; and this is a problem. Many companies after migrating, they slowly move to Python or Java for their backend services; and eventually, to FaaS. Serverless and FaaS are now very popular because allow us to focus on writing functions without worrying about the infrastructure. They still run inside containers but the cloud provider manage its life cycle. The neat thing is that after certain time, the cloud provider will kill completely the container and start it again on the next call, so you only pay for the usage. The first call to a function may take a bit longer, this is the famous cold start. This is happens because the container needs to boot up. With Python or JavaScript this is not a huge problem, but for Java this could be 10–15 seconds which is a deal breaker and the cause of the decline in Java. Now we need code that can run, do its job and then stop. We don’t want multiple threads or long running processes, we want short lived processes that can boot very quickly.
If you read tech blogs or follow the news, you may think that Serverless is eating the world, everybody is super excited about it. Startups can now write functions as a service in the cloud using JavaScript and scale them to support millions of users without having to manage any infrastructure. Then, you have the real word, outside Silicon Valley; financial institutions, government, retail and many other industries which have millions of lines of code written in Java which they cannot afford to rewrite, so they kind of accept the fact that they need to live with heavy weight containers.
GraalVM and specifically Substrate VM are now opening the door for a bright and long future for the Java language. GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala or Kotlin. The cool thing is that GraalVM allows you to compile your programs ahead-of-time(AOT) into a native executable. This means, that you can compile your Java code directly into a machine specific code. The resulting program does not run on the Java HotSpot VM, but uses necessary components like memory management, thread scheduling from a different implementation of a virtual machine, called Substrate VM. Substrate VM is written in Java and compiled into the native executable. The resulting program has faster startup time and lower runtime memory overhead compared to a Java VM. This is great, but you may be thinking, AOT? this is against the whole idea of having the JVM, this is Java code that I cannot run anywhere!, this is crazy!!!. But think about it, we have containers now, we don’t need the JVM. The common container based Spring boot apps have an extra level of abstraction which is completely unnecessary in the world of Kubernetes. You have a Java app running on JVM inside a container, this container will never change, since the deliverable nowadays is the container not the app, you package containers and not WARs anymore. So, all the overhead of running the app in a JVM inside a container is useless, so AOT makes perfect sense if you are going to package your apps in containers.
However, the dynamic nature of Java is severely constrained by AOT compiling (class loading at runtime, reflection, proxies, etc). In practice, it means 90% of the Java ecosystem does not work without change. So the Java ecosystem must adapt. The good news is that we can do most of them at build time!
This is the power of Quarkus. It uses GraalVM and provides an ecosystem that supports AOT at build time, so you can create native binaries using Java. Quarkus makes GraalVM usable for Java developers!
As explained above, Quarkus provides ahead of time compiling for Java applications creating a Supersonic Subatomic Java ecosystem that has a extremely small size and super fast boot time, bringing Java back in the game for cloud native development. I have never been as excited about a new technology in years, and I’m not the only one.
Try the getting started guide to see it for yourself. Follow the 3 guides to see the power of Quarkus and mesure the boot time. There are still thousands of companies are there that use Java+JPA, inside a container, this can take 15 seconds to boot, in Quarkus 0.005!
You use the same IDE and tooling as you are used to in the Spring Boot world. You use Maven or Gradle to build your project. You can run it directly in the IDE, and on top of that, you have hot live reload of any changes, no need to restart the app. Quarkus it is not Spring, so if you are using Spring Boot you will need to migrate the Spring specific code, fortunately, Quarkus comes with a Spring DI compatibility layer that makes this very easy. The Quarkus framework is based on standards which means that the code will be portable and easy to maintain.
Quarkus can run in dev mode which is similar to Spring Boot, you can also package you project into fat jars. This is great to test your code and debug since it support live reload; but for prod ahead of time compiling is required. The diagram below shows the process:
Quarkus has a lot more features that just native Java code.
In short, now you can run your traditional JPA/JTA transactional services in super fast lightweight containers in any environment, cloud or on-prem.
In this section, lets simplify the getting started guide to get an idea of the power of Quarkus.
The easiest way to create a new Quarkus project is to open a terminal and run the following command:
It generates a Maven project with a GreetingResuce exposing /hello end point. It also generates a Dockerfile for native and jvm(fat jar traditional images) docker images. The code is very clean and simple:
To run our application. Use: ./mvnw compile quarkus:dev
The application is packaged using ./mvnw package. It produces 2 jar files:
You can run the application using: java -jar target/getting-started-1.0-SNAPSHOT-runner.jar
Then, you need to download and install GraalVM and set GRAALVM_HOME environment variable.
Now you can create a native executable using: ./mvnw package -Pnative -Dnative-image.docker-build=true.
The create the Docker image: docker build -f src/main/docker/Dockerfile.native -t quarkus-quickstart/quickstart .
Now you can run it in any container orchectration engine, in case you use minishift:
And that’s it!; you have a container with a Java REST service booting up in 0.004 seconds!
I’m really excited about Quarkus which is supported by Red Hat and has gotten lots of attention since its release just a week ago. I do believe it is going to change the Java landscape and make possible a real migration to the cloid for regular enterprises.
Kubernetes + Knative + Quarkus are a game changer for cloud native development and a joy for any Java developer.
Checkout this GIT Repo with lots of examples!
I hope you enjoyed this post. Feel free to leave a comment or share this post. Follow me for future posts.
ITNEXT is a platform for IT developers & software engineers…
274 
1
274 claps
274 
1
Written by
Certified Java Architect/AWS/GCP/Azure/K8s: Microservices/Docker/Kubernetes, AWS/Serverless/BigData, Kafka/Akka/Spark/AI, JS/React/Angular/PWA @JavierRamosRod
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Certified Java Architect/AWS/GCP/Azure/K8s: Microservices/Docker/Kubernetes, AWS/Serverless/BigData, Kafka/Akka/Spark/AI, JS/React/Angular/PWA @JavierRamosRod
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://faun.pub/approaching-cloud-native-2903a253b8f9?source=search_post---------13,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Continuous development, testing, integration, and delivery are, for sure, amongst the important pillars of DevOps, but if you want to create a healthy DevOps strategy, one of the necessary and vital things to do is using the power of IaC and Cloud Computing.
In other words, becoming “Cloud Native”. This is what we are going to see in the following analysis.
IaC is simply managing hardware and physical resources using code.
IaC is not cloud computing; both concepts have different meanings, even if there is a deep link between them.
The elasticity of cloud resources and the disposability of cloud machines make IaC meaningful. In a single press of a button, you can create hundreds of provisioned machines. The same thing would need a team of system administrators some years ago.
Note that cloud computing does not necessarily mean AWS or GCP; you can build your own cloud too.
If we look into the simplest concepts in DevOps like continuous integration and delivery, we automatically think of delivering small chunks of software to be built, tested, and deployed.
Tests are run against an evolved version of the software that is not really very different from the previous one. In other words, “the diff is not huge”.
Some companies with technical maturity tend to deploy hundreds of times a week. Github deploys dozens of times a day.
SlideShare deploys up to 20 times a day.
Other companies like Amazon make more than 20k deployments per day (source ITRevolution.com)
This means every 4 seconds there is a new deployment.
Don’t be upset when you compare your company deployment frequency to the frequencies above; it is not just about maximizing the number of daily deployments, not a race.Whether you deploy 10 times per day or 100 times per second, cloud computing is a must because it can be leveraged using code.
It allows you to “rent” the computation, storage, networking, and the necessary resources to run ephemeral testing and staging environments that disappear when you don’t need them. You can interface with your cloud provider using your favorite language (or DSL).
The Things You Own End up Owning You ~ Tyler Durden
I don’t know if you watched Fight Club, but this quote is one of the things that marked me in that great movie.
We are neither talking about the “having vs. the being” nor seeking the paths of wisdom here, but if you apply the same quote to your on-premises infrastructure, you will understand that “The Server You Own Ends up Owning You”.
The advantage of cloud computing is that we never have machines, we always supply resources on demand.
If I summarize the last paragraph, I’d say two things:
The second option, which is building your own, has two drawbacks:
Most businesses will choose using an existing cloud computing provider.In both cases, using the cloud does not mean that you are cloud native. More than that, if you are using the cloud and your software engineering approaches and methodologies are not following the cloud native best practices, you are probably wasting money, time, and effort.
In 2011, Gartner identified 5 migration strategies. AWS identified 6:
Rehosting is about the redeployment of application and data in the cloud without modification (IaaS). Thinking about re-architecting the application is the next step here.
Your organization will develop better skills in using your cloud provider and can re-architect your application in an easier way, however, you will not be able to leverage the full power of the cloud
Replatforming is about moving to the cloud with a small amount of up-versioning. It is done by upgrading your existing on-premise applications and implementing cloud paradigms and best practices like multi-tenancy or auto scalability and the 12-factors app.
Replatforming is faster than refactoring, it allows taking immediate, but modest advantage of the cloud.
Repurchasing is replacing the legacy on-premise application by a commercial SaaS platform.
This solution reduces the development efforts but has some disadvantages like vendor lock-in and interoperability issues.
This is when you re-architect your legacy applications and replace them with modern cloud-native apps.
Refactoring allows companies to benefit from the full power of the cloud but this approach is riskier and requires more knowledge and work.
By looking at the whole IT system of an organization, we usually realize that there are some applications that are either no longer used or with low business value.
Retiring is phasing out these applications.
In some cases, moving to the cloud is not the best solution to your problem. Not moving to the cloud and retaining is also a strategy.
The main role of a “DevOps engineer” or a “DevOps team” is implementing DevOps within an organization until it becomes evidence.
Since DevOps is also a philosophy, implementations differ in function of one’s understanding and interpretation.
When implementing a DevOps strategy, we usually think about automation, however, we might forget that automating the wrong process is multiplying faults. For that very reason, thinking about the process should be upstream of any implementation.
To find the best process you must find the development, production and deployment bottlenecks and move them to the beginning of your process.
On the other hand, because DevOps is not a framework or a set of instructions to follow, and because if we keep things at their philosophical level, everyone will have a different opinion and the DevOps implementation approach diverges in several directions; we need standards.
There were several initiatives to create standardized technical approaches such as the Webscale, Twelve-Factor Apps, 13 Factor Apps, The Reactive Manifesto or the work done by the OCI and the CNCF.
Driven by passion, some developers and engineers think of code as the goal, however, it is not.
The code is the tool and the goal is solving business and real-world problems.
Unless you are coding a side project for fun or for learning purposes, coding should not be treated as a goal.
Adding more code is always adding entropy to your code, which means more tests, more maintenance, and most probably more bugs, so think before you code.
Code Less, Think More
A soon as there is a new project, a new problem to solve, some will jump up and create a REST API or a CRUD application, deploy it to production until the first incident happens and the product limitations start to appear: e.g not scalable, vulnerable, everybody thought about deploying and no one thought about rollback ..etc
Sometime, when you ignore traditional thinking and revise old processes, they will see positive change.
In this universe, there are laws that can be noticed everywhere … like entropy.
Entropy is “the measurement of disorder”.
The entropy of the universe increases with time, the more time passes, the more there is disorder increases if you compare it to the initial state.
The heat of a coffee left on the table will be transmitted to the cup: We say that the entropy of the cup increases.
There is plenty of entropy manifestation around us, but what is important for us is about the entropy in software and IT systems.
Software entropy refers to the tendency for software, over time, to become difficult and costly to maintain. A software system that undergoes continuous change, such as having new functionality added to its original design, will eventually become more complex and can become disorganized as it grows, losing its original design structure.
In theory, it may be better to redesign the software in order to support the changes rather than building on the existing program, but redesigning the software is more work because redesigning the existing software will introduce new bugs and problems. ~ source: Webopedia
We can outline two important concepts here:
Immutability and self-healing platforms are some of the new ways of thinking and designing applications that solve this problem. These paradigms showed us their strengths in creating stable and scalable applications at a faster pace.
Immutable: Cloud virtual machines and/or containers are never changed after deployments. When there is a new deployment, everything is re-created.
Self Healing: The system can identify its problems then resolves them, usually, by returning to an initial state.
Microservices, containers, and orchestration are the technical tools to implement these paradigms.
They also have other advantages, for example, containers coupled to microservices allow deploying a single service instead of re-deploying the whole monolithic application and containers orchestration frameworks allow autoscaling a microservice under load, instead of scaling the whole monolithic application.
When we approach containers, microservices, and orchestration, there is always an important aspect, common to the three concepts: Event-driven approaches.
Microservices, containers, and orchestration are reactive to change and act in function of events.
Cloud Native is reactive, that’s why we will discover what is the Reactive Manifesto.
This schema explains a lot about the philosophy of the “Reactive Manifesto”:
The goal of the “Reactive Manifesto” is creating responsive applications.
Regardless of the conditions, a responsive application must always have the same behavior.
To achieve this responsivity, the application must be elastic and resilient.
Without being elastic and resilient, we can not speak of a responsive application.
Finally, building message-driven applications is the key to elasticity and resiliency.
There is something at the center of everything we develop: data. There is always data input or output.
Databases were very involved in solving the data problem, but they created a lot of other problems, in particular, problems of performance and scalability.
To optimize our use of databases, we tried several solutions, such as code optimization, which is limited.
We tried caching techniques which are also limited because we always cache a small part of the whole data. This can be limiting in the case when data demand is dynamic.
We tried materialized view to solve caching problems but it adds load to the database.
We tried to use cloud databases, and they are expensive.
We tried vertical scaling and replication, and they are also expensive and have some performance limits.
We tried NoSQL databases which are not suitable for all scenarios and use cases. They may also be expensive.
Message-driven applications solve many of the above problems. In short, they are based on message exchanges. To dive into this concept, let’s get back to the history of databases.
If we take one of the most used databases, MySql (or any other alternative technology like MariaDB), we can notice that the transaction log is the heart of the database.
It records all the events that change the state of the data and it contains events: delete, create, update, insert .. etc
The database tables hold the current state representation of these events, replication is a replay of the transaction logs and even the cache layer is a partial copy of this data.
The transactions log can be seen as a proof of concept of a message-driven system.
If this log’s events are streamed by publisher components and consumed by consumer components of the same application, we can create optimized polyglot persistence stores for each data consumer. Each service can use the most suitable storage technology (SQL, NoSQL, SQLite, raw files, graph database ..etc)
We can also avoid storing unnecessary data and add a load to the database.
The good thing about message-driven applications is that it completely fits the microservices development, production and data model.
Let’s take the example of Uber, do you think that the same database model and technology are suitable for all of these three services?
Uber uses Cassandra, MySQL, PostgreSQL, and other home-grown solutions to manage their data and by using microservices they can choose the right database and data model to manage passengers without impacting how drivers’ and trips’ data is managed.
To maintain the reactivity and especially the coherence of a system composed of microservices. One could use transaction logs to inform other services of changes to a particular service.
The event sourcing is another practice that we can find in applications based on messages exchange.
When updating your Facebook profile from your smartphone, there are several bricks that need to be notified.
The monitoring application needs to detect fraud.
There is at least one database that needs to store the changes that happened.
The indexing and search datastores (like Solr and ElasticSearch) need also to know about it.
The newsfeed service needs to publish your update so it should be notified.
In short, there are multiple data stores and services that need to be notified about a change in your profile.
Imagine the case where the mobile application must notify all of these applications as soon as you change a single letter in your first name.
The Facebook mobile application will probably take a few minutes to update all these services at once.
The solution is to write the update on a message and send it once to a streaming system like Kafka. Each other service will consume the data that interests him.
Change is published once and consumed several times.
We have already seen that one of the advantages of cloud computing is providing IaC tools and environment that can be used in your cloud-native journey. The other advantage is the reliability of its services. Most cloud services have an SLA of 99.99 %.
We have also seen how polyglot microservices, reactive programming, and message-based services fit together perfectly like puzzle pieces. However, there are some disadvantages in relying on such architectures.
Everything will be down and dysfunctional when the streaming system is down. This is when cloud computing is a savior.
Since all of your application blocks rely on streaming data, the use of a system that has high availability is advisable.
e.g: Amazon Kinesis stream can be used for event sourcing.
John and Jane are preparing some cheese pancakes. This is what John followed as steps:
..and this is what Jane did:
There is a difference between the first way of getting things done: The second way is faster because of its asynchronicity.
To create cloud native applications, we should eliminate all synchronous inter-component communication.
Event streaming helps not only in creating message-driven architectures but also in implementing asynchronously within your application.
Event streaming is the basic pattern. We always have a producer and a consumer.
The first publishes a message and the consumer subscribes to a topic to receive these messages.
It is advisable to use a fully managed streaming service.
When you leave a message on the streaming system, you do not have to wait for the application that consumes that message. Therefore, the communication is asynchronous.
Event sourcing uses the first pattern (event streaming) except that there is a database that plays a role in this model.
When we talk about event sourcing, we call up the process of transforming a series of events from our streaming system to a persistent data store.
There are 2 approaches to doing this.
Event First: The publisher sends its message to the event stream, the consumer react to this event and it records it in the data store.
Database First: The user first records the state in the database, then the database propagate this change to the entire application. This can be done with CDC databases. This is not the case for all databases, generally, cloud databases are equipped with this mechanism.
Wikipedia defines the CDC as follows:
In databases, change data capture (CDC) is a set of software design patterns used to determine (and track) the data that has changed so that action can be taken using the changed data.
Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture, and delivery of the changes made to enterprise data sources.
The CQRS is based on the event sourcing pattern but separates the reading of writing.
CQRS separates reading (query) from writing (command).
The publisher records each change, usually as an event. Processing is then performed, often asynchronously, to generate denormalized data models.
The consumer simply queries these templates in order to minimize the querying of the database.
In order to better understand, let’s take the example of a user who, using a web browser, made a change in the user interface.
Any change produces an event that is sent to the event stream and is consumed by a subscriber that creates a materialized view for the consumer (reading).
In other words, the event is transformed into a state in the database of the reading.
The event is also written to the (writing) data store.
The advantage is that we can, for instance, write in a Mysql dat store and read from a NoSQL data store .
The title is a quote said by Tim Berners-Lee, the inventor of the World Wide Web.
Indeed, IT systems and architectural evolution show us today that the data didn't”t change but we changed the way we consume and produce it.
(Orchestrated) microservices is the best example that shows us how far software and infrastructure architecture has changed.
Microservices has the advantage of allowing developers to choose freely the database technology to use with a microservice or a specific operation (eg. reading vs writing in CQRS).
This solves many problems related to data like performance, replication, scalability, and complexity.
Choosing the right data store technology brings icing on the cake. With more than 300 database technology, this can be intimidating:
So, start by picking up your choice criteria like read/write performance and latency. This will get you closer to the right choice.
These are some good articles that may help you make a better choice:
thenewstack.io
www.dataversity.net
We have seen 3 patterns of cloud-native architectural patterns, there are more to see
A book that I recommend, if you want to read more about these patterns, is Cloud Native Development Patterns and Best Practices: Practical architectural patterns for building modern, distributed cloud-native systems:
www.amazon.com
These are some patterns:
Designing cloud-native applications and systems is not only about architecture and technical patterns but also about how processes are implemented and managed.
The challenge of microservice is also human.
In fact, The Conway Law is among the known laws in leadership.
This law assumes that the architecture of a system will be a copy of the communication scheme within an organization.
Besides, there are some caricatures that can be found on Intenet.
This law can be applied in software architecture to create microservices. The organization of teams and the interaction between them, directly affect the architecture of your application.
If you understood how Conway law may affect your IT architecture, you can imagine how terrible will be the communication between your microservices, if development teams( communication is also terrible.
Large teams are always a bad idea, they are less autonomous, less innovative, more dependant to other teams and less transparent since the communication becomes difficult within the team members.
However, the problem of large teams is not exactly its size but the number of links between teammates.
To calculate the number of links or communication channels, Richard Hackman, the author of Leading Teams: Setting the Stage for Great Performances created this formula:
The number of links = N(N-1)/2
(where N is the number of people)
You can see how the number of link increases rapidly and can reach 91 channel of communication when the team has a size of only 14 people.
This part could also be entitled “The Brooks’s Law”.
Fred Brooks in his book The Mythical Man-Month, highlighted a central idea saying that “adding manpower to a late software project makes it later”.
Some of the causes of Brooks’ Law are training time since it takes time for new people to speed up even if they are experts in the used technology and the fact that communication becomes more complex when adding new people to a team.
Jeff Bezos, the founder of Amazon also agrees on the previous laws and has his own vision of team management.
If 2 pizzas are not enough for your team, you should reduce the number of people. This philosophy is among the success factors of Amazon particularly on the technical level.
A cross-functional team is a group of people with different functional expertise (marketing, operations, development, QA, account managers ..etc) working for the same goals and projects.
A group of individuals of various backgrounds and expertise is assembled to collaborate in better manners and solve problems faster.
As said in Wikipedia: The growth of self-directed cross-functional teams has influenced decision-making processes and organizational structures. Although management theory likes to propound that every type of organizational structure needs to make strategic, tactical, and operational decisions, new procedures have started to emerge that work best with teams.
In DevOps context, the dev and ops teams should not live in separate silos. Each team should provide support and pieces of advice in order to take advantage of the skills of everyone.
According to some management studies, like Peter Drucker’s on management by objectives in his book The Practice of Management, cross-functional teams are less goal dominated and less unidirectional which stimulates the productivity and the capability of dealing with fuzzy logic.
Cross-functional teams are the best fit for microservices development, as products are developed, tested, built and deployed by the same team, which makes work faster, easier and transparent.
For more reading, I recommend my article The 15-point DevOps Check List:
medium.com
In 1942, Albert Einstein was a professor at Oxford University and one day he just gave a physics exam to his students.
When he was walking around one day, his assistant asked him a question:
“Dr. Einstein, this exam you gave your students, it’s exactly the same you gave last year ?!”
Einstein replied:
“Yes, it’s the same exam, but the answers have changed. “
It’s a bit similar in software engineering, we are always trying to solve the same problems: How to create a stable and functional application, but the answers change quickly.
Today, we have some answers to the problems of modern software development, but that does not mean that these answers will not change.
The answers have changed and will always change.
Make sure to follow me on Medium / Twitter to receive my future articles. You can also check my online training Painless Docker and Practical AWS.
This story unline most stories on Medium is free and not behind a paywall. If you liked this work, you can support it by buying me a coffee here.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
543 
543 claps
543 
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.heptio.com/cloud-native-part-1-definition-716ed30e9193?source=search_post---------14,"As Craig and I start our journey toward building Heptio, we have been doing a lot of thinking around where we see the industry going. We spent quite a bit of time at Google (16 years between the two of us) and have a good understanding how Google builds and manages systems. But chances are you don’t work at Google. So how do all of these evolving new concepts apply a typical company/developer/operator?
This is the first part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
There is no hard and fast definition for what Cloud Native means. In fact there are other overlapping terms and ideologies. At its root, Cloud Native is structuring teams, culture and technology to utilize automation and architectures to manage complexity and unlock velocity. Operating in this mode is as much a way to scale the people side of the equation as much as the technology side.
Cloud Native is structuring teams, culture and technology to utilize automation and architectures to manage complexity and unlock velocity.
One important note: you don’t have to run in the cloud to be “Cloud Native”. These techniques can be applied incrementally as appropriate and should help smooth any transition to the cloud.
The real value from Cloud Native goes far beyond the basket of technologies that are closely associated with it. To really understand where our industry is going, we need to examine where and how we can make companies, teams and people more successful.
At this point, these techniques have been proven at technology centric forward looking companies that have dedicated large amounts of resources to the effort. Think Google or Netflix or Facebook. Smaller, more flexible, companies are also realizing value here. However, there are very few examples of this philosophy being applied outside of technology early adopters. We are still at the beginning of this journey when viewed across the wider IT world.
We are still at the beginning of this journey.
With some of the early experiences being proven out and shared, what themes are emerging?
At Heptio, we are incredibly excited to help bring the benefits of Cloud Native to the wider IT industry. In the rest of this series we’ll be looking at integrating with existing systems, DevOps, containers and orchestration, microservices, and security. Please stay tuned and let us know what is most interesting to you.
Continue with part 2 as I cover practical considerations of applying Cloud Native.
Heptio
238 
4
238 claps
238 
4
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://medium.com/planet-stories/cloud-native-geospatial-part-2-the-cloud-optimized-geotiff-6b3f15c696ed?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
This article is part of a ‘Cloud Native Geospatial’ series exploring how the geospatial world looks different when systems and workflows are built from the ground up for the cloud. This time we are going to take a deep look at arguably the most important enabling technology for truly cloud native geospatial: the Cloud Optimized GeoTIFF.
Cloud Optimized GeoTIFF’s, also known as COG’s, are a specially formatted GeoTIFF file that leverages a newer feature of HTTP called Byte Serving. You can learn much more about Cloud Optimized GeoTIFF’s at cogeo.org. Byte serving is the technology that lets you stream a video or music file online and skip forward or backwards through the content. Instead of having to download the full video file, you can tell the server that you want to start at a particular point. The COG format works the same way by allowing users and processes to access just the portion of a raster file that they need.
Jumping to a desired portion of a raster file opens up varied new workflows, as data can be ‘streamed’ like a video instead of being transferred whole across networks. In the geospatial world users can access online web tiles in a streaming manner, but to do actual analysis requires source raster files. Traditionally that has meant long download times to acquire files that are hundreds of megabytes and larger. This is because the source raster files are distributed online and on the cloud, but they aren’t formatted for streaming, so users must fully download the files before processing and visualization could start.
The Cloud Optimized GeoTIFF format began as a collaboration between Amazon, Planet Labs, MapBox, ESRI and USGS to put the Landsat Archive onto AWS in a more accessible way. The GeoTIFF is the most widely used imagery file format, but there was extensive discussion on the Landsat-pds mailing list on how to best format the data so that it could be streamed and processed on the fly. A good solution for formatting Landsat data enabled companies to all leverage the archive in their existing workflows on Amazon Web Services without duplication and reprocessing. Once this pattern of access was established, new software started leveraging the data in the same way, greatly increasing the use of the data.
From there the practice of formatting GeoTIFF to be optimized for cloud workflows has evolved to a documented best practice, with a full implementation in GDAL (the most widely used geospatial library) including documentation and performance testing results published on the GDAL Wiki. Planet Labs transitioned to producing all of the data going through its processing pipeline as Cloud Optimized GeoTIFF’s, with partners like FarmShots and Santiago & Cintra re-architecting their domain specific applications to leverage it.
Reflecting industry adoption, leading open source projects like GDAL, QGIS and GeoServer can already read the format (though QGIS and GeoServer take advanced configuration). DigitalGlobe recently shifted their IDAHO system to leverage COG’s, while reprocessing a significant amount of data to make use of it. OpenAerialMap built their whole architecture on turning user uploaded data into web accessible GeoTIFF’s and then streaming tiles directly from that data. GeoTrellis projects supporting COG on their short term roadmap, and a number of others, including similar cluster computing geospatial processing systems have indicated an intent to support it.
These newer on-the-fly processing systems underscore the power of cloud native geospatial architecture. Such systems can simultaneously process imagery on hundreds and even thousands of computers, returning in seconds analyses that previously would take days or weeks. Despite these modern advances, because the core of the standard is GeoTIFF, any software can read the data, even older desktop applications.
Though the core concept is quite simple — put imagery online and streamable — it is a fundamental building block of a truly cloud native geospatial ecosystem. Data can live on the cloud and numerous software systems can run next to the data to derive value from it without anyone having to incur additional download and storage costs. A full exploration of that ecosystem is a topic for future posts, but the core COG building block will be a foundation to enable users to spend their time actually using data and gaining insights in near real time instead of finding data and relying on a small number of experts who have expert geospatial processing skills.
Although cloud optimized GeoTIFFs are still a relatively new format, backward compatibility and ease of implementation make the format a compelling next step, and the founding group of organizations aim to encourage more software implementers and data providers to adopt it. If you are interested in helping out and learning more, as a software implementer, user or data provider, check out cogeo.org.
Up next in this series: a couple posts taking deeper looks in to actual Cloud Native Geospatial architectures.
Using space imagery to tell stories about our changing…
340 
4
340 claps
340 
4
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/capital-one-tech/lightweight-cloud-native-messaging-with-nats-ad730ca2becf?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
When building applications for the cloud, we often devote a lot of effort to breaking down our monoliths and building applications as small, containerized workloads that follow the 12 (or 15) factors for cloud native apps. With our focus narrowed on the internals of our codebase, we often leave discussions and designs about messaging in the backlog.
Messaging is the central nervous system of any large-scale distributed system. Whether we’re doing event sourcing or more simple work dispatch models, messaging is the glue that makes it all work. Without it, our wonderful distributed systems grind to a halt.
So how do we choose a message broker or messaging architecture for our application? It can feel pretty overwhelming, with a large number of options already available and new ones popping up every day.
At the far end of complexity and size, we’ve got Kafka. Kafka is often referred to as a distributed log store. It’s assumed that the messages published to topics in Kafka will persist for some time, and the concept of consumer groups allows messages to be distributed evenly among multiple instances of the same service. It is extremely powerful, but with that power comes great responsibility. Kafka is difficult to maintain, and has a steep learning curve for any team looking to skill up on the technology.
Another pretty common choice is RabbitMQ (or any AMQP-compliant broker, really). Rabbit is significantly lighter weight, but instead of using the concept of unique consumer groups, Rabbit takes the simpler approach of having clients consume queues. If a client doesn’t acknowledge a message, it’ll go back in the queue to be processed by another. Subtleties arise from this architecture, like allowing small time windows where it’s possible for two workers to receive the same dispatch, etc.
Even applications like Redis that don’t bill themselves as message brokers support pub/sub messaging. As you can see, the list of products and services around message brokering is vast and overwhelming.
Each of these products have their sweet-spots, places where they shine. Kafka shows some of its real muscle in large-scale message streaming and aggregation scenarios with persistent message logs. Rabbit thrives in environments that need simpler pub/sub functionality and enforced idempotency of work outside the confines of the message broker. Redis might even be a good fit if all your messaging clients are also talking to Redis for caching, and you don’t need to persist messages.
What if I want to really embrace the “central nervous system” idea, but I don’t want all the overhead of some of the other solutions? What if I want to be able to do traditional pub/sub, but also request/reply and maybe even scatter-gather, all while keeping things nimble and light? This is where NATS might be a better fit.
NATS is an incredibly fast, open source messaging system built on a simple, yet powerful, core. The server uses a text-based protocol, so while there are a number of language-specific client libraries, you can literally telnet into a NATS server and send and receive messages. NATS is designed to be always-on, connected and ready to accept commands. If you are old enough to know what a “dial-tone” is, then it’s worth mentioning that the NATS documentation likes to use that analogy for this design.
To get a feel for what it’s like to build applications on NATS, let’s walk through a couple common use cases.
In the simplest pub/sub model, we have a publisher that emits messages to a subject (although you may be familiar with the term topic instead). Any party interested in the messages on that subject subscribes to it. NATS will then guarantee at most once delivery. This means that messages sent from a single publisher are guaranteed to arrive in order, but order is not preserved across multiple publishers. I will save the “global message ordering” rabbit hole discussion for a future blog post as that debate can go on for days.
Suppose we are building a video analysis system that does facial recognition. As the analyzer makes progress on a large piece of media, we want to publish that progress to anyone who might be interested.
Since NATS is a text protocol, you can just issue a command that looks like:
We tell NATS the subject ( analysis.progress ) and the content length (55 bytes). Then, a newline precedes and follows the actual data. If everything went well, NATS gives us back a +OK response. This is in stark contrast to some of the complex and even proprietary binary protocols used by some message brokers. That feeling we get when we can easily debug a RESTful service with the POSTman add-on is similar to how I feel when I can just telnet to my NATS server.
To subscribe, we create a subscription with a unique subject identifier (the subject ID is private to my connection):
This means subject ID 50 represents the subscription to analysis.progress . Every subscriber will then get a message that looks like this:
As with the publication, the payload is separated from the metadata with just a simple newline/carriage-return combo. Each MSG protocol message contains the subject ID and the content length of the raw message.
To compare to some other brokers, in some cases we have to write administrative scripts to create topics ahead of time before our services can even start. Kafka requires explicitly created topics, as does Rabbit, whereas Redis and NATS let you create channels and subjects (their respective terms) on the fly.
This ability to create subjects on-demand turns out to be key to enabling request-reply semantics.
When we make RESTful service calls where we issue an HTTP request to a service and then get a reply, we’re using a traditional synchronous request-response pattern.
With many messaging systems, the request-reply pattern is often difficult or requires some very awkward and debt-heavy compromises. With NATS, request-reply is a fairly simple operation that involves supplying a “reply-to” subject when publishing a message.
Here is a breakdown of what happens in a NATS-based request-reply scenario where we want to ask for a list of videos in which a particular person appears. One important thing to keep in mind here is that we don’t know who or what we are asking for this information. All we’re doing is publishing our desire for an answer, and it is up to the system to satisfy our request. This loose coupling is incredibly empowering and gives us tremendous flexibility to upgrade and enhance the system over time without requiring “stop the world” releases.
To keep multiple concurrent requests of the same type from stepping on each other, and to ensure that whatever code handles the request replies to only the single request publisher responsible for that message, the “reply-to” subject is unique per request, often with a GUID suffix. For example, we might publish a video identification inquiry request as follows:
This complexity is almost always dealt with under the hood by the language-specific client. For example, the Go client hides the creation of the reply-to subject entirely:
This publishes a request and waits 100 milliseconds for a reply. The Go library is hiding the reply-to subject detail from the developer. It is also hiding the subscription to and un-subscription from the reply-to subject. If the library you’re using doesn’t implement this for you, it’s pretty easy to create a wrapper that does.
In the scatter-gather pattern, a single publisher publishes a message on a topic to an unknown number of subscribers concurrently. It’s assumed that all of the listeners will then start working. The publisher then awaits replies from some or all of the subscribers and then aggregates the results somehow.
Let’s say I’ve got a few thousand drones that make up my fleet of package delivery devices. A request has come in to deliver a package and I’d like to pick which drone I want to use. Doing things “the old way”, I could iterate through the known list of all drones, interrogate each one individually over slow and potentially unreliable networks, and then when I am done with this loop I can finally make a decision based on remaining battery, weight capacity, and current location. This is slow, error-prone, and horribly inefficient.
To get around this problem, we can use scatter-gather instead. I will publish a message on the package.auction topic as follows:
Note that we’re still using a unique reply topic. This allows all of the listening drones in the fleet to reply to this specific auction request and not interfere with any other auction requests being handled at the same time.
Those that didn’t receive the message are obviously not good pick-up candidates, neither are those that don’t reply within our expected timeout period. Messages might come back looking like this:
We can then collect all the results we receive within a timeout period and decide from among the replies based on drone battery life, location, capacity, and how much battery it might have remaining at the end of the drop-off.
It isn’t its complexity that makes NATS so powerful, it is its simplicity. I have a particular fondness for the elegance of simplicity (the Japanese have a word for this, kotan) and NATS embodies this quite well. By keeping the underlying protocol simple, by focusing on performance and cloud native reliability, we can build up all kinds of really powerful messaging patterns without having to shoehorn in awkward functionality or carry the burden of tons of unused functionality in a larger product.
Hopefully this post has inspired you to not just take a look at NATS, but to evaluate the level of complexity your message broker needs with a critical eye. If you want to experiment with NATS, you can go grab the gnatsd docker image and start playing around with it.
DISCLOSURE STATEMENT: These opinions are those of the author. Unless noted otherwise in this post, Capital One is not affiliated with, nor is it endorsed by, any of the companies mentioned. All trademarks and other intellectual property used or displayed are the ownership of their respective owners. This article is © Capital One 2018.
The low down on our high tech from the engineering experts…
321 
321 claps
321 
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
Written by
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
"
https://blog.heptio.com/heptio-will-be-joining-forces-with-vmware-on-a-shared-cloud-native-mission-b01225b1bc9e?source=search_post---------17,"Today we are incredibly excited to announce that Heptio will be acquired by VMware. It is a watershed day for our company, and we hope for the industry as a whole. The inevitable question is … why have we decided to join forces (now)?
Life at Heptio has been pretty exceptional since we founded the company two years ago. In a short period, we have made strong contributions in the Kubernetes and cloud native ecosystem, assembled a remarkable team and onboarded some of the most prestigious enterprises as customers. We were incredibly well capitalized and supported by our investors. So what gives?
Shared vision.
Heptio’s mission is to build a platform that accelerates IT in a multi-cloud world. We are on the precipice of a major transformation—the de-coupling of applications from the environments where they are run. And we feel a responsibility to help organizations navigate this transformation to true cloud native architecture. To realize the greatest possible impact, Heptio would need access to an entirely different level of resources and execution capabilities than we have today.
Who is best positioned to lead this transformation? The company that led a parallel transformation—the software defined data center. VMware. They have experience, execution muscle, customer trust and full leadership commitment.
When we first started conversations with VMware, the alignment of our respective visions was uncanny. With virtualization, VMware helped enterprises change the way their infrastructure operates. VMware values our products and services—together we can apply these technologies to change the way business operates, and where they run their applications.
Customer Value.
We live in a really interesting time. Enterprise companies are dealing with waves of disruption in the software space, and increasingly fragmented and complicated hosting environments. Kubernetes has an important role to play as a ubiquitous, uniform framework—to be as available and invisible as a utility, like electricity. We believe that an enterprise should pick their infrastructure hosting environment based solely on pragmatic attributes: cost economics, data locality and sovereignty, and connectivity to the consumers and workloads they support.
The value for enterprises is not the electricity, nor the vehicle through which it is delivered; value is created when applications are plugged in. The missing piece is a control plane that shapes the experience in deploying and accessing cloud native technologies. It must address day 2 challenges, integrating technologies into a practical enterprise environment, and instituting policies and management capabilities. It is going to take a hard push from an engaged, enterprise-friendly company to make it real. We are convinced that VMware possesses the ability and commitment to create a platform that works everywhere and meets the unique needs of enterprises. Together we can change the game.
Community Connection.
From the start, Heptio has maintained a healthy relationship with the open source community. We’re tied into the Kubernetes steering committee and a number of SIGs, plus our team has shepherded five open source projects (Sonobuoy, Contour, Gimbal, Ark and ksonnet). We feel like the community trusts us. That trust continues to be well placed. The team at VMware have a parallel appreciation for the community; they fully understand the importance of being closely connected to foster more innovation. They have so much energy and resources already focused on this area; the time is right to join forces and accelerate the value delivered to the open source community.
Culture First.
I’ve left culture to the final topic for this post, but the fact that VMware puts its culture first is central to our decision to join their fold. We think a lot about the culture of a company not only as an expression of its values, but as a blueprint for how it creates value in the world. Even before we started conversations with VMware, we were aware of similarities in our culture and core values. We have some great people working at Heptio that ‘grew up’ at VMware—they enjoyed their work and had tremendous respect for their colleagues. This made us feel good about joining them, and instilled confidence that our teams would gel and we could focus our energy on our shared mission.
In Closing.
At Heptio, we’ve often (internally) lamented that we’re not great at celebrating our achievements. But today, we can’t avoid a proper celebration. I’m so proud of our team and what they’ve built in such a compressed time frame, and so grateful to our community for their incredible support. I’m immensely excited to join forces with an organization that shares our mission and that has proved they know how to deliver transformative technology. We’re fired up to have an even bigger impact.
Heptio
464 
6
464 claps
464 
6
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
"
https://medium.com/planet-stories/cloud-native-geospatial-part-1-basic-assumptions-and-workflows-aa67b6156b53?source=search_post---------18,"There are currently no responses for this story.
Be the first to respond.
The last few years have seen the rapid rise of ‘Cloud Native’ architectures and applications in IT, crystallizing a number of best practices and tools pioneered by Google, Amazon and others for building complete developer workflows that only run on the cloud. At the same time, the geospatial world has been incrementally migrating to ‘the cloud’, but there has been little change in the core architectures and data workflows for producing geospatial information and insight. Desktop algorithms have been ported to the cloud, but users still work in the same way, just leveraging the cloud for processing loads that a single desktop can’t handle, or to put their data up on a web map.
But what would the geospatial world look like if we built everything from the ground up on the cloud?
This series of articles aims to explore what a truly Cloud Native Geospatial world would look like, highlighting the architectures and best practices that are emerging. How does ‘Cloud Native’ translate from developers to geospatial users and practitioners? What changes in people’s workflows and processes when the desktop is no longer even involved in creating geospatial information and insights? Many of the assumptions of how people work with data and gain insight from it change when there is infinite storage and compute capacity, so it is worth throwing out the guide book and seeing where the path leads…
So how does Cloud Native Geospatial work? Let’s first look at the four assumptions that underpin a truly Cloud Native Geospatial environment:
It makes sense that the developer world was the first to really explore how many core assumptions and workflows change when one takes as a given that all data and computing can live on the cloud. The results have been quite powerful, giving rise to a host of innovations, like container orchestration and micro-service architectures not only in pure technology but also processes and methodologies.
Fully exploring all the implications of these four tenets is an extensive project, and indeed will lead to new workflows we haven’t imagined yet. But in the short history of Cloud Native Geospatial a few evolutions have already emerged:
When all data is in a cloud location that a variety of software can work with directly, there is no need to maintain multiple copies of any data. The data provider should pay for the storage costs, and then can charge for access. But users of the data access the data ‘in place’, doing what they need — be it building visualizations with web tiles or processing a derived data product.
Once the data is in one place, this leads to a paradigm shift of sending algorithms to the data, instead of the other way around. If all of your data is in the cloud in one location, data becomes too massive to download and process locally, so users must package up their algorithms to run on the cloud. This could be a full container, like a complete Docker processing instance; but with more advanced cloud geospatial systems like RasterFoundry and Google Earth Engine a user just sends a script or a descriptions of operations to run.
With the data accessible in one place and a host of algorithms available on the cloud the processing tools become much more shareable than in the desktop paradigm. Indeed increased collaboration is perhaps the most important thing that the web and the cloud enable. In the future most users will just select from the most popular algorithms, instead of making everyone do their own Top of Atmosphere Radiance conversion, cloud removal, atmospheric correction and surface reflectance calculations to make an NDVI output. There will be one pre-processing plus NDVI algorithm that is the most popular one, and most people can just select it. Web-tiled mapping will enable much of this collaboration around analysis, as any step in a processing chain can be visualized, correcting it on the way.
The last major implication to preview here is ‘real time GIS’. As more data flows from new IoT sensors, cell phones and satellite imagery, it will be constantly updating databases. Combine that with the Computer Vision and Deep Learning advances that are also being added to the geospatial toolbox (object identification, change detection, etc) and you’ll have access to a continually updated set of maps. The limitations only lie in the rate of data collection.
With processing on the cloud those updates will kick off new processes to create higher level analytic products, like detecting new planes at an airport, significant changes in an agricultural field’s health or new oil well pads.
For me, it gets really interesting when the new derived data point turns into an alert that reaches a user who cares about that information. Right now the output of a GIS or Remote Sensing workflow is generally a map, often one embedded in a powerpoint slide. But the point of a map is to communicate some information and insight — that there are 10 new airplanes at this airport. Users who care about that information should be able to subscribe to that information directly; they should get alerted when a certain threshold of information is reached in the area they care about, rather than visually inspect every new image. Any step in that geospatial intelligence processing pipeline should be visualizable, but most users should interact with the information feeds resulting, using a map more to verify than as the primary information interface.
With the introduction of accessible, centralized data, and the dramatically different workflows that follow, Cloud Native Geospatial has the potential to introduce new, non-specialized users to the power of geospatial information that GIS practitioners have enjoyed for decades. The beneficiaries of that spatial intelligence today are those who have built up an ecosystem of GIS practitioners who are able to work with the data and create information products by hand — printed maps, reports, curated web maps. When that ecosystem itself is truly Cloud Native, the exploitation of valuable information can be decoupled from the practitioners. The ecosystem of geospatial experts will collaborate to create analyses and insight, but any non-expert user will be able to select and apply those to the geographic area they care about.
With right cloud architecture in place, and an influx of new, creative users deriving insights from geospatial data, we have the potential to discover entirely new applications for GIS technology.
There is much work ahead to make Cloud Native Geospatial a reality, but a diverse community of organizations and individuals are working together to make it happen. Stay tuned for more in this series, as we dive deeper into how Cloud Native Geospatial works and explore some leading cloud native geospatial architectures running right now.
Up next in this series: a look at Cloud Optimized GeoTIFFs — a format on top of which Cloud Native GeoSpatial is built.
Using space imagery to tell stories about our changing…
344 
2
344 claps
344 
2
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/pinterest-engineering/memq-an-efficient-scalable-cloud-native-pubsub-system-4402695dd4e7?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
Ambud Sharma |Tech Lead and Engineering Manager, Logging Platform
The Logging Platform powers all data ingestion and transportation at Pinterest. At the heart of the Pinterest Logging Platform are Distributed PubSub systems that help our customers transport / buffer data and consume asynchronously.
In this blog we introduce MemQ (pronounced mem — queue), an efficient, scalable PubSub system developed for the cloud at Pinterest that has been powering Near Real-Time data transportation use cases for us since mid-2020 and complements Kafka while being up to 90% more cost efficient.
For nearly a decade, Pinterest has relied on Apache Kafka as the sole PubSub system. As Pinterest grew, so did the amount of data and the challenges around operating a very large scale distributed PubSub platform. Operating Apache Kafka at Scale gave us a great deal of insight on how to build a scalable PubSub system. Upon deep investigation of the operational and scalability challenges of our PubSub environment, we arrived at the following key takeaways:
In 2018, we experimented with a new type of PubSub system that would natively leverage the cloud. In 2019, we started formally exploring options on how to solve our PubSub scalability challenges and evaluated multiple PubSub technologies based on cost of operations as well as reengineering cost for existing technologies to meet the demands of Pinterest. We finally landed at the conclusion that we needed a PubSub technology that built on the learnings of Apache Kafka, Apache Pulsar, and Facebook LogDevice, and was built for the cloud.
MemQ is a new PubSub system that augments Kafka at Pinterest. It uses a decoupled storage and serving architecture similar to Apache Pulsar and Facebook Logdevice; however, it relies on a pluggable replicated storage layer i.e. Object Store / DFS / NFS for storing data. The net result is a PubSub system that:
The secret of MemQ is that it leverages micro-batching and immutable writes to create an architecture where the number of Input/output Operations Per Second (IOPS) necessary on the storage layer are dramatically reduced, allowing the cost effective use of a cloud native Object Store like Amazon S3. This approach is analogous to packet switching for networks (vs circuit switching, i.e. single large continuous storage of data such as kafka partition).
MemQ breaks the continuous stream of logs into blocks (objects), similar to ledgers in Pulsar but different in that they are written as objects and are immutable. The size of these “packets” / “objects,” known internally in MemQ as a Batch, play a role in determining the End-to-End (E2E) latency. The smaller the packets, the faster they can be written at the cost of more IOPS. MemQ therefore allows tunable E2E latency at the cost of higher IOPs. A key performance benefit of this architecture is enabling separation of read and write hardware dependending on the underlying storage layer, allowing writes and reads to scale independently as packets that can be spread across the storage layer.
This also eliminated the constraints experienced in Kafka where in order to recover a replica, a partition must be re-replicated from the beginning. In the case of MemQ, the underlying replicated storage only needs to recover the specific Batch whose replica counts were reduced due to faults in case of storage failures. However, since MemQ at Pinterest runs on Amazon S3, the recovery, sharding, and scaling of storage is handled by AWS without any manual intervention from Pinterest.
MemQ client discovers the cluster using a seed node and then connects to the seed node to discover metadata and the Brokers hosting the TopicProcessors for a given Topic or, in case of the consumer, the address of the notification queue.
Similar to other PubSub systems, MemQ has the concept of a Broker. A MemQ Broker is a part of the cluster and is primarily responsible for handling metadata and write requests.
Note: read requests in MemQ can be handled directly by the Storage layer unless the read Brokers are used
The Governor is a leader in the MemQ cluster and is responsible for automated rebalancing and TopicProcessor assignments. Any Broker in the cluster can be elected a Governor, and it communicates with Brokers using Zookeeper, which is also used for Governor election.
The Governor makes assignment decisions using a pluggable assignment algorithm. The default one evaluates available capacity on a Broker to make allocation decisions. Governor also uses this capability to handle Broker failures and restore capacity for topics.
MemQ, similar to other PubSub systems, uses the logical concept of Topic. MemQ topics on a Broker are handled by a module called TopicProcessor. A Broker can host one or more TopicProcessors, where each TopicProcessor instance handles one topic. Topics have write and read partitions. The write partitions are used to create TopicProcessors (1:1 relation), and the read partitions are used to determine the level of parallelism needed by the consumer to process the data. The read partition count is equal to the number of partitions of the notification queue.
MemQ storage is made of two parts:
MemQ allows for pluggable storage handlers. At present, we have implemented a storage handler for Amazon S3. Amazon S3 offers a cost effective solution for fault-tolerant, on-demand storage. The following prefix format on S3 is used by MemQ to create the high throughput and scalable storage layer:
s3://<bucketname>/<(a) 2 byte hash of first client request id in batch>/<(b) cluster>/topics/<topicname>
(a) = used for partitioning inside S3 to handle higher request rates if needed
(b) = name of MemQ cluster
Availability & Fault Tolerance
Since S3 is a highly available web scale object store, MemQ relies on its availability as the first line of defense. To accommodate for future S3 re-partitioning, MemQ adds a two-digit hex hash at the first level of prefix, creating 256 base prefixes that can, in theory, be handled by independent S3 partitions just to make it future proof.
Consistency
The consistency of the underlying storage layer determines the consistency characteristics of MemQ. In case of S3, every write (PUT) to S3 Standard is guaranteed to be replicated to at least three Availability Zones (AZs) before being acknowledged.
The notification system is used by MemQ for delivering pointers to the consumer for the location of data. Currently, we use an external notification queue in the form of Kafka. Once data is written to the storage layer, the Storage handler generates a notification message recording the attributes of the write including its location, size, topic, etc. This information is used by the consumer to retrieve data (Batch) from the Storage layer. It’s also possible to enable MemQ Brokers to proxy Batches for consumers at the expense of efficiency. The notification queue also provides clustering / load balancing for the consumers.
MemQ Data Format
MemQ uses a custom storage / network transmission format for Messages and Batches.
The lowest unit of transmission in MemQ is called a LogMessage. This is similar to a Pulsar Message or Kafka ProducerRecord.
The wrappers on the LogMessage allow for the different levels of batching the MemQ does. Hierarchy of units:
Producing Data
A MemQ producer is responsible for sending data to Brokers. It uses an async dispatch model allowing for non-blocking sends to happen without the need to wait on acknowledgements.
This model was critical in order to hide the upload latencies for the underlying storage layers while maintaining storage level acknowledgements. This leads to the implementation of a custom MemQ protocol and client, as we couldn’t use existing PubSub protocols, which relied on synchronous acknowledgements. MemQ supports three types of acks: ack=0 (producer fire & forget), ack=1 (Broker received), and ack=all (storage received). With ack=all, the replication factor (RF) is determined by the underlying storage layer (e.g. in S3 Standard RF=3 [across three AZs]). In case acknowledgement fails, the MemQ producers can explicitly or implicitly trigger retries.
Storing DataThe MemQ Topic Processor is conceptually a RingBuffer. This virtual ring is subdivided into Batches, which allows simplified writes. Messages are enqueued into the currently available Batch as they arrive over the network until either the Batch is filled or a time-based trigger happens. Once a Batch is finalized, it is handed to the StorageHandler for upload to the Storage layer (like S3). If the upload is successful, a notification is sent via the Notification Queue along with the acknowledgements (ack) for the individual Messages in the Batch to their respective Producers using the AckHandler if the producers requested acks.
Consuming DataMemQ consumer allows applications to read data from MemQ. The consumer uses the Broker metadata APIs to discover pointers to the Notification Queue. We expose a poll-based interface to the application where each poll request returns an Iterator of LogMessages to allow reading all LogMessages in a Batch. These Batches are discovered using the Notification Queue and retrieved directly from the Storage layer.
Other FeaturesData Loss Detection: Migrating workloads from Kafka to MemQ required strict validation on data loss. As a result, MemQ has a built in auditing system that enables efficiently tracking E2E delivery of each Message and publishing metrics in near real-time.
Batch & Streaming Unification: Since MemQ uses an externalized storage system, it enables the opportunity to provide support for running direct batch processing on raw MemQ data without needing to transform it to other formats. This allows users to perform ad hoc inspection on MemQ without major concerns around seek performance as long as the storage layer can separately scale reads and writes. Depending on the storage engine, MemQ consumers can perform concurrent fetches to enable much faster backfills for certain streaming cases.
PerformanceLatency
MemQ supports both size and time-based flushes to the storage layer, enabling a hard limit on max tail latencies in addition to several optimizations to curb the jitter. So far we are able to achieve a p99 E2E latency of 30s with AWS S3 Storage and are actively working on improving MemQ latencies, which increases the number of use cases that can be migrated from Kafka to MemQ.
CostMemQ on S3 Standard has proven to be up to 90% cheaper (avg ~80%) than an equivalent Kafka deployment with three replicas across three AZs using i3 instances. These savings come from several factors like:- reduction in IOPS- removal of ordering constraints- decoupling of compute and storage- reduced replication cost due to elimination of compute hardware- relaxation of latency constraints
ScalabilityMemQ with S3 scales on-demand depending on write and read throughput requirements. The MemQ Governor performs real-time rebalancing to ensure sufficient write capacity is available as long as compute can be provisioned. The Brokers scale linearly by adding additional Brokers and updating traffic capacity requirements. The read partitions are manually updated if the consumer requires additional parallelism to process the data.
At Pinterest, we run MemQ directly on EC2 and scale clusters depending on traffic and new use case requirements.
Future WorkWe are actively working on the following areas:- Reducing E2E latencies (<5s) for MemQ to power more use cases- Enabling native integrations with Streaming & Batch systems- Key ordering on read
MemQ provides a flexible, low cost, cloud native approach to PubSub. MemQ today powers collection and transport of all ML training data at Pinterest. We are actively researching expanding it to other datasets and further optimizing latencies. In addition to solving PubSub, MemQ storage can expose the ability to use PubSub data for batch processing without major performance impacts enabling low latency batch processing.
Stay tuned for additional blogs about how we optimized MemQ internals to handle scalability challenges and the open source release of MemQ.
Building MemQ would not have been possible without the unwavering support of Dave Burgess and Chunyan Wang. Also a huge thanks to Ping-Min Lin who has been a key driver of bug fixes and performance optimizations in MemQ that enabled large scale production rollout.
Lastly thanks to Saurabh Joshi, Se Won Jang, Chen Chen, Divye Kapoor, Yiran Zhao, Shu Zhang and the Logging team for enabling MemQ rollouts.
To learn more about engineering at Pinterest, check out the rest of our Engineering Blog, and visit our Pinterest Labs site. To view and apply to open opportunities, visit our Careers page.
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
204 
1
204 claps
204 
1
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
"
https://medium.com/swlh/what-does-cloud-native-really-mean-1b10ed003aa9?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
Kyle Brown and Kim Clark
Note: This is part 1 of a multipart series You can jump to Part 2, Part 3, Part 4, or Part 5.
All too often, conversations around cloud native dive straight into technology choices like containerization and microservices. These are definitely potential ingredients of a cloud native project, but they are most certainly not the whole picture. Across this article series we will explore cloud native from several different angles, including technology and infrastructure of course, but also architecture, design, and perhaps most overlooked, people and processes. Put in the simplest possible terms, cloud native means not just moving to cloud, but fully leveraging the uniqueness of cloud infrastructure and services to rapidly deliver business value.
Cloud native concepts existed before the term itself came into use. In a sense, cloud native began when public cloud vendors started providing easy and affordable access to elastic instances of compute power. The question then became, how can you write applications to capitalize on the flexibility of this new infrastructure, and what business benefits can you achieve as a result?
Cloud native methods and technology have changed a lot over the last ten years, and are still evolving, but the core technical and business objectives that cloud native applications set out to achieve have remained the same. These include:
We will break these goals down more in a later article when we look back at the “why” of cloud native, but hopefully even from this simplistic definition, it should be clear that cloud native is broader than simply a move to a new type of infrastructure. However, while these goals are accurate, it’s hard to see they apply to cloud native specifically. We need to do more to define what cloud native actually means.
Popular reference points related to cloud native, such microservices, and older manifestos such as 12factor apps might lead you to conclude that cloud native is a description of an architectural style, and the other choices follow from that. There is certainly some truth in that and cloud native architectures definitely do exist. However, in order to succeed with cloud native, companies must take a more holistic view. Alongside architectural and infrastructural decisions, there are also organizational and process decisions. That has led us to a key realization:
Technology alone cannot attain business outcomes
The diagram below shows how these decisions interact.
A good example of how these aspects are interlinked, along with warnings as to what happens if the links are broken, is described in our article “Avoiding Incomplete Cloud Native Adoption”. In this article series, we will show how success in cloud native relates the coordination of changes across these three key areas in order to be coordinated in order to succeed: architecture & design, technology and infrastructure, people & process. Lets explore each of these in more detail.
A decade or more ago the term “cloud” was largely about location. It usually referred to infrastructure located in someone else’s data center accessible over the internet. However today “cloud” is more a statement about how you interact with that infrastructure. Indeed the location element has all but disappeared as it is now common to have a cloud-like facility that runs on in your own data center — a “private cloud”, as well as hybrid solutions that may involve services and workloads running across both.
So cloud today is more about how you engage with the infrastructure, which at a minimum must provide the following:
However, as cloud platforms and concepts have matured, the cloud in cloud native really also implies a greater abstraction from the underlying infrastructure.
In the early years of cloud native, these capabilities were typically highly proprietary, but now this comes almost ubiquitously in the form of containers, and container orchestration capabilities such as Kubernetes. As such, the above list is quite specific to the vocabulary of containers, but it’s worth recognizing that there are other options such as serverless/function as a service that further abstract from the infrastructure, and will likely become more prominent in the future.
We could include more, such as build automation, service mesh, logging, tracing, analytics, software defined networking and storage, etc. However we would then be stepping into what are currently more proprietary aspects of cloud platforms. Hopefully over time these too will become more standardized. So “cloud” in this context really means infrastructure and technology with the special properties listed above.
By “native” we mean that we will build solutions that don’t just “run on cloud”, but specifically leverage the uniqueness of cloud platforms. Applications don’t just magically inherit the benefits of the underlying cloud infrastructure, they have to be taught how.
We need to be really careful with language here. When we use “native” to refer to the “uniqueness of cloud platforms”, we do not mean vendor specific aspects of specific cloud providers. That would be “cloud provider native”, and indeed that would go completely against objectives around portability and use of open standards. What we mean is the things that are conceptually common to all cloud platforms. In other words, the things we highlighted in the preceding section on infrastructure and technology.
There are important implications on architecture and design. We need to write our solutions to ensure, for example, that they can scale horizontally, and that they can work with the auto-recovery mechanism. It is here that cloud-native perhaps overlaps most with microservices concepts. This includes for example writing components that:
We will describe these in more depth in our next article, but for now, perhaps the most important thing to note is that they are all highly interdependent. It is much harder, for example to create a component that is disposable if it is highly stateful. Reducing dependencies will inherently help to make a component more lightweight. Having well defined interfaces will enable a disposable component to be more easily reinstantiated, and so on. This is a small example of the broader point that moving to a cloud native approach requires changes on many related fronts simultaneously. These cloud native ingredients we are gradually uncovering are mutually reinforcing.
What may be less obvious is that when we work with the above assumptions and decisions about the architecture and underlying infrastructure, it provides us opportunities to radically change the way we handle people and processes. Indeed it could be argued that it necessitates those changes.
Below we’ve explored some of the people/process implications resulting from a microservices approach:
Likewise, there is an effect of container technology on required skillsets, roles, and processes:
Bringing together what we’ve discussed so far we can see that cloud native needs to be defined from three different aspects.
Today the technology aspect is of course heavily focused on containerization, but it is the properties such as self-provisioning, elasticity, and auto-recovery of that technology that are important, not the technology itself.
Architecturally we most commonly look to microservices principles to create more lightweight, fine grained, state minimized components that map better to abstracted infrastructure. Without the right design principles, our solution will not benefit from the platform. For example, it will not dynamically scale, or offer granular resilience, or offer rapid build and deployment, or have operational consistency with other applications on the platform.
People and process changes are often seen as separate from cloud native, but in reality they go hand in hand, and we consider them part of the defining characteristics. Lack of automation of the software development life cycle will mean a team spends more time on the mundane, and comparatively little time on business value. A heavy, top-down organizational and governance structure will not provide teams the autonomy they need to help the business innovate.
So, with a more concrete definition of what cloud native actually means, we’re ready to take the next step and expand out our previous diagram.
In the diagram above we provide some teasers as to what the key ingredients are in each of these aspects. In subsequent articles of this series, we’ll consider “how” you go about building cloud native solutions and look into each of those ingredients in detail, starting with people and process issues.
However, it should already be clear that to go fully cloud native is non-trivial, and requires business sponsorship. Therefore, in another article we will draw together what we have learned about the commitment required to be successful with cloud native and take a step back to re-consider “why” you might be making the cloud native move in the first place, and what benefits you might hope to achieve.
Acknowledgements
We would like to extend sincere thanks to Holly Cummins and Callum Jackson for their input and review on this article series.
Get smarter at building your thing. Join The Startup’s +750K followers.
207 
207 claps
207 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/planet-stories/cloud-native-geoprocessing-part-1-the-basics-9670280772c8?source=search_post---------21,"There are currently no responses for this story.
Be the first to respond.
I recently wrapped up a series of posts on ‘Cloud Native Geospatial,’ which explore geospatial architectures that are built for the cloud, from the ground up. In this series, I’m delving deeper into one aspect of these architectures: the processing of geospatial data, also known as ‘geoprocessing.’
The ability to process massive amounts of data across many machines is arguably the biggest advantage of the cloud. Users can marshall unprecedented amounts of computational resources with the click of a button, scaling up to almost any workload that can be imagined. But doing such massive computation on geospatial data can be challenging for a number of reasons.
This series will explore the current state of geoprocessing on the cloud, including the best tools available now and coming soon. We will also explore the issues of trust on the cloud that come with running algorithms on someone else’s system. We’ll conclude with some thoughts on interoperability and what a fully collaborative processing ecosystem could look like.
Establishing a baseline for what constitutes ‘cloud native geoprocessing’ is actually more challenging than it seems. One of the biggest reasons is that most of the initial geospatial cloud software were released right when cloud computing hit the ‘Peak of Inflated Expectations’ in the technology hype cycle.
A number of geospatial software providers rushed to join the hype, talking about their ‘cloud capabilities.’ For most, it was relatively easy to claim a cloud product by taking their same code and running it on the cloud, even if that required a Microsoft Windows OS on a huge virtual machine.
But almost none of them were able to seamlessly scale up geoprocessing operations. One could run a huge compute job, but it took lots of hand-holding — sending custom commands to many different machines. So most ‘cloud editions’ of various geospatial software put all the onus of scaling up onto their users. In the early days of Planet, we saw several vendors who claimed to have a ‘cloud’ solution, but after asking if they were running it themselves, it became clear they were just pushing the hardest problems on to us.
Meanwhile, the broader IT ecosystem had been developing a number of ways to handle seamless scaling, with Hadoop, Spark, Kubernetes, Lambda, and others. It’s become clear, however, that truly ‘cloud native geospatial’ is best done using geospatial code that is optimized for scale — not simply repackaging old code.
As such, a true ‘Cloud Native Geoprocessing’ solution should make it not only possible, but also incredibly simple to run a workflow across 10 or 10,000 virtual machines. It should take no separate configuration to scale it out across any number of nodes.
Cloud Native Geoprocessing aims to decrease operations work needed to process data, increase the ability to scale, and to do it cost effectively. In a survey of the current Cloud Native Geoprocessing landscape, there emerge different ‘groups’ of software that have similar approaches and functionality. The following sections aim to elucidate those, and in our next post we’ll do a deeper inventory of the landscape.
Batch Geoprocessing systems were the first to take advantage of cloud capabilities. The core advantage is to take massive datasets and break up the computation across a number of processors. For example, one could take a global elevation model and run a process that computes the slope or creates a terrain-shaded relief map. The user just needs to specify the operation and the number of nodes to use, and the system runs the whole process in a matter of hours (as compared to days or weeks).
The initial systems were based on Hadoop, an open source framework for doing parallel processing of huge amounts of data using the ‘MapReduce’ programming model. More modern systems usually use Docker and some orchestration service like Kubernetes. This lets batch systems run just about any arbitrary code as long as the operations are parallelizable on geospatial information.
Another class of systems can do the same type of operations as batch processing systems, but aim to do all the operations quickly enough to return results to users as they interact online, often with sub-second responses. The first of these systems was Google Earth Engine, which tapped into a number of advanced internal Google systems.
The key is organizing the data in the right way and then using enough nodes to do the processes in memory. These systems will typically reprocess data being ingested (though they are starting to be able to use Cloud Optimized GeoTIFFs natively), and also tend to be a bit more limited in their operations. This is because operations need to be rewritten for the framework, so that they will perform quickly enough. They have a real advantage in algorithm development, as one can quickly see the results of applying an operation. The results can be very impressive when hooked up to large compute clusters, completing countrywide and global operations across massive datasets in seconds or less.
The third major category of geoprocessing is still emerging. While the previous two modes depend on a user running them on data already acquired, these run a geoprocessing operation without a user taking any explicit action. The processing runs whenever new data comes in, delivering the finished result to the user as soon as it is completed. The core of a geoprocessing subscription can be batch or on-the-fly based. The real-time responsiveness of on-the-fly services matters a bit less in this workflow, since the overall acquisition and provider processing time of most new data is far more than the time it takes to run an end user’s algorithm.
A subscription system is geared towards streams of data, when there are continually new results. These are usually fairly massive datasets, where it can be prohibitive to download and process all the data, even to another batch system. So subscription geoprocessing systems are usually offered by the data provider, and will match based on a filtering criteria (for eg, only process for a certain geographic area and cloud cover metric). They aim to deliver a higher level of information than just the raw data. Often the output will be a new raster or vector dataset, which is smaller than the full raw data, but is still used as input to even higher level processing systems.
Beyond the three main types of geoprocessing above, there is one more worth mentioning: fully browser-based geoprocessing. Javascript is incredibly powerful and there is an emerging ecosystem of geospatial algorithms that can run fully client side. The key to making this work well is coordinating the data delivery between the client and server, through new formats like Cloud Optimized Geotiffs, which can deliver just the information needed for the current view. More work is needed for this to operate seamlessly, but advanced systems could even run the same javascript code in the browser and the server for maximum flexibility.
In the next post, we’ll dive deeper into the current state of the Cloud Native Geoprocessing world, looking at leading implementations of each type of system.
Using space imagery to tell stories about our changing…
219 
3
219 claps
219 
3
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pinterest-engineering/pinterest-joins-the-cloud-native-computing-foundation-e3b3e66cb4f?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
Micheal Benedict | Pinterest technical product manager, Cloud & Data Infrastructure
We’re excited to announce that Pinterest has joined the Cloud Native Computing Foundation (CNCF). By joining CNCF, we intend to work closely with the community to 1) Share lessons learned from operating Pinterest at scale in a public cloud environment 2) Contribute to cloud-native standards and 3) Adopt, support and collaborate on cloud-native technologies.
Pinterest is powered by hundreds of microservices on tens of thousands of hosts. Our data processing platform has also grown dramatically, from processing a few terabytes (TBs) to tens of hundreds of petabytes (PBs) daily.
The path to support this growth without sacrificing on developer velocity and cost efficiency has been both exciting and challenging. For example, we invested early in systems focused on traffic management, cloud resource provisioning & management, identity management & access control, deployment orchestration and more since cloud-native was not an established concept.
We recently presented at KubeCon/CloudNativeCon ’17 Austin to a packed audience on Pinterest’s Journey from VMs to Containers.
In the presentation, we shared our vision for the Compute Platform, which is to provide the “fastest path from an idea to production, without forcing developers to worry about infrastructure.” We also announced our decision to use Kubernetes for container orchestration.
We’re excited for the possibilities Kubernetes and other cloud-native technologies offer and look forward to sharing our experiences in leveraging them on our blog. We’d like to thank CNCF for its stewardship in bringing structure, clarity and standardization around these technologies.
Acknowledgements: A special note of thank you to Karan Gupta, Pinterest’s Head of Infrastructure for his executive sponsorship, David Chaiken, Pinterest’s Chief Architect for his leadership in technical strategy & direction and Chris Aniszczyk, COO of CNCF for shepherding this process.
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
248 
248 claps
248 
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
"
https://blog.devgenius.io/10-predictions-for-cloud-native-in-2021-38796ad60bc3?source=search_post---------23,"Key takeaways from the DevOps Conference Keynote by Cheryl Hung
Back in January, I wrote a recap of the 2020 CNCF Annual Report, summarizing the key updates from the Cloud Native Computing Foundation (CNCF). Recently at the DevOps Conference 2021, Cheryl Hung, the VP Ecosystem at CNCF, gave her predictions for cloud native in 2021. Cheryl has a unique perspective from her role at CNCF…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/15-best-practices-to-design-cloud-native-modern-applications-a2aa9f19cda0?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
To be clear about the term “Cloud”, I’d like to start with this quote from Paul Maritz, former CEO of VMware and Pivotal:
The cloud is about how you do computing, not where you do computing.
Michael Dell, the founder of Dell Technologies also said:
The cloud isn’t a place, it’s a way of doing IT.
So let’s make sure from the beginning that the word Cloud in this story is not about Google or AWS Cloud, but about a philosophy and a set of concepts.
We may have different opinions about whether “cloud-native” is the same as “cloud-ready”, but one thing is sure, cloud-native applications are cloud-ready.
Think of cloud-native as an advanced form of software evolution during the last decades. Cloud-native approaches, like any other approach in software development, should not be considered as a replacement for another traditional software but should be seen as better adapted to the very different environment of the cloud.
The long beak variation is an adaption to extract food out of narrow openings. Birds with shorts beak would slowly die off because of their inability to access their food. The software follows this analogy.
To produce one of the above species, nature needs some patterns. In this story, I will talk about these patterns but in software engineering, more specifically the patterns to design and create cloud-native applications.
During the evolution journey of modern software engineering, many approaches to developing and deploying software were conceived and adapted, from Netflix successful DevOps implementations, The 12 Factor App of Heroku, the Cloud Native Computing Foundation graduated and incubated projects, The 13 Factor App (Mesosphere), Beyond the Twelve-Factor Application (Pivotal) to Web-scale IT (Google, Amazon ..)..etc ..etc ..etc
In this essay, I am stopping by the most important and common patterns in the approaches mentioned above, in order to “extract” the best practices.
I stopped by 15 patterns, and all of them can be classified into three categories.
Think of this as an equation with three variables.
From a business view, your goal should be resolving the equation.
Software development, considered as projects that are budgeted and delivered during a limited time slot — doesn’t fit the needs of the modern business.
The mental model of a project when something is planned, executed then delivered within defined time and budget slots were challenged by the Agile methodology.
The up-front determination should be replaced by an on-going discovery and continuous optimization model.
Same as the first principle, prioritizing releases over features, can have negative consequences on people, processes and the use of technologies.
Thinking about features makes implementing proven methodologies like “Feature Teams” and “Two Pizza Team” easier.
A feature teams is a long-lived, cross-functional, cross-component team that completes many end-to-end customer features — one by one (featureteams.org)
Re-usability is more known in software development, think of functions and routines. The same principle can now be employed in infrastructure (like IaC) and during software builds.
Docker images are reusable and they are also using base images, that may use other base images .. etc.
As the variety of computing devices and needs continues to expand, business demands become more complex and customers become more demanding, the software that contributes to those capabilities must stay ahead of that growth.
Polyglot microservices is a form of technology agnosticism where a team which is responsible for a certain set of services decide which tech stack they are going to use to solve the problems.
This approach, even if it has some drawbacks, increases the developer productivity and autonomy, therefore decrease the time-to-market.
Portability is usually attributed to a computer program that can run on different OSs. Its alternative in the DevOps world would be a platform that can run on different infrastructures and cloud providers without requiring a major rework.
When you containerize your applications, you are sure that you will able to run them on Kubernetes and in Docker Swarm without changing the images.
Also, if you made the choice to orchestrate your containers using Kubernetes, you have the choice to deploy your cluster to GCP, AWS or any other cloud.
Abstraction is more known in object-oriented programming. A programmer can hide all but the relevant data about an object in an abstract class in order to reduce complexity and increase efficiency.
If we take a look at this blocks diagram, we can see that most networking code today are Virtual Network Functions (VNFs) and those can run on top of OpenStack or Kubernetes. Kubernetes can run on top of bare-metal or any public cloud.
In the future, the CNCF expect more expect many of those networks functions to be repackaged not in virtual machines like it’s the case, but in containers to become CNFs or Cloud Native Network Functions.
This is when Kubernetes can become a universal abstraction layer that allows all kind of workloads to runs on top of it.
No one can say that reducing costs is an afterthought or a secondary topic. From a business point fo view, sales margins increase when costs are reduced. During the latest years, new disciplines like FinOps have emerged to manage the variable cost of cloud and distributed systems.
Using a technology over another can also reduce the costs without reducing the quality. After the democratization of containers, developers are able to run multiple containers in the same virtual machine which helps IT organization reduce the number of machines and also the number of operating systems, which is also a cost-cutter.
Because developing at a fast pace is one of the value cores of most methodologies and implementations like ITIL, DevOps, and Agile.
Serverless is one of the computing models whereby application developers don’t have to provision servers or manage to scale for their app.
Instead, those tasks are abstracted away by the cloud provider (AWS Lambda, GCP Cloud Functions), allowing developers to deliver code to production much faster than in traditional models.
A monolith application is scalable when it allows adding new features and adapting old features to new business needs without a major rework.
Note that a system becomes more complex every time code is added to the application.
Software entropy refers to the tendency for software, over time, to become difficult and costly to maintain. A software system that undergoes continuous change, such as having new functionality added to its original design, will eventually become more complex and can become disorganized as it grows, losing its original design structure. (webopedia.com)
Adopting self-service platforms within an organization helps teams deliver software continuously.
Let’s take the very common example of managing different environments within a single team:
Deployment of different environments like development, testing and staging environments should be automated. Using tools like Terraform, Ansible and cloud VM instances on-demand, any developer should be able to create and destroy an environment.
Wikipedia defines elasticity as:
The degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible
Let’s take the example of time-based elasticity and volume-based elasticity.
Time-based elasticity means turning off the resources that are not in use (e.g: development environments during none-business hours).
Volume-based elasticity means matching scale to the intensity of demand like compute cores, storage sizes, or throughput. (e.g: AWS Elastic File System will scale its storage capacity dynamically so you don’t need any provisioning effort).
The system stays responsive in the face of failure.
A failure can be produced by many scenarios like the incapacity to handle a greater workload/traffic or the sudden termination of a process.
Let’s compare monolithic applications and containers-based applications (microservices):
Each service is isolated in a way that eliminates the entire application from going down when the service itself goes down.
Monolithic applications have multiple single points of failure and if any one of them is impacted the whole application encounter more rik to be impacted too.
Security is not an afterthought. This should be known to all developers. Security should be implemented in design as it’s not a layer to add at the end of a project or a feature.
Observability is a feature that makes a system monitorable. It enables “white box monitoring” and the understanding of a system behavior through its internals.
With the complexity of distributed computing and cloud models, running an efficient computing infrastructure requires a spirit of radical simplification.
Let’s be clear, using containers or load balancers are not goals, they are tools or means.
In the part of the patterns, I tried to list and briefly explain the characteristics of a cloud-native modern application.
I started with the WHY now I’m moving to the HOW.
So how to build applications implementing the 15 patterns described above?
I created a 2d table mentioning the patterns in the rows and I added the practices to implement these patterns in the columns.
This is what I found:
Cloud-native is purely technical but the human side is important. There’s a link between your teams' structure and culture and what they produce as a result. This is what Melvin Conway coined:
Organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.
Build microservices and decompose your monolith into simple, lightweight, loosely coupled services that can be developed and released independently of each other.
“I think the automation of vision is a much bigger deal than the invention of perspective.” ~ Trevor Paglen
Automation has always been one of the good practices when building infrastructures and online platforms. Automating cloud-native systems include:
Make sure to simplify and secure before automation.
Stateless services are “smarter” than stateful services. They allow these features to be implemented easily:
One may ask about the state of a service using a session persistence between the client and the server in a stateless service.
Well, in a stateless design, there is no state between client requests. Every client request will have sufficient info to perform the requested action.
As it’s described in The 12 Factor App: Keep development, staging, and production as similar as possible. Dev/Prod parity makes processes like CI and CD easy.
Cloud-native applications are designed in a way to run without having affinities with the server and the operating system.
As it’s described in the Reactive Manifesto, reactive systems are responsive which means they should absolutely have two features: elasticity and resilience. This is implemented using message-driven applications (e.g: streaming logs).
Reactive Systems rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency
Cloud-native services communicate using lightweight APIs that are based on protocols like REST, NATs or gRPC.
Develop, and deploy your services using highly automated platforms that provide a service abstraction. Abstraction and automation are highly related.
The example of abstracting the data and separating it from the computing layer and the business logic layer allows automation and makes operations on data like data caching easier and more performant.
DevOps is about aligning teams around common business goals, but this will not happen unless you design accordingly. So create business capability teams and deploy business services.
Developers used to continually SSH into servers to continually update the deployed artifacts, from a cloud-native perspective this is an artisanal/handicraft way of doing things.
Immutability in this context is replacing the old version of an application by building new servers from an artifact and totally replacing the old one.
Design observable applications that can interface with observability tools and give insights (metrics, logs, traces ..etc) into what your code does in production (or any other environment).
Making the difference between noise and signals when designing for observability make things easier.
Or develop with security in mind and secure by design. Security is not an afterthought.
Simple can be harder than complex but keep in mind that in order to build complex systems that work, you need to build it from simple subsystems that work.
Unix philosophy originated by Ken Thompson can be an inspiration to build complex systems like Linux while keeping in mind a philosophy of minimalism:
Nature has 13 billion years of experience in research and development on her CV.
The dinosaurs, weighty and slow creatures have disappeared and left the earth for smaller, more agile and more intelligent species.
This is how software engineering is evolving: Monolith applications are giving the place to distributed systems composed of lighter and more sophisticated systems.
Don’t think that monolith will totally disappear, they will continue to support the IT ecosystem for years. The fact that more sophisticated approaches were invented, doesn’t mean a total instantaneous disappearance of other approaches.
When a whale dies, it supports a community of organisms and provides life for hundreds of marine animals for more than 50 years.
The other thing that nature can inspire us is the “afterward optimization” approach. Remember: Dinosaurs evolved into birds.
I am not an expert in this area, but during the transmission of DNA information, mutations can happen and in some cases, they can give birth to new phenotypes that are more sophisticated and evolved.
This is how nature works, things are never optimized from the beginning, they are post-optimized.
Richard P. Gabriel suggests that a key advantage of Unix was that it embodied a design philosophy he termed “worse is better”, in which simplicity of both the interface and the implementation are more important than any other attributes of the system — including correctness, consistency, and completeness. Gabriel argues that this design style has key evolutionary advantages.. (wikipedia)
Practically in the case of cloud-native, you can, for example, adopt the Monolith First approach.
Experts recommend taking the same approach by building a monolith application at the beginning, even if your intention is to create a microservices architecture.
This will help you in understating the domain and the business boundaries of your application first to make decomposing the application easier and avoid disrupting the organization.
Microservices and cloud-native are to monolith what birds are to dinosaurs, if you can’t start with since you can evolve into.
If you liked this story, you will like other stories I wrote about similar topics:
medium.com
medium.com
Make sure to follow me on Medium / Twitter to receive my future articles. You can also check my online training Painless Docker and Practical AWS.
Unlike most stories on Medium is free and not behind a paywall. If you liked this work, you can support it by buying me a coffee here.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
218 
218 claps
218 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/graalvm/lightweight-cloud-native-java-applications-35d56bc45673?source=search_post---------25,"There are currently no responses for this story.
Be the first to respond.
In this short note, we want to visualize the benefits GraalVM native images offer you.
Several projects have accepted GraalVM Native Image as a platform for their applications: Quarkus, Micronaut, Helidon — support GraalVM native images. Spring Framework recently announced the upcoming support for native images.
We measured startup and memory footprint of a basic microservice in Helidon, Quarkus, and Micronaut and here are the results we got.
Let’s start with the startup time. GraalVM native images allow any framework to start up in milliseconds. While there are differences in the exact measurements between the implementations, the largest portion of the benefits are coming from using native images.
We see similar results when we measure memory footprint, which on the graph below is the full RSS of the process. Again, for all frameworks GraalVM native images significantly reduce the runtime memory requirements compared to running on HotSpot.
We believe that GraalVM native images can become the best way for deploying cloud native applications and we are excited to see many in the ecosystem believing the same. While we continue to work on enabling easier configuration and broader support for the existing libraries, already now GraalVM native images can be used with several web-application frameworks and significantly improve their startup times and runtime memory usage.
More information about getting started with GraalVM native images can be found on the GraalVM website.
When you download GraalVM binaries: graalvm.org/downloads, check out these helpful links to the getting started guides for the frameworks we looked at:
GraalVM team blog - https://www.graalvm.org
171 
3
171 claps
171 
3
Written by
Developer advocate at AtomicJar
GraalVM team blog - https://www.graalvm.org
Written by
Developer advocate at AtomicJar
GraalVM team blog - https://www.graalvm.org
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/the-rise-of-cloud-native-programming-languages-211a5081f1b2?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
Deploying code has become increasingly easier over the past years. First, with the introduction of containers and container platforms such as Kubernetes, then through serverless or Function-as-a-Service offerings like AWS Lambda, and now we witness the rise of cloud native programming languages.
I like to think of it as a spectrum which can be understood along two dimensions: control vs. convenience:
Control means the level you can adapt aspects of your runtime environment to your needs. For example, you might need full control over the network stack or a specific version of an operating system. Convenience, on the other hand stands for the effort of a developer to create said environment in order to run her app. Note that while it’s true that higher abstraction level mean less work for developers, there’s still the need for someone to take care of creating and maintaining the underlying infrastructure. Say, you’re using a serverless offering: typically these are executed in containers and need to run on some host—this part is taken care of by the provider of the serverless offering, but someone is still responsible for provisioning and maintaining these bits.
With this out of the way, let’s nowhave a look at cloud native programming languages.
At KubeCon 2017 in Austin, Brendan Burns gave a keynote, formally introducing Metaparticle:
WSO2 also started out in 2017 with Ballerina. They had a strong presence at KubeCon 2018 in Copenhagen showing how to use it on Kubernetes.
The newest contender in this space is Pulumi, launched in June 2018, which also supports Kubernetes.
Now, what does this mean for you as a developer? I’d argue that we now have a rich toolbox at our disposal, from directly working with containers to leveraging FaaS offerings to cloud native programming languages. Depending on your use case and the needs of your organization, you can pick the right tool for the task. I think we can also expect to see more (maybe even more specialized) cloud native languages to appear in the near future, so there’s certainly no harm to keep an eye on this space.
#BlackLivesMatter
271 
1
Public domain.

271 claps
271 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Solution Engineering Lead in the AWS open source observability service team
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://towardsdatascience.com/20-best-visual-studio-code-extensions-for-cloud-native-development-in-2020-e0358e50ba87?source=search_post---------27,"Sign in
Md Kamaruzzaman
Jun 29, 2020·13 min read
In the last few years, we are seeing a rapid growth of Cloud Native software development. As per Canalis insights, Global Cloud services has seen a 34% growth in Q1, 2020…
"
https://medium.com/lenny-for-your-thoughts/introducing-the-cloud-native-landscape-86c536411373?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
Today, Redpoint Ventures and the Cloud Native Computing Foundation are proud to announce the Cloud Native Landscape. This project leverages the CNCF’s newly-announced reference architecture (below) to segment the stack and identify the prevailing projects, products and companies that enable organizations to build, test, deploy and scale cloud native applications. The goal, ultimately, is to provide clarity as to how different projects and companies fit together and to help ground the conversation for customers, practitioners and analysts as they embark on their cloud native journeys.
In the last decade, we’ve witnessed the emergence of a new computing paradigm characterized by open, elastic infrastructure, microservices application architectures and DevOps development and deployment principles. These massive deflationary forces have eviscerated barriers to adoption, distribution and innovation for technology. Correspondingly, there has been a Cambrian explosion of new projects, products and companies while yesterday’s tech giants — minted in the client/server era — have seen their businesses come under pressure from the commoditizing power of open source and cloud.
The practical implication is that every organization is in the midst of a massive digital transformation where IT shifts from a cost center that is adjunct to core business to a profit center that is its lifeblood. Competitive advantage in this world is defined by a company’s ability to out-innovate its rivals, which most plainly translates into the ability to write better software faster.
Cloud native was a catchall term created to represent this new computing paradigm, one that is optimized for modern distributed systems capable of scaling to tens of thousands of self-healing multi-tenant nodes. It is informed by technical merit and end-user value, and inspired by Internet-scale computing born inside organizations like Google and Facebook.
If this sounds nebulous, it’s because it is. For the last several years, as the velocity of innovation has accelerated to breakneck speed, so too has confusion for end-users. Orchestration, scheduling, distributed coordination, service discovery? Suddenly a new computing vocabulary had emerged, but no clear taxonomy or reference guide existed to help customers along their path to become cloud-natives.
At Redpoint, we have been closely observing the radical innovation unfolding, remarking these changes were fundamental and would reshape and recast the entire IT value chain. Last year we began to map out the ecosystem, and as those efforts continued in 2016, we got in touch with Dan Kohn and Alexis Richardson of the Cloud Native Computing Foundation, which itself was in the midst of defining and voting on a reference architecture. Over the course of a few conversations it became evident that collaborating on a definitive industry landscape would benefit the entire ecosystem — from individual developers to customers to startups to large incumbents — which ultimately brought us to today.
Please note that this version 0.9 is a beta, and we will be working towards a 1.0 release in the coming months that will incorporate a more nuanced architecture and segmentation and feature more projects and companies which may be absent in the current version. The beta version is available at https://github.com/cncf/landscape and we’re eager to accept feedback. We encourage projects and companies that have not been highlighted in the current release to open an issue so they can be included in version 1.0.
We look forward to hearing from and working with you!
Perspectives on developer tools, distributed systems, cloud…
82 
2
82 claps
82 
2
Perspectives on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants
Written by
VC @AmplifyPartners. Deep thoughts on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants.
Perspectives on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants
"
https://blog.getambassador.io/optimizing-cloud-native-development-workflows-combining-skaffold-telepresence-and-argocd-8774d12bf22f?source=search_post---------29,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I had the pleasure of presenting at the GOTOpia EU on Tuesday and Wednesday this week, and I decided to role out an entirely new talk based on our current approach to building effective developer workflows at Ambassador Labs: “Cloud Native Development Without the Toil: An Overview of Practices and Tooling”.
We all know that the CNCF do amazing work in the cloud community, and we also all know that the CNCF tooling landscape is a bit of an eye chart. This is understandable though, as the core purpose of the landscape is to show the breadth of technologies.
What I’m most interested in at the moment is how to assemble an opinionated selection of these technologies in order to create an effective idea-to-value toolchain for developers :
Many thanks to Manju Bhat for this great tweet!
The core premise of the talk was that you have to think holistically about the necessary changes to the software development and delivery workflow when migrating away from monoliths and VMs to microservices and Kubernetes.
Bringing along old mental models and ways of working won’t help. Instead you need focus on creating “paved paths” platforms and new developer workflows:
Taking some cues from the Information Security domain, I argued that engineers need to think about both safety and speed when adopting new ways of working:
As the above slide hints, a lot of these new ways of working are coupled to the platform that your organization is using (whether this has been created intentionally or not).
As my Ambassador Labs colleagues Richard Li and Bjorn Freeman-Benson recently discussed in the blog post: “The Rise of Cloud-Native Engineering Organizations: SRE, Platform Engineering, DevOps, and GitOps”, over the past decade engineering and technology organizations have converged on a common set of best practices for building and deploying cloud-native applications.
These best practices include continuous delivery, containerization, and building observable systems.
Modern platform engineers need to constantly examine the entire software development lifecycle from source to production, looking for opportunities to improve safety and speed.
The above slide mentions three key patterns for an effective cloud development workflow: artifact syncing, dev environment bridging, and GitOps. Let’s now look at some opinionated tooling that can be used to implement this patterns.
The first pattern, artifact syncing, is all about minimizing friction for building and deploying a local container image to a remote development cluster. Skaffold is an excellent tool for watching a local code folder, rebuilding a container image when a change is detected, and deploying this to a remote cluster for integration testing.
The next pattern, development environment bridging, takes the previous pattern a step further by effectively bridging the network between your local machine and a remote cluster.
Using a tool like Telepresence “puts your laptop in the cluster”, and allows you to not only locally call remote services as if you were in the cluster (e.g. `curl k8s-service-name:8080/test`) but you also get that “hot reload” experience on any arbitrary services deployed with your cluster: you can make a request against the remote ingress, re-route the associated upstream request to a locally running instance of the service that you are modifying/debugging, and then send the response back into the cluster to see the results of your change.
With Ambassador Cloud you can also share preview URLs that enable specified groups of developers to see the results of traffic that is being rerouted and intercepted selectively to a local service. Your entire dev team can share a Kubernetes cluster containing all of the services, and each group can send test traffic and iterate on their individual service (or services) in isolation from others.
The final pattern discussed was GitOps. This pattern is focused on removing pain from deploying applications to Kubernetes by ensuring that all declarative config is stored within version control (git) and any changes to this are automatically reconciled against a target cluster. I discussed both Argo CD and Argo Rollouts in this section of the talk
You can find the latest iteration of the talk slides on our SlideShare. Many thanks to all of the great questions and feedback from GOTOpia attendees that drove the evolution of the talk over the two days I presented it:
As with any new talk, there is much more to talk about. I left a bunch of content on the cutting room floor, and I’m still thinking about many of the great questions asked by attendees.
If you have a question please get in touch. Find me as @danielbryantuk on Twitter and on the Ambassador Labs Slack
Developer-First Kubernetes.
81 
81 claps
81 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@odedia/production-considerations-for-spring-session-redis-in-cloud-native-environments-bd6aee3b7d34?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Oded Shopen
Dec 5, 2017·13 min read
This is the second article on Spring Session Redis. In the previous article, I explain the ideas behind the framework and provide a quick demo.
In this article, I will provide some of the best practices and production considerations when deploying an application backed by Spring Session Redis.
At a minimum, Redis should be deployed in a master-slave configuration. A single instance is obviously a single point of failure to any system. Master-slave would allow the system to failover to a different instance in case the master is having problems or is unavailable.
Redis uses the notion of Sentinels to identify who is the current master. Think of Sentinels as bystanders who keep checking the health of the actual master or slave servers. When an issue is identified with the master, the Sentinels basically cast a vote on which slave should be promoted to become the new master. I won’t go into all the details in this article, but the following link provides in-depth explanation of the options you have. At a minimum, you should setup a configuration of one master and two slaves, all on separate machines/VMs.
Spring Session Redis uses the Spring Data Redis framework to talk to Redis. This means that the same considerations and best practices are applied to both frameworks.
When you use a sentinels-based configuration, you do not provide the redis host/port information to spring data redis. Instead, you should provide the property for the master server and a list of sentinel URLs. Each sentinel process has its own configuration file that lists the master redis server. For example:
There is no need to specify the slaves in the config, as those are auto-discoverable. Again, see here for complete details. Once you configured your master, slaves and sentinels, you will need to change the spring data redis configuration in your application to work with the sentinels instead of host/port. For example, on a local machine it would look like this:
A different approach is to use a Redis cluster, where means you will have multiple master servers. The data is essentially sharded/partitioned across all the available masters, which increases the performance of the database since each node only has to deal with a subset of the requests. A good tutorial for redis cluster setup is available here.
Redis Labs provides an enterprise version of Redis. It also provides the ability to interact with the cluster via a DNS proxy. This greatly simplifies things on the client side, since you no longer need to know the topology of the cluster, thus letting you make changes to the database without changing the clients. All you really need to provide to with such a setup is the Redis proxy host and port. You can download a trial version of redis labs here.
Let’s have a look at the available spring application properties for Spring Session Redis:
Note: I recommend bookmarking the “spring common application properties” page. There’s a ton of useful tweaks and configuration details there.
Spring Data Redis uses a JedisConnectionFactory to manage its connection pool. The relevant pool settings from above are max-active, max-idle, max-wait and min-idle. These define how many connections will be open to the Redis database.
Spring Data Redis uses a JedisConnectionFactory connection pool to talk to Redis. Behind the scenes, this framework simply uses an Apache Commons GenericObjectPool (source is available here).
It is critical to be able to monitor the status of the pool in a production environment. Most importantly, it’s important to know if the number of used connections passes a certain threshold, as this may indicate a problem.
Spring Boot Actuator is spring’s production-ready monitoring solution. Among others, it exposes a /metrics endpoint containing valuable information. Unfortunately, The JedisConnectionFactory object pool is not a part of that information.
However, actuator is extensible. And lucky for us, someone already took the time to write a Redis pool extension to actuator. You can download the source code from here: https://github.com/nysd/spring-boot-redis-metrics. Although the README is in Japanese, the implementation is pretty straightforward. Simply add the following dependency and repository to your pom.xml (or compile the source code yourself):
The result is additional monitoring entries in the /metrics API:
If you plan to use the open source version of Redis, you will not get any fancy dashboards or time-based valuable information. Redis Labs enterprise provides a very useful monitoring dashboard in a production environment:
If you do decide to go the open source route, remember that Redis is a single-threaded application. Monitoring the machine or VM that runs your Redis database might provide unreliable results. For example, if you run Redis on a dual-core machine, the VM would report a 50% CPU utilization at max load, while the Redis process itself may report 100% CPU utilization. If you were to run Redis on a quad-core machine or VM, the VM would report a 25% CPU utilization, while the Redis process itself is actually already maxed out at 100%. Make sure you monitor the Redis process itself.
Redis has some commands that should never be used in a production environment.
I saved the best (or worst) for last. There’s an implementation detail in the framework that you should be aware of before deploying to a production environment. I think the best way to explain it is with an example.
I’ll start by running our demo project in a test machine. I’m running one Eureka server, one Config server, one gateway and one order-management microservice.
For this simulation, I changed the server session timeout to be only 2 minutes instead of the default 30 by adding the following to the start script:
When I go to the Redislabs monitoring dashboard, the graph is quite boring:
Now, I’m going to add more server instances, so that I’ll have 8 gateways and 8 order-management instances. This is not entirely representative of a performance testing environment since I’m running on a single test machine, but the change is immediately clear:
Wow! What just happened? The servers should be completely idle, since there are 0 connected clients. And yet, every minute, on the minute, there is this small spike of reads and ops per second. Nothing to be alarmed about, I’m sure… or is it?
Now, let’s start logging in to the system indefinitely. I’m going to use a simple curl command for that, and I’m going to do all the logins from a single thread, to create a steady, predictable stream of logins to the system. Also — I’m only logging in to a single instance out of the 8 gateways.
My expectation would be an increased load on Redis, that would eventually even out to a simple, predictable flat line representing the incoming requests. What I actually got is this:
Looks really suspicious, isn’t it? Once again, every minute, right around the :00 seconds mark, there’s a sudden spike of ops per second and reads per second to Redis. As you can see, the spikes are now much more worrying. It is no longer a spike from 0 to 5, it’s a spike from around 700 to almost 4000 ops per second. If we’ll view the “other commands” graph, we’ll see a similar pattern, indicating that some of these commands are not reads or writes:
What’s going on here?
To understand this, we need to go deeper into the source code.
Spring Session Redis configures the system in a Spring Repository class called RedisOperationsSessionRepository. It handles many aspects of the framework, among them is how to cleanup expired sessions:
As you can see, the cleanupExpiredSessions method is scheduled to run every minute. This means that every minute, at the same second, all the instances you have in a production environment would connect to Redis at the same time. What would they do in this cleanExpiredSessions() method?
This block of code:
Again, this is done from every instance connected to Redis, and at the same time.
You might be wondering why we even need this implementation. As mentioned in the previous article, Redis can self-expire its keys and indeed there is an expiration set for the sessions.
The reason is that Redis does not guarantee exactly when these keys would get deleted. It is possible that a long time would pass without any access to this session, which means it would not get deleted long after it actually expired. Although this might not seem like a problem (the session would immediately expire upon the next access), there are SessionExpiredEvent listeners that developers might have implemented that are expected to execute immediately once the session expires (to cleanup web sockets, for example). Therefore, the framework tries its best to expire the key closest to the actual expiration time.
We’re not done yet.
There’s a listener created in the @Configuration class RedisHttpSessionConfiguration, that is registered to receive deletion and expiration events from Redis. The relevant source code is below. redisMessageListenerContainer creates the listeners, while setConfigureRedisAction tells Redis to notify this particular calling client (the server instance) on any key deletion or expiration:
Going back to RedisOperationsSessionRepository, we see the following code:
That’s a lot of code to digest, but the gist of it (no pun intended) is as follows:
So, to recap:
That’s a lot of activities done at once. Let’s do a little a real-world calculation for a production environment:
If we happen to have 100 server instances, and we have 2,500 sessions expiring every minute (a fairly common production use case), this means that every minute there would be at least 500,000 calls to Redis at the exact same second!
This can become a serious problem, and you’re probably beyond the capacity of a typical Redis cluster.
But there’s an even bigger problem.
Usually, in order to support increasing load — you add more server instances to a production environment, perhaps even with auto-scaling in some cloud environments. However, at its default setup, the more servers you add to a production environment, the worse the problem becomes, and you put your production at an even greater risk.
Note: I opened a github issue on this item. The team responded that they added a property to change the cron schedule if needed, and believes it is important to keep the listener configuration in place to allow for custom expiration handling by developers. I completely respect that, however you as a developer should be aware of this tradeoff.
There are a few ways to address this concern:
Note: the new milestone 2.0.0 version of Spring Session Redis appears to have removed the @Scheduled cron annotation, however it appears to just be configured differently. I’ll update this article if 2.0.0 provides a simple way to disable the cron entirely.
In this article I explained some of the production considerations when deploying an application backed by Spring Session Redis to a production environment. It is always important to do serious performance testing. I would recommend using tools that provide good monitoring visibility. As you saw above, this is something that can easily go unnoticed without a graph-based monitoring tool.
Good luck, and happy coding!
Oded Shopen
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
103 
3
103 claps
103 
3
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/memory-leak/cloud-native-landscape-celebrates-first-anniversary-69a4eb829505?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
It has been a year since we originally presented the Cloud Native Landscape in conjunction with the Cloud Native Computing Foundation (CNCF) and Amplify Partners. Since the initial working exhibit, we’ve incorporated feedback from the community and continuously iterated to improve the landscape. On October 23rd, we published our most recent edition v0.9.9. Over the past year it’s been wonderful seeing the community’s involvement and enthusiasm for the initiative. Considering the anniversary, we wanted to look back and analyze the landscape’s metamorphosis.
Categorization stays relatively consistent. All the broader categories have remained constant except Infrastructure, which we redefined as Cloud to better reflect both public and private offerings. Importantly sub-categories have become more precise, and we’ve added additional groups. As we move up the stack, we see that Provisioning now includes Key Management (e.g. HashiCorp’s Vault) and Container Registries (formerly Registry Services), which previously lived in Application Definition & Development. We also classified Cloud-Native Storage and Cloud-Native Network offerings as Runtime. The App Definition and Development tier transformed the most with the expansion of Databases to include Data Analytics; addition of Streaming; removal of Languages & Frameworks and 3rd Party APIs; and recategorization of Registry Services. Saliently, Platforms broadened to encompass Serverless/Event-based solutions (e.g. Platform9’s Fission).
The number of ecosystem players exploded. Since the original landscape the total number of projects and companies has nearly tripled from 116 to 337. Observability (+44) experienced the largest growth while Cloud (+12) saw the least expansion. Double clicking, the sub-groups Monitoring (+33) and PaaS/Container Service (+26) expanded the most while SCM (+1) remained relatively consistent. Interestingly the number of solutions available in every category increased if we exclude the categories that have been removed from the landscape (figure 1).
CNCF projects more than triple. When we published the landscape on November 8, 2016, the CNCF stewarded only three projects: Kubernetes, Prometheus, and Opentracing. The landscape exhibited five additional projects Linkerd, GRPC, Envoy, fluentd, and rkt that CNCF eventually voted into its portfolio. CNCF now hosts 14 projects including two security projects, Notary and TUF, which were added on October 24, 2017 (figure 2).
Kubernetes takes the lead. Kubernetes evolved out of Borg, a container cluster management solution built by Google for internal use. Since its initial release in July 2015, Kubernetes, the first CNCF hosted project, has become the most popular container orchestration engine. SDxCentral’s “2017 Container and Cloud Orchestration Report” indicates Kubernetes is in use almost twice as much at 64% than alternatives Docker Swarm at 36% and Apache Mesos/Mesosphere at 18% (respondents could choose all that they used). Kubernetes’ reach has expanded beyond test/dev. The New Stack’s “The State of Kubernetes Report,” noted that of the 470 developers surveyed, some 27% of respondents stated their organizations have already adopted Kubernetes broadly in production. In September Mesosphere announced support for Kubernetes on its DC/OS platform and in October Docker announced native support for Kubernetes. Three of the top four public cloud providers — Google, IBM and Microsoft — offer Containers-as-a-Service (CaaS) platforms based on Kubernetes. Microsoft even rebranded Azure Container Services (ACS) to AKS, with the “K” standing for Kubernetes. There are also start-ups offering managed Kubernetes-as-a-Service for multi-cloud environments like Platform9. The battle for the orchestration layer has been tough, but it looks like Kubernetes is the frontrunner.
The future includes Serverless/Event-Driven applications. We wanted to highlight the addition of the Serverless/Event-Driven category because it demonstrates the continuous evolution of application architectures and increasingly rapid innovation cycle. Serverless is the next logical abstraction layer since it allows developers to further decompose services to extract even greater value. While developers currently use serverless technologies for background services like cron jobs or storage bucket upload post-processing, as FaaS platforms become more performant and predictable, serverless will be adopted for broader range of use cases. Like the transition from VMs to containers that generated heterogenous environments, we envision a world where composable applications incorporate both containers and functions, each leveraged for their respective strengths. Kubernetes-native FaaS platforms like OpenFaaS, Bitnami’s Kubeless, etc., demonstrate that many developers like solutions that facilitate interoperability between the old-ish and the newer. Here at Redpoint we published a working landscape identifying 60 FaaS offerings to provide clarity around the quickly evolving ecosystem.
It’s been incredible watching the solutions inspired by and often born within the hyperscale computing environments shared with the broader community. Over the past twelve months we’ve witnessed increased excitement and solution proliferation combined with improved clarity around cloud native architectures. A huge congratulation to those involved in these projects and cheers to the year ahead!
VC Astasia Myers’ perspectives on distributed computing…
190 
190 claps
190 
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/planet-stories/cng-part-3-planets-cloud-native-geospatial-architecture-31fb4a20fa77?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
So far we have talked about Cloud Native Geospatial in the abstract, and introduced a core CNG format — the Cloud Optimized GeoTIFF. At this point the most helpful thing is likely to get a bit more concrete about what an actual Cloud Native Geospatial Architecture looks like. Planet has been building one for several years now, so it’s a great place to start.
First a bit of history. Planet was started about 6 years ago in the heart of Silicon Valley, with big ambitions to scale. For the past few years, venture capitalists have looked down upon any start up that is paying for its own hardware. The cloud may be more expensive when startups are small, but if they are hoping to grow big then needing to buy servers can really slow down growth. So Planet was built from the ground up on the cloud, with data going straight from the satellite to the ground station to cloud storage and processed in a truly cloud native architecture.
Some have actually been concerned that Planet might outgrow commercial clouds, generating and storing so much data that it would be cheaper to build its own cloud. Every couple years this question is analyzed deeply, generally by Troy Toman, who built out Rackspace’s public cloud, and it always comes back with a resounding ‘no’. My favorite line of his is ‘the only people who want to build their own cloud are usually people that haven’t done it before’. Though he has the team that could build a world class cloud, they are much more effectively used on the unique Planet problems, like building a storage system for an archive that grows by over 1 million images every day. The economies of scale that Amazon, Google, and Azure are able to achieve are quite hard to compete with, so even if the equation works now it is hard to keep up with their continued innovation. So Planet is quite happy to be a user of cloud services and innovate in its application of the latest technology to earth imagery.
Planet built its imagery pipeline primarily for internal use — to take raw images from space and flat field, calibrate, georectify, orthorectify, detect clouds and then make the processed data available for web and API consumption. The internal ‘jobs system’ manages all the processing, working directly with files stored on cloud storage. The input data comes from cloud storage, and the outputs are also on cloud storage. Outputs for customers sit behind the Planet Data API, to authorize proper access, but every single bit of data produced by Planet is always Cloud Optimized GeoTIFF in a cloud bucket. So all of the data in Planet’s API’s can be streamed with most any geo software built on GDAL, accessing the data in a fully cloud native manner.
In the early days customers would often ask if they could get special access to the Planet Platform to run their own algorithms. The answer would always be to run in the same availability zone of the cloud as Planet and they’d have the exact same access that internal developers do. They often wouldn’t like that answer, as they’d want to believe there was some special access, some faster way of reading the data that Planet had internally. But the point of a cloud native architecture is that internal devs use the cloud with the same best practices that are recommend to customers. Most customers do get a more finished product (ortho-rectified instead of ‘basic’ imagery) than planet uses for internal processes, but both are served in the exact same way. Yes, there are some quirks of working in the cloud, but Planet believes that operating the same way as its customers will create the best experience for customers.
A handful of those customers have really taken advantage of the cloud native geospatial architecture. Santiago & Cintra as well as a couple Agriculture applications were built to run in the same cloud, and they deliver a more finished information product, derived from Planet’s cloud data, to their end users. They are able to deliver up to date results faster, and also save drasticly on hosting costs. They stream the data direct into their processing pipelines, and then render their own tiles or even leverage Planet’s tile serving capabilities.
Those tile serving capabilities are another core pillar of Planet’s architecture. Every single scene and mosaic in Planet’s vast catalog can be rendered instantly in an online map, as a ‘slippy map’ — working just like Google Maps, but with up to date imagery from Planet. A large number of use cases of imagery are still primarily users looking at the picture and making conclusions about the world. In traditional workflows, users needed to download full scenes (often hundreds of megabytes or even gigabytes), and then load them up in expensive GIS or Remote Sensing software just to look at what was captured. With web tiles users can zoom in to full resolution in an online map, using Planet Explorer, before deciding to actually download and do further analysis. Or they may be able to do their complete workflow fully online — making a decision by simply looking at the imagery at full resolution.
Full resolution data in Open California is available to all users of Planet Explorer, and any user who purchases a data subscription also gets that data in full resolution online. While web mapping has been around for quite awhile, the cloud native geospatial workflows makes it the first way to visualize imagery, instead of the last step of publishing to a web map server.
Another key aspect of the web tiles is that they are not limited to just the main Planet GUI (Planet Explorer). The same tiles are available as a service, as XYZ tiles, as well as the WMTS standard from the Open Geospatial Consortium. This enables developers to build apps that leverage the exact same web tiles available in Planet’s web interfaces.
One cool side effect of using an open standard like WMTS is that the tiles can stream in to desktop systems like Esri and QGIS. For visual inspection of data as well as GIS tracing type applications this is an ideal delivery mechanism. Users don’t need to download a huge mosaic to be able to zoom in anywhere on the globe to high zoom levels — they can just stream from the tiles. And they can also easily grab the latest tiles, as there are newly published mosaics every month. And Planet scenes can also be instantly streamed as web tiles, bypassing the ‘activation’ data preparation step.
The other major leg of Planet’s cloud architecture is building everything as an API. The development team has fully embraced microservices, so all the internal functionality to process and serve up imagery is also completely API-first. The team has taken a shining to Go, and the microservices architecture enables us to bring Go in incrementally, providing new services, or being implemented in the refactoring of services. Many of the internal interfaces are built with gRPC, which has been working well.
Working in a microservices manner internally enables Planet to release modular API’s externally more easily. It is the same style of evolution that Amazon Web Services has undergone — first code modular internal components, then start to open those up and productize them more.
The center of the Data API is the catalog of every single image in Planet’s holdings. The philosophy of Planet is to have an open catalog, so anyone can search Planet’s imagery, through the API, through Planet Explorer, or through integrations built on the API. The API can be queried by any combination of parameters — by geography, by time, or by any of the metadata fields. Downloading data is only available in places where a user has access rights, which makes sense for Planet as a data company, but all data is on the cloud and online, able to be downloaded with minimal pain. Making that catalog available as an API that anybody or any program can access is a key tenant of Cloud Native Geospatial architectures — the core location data lives is online. Everything else is a copy of the canonical data. But with Cloud Optimized GeoTIFF everyone should stream that data when using it, instead of even bothering to make a copy.
Building API-first and on the cloud has enabled Planet to approach a number of traditional geospatial processing workflows differently. Acquiring RapidEye gave a deep insight into many of those traditional workflows, as their production pipeline worked in a more manual way. Perhaps the biggest core difference relative to traditional imagery pipelines is generation of imagery products on demand, instead of on acquisition. Planet’s data processing pipeline runs a number of operations when imagery is ingested, but does not turn those into pixel outputs.
Rectification is a good example — a traditional process ends with a ‘3B product’ — an image that is stored on disk. The output of Planet’s rectification is simply the rational polynomial coefficient (RPC) — the compact representation of the ground to image geometry. This is stored as metadata, as are the core pixels that were captured from space. It is only when an end-user requests a Visual or Analytic image that the pixels get transformed into a traditional imagery product. The process is done on demand, requested through the API. This request may come directly from a developer, it could be a user working with Planet Explorer that kicks it off, or it may be third party software that has integrated with the API. Planet’s jobs system manages tens of thousands of virtual machines at once, so it just allocates a few more for the duration of the generation of the images. They are then cached, so that when another user requests the same image they don’t have to wait again for the generation.
Doing everything on demand and through an API then opens up a number of possibilities to tailor the data more to how the end user wants it. The first of these was to enable ‘clipping’, which lets a user request just the geometry they care about, instead of trying to select the scenes that overlap with their area of interest. This will evolve to enable co-registration of images, application of TOA, atmospheric correction, surface reflectance and eventually full analytic processing of images with operations like band math to create indices and even computer vision-based object detection.
The power of doing more analytics on the fly in the pipeline becomes clear when one looks at how imagery used to be delivered. Previously, a couple of customers in South America required co-registered RapidEye and Landsat images, which involved to a huge pre-processing job, as all the existing image products had to be re-processed with the proper co-registration. In the on-the-fly, API-driven world, the production of the co-registered stack of images will happen as the user requested them. Images that are never requested do not need to be processed, but all those that are needed are instantiated upon the user’s request.
Planet’s longer term vision is to expand that processing pipeline, so analytic information and insight is produced on demand, streaming to end-users directly — and often not even showing them the imagery it was derived from. But behind those information APIs there will always be a rich ecosystem of geospatial processes that all occur natively in the cloud. Indeed, many information streams may only be consumed by other software, adding value and supplemental information, to get at the end information product. In this world, the cloud native geospatial architecture becomes a requirement, not a nice-to-have. Passing that massive amounts of data downlinked in and out of the cloud would not be able to keep up with the new data flowing in.
The key to this vision of streaming information feeds that abstract out remote sensing and GIS is a true ecosystem of cloud native geospatial collaborators. To help realize this, Planet is building its platform as a set of loosely coupled components that can integrate with others. Instead of a monolithic platform that one is either in or not, Planet’s architecture can be adopted and adapted by others who see the potential of cloud native geospatial to make the power of insights derived from geo data available to everyone.
Using space imagery to tell stories about our changing…
232 
3
232 claps
232 
3
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/how-to-learn-and-stay-up-to-date-with-devops-and-cloud-native-technologies-44526658a4fb?source=search_post---------33,"Tell me and I forget. Teach me and I remember. Involve me and I learn.
Bejnamin Franklin
A famous Greek philosopher Aristotle (384 BC — 322 BC) has been called the last person to know everything there was to know in the science. Since than science grown exponentially, became divided and subdivided into specialized, narrow disciplines.
Similarly modern technology landscape is evolving in neck breaking pace and unlike Aristotle, we don’t have a luxury to know “all there is to know”. In modern day, especially in the field of practical application of technology, the problem moves from “how to know all there is to know”, to “how to filter information efficiently and know what is important to know”.
In the context of DevOps and Cloud Native technologies, there are two types of activities:
It is important to relay on proven information sources and find the ones that resonate with you the most. Here is a very abbreviated list of resources that I tend to come back to over and over to hone my practical skills.
DevOps Toolkit is a relatively new YouTube channel, but the person behind it is a very experience DevOps engineer. This is my go to channel to learn about new trends and tools.
www.youtube.com
Just me and Open Source is a great YouTube channel with lots of practical examples especially around Kubernetes and Cloud Native technologies.
www.youtube.com
Azure specific channel with very clear explanations.
www.youtube.com
In my experience one of the best learning platforms for Cloud Native technologies
kodekloud.com
Explore awesome lists on GitHub. An awesome list is a list of awesome things curated by the community. There are awesome lists about everything from CLI applications to fantasy books. The main repository serves as a curated list of awesome lists.
github.com
Staying up to date means knowing what is happening in the industry, what are new trends and changes that we should be aware of.
Follow CNCF YouTube channel
www.youtube.com
Subscribe to DevOps and Cloud Native mailing lists:
# Cloud Native News.
blog.nativecloud.dev
# DevOps’ish.
devopsish.com
# CNCF newsletter about Kubernetes
www.cncf.io
At the end getting access to information is almost trivial nowadays. The only prerequisites are having access to internet and curiosity to learn.
What is difficult is how to discriminate important information from less relevant.
Here are is an example process and tools that I use on a daily basis to great success.
Learning must be directed towards a goal. Having a clear plan helps a lot. If you don’t have a plan yet, don’t worry, discover what is interesting you and start researching until you can formulate a plan.
Plan must have a goal, it can be for example
Once you have a plan, follow any system to organize your tasks and work. I like the PARA method:
fortelabs.co
Another simple and useful method is OKRs (Objective Key Results), popularized by Google
en.wikipedia.org
Use simple mechanisms to track your progress. For this Trello works really well.
trello.com
Gathering information is first step. On top of the above mentioned resources, I use Weava to highlight passages of text and images on web pages. Think about it as an intelligent clipboard that helps you organize initial information.
www.weavatools.com
Information acquisition and retention happens naturally by association and linking new information to existing structures. This is best represented by graph data models. Unfortunately, most of the available software like One Note or Ever Note etc does not offer this type of experience.
Once such tool I found recently and I’m really happy with is Obsidian
obsidian.md
Obsidian is described on their web site as
A second brain,for you, forever.
Obsidian is a powerful knowledge base that works on top ofa local folder of plain text Markdown files.
One of the key aspects of how Obsidian and similar software is different is a concept of backlinks. Backlinks associate information bi-directionally and thus mimic how brain works and acquire information.
The goal of this part is to keep adding new information in an associative manner and expand you DevOps and Cloud Native knowledge base.
Those tools are of course not DevOps or Cloud Native specific and can be used in any context.
What we’ve done so far:
Last, but I think most important point of learning is purpose. For me the purpose of learning Cloud Native technologies and DevOps is to enrich the community and give back.
Get involved and share what you learn/work on. Teaching someone is the best ways to learn. Start writing blogs, make content on YouTube or engage with open source community on GitHub or GitLab.
This way learning will not only improve chances for getting better job, but will became part of your daily routine and will benefit everyone around you.
ITNEXT is a platform for IT developers & software engineers…
125 
125 claps
125 
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://blog.getambassador.io/selecting-an-api-gateway-for-continuous-delivery-of-cloud-native-applications-8ba05fa1c74?source=search_post---------34,"With the vast majority of organisations either assessing a move to public/private cloud platforms or actually migrating workloads, much is being written about determining if your workloads and organisation are “cloud native” or “cloud ready”. The excellent books “The DevOps Handbook”, “Architecting the Cloud” and “The Practice of Cloud System Administration” cover the organisational, design and operation perspectives in depth. Individual cloud vendors like AWS, GCP and Azure have written their own migration guides, each with their own approach. However, at Datawire we often see organisations struggle with the assessment of the cloud-readiness of their development teams, tools and processes. One area in particular, which we will explore in this article, is the selection and use of a cloud native API gateway.
An API gateway is a vital piece of infrastructure, as every inbound (user) request will pass through this platform component — it literally is the front door to your system. The API gateway is the place to implement support for cross-cutting concerns like release management (A/B testing, canary releasing, header augmentation), security (authz, authn), and basic fault tolerance (circuit breaking and exception catching).
This is not a particularly new area of technology, and enterprise organisations have driven the development of API gateways focused around the “productisation” of APIs, such as API monetisation and API management. However, migrating to the cloud — and taking advantages of the associated benefits of “cloud native” technologies like containers (Docker), dynamic orchestration (Kubernetes), and the microservice architecture pattern — often require a different focus with the API gateway.
Increasingly we are seeing clients with dynamic and fast-moving businesses move away from enterprise style gateways towards the more “cloud native” API gateways. Although the newer gateways may not offer the GUIs and “drag and drop” management (which are useful for centralised API management and product teams), they do offer support for infrastructure as code (IaC) and declarative configuration that can be used by individual cross-functional, product-focused teams to release and manage services and business functionality as part of their typical development workflow.
We’ve compiled this table below that highlights some of the features we believe are important for organisations looking to embrace cloud native technologies, architectures, and workflows. We often get asked about Ambassador vs Kong, OpenResty (NGINX) vs Ambassador, or Kong vs Traefik, and although this is by no means a complete list of available API gateways, we believe this is a representative sample from across the industry, and should provide insight for teams looking to implement an API gateway:
Once you have chosen your API gateway, you can learn more about how to integrate a cloud native API gateway into your continuous delivery practices and tooling within out latest blog post, “Continuous Delivery — Ambassador API Gateway”.
The next post in this series, “Next-Level Testing with an API Gateway and Continuous Delivery” covers how you can use your gateway to learn how your service will perform under realistic use cases, load, and failure scenarios.
If you have any questions, then please get in touch, or please join us on the Ambassador Gitter.
Developer-First Kubernetes.
86 
1
86 claps
86 
1
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
"
https://itnext.io/defining-the-cloud-native-stack-822c9bcb9ab5?source=search_post---------35,"If you keep abreast of the popular trends in IT, then you cannot have failed to notice the buzz around ‘cloud-native’ in recent years. Everyone talks about it and whole conferences are organized to discuss it. There’s even a prominent, vendor-neutral foundation whose purpose is to promote ‘cloud-native’ computing.
But what is it, and what benefits does it provide us? Let’s start by defining what we mean by…
"
https://medium.com/memory-leak/serverless-cloud-native-landscape-new-from-redpoint-ventures-and-the-cloud-native-computing-181711d885f7?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
In October 2017 Redpoint released our Function as a Service (FaaS) landscape that identified over 50 FaaS offerings. The tremendous feedback we received from leaders in the space coupled with the dynamic environment prompted us to revisit the landscape and provide an update. Today we are releasing the Serverless Cloud Native Landscape v0.9.5, which has been reviewed by the CNCF serverless working group but remains a work in progress. Our plan is to continue to iterate on this landscape as long as there are developments so please send us feedback at amyers@redpoint.com.
As investors we are excited by the transition to serverless technologies and its ability to lower barriers to building new technologies and accelerate software development. Most of all, we remain inspired by entrepreneurs building FaaS-focused businesses today. If you’re working on a FaaS technologies please don’t hesitate to reach out! We are always learning from our conversations with FaaS leaders, and we look forward to meeting and collaborating with you!
Libraries
· Python-λ: Library for developing and deploying serverless Python code in AWS Lambda
Tools
· Dashbird: Monitoring, analytics and error tracking of Lambda functions
· IOPipe: Application performance monitoring for Lambda
· LambCI: A continuous integration system built on AWS Lambda
· Microcule: SDK and CLI for spawning streaming stateless HTTP microservices in multiple programming languages
· Node Lambda: Command line tool to locally run and deploy your node.js application to Amazon Lambda
· Stackery: Serverless operations console that provides automation, predictable performance, and operational control
· Thundra: Observability for AWS Lambda (from OpsGenie)
Frameworks
· AWS Chalice: Python serverless microframework for AWS
· Claudiajs: Automates all the error-prone deployment and configuration tasks, and sets everything up the way JavaScript developers expect out of the box
· Dawson: Serverless web framework for Node.js on AWS
· Deep: Full-stack JavaScript framework for cloud-native web applications
· Gordon: Tool to create, wire and deploy AWS Lambdas using CloudFormation
· Kappa: Command line tool to make it easier to deploy, update, and test functions for AWS Lambda.
· Lambda SAM Local: Prescribes rules for expressing serverless applications on AWS
· Serverless: Single toolkit for deploying serverless architectures to any provider
· Shep: Framework for building APIs using AWS API Gateway and Lambda
· Sparta: Go framework for AWS Lambda microservices
· Spring Cloud Function: Support a uniform programming model across serverless providers, as well as the ability to run standalone (locally or in a PaaS)
· APEX UP: Helps deploy serverless apps, APIs, and static websites
· Zappa: Helps developers build and deploy server-less Python applications on AWS Lambda and API Gateway
Platforms
· AWS Lambda: Platform that lets developers run code without provisioning or managing servers
· Auth0 Webtask: Platform written in Node.js
· Clay.run: Creates serverless HTTPS functions instantly
· Google Functions: Serverless environment on Google’s infrastructure
· Hyper Func: Docker-centric Serverless platform that lets users wrap functions in Docker images and have them run on demand.
· IronFunctions: Open source serverless platform that can run anywhere
· IBM Cloud Function: FaaS based on Apache OpenWhisk
· Microsoft Azure Functions: Event-driven serverless compute experience
· Nano Lambda: Automated compute service which runs and scales your micro-services
· Oracle Fn: Serverless app platform empowering enterprises to reliably scale their Docker-based jobs on any cloud, public, private, or on-premises
· Overclock (OvrClk): Serverless platform provider that delivers high-performance workloads at the edge
· Red Hat OpenWhisk: Lets developers run event-driven, serverless functions at scale
· PubNub Blocks: Serverless environment to execute functions on the edge, transforming, enriching, and ﬁltering messages as they route through the PubNub network
· Spotinst Functions: Fully managed serverless compute platform that allows running code without thinking about cloud providers
· Spring Cloud Function: Helps users create decoupled functions for serverless hosting providers or any other runtime target without vendor lock-in and enables the use of Spring Boot features like auto-configuration, dependency injection, etc. on serverless providers.
· Standard Library (StdLib): Standard library for functions to help turn them into infinitely scalable, self-healing web services paired with compute
· Syncano: Serverless application platform to build powerful real-time apps
· Twilio Functions: Serverless environment to build and run your Twilio code
· Weblab.io: Platform to run functions that are self-heal and scale as needed
· Zeit Now: Allows users to take their JavaScript (Node.js) or Docker powered websites, applications and services to the cloud with ease
Hybrid
· Apache OpenWhisk: Serverless, open source cloud platform that executes functions in response to events at any scale
· Binaris: FaaS platform that’s optimized for predictable, low latency
· Cloudboost: Open source serverless platform to build scalable apps
· FX: Tool to help deploy Function-as-a-Service on a user’s own server
· Galactic Fog: Container management and serverless implementation
· Iguaz.IO Nuclio: High-performance serverless event and data processing framework
· LunchBadger: Multi-cloud Platform for Microservices and Serverless
Kubernetes-native
· Bitnami Kubeless: Kubernetes native serverless platform
· Red Hat Funktion: Open source project which implements an event-based Lambda style programming model for Kubernetes
· OpenFaaS: Serverless Functions Made Simple for Docker and Kubernetes
· OpenLambda: Apache-licensed serverless computing project, written in Go and based on Linux containers
· Platform9 Fission: Open-source, serverless platform built on Kubernetes with a focus on developer productivity and high performance
Security
· PureSec: Security built for serverless architectures
· Snyk: Finds and fixes vulnerabilities in dependencies
VC Astasia Myers’ perspectives on distributed computing…
114 
4
114 claps
114 
4
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/planet-stories/cng-part-7-a-vision-for-the-cloud-native-geospatial-ecosystem-7a55ae782690?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
So, I’ve decided to bring the ‘Cloud Native Geospatial’ blog series to a close with this post. I’ve got a lot more to write on the topic, but it makes sense to wrap up the core articulation of the concept. We’ll end looking forward, describing a potential vision of what the world could look like if we truly build geospatial architectures in a cloud native manner. Future posts will delve deeper into many of the concepts mentioned here, and I’ll continue to tag everything with ‘Cloud Native Geospatial.’
I welcome anyone else writing on similar topics to use the same tag, and encourage others building next-generation platforms to describe their architectures in detail. Having an open dialogue about how everyone is building their solutions can enable a more interoperable future, and one that multiplies the impact of geospatial from a relatively small niche toward a fundamental way for everyone to understand their world.
In an ideal Cloud Native Geospatial future, the first major difference to how people work today is that searching for relevant geospatial information should become much easier. Most all ‘search’ capabilities today are provided by the owners of the data, which stands in stark contrast with the web. This is because there was sufficient data fully online that companies like Google could create value by making it accessible for users. And Google could access all that data in a standard way.
Although numerous ‘catalogs’ have arisen in the geospatial world, they most often did not have direct links to the actual data for exploitation. And they were built on a paradigm of each provider standing up their own catalog server and website to provide search. Once there is sufficient valuable data that is online and accessible, both programmatically and by users, then other organizations will start to index it and enable searches on the data.
This can be easily seen with Landsat data. For many years the USGS has had the data online, but it was not very accessible. Google Earth Engine then mirrored the data, and it was slightly more accessible, since it could be downloaded in bulk, without having to navigate through the USGS user interface. But it was stored as 800 megabyte files, which still would take a long time, especially if you only needed a small subset of the data.
Amazon Web Services took the key step with their Landsat Public Data Set release, making the data truly accessible for the first time, by doing an early version of Cloud Optimized GeoTIFF’s. Within a year GUI tools from Planet, AstroDigital, ESRI and EOS all add Landsat search capabilities. The major difference was that they could do far more than just show ‘search results,’ they could create web tiles and make the data available for download to users, which added a major value to their platform.
As more valuable imagery is available online with crawlable metadata and available as streaming Cloud Optimized GeoTIFFs, we can expect more search services to add more data. And as workflows are done fully online it will become possible to ‘rank’ results by their actual usage, bringing relevant results to geospatial in the way that Google innovated with PageRank, to discover what is relevant by mining the structure of the web to determine what is popular.
The next difference in a truly cloud native geospatial ecosystem is that collaboration will be at the core of everything. The geospatial world has one amazing example of collaboration, which is OpenStreetMap (OSM). All OSM data is online and accessible, which is the building block that makes collaboration possible. But many of the traditional geospatial workflows still are centered on people’s desktop environments. And even when data is available on the web it is in a ‘publish’ paradigm — you can read it like a web page, but you can’t start collaborating on it like Wikipedia or a Google Doc.
Having all data available in canonical online locations opens up more possibilities for collaboration. The first is in creation of maps and annotations. The main workflow today is still to create a map and then embed it in Powerpoint. It is a static thing, an artifact of whatever the original analyst thought of the situation. Instead that should be an online editable object, that others can add to, make their own, or collaboratively edit.
When someone tweets it can spark a whole conversation, tracking the history of every comment. Imagery marked up with notes and added vector information should similarly kick off a conversation, but one that is done directly on the map, tracking what different users change or add. The tooling to do this should be as easy as creating a Powerpoint — anyone should be able to tweak someone else’s map, or to just create their own map-based commentary on some imagery or basemap.
Another collaboration potential is with the processing of geospatial information — applying different algorithms to create derived data and gain greater insight. In a cloud native geospatial environment, tools that do this processing of data would not be locked in individual desktops but would be available online. You can see the start of this in RasterFoundry’s Lab Templates:
There is a public catalog of tools to process imagery, and any user can publish their algorithm to the public catalog. In time there will also be sharing of the templates within an organization, but not with the public. Each ‘tool’ can also be optimized to work with a single dataset. This opens up workflows where one user can take another’s template and adapt it to their dataset, or tweak parameters to their liking. And then usage of tools can be tracked, so people can see the most popular tools or search by tools for their dataset.
Drawing insights and doing processing of data can become a collaborative process, because the data and algorithms are all online where they can be instantly shared and tracked. In time the ecosystem should develop open standards to even share algorithms across different platforms. The key in the cloud ecosystem is to send the algorithms to the data, since that will most always be faster.
The other major shift will be toward maps and insights that constantly update with new information, instead of static maps. Most maps today are limited because the data is locked in at the time the map was created. When maps are built directly off of cloud native data they can also be constantly updated as new information comes in. Planet offers basemaps of monthly imagery as tiled services. Building a map with that as a backdrop can be automatically updated every month with the latest imagery, so the map is always up-to-date.
Tapping into vector data from Carto as web tiles would make the overlays automatically update as new information comes in, say crime data or new construction. And the update process can be run automatically when new data comes in, since all processing is on the cloud. The dominant form of interaction in a cloud native geospatial environment may well come to be the ‘alert’ — a real-time notification that was a result of processing disparate information as it comes in to the cloud. It will embed geospatial into our lives, into existing apps and tools, instead of making users jump into their GIS environment.
Realizing a vision of a real-time, collaborative, accessible world of geospatial information is not possible for any single organization to achieve alone. The goal of GIS and Remote Sensing is to provide actionable information about the world to drive better decisions. No single data source is sufficient to make great decisions: it must be combined with other sources and validated to turn in to real insight.
To realize the potential of the vision described above means that the different software systems and data providers must interoperate with one another, using open standards that enable anyone to easily become part of the ecosystem. The cloud opens up the potential to make geospatial data more accessible, collaborative and up-to-date. But realizing that potential depends on diverse organizations adopting common ways to describe their data along with interoperable API components.
Once that baseline is in place I believe we’ll see a huge leap in innovation, leading to the ability to gain real insight about our earth and what people are doing with it. And that insight will drive decisions, to make a better planet for all of us. I believe that Cloud Native Geospatial is the first step, and together we can establish an interoperable and accessible base of raw information that can be built upon.
I hope this series of articles has been informative. In my future posts I’m going to continue to dive deeper into building a real interoperable, standards-based foundation for the next generation of geospatial processing and insight (starting with WFS). I believe we need new standards — small pieces loosely coupled that can be remixed and combined in different ways — on top of the core elements of Cloud Native Geospatial architectures (COG’s, catalogs + web tiles).
The other major topic to explore more deeply is the implications of computer vision and deep learning for the geospatial field. The potential to extract meaningful information and insight out of imagery is huge. But there is still a lot to do to out the systems, APIs and interfaces to bring those insights to the broader world. There’s been a lot of internal work at Planet, and I’m excited to share how we’re thinking about it and building the future.
Using space imagery to tell stories about our changing…
212 
212 claps
212 
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/@azeynalli1990/cloud-native-architecture-patterns-60a010d90978?source=search_post---------38,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ali Zeynalli
Oct 17, 2021·3 min read
Software Architecting might take a slightly different approach in applications that are build in cloud-native environments. These applications are extensively built in forms of microservices. Beside that, these applications should be able to run in dynamically orchestrated and containerized environments in…
"
https://faun.pub/the-most-read-devops-cloud-native-stories-from-2020-ed7ef7b59d2e?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
We hope it’s not too late to wish you a Happy New Year. It’s still a mystery how 2020 turned out to collectively be one of our challenging years in recent times, but we are grateful in spite of it all. Here’s to hoping 2021 is much better for all of us. Welcome to the new year.
We would like to especially thank you for being an ardent reader of our publication, and to the authors who contribute to our growing numbers, thank you — you are amazing! Your insightful tutorials, well-written blogs, tricks, and tips have helped thousands of DevOps readers, newbies, and professionals around the world. Thank you again.
On this note, we have compiled the Most Read Stories of 2020 — articles that readers found really informative, educating, and interacted with the most. Enjoy the read.
By Kim Wuestkamp
We started the year with this challenge by Kim Wuestkamp. A Kubernetes CKA hands-on challenge that was tested on k8s 1.18. It’s no surprise that this tops our list — readers loved this challenge and a lot of them participated. The task for this challenge was investigating the multi-container pod issue. The rules, notices, and scenario set up are listed in this article and of course, it also includes Kim’s solution — which you shouldn't take a peek at until you have finished your own challenge.
Not late to try it out if you haven’t yet; check it out here https://medium.com/faun/kubernetes-cka-hands-on-challenge-1-multi-container-issue-5a8c007686ed
By Piotr
Congratulations again Piotr for passing the CKA Exams. In this article, he shares his preparation strategies, learning tips, and the resources that helped him pass the Certified Kubernetes Administrator (CKA) exam. One of the tips he shared was creating your own Kubernetes cluster to play around with; according to him, practice is very important. Then he goes on to share resources on how to set us Kubernetes and AKS cluster. If you’re looking to write this exam as a DevOps Professional this year, then this is a good read for you.
Find them here https://medium.com/faun/preparation-and-resources-for-cka-exam-ca868fc678c9
By Cloud_Freak
Next on our list is an article by Cloud Freak. They started by countering the popular phrase “It works on my machine” that is often used by developers to escape the burden of having to investigate problems experienced by end-users that they are unable to replicate on their development machines. So, in this article, they discuss two of the most popular tools — Docker and Vagrant — to see how they try to solve these problems and help you choose which one to use for development.
Read the full article here https://medium.com/faun/vagrant-vs-docker-which-one-to-use-for-development-122c99ad563f
By Vamsi Jakkula
Vamsi’s articles almost always made our curated list and it is no surprise that he is on this list. We loved his articles just as much as our readers did. Here, he discusses how we can use Splunk Connect for Kubernetes for log collection from Minikube i.e. single-node Kubernetes Cluster and a local Splunk instance on Ubuntu 18.04. He goes in-depth and discusses architecture, prerequisites, Splunk configuration, and how to deploy Splunk for Kubernetes using Helm. This is a must-read tutorial.
Find it here https://medium.com/faun/logging-in-kubernetes-using-splunk-c2785948fdc0
By YURY MUSKI
After being inspired by the news that HTTP3 protocol was released and can be played around with by Developers; Yury attempted to, faced some issues during implementation and then wrote this article to help other Developers who might have the same problems. This tutorial by Yury takes us through what is needed to set up HTTP 3 Nginx support. He shares what is needed to do this and how it can be tested.
Read the full article here https://medium.com/faun/implementing-http3-quic-nginx-99094d3e39f
By Alex Ellis
In Alex’s words, “For some teams, Kubernetes is the new hotness, a must-have; either because of bottom-up pressure or from top-down because your CIO heard that it will solve all your problems. You may not actually need Kubernetes.” In this story, Alex shares from his point of view why your team may not need Kubernetes. He discusses moving to the cloud — the pros and cons of moving to the cloud, moving to containers — the pros and cons of moving to containers, moving to Kubernetes — the pros and cons of doing it; and gives you an opportunity to answer the question “why are you doing this?” truthfully as a developer.
It is an interesting take and you should read it. https://medium.com/faun/your-team-might-not-need-kubernetes-57240e8d554a
By Kim Wuestkamp
What do you know! Another interesting entry by Tim, and also on Kubectl. In this tutorial, he shares a collection of the fastest ways to create k8s resources using kubectl ≥ 1.18. He reiterates the importance of being fast in the Kubernetes certifications. According to him, when you need to create new resources, it will be much faster generating most of the YAML using kubectl and just editing the rest using your favorite editor. So he shares a step by step method on how to do this.
Read here https://medium.com/faun/be-fast-with-kubectl-1-18-ckad-cka-31be00acc443
By Gareth Erskine-Jones
In this tutorial, Gareth starts by explaining how an Angular app, when compiled and deployed, is served as a static website — an HTML file, a bunch of javascript and CSS files, and perhaps some additional assets like images and fonts. This tutorial is a quick walk-through of the relevant parts of his Nginx configuration that works well with a simple, out-of-the-box Angular application and gets an “A” score on securityheaders.com.
You can find the tutorial here https://medium.com/faun/my-nginx-configuration-for-angular-6f748a4ff683
By Vamsi Jakkula
I don’t think our compilations would ever be complete if there weren’t multiple articles by Vamsi. Thank you, Vamsi for an amazing 2020 with us. Here’s another interesting read that discusses how we can scale Kubernetes pods using Horizontal Pod Autoscaler(HPA) based on CPU and Memory. He also shows us how HPA can be implemented on Minikube. It is a well thought out tutorial and you will find it useful and educating.
Read here https://medium.com/faun/kubernetes-horizontal-pod-autoscaler-hpa-bb789b3070e4
By Ismail yenigül
After spending a day deploying Nextcloud with docker-compose and finding out that the official Nextcloud documentation recommends Nginx as a reverse proxy, Ismail shares with us why he thinks Traefik 2.0 is much better and easier. He wrote this article concentrating on a single docker-compose.yml with Traefik containers in the same network, then goes on to share a quick documentation to deploy Nextcloud with docker-compose.
Find the full article here https://medium.com/faun/deploy-nextcloud-with-docker-compose-traefik-2-postgresql-and-redis-fd1ffc166173
By Vamsi Jakkula
Another tutorial entry by Vamsi that discusses how to set up CI/CD Pipeline using Jenkins to deploy on Kubernetes. He shares the step by step processes required to build and deploy a Jenkins stand-alone pod in Kubernetes. It is a hands-on tutorial and if you have been trying to figure out how this is done, then you’re on the right track.
Find the full article here https://medium.com/faun/ci-cd-pipeline-using-jenkins-to-deploy-on-kubernetes-cf2fd5e185b8
By Bradley Simonin
If you’re learning or using Ansible as a Developer, then this article by Bradley is a must-read. This article is a quick document on how to set up and use the Ansible dynamic inventory plugin for AWS EC2 host management. He starts by explaining how Ansible comes with various dynamic inventory plugins — one of which is aws_ec2. Then he explains how the aws_ec2 plugin is a great way to manage AWS EC2 Linux instances without having to maintain a standard local inventory.
This is quite an interesting read. Find it here https://medium.com/faun/learning-the-ansible-aws-ec2-dynamic-inventory-plugin-59dd6a929c7f
By beSharp
beSharp are one of our regular contributors, we love all of their articles since they’re quite useful. They start this article by telling us how using Docker as an infrastructure container for local testing purposes is becoming more and more common these days by Developers. According to them, developers exploit docker-compose functionality to create an infrastructure stack that contains their application, web server, and databases in different docker containers. In this article, they discuss how to deploy an entire solution inside your AWS environment using AWS ECS Fargate service but start first by explaining what AWS ECS Fargate is.
Read here https://medium.com/faun/deploy-of-a-docker-compose-application-in-an-aws-environment-using-ecs-cli-a4441ed3d445
By Chris I.
Using his personal experience of how he wanted to track memory utilization during a long-running job on an EC2 but found out it is not monitored by default, Chris takes us on a step-by-step tutorial on setting up automatic monitoring with CloudWatch. He discusses the alternative — that is if you only want to check the memory or hard disk utilization at the current time., launching an EC2, creating an IAM role, configuring the EC2, and finally, monitoring in CloudWatch.
Read here https://medium.com/faun/monitor-memory-and-hard-disk-utilization-on-aws-ec2-instances-with-cloudwatch-3c191039d50b
By Munish Goyal
This tutorial by Munish takes us through orchestrating parallel jobs on Kubernetes with the container-native workflow engine. He starts by describing Argo Workflows as an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Then he goes on to discuss the steps in Argo workflow, share the step by step method involved in deploying the application and Argo workflow specs. This article basically explores Argo as a tool to orchestrate workflows on Kubernetes and shows how it can also be used to design data workflows instead of using language-specific tools such as Airflow.
Find the complete article here https://medium.com/faun/designing-workflows-using-argo-9d0dc5036348
By Ismail yenigül
This article is another entry by Ismail that shows us how to build a Docker image from private GitHub repo and push it to a docker hub with a declarative pipeline in Jenkins. He starts by creating a sample repo with basic Dockerfile and Jenkinsfile and then describe the steps required to achieve his goal. As with other tutorials by Ismail, it is also hands-on. So get ready to learn and try this on your own.
You can find it here https://medium.com/faun/docker-build-push-with-declarative-pipeline-in-jenkins-2f12c2e43807
By Daniel Thiry
Have you been having issues working with Kubernetes Multi-Tenancy? This guide by Daniel teaches us how to tackle typical Kubernetes multi-tenancy challenges by implementing some of the best practices he has come across. According to him, Kubernetes is not exactly a multi-tenant system, and how getting multi-tenancy right comes with some challenges. In this article, he describes these challenges and how to overcome them and shares some useful tools for Kubernetes multi-tenancy.
Find the full article here https://medium.com/faun/kubernetes-multi-tenancy-a-best-practices-guide-88e37ef2b709
By Daniel Correa
Daniel is a Professor at the University EAFIT in Colombia, where he teaches software architecture and has a Ph.D. in computer science. After reading Continuous Delivery with Docker and Jenkins by Rafal Leszko, he decided to apply this understanding of CI/CD with Docker and Jenkins with Laravel instead. This article explains how to apply some CI/CD principles in Laravel projects (with the use of Jenkins, Docker, and Github).
Read here https://medium.com/faun/configure-laravel-8-for-ci-cd-with-jenkins-and-github-part-1-58b9be304292
By Tianchen Wu
This article by Tianchen describes a systematic way of applying terraform at scale and is organized into three parts — essential component — modules, static layout to store .tf and .tfvars, and dynamic mechanism to deploy .tf. In summary, this article introduced a modularized hierarchical architecture to enable systematical development and deployment of terraform at scale.
Read the full article here https://medium.com/faun/terraform-at-scale-modualized-hierachical-layout-cb5dbe5a368d
By Kubernetes Advocate
Our compilation would not be complete without an entry by Kubernetes Advocate. He started with Hell basics then showed us step by step how to use this tool to create Kubernetes Deployments on AWS.
Find here https://medium.com/faun/kubernetes-deployment-with-helm-charts-6f571022a9a9
By Raphael Socher
An amazing piece of writing by Raphael. Thank you for joining the Faun family as an author and for your contributions so far. This article basically takes a look at Terraform templates, providers, resources, and modules. Raphael starts by explaining what Terraform Templates are. Although it is geared towards developers who are familiar with the basics of Terraform, he shares links and resources for beginners.
Find the full article here https://medium.com/faun/an-introduction-to-terraform-templates-a458d813fe95
By Vikram Shinde
It’s almost no surprise we have two articles that discuss Terraform back to back. Here’s a Terraform guide by Vikram for developers who want to take the HashiCorp Infrastructure Automation Certification. According to him, developers will be best prepared for this exam if they have professional experience using Terraform in production, but performing the exam objectives in a personal demo environment may also be sufficient. It is a long but insightful read, filled with learning tips and tricks.
If you’re looking to write this exam this year or anytime at all, then bookmark this https://medium.com/faun/terraform-terraform-terraform-115d0aead669
By Martin Thoma
Martin draws our attention to how PDF, Excel, SVG, ebooks — all use XML and how they can be vulnerable. He starts by telling us how XML is probably the most commonly used markup language and shares an interesting property about XML — how you can reference external entities by including another file. He then goes on to show us why we should care about XML vulnerabilities, the types of XXEs, and mitigations.
This is quite an interesting read on cybersecurity. https://medium.com/faun/xxe-attacks-750e91448e8f
By Rishi Raj Singh
This article by Rishi discusses EInnovator Cloud Manager, a Kubernetes (K8) based cloud management front-end dashboard aimed to simplify the experience of deploying, scaling, and configuring applications, services, jobs, and cloud resources. He also discusses the benefits of how it can also be used for setting-up CI/CD pipelines in integration with Git-based VCS.
Read here https://medium.com/faun/do-it-all-kubernetes-dashboard-81375833e01c
By Ivan Porta
Ivan begins this article by telling us how every time you run a pipeline using Microsoft-hosted agents, Microsoft allocates a new virtual machine to it. One or more agents (a computing infrastructure with installed agent software) begin executing their jobs one at a time. And after the pipeline is completed, the virtual machine is discarded, and its content removed. He then goes on to explain how this performance issue made many Azure Pipelines users ask for caching features. In July 2019, Microsoft released the public preview of pipeline caching in Azure Pipelines. This feature aims to improve the hosted build agents’ performance by adding caching for common scenarios like package restoration. This article discusses how to reduce your time build using caching in Azure pipelines.
Read here medium.com/faun/reduce-your-build-time-using-caching-in-azure-pipelines-7a7bd0201cee
And that is it! Our most-read articles of 2020. A huge thanks to our amazing authors that made this possible, and to our amazing readers — you are the best! We hope you keep coming back to read, learn more, and of course share our publication with your communities and new developers looking to dive into DevOps.
Have an amazing 2021 and don’t forget to join Faun Topics.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
117 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
117 claps
117 
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/epbf-understanding-the-next-gen-networking-security-observability-for-cloud-native-workloads-1fe8ad87ee0f?source=search_post---------40,"Recently I got a chance to learn about eBPF from Liz Rice at one of the InfoQ live sessions. And I was surprised to see the superpowers and capabilities it can bring to the table when it comes to networking, security, and observability.
"
https://developers.ascendcorp.com/%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%87%E0%B8%B2%E0%B8%99-cloud-native-java-patterns-for-microservice-architectures-meetup-meetup-%E0%B9%81%E0%B8%A3%E0%B8%81%E0%B8%88%E0%B8%B2%E0%B8%81-cloud-af84ccc80374?source=search_post---------41,"เนื่องจากผมได้มีโอกาสได้ไปร่วมงาน Cloud Native Java Patterns for Microservice Architectures Meetup ซึ่งเป็น Meetup แรกจากกลุ่ม Cloud Native Java — Bangkok โดยสถานที่จัดงานในครั้งนี้ก็คือที่ Ascend Group อาคาร AIA Tower ชั้น 18 ซึ่งอยู่แถวๆ MRT สถานีศูนย์วัฒนธรรมฯ โดยงานในครั้งนี้เป็นแบบ public ซึ่งผู้ที่เข้าร่วมงานไม่มีค่าใช้จ่ายใดๆ โดยทาง Ascend มีอาหารพร้อมเบียร์เย็นๆไว้บริการผู้มาร่วมงานแบบไม่อั้น เท่านั้นยังไม่พอ ทาง Ascend ยังได้แจกเสื้อยืดสวยๆให้ผู้มาร่วมงานทุกคนแบบฟรีๆอีกด้วย
สำหรับเนื้อหาของงานได้ทำการแบ่งออกทั้งหมด 5 sessions จาก 7 speakers โดยทุก presentations เป็น “ภาษาอังกฤษ” แต่เป็นภาษาอังกฤษในระดับที่คนไม่แข็งแรงภาษาอังกฤษอย่างผมสามารถเข้าใจได้ไม่ยาก
สำหรับ session แรกของงานเป็นหัวข้อ An Introduction to Distributed Tracing and Zipkin โดยคุณ Adrian Cole ซึ่งปัจจุบันทำงานอยู่ที่ Spring Cloud OSS team จาก Privital และ Adrian Cole ยังเป็น contributor ของ open source projects ดังๆหลาย projects ไม่ว่าจะเป็น Apache jclouds, Netflix feign และ Zipkin project
ใน session ของ Adrian Cole จะเป็นการนำเสนอเกี่ยวกับการ debug latency เพื่อหาว่า component ไหนในระบบของเราที่ทำงานช้าหรือทำงานไม่ปกติ พร้อมการ demo การ debug latency ซึ่งทำให้เราเข้าใจ production environment ของเราด้วย Zipkin
Adrian Cole เล่าว่าการที่เราจะเข้าใจ latency ทำไมบาง APIs หรือบาง components ใช้เวลานาน โดยอันดับแรกเราต้องเขาใจ architechture ของระบบก่อน เพราะว่า microservice จะอยู่ในรูปแบบ graphs ของ components ต่างๆที่แพร่กระจายอยู่บน network ซึ่งบางครั้งการ call กันระหว่าง component สามารถ delayed หรือ fail เนื่องจากหลายๆปัจจัย ถ้าเราเข้าใจ architechture ของระบบของเรา ก็จะทำให้เราสามารถทำ latency analysis ซึ่งเป็นการแตกย่อยปัญหาในส่วนสำคัญต่างๆของระบบ จนทำให้เราเข้าใจอะไรที่ทำให้ delayed หรือ fail จนส่งผลถึง users ของเรา
สำหรับการ distributed tracking ระบบเป็นการเก็บ latency grahps (traces) แบบ end to end ที่ใกล้เคียง realtime มากที่สุด ทำให้สามารถเปรียบเทียบ traces และเข้าใจว่า ทำไม Api เดียวกัน บาง request ถึงใช้เวลานานมากกว่า requests อื่นๆ นอกจากนั้น Adrian Cole ยังพูดถึง distributed tracking vocabulary ประกอบไปด้วย
Tracers systems เป็น tools ที่สามารถช่วยในการเฝ้าตรวจสอบระบบโดยการใช้ data เช่น การรวม spans เข้าด้วยกันเป็น trace trees ทำให้เราเห็นจุดที่ไม่ปกติของระบบ โดยปกติแล้ว tracing system จะมีการเก็บรักษาไว้ช่วงระยะนึง ปกติแล้วจะใช้เป็นหลักวัน โดย Adrian Cole ได้ยกตัวอย่าง เช่น Uber จะเก็บไว้ 2 วัน นอกจากข้อดีของ tracing ที่ทำให้เราเข้าใจระบบมากยิ่งขึ้น เท่านั้นยังไม่พอ tracing ยังช่วยให้เราหา service หรือ component ที่ไม่ได้ใช้งานในระบบเป็นต้น
Zipkin เป็น distributed tracing system ที่ช่วยในการจัดการรวบรวมและค้นหา data ซึ่งจะช่วยในการแก้ปัญหา latency ใน microservice architectures โดย Zipkin แรกเริ่มสร้างโดย Twitter ตั้งแต่ปี 2012 และในปี 2015 ก็ได้เป็น OpenZipkin บน Github
การใช้ Zipkin ทำให้เราเข้าใจ transaction ต่างๆของ users ที่เรียกกันระหว่าง services ต่างๆใน microservice โดย services จะทำการส่ง spans ไปที่ Zipkin โดย Zipkin จะทำหน้าที่รวมรวมและจัดเก็บ data ลงใน database ไม่ว่าจะเป็น MySQL, Cassandra หรือ Elasticsearch และ Zipkin ยัง provided API และ UI สำหรับการ lookup ข้อมูลเหล่านั้น
สำหรับ session Spring Cloud Service Discovery with Eureka ทาง speaker ได้ทำการ introduce และอธิบาย key concept ของ Service Discovery พร้อมทำการ demo ในการสร้าง Service Discovery ด้วย Eureka
Service Discovery เป็นระบบในการสร้าง client-server model โดยเราสามารถมี Eureka Servers ได้หลายตัว โดยสามารถทำเป็น cluster และสามารถมี Eureka Clients ได้หลายตัว เช่นกัน โดย client จะมาทำการ registry กับ Servers และ Servers ก็จะทำหน้าที่ในการหาตำแหน่งของ Clients ให้เอง โดยในการใช้งานใน code เพียงแค่เราระบุชื่อ service ที่เราต้องการ ตัว Service Discovery ก็จะทำการส่ง List addresses ของ clients มาให้เราใช้งาน และ Service Discovery จะช่วยแก้ปัญหา dynamic ของ service addresses ได้ด้วย
คุณ Wutt ยังได้เสนอเกี่ยวกับ Client Side Load Balancing ซึ่งได้ยกตัวอย่าง load-balancer แบบเดิม ซึ่งจะเป็น server-side load balancing ซึ่งมีข้อเสีย คือ single point of failure หมายความว่า ถ้า load balancer down ทุก applications ที่ใช้ load balancer ตัวนั้นก็จะ down ด้วย ซึ่งจะตรงนี้ทาง server-side load balancing จะใช้วิธีสร้างตัว slave มาคอย support ซึ่งมีข้อเสียที่ไม่ได้ใช้งานใดๆเลยถ้าตัว master ไม่ได้เสีย และด้วย server-side load balancing ทำให้เราไม่สามารถทำ horizontal scalability ได้
ด้วยข้อกำจัดต่างๆของ load balancing แบบเก่า คุณ Watt ก็ได้แนะนำการทำ Client Side Load Balancing ด้วย Ribbon และ Service Discovery โดยเราไม่ต้องมาจัดการกับ List addresses ของ clients อีกอีกต่อไป ในการใช้งาน ถ้าเราใช้ spring Boot เราสามารถใช้ Ribbon ร่วมกับ Resttemplate เพียงแค่ทำการใส่ annotation @LoadBalanced
สำหรับ session ของคุณ Max นำเสนอ Spring Cloud Routing with Zuul ซึ่งเกี่ยวกับการ routing API ด้วย API Gateway component ซึ่งทำหน้าที่เป็น reverse proxy ทำหน้าที่ในการ forwards requests จาก client ไปหา services เป้าหมาย ซึ่งการใช้ API Gateway ทำให้เราสามารถ mapping routes ของทุก services ใน microservice เป็น single base URL และสามารถใช้ความสามารถต่างๆของ Zuul ในการ handle request
Zuul คือ services gateway ที่ง่ายในการ set up และใช้งานด้วยไม่กี่ annotation และ configutaion เพียงเล็กน้อย โดยสามารถทำ mapping routes สำหรับทุก components เป็น single URL และยังสามารถใช้ความสามารถ filter ของ Zuul ในการนำมาปรับใช้งานกับ besiness logic
สำหรับ filters ของ Zuul จะมีอยู่ 3 ประเภท คือ
และทุก filter สามารถใช้ตัวเลข ในการระบุ “ลำดับ” ของการเรียกแต่ละ filter เช่น Pre filter order 1 จะเรียกก่อน Pre filter order 99 เป็นต้น
โดยคุณ Max ได้ยกตัวอย่าง use case ของการใช้งาน Zuul fillters ดังภาพด้านล่าง
สำหรับ session ของคุณ Might เป็นเรื่องของการทำ client resiliency patterns ซึ่งคือ การปกป้องหรือว่าป้องกันการ call service ต่างๆใน microservice หรือแม้กระทั้งติดต่อกับ database ในกรณีที่ resource เหล่านั้น failing หรือตกอยู่ในสภาวะย่ำแย่
ในทุกๆระบบโดยเฉพาะอย่างยิ่ง distributed systems จะต้องประสบกับ failure และบางทีพอเจอ failed แล้วระบบไม่รองรับ “fail fast” ซึ่งคือ ระบบจะไม่ทำการ consume resources เช่น database connections และ thread pools ในกรณีที่ service มีปัญหา กลับกลายเป็นว่า ยังคงมี call service ที่มีปัญหาเรื่อยๆจนนำไปสู่การ crash ทั้งระบบ แล้วเราเราจะสร้าง applications ที่ยืดหยุ่นพอที่จะสามารถจัดการกับ failure เหล่านั้นอย่างเหมาะสมได้อย่างไร คำตอบก็คือ เราต้องสร้างระบบที่รองรับ fail fast ซึ่งโดยปกติแล้ว การสร้างระบบที่รองรับ client resiliency patterns มีด้วยกันหลายรูปแบบ ไม่ว่าจะเป็น client-side load balancing, fallbacks และ circuit breakers เป็นต้น โดยใน session ของคุณ Might ได้นำเสนอในรูปแบบของการทำ fallbacks และ circuit breakers ด้วย Netflix Hystrix
ด้วย Netflix Hystrix ทำให้เราสามารถสร้าง circuit breaker และ fallback ได้ง่าย และด้วยการทำ circuit breaker และ fallback ช่วย ensures ว่า client จะไม่ทำการ call failing service ซ้ำอีก จนกว่า failing service จะกลับมาปกติ และเมื่อมีการ call failing service ตัว fallbacks จะทำการเรียก execute alternative ที่เราได้จัดเตรียมไว้
ใน session คุณ Might ยังได้ทำการ demo พร้อมกับทำการ show Hystrix Dashboard ซึ่งเป็นความสามารถในการ monitor status ต่างๆผ่านทาง dashboard โดยเราสามารถ monitor dashboard โดยการ browser ไปที่ http://localhost:8080/hystrix นอกจากนั้นยังมี Turbine ซึ่งเป็นความสามารถในการรวบรวมหลายๆ hystrix.stream มาเป็น Hystrix Dashboard ตัวเดียว
สำหรับ session สุดท้ายของงานเป็นเรื่องของการ control configuration ของ microservice ด้วย Spring Cloud configuration server และด้วยความสามารถของ configuration server ทำให้เราสามารถ manage configuration ที่มีความหลากหลายของ environments ไม่ว่าจะเป็น dev, test และ production
คุณ NokNoi ได้แนะนำความสามารถของ Spring Cloud Config ที่สามารถสร้าง configuration server ได้อย่างงายดายด้วยการใช้ annotation ไม่กี่ตัวและ comfiguration เพียงเล็กน้อย โดยสามารถใช้งานร่วมกับ configuration repository เช่น Git ไม่ว่าจะเป็น GitHub, Gitlab และ Bitbucket เป็นต้นในการเก็บ file configuration นอกจากนั้นด้วยความสามารถของ Spring Cloud Config เรายังสามารถทำการ encrypting ค่า properties ที่มีความ sensitive เช่น username และ password ได้ด้วย และด้วยการใช้งาน
ด้วยการใช้งาน external configuration ของ Spring Boot เวลามีการแก้ไขค่า properties โดยปกติแล้ว เราต้องทำการ restart application ของเราเสมอ แต่ด้วยความสามารถของ Spring Cloud Config เราไม่ต้องทำการ restart application อีกต่อไป เพียงเราทำการเพิ่ม @RefreshScope annotation ซึ่งจะทำการ reload ค่า properties ให้เรา ซึ่งการที่จะทำให้ application ทำการ reload ค่า properties เราต้องทำการเรียก http://<yourserver>:8080/refresh ก่อนเสมอ
ด้วยความเจ๋งของ RefreshScope ที่สามารถเปลี่ยนค่า properties แบบ dynamic แล้ว สิ่งหนึ่งที่เราต้องพิจารณาก็คือ ใน microservice ของเราอาจจะมี multiple instances ของ services เดียวกัน เช่น เราอาจจะมี instances ของ API Gateway เป็น 2 instances, 5 instances หรือไปจนถึงหลักสิบหลักร้อย สำหรับ microservice ขนาดใหญ่ ซึ่งก็น่าจะเหนื่อยเหมือนกันนะ ถ้าเราต้องมาทำการเรียก refresh endpoint ให้ครบทุกๆ instances ซึ่งคุณ Ko และคุณ Atom ก็ได้ทำการ introduce Spring Cloud Bus ซึ่งจะเข้ามาแก้ไขปัญหาในจุดนี้
จากปัญหาของการต้อง refresh ทุกๆ services เพื่อที่จะให้ services ทำการ reload ค่า properties คุณ Ko และคุณ Atom ก็ได้นำเสนอวิธีการใช้ Spring Cloud Bus ซึ่งเปิดให้เราใช้ configuration server ในการ publish ไปทุกๆ clients ที่ใช้ configuration server ในการ refresh properties และด้วยความสามารถนี้ เราต้องอาศัย message broker เช่น RabbitMQ หรือ Kafka ในการ push refresh event ซึ่งด้วยวิธี push mechanism ต้องระวังนิดนึงนะครับ เพราะไม่ใช่ทุก Spring Cloud configuration จะ support รูปแบบการ push refresh ซึ่งนั้นก็คือ Consul server
สำหรับงาน meetup ในครั้งนี้ หลายๆ sessions มีความน่าสนใจและได้เสนอแนวคิดต่างๆในการนำมาปรับใช้กับ microservice architectures บน Cloud ให้ทำงานได้ดียิ่งขึ้น จนนำไปสู่คำว่า Cloud Native แต่ส่วนตัวของผมเอง แอบเสียดายอยู่เหมือนกัน ที่ไม่มี session ที่แนะนำถึงภาพรวมของ spring cloud หรือของ microservice ก่อน ค่อยแบ่ง session แบบ specifig อีกทีนึง ซึ่งจะทำให้ผู้ที่ยังไม่คุ้นเคยกับ spring cloud ได้ค่อยๆเห็นภาพก่อน ค่อยลงลึกในรายละเอียดของแต่ละกลุ่ม tools และสุดท้ายนี้ สำหรับผู้ที่สนใจข่าวสารความเคลื่อนไหวของกลุ่มและข่าว Meetup ครั้งต่อๆไป สามารถติดตามได้ที่ Cloud Native Java — Bangkok
Creating opportunities for all through world-class digital…
45 
45 claps
45 
Written by
Software Engineer at LINE Thailand | Learning is a journey, Let's learn together.
Creating opportunities for all through world-class digital platforms & services
Written by
Software Engineer at LINE Thailand | Learning is a journey, Let's learn together.
Creating opportunities for all through world-class digital platforms & services
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ibm-garage/avoiding-incomplete-cloud-native-adoption-17e62b2ae13?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
Kyle Brown, IBM Fellow, IBM Garage
One of the most depressing parts of my job is doing postmortems on failed cloud adoption projects. This happens fairly regularly; we get called in to a client to help them understand why the steps they’ve taken toward cloud adoption have not lived up to their expectations. There is often recrimination and finger-pointing all around, and the most important thing I can do in a situation like that is remain calm and help the client arrive at a conclusion based on facts and not supposition. One of the most common problems that I often see resulting in these failures is that the team only went part of the way toward a full cloud-native transformation, and stopped before they got there.
It’s not much of a stretch to say that cloud native isn’t as much of a description of an architecture (although there are definitely cloud-native and non-cloud-native architectures) as it is a holistic description of not only a set of architectural decisions, but also decisions around process and organization that are required to support that architecture. The technical decisions are necessary but not sufficient for a team to be successful at building and especially managing and maintaining cloud native systems. You need all three sets of the following decisions to be coordinated in order to succeed:
Technology — These are (relatively) the easier decisions. They include things like applying a microservices approach, writing components so that they can take advantage of horizontal scaling, and taking advantage of open source technologies so that you can leverage the efforts of the community. But each of these decisions come with corresponding process and organizational decisions that are needed to support the technology decisions.
Team Organization — Microservices implies that you are building your services in small, autonomous teams. This is simply the application of Conway’s Law — if you want your system to be composed of small, decoupled components, then you have to allow your teams to also be small and not tightly coupled to other teams — the loose connection to other teams should echo the architectural approach of only allowing formal inter-process communication through APIs.
Processes — Microservices implies that you are applying DevOps principles to your development processes such as Continuous Integration and Continuous Delivery. But this itself requires you to adopt other specific technical processes such as Automated Testing, and strongly leads you toward Trunk-based development. In fact, it will be much easier for you to adopt these practices if you first adopt development practices like Test Driven Development and Pair Programming.
The problem comes in when a company tries to skip one of these three legs — if you knock out one leg of a stool the entire thing falls down. For instance, a common problem we see that is indicative of a larger problem is when companies tell us they are building multiple “microservices” while at the same time they are applying the SAFe method to manage a very complex interaction of multiple release trains in the resulting system. You can’t require both; this is a self-defeating proposition.
If your design requires multiple complex release trains, then you probably aren’t following one of the key principles of microservices, which is that the DevOps pipelines for each micro service should remain independent of each other (or at least as independent as possible). What’s more, this kind of coordination also strongly implies that your system is more tightly coupled than it should be because you need that level of coordination. What’s more, requiring this level of coordination means that your teams are not truly autonomous — you’re probably just breaking a big team into smaller arbitrary chunks and not giving them true autonomy.
Another root of this kind of thinking can often be traced all the way back to the interaction between IT and the Business. We see this in the following diagram:
The only reason for a team to adopt a cloud-native development approach (such as one incorporating microservices) is so that the team can deliver measurable units of business value, incrementally. But if your business cannot work that way — it cannot think in terms of small units of business value that can be independently delivered over a period of time, then the microservices architecture, and indeed, most of the rest of the cloud native approach, will not be of much value.
So whenever I see an incomplete transformation, my first thought is to sit down with both the business and IT leaders and ask them what their expectations of the cloud adoption were. Often, you’ll see that the expectations differed dramatically. For instance, in an “IT-led” transformation effort, you’ll sometimes see the business left out of the transformation — when in fact, they should not only have a seat at the table, but they should be leading the effort. One symptom of this is when I ask the Agile teams who the Product Owner reports to — if it is someone in IT, then we may have a problem in that the business is really not involved as deeply as they should be. But that’s not the only symptom of an incomplete transformation. A couple of others include:
There are many other examples of ways in which teams can fail with an incomplete transformation; way too many to list here. With apologies to Tolstoy, “all successful transformations are alike; all unsuccessful transformations fail in their own way.” Sticking to the basics of making sure that all three pieces are enabled, and that IT is aligned with the business is all you need to do, but it’s not easy to ensure that this happens. You have to keep revisiting these basics at every stage in order to push the transformation to completion.
So what positive actions can you take to make sure you follow through on your transformation and not stop in the middle? What does going back to these basics practically mean?
Learn more about how IBM can help you innovation and transform with cloud and AI at ibm.com/garage.
Where innovation and transformation come together
80 
80 claps
80 
Where innovation and transformation come together
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Where innovation and transformation come together
"
https://medium.com/@mesirii/cloud-native-with-micronaut-733c9784850f?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Hunger
Nov 8, 2018·14 min read
This article was originally published in German in JavaSpektrum 05/18 (Early fall). Now that Micronaut 1.0 went GA and the grace-period after the print publication is over, I wanted to make my observations available to a wider audience.Please excuse if any oddly formulated sentences made it through the translation :)
Due to Micronaut’s many cool features, one article simply was not enough to adequately cover the framework. So far we looked at creating Micronaut apps, http server and client, reactive type support, jobs, database integration
Here we want to continue with topics like: cloud deployment, monitoring and orchestration, support for serverless and cloud functions, creating command line apps and the new Kafka integration.
Since the writing the first article, there have been a few new Micronaut releases. When this article is published, we will hold the 1.0 release in our hands. The changes per release are listed in the documentation, as well as the “breaking-changes” between the milestones.
For new applications and the migration of existing applications to a set of independent services, support for the development process is important, but also for deployment and operations, especially with a focus on cloud infrastructure.
Due to the many different providers, libraries and components in the cloud environment for attention, you can quickly lose track. I really would love a to have a quick glossary of terms across all the platforms :)
In principle, all “cloud-native” applications have to handle most of the following requirements (i.e. Adam Wiggins’ 12-Factor App) :
Micronaut already supports most of these requirements from the very beginning. For this purpose, corresponding libraries are integrated via “features”. For special application types (service or function) and their aggregation as a federation, there are profiles that contain the corresponding code, configuration, and dependency templates. Large parts of the detailed Micronaut documentation explain the necessary steps, features and configurations in detail.
The necessary cloud services (e.g. Consul or Eureka) can be started locally for development or testing via Docker or Kubernetes. In part, they are also available for testing as embedded libraries.
Because you can not hardcode dependencies between services in a dynamic environment, a discovery infrastructure is used to resolve names to addresses and configurations. Micronaut contains support for Consul, Eureka and Kubernetes. For certain environments, name resolution can also be configured to a fixed list of named service urls.
After enabling and configuring the naming service as a feature, your Micronaut service and application instances automatically log in and out of the directory service on startup and shutdown. Clients are supplied with the addresses of required services via name resolution (name provided in @Client annotation).
Here’s an example for Consul
First you should start Consul e.g. with Docker. You can then find the UI on http://localhost:8500/ui. There you can see a list of the registered services.
You can configure Consul in src/main/resources/application.yml
Then other services can find our service only by its name, here in a generated HTTP-Client.
When services have been scaled to more than one instance, the Micronaut client implementation uses a client-side round-robin distribution. Services can also forward requests to other instances if they are overloaded.
However, specific load balancers can also be integrated, such as Netflix “Ribbon”. It is configured in application.yml:
Of course, IP-based load balancers like HA-Proxy or Elastic Load Balancer (ELB) are also supported.
In large distributed systems, failures occur continuously. Therefore, already during development we should add protections using resilience patterns for our systems against failures in depended-upon systems.
In Micronaut, this is done with corresponding annotations (e.g. @Retryable and @CircuitBreaker) on client interfaces, which are automatically implemented via AOP advices. This can be done on a per-method basis or for the whole API (interface or package). All pattern-annotations come with meaningful defaults, but can be configured as desired.
Here is an example of retryable calls for all methods of this client:
With CircuitBreaker calling of the remote service is paused for a certain period of time (reset) after a repeated error (attempts), and reset after a ""cooling down time"". This allows to handle both short-term failures and overload situations.
Useful for resilience is also @Fallback which can be used to annotate the classes that provide a ""safe"" minimum implementation in case of failure.
It is important that all resilience integrations report their status and history to a monitoring component so that issues can be identified and alarms or remedial actions triggered.
Micronaut also integrates Netflix’s Hystrix library, which provides dedicated implementations of resilience patterns. By including the io.micronaut.configuration:netflix-hystrixdependency and annotating relevant methods with@HystrixCommand they are automatically wrapped and executed with commands. A Hystrix dashboard is then optionally available.
For monitoring services and applications, Micronaut provides several types of endpoints. Each endpoint can be individually configured and activated:
All management endpoints automatically integrate with the security features of Micronaut. If information should also be shown to non-registered users, you need to add details-visible: ANONYMOUS to the configuration. For special requirements, you can also implement your own management endpoints using @Endpoint annotated classes.
Since Milestone 4 Micronaut integrated monitoring with Micrometer via the micrometer features. Once this feature is active, the Meters registered in the MeterRegistry are available from the /metrics endpoint.
Micronaut provides various modifiers, filters, and binders (sources such as JVM, system, web requests, logging) for micrometer. Of course, your own metrics can be integrated as well. There are custom configurations for feeding the supported metric services (Graphite, Prometheus, Statsd, Atlas).
Especially in distributed architectures, it is important to track requests across service boundaries. For this purpose, the OpenTracing API can be used by integrating “Zipkin” (from Twitter) or “Jaeger” (from Uber).
After activation of the tracing feature, named request and other runtime information (“spans”) are generated, but only small fractions (eg 0.1%) are transmitted to the respective service. These tools can then generate a runtime graph and visualize aggregated latency, dependency and error reports.
Micronaut uses various mechanisms (instrumentation, HTTP headers) to ensure that the relevant information is propagated correctly across thread and service boundaries.
The name information and payload information for the tracing APIs are derived from annotations on service methods. Using @NewSpan(""name""), a new trace is started, which then continues on methods with @ContinueSpan. Method parameters annotated with @SpanTag(""tag.name"") are added to the trace.
The respective clients can of course still be configured individually, there is also the possibility to integrate your own tracers.
Because microservice systems consist of several, manageable services that communicate with each other, it makes sense to manage them in separate modules. However, many of the infrastructure services (orchestration, monitoring, resilience, event logging) are necessary in each of the subprojects. Other features such as database connectivity, or machine-learning libraries may differ per project.
The “Federation” profile can be used to generate an overall project that also generates and configures the subprojects, but also provides a build configuration for the entire project.
With Micronaut’s “function” or “function-aws” profiles, it is easy to develop and deploy individual functions for “serverless” infrastructure. With mn create-function you create these instead of an application with services.
Groovy simply uses top-level functions and Java / Kotlin use beans with annotated methods, that implement the functional interfaces from java.util.function.*.
Like services, functions register with the discovery service that may have been configured.
Functions are consumed via a special client, similar to the HttpClient, annotated only with @FunctionClient(""name"") . Each method of the client interface represents one function that can of course also use reactive types as results. The auto-generated implementation of the client then takes care of the lookup of the function and the subsequent execution.
To test functions, you can call them directly in the test, or even run them locally using the function-web feature in the HTTP server. Then the functions are available either as a GET or POST operations, depending on whether they accept parameters or not.
Functions can also be run as CLI applications, an approach which some of the FaaS project like fn-project use. The executed fat-JAR accepts parameters via std-in and returns results via std-out.
AWS Lambda functions can be deployed directly to AWS using the “function-aws” profile with additionally activated Gradle plug-ins. And then be called from gradle, provided that AWS credentials are available.
These functions have to be made known to the FunctionClient in the application.yml.
Docker also supports “OpenFaaS” deployment. You have to use the openfaas feature. Here the mentioned cli-execution of functions is used as well.
By default, Micronaut generates a Dockerfile for each project, which can be used directly in the build process and is also suitable for ""immutable deployments"". It is based on the Alpine images and includes the fat-JAR from the build process, which is then started via java -jar
Dockerfile
Building and Running the Fat-JAR and Docker
Micronaut can be deployed to the Google Cloud using a Fat-JAR that includes the application with the necessary server and libraries using the gcloud command-line tools.
In an intro guide from OCI the individual steps are explained.
In principle you load the JAR into a bucket and then write a start-script for the instance, which loads the jar, installs Java and starts our service with java -jar. That script is then used by gcloud compute instances create. Then you only need to create a firewall rule for port 8080. After a few minutes the service is started and available.
As alredy mentioned, by using a Gradle plugin, Lambda functions can be deployed and called directly from the build process, as long as you have valid AWS credentials in .aws/credentials .
Using this task to deploy & invoke our function.
In microservices architectures, event-based integration layers are used more and more. Although Micronaut already offered a reactive HTTP server which also provides flow control, other aspects of distributed, persistent event logs are quite beneficial. Therefore, Micronaut Milestone 4 added support for Apache Kafka.
There is also a new profile for pure Kafka services, without an HTTP server. Generally, Services and functions can be equipped with Kafka and Kafka streams using feature flags. If there is a micrometer registry enabled, Kafka metrics are available there too, and the /health endpoint provides information about the state of the Kafka connections.
To generate a pure Kafka service without HTTP server, use
This service communicates as configured with Kafka via localhost:9092 . One or more Kafka servers can be exposed to the application using KAFKA_BOOTSTRAP_SERVERS, but also via KAFKA_BOOTSTRAP_SERVERS .
Configuration in application.yml
For testing you can either use EmbeddedKafka (using kafka.embedded.enabled) or start Kafka using Docker.
Micronaut services and functions can be declaratively marked via annotations as consumers and publishers of events on topics.
Somewhat confusingly named, Beans annotated with @KafkaClient are a source of events.
As usual, the implementation of the interface is handled by Micronaut. In addition to the payload, other annotated parameters can be passed, such as partition or header. Again, reactive types like Flowable or Single are supported for payload and results, so you can subscribe to the results of the publication. You can also return the Kafka RecordMetadata, which will contain all details of the send process.
Batching is activated with @KafkaClient(batch=true), then lists of multiple entities are treated as a batch and not serialized as a single, large payload.
Our producer is used as follows:
Production deployments of Kafka support a variety of configuration options which can be passed to the @KafkaClientannotation - serialization, retries, acknowledgment, etc. By default, Jackson serializers are used for JSON, but serializers are configurable either globally or per producer/consumer. For very special applications you can inject the underlying KafkaProducer instance of the Kafka API and then have the full flexibility in what you want to do.
You use beans annotated with @KafkaListener to receive updates from one or more topics.
Again, a lot of additional method parameters can be specified, such as offset, partition, timestamp, topic, header, or just a Kafka ConsumerRecord . For batch processing, @KafkaListener(batch=true) can also be used and then either lists or reactive streams of messages are processed in batches.
Conveniently, the return value of the receiver method can be forwarded to another topic using @SendTo(""topic"",…​) annotation.
There are other configurations for thread management, timeouts, serialization for individual consumers, or groups, which are discussed in detail in the documentation. Offset Commit Management is a separate topic in itself that is covered there, including error handling, asynchronous processing, confirmation management, offset recovery and re-delivery.
Streaming Data (Fast Data) architectures (Akka, Kafka, Flink, Spark) are becoming more and more common. Our own code runs as processors on the stream, which can aggregate, filter or create new streams. Micronaut’s lean runtime should cause little overhead for such processing, so support for Kafka stream processors is also available.
For Kafka streams usage, the libraries and the Kafka configuration require an @Factory whose processing method takes a ConfiguredStreamBuilder and returns a typed KStream of the Kafka-Streams API.
Here is a minimal example, without the serialization configuration code.
The topics of these streams can then be regularly supplied with data by upstream producers and their results processed by downstream consumers.
The mn tool was rewritten using picocli. As a nice side-effect, it now offers picocli support for developers too. You can create a command-line application using create-cli-app and then add additional commands with create-command. More information about the APIs is available at the PicoCLI site.
The generated command could then be adapted like this:
As you can see it supports full injection and the other features of Micronaut.
In addition to gradlew run, you can also use the gradlew assemble command to package your line application as a zip distribution, which then contains all dependencies and shell scripts for OSX, Unix and Windows.
Then we can run our cli with bin/list -c .
It would be nice for these CLis to support a ahead-of-time (aot) compiled GraalVM variant, or a shell executable jar like in Spring-Boot.
Micronaut is not a classic web framework for rendering HTML and other content. Recently, however, support for those was added via the io.micronaut:micronaut-views module, plus the respective libraries of a template engine, such as Thymeleaf, Velocity, or handlebars. The template files are located in src/main/resource/views and controller methods annotated with @View(""name"") can return Maps, POJOs or ModelAndView instances to provide the render information.
The @Requires annotation for dynamically activating beans depending on external conditions is extremely flexible, here are a few examples
With Micronaut you are well equipped to develop, integrate, deploy, run and monitor complex service-based systems. Thanks to the recency of the framework, modern tools for these tasks are already integrated. There is still a lot to do for supporting different cloud-providers. For instance for cloud functions, currently only AWS is automatically supported. Kafka integration gives you the choice to use HTTP or event-based protocols for inter-service communication.
Micronaut can not only be used for classic backend services. OCI developer Ryan Vanderwerf shows in the GalecinoCar project how Micronaut, together with ML-Frameworks and Robo4j, controls a self-propelled model car on a Raspberry PI.
I’m looking forward to the further development of the framework. So far, the features are relly well thought-through. Help and activity in the community and the quick bug fixes are very impressive.
I really miss is the ability to enable ""features"" in existing projects using mn --feature, to add new dependencies and configurations correctly and consistently.
(From print article)
A software developer passionate about teaching and learning. Currently working with Neo4j, GraphQL, Kotlin, ML/AI, Micronaut, Spring, Kafka, and more.
See all (98)
178 
1
178 claps
178 
1
A software developer passionate about teaching and learning. Currently working with Neo4j, GraphQL, Kotlin, ML/AI, Micronaut, Spring, Kafka, and more.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nodejs/q-a-with-new-node-js-foundation-member-bitnami-about-kubernetes-cloud-native-and-more-bcc11bac35a4?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Node.js
Oct 11, 2017·4 min read
Bitnami is one of the newest Silver Members to the Node.js Foundation — you can read about all of the new members here. The company packages applications for any platform, making it possible for organizations to quickly deploy software on the patform of their choice, from native installers to cloud images to containers.
We recently sat down with them for a Q&A on how their users are working with Node.js, growth that they’ve seen in the cloud-native space in the last few years, and why they joined the Founation. Read the Q&A below.
What is Bitnami and how are Node.js developers using this technology? Bitnami is best known for application packaging. Through the Bitnami application catalog, we deliver and maintain 140+ ready-to-run server applications and development environments (mostly open source) to simplify software installation with an emphasis on accelerating cloud adoption for the world’s leading cloud providers.   Node.js is one of the more popular developer tools that we package. With the Bitnami Node.js stack, compiling, configuring and all of its dependencies are taken care of, so it works out-of-the-box, providing a one-click install solution.
Visitors to Bitnami.com can download local installers or vms or launch their own Node.js server directly into any of our cloud provider partners’ clouds using virtual machines or container images. We populate cloud marketplaces for AWS, Amazon Lightsail, Microsoft Azure, Google, Oracle and others with a ready-to-run Node.js stack that includes the latest version of Node.js, Apache, Python and Redis. Additionally, we package Node.js as a component of the Bitnami MEAN stack to provide a complete development environment.
What cloud platforms do you work with; what are the most popular platforms for you and why? Bitnami customers include Amazon, Google, Microsoft, Oracle, VMware, 1&1, Centurylink, Deutsch Telekom’s Open Telekom Cloud, Huawei and more.
How has Bitnami grown in the last few years? Bitnami has seen tremendous growth as we’ve progressed from packaging local installers and VMs to multi-tier VMs and embraced cloud platforms, container images and most recently Kubernetes templates. Most of the major cloud vendors feature Bitnami packages in their cloud marketplaces and we’re currently seeing over 1 million deployments of Bitnami packaged applications every month.  What trends are you seeing in the space?  As more enterprises focus on software development and cloud migration as part of a larger digitization strategy, we’re seeing increased interest in Docker images and Kubernetes templates, especially for our packaged development tools and runtimes. In a recent user survey, we saw the number of Bitnami users adopting containers grow by more than 2x over the previous year. The survey also showed that container usage is growing rapidly with production usage growing more than 150% year over year. 65% of respondents reported running containers in production, with the fastest growing segment of that group now running 50 or more container host machines.
Why was it important for you to join the Node.js Foundation? As users of Node.js, and a company that packages and distributes Node.js, we felt it was important to support the Foundation. As more developers embrace new development tools like Node.js, we want to be a primary enabler of that transition, providing users with pre-configured Node.js environments that can simplify their first experience using it on a local machine or providing secure, optimized packages or stacks for multi-cloud usage.
As we continue to build our portfolio of open source developer tools for containers, Kubernetes and whatever is next, we’d like to engage the community in a discussion around those platforms, their needs and how Node.js can be best used to create cloud-native infrastructure and core developer tools.
Are you working on any big projects that you are excited about, and want to share with the Node.js community?
We’ve got a lot of exciting projects and partnerships underway but two that I am excited about and I think the Node.js community will relate to are 1.) All of the Kubernetes work that we’ve been doing and 2.) our soon to be released automated application packaging and deployment platform for enterprise developers.
On the Kubernetes front we have been very busy with Bitnami-led open source projects such as the Kubeless native serverless framework, the Kubernetes application deployment management tool Kubecfg, our Kubeapps.com Kubernetes-ready application catalog, Cabin for mobile kubernetes management and core contributions to Kubernetes projects such as Helm, Monocular and Ksonnet, Kompose and more.
We even have a chart for a Node deployment on a Kubernetes cluster using the Helm package manager. We made an early commitment to Kubernetes as the platform of the future and we continue to make investments in helping our customers by delivering kubernetes-ready tools and templates.
For our upcoming enterprise offering we’re productizing much of the tooling we’ve built over the years to manage the Bitnami cloud catalog at scale. The automation platform that powers our application catalog offering today, managing thousands of package builds and updates, will soon be available to enterprise developers. So businesses that are looking to accelerate development and deployment of internal software projects or to migrate their existing applications into the cloud will have a powerful new tool to enable that transition.
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
95 
95 
95 
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
"
https://medium.com/planet-stories/cng-part-4-open-aerial-maps-cloud-native-geospatial-architecture-a7f784cf7c2f?source=search_post---------45,"There are currently no responses for this story.
Be the first to respond.
In the previous post, we looked at Planet’s Cloud Native Geospatial architecture. This post aims to help elucidate the essence of a Cloud Native Geospatial architecture by taking a close look at one of the cleanest and most advanced cloud native geospatial architectures — OpenAerialMap (OAM).
The project was started by the Humanitarian OpenStreetMap Team (HOT) to help get imagery online in the wake of disaster, so it could be quickly traced into OpenStreetMap (OSM) by non-imagery experts. Any openly available imagery data can be uploaded to the OAM platform, where it is hosted for free. Since all data on the platform must be openly available, the architecture is a lot more straightforward than Planet’s. And the team behind it has done a really innovative job of rethinking how things should work when the entire workflow of sharing imagery is done on the cloud. In particular, they’ve nicely componentized the system so that each part can be used separately, and those parts can easily be used with other cloud native geospatial tools.
The team has published a lot of great documentation on that system, both the Catalog Components to browse the data as well as the Contributor Components that let anyone upload new imagery. So this post will not recount all the details of their great architecture, but will draw some attention to some of the really nice pieces.
Cloud Optimized GeoTIFF at the center
The core of OpenAerialMap is the Humanitarian OpenStreetMap Node of the Open Imagery Network. This is in many ways a very simple component — an AWS S3 bucket of Cloud Optimized GeoTIFF’s (COG’s), with JSON metadata that follows the OIN Metadata Spec. (as of this writing, the files aren’t fully compliant COG’s, but they are 99% implementing and plan to reformat to be 100% compliant). But since it follows the Cloud Native Geospatial principles it is incredibly powerful. All data inserted into the bucket by the OAM Uploader is processed on upload, so that every piece of imagery on OpenAerialMap is a Cloud Optimized GeoTIFF. This enables every dataset to automatically show up in the OAM Catalog API as well as the OAM Dynamic Tiler. All data in these API’s means that the front end on OpenAerialMap.org instantly gets the data.
One does not need to figure out the complexities of hosting a full web tile server, instead an organization can just upload imagery to a cloud storage location. They can even leverage OAM’s uploading tools to help format it properly.
Web Tiles of all imagery
Perhaps the coolest piece of technology in the mix is the Dynamic Tiler. Stamen put up a great post on the core work that started it, experimenting with using AWS Lambda to generate tiles.
The really cool thing about the OAM Dynamic Tiler is that it can read any Cloud Optimized GeoTIFF (COG). This was designed so that different Open Imagery Network buckets could be displayed on Open Aerial Map easily. But it means that even non-open imagery can be rendered with the exact same tooling. Thus, any COG that is online can be easily viewed as a web map, just by virtue of being stored in a cloud native manner. The dynamic tiler technology has evolved to be an independent project called Marblecutter that is also used by Mapzen, who helped fund its evolution to also deal with terrain data. So anyone can use the same on-the-fly tiling, going from COG’s to web tiles in milliseconds.
Crawlable Metadata for all imagery
The other key of a Cloud Native Geospatial architecture is to make metadata for all imagery accessible.
OpenAerialMap actually shows an emerging best practice, which is to have a static catalog that is simply indexable files on cloud storage, as well as a Catalog API, which enables search of the data through web front-ends, command line tools and integrated applications. The full details and implications of static catalogs and API’s will be in a future post going deep into the results of the Boulder Catalog Sprint, but the Open Imagery Network that powers OAM is really the pioneer of this approach. It focuses on exposing the data in an incredibly reliable way, since there are no servers to maintain. And then the Catalog API on top of it can also be queried and even crawled as well.
A decoupled approach
The best part of OpenAerialMap is that each piece is decoupled from the others. The uploader, static catalog (OIN), catalog api, dynamic tiler and front-end are all independent components that can easily point at other instances or interfaces providing the same functionality. Organizations can also adopt parts of the OAM architecture and software components that might work for them — one might reuse the OAM uploader but choose to use rio-tiler to actually serve up the COG data as tiles, for example. And every component is completely open source.
Even better, the Open Imagery Network is a network, so organizations don’t need to upload their data to OAM — they can expose their own data in the same COG + static catalog manner. All the plumbing is in place for Planet, DigitalGlobe and others to be able to expose their disaster data from their Cloud Native Geospatial architectures as COGs and have it stream straight into OpenAerialMap, as they are all embracing the same architectural principles. Hopefully, in the next few months we’ll be able to show that working end-to-end, continuing the momentum of collaborating in Boulder.
The Cloud Native Geospatial Architecture
While there are a handful of platforms that enable users to upload imagery and view it as webtiles, they are usually walled gardens that are not fully open. The Open Aerial Map and Open Imagery Network architecture show how cloud geospatial can be done in a fully open manner, enabling any platform to tap into the data that is available. And since it doesn’t have to deal with authorization and authentication like commercial data providers, it is easier to understand as there isn’t complexity around what people can and cannot see. The well componentized design arguably makes it the reference implementation for a pure Cloud Native Geospatial architecture.
In the next post, we’ll extract out the core principles of a Cloud Native Geospatial architecture, but most of it should be fairly obvious by now.
Using space imagery to tell stories about our changing…
97 
97 claps
97 
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/hackernoon/three-patterns-for-an-effective-cloud-native-development-workflow-6f59525f5bf1?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
Many developers are moving towards “cloud native” development, whether that is taking advantage of the services and convenience of public cloud, or deploying services on their own internal cloud. However, the new architectures and technologies that are emerging as part of the cloud native development space — microservices, containers, orchestrators — require new developer workflow patterns.
In this article I will introduce three patterns that I have found useful as I’ve learned about working with cloud technologies over the past several years.
The ability to define cloud infrastructure as code and provision this on-demand has been revolutionary in the way we deploy software. However, although the initialisation of the infrastructure is fast, it is typically not instantaneous (as you might want, say, in a TDD cycle). This means that developers who require provisioning of infrastructure in order to complete a build and deploy cycle often do not get the fast feedback they require. This can lead to task/context switching becoming a problem. The solutions to this include simulated local development infrastructure, re-usable remote infrastructure, and local-to-production development.
The simulated local development infrastructure pattern can be seen with AWS SAM Local. This tool provides a CLI and Docker-based replication of a production serverless environment in order to enable the efficient local development of AWS Lambda functions that use associated AWS services such as Amazon API Gateway and DynamoDB. This tool can be further augmented with service virtualisation (covered below) to simulate services, and in-memory data stores and middleware, for example LocalStack, which can be used to provide simulations of AWS services like Kinesis and SQS.
The re-usable remote infrastructure pattern is often implemented in a bespoke fashion, with platform teams provisioning multiple test environments that can be leased on-demand by engineers. Typically the configuration and corresponding state (data stores) are reset when the lease is completed, which make this ready for use by the next developer. The open source Kubernaut tool also provides the same experience for Kubernetes, and maintains a collection of initialised clusters that can be leased on-demand.
The local-to-production development pattern is arguably the most cloud native of the patterns, as this involves a developer coding an application against production. Development and test environments must be as high-fidelity as possible in order to get the most accurate feedback, and obviously the most production-like environment there is is production itself. Azure provides dev spaces, which allows an engineer to spin up a managed Kubernetes cluster on-demand and connect a local VSCode editor into this. The tool manages the build and deploy of any code changes into a container, which is then deployed into the dev space in near real time.
The CNCF-hosted Telepresence tool allows developers to proxy their local development environment into a Kubernetes cluster, which allows an engineer to run and debug any code and application locally, as if it was in the cluster. This allows a real-time developer feedback loop, as requests can be made against the production application and the service debugged locally using actual traffic (or shadowed traffic) that is forwarded to the local development environment.
Cloud native systems are typically developed as modular (service-based) systems, which means testing a single service can be challenging due to the required interaction with external service dependencies. Obviously services should be designed to be as cohesive and loosely coupled as possible, which means that the can be developed in isolation. However, when this is not practical, or an engineer wants to drive a more production-like test, techniques like service virtualisation and consumer-driven contracts can be useful patterns.
Modern service virtualisation tools like Hoverfly, WireMock and Mountebank act as proxies that sit between services and systems and capture traffic for later replay. This allows for the execution of tests that span multiple services, and the recording of the associated requests and responses from the dependent services involved. The recording can then be replayed without running the actual dependencies themselves, which is very valuable for running isolated tests in a CI/CD build pipeline. Both of these tools can also be used to generate virtual responses from services that do not yet exist, and Hoverfly allows the injection of faults, which can be used to test the handling of failures in a deterministic fashion.
Consumer-driven contracts (CDC) can also be used to not only drive the design of services outside-in (i.e. TDD for APIs), but also for verifying that a service provides the required functionality and doesn’t regress as the service evolves. There is an excellent article about this on the Martin Fowler Blog, and although the process can appear daunting at first glance, in my experience it does become quite mechanical once a team have experimented with the approach over a few iterations.
Cloud native systems are complex and constantly evolving, and so testing in pre-production typically cannot provide complete validation of functionality and interactions with what is currently running in production. The solution to this problem is to reduce the impact of deployment by canarying — initially routing only a small fraction of production traffic against the new service (the “canary in the coal mine”) and observing behaviour and other KPIs, and then incrementally increasing the percentage of traffic this until the new service is taking all of the traffic.
For developers working with Kubernetes, the open source Ambassador API gateway, which is built on the Envoy Proxy, provide canary testing functionality, which is driven via simple annotations on Kubernetes services. The Istio service mesh also provides canarying, but this has to be configured outside of Kubernetes. With a little glue code, both of these systems can provide an automated canary release of functionality, and an automated rollback if issues are detected.
For developers working with serverless code, similar functionality is offered by many of the cloud vendors. For example, AWS Lambda provides traffic shifting using function aliases, which can be orchestrated to provide a canary rollout. As with the Kubernetes approach above, a developer can write some glue code to automate gradual releases and rollbacks based on AWS CloudWatch metrics.
#BlackLivesMatter
121 
121 claps
121 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/radiant-earth-insights/a-cloud-native-geospatial-interoperability-sprint-483d9c299595?source=search_post---------48,"There are currently no responses for this story.
Be the first to respond.
Last week, in Boulder, Colorado, 25 people representing 14 different organizations gathered together to work towards common approaches to better enable the sharing and searching of imagery. Many providers of data and software in the earth observation field have been building API’s to expose their data holdings to modern developers. Lacking a clear standard for massive amounts of imagery using modern tech (JSON-based, RESTful), everyone was building their own slightly different systems. A small group of developers from different organizations started gathering examples of their API design (in OpenAPI 2.0 where possible) and JSON metadata responses, in order to see what is different and if a collaborative specification was possible.
Radiant (radiant.earth) is a new organization aiming to bring the power of earth imagery to the NGO space, encouraging open geospatial data for positive global impact and improved decision-making. They saw the potential for increased interoperability to help not just NGO’s but all consumers of geospatial information, and stepped up to sponsor an in-person gathering of interested parties. Meeting in person enabled high-bandwidth exchanges to actually resolve differences and build a core specification, instead of just talking about what could be.
The enthusiastic response from so many different organizations was a testament to the fact that the timing is right for better interoperability with more modern technical approaches. A catalog making it easy to crawl and search cloud optimized geotiffs is one of the most fundamental pieces of a Cloud Native Geospatial architecture. Data providers like Planet and DigitalGlobe showed up with several people, and the two main traditional remote sensing software providers, Harris’s ENVI and Hexagon Geospatial with Erdas, sent senior engineering leaders. Modern scale-out platforms like RasterFoundry/GeoTrellis and Google Earth Engine both sent lead developers, and a number of others sent their key people — see the full (and awesome) organization list. The goal was to prioritize attendance of the developers who will be tasked with implementing the standard, in order to reach something much more practical and useful than what a bunch of architecture astronauts like me would come up.
The agenda was quite ambitious, with a goal of walking out with not only draft specifications but also some first implementations that exercised it. To make this work there was a lot of prep work done by everyone to ensure we could hit the ground running. The prep work, as well as extensive in depth notes from most of the sessions, can be found in Radiant’s boulder sprint repo on GitHub. After the welcome session the group went through a great set of lightning talks to help everyone get on the same page. And then it was small, productive discussions and heads down work on specifications and code.
The gathering was a big success, with substantial progress on each of the workstreams. We divided up in to four groups so that parallel progress could be made, with check-ins every two hours. In the words of one participant, ‘I went back to my team at the office doing two week sprints and told them about the two hour sprints we did in Boulder’.
Follow up posts will go in to more detail about what all was accomplished, and for those who like to dig in to everything there is an extensive overview in GitHub. The group reached consensus on a core ‘static catalog’ that can be implemented just by putting up files on a web server, as well as a draft OpenAPI specification of a catalog API. And though our naming sessions in person fell short, the online chat surprisingly reached consensus, calling the core spec the SpatioTemporal Asset Catalog. The core metadata profile is called SpatioTemporal Asset Metadata, with the goal that it will be reusable in other contexts.
The group decided the core should be applicable more than just imagery, it is designed to handle any geospatial asset that is referenced in space and time. So that includes not just imagery but also potentially SAR, derived rasters like NDVI, DEM’s, LiDAR, Hyperspectral, etc. Community profiles will be needed to make those searches more relevant, but a user should be able to search a geography and a time range and get back all the relevant information, filtering for the data types they care about.
We are not quite ready to encourage everyone to start using and contributing to the specifications yet, as there are a few decisions reached at the end of day 3 that need to finalized and incorporated. But in the spirit of openness and transparency there is a STAC spec repository where work is being done, and if anyone is really excited to help out they can join us on our gitter channel and jump in.
When the repository is all ready to go I’ll write another post here, and go deeper into some of the design decisions behind the specs and next steps. The next few weeks will see everyone working to build real world implementations and refining the spec with input from the practical problems that arise in moving from spec to code. Our hope is to get many more data providers and software tools to implement the specification, to greatly increase the interoperability of this data.
Many of the organizations involved have already pledged their support to implement the specification. Our goal is to convene again in person in three to four months to truly compare all the implementations, and walk out as close as possible to a finalized specification — with reference implementations, testing engines, client libraries and documentation. Radiant has already pledged that they will support an in person gathering once there is sufficient number of implementations to warrant a gathering. And the end goal is to turn it into an international standard. The group was excited by the potential to make STAC compatible with WFS 3.0 from the Open Geospatial Consortium, so the OGC may make a natural home after it is solidified.
Thanks again to Radiant for convening the event and sponsoring the first day, as well as Planet for sponsoring the second day, and DigitalGlobe for the third day as well as a great dinner. And a special thanks to every single organization that sent developers and gave them time to collaborate and work on implementations.
Earth Imagery for Impact
108 
108 claps
108 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@italypaleale/cloud-native-container-ready-php-ca5c811e19ff?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alessandro Segala
Jul 24, 2019·8 min read
PHP is the language that everyone loves to hate. It was the first language many of us used to build web apps (including myself!), but it’s fallen out of favors with developers. There is plenty of reason behind that, and most criticism is, or at least was, justifiable: PHP 5 suffered from multiple design and performance issues, had various inconsistencies, lacked any Unicode support, etc.
"
https://medium.com/@YuriShkuro/observability-challenges-in-microservices-and-cloud-native-applications-72857f9d03af?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yuri Shkuro
May 28, 2019·9 min read
In this article we explain the concept of observability in microservices, and its challenges for traditional monitoring tools.
In the last decade, we saw a significant shift in how modern, internet-scale applications are being built. Cloud computing (infrastructure as a service) and containerization technologies (popularized by Docker) enabled a new breed of distributed system designs commonly referred to as microservices (and their next incarnation, FaaS). Successful companies like Twitter and Netflix have been able to leverage them to build highly scalable, efficient, and reliable systems, and to deliver more features faster to their customers.
This article is an extract taken from Chapter 1 of my book Mastering Distributed Tracing. This book will equip you to operate and enhance your own tracing infrastructure. Through practical exercises and code examples, you will learn how end-to-end tracing can be used as a powerful application performance management and comprehension tool.
While there is no official definition of microservices, a certain consensus has evolved over time in the industry. Martin Fowler, the author of many books on software design, argues that microservices architectures exhibit the following common characteristics:
Because of the large number of microservices involved in building modern applications, rapid provisioning, rapid deployment via decentralized continuous delivery, strict DevOps practices, and holistic service monitoring are necessary to effectively develop, maintain, and operate such applications. The infrastructure requirements imposed by the microservices architectures spawned a whole new area of development of infrastructure platforms and tools for managing these complex cloud-native applications. In 2015, the Cloud Native Computing Foundation (CNCF) was created as a vendor-neutral home for many emerging open source projects in this area, such as Kubernetes, Prometheus, Linkerd, and so on, with a mission to “make cloud-native computing ubiquitous.”
The term “observability” in control theory states that the system is observable if the internal states of the system and, accordingly, its behavior, can be determined by only looking at its inputs and outputs. At the 2018 Observability Practitioners Summit, Bryan Cantrill, the CTO of Joyent and one of the creators of the tool dtrace, argued that this definition is not practical to apply to software systems because they are so complex that we can never know their complete internal state, and therefore the control theory’s binary measure of observability is always zero (I highly recommend watching his talk on YouTube: https://youtu.be/U4E0QxzswQc). Instead, a more useful definition of observability for a software system is its “capability to allow a human to ask and answer questions”. The more questions we can ask and answer about the system, the more observable it is.
There are also many debates and Twitter zingers about the difference between monitoring and observability. Traditionally, the term monitoring was used to describe metrics collection and alerting. Sometimes it is used more generally to include other tools, such as “using distributed tracing to monitor distributed transactions.” The definition by Oxford dictionaries of the verb “monitor” is “to observe and check the progress or quality of (something) over a period of time; keep under systematic review.”
However, it is better scoped to describing the process of observing certain a priori defined performance indicators of our software system, such as those measuring an impact on the end-user experience, like latency or error counts, and using their values to alert us when these signals indicate an abnormal behavior of the system. Metrics, logs, and traces can all be used as a means to extract those signals from the application. We can then reserve the term “observability” for situations when we have a human operator proactively asking questions that were not predefined. As Bryan Cantrill put it in his talk, this process is debugging, and we need to “use our brains when debugging.” Monitoring does not require a human operator; it can and should be fully automated.
If you want to talk about (metrics, logs, and traces) as pillars of observability –great. The human is the foundation of observability! — Bryan Cantrill
In the end, the so-called “three pillars of observability” (metrics, logs, and traces) are just tools, or more precisely, different ways of extracting sensor data from the applications. Even with metrics, the modern time series solutions like Prometheus, InfluxDB, or Uber’s M3 are capable of capturing the time series with many labels, such as which host emitted a particular value of a counter. Not all labels may be useful for monitoring, since a single misbehaving service instance in a cluster of thousands does not warrant an alert that wakes up an engineer. But when we are investigating an outage and trying to narrow down the scope of the problem, the labels can be very useful as observability signals.
By adopting microservices architectures, organizations are expecting to reap many benefits, from better scalability of components to higher developer productivity. There are many books, articles, and blog posts written on this topic, so I will not go into that. Despite the benefits and eager adoption by companies large and small, microservices come with their own challenges and complexity. Companies like Twitter and Netflix were successful in adopting microservices because they found efficient ways of managing that complexity. Vijay Gill, Senior VP of Engineering at Databricks, goes as far as saying that the only good reason to adopt microservices is to be able to scale your engineering organization and to “ship the org chart”.
So, what are the challenges of this design? There are quite a few:
When we see that some requests to our system are failing or slow, we want our observability tools to tell us the story about what happens to that request.
Traditional monitoring tools were designed for monolith systems, observing the health and behavior of a single application instance. They may be able to tell us a story about that single instance, but they know almost nothing about the distributed transaction that passed through it. These tools “lack the context” of the request.
It goes like this: “Once upon a time…something bad happened. The end.” How do you like this story? This is what the chart below tells us. It’s not completely useless; we do see a spike and we could define an alert to fire when this happens. But can we explain or troubleshoot the problem?
Metrics, or stats, are numerical measures recorded by the application, such as counters, gauges, or timers. Metrics are very cheap to collect, since numeric values can be easily aggregated to reduce the overhead of transmitting that data to the monitoring system. They are also fairly accurate, which is why they are very useful for the actual monitoring (as the dictionary defines it) and alerting.
Yet the same capacity for aggregation is what makes metrics ill-suited for explaining the pathological behavior of the application. By aggregating data, we are throwing away all the context we had about the individual transactions.
Logging is an even more basic observability tool than metrics. Every programmer learns their first programming language by writing a program that prints (that is, logs) “Hello, World!” Similar to metrics, logs struggle with microservices because each log stream only tells us about a single instance of a service. However, the evolving programming paradigms creates other problems for logs as a debugging tool. Ben Sigelman, who built Google’s distributed tracing system Dapper, explained it in his KubeCon 2016 keynote talk as four types of concurrency:
Years ago, applications like early versions of Apache HTTP Server handled concurrency by forking child processes and having each process handle a single request at a time. Logs collected from that single process could do a good job of describing what happened inside the application.
Then came multi-threaded applications and basic concurrency. A single request would typically be executed by a single thread sequentially, so as long as we included the thread name in the logs and filtered by that name, we could still get a reasonably accurate picture of the request execution.
Then came asynchronous concurrency, with asynchronous and actor-based programming, executor pools, futures, promises, and event-loop-based frameworks. The execution of a single request may start on one thread, then continue on another, then finish on the third. In the case of event loop systems like Node.js, all requests are processed on a single thread but when the execution tries to make an I/O, it is put in a wait state and when the I/O is done, the execution resumes after waiting its turn in the queue.
Both of these asynchronous concurrency models result in each thread switching between multiple different requests that are all in flight. Observing the behavior of such a system from the logs is very difficult, unless we annotate all logs with some kind of unique id representing the request rather than the thread, a technique that actually gets us close to how distributed tracing works.
Finally, microservices introduced what we can call “distributed concurrency.” Not only can the execution of a single request jump between threads, but it can also jump between processes, when one microservice makes a network call to another. Trying to troubleshoot request execution from such logs is like debugging without a stack trace: we get small pieces, but no big picture.
In order to reconstruct the flight of the request from the many log streams, we need powerful logs aggregation technology and a distributed context propagation capability to tag all those logs in different processes with a unique request id that we can use to stitch those requests together. We might as well be using the real distributed tracing infrastructure at this point! Yet even after tagging the logs with a unique request id, we still cannot assemble them into an accurate sequence, because the timestamps from different servers are generally not comparable due to clock skews.
Distributed tracing is uniquely positioned to answer many of the questions that arise when operating or troubleshooting modern distributed systems:
To learn more about distributed tracing and observability for cloud-native applications, check out my book Mastering Distributed Tracing.
This article originally appeared on Packt Hub and is reposted here with permission from Packt Publishing.
Software engineer. Creator of tracing platform Jaeger. Author of “Mastering Distributed Tracing”. https://shkuro.com/
80 
80 
80 
Software engineer. Creator of tracing platform Jaeger. Author of “Mastering Distributed Tracing”. https://shkuro.com/
"
https://blog.heptio.com/cloud-native-part-6-18e1587355db?source=search_post---------51,"This is the sixth part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
Note: this post doesn’t cover all of the angles around security in the new “Cloud Native” world. Also, while I’m not a security expert, it is something that I’ve paid attention to throughout my career. Consider this a part of a map on things to consider.
Security is still a big question in the Cloud Native world. Old techniques don’t apply cleanly and so, initially, Cloud Native may appear to be a step backward. But his brave new world also introduces opportunities.
There are quite a few tools that help users to audit their container images to ensure that they are fully patched. I don’t have a strong opinion on the various options there.
Once a vulnerable image is found this changes things from being a technical issue to a process/workflow issue.
The real problem: what do you do once you find a vulnerable container image? This is a place where the market hasn’t provided a great set of solutions. Once a vulnerable image is found this changes things from being a technical issue to a process/workflow issue. You will want to identify which groups within your organization are impacted, where in your container image “tree” to fix the problem and how best to test and push out a new patched version.
CI/CD (Continuous Integration/Continuous Deployment) is a critical piece of the puzzle as it will enable automated and quick release processes for the new images. Furthermore, integration with orchestration systems will enable you to identify which users are using which vulnerable images. It will also allow you to verify that a new fixed version is actually being run in production. Finally, policy in your deployment system can help prevent new containers from being launched with a known bad image. (In the Kubernetes world this policy is called admission.)
But even if all of the things you are running on your cluster are patched, it doesn’t ensure that there isn’t untrusted activity on your network.
Traditional network based security tools don’t work well in a dynamically scheduled short lived container world. Short lived containers may not be around long enough to be scanned by traditional scanning tools. And by the time a report is generated, the container in question may be gone.
With dynamic orchestrators, IPs don’t have long term meaning and can be reused automatically. The solution is to integrate network analysis tools with the orchestrator so that logical names (and other metadata) can be used in addition to raw IP addresses. This will likely make alerts more easily actionable.
Many of the networking technologies leverage encapsulation to implement an “IP per container”. This can create issues for network tracing and inspection tools. They will have to be adapted if such networking systems are deployed in production. Luckily, much of this has standardized on VXLAN, VLANs or no encapsulation/virtualization so support can be leveraged across many such systems.
The biggest issues are around microservices.
However, in my opinion, the biggest issues are around microservices. When there are many services running in production, it is necessary to ensure that only authorized clients are calling any particular service. Furthermore, with reuse of IPs, clients need to know that they are speaking with the correct service. As of now, this is largely an unsolved problem. There are two (non-mutually exclusive) ways to approach this problem.
First, the more flexible networking systems and the opportunity to implement host level firewall rules (outside any container) to enable fine grained access policies for which containers can call which other containers. I’ve been calling this approach network micro-segmentation. The challenge here is one of configuring such policy in the face of dynamic scheduling. While early yet, there are multiple companies working to make this easier through support in the network, coordination with the orchestrator and higher level application definitions. One big caveat: micro-segmentation becomes less effective the more widely any specific service is used. If a service has 100s of callers, simple “access implies authorization” models are no longer effective.
The second approach is for applications to play a larger role in implementing authentication and encryption inside the datacenter. This works as services take on many clients and become “soft multi-tenant” inside a large organization. This requires a system of identity for production services. As a side project, I’ve started a project called SPIFFE (Secure Production Identity Framework For Everyone). These ideas are proven inside of companies such as Google but haven’t been widely deployed elsewhere.
Security is a deep topic and I’m sure that there are threats and considerations not listed here. This will have to be an ongoing discussion.
This concludes this series on Cloud Native. Please let us know what you think by either responding here in Medium or reaching out to jbeda, cmcluck or heptio on Twitter.
Heptio
52 
52 claps
52 
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://medium.com/planet-stories/cng-part-5-cloud-native-geospatial-architecture-defined-193d5ffdd681?source=search_post---------52,"There are currently no responses for this story.
Be the first to respond.
Now that we’ve looked at a couple instances of cloud native geospatial instances (Planet and OpenAerialMap) we can pull out a more solid definition of what makes for a true Cloud Native Geospatial architecture. For now this is fully focused on imagery — geospatial vector data will be examined in the future. The result of fitting into this architecture paradigm is that other technologies and datasets that follow the same principles will be able to interoperate with ease, combining into one coherent cloud ecosystem of geospatial information.
Exposing imagery as COG is the fundamental building block of a cloud native for geospatial architecture. The properly formatted imagery can sit on a public S3 / Google Cloud / Azure bucket, and even be behind most any type of authorization — as long as it works with HTTP Range requests. Imagery as COG enables workflows that take place completely on the cloud, as operations to process and display web tiles can perform fast enough that there is not a need to download the data for local processing. An organization’s stores of imagery data can sit in one cloud location, and algorithms are sent to operate on the same cloud location as the data.
Any imagery online should have an endpoint that serves Web Tiles — the 256x256 pixel images that make up web maps. These are ideally served with OGC compliant WMTS (though that spec could use an upgrade to a full JSON / REST approach like WFS 3.0 is undergoing), but following best practices of ‘XYZ tiles’ (like OpenStreetMap, Google Maps or TMS) also works great. This enables users to view data being processed or stored in the cloud at any state. People should not have to download data just to view it, and the online experience should be as good as working locally. Using on-the-fly web tile services that read COG directly like marblecutter or rio tiler makes this easy if the first principle is followed.
All imagery metadata should be crawlable online. Unfortunately, there are fewer established best practices and standards for this, though the new SpatioTemporal Asset Catalog specification looks to help with this. Ideally, any provider of imagery would not even need to stand up their own RESTful endpoint, which is the idea of STAC Static Catalogs. Indeed, some software would hopefully crawl static catalogs and stand up more comprehensive search services, like Google does for the broader internet. But the key building block is exposing the core metadata to be crawlable online. Following the Spatial Data on the Web Best Practices also emphasizes the importance of HTML views of everything for search engine accessibility. Making all metadata crawlable will enable greater discovery of cloud native geospatial data, focusing on reliable access and ingestion into search indexes over every organization providing its own ‘catalog service’ that must be specifically found and then searched.
These three aspects are the true core of any cloud native geospatial architecture. The only other piece that will come in the future is tracking at least the ‘provenance’ of online processing — tracking the creation of new derived data products from existing online imagery. Processing should be done next to the data, and all derived data should link back to the process that created it and the data that went into it. Provenance and online processing will be the subject of a future article.
One may be thinking: ‘Shouldn’t there be more?’ In time there will likely be much more, but the core will stay the same. The core is simple because it is just a fundamental building block that many additional services and products can leverage. The beauty of the architecture is that it doesn’t require a number of different web services and API’s, as it is simply working to expose the data. Numerous web services can be built on top of it, but joining the architecture does not require anything more than putting one’s imagery on the cloud, as long as it’s formatted for interoperability and performance. Indeed a static, reliable architecture opens up the opportunity for additional software to add value on top of the core. The architecture is also quite compatible with the traditional OGC W*S standards as well as the latest proprietary web services, as most software can easily adapt to reading cloud native geospatial data.
Focusing on these fundamental building blocks mirrors how the world wide web was built. The key feature is stable, referenceable information that doesn’t dictate how it should be consumed. And the ability to actually use that information directly as Cloud Optimized GeoTIFF’s, instead of having to download and copy it, can change the landscape in a fundamental way. The resulting cloud native geospatial ecosystem will look different than the geospatial world today, with users able to more easily find relevant and useful information and moving towards near real-time information feeds instead of static maps.
In the next posts we’ll go deeper into the vision and implications of a Cloud Native Geospatial ecosystem, and explore how processing and provenance can work.
Using space imagery to tell stories about our changing…
122 
122 claps
122 
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/the-node-js-collection/how-to-build-and-deploy-a-cloud-native-node-js-app-in-15-minutes-7b540f7fde14?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
The goal of this tutorial is to show how you can turn a simple Hello World Node.js app into an application running on Kubernetes with all the best-practices applied. The tutorial shows you how to use the assets and tools provided by CloudNativeJS in order to build an enterprise-grade application, turn it into a Docker container image and then deploy that image easily on Kubernetes.
This article was written by Tamas Hodi — a senior engineer at RisingStack. RisingStack is software development and consulting agency specialized in Node.js, Kubernetes & Microservices. RisingStack is a contributor & supporter of CloudNativeJS. You can read why in their recent announcement: RisingStack Joins the Cloud Native Node.js Project
You can find the code of the sample Node.js application in this repository: https://github.com/RisingStack/cloudnativejs-risingstack-tutorial/tree/master/src
You can find a prepared example app, a Node.js web server based on express, within the ./src folder which we will use during the tutorial. In case you are more into adventures, you can use the Hello world example app written by express or your existing application as a starting point.
Either way you go, start with installing your dependencies by using npm i on a prepared project or npm init && npm i express.
At this point, you should be able to run the server by node server.js which is reachable on localhost:3000.
Kubernetes provides health checking in order to help you manage the lifecycle of your apps and detect the malfunctioning ones. We are going to prepare liveness and readiness probes to know when to restart the container and make the application more available and more available and resilient against downtime.
To do so, we are going to use middlewares provided by CloudNativeJS/cloud-health-connect. Add it to your dependencies with:
Now, edit the server.js and register the new endpoints called /health and /ready.
In order to see the application’s state, restart the app and open localhost:3000/ready.
You will see that the application is still booting, waiting for the resolution of all the registered ReadinessChecks, so the response is 503 UNAVAILABLE:
Once all the ReadinessChecks have got resolved, it will send you 200 OK with the following payload, so Kubernetes will be able to start forwarding traffic to this container:
Prometheus, is an open-source monitoring solution under CNCF, which helps you powering applications with metrics and alerting.
Service monitoring means tasks of collecting, processing, aggregating, and displaying real-time quantitative data about a system. Prometheus gives you the ability to observe the application’s state and address issues before they impact your business.
You can find additional information about getting started with Prometheus here or use this helm chart to deploy the tool and start monitoring and collecting metrics.
The package called appmetrics-prometheus from CNJS provides the basic metrics (CPU, RAM usage) to be scraped by Prometheus.
After installing the module with npm install appmetrics-prometheus we have to require it before creating a web server, like in the example below:
The prometheus.attach() must be called before instantiating the server because it will patch the constructor as well as the createServer method of the native modules in order to provide the /metrics endpoint with the collected metrics.
Distributed tracing is a method used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance.
We will use CloudNativeJS/appmetrics-zipkin to instrument our application automatically for an OpenTracing based request tracker called Zipkin.
Install the necessary package with npm install @cloudnative/appmetrics-zipkin.
Now you only have to include the require('appmetrics-zipkin') line in the entry point of your app before requiring other packages to let it correctly instrument other modules, so add it to the first line of your server.js file.
Please note that this way the instrumentation will use the default configurations, but you probably want to use your Zipkin instance running within your Kubernetes cluster instead sending the traces to localhost:9411 so for further options take a look at the custom configurations.
For further reading about OpenZipkin please visit the following guide or use this helm chart to get your first instance up and running quickly.
In order to use the template files, copy them from the CloudNativeJS/docker project into your application directory.
The original Dockerfile prepares some configurations for you, which you might like to customize:
Our example app has been already configured to respect these rules, like the port:
start script has been added to the package.json as well to start the server: node server.js
We can build the image with running the following command:
Test the image by running the container:
CloudNativeJS/helm provides template Helm Charts for deploying a Node.js web application into any Kubernetes based cloud. To do so, copy the template files from the project into your application directory.
Now take a look at the ./helm/chart/nodeserver/values.yaml which contains the initial parameters for a simple deployment.
First, we have to change the value of image.repository to use our own image called cloudnativejs-example.
Please note that we have used the same image tag during build, so you might change this later.
You can change other parameters as you wish, like Deployment (resources.limits, hpa) and Service (type, port) configurations.
In order to check the generated resources you can run the following command:
You can deploy and run your application by kubectl apply -f directly on these objects or in case of having Tiller installed on the cluster, use instead:
Feel free to save and commit your application Chart alongside your source code.
As you can see, with the libraries provided by CNJS you can easily add monitoring and distributed tracing to your app which you can then Dockerize and deploy to Kubernetes using Helm Charts.
At RisingStack we believe that these tools are extremely useful for Node.js developers to standardize applications within the cloud and speed up the development process.
Originally published at https://www.cloudnativejs.io/blogs.html on March 19, 2019.
Community-curated content for the millions of Node.js
114 
114 claps
114 
Community-curated content for the millions of Node.js users.
Written by
Consulting, training & development services — with a strong focus on JavaScript, Node.js, DevOps, Microservices & Kubernetes | contact: info@risingstack.com
Community-curated content for the millions of Node.js users.
"
https://medium.com/@dominik-tornow/glossary-cloud-native-applications-faa931e7eea6?source=search_post---------54,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dominik Tornow
Mar 15, 2019·4 min read
Ever since the term “cloud native” made its debut in 2010, its usage has grown in popularity. However, a concise and accurate definition is not readily available. This article provides a set of related definitions for technical and non-technical stakeholders.
This blog post provides a concise and accurate definition of the concept cloud native. In addition, this blog post provides concise and accurate definitions of related concepts.
Responsiveness is defined as an application’s ability to meet its service level agreements (SLAs), such as response time or throughput.
Scalability is defined as responsiveness in the presence of load.
Reliability is defined as responsiveness in the presence of failure.
The combination of the scalability of an application and the reliability of an application is also referred to as the Elasticity of an application.
In practice, service level agreements must restrict the required scalability and reliability of an application. After all, a finite set of resources cannot sustain infinite load or infinite failure.
A Cloud Platform is a service provider that enables a service consumer to request and release resources on demand.
Consequently, a Non-Cloud Platform is any platform that does not enable the service consumer to request and release resources on demand.
According to this definition, a cloud platform is any provider that enables a service consumer to request and release resources on demand:
A Cloud Application is an application that is hosted on a cloud platform
Consequently a Non-Cloud Application is an application that is not hosted on a cloud platform.
For example, according to this definition, a vanilla Wordpress installation hosted on a cloud provider is a cloud application.
A Cloud Native Application is a cloud application that is scalable and reliable by construction.
Consequently, a Non-Cloud Native Application is an application that is not scalable and reliable by construction.
The operator “by construction” is in stark contrast to “by requirement”: The design and implementation of a cloud native application guarantees scalability and reliability (within the limits set by SLAs).
According to this definition a cloud native application must be able to
Note, that this definition does not state how an application needs to be able to detect and mitigate either load or failure — A wide range of detection and mitigation strategies exist.
For example, according to this definition, the aforementioned vanilla Wordpress instance is not a cloud native application as it lacks load and failure detection and mitigation facilities.
Common definitions state that cloud native applications are container packaged, microservice oriented, and dynamically orchestrated.
While cloud native applications commonly exhibit these characteristics, the characteristics themselves do not serve well as defining criteria:
Designing an application as a containerized application is a technical choice. Other technologies, like packaging applications as virtual machines, may be the appropriate choice and do not disqualify an application as a cloud native application.
Designing an application as a microservice oriented application is an architectural choice. Other architectures, like massively parallel batch processing, may be the appropriate choice and do not disqualify an application as a cloud native application.
While all cloud native applications are applications that rely on dynamic orchestration, not all applications that rely on dynamic orchestration are cloud native applications.
Concise and accurate definitions are essential to communicate complex concepts, both to technical and non-technical stakeholders. If we do not share a common vocabulary, we cannot share a common understanding!
Principal Engineer at temporal.io
86 
Thanks to Andrew Chen. 
86 claps
86 
Principal Engineer at temporal.io
About
Write
Help
Legal
Get the Medium app
"
https://blog.heptio.com/cloud-native-part-5-1c1106a4caf8?source=search_post---------55,"This is the fifth part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
Microservices are a new name for a concept that has been around for a very long time. Basically, it is a way to break up a large application into smaller pieces so that they can be developed and managed independently. Let’s look at some of the key aspects here:
Sizing of microservices can be a tricky thing to get right. I’d say to avoid services that are too small (pico-services) and instead aim to split services across natural boundaries (languages, async queues, scaling requirements) and to keep team sizes reasonable (i.e. 2 pizza teams).
The application architecture should be allowed to grow in a practical and organic way.
The application architecture should be allowed to grow in a practical and organic way. Instead of starting with 20 services start with 2–3 and split services as complexity in that area grows. Oftentimes the architecture of an application isn’t well understood until the application is well under development. This also acknowledges that applications are rarely “finished” but rather always a work in progress.
Are microservices a new concept? Not really. This is really another type of software componentization. We’ve always split code up into libraries. This is just moving the “linker” from being a build time concept to a run time concept. (In fact, Buoyant has an interesting project called linkerd based on the Twitter Finagle system.) This is also very similar to the SOA push from several years ago but without all of the XML. Viewed from another angle, the database has almost always been a “microservice” in that it is often implemented and deployed in a way that satisfies the points above.
Constraints can lead to productivity. While it tempting to allow each team to pick a different language or framework for each microservice, consider instead standardizing on a few languages and frameworks. Doing so will improve knowledge transfer and mobility within the organization. However, be open to making exceptions to policy as necessary. This is a key advantage of this world over a more vertically integrated and structured PaaS. In other words, constraints should be a matter of policy rather than capability.
While most view microservices as an implementation technique for a large application, there are other types of services that form the services spectrum:
As services shift from being an implementation detail to a common infrastructure offered up within an enterprise the service network morphs from being a per-application concept to something that can span the entire company. There is an opportunity and a danger in allowing these types of dependencies.
In the next part of this series we will look at how Cloud Native creates both problems and opportunities in the security domain.
Heptio
104 
104 claps
104 
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://medium.com/swlh/the-how-of-cloud-native-architecture-and-design-perspective-7bb629255bb3?source=search_post---------56,"There are currently no responses for this story.
Be the first to respond.
Top highlight
By Kyle Brown and Kim Clark
Note: This is part 3 of a multipart series. For the first article in the series, start here, or jump to Part 2, Part 4, Part 5.
In our previous article in this series we discussed how a move to a cloud native approach might affect how you organize your people and streamline your processes. In this post we will drill down on how it relates to architecture and design principles.
It is the architectural approach that brings the technology to life. It is possible to deploy traditional, siloed, stateful, course-grained application components onto a modern container-based cloud infrastructure. For some, that’s a way to start getting their feet wet with cloud, but it should only be a start. If you do so, you will experience hardly any of the advantages of cloud native. In this section we will consider how to design an application such that it has the opportunity to fully leverage the underlying cloud infrastructure. It should quickly become apparent how well-decoupled components, rolled out using immutable deployments, is just as essential as embracing the agile methods and processes discussed already. We show the pieces of cloud native architecture below:
Fine-grained components
Until relatively recently, it was necessary to build and run software in large blocks of code in order to use hardware and software resources efficiently. More recent developments in technology, such as containers, have made it realistic to break up applications into smaller pieces and run them individually. There are a few different aspects to what we mean by fine-grained:
When building applications in this way this is typically known as microservices, although it should be noted that a true “microservices approach” is much broader than just fine-grained components, and indeed overlaps significantly with the concepts of cloud-native described here.
The core benefits of more fine-grained components are:
· Greater agility: They are small enough to be completely understood in isolation and changed independently.
· Elastic scalability: Each component can be scaled individually maximizing the efficiencies of cloud native infrastructure.
· Discrete resilience: With suitable decoupling, instabilities in one microservice do not affect others at run time.
While what is above can provide dramatic benefits in the right circumstances, designing highly distributed systems is non-trivial, and managing them even more so. Sizing your microservice components is a deep topic in itself, and then there are further design decisions around just how decoupled they should be, and how you manage versioning of the coupling that remains. Spotting necessary cohesion is just as important as introducing appropriate decoupling, and it is common to encounter projects that have gone too fine grained and have had to pull back. In short, your microservices application is only as agile and scalable as your design is good and your methods and processes are mature.
Note: Microservices architecture is often inappropriately compared to service-oriented architecture (SOA) because they share words in common and seem to be in the same conceptual space. However, they relate to different scopes. Microservices is about application architecture, and SOA is about enterprise architecture. This distinction is critical, and is explored further in “Microservices versus SOA: How to start an argument”.
Appropriate decoupling
Many of the benefits of fine grained components (agility, scalability, resilience) are lost if they are not decoupled from one another. They need to have:
Writing modular software is hardly new. All design methodologies from functional decomposition through object oriented programming to service oriented architecture, have aimed to break up large problems into smaller, more manageable pieces. The opportunity in the cloud native space is that by taking advantage of technologies such as containers we have the opportunity to run each as a truly independent component. Each component has its own CPU, memory, file storage, and network connections as if it were a full operating system. It is therefore only accessible over the network and this alone creates a very clear and enforceable separation between components. However, the decoupling provided by the underlying platform is only part of the story.
From an organizational point of view, ownership needs to be clear. Each component needs be completely owned by a single team who has control over it’s implementation. That’s not to say that teams shouldn’t accept requests for change, and indeed pull requests from other teams, but they have control over what and when to merge. This is key to agility since it ensures the team can feel confident in making and deploying changes within their component so long as they respect their interfaces with others. Of course, even then, teams should work within architectural boundaries set by the organization as a whole, but they should have considerable freedom within those boundaries.
Components should explicitly declare how you can interface with them and all other access should be locked down. They should only use mature, standard protocols. Synchronous communication is the simplest and HTTP’s ubiquity makes it an obvious choice. More specifically, we typically see RESTful APIs using JSON payloads, although other protocols such as gRPC can be used for specific requirements.
It is important to differentiate between calls across components within the same ownership boundary (e.g., application boundary) and calls to components in another ownership boundary. This is important but beyond the scope of this article. See this post for more information.
However, the synchronous nature of HTTP APIs binds the caller to the availability and performance of the downstream component. Asynchronous communication through events and messages, using a “store and forward” or “publish/subscribe” pattern, can more completely decouple themselves from other components.
A common asynchronous pattern is that owners of data publish events about changes to their data (creates, updates, and deletes). Other components that need that data listen to the event stream and build their own local datastore so that when they need the data, they have a copy. This process is related to other event driven architecture patterns such as event sourcing and CQRS.
Although asynchronous patterns can improve availability and performance, they do have an inevitable downside: They result in various forms of eventual consistency, which can make design, implementation, and even problem diagnosis more complex. Use of event-based and message based communication should therefore be suitably qualified.
Minimal state
Clear separation of state within the components of a cloud native solution is critical. There are three key common topics that come up:
Statelessness enables the orchestrating platform to manage the components in an optimal way, adding and removing replicas as required. Statelessness means there should be no changes to configuration or the data that is held by a component after it starts that makes it different from any other replica.
Affinity is one of the most common issues. Expecting a specific user or consumer’s requests to come back to the same component on their next invocation, perhaps due to specific data caching. Suddenly, the orchestration platform cant do simple load-balanced routing, relocation, or scaling of the replicas.
Two phase commit transactions across components should also be ruled out, as the semantics of the REST and Event-based protocols do not allow the communication of standardized transaction coordination. The independence of each fine-grained component, with its minimal state, makes the coupling required for a 2PC coordination problematic in any case. As a result, alternative ways of handling distributed updates, such as the Saga pattern, must be used, taking into account the issues of eventual consistency already alluded to.
Note that this concept of minimal state should not be confused with a component interacting with a downstream system that holds a state. For example, a component might interact with a database or a remote message queue that persists state. However, that does not make our component stateful, it is just passing stateful requests onto a downstream system.
There will always be some components that require state. Platforms such as Kubernetes (K8s) have mechanisms for handling stateful components with extra features, and associated restrictions. The point is to minimize it, and to clearly declare and manage it when it does occur.
Immutable deployment
If we are to hand over control to a cloud platform to deploy and manage our components, we need to make it as straightforward as possible. In short we want to ensure there is only one way to deploy the component, and once deployed, it cannot be changed . This is known as immutable deployment and is characterized by the following three principles
From “appropriate decoupling” we already know that our components should be self contained. What we need is a way to package up our code and all it’s dependencies that will enable extremely consistent deployment. Languages have always had mechanisms to build their code into a fixed “executable”, so that’s not new. Containers bring us the opportunity to go a step further than that, and package up that code/executable along with the specific version of the language runtime, and even the relevant aspects of the operating system in to an fixed “image”. We can also include security configuration such as what ports to make available, and key metadata such as what process to run on startup.
This allows us to deploy into any environment consistently. Development, test, and production will all have the same full stack configuration. Each replica in a cluster will be provably identical. The container image is a fixed black box, and can be deployed to any environment, in any location, and (in an ideal world) on any container platform, and will still behave the same.
Once started, the image must not be further configured at runtime, to ensure it’s ongoing consistency. This means no patches to the operating system, no new versions of the language runtime, and no new code. If you want to change any of those things, you must build a new image, deploy it, and phase out the original image. This ensures we are absolutely confident of what is deployed to an environment at any given time. It also provides us with a very simple way of rolling back any of these types of change. Since we still have the preceding image, we can simply re-deploy it — assuming, of course, that you adhered to “minimal state”.
Traditional environments were built in advance, prior to deployment of any code, and then maintained over time by running commands against them at runtime. It’s easy to see how this approach could often result in configuration divergence between environments. The immutable deployment approach ensures that code is always deployed hand-in-hand with its own copy of all the dependencies and configuration it was tested against. This improves testing confidence, enables simpler re-creation of environments for functional, performance and diagnostics testing, and contributes to the simplicity of elastic scaling.
Note that in theory we could have done image based deployment with virtual machine images, but they would have been unmanageably large. It was thus more efficient to run multiple components on a virtual machine instance, which meant there was no longer a one to one relationship between the code and it’s dependencies.
Zero trust
Put simply, zero trust assumes that all threats could potentially occur. Threat modelling has a long history, but the nature of cloud native solutions forces us to reconsider those threats and how to protect against them. Some (but not all) of the key tenets for a zero trust approach are:
It has long been known that traditional firewall-based access control results in inappropriate trust of the internal network. Indeed the assumption that you can create trusted zones in a network is, at best, only a first line of defense. Identity needs to become the new perimeter. We should aim for fine grained access control based on the identity of what we are trying to protect: users, devices, application components, data. Based on these identities, we should then offer only the least amount of privileges. Connectivity between components should be explicitly declared and secured (encrypted) by default. Administrators should only be given the precise privileges they need to perform their role. We must also regularly perform vulnerability testing to ensure that there are no paths to permissions escalation.
Applications must accept that they have a responsibility to keep their user’s data safe at all times. There are ever more sophisticated ways to correlate data from multiple sources, deriving new information for malicious purposes. Applications should consider privacy of all data they store, encrypt any sensitive data both at rest, and in transit, and ensure it is only accessible by identities with explicit permission.
Application components must be built secure from the start. We must assume that all environments, not just production, are vulnerable to attack. But more than that, through a “shift left” of security concerns, we should ensure that application designers collaborate early on with the security team, and embed secure practices seamlessly for example in build and deploy pipelines. This further improves the speed, consistency, and confidence with which we can continuously deliver code to production.
In the next article, we’ll look at the unique aspects of cloud technology and infrastructure to see how they enable the cloud native approach.
Get smarter at building your thing. Join The Startup’s +750K followers.
87 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
87 claps
87 
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/cloud-native-developer-workflow-3f302dcdd855?source=search_post---------57,"There are currently no responses for this story.
Be the first to respond.
Software development tooling and processes have evolved rapidly in last decade to meet growing needs of developers. On top of mastering, often a few, programing languages and paradigms, software developers must learn to navigate increasingly complex landscape of tools and processes.
My motivation for writing this blog was an introduction to a concept of software development in the Cloud Native ecosystem. I’m going to be focusing more in depth on software development for containerized workloads orchestrated by Kubernetes.
According to Stack Overflow Developer Survey for 2020 there is a high chance that if you are reading this, you use JavaScript, HTML and CSS. Most of you will also use SQL, Python, Java or C# and number of scripting languages. There is also a high chance that some of you will use mix of frontends and backend languages.
This is already a lot to learn and keep track of, as the languages evolve. Most of the languages grow their own ecosystem of reusable code packages, SaaS offerings, IDEs, etc. As developers you need to keep track of all this to deliver value to customers faster and faster.
In order to support growing complexity of distributed systems development and operationalization, cloud native ecosystem developed amazing open source projects to move faster and help IT professionals benefit from cloud native paradigm.
What is cloud native? CNCF gives us an official definition:
Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.
These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.
The above list of products is constantly growing and evolving. It is an extremely busy space and it’s hard to keep up the pace.
Let’s refocus on the role of developer in this environment. Now developers not only focus on programming languages, but also need to take under consideration a lot of other tools. One of the most prominent tools from CNCF landscape that reshaped how software is hosted, operated and delivered is Kubernetes.
I encourage developers to learn about Kubernetes and Docker, but very often thigh schedules and deliverables prevent this. For those people who do not want or cannot get in depth into Kubernetes but need to develop apps that run on it, there are tools to help maintain developer workflow and minimize the need to interact with Kubernetes.
On a high level each software developer will follow those basic steps:
From there we are far from “done”. Hopefully there are end to end tests, security checks and other steps running as part of your deployment pipeline.
As long as hosting environment can be easily emulated on local developer machine and system is monolithic, the above workflow is fine, but what if you are working with a modern distributed system built using microservices-oriented architecture, service oriented architecture or serverless approach(or combination of all).
In fact modern software development moves towards Cloud Native model. Most of the workloads run in containers and Kubernetes. So, do you need to download and configure half of the internet on your local machine ;) to develop a simple app just because it’s running in docker container and in Kubernetes?! Well you surely can and many developers still do, but there is a better way!
Let’s try to revisit the “standard” software development workflow and see how we can adjust it to take advantage of Cloud Native paradigm:
Essential steps of the workflow stay the same, but now we can improve each step by taking advantage of what Cloud Native paradigm has to offer.
Software development workflow is something that every developer is very familiar with. We follow same proven steps from project to project often not realizing that there are better, more optimized ways of adjusting our workflow especially when working with modern, Cloud Native projects.
In next blogs we will take a closer look at some of the tools and learn how we can benefit from Cloud Native paradigm and modernize development workflow to develop software faster and with less friction.
Get smarter at building your thing. Join The Startup’s +750K followers.
164 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
164 claps
164 
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kevinhoffman/distributed-transactions-in-a-cloud-native-microservice-world-7528f8baa8da?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Hoffman
Aug 13, 2015·7 min read
Yesterday I was having a great conversation with a bunch of super smart people (if you haven’t tried surrounding yourself with geniuses, you should try it sometime), and the topic of dealing with distributed transactions in cloud-native, microservice architectures came up.
First, let’s take a look at the problem. When we build the big enterprise apps and servers that we’ve been building for years, sometimes we need to coordinate multiple activities in a transaction. The classic example of a transaction that everyone uses is transferring money in a bank: you withdraw from one account and deposit into the other. If either of these activities fails, you need to be able to restore the state of your account to what it was prior to the transaction, or someone’s going to lose money. The acronym ACID (Atomic, Consistent, Isolated, Durable) comes up often when talking about these kinds of transactions. The problem becomes even more involved when the activities being coordinated as part of a transaction are distributed — they don’t all take place in the same memory space, on the same server, or even in the same data center.
Many of us have spent years of our lives drowning in the quagmire of distributed transaction coordinators, compensating resource managers, and the rest of the nightmare induced by distributed transactions. Transactions are a hard problem, even when isolated to a single process on a single box. Companies make their entire fortune making and selling transaction management libraries, components, servers, etc.
So now we’ve decided to embrace the cloud and we’re all-in on the concept of microservices and we love our 12-factor applications — now what? Unfortunately, there’s no easy fix, no salve one can apply to stop the bleeding on this wound.
I’ve seen this particular problem a number of times: we decide to migrate a service to the cloud. The service uses a transaction manager to coordinate transactions, so how do we make this work cloud-native? One reason why a service might need a transaction coordinator is that it is doing more than one thing. If you’re transferring funds, then the service might be able to deal with transactions internally. There’s nothing wrong with hiding transactions inside your service, the problem comes when you need a transaction to span across multiple services.
Before trying to figure out how to do that (discussed next), ask yourself if the existing service is violating SRP. If it is, refactor it so that it is a true microservice in every sense of the word. Once you’ve got multiple microservices that don’t violate SRP, you may find that some of your presumed need for transactions falls away, leaving you with a smaller subset of transaction problems to deal with.
You may also want to avoid using the money transfer analogy when talking about this problem. A single service that transfers funds from one account to another is not necessarily a candidate for distributed transactions across microservice boundaries. This is ugly, difficult work, so before trying to do it the hard way, see if there aren’t other designs or refactoring patterns you can adopt to avoid the need for a transaction to cross service boundaries.
A pretty common mistake people make when designing RESTful services and APIs is to directly map the resource model (the URLs exposed by the service) to the internal data model. This problem continues to proliferate because tools and scaffolding systems often default to this approach (I’m looking at you Rails, and you too, MEAN). While this is handy for a simple hello world just to get things up and running, it is rarely ever the best approach to exposing a public API for interacting with your system. This is especially true in the case of transactional systems.
Modeling interactions this way is often a sore spot with REST purists who claim that this is where the beauty and purity of REST breaks down and people end up “hacking” resource models to bend to their will. I will leave that philosophical discussion for another time.
Let’s say you have a service exposed that allows you to buy something. We don’t normally think about it like this, but this is a natively transactional API. The act of buying something is either the start of or in the middle of some kind of purchasing workflow, where all activity is correlated to an transaction (an Order ID, for example). The stuff we bought, how much we paid for it, when we paid, where it’s going, and when it ships all belong to a single transaction. This transaction may spawn other back-end transactions, but you get the idea.
When you POST to a resource collection, say /orders, you get a new order and a new transaction has been created implicitly. When you then interact with that order (check status? GET /orders/ABCXYZ) you are interacting with a long-lived system transaction. This is usually a very different feeling than the short-lived database transactions we often use to justify the use of a distributed transaction coordinator.
I realize there are always exceptions to every rule, but one to really try and live by is this: if you need a DTC you’re doing it wrong. There is almost always a better way and, in many cases, refactoring out the need for a DTC provides huge ROI in terms of other benefits — elegant design, performance, scalability, reliability, etc.
In a classic DTC (distributed transaction coordinator)/two-phase commit scenario, some bit of code starts a transaction. Other bits of code in other locations then perform their pieces of the larger task and they vote or elect on the outcome of the transaction. If everyone voted thumbs up then the transaction commits. If some piece of the transaction was unable to complete, it fails outright or votes thumbs down, and the transaction rolls back.
This is all well and good when you’re running a monolith on a single box, the DTC is under your control, and you have a synchronous process where you can sit and wait for all the work to finish. This is not the case in the cloud-native world.
In today’s applications, a transaction can start, some work takes place in parallel, other pieces of work may be advanced asynchronously from a mobile application triggered by the user, and further work moves the transaction asynchronously from back-end processes. Now remember that this is running in the cloud — some of the transaction participants may be temporarily out of network contact with each other as they move from one virtual data center to another. Others might be dynamically scaling, so you could start a transaction with 2 instances of a participant and finish it with 30.
If you try and apply the “DTC” mindset to this problem, your head will very likely burst into flames.
To give a concrete sample of this that we’re all likely familiar with, take a look at the process of buying tickets online. This could be for a movie, could be for a concert, or for a flight. In a flight scenario, you start by searching flights, but at some point you start the process of booking. You then see some kind of message that says that your seats are reserved for the “next 5 minutes” or something like that. If you don’t complete your ticket purchase successfully in that time frame, you lose the seats.
This is where the pattern of eventual consistency comes in. If you’re not familiar with it as an architectural pattern, you should definitely look it up. It will save your life, will make you coffee in the morning, and tuck you in at night.
We know that our systems are spread out across the cloud, that our services are being invoked in unpredictable sequences, and sometimes some of them might need to be re-invoked to deal with random failures throughout the system. Instead of being afraid of this, we need to accept this as our new reality and embrace it.
In the case if booking a flight, you get a transaction when you go beyond some point online. The transaction then starts a countdown timer where, if it doesn’t get completed successfully, failure is assumed and the seats you reserved are released.
This type of transaction is designed specifically to deal with the unpredictable, eventually consistent nature of things. In an old enterprise app, you might perform a single distributed transaction to book a flight or rollback a flight booking. In this new design, you embrace the fact that the state of a booking is eventually consistent and that a transaction is considered a potential failure until proven otherwise.
Again, this isn’t the only solution to the problem, but it’s an example of how embracing the unpredictable, cloud-native nature of things can give you benefits above and beyond simply porting a DTC-dependent process to the cloud.
There are a number of concrete patterns for dealing with failure in and outside of transactions, but I think that’s a larger topic better covered in another post.
So, in summary: if you are looking for a strategy to port a DTC-dependent process to the cloud, try thinking about it differently. Instead, ask yourself how you can embrace the spirit of microservices and cloud-native architectures to refactor and redesign away the need for a DTC. You are likely to find that in that new future, there are other great benefits.
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
35 
1
35 
35 
1
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
"
https://medium.com/adobetech/project-firefly-build-custom-cloud-native-adobe-apps-54d157adb473?source=search_post---------59,"There are currently no responses for this story.
Be the first to respond.
Adobe solutions are incredible tools to help enterprises meet their marketing and business goals. Enterprise developers, however, know that sometimes an out-of-the-box solution just won’t do — every enterprise needs their systems to be agile and flexible to respond to changing market conditions and gain the biggest competitive advantage. Project Firefly is a complete framework for building cloud native apps, tailor-made to the business and workflow needs of a particular company, and it’s the best way to achieve that competitive advantage. Firefly provides everything you need to build custom web apps that extend the functionality of Adobe Experience Platform and Adobe Experience Cloud solutions, using Adobe I/O Runtime, Adobe’s serverless platform.
At its core, Project Firefly is a set of open source developer and automation tools to rapidly extend your enterprise. It contains all the resources you need to build single page applications with React Spectrum (Adobe’s UI toolkit), create microservices, and orchestrate APIs in Adobe Experience Cloud. With Firefly, you’ll be able to quickly execute your app development workflow, going from concept to final product, ready for your team to use in a matter of hours or even minutes.
Right now, Firefly is in developer preview. Enterprise developers will have the chance to preview the technology and try building an app themselves. We'll then gather feedback and deliver it to our product team, who will use the information to further shape the development of the product.
Apply to be part of the free developer preview here, and read on for more details on what Project Firefly is and how it can help you build a custom app to achieve your specific goals.
Project Firefly is a collection of resources, all accessible in one framework. By using all of those resources together, you’ll be able to create the custom web app you need, from the start of production to final deployment. Firefly is made up of the following parts:
Using Project Firefly to build your custom app comes with considerable advantages. Since Firefly operates on Adobe I/O Runtime, our serverless platform, you’re able to build secure, scalable apps without the worry of infrastructure overhead. Integrate and orchestrate different systems and run custom code, free of app or user limits and their corresponding resource costs. If you’re already working with Adobe solutions, it also makes perfect sense to extend them within the Adobe universe, close to your data. There’s no need to switch contexts or authenticate with different usernames and passwords when building custom apps, now and into the future.
Project Firefly is also secure. You’re able to quickly set up API authorization and easy-to-manage user access controls. If you’re already used to handling user access control in other Adobe solutions, you’ll be able to jump right into user access control for your custom apps, as this is managed entirely with the Adobe Admin console. All of this should save you considerable time in setting up, deploying, and managing your custom app.
So, how do all the tools in Project Firefly work together? Since Firefly is a complete framework, the focus is on giving you everything you need to build and distribute a custom app from start to finish. Below is a sample workflow, stretching across different teams and stakeholders, of what goes into building a custom app with Project Firefly:
Since all of this is happening within I/O Runtime’s serverless framework, close to your existing Adobe solutions and data, it represents the quickest and most efficient way to extend Adobe solutions. Using Project Firefly, you can create custom apps like:
“With Firefly, we were able to create a lightweight, custom application that integrates with Adobe technologies without ever losing momentum on our idea,” said Ray Blaak at Ensemble, who used Firefly to build a better way to track campaign codes for use with their customers.
“Firefly enabled us to integrate with Adobe solutions and deliver a custom web app with better performance, higher security, a lower cost to scale, and a better developer experience.”
To learn more about Project Firefly, check out the overview videos below:
Project Firefly represents the next big step forward in extensibility at Adobe. With Firefly, we know developers will be able to simplify their workflows while obtaining better results with the custom apps they build for their organizations.
We need your help to make sure Project Firefly becomes the best tool possible. If you’re an enterprise developer and you build apps (or want to build apps) that extend Adobe Experience Cloud solutions, apply to take part in our free developer preview and continue to watch the Adobe Tech Blog for more news, resources, and stories about Project Firefly. We’ll share how developers across the globe are using it to create custom apps that improve business outcomes. To learn more, head over to the Project Firefly homepage.
News, updates, and thoughts related to Adobe, developers…
51 
51 claps
51 
News, updates, and thoughts related to Adobe, developers, and technology.
Written by
Sr. Product Marketing Manager at @Adobe.
News, updates, and thoughts related to Adobe, developers, and technology.
"
https://medium.com/hackernoon/writing-sky-high-applications-a-guide-to-cloud-native-development-9f3c1c020471?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
The cloud native era of application development has arrived, and with it a growing range of opportunities and challengers for developers. Designed to enable loosely coupled, resilient, manageable, and observable systems, cloud native technologies are fundamentally changing the way applications can be built and run for improved scalability in new, cloud-based environments.
Drawing on insights from an Alibaba developer, this article looks at the key practices cloud native developers should observe as they write these highly-autonomous applications, from ensuring elastic scalability and fault tolerance to enhancing ease of management and observation.
The first step in cloud native development is to give applications their required shapes. Traditional applications are often akin to snowballs that continue to accumulate functions, meanwhile becoming increasingly cumbersome and difficult to modify while falling out of step with the businesses they support as they evolve. By contrast, one of the primary goals of cloud native development is to enable rapid iteration, trial and error, and business innovation. To this end, developers must start by defining their applications’ structures with the architectural concepts present in microservices.
Ensuring the agility of an entire application system requires decomposing the application into multiple self-contained parts that can be independently implemented, pushed through evolution, and scaled — i.e., microservices.
Making divisions among microservices is somewhat of an art, in which the general principle is to divide them according to their business domains. This can be done at a fine or coarse level of granularity. A general approach is to identify the primary business components that can be individually implemented by a microservice, as shown in the following example from an online store divided into components like commodities, users, payment, and inventory:
Having determined the appropriate divisions, the next logical step is to define and implement each microservice.
The most important part of defining a microservice is defining its API, which can be an HTTP protocol API, a REST-style API, or an RPC-based API. The advantages of the HTTP approach are that the HTTP protocol is widely accepted (with mature and complete standards, support for all programming languages, and sound programming frameworks and ecosystem) and that it is ubiquitous in the network infrastructure with a full package of supporting features such as load balancing, firewall, and cache optimization. In terms of disadvantages, the HTTP approach has a higher level of encapsulation that leads to increased overhead and lower performance. While RPC approaches like the Thrift and Dubbo protocols have better performance and latency, they are not as widely accepted despite being able to achieve cross-language operations. As a compromise between the above approaches, gRPC is based on HTTP 2.0.
In the past, choosing a particular protocol had a strong influence on the subsequent choice of which microservice invocation framework to use. For example, using the HTTP approach, it would make sense to use the Netflix open-source components as an invocation framework for microservices, including Eureka, Ribbon, Zuul, and so on. Spring made a good choice for Java developers, given its good integration of these Netflix components and its being fused into its Spring Boot system. Using the RPC approach, Dubbo’s complete operation and management system would make a good option, and its support for multiple languages has been gradually improving through previous years.
Today, the emergence of Service Mesh technology makes the data plane and management plane for microservices clearly decoupled, allowing multi-protocol support and various management capabilities to be plugged in more easily. More importantly, the sidecar approach enables applications to run independently from the microservices management system, greatly reducing its intrusiveness on applications. Istio is a popular service mesh implementation, supporting protocols like HTTP, gRPC, and TCP. The Dubbo protocol support is also being added.
The microservice API is mainly intended for internal interactions — i.e., interactions between individual microservices. As a native cloud application, it also needs an external public API in order to communicate flexibly with other applications on the cloud and various end devices. APIs at this layer are usually managed and exposed through an API gateway, which talks to the API of backend microservices. The API gateway can support simple orchestration and enable access control, traffic control, metering, and analytics.
The third step involves the application’s deployment and management. Containers are undoubtedly the ideal packaging and deployment vehicle for cloud native applications, with their greatest advantage being portability. This not only makes the development and deployment environments more consistent, but also allows applications to more easily migrate between private and public clouds of different vendors.
Each microservice can be packaged into one or more containers for deployment. Although it is possible to use an atomic tool such as Docker for deployment, deploying and managing these containers using a container orchestration tool such as Kubernetes saves a great deal of trouble, due to the tendency for cloud native applications to have a large number of containers. Meanwhile, Kubernetes also supports configuration externalization through Secrets and ConfigMaps, which is one of cloud native’s best practices for following the principles of immutable applications.
Mainstream cloud vendors provide Serverless Kubernetes service, with which users do not need to manage the underlying computing nodes required in the run of containers, but can rather describe the application according to the Kubernetes specification and then deploy it in one command or click; further, resources are provisioned on demand, which also boosts the experience of cloud native development.
Cloud Foundry takes a more radical approach, and is aimed to give developers a pure application-centric experience. As long as code is pushed, Cloud Foundry calls the corresponding buildback to package the application, finally deploying it in the form of a container. This method is better suited to applications that have simple topology and inter-dependencies, and especially web applications.
Everything comes at some cost, and while Cloud Foundry offers developers greater convenience it also limits their control over applications’ environments and the underlying management. OpenShift attempts to introduce a similar deployment experience for the Kubernetes system while retaining developers’ control over the Kubernetes layer, but its broader ecosystem remains relatively monotonous at present.
With its shape established, the next step for developers is to give a cloud native application its “soul”, or unique character. This requires a series of involved processes ranging in emphasis from scalability to support for frequent changes.
Giving applications the quality of scalability requires a series of three steps, beginning with ensuring the scale-out capability of the application logic itself; this forms a key foundation in that the application logic of each microservice can thus achieve more powerful processing capabilities by spinning up more instances. An important practice is to externalize application data and state, which can be supported by a range of off-the-shelf cloud services on the public cloud, such as by putting data into RDS or NoSQL services and putting state into Redis services. With this foundation, it becomes possible to further implement auto-scaling, which is to say that the application can automatically scale in or out according to the load. Kubernetes offers auto-scaling capability, thus enabling each of the application’s microservices to scale independently.
The second step is to ensure application management ability, such that the load balancing rules of the front-end access layer and the invocation routing among microservices can be updated in real time to reflect the coming and going of each microservice instance. This is generally realized by virtue of the load balancing service and microservice invocation frameworks provided by cloud vendors.
The third step is to ensure the scalability of the cloud services that the application depends on. Cloud services need to be able to match changes in the scales of applications. For stateless applications, the data layer often becomes the bottlenecks, so having a scalable data service is critical. If cloud services cannot provide transparent scalability, scalability of the application is out of the question. Cloud native databases like AWS’s Aurora and Alibaba Cloud’s POLARDB offer high scalability and make the best choice for future-proofing applications.
It is also important, however, to choose an appropriate transaction model according to the characteristics of the business that the application supports. Traditionally, developers have become accustomed to storing all data in a relational database and using the ACID transaction model, which is simple for the purposes of programming but sacrifices performance and scalability. In cloud native applications, NoSQL database and BASE transaction strategy offer another option. They can be used to manage non-transactional data like user reviews, tags, and so on in order to improve performance and scalability.
As with elastic scalability, giving applications the quality of fault tolerance involves work on multiple levels.
The first concerns multi-AZ, or even multi-region disaster recovery deployment and backup, which from a macro perspective has long been a best practice for cloud native applications. With this it is possible to ensure that when a certain AZ or even an entire region suffers a system failure the application will still be able to continue providing services.
The next step is to ensure fault tolerance and service downgrading capabilities needed in the event of a microservice or external dependency of an application failing. Netflix has gained notable experience in this area, and its open source Hystrix has achieved strong circuit-breaking and downgrading capabilities.
The third step must address the inevitable failure of some microservice instances, which means their work must be replaced by other instances. Statelessness offers an important means of ensuring this, but the load balancing service and microservice invocation framework need to be able to update the route immediately. Management platforms such as Kubernetes can create new instances to replace failed ones.
As one saying goes, the best way to avoid failures is to fail often. The “chaos engineering” that Netflix advocates has the valuable impact of challenging a team’s creativity to deal with issues as they emerge. To this end, failing proactively can help teams discover weak points in their systems and verify fault tolerance, thus strengthening applications.
Giving applications the quality of openness to observation and management requires using the proper tools and platforms. Kubernetes, Dubbo, and Istio, for example, provide numerous convenient management capabilities, with the latter two being able to display multiple indicators for the health of microservices. AIOps has recently become a popular trend. But before we talk about intelligence, visibility into the state of applications and automation (automatically performing specific operations according to the state) are the basis for management; only when these two have been done to a sufficient extent and when enough data has been gathered can an intelligent understanding of data be formed.
Automated continuous build and delivery capabilities like multi-environment testing and Canary Release are indispensable to achieving receptiveness to changes in an application. Mainstream cloud vendors offer off-the-shelf DevOps toolchains, which are highly beneficial to this end. For a cloud native application, it is best to start using such tools for build and release from day one.
The key trends that will define the future of cloud native development have in fact already arrived: serverless development, and AI. In common, both offer developers a way of diverting human attention from relatively burdensome aspects of development, enabling developers to focus on big-picture questions.
The serverless paradigm allows for an increasingly high level of abstraction in application development, leaving developers with fewer and fewer things to worry about over time. Serverless container service enables developers not to worry about the resources needed to run containers, while serverless function service allows developers to focus exclusively on fragmented code.
In some ways, serverlessness is the purest form of PaaS, and function computing is the epitome of this. The power of function computing is not only its cost-light model but also its ability to weave many services into an event-driven system and to divide application logic to an extremely fine level of granularity. This introduces an unparalleled level of flexibility into the evolution of applications.
Naturally, such a degree of fragmentation also poses challenges for application management, while developers today are still in the midst of struggling with the complexities of application management and operation and maintenance that microservices have introduced. At present, function computing can be used to implement small applications, or as a complement to large-scale application development. In the future, when cloud services are increasingly accessing the event system, function computing may take more of a protagonist’s role, particularly since many developers have adapted to purely event-driven programming models such as Node.js.
While few are likely to doubt the importance of AI in future applications, the question remains of what developers need to do with their applications to make them AI-ready. The first step in answering this unknown is to identify the scenarios where AI can bring value to services. This now challenges developers to think in terms of what they would hope to do if various impossibilities were all at once made magically possible, or what they would do if they could read their clients’ minds. For instance, AI’s predictive powers raise the question of how one might develop and optimize applications knowing that they would be able to see a day or more into the future. The next problem then becomes seeking out data that would prove decisive in such applications, followed by the actual models and algorithms.
In light of the above, application development should focus on maximizing data collection. What today may not have a meaning could prove valuable beyond measure in future scenarios, meaning that developers should seek ways to record as much as possible of user actions and business events in their applications.
As cloud native application platforms on the public cloud advance, a growing number of powerful native cloud concepts, best practices, and technical mechanisms like containers, microservices, service mesh, and APIs have emerged. Meanwhile, function computing, data analytics, and AI services continue to mature. Through these changes, the essence of applications remains their data models and processing logic for services, which as always depend on the insight and intelligence of human developers. As such, cloud native development is an advancing approach that can effectively hone developers’ work in a time with great intricacy, exploding information, unpredictable changes, but also endless possibilities.
(Original article by Cai Junjie蔡俊杰)
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
#BlackLivesMatter
148 
2
148 claps
148 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@arschles/cloud-native-comes-with-new-challenges-ca9c22dbbb7?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aaron Schlesinger
Dec 20, 2017·4 min read
On the last day of KubeCon 2017 in Austin, I tweeted that we’ve moved on from asking whether to run our apps on Kubernetes, to asking how best to do so.
We’re so clearly past the early adopter/hobbyist phase in our community.
Popularity and buzz aren’t hard to come by these days, and neither is adoption. The “big three” cloud providers were all in the sponsorship hall, with plenty more companies looking to get in on this cloud native thing.
With all this adoption comes innovation, and we have plenty of that too.
This was the year of the service mesh, and there are at least two major players in that space — with more on the way.
If service meshes weren’t enough, I recently wrote a post about solving service dependencies for cloud native apps as well, so that’s coming too.
All these innovations are solving big problems that folks have in production, but we’re forgetting something: ease of use.
It’s damn near impossible for someone new to Kubernetes to figure out how to get started. Hours of research and study are the price of admission to be in the cloud native club, and it shouldn’t be that way.
Let’s break down the challenges we have in cloud native applications:
The community has met lots of these challenges by building new stuff. It’s a Cambrian explosion of systems and tools to help build and manage cloud native apps.
But we don’t tell anyone how to put everything together to actually build an app.
Tons of folks at KubeCon had the same question: “how do I get my code into production?”
I have my go-to tool to put things together — Helm — but it’s rudimentary. Other people have their tools as well. But we as a community don’t even have good recommendations on how to put things together. You start with a Kubernetes and then what?
The standard answer is to learn all this stuff:
And then, you can figure out for yourself how to get your code tested, built into an image, and onto your cluster.
We need to delete that big ass list above and make it simple to get your code onto your cluster.
We need to build a system to make it easy for everyone to “do Kubernetes”
We need a system that would run in Kubernetes and would figure things out for you, so at least you could get started in a weekend.
If you’re new, you could get going fast. If you’re already in production, you could still customize this thing and improve your workflow.
Here’s what the system needs to do:
Yea, there are tons of features in that list. That’s on purpose.
But here’s the magic of our community: there’s something already built for every bullet point in that list. We’re putting those pieces together here, and the point is to make decisions on how they should fit.
This system is not an invention, it’s an extraction.
We’re gonna take strong opinions on how apps should look, on how tools should be configured, and how developers should develop in our cloud native world.
We even have prior art to draw on:
Look at how our ecosystem is growing now. I talked about the sponsor hall at KubeCon 2017 and how much momentum we have. We’re going to be 10 times as big in 5 years.
Imagine KubeCon being held at a 30,000 person convention center in 5 years.
And the way we’re going to get there is in large part by bringing in newcomers with opinionated frameworks like this one.
I can’t wait.
Credit to Mark Bates for the “not an invention, it’s an extraction” idea. You can see the wonderful talk in which he introduced the idea here.
Gopher, containerizer, and Kubernetes-er
59 
5
59 
59 
5
Gopher, containerizer, and Kubernetes-er
"
https://medium.com/@nodejs/node-js-and-the-cloud-native-experience-13f541caeb29?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Node.js
Jun 8, 2017·4 min read
In April, DockerCon descended upon Austin, TX for four days of conversations, networking and education around the next generation of distributed apps built with containers. The Node.js Foundation had the opportunity to set up shop in the Cloud Native Computing Foundation (CNCF) booth, another project under the Linux Foundation, to provide more information around Node.js to DockerCon attendees.
A special thank you 😀 to our Node.js Foundation members Codefresh and NodeSource for helping spread the Node.js love at DockerCon!
NodeSource, a company that delivers enterprise-grade tools and software for Node.js environments, and Codefresh, a company that provides pipeline automation and staging environments on-demand for Docker, attended DockerCon to connect with the operations teams that use their products, and to spread more voice around using Node.js in the enterprise.
Many DockerCon attendees were pleasantly surprised to see a larger Node.js presence at the show. Node.js is quickly becoming the runtime of choice in cloud native infrastructure. Given that cloud native is really defining how developers deploy next-gen apps, it was important to have a presence at the conference and get feedback from operations and DevOps teams.
We sat down with two Foundation members: Dan Shaw, Co-Founder and Chief Evangelist of NodeSource, and Dan Garfield, Full-stack Developer at Codefresh, for conference highlights via a Node.js lens. If you weren’t able to come, here’s the lowdown on the event.
Why was it important for you to be at DockerCon?
Dan Shaw: This was a great opportunity for us to engage with more folks on the Ops team. Often times, we find that the developers that we work with need more buy-in from the Ops team. Overwhelming, Node.js products are in Docker containers and orchestrated in Kubernetes, so this is a great place to be.
From the standpoint of the larger Node.js ecosystem, this year we are really concentrated on connecting the dots on how Node.js is the key runtime in cloud native infrastructure. Cloud native is defining how we are doing deployments for the next generation of apps and Node.js is overwhelmingly the runtime of choice.
We are really focused on ensuring that Node.js can scale within these environments and everyone is well serviced for a great Node.js experience.
Dan Garfield: For Codefresh, it’s really about us being able to show engineers our product. It changes how folks view and interact with you. We showed one guy a demo, and his jaw literally dropped. Those are the kinds of reactions we really like to see. It’s also important for our more junior developer evangelists to go to these conferences to meet people and network.
What were some interesting technologies that you saw at the conference that you think would be helpful to Node.js developers?
Dan Garfield: The most exciting thing to come out of DockerCon was multi-stage build support. Multi-stage build support makes it much easier to create lean Docker images.
Before multi-stage, Codefresh got around this by providing an API to facilitate image creation, but now anyone can bring their Dockerfile with all the build stages included. We can onboard people even faster now.
Dan Shaw: A big highlight for me was seeing storage in a containerized environment. There was a lot of vendor interaction around managing storage in cloud native environments. This has been a major challenge and it’s great to see this come to the forefront.
It’s also nice to see all the efforts that Docker the company has put into open source. They’ve made a lot changes this year with their product, and the company together, especially with Moby. I’m impressed with the evolution they’ve made this year. Moving open containers into a much larger open source ecosystem will really enable the future that they are investing in.
What were some of the biggest takeaways for you?
Dan Shaw: It was really great connecting with Dan G. and members of the Cloud Native Computing Foundation. There’s a lot of great progress that we can make as a group and it was nice to collaborate and see how we might be able to make technology for the future.
Dan Garfield: One cool thing about DockerCon is that Docker has so many use cases, IT, DevOps, developers. It felt like every crowd was represented and our product really intersects across all of these areas.
Also to reiterate what Dan S. said, it was great to collaborate with him and also CNCF. This really gave us the chance to share our shared values in open source.
And a final note from Dan and Dan:
For more DockerCon insights check out Codefresh’s blog. The company is also offering a $300 credit when you sign up for Codefresh through June 15th*. Use code: NODELOVESDOCKER47
If you’re still trying to figure out a way to get to Node Summit 2017, Node Source is offering a chance to win a free ticket to the event. The company is giving away 2 tickets to Node Summit 2017, which will be held on July 26–27 in San Francisco, CA. You can enter to win here — you’ll need to submit before June 15.
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
See all (221)
22 
22 claps
22 
Node.js is a collaborative open source project dedicated to building and supporting the Node.js platform. https://nodejs.org/en/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adrianco/cloud-native-cost-optimization-f379c2f623e9?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
adrian cockcroft
Mar 25, 2020·2 min read
One of the impacts of COVID-19 has been a large rapid unplanned change in business activity for most of the global economy. There are several outcomes, some markets like work from home tools and services, healthcare and grocery retail are extra busy, social media applications are extra busy but their revenue from advertising may be impacted, and many businesses in manufacturing, travel and entertainment are have largely been shut down because their workforce and customers are in lockdown. While datacenter based IT costs are largely fixed, in a three year depreciation schedule, cloud costs are inherently variable, and can be sized to fit the need. When times get tough, a focus on cost optimization can also greatly reduce cloud costs, and the results take effect in next month’s bill.
I’ve talked a lot about this in the past, and have found some more recent content that should be helpful to people facing these challenges.
My 2014 re:Invent talk provides a structured basis for covering all the aspects of cost optimization, although some of the examples are now out of date. In particular, reservations have been split into Savings Plans and On Demand Capacity Reservations to decouple cost optimization from capacity availability with a lot more flexibility and ease of management.
One of the customers I’ve worked with more recently is Expedia, and here’s a more practical and up to date set of examples from 2017.
Abiade moved from Expedia to work at AWS and she presented again in 2019 at re:Invent with examples from Lyft showing how they cut the AWS spend component of their cost per ride by 40% by making cost visible and providing tools to optimize.
My old friend Constantin Gonzales (we worked together over 20yrs ago at Sun) has a regular presentation at re:Invent on running Lean Architectures, with a customer appearance by HERE.
For most people, their compute load dominates their bill, and EC2 General Manager Jeanine Banks gives an in-depth talk on cost optimization, including the new AWS Compute Optimizer tool and a summary of Savings Plans.
For a deep dive on savings plans, this video is excellent. If you haven’t already got this setup, it’s a good time to focus on getting it done.
I hope you find these resources useful. AWS account management teams are engaging with customers globally to help them manage their costs and financial agility in these difficult times. Best wishes, and stay safe.
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
102 
102 
102 
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
"
https://medium.com/swlh/the-why-of-cloud-native-goals-and-benefits-5c559a4e73a5?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
Kyle Brown and Kim Clark
Note: This is part 5 of five-part series. For the first article in the series, start here or jump to Part 2, Part 3, Part 4
In our past articles, we’ve established “what” cloud native refers to and even “how” cloud native works. However, there’s a bigger, more fundamental question we haven’t addressed. Why should anyone care? Given the background from the previous two articles, we can now explore the “why” and look at the benefits of cloud native, starting at a high level of what it means to the business, then dropping down to what it means on the ground.
Business perspective on goals and benefits
Let’s face it, IT fads come and go. Many business teams try to stay above those fads and let the IT folks go their own way — after all the choice of an application server or language runtime usually doesn’t have much bearing on the way the business operates. What makes cloud native any different? Why should the business support a move to cloud native? In order to understand that, we need to start from a point of view taking into account three key things nearly all businesses must focus on to be successful: growth, risk mitigation and cost reduction. We would argue that building applications with a cloud native approach has the potential to provide benefits in all of these categories.
Market growth
Market growth is all about capturing new customers and keeping their interest. In order to win new customers and retain the interest of customers that are marginal, you have to be able to bring good, new ideas to market faster than your competition. Factors such as the adoption of lean methods and the streamlining of the path to production through pipeline automation enable teams to bring business ideas to production more quickly. This enables a reduced time to market, helping IT to move at the speed of the business in bringing out new features. However, new features will not, by themselves, ensure market growth. You have to be able to winnow out those new features that have a negative effect on customer retention and customer acquisition and allow those to die, while keeping those new features that have a positive effect on those two measures. The real key is to ensure innovation readiness; letting the business boldly and rapidly bring disruptive ideas to life in order to capture new market niches ahead of the competition, while at the same time putting in place measurements that allow you to determine empirically which ideas were good, and which ideas were not good.
Risk mitigation
New features are not everything a business needs, however. If a business could just forge ahead by constantly delighting its customers with new, awesome features, then we would have an easier job. The reality is more complex and difficult, but no less important. Customers need to trust your business. This is, of course, most visible in highly regulated industries such as financial services or healthcare, but relevant to every business. The solutions you provide need to have an appropriate degree of resilience and security to ensure that customers can be sure you’ll be there when they need you, and they can trust you with their money, their data, or their lives.
As we covered in a previous article, well written cloud native solutions make use of abstraction to decouple themselves from the underlying physical infrastructure, offering implicit resilience. Furthermore, they also follow a zero trust model for their components, since they must assume portable deployment into any cloud environment. However, the more we are concerned with risk, the more likely we are to put up barriers to making changes, which works against our need for market growth. It is here that cloud native can help in risk mitigation by providing highly automated and consistent ways to put things into production, reducing the fear of change through deployment confidence.
However, a cloud native approach doesn’t magically reduce risk on its own. Techniques like feature flags and canary testing can enable the business to do things that they might previously have rejected as being too risky, but that requires close cooperation with the business in order for those techniques to become valuable to the business. We must work with the business to update how they measure and control risk so that it is compatible with the methods and processes being introduced.
Cost reduction
No business can ignore cost. It doesn’t matter how big your market share is, or how much your customers trust you, if you can’t control costs, you can’t count on reliable profits. Businesses are run by people (at least today!) and people cost money. We need to ensure we can get the best from the smallest number of them, and this is all the more true of those with the deepest skills. The platforms on which cloud native solutions are built should aim to standardize and, wherever possible, automate the day-to-day tasks of building, deploying and managing software. This makes for optimized high value staff who can focus on directly adding value to the business rather than getting bogged down with day-to-day operations. Of course, those platforms and their underlying infrastructure need to be paid for too. Fortunately, cloud native solutions are designed to use only the resources they need, so you should take advantage of elastic cost models provided by the platform to enable you to take advantage of that cost efficiency.
IT perspective on goals and benefits
To this point, our discussion has been a bit high-level in terms of how cloud native impacts the business. In order for the business to comprehend the benefits of cloud native, we have to translate the IT benefits we have discussed into corresponding business benefits. We’ll next show how each of the key business benefit areas above map to our earlier cloud native ingredients.
Agility and productivity (Market growth)
Every business wants more features from IT — they also want them faster, and they want them to more accurately reflect what they need. How does cloud native help with that?
Faster delivery of components
Delivery acceleration is of the most commonly stated goals for a cloud native approach and pulls together three core aspects: cloud platforms, agile methods and microservices. Cloud platforms, through aspects such as elastic provisioning and component orchestration, enable us to focus on building business functionality by automating and simplifying most of the day-to-day operations work. By reducing toil, they allow teams to focus on higher-value work. Agile methods should enable us to shorten the distance between requirements and implementation and improve alignment with business goals. That means that less rework is required, and bad ideas are discovered and corrected more quickly. Design approaches such as microservices enable us to deliver functionality incrementally, with fewer dependencies, and thereby more rapidly.
Autonomous teams with freedom to innovate
An intentional consequence of fine-grained and discrete components is that it offers more autonomy to the teams creating them. As long as the key rules of decoupling we earlier described are followed, the components can be treated largely as a “black box” by the receiving platform. While cross-team collaboration and common practices should be encouraged, teams are free to, for example, use whatever language runtimes and frameworks are most productive for their needs. This freedom to innovate allows teams to “think out of the box” and deliver solutions to the business not only more quickly, but also to deliver solutions that are more innovative in terms of the business. A key aspect of this autonomy is that the business must be part of each autonomous team. The notions of a Product Owner and Sponsor Users are critical to building not only productive, but innovative teams.
Responsive to changing business conditions
Earlier, we talked about how the ability to not only innovate, but to determine if an innovation is valuable through concrete and empirical measurements were important in order to make a team ready to move at the speed of the business. A critical aspect of this is the ability of the business and IT to work together through Hypothesis driven development. Simply put, Hypothesis Driven Development is phrasing business ideas as scientific hypotheses that can be either proven or dis-proven. Cloud native development only brings benefit to the business if the business is engaged throughout the development cycle.
A primary form of engagement is through A/B testing — if you put a measurement in place, such as the percentage of abandoned carts, or the percentage of customers that click “buy” after browsing, then you can compare different ideas empirically. You can direct some of your customers to a new approach featuring a new idea, and others to the existing approach, and then compare the difference in the measurement between the two over time. The key here is that this requires the business to think in terms of measurable differences. Much as a scientific hypothesis isn’t a hypothesis if it cannot be dis-proven, the same applies to a business hypotheses. The business has to be able to help determine a quantifiable measure by which two different ideas or approaches can be compared. That means that they have to be involved throughout the process in helping determine not only what ideas should be tested, but how they should be tested, and what the definition of success means. Cloud native is well suited to enabling this responsive behavior. It is an essential part of agile methodology, and furthermore the use of fine grained, well-decoupled components makes it safer to add new functionality without disturbing what’s already there. Furthermore, generic mechanisms such as a service mesh can be used to selectively route requests across the ideas being tested, as well as simplify the collection of data for hypothesis assessment.
Resilience and scalability (Risk mitigation)
Cloud platforms, and especially containers, inherit a number of abstractions from their underlying infrastructure. This enables common and lower risk approaches to non-functional requirements such as availability, robustness, performance and security. Some key examples are:
Fine-grained elastic scalability and resilience
A well written cloud native solution is built with fine-grained, lightweight components with minimized state. This enables the cloud platform to inherently provide robustness and scalability by rapid replication and disposal of components as required. Thanks to containerization, this can be provided in a standardized way, resulting in significant operational simplification. Furthermore, the ability for container orchestration platforms to distribute container instances across multiple physical servers, over multiple regions, further increases the level of resilience possible.
Consistent environments and reversible deployments
For the agility and productivity discussed in the earlier section to become a reality, we need to be able to confidently deliver new code into environments, safely re-route traffic, and indeed reverse those deployments with minimal pain if necessary. Ideally, cloud native code should be delivered in immutable images containing all dependencies, and including complete, declarative deployment instructions. This ensures the deployment process and artifacts are identical on all environments, which eliminates the risk of environment drift. Furthermore, features of the cloud platform such as routers and service meshes enable the code to be canary tested (passing a small amount of load through the new code) before full rollout. This also simplifies rolling back a release, as we can simply revert to the images and deployment instructions of the previous release. This simplicity is important to the business, as it gives them assurance that changes can be quickly and safely reversed if the outcome is not what was expected.
Continuous adoption of software runtimes
Agile methods dictate that the path to production must be as automated as possible in terms of build, test and deployment. Often termed continuous integration/continuous delivery (CI/CD) these pipelines are typically triggered as a result of changes to the source code. However, since a declarative deployment is also code, a change to the version of an underlying runtime can also trigger a new build/test cycle — an example of “GitOps”. Assuming sufficient confidence in our automated tests, this should enable us to keep underlying runtime versions much more current than most applications do today, ensuring not only that we can capitalize on the latest features but also that we are not at risk from known security vulnerabilities. Again, this should be valuable to the business in that it reduces the fiduciary risk of the loss of customer data or assets.
Optimization and efficiency (Cost reduction)
Cloud based computing enables us to pool resources: hardware, software, and indeed people too. By sharing these resources across applications, across domains in the organization, and even across organizations. we have the opportunity to reduce operational cost. Cloud native ensures applications are written to gain the most from those optimizations.
Consistent skills on the underlying platform
Underlying the obvious characteristics of containers — lightweight scalable components — there is a much greater gem. Perhaps their greatest benefit over the long term is in operational consistency. Using exactly the same skills to build, deploy, provide high availability, scale, monitor, diagnose, and secure regardless of the runtimes within a set of orchestrated containers is a huge leap forward. At a minimum, this means transferable, common skill sets across previously siloed parts of the IT landscape. At best, the opportunities for automation of operations should result in a reduction in the number of people required to run a given infrastructure, and an increase in its reliability. However, since these technologies are new, there is a steep learning curve that individuals, and companies must go through before these benefits become a reality.
Optimized infrastructure usage and licensing
Arguably the most fundamental definition of “cloud” is abstraction from the underlying physical infrastructure. Virtual machines gave us the first level of indirection, in that we were no longer tied to specific hardware within a physical machine. Containers, if coupled with cloud native ingredients such as minimal state, immutable deployment, etc. take us much further. Cloud native enables us to invisibly distribute components across many machines in multiple data centers, providing greater opportunities for economies of scale. Software licensing of course needs to rise to this challenge with new models and potentially more sophisticated and dynamic metering.
Rapid self-provisioning of resources and capabilities
Self-provisioning is one of the key promises of cloud, enabling rapid requisition of virtual compute, memory, storage, networking and more. Container platforms further abstract this by allowing declarative requests for resources at the point of deployment, and setting policies for how these change at runtime based on load. Whole new environments can be created with a single click, and segregated from other environments through software defined networking. To make the most of all this, applications need to be written differently. Applications need to be stateless, disposable, fine grained, and indeed, all of the things we have discussed in this series.
Cloud native, just like most significant changes in approach, requires a level of commitment in order to achieve your goals. As we have seen, that requires many separate ingredients to be in place. For many, perhaps most, organizations it may be impossible to get all of those ingredients in place from the start, so the key to success is prioritization.
By selecting the benefits that are most important to you, we hope our series can help you to prioritize which of the cloud native ingredients you should focus on maturing first.
Get smarter at building your thing. Join The Startup’s +750K followers.
81 
81 claps
81 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/swlh/the-how-of-cloud-native-technology-and-infrastructure-perspective-765be1606840?source=search_post---------65,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Kyle Brown and Kim Clark
Note: This is part 4 of a multipart series. For the first article in the series start here or jump to Part 2, Part 3, Part 5.
While the people, process, architecture and design issues we covered in the last two articles are all critical enablers for cloud native, cloud native solutions ultimately sit upon technology and infrastructure, which is what we’re going to cover in this article.
Cloud infrastructure is all about abstracting away the underlying hardware to enable solutions to be rapidly self-provisioned and scaled. It should enable administration of different language and product runtimes using the same operational skills. Furthermore it should promote automation of operations, and provide a framework for observability. Let’s take a closer look at exactly what the key characteristics of that infrastructure are that are leveraged in a cloud native approach.
Elastic, agnostic, secure platform
For cloud native to work, we have to ask a lot from the platform on which we deploy our components. The platform should help us to not worry about non functional concerns by using common mechanisms across whatever we deploy, and likewise should “burn in” security. Therefore, our top requests from the platform should be:
If developers are to increase their productivity, they need to be able to focus purely on writing code that creates business value. That means the platform should take care of concerns such as load balancing, high availability, scalability, resilience, and even some elements of disaster recovery. At deployment, we should be able to specify high level non-functional requirements and let the platform do the rest. We will use Kubernetes container orchestration as a powerful (indeed ubiquitous) example of this kind of thinking, but cloud native is definitely not limited to the Kubernetes platform.
A Kubernetes cluster provides automated, elastic provisioning of resources such as cpu, memory, storage and networking based on the requirements of the component being deployed. The pool of resources can be spread across many physical machines, and over multiple availability zones in many separate regions. It takes on the responsibility of finding the resources you need, and deploying your components to them. You only need specify your requirements — what resources you need, how they should or should not be spread out, and how they should be scaled and upgraded. Arguably, we could also have said that about platforms based on virtual machines, but as we will see, containers bring something more to the party.
Assuming we adhere to the architectural principles from the previous section, delivering application components in containers enables Kubernetes to perform deployment and subsequent operations in a standardized way, regardless of the contents of any given containers. It the components are largely stateless, disposable, fine-grained, and well-decoupled, this makes it easy for the platform to deploy, scale, monitor, and upgrade them in a common way, without knowledge of their internals. Standards like Kubernetes are part of a trend of gradually moving away from proprietary installation and topology configuration for each software product. Now they all work the same way, and we benefit from operational consistency, reduced learning curves, and broader applicability of add-on capabilities such as monitoring and performance management.
Finally, we want to have security burnt in to the platform so we can be confident it is a safe environment for our applications from day one. We should not need to re-engineer core aspects of security every time we design a new component; we should instead be able to inherit a model from the platform. Ideally, this should cover identity management, role based access to administration, securing external access and internal communications. We will note that this is an example where Kubernetes itself is only a partial solution; added elements such as a service mesh for internal communication, and an ingress controllers for inbound traffic are also required for a complete security solution.
Lightweight runtimes
We can only achieve operational agility if the components are as straightforward and lightweight as possible. We can boil this down into three main properties.
To manage availability and scaling, components must be able to be rapidly created and destroyed. That means the runtimes inside the containers must start up and shut down gracefully and optimally. They must also be able to cope with ungraceful shutdowns. There are many possible optimizations this implies: from removing dependencies, reducing memory population performed during initiation, through enabling a “shift left” of compilations into the image build. At a minimum, runtimes should be able to start within the order of seconds but that expectation is constantly lowering (e.g. Quarkus).
We also want builds to be as straightforward and timely as possible if we are to embrace continuous integration. Most modern runtimes have removed the need for separate installation software, instead simply allowing files to be laid down on a file system. Similarly, since an immutable image by definition shouldn’t be changed at runtime, they typically read their configuration from properties files rather than receiving them through custom runtime commands.
Equally, your actual application code can also be placed on the filesystem rather than being deployed at runtime. The combination of these features enables builds to be done rapidly through simple file copies and is well suited to the layered filesystem of container images.
Automated operations
What if we could deliver the entire blueprint for how to stand up our solution at runtime — including all aspects of infrastructure and topology — as part of the release? What if we could store that blueprint in a code repository, and trigger updates just like we do with our application code? What if we could laser-focus the role of operations staff into making the environment autonomous and self-healing? These questions lead to some key ways in which we should approach the way we work with infrastructure differently:
Image based deployment, as discussed earlier, has already brought us a long way toward ensuring greater consistency. However, that only delivers the code and its runtime. We also need to consider how the solution is deployed, scaled and maintained. Ideally, we want to be able to provide this all as “code” alongside our component’s source to ensure that it is built consistently across environments.
The term “infrastructure as code” initially focused on scripting low level infrastructure such as virtual machines, networking, storage and more. Scripting infrastructure isn’t new, but increasingly specific tools such as Chef, Puppet, Ansible, and Terraform have advanced the art of the possible. These tools begin with the assumption that there is available hardware, and they provision and then configure virtual machines upon it. What is interesting is how this picture changes when we move to a container platform.
In a perfect world, our application should be able to assume that that there is, for example, a Kubernetes cluster already available. That cluster might itself have been built using Terraform, but that is irrelevant to our application; we just assume the cluster is available. So what infrastructure as code elements do we need to provide to fully specify our application now? Arguably, that would be the various Kubernetes deployment definition files packaged in helm charts or Kubernetes Operators and their associated Customer Resource Definition (CRD) files. The result is the same — a set of files that can be delivered with our immutable images that completely describe how it should be deployed, run, scaled and maintained.
So if our infrastructure is now code, then why not build and maintain our infrastructure the same way as we do our application? Each time we commit a significant change to the infrastructure we can trigger a “build” that deploys that change out to environments automatically. This increasingly popular approach has become known as GitOps. It’s worth noting that this strongly favors infrastructure tools that take a “declarative” rather than “imperative” approach. You effectively provide a properties file that describes a “to-be” target state, then the tooling works out how to get you there. Many of the tools mentioned above can work in this way, which is fundamental to how Kubernetes operates.
Real systems are complex, and constantly changing, so it would be unreasonable to think that they will never break. No matter how good a platform like Kubernetes is at automating dynamic scaling and availability, issues will still arise that will at least initially require human intervention to diagnose and resolve. However, in the world of automated deployments of fine-grained, elastically scaled components, it will become increasingly impossible to sustain repeated manual interventions and these need to be automated wherever possible. To enable this, operations staff are being retrained as “site reliability engineers” (SREs) to write code that performs the necessary operations work. Indeed, some organizations are explicitly hiring or moving development staff into the operations team to ensure an engineering culture. Increasingly, ensuring the solution is self-healing means that when the systems scale up, we no longer have the problematic need for a corresponding increase in operations staff.
Observability and monitoring
As organizations move towards more granular containerized workloads treating monitoring as an afterthought is untenable. This is yet another example where we need to “shift left” in order to be successful in cloud native. Addressing the problem depends upon around being able to answer three questions about our components:
Observability is not a new term, although it is seeing renewed use in IT and particularly around cloud native solutions. Its definition comes from the very old discipline of control theory. It is a measure of how well you can understand the internal state of a system based on what you can see from the outside. If you are to be able to responsively control something, you need to be able to accurately observe it.
Some definitions create a distinction that that monitoring is for your known unknowns such as component status, whereas observability is for your unknown unknowns — finding answers to questions you hadn’t thought to ask before. In a highly distributed system, you’re going to need both.
The platform must be able to easily and instantly assess the health of the deployed components in order to make rapid lifecycle decisions. Kubernetes for example requests that components implement simple probes that report whether a container has started, is ready for work, and is healthy.
Additionally, the component should provide easily accessible logging and tracing output in a standard form based on its activity for both monitoring and diagnostics purposes. In containers typically we simply log to standard output. The platform can then generically collate and aggregate those logs and provide services to view and analyze them.
With fine-grained components, there is an increased likelihood that an interaction will involve multiple components. To be able to understand these interactions and diagnose problems we will need to be able to visualize cross component requests. An increasingly popular modern framework for distributed tracing that is well-suited to the container world is OpenTracing.
Looking back on the Perspectives
Based on what we’ve seen, these are the key ingredients across all the previously mentioned perspectives that are required to make cloud native successful:
They are often inter-related, and typically mutually reinforcing. Do you need to do them all? That’s probably the wrong way to phrase the question, as few if any of the above are a simple binary “yes you’re doing it” or “no you’re not”. The question is more to what level of depth are you doing them. You certainly need to consider your status for each one, and assess whether you need to go further. In our next article, we’ll return to the question of “why” people are drawn to cloud native, and see if we can use that to help prioritize how, when and to what depth we embrace the above ingredients.
Get smarter at building your thing. Join The Startup’s +750K followers.
80 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
80 claps
80 
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.devgenius.io/what-is-cloud-native-what-to-know-for-your-interview-dc8da4fc1728?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
Cloud-native technologies have become an important part of modern software development. The cloud-native architecture allows us to handle sudden surges in demand in an organized and efficient way through the use of containers, microservices, DevOps, and APIs. With cloud jobs in such high demand, you can help set yourself apart from the competition by developing strong cloud skills.
In this article, we’ll explore the term cloud-native and discuss the benefits of cloud-native architecture, how to build a cloud-native application, and what to know for your cloud interviews.
Let’s get started!
We’ll cover:
Cloud-native is an approach to application development that uses the cloud computing delivery model. Cloud-native development was designed to enhance modern application development by employing the scalability, resiliency, and flexibility that the cloud provides.
As defined by the Cloud Native Computing Foundation (CNCF), “cloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds.”
Note: The CNCF was created by the Linux Foundation in 2015. It’s an open-source software foundation that promotes cloud-native technologies. Companies like Microsoft, Oracle, and Intel are members of the foundation. They support open-source projects such as Kubernetes, Envoy, and many more.
Cloud-native app development leverages things like containerization, service meshes, declarative APIs, and microservices to allow you to build, deploy, and manage high-impact cloud-native applications. Cloud-native services use technologies such as Docker, Kafka, Kubernetes, and serverless functions to help you efficiently build and run scalable applications in private and public cloud environments.
Cloud-native architecture focuses on designing apps or services that were made to exist in the cloud rather than on-premises infrastructure. The cloud-native architecture enables us to create and deploy applications that are easy to maintain and have flexibility without relying on physical servers.
Microservices and serverless functions are very important in cloud-native architectures. Microservices are one of the main parts of cloud-native app architecture. Many companies use them because they support DevOps, improve scalability, reduce costs, and enable flexibility.
Microservices and containerization support cloud-native apps by enabling us to switch between cloud providers, deploy services independently, and deploy services in different languages or frameworks without issues. There are many benefits to implementing a cloud-native architecture into your application development process. Let’s take a look at some of the benefits and challenges:
Benefits
Challenges
Traditional apps are basic apps that run on a mainframe environment or that have a client/server environment. Cloud-native apps implement the cloud computing delivery model to improve the app development process.
Let’s take a look at some of the main differences between cloud-native apps and traditional apps:
Cloud-native
Traditional
There are many different tools and practices we can use when building and operating cloud-native applications. Let’s take a look at some of the fundamentals:
DevOps is a combination of philosophies, practices, and tools that increases an organization’s ability to deliver apps and services quickly and efficiently. DevOps allows us to fully use the native cloud capabilities and ensures that dev and operations teams work together with regular communication and common goals. With DevOps, our software development processes and more consistent and efficient.
Microservices architecture involves developing applications as collections of smaller services. Each microservice can be manipulated independently of other services within the same application. This enables a more streamlined application development lifecycle without negatively impacting users.
Continuous delivery is made possible by Agile development practices. It means that we constantly make phased software changes through automation. It’s a very reliable way to release and deliver software more frequently and safely.
Since cloud-native applications rely heavily on microservices, we need a well-defined way for these separate services to communicate with each other. This is where APIs come in. Application programming interfaces (APIs) connect products and services and allow them to easily communicate so we can maximize the development process.
Using containers gives us more efficiency and speed compared to traditional virtual machines. With containerization, a single operating system instance is divided across one or more containers, which allows us to create and deploy individual microservices.
If you want to work in the cloud space, it’s important to know about cloud-native application development. Let’s discuss some fundamental information you should know and useful things you can do before your cloud-related interview:
Your expected skills and knowledge will depend a lot on the position you’re interviewing for. Overall, you still need to have a solid understanding of the fundamentals of cloud computing. Make sure you have the knowledge to answer basic questions about the cloud.
Your preparation and study topics will depend on the position you’re applying for. Make sure to brush up on the relevant ideas and technologies that pertain to the job you want. It’s also important that you demonstrate that you have some hands-on experience with cloud computing. If you don’t have experience, you should have a solid understanding of the concepts and be able to explain them at a higher level.
A cloud certification shows potential employers that you have a solid understanding of the fundamentals of cloud computing. Over 80% of hiring managers say that cloud certifications make applicants more desirable. The most popular certifications on the market now are for AWS, Azure, and GCP. Regardless of the certification you choose, multi-cloud skills are in high demand, so you’ll have room to move around.
DevOps is a strategy to help speed up application development by allowing operation feedback to come right to the developers. This means that cloud engineers follow their app throughout its entire lifecycle. For interviews, prepare to answer questions about things like disaster control, feedback, and automated data management.
Don’t forget the soft skills! Now more than ever, tech companies are looking for well-rounded candidates with technical skills and soft skills. Make sure to highlight experiences that demonstrate your flexibility, adaptability, communication skills, and your customer service mentality.
Congrats on taking your first step with cloud-native applications! Cloud-native development is growing in popularity, so it’s an important thing to know for a career in software development. There’s still so much more to learn about cloud-native app development, such as:
Happy learning!
Coding, Tutorials, News, UX, UI and much more related to development
62 
62 claps
62 
Coding, Tutorials, News, UX, UI and much more related to development
Written by
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
Coding, Tutorials, News, UX, UI and much more related to development
"
https://medium.com/buildpacks/cloud-native-buildpacks-2020-roadmap-b7e43876473a?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
“It matters not what someone is born, but what they grow to be.” — Dumbledore
Welcome to another year with Cloud Native Buildpacks! Earlier this month, the core team sat down to discuss our plans, priorities and goals for 2020. We received lots of user feedback in 2019 that we want to address while continuing regular maintenance, but we also have a lot of new features we want to implement. In order to help prioritize this work, we’ve established a north star for this year’s roadmap.
The theme of the 2020 roadmap is: Coming of Age
Our roadmap will emphasize features and fixes that bring Cloud Native Buildpacks to the next chapter in its journey. Last year, our goal was to get the project’s first set of production users — the early adopters. This year, we’re targeting a 1.0 release, signaling that the project is mature enough for anyone to use it in production. Continue reading to learn how we’ll do it.
We shipped a lot of new things last year, but now it’s time to make sure we’re delivering them to our users the best way we can. We want to establish a well defined Release Cadence and improve our end-user documentation.
We also want to make sure that the tools we’ve shipped measure up to our quality standards, which is why we’re including Reproducibility, Content Signing, and Docker history metadata on the roadmap.
Above all, we want to finish some of the things we started in 2019; in particular features that are in the final phases of being defined, or have been specified but not yet implemented. This includes Service Bindings, Distribution Specification, OS Packages, and a Project Descriptor.
We’re flattered by how many people have tried Pack today. They’ve run buildpacks on their local workstations to create OCI images they can run with `docker run` and Kubernetes. But using that process as part of a deployment pipeline where source code is automatically transformed into a Docker image still requires a bit of a mental leap. We want to help bridge that gap for our community.
The CNB project does maintain a Tekton Template that can be used as an example, but not everyone is running Tekton. It’s also difficult to translate that pattern to other CI/CD systems. We need to offer more guidance in this area.
One of the goals of our 2020 roadmap is to go beyond Pack. We want to begin supporting templates and tools that will help our community use buildpacks in CI/CD pipelines of all kinds. In the early phases we’re targeting Jenkins and CircleCI, but we want to work with you to support your platform of choice.
We also want to make it easier for buildpack authors to create and publish their own buildpacks, and for buildpack users to find them. We’re working on a proposal for a Buildpack Registry that would allow the community to share and search for buildpacks no matter what platform they use.
The number one question at KubeCon was about when buildpacks will be ready for use in production. It’s not a simple answer though — it depends on how you want to use them, what features are important to you, and what “production ready” means to you.
Buildpacks are used in a number of production platforms today including kpack, Salesforce Evergreen, and Google Cloud Run. But even those examples don’t answer the question for everyone.
One of our goals for the year 2020 is to release a 1.0 version of Cloud Native Buildpacks. A prerequisite for this release is a level of stability in the API and interfaces that ensure breaking changes are uncommon in the future. We also need to implement some features that we consider table stakes in the cloud native ecosystem.
Beyond the bits and commits, we’re also getting ready to apply for Incubation status in the CNCF. All CNCF projects have a maturity level of sandbox, incubating, or graduated; which corresponds to the Innovators, Early Adopters, and Early Majority tiers of the Crossing the Chasm framework.
In 2019 our first set of production users, the early adopters, signaled that buildpacks are ready for more people to try. Now we’re looking forward to hearing from all of our new users as the project comes of age.
To learn more about the 2020 Cloud Native Buildpacks roadmap, see our Community Github repository.
Buildpacks are pluggable, modular tools that translate…
45 
1
45 claps
45 
1
Buildpacks are pluggable, modular tools that translate source code into OCI images.
Written by
I’m an architect at Salesforce.com who writes about software and related topics. I’m a co-founder of buildpacks.io and the author of The Healthy Programmer.
Buildpacks are pluggable, modular tools that translate source code into OCI images.
"
https://medium.com/buildpacks/cloud-native-buildpacks-hit-beta-4d9f2c85dd22?source=search_post---------68,"There are currently no responses for this story.
Be the first to respond.
The buildpacks.io project is excited to announce its first beta release. We’ve spent the last several months working on developer experience, and now we’re ready for the public to try buildpacks and provide feedback.
This milestone includes a beta release of the pack CLI. Application developers can use pack locally to run buildpacks that turn their code into executable Docker images, while buildpack authors can use it to test buildpacks and prepare them for production use.
To get started, download the latest GitHub release of pack, or run the following commands if you’re using MacOS with Homebrew:
Then use the pack build command on your favorite app or our sample Java app. You can learn more about how to use this command in the pack documentation.
This release highlights some significant improvements to the developer experience of pack and Cloud Native Buildpacks in general. We’ve collected feedback from early adopters in the community and worked carefully to address those concerns.
If you tried pack before this release, the first thing you’ll notice is an improved user interface. The CLI commands, arguments, and output have been adjusted to provide only the most meaningful output and we’ve elided some of the confusing or extraneous elements that were meant for debugging.
We’ve also improved some of the core concepts like stack. A user who is only running pack to execute buildpacks against their source code no longer needs to add, update, and remove stacks. Instead, the builder images contain metadata that lets pack manage this for you.
As with previous versions, pack is an interface to the buildpack lifecycle, which is the core execution environment for buildpacks. It can be used by platform developers who need to execute Cloud Native Buildpacks on a server.
Of course, we know there’s always room for improvement, which is why we need you to try the CLI today and give us your feedback. We can’t get the developer experience right without developers.
Buildpacks have always been about the developer experience. We want buildpacks to make your job easier by eliminating operational and platform concerns that you might otherwise need to consider when using containers. This release is a great step in that direction.
To continue following the progress of this project or to provide feedback on your experience with the pack CLI, join us on Slack or drop us a note on the Cloud Native Buildpacks mailing list.
Buildpacks are pluggable, modular tools that translate…
171 
171 claps
171 
Buildpacks are pluggable, modular tools that translate source code into OCI images.
Written by
I’m an architect at Salesforce.com who writes about software and related topics. I’m a co-founder of buildpacks.io and the author of The Healthy Programmer.
Buildpacks are pluggable, modular tools that translate source code into OCI images.
"
https://towardsdatascience.com/monitoring-and-observability-in-cloud-native-architectures-54e68e52b103?source=search_post---------69,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dimitris Poulopoulos
Aug 18, 2021·5 min read
Imagine being imprisoned in a windowless cell with no calendar or wall clock. Moreover, you get to have no visitors, and your only company is a guard who does not like small talk. I know this is a very dark scene I am putting…
"
https://medium.com/planet-stories/cng-part-6-metadata-in-a-cloud-native-geospatial-world-2fa83cc00c95?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
As geospatial workflows move fully to the cloud there is large potential to improve on some of the main annoyances in the workflows. One of those is metadata — filling out metadata fields is the bane of many GIS and Remote Sensing practitioners, but having reliable metadata about where information comes from is hugely important. To trust an analysis or accurately read a map, one must understand where the contributing data came from and how accurate it is. Good metadata also makes searching and finding geospatial information not only easier but possible. The current geospatial search paradigms unfortunately often rely on the dataset creators taking additional time to fill out forms with many fields, and often that step doesn’t happen. But modern search technology points to a better way — track and link everything and then use automatically created information as metadata.
Google uses the PageRank algorithm-leveraged links between pages as metadata to rank content. It worked far better than relying on the html meta tags that relied on web page owners filling out metadata. Similarly, Github does not require projects to fill out lots of metadata about the code in them, but all kinds of information about various projects is easily findable. That is because all activity that is done on the code is fully tracked, automatically. And user metrics on following and forking repositories is also used to rank search results.
As the move to cloud-first workflows unfolds in the geospatial world, we have a big opportunity to automatically track more information that should alleviate the requirements of user created metadata. And tracking more information will also open up new possibilities for gaining more insight from geospatial data. The key piece missing from many traditional geospatial workflows is tracking ‘provenance’ — what is the source data that imagery or vector data is derived from, and what processes were applied to it? Some geospatial software tracks this for its own ecosystem, but the core formats leave this as an optional piece that is in practice rarely used.
In addition to the process applied, a cloud native geospatial system can also track exactly which user did what analysis, linking to their online profile to extract even more of the traditional geospatial metadata. Other types of traditional metadata can also be inferred from additional pieces of information that are automatically stored in the system, like the geography and the time. Deriving this type of information does not require a cloud native architecture, but having all the data and derived data in one location certainly makes it much easier.
Many of the emerging cloud-based geospatial systems are actively building such tracking and metadata into their systems, since it is easier with all data in one location and users are interested. The real value will be to standardize the provenance tracking and metadata across different cloud-based geospatial systems. This does not need to occur immediately, and indeed it would likely slow down innovation to try to coordinate all the various systems into standard metadata. But all systems should hopefully track the provenance and processing of everything done in their environment and the common patterns can be standardized in the future.
The final piece that will help make all of this tracking even more valuable is to also catalogue usage, leveraging that as a key piece to enable ‘search’ of geospatial information. Currently, users still spend a majority of their time finding geospatial information, instead of using it. Automatically creating metadata will ensure that all data can be found, which is not the case today. Layering both the provenance and usage data on top will enable much more intelligent search, by enabling ranking by popularity. Provenance ensures that users understand that the source data is very popular, even if it’s several derived-data products that are used more than the source.
Cloud-centric architectures are not guaranteed to be any less siloed than the desktop and server computing architectures that came before. The three main principles of Cloud Native Geospatial articulated in the previous post nudge us toward a more interoperable direction, with a baseline of being able to view data as web tiles from different sources. But the ‘provenance’ piece described here will be the most important to move from silos to an interconnected ecosystem. Good analysis today is done from multiple data sources, with higher and higher levels of processing and information extraction. Data will be accessed from multiple systems, with analysis done on multiple platforms. A baseline of interoperable provenance tracking will help ensure that we don’t end up with monolithic platforms that don’t work together.
Beyond merely preventing silos, properly tracking provenance and usage in open ways and leveraging standards will enable geospatial information to be far more accessible to the wider world. The geospatial ecosystem today is relatively closed, depending on knowing who has the right data, with trust based on individual relationships. Tracking all the core artifacts will enable the geospatial equivalent of the Google PageRank algorithm, and even more interesting search and discovery operations.
While there have been a couple small efforts on tracking provenance of geospatial information, I believe we need more innovation and real-world implementations. And these should be done from a cloud-first perspective, which makes some aspects easier, and certainly changes some variables. Planet is just starting to work on exposing our internal compute engines for customer use, so this will be something we’re looking at seriously, and are excited to collaborate with others on open standards to at least track the processing histories of imagery and its derived data products.
Using space imagery to tell stories about our changing…
36 
36 claps
36 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/swlh/a-cloud-native-coda-why-you-probably-dont-need-elastic-scaling-46b9315df635?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
Kyle Brown and Kim Clark
One of the most common features of Cloud Native development that we constantly hear touted as being of supreme importance is elastic scaling. Many companies have told us that they see taking advantage of Elastic Scaling as being a key requirement for their teams evaluating cloud platforms. However, we rarely hear those same teams tell us why they need elastic scaling.
In fact, we might go so far as saying Elastic scaling is one of the hallmarks of “being on the cloud”. All cloud platforms that bear up under the name have some sort of elastic scaling support. Whether that be Horizontal Pod Autoscalers in Kubernetes (which will automatically scale the number of Pods based on observed CPU utilization) or vendor features such as AWS Autoscaling, which will automatically scale EC2 instances, Dynamo DB tables and many other resource types, elastic scaling is often viewed as being the single most desirable feature of the cloud.
If you are a brand new startup building a new B2C application, then elastic scaling might be critically important because you can’t predict when your business will suddenly take off. But Enterprises are not startups. They have existing customer bases, and their usage patterns are, for the most part, well-known. Instead, in most Enterprises we see a split of all the different workloads that the company as a whole run divide into approximately the following categories:
We show this split (taken from a real customer, but shown here for illustrative purposes only) below:
The problem is that many teams want to build their applications today as if they would be part of the 5%, when in fact only very few applications fall within that subset that will ever be hit by unplanned load. Now, this is not a new problem. In more traditional application environments, the common approach was always to put far too much infrastructure in place “just in case”. That was obviously wasteful, and one of the reasons why teams wanted to move to the cloud. However, the utopian cloud native approach would be to assume you must deploy every function such that it can scale infinitely. However, for the types of big enterprises that we work with, the better approach is instead to attempt to identify those 5% of functions, separate them from the monolith, and build them in a cloud native way (they can be good early candidates for the strangler pattern, for example).
So as a result, assuming that all your Cloud native programs must be elastic and infinitely scalable is usually misleading — instead, for 95% of enterprise applications, what is much more important is resilience rather than elastic scaling. What we need instead is horizontal stability, which is a necessary condition to achieve Horizontal Scaling if, in fact, you’re in that 5% of programs that actually need that.
So in other words, your program should continue to run in a stable way, without service interruption, if a node or instance is lost, replaced or restarted — which is much more common than suddenly needing extra nodes for scaling. Now it turns out that key cloud native ingredients such as loose coupling, not using shared databases, and inter-process communication only through standard, scalable protocols like HTTP and messaging systems is a good way to achieve this. So writing applications in a cloud native way is still the right thing to do, but perhaps not for the reasons you might think.
What’s more, elastic scaling turns out to be an anti-pattern for many enterprises. For instance, when we spoke to one bank about elastic software licensing models, we were told that because of the way they did their application cost planning, they actually needed up-front fixed software costs for their business cases for new projects. This is, unfortunately, true in far too many enterprises we work with.
One of the recurring horror stories we often hear (so often as to wish it were apocryphal, but unfortunately we have witnessed multiple instances of this) is that a team deploys a new cloud native application with autoscaling turned on, only to result in an enormous first-month cloud provider bill due to autoscaling, not because of high customer use, but because of an unexpected error not encountered in testing that resulted in anomalously high CPU utilization when under normal loads. Until costing models and financial planning catches up with the technology, you may be best off building applications in a cloud native way, but deploying either in fixed capacities, or with capacity-limited autoscaling setups.
Get smarter at building your thing. Join The Startup’s +750K followers.
56 
1
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
56 claps
56 
1
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.heptio.com/cloud-native-part-3-6f9d888c5f07?source=search_post---------72,"This is the third part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
It is probably most useful to think of DevOps as a cultural shift whereby developers must care about how their applications are run in a production environment. In addition, the operations folks are aware and empowered to know how the application works so that they can actively play a part in making the application more reliable. Building an understanding and empathy between these teams is key.
But this can go further. If we reexamine the way that applications are built and how the operations team is structured, we can improve and deepen this relationship.
Google does not employ traditional operations teams. Instead, Google defines a new type of engineer called the “Site Reliability Engineer”. These are highly trained engineers (that are compensated at the same level as other engineers) that not only carry a pager but are expected and empowered to play a critical role in pushing applications to be ever more reliable through automation.
What defines an SRE is what happens at 10am the next morning.
When the pager goes off at 2am, anyone answering that page does the exact same thing — try to figure out what is going on so that he/she can go back to bed. What defines an SRE is what happens at 10am the next morning. Do the operations people just complain or do they work with the development team to ensure that a page like that will never happen again? The SRE and development teams have incentives aligned around making the product as reliable as possible. That, combined with blameless post-mortems, can lead to healthy projects that don’t collect technical debt.
SREs are some of the most highly valued people at Google. In fact, often times products launch without SREs with the expectation that the development team will run their product in production. The process of bringing on SREs often involves the development team proving to the SRE team that the product is ready. It is expected that the development team will have done all of the leg work including setting up monitoring and alerting, alert play books and release processes. The dev team should be able to show that pages are at a minimum and that most problems have been automated away.
As the role of the operations becomes much more involved and application specific, it doesn’t make as much sense for a single team to own the entire operations stack. This leads to the idea of Operations Specialization. In some ways this is a type of “anti-devops”. Let’s take it from the bottom up.
There is probably room for other specialized SRE teams depending on the needs of the organization. For instance, storage services may be broken out as a separate service with dedicated SREs. Or there may be a team responsible for building and validating the base container image that all teams should use as a matter of policy.
In the next part of this series we will look at how Cloud Native relates to Containers and Container Clusters.
Heptio
82 
Thanks to Craig McLuckie. 
82 claps
82 
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://articles.microservices.com/developer-workflow-trail-for-cloud-native-applications-request-for-feedback-a9365b64c790?source=search_post---------73,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
As part of my work with Datawire, I’ve been working with the team around the idea of defining an effective developer workflow when building “cloud native” applications. I shared our initial discussions in a TheNewStack article “Kubernetes and PaaS: The Force of Developer Experience and Workflow”, and this article is a request for feedback on our attempts to expand on this.
For the majority of organisations (at least those looking to embrace cloud native principles) the keys to competitive advantage are speed and stability. These aren’t just my thoughts, I’m standing on the shoulders of giants such as Adrian Cockcroft and Steve Smith. Just this morning I opened up Twitter and saw a great tweet from Fintan Ryan of Redmonk, echoing the same concepts from his recent experiences at the Cloud Foundry Summit.
Fintan mentions about “developer velocity”, which is a key part of the developer experience, and runs the gamut from developers being aligned with business goals, to having appropriate tooling and processes from build to operations.
The Cloud Native Computing Foundation’s (CNCF) Cloud Native Trail Map is a good attempt to describe the steps an engineering team may take when exploring cloud native technologies, although it is rather infrastructure (and hence, operations) focused.
I’ve tried to capture some of my learnings of the steps an organisation typically takes when embracing cloud native principles like microservices, public cloud and containers, and have placed this in a publicly accessible Google doc (with comments open) titled “Developer Workflow Trail for Cloud Native Applications”:
Please do let me know what you think in the Google doc! I plan to make this look at little nicer in the future, but at the moment it would be good to here whether you think this makes sense, or whether your experiences have been vastly different!
Technical posts about adopting microservices architectures
36 
36 claps
36 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Technical posts about adopting microservices architectures
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Technical posts about adopting microservices architectures
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/the-evolution-of-cloud-native-products-a08ce3fd0132?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
Please welcome Iron.io CEO Dylan Stamat to Hacker Noon! Iron.io is a cloud application services provider that eliminates the need to worry about infrastructure, and a marketplace partner of our past weekly sponsor, Manifold.
Manifold is the marketplace for independent developer services. You can learn more about their partners by checking out this interview with JawsDB Founder John Willman and this interview with LogDNA Founder Chris Nguyen. For Hacker Noon readers, Manifold is offering $10 off any service with code HACKERNOON2018.
David: Let’s talk numbers, what is the scale of your business? And what metrics do you use to gauge company progress?
Dylan: Our metric for company progress is the growth of our customers that are actively using our products. When you provide cloud services, you’re operating in a landscape that didn’t exist 10 years ago. And in cloud specifically, you’ve got one major player that’s dominant and unafraid to eat its ecosystem. So what we’ve been focusing on is delivering long lasting value to those folks where our particular offer is a great enduring fit.
How has the technical infrastructure to rapid scaling evolved over the last 5 years?
Like many startups, our initial prototypes were in self-contained code we architected with centralized services and bottlenecks. As we’ve handled scale of 100MM requests/second bursts, we’ve had to remove those bottlenecks. You’ll see in our codebase similar architectural choices to what’s happening in most other distributed systems: leader election that moved from paxos to raft, a reduction of dependencies on underlying services that just don’t scale very well (sorry Mongo), etc. On a language level, we’ve moved from lightweight scripting in our earliest prototypes to Golang. Our latest products are being written and profiled in Rust and proving to be strong in their operational guarantees and performance.
Your flagship products are IronWorker and IronMQ. Could you break down how these products fit together, and usage numbers for each?
It’s funny, in the race to taking a cloud computing lens on every problem, it’s batch processing, a 50 year old challenge that we’re great at solving.
This shows itself in many, many diverse business opportunities: ETL, at-scale messaging, data consumption and aggregation. For instance, we being used for ad-hoc data transition services by one of the country’s largest telecom companies; and simultaneously we batch process IoT data from an extremely successful bar automation service. What all of these folks have in common is a need to massively scale out a worker pool to a varying influx of tasks. We started IronWorker around providing the best ad-hoc worker experience (and we still do!) but found that the queues available to run it (SQS/Rabbit/etc didn’t cut it) didn’t meet our needs, so we built IronMQ from scratch.
We want our products to be stable and scalable, but also extremely easy to use. If you want to use IronMQ for example, you don’t need to set up security groups, roles, network configuration endpoints, etc. You grab a client library (or use our API’s) in whatever language you’d like and start using it. There are some nice synergies between products as well. For example, IronMQ has a type of queue we call “Push Queues” which can fire off events when messages are received. One type of event is a direct hook into IronWorker, which means you can fire off workers based on message payloads. This cuts down a lot of the code you’d need to write to coordinate things. Another benefit is that we offer on-premise deployments… you’re not tied to a public cloud. You’re able to future-proof your choice in a way by minimizing lock-in. Just as an example, the operational expense of moving IronMQ to a different platform would be much, much lower than that of migrating your SQS implementation to Cloud Pub/Sub. No actual application code changes needed.
In May 2017, Xenon Ventures acquired Iron.io. Over the last year, how has the company evolved post acquisition? And does Iron.io fit into Xenon Ventures bigger vision?
Xenon was one of the first investors in Iron and our team founded an early cloud management platform called RightScale. So, we have an appetite for cloud services and understood the team and technology. What Xenon is able to do is treat cloud services as independent businesses that do not need to find a future acquirer to be cash flow positive and growing. It’s a bit weird to say this since you might think that every company wants to be able to survive without needing to raise more money or get acquired, but that’s what Xenon’s vision is for Iron. To be self-sustaining and under no pressures except to win customers and keep their business. Yes, we see lots of trends and cycles in cloud infrastructure, but we’re not so big that we need to be part of any bright and shiny framework in order to thrive. If you spoke to me two years ago you might wonder if we were going to support Meteor, now it’s probably whether we’re going to support a GraphQL interface :) The reality is we’re attacking a constant, plaguing business problem and it will appear in whatever greater movements are happening in the cloud.
I see you’re based in Las Vegas. What is the Las Vegas tech scene like? The Downtown Project has been something I’ve been following from afar. How does the tech community fit into to everything else that is happening in Vegas?
Vegas is awesome and downtown is amazing. You’ll often find me at The Laundry Room, Atomic Liquors, or Palomino.
Vegas is the antithesis to the bubbly feel of San Francisco. It’s an hour away, incredibly easy on the budget, and has a chance to become a second hub for Silicon Valley companies.
We hold meetups in San Francisco and, well, sometimes folks here might be a bit opinionated about the quality of the free pizza or even the free non-craft beer. It’s like Richie Rich took over the tech scene. Ironically, in the glitz and glamor of Vegas it’s the opposite. Hard working teams really standing for something by going against the grain. The focus is not on getting that $1000 Boosted Board or trying to rent in the city, but instead making real business impact and focusing on growth and customers. Yes, there’s downsides, but if you asked me where a startup with $1MM would get the most value, I’d say Vegas. Our building has a coworking space in it and you’d be amazed the companies that have a presence there.
Why did you decide to partner with Manifold?
We’ve actually had some past success in other marketplaces, so we started an initiative to seek out other marketplaces we may want to move into. When we ran across Manifold I was immediately interested. I really, really, like the fact that it’s platform agnostic. Like many engineers, I’m not a fan of coding myself *into* an individual platform. The integration was crazy easy and we were integrated, tested, and on the platform way faster than we originally planned for. Another benefit of Manifold is that we weren’t just dumped into the marketplace and left to wither. Their team is extremely customer centric, they have marketing strategies, and they know technology. It was definitely a perfect storm type of situation for us and they are an amazing team to work with.
#BlackLivesMatter
411 
411 claps
411 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://faun.pub/salary-trends-for-devops-and-cloud-native-professionals-6a120ae970d1?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/radiant-earth-insights/cloud-native-geospatial-sprint-awards-bounties-4f929727aa9c?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
It’s been two months since we wrapped up the Cloud Native Geospatial Outreach Day and Sprint. I meant to get this post out much sooner, but life happens and 2020 is a tough year. But I wanted to announce all the winners of our experiment with ‘Community Awards’, as a companion to the Outreach Day Recap.
I know I’m a broken record thanking our sponsors, but I think it’s appropriate to do so for this section, as it’s entirely possible due to their support of our communities.
The idea with the awards was to encourage and recognize key contributions to the Cloud Native Geospatial community, using the sponsorship money that would have normally gone to travel, venue, food, etc. We had a list of potential awards, but we didn’t have full participation in all our anticipated categories, so for a few of them, we’ve decided to turn them into ‘Bounties’, where we will give out the awards at the next sprint, for contributions between now and then. So without further ado, the winners!
Community Prize: stacindex.org, from Matthias Mohr, was selected by the community of contributors to win the $5000 ecosystem prize.
It’s a project that clearly demonstrates the future for STAC, pulling together all that has been done in the community into a much more accessible interface. It lets anyone with a SpatioTemporal Asset Catalog list theirs for everyone to see, and automatically creates a STAC Browser so the data is crawlable. And then it also lists all the tools that are available to work with STAC. I’ve already found it to be super useful, and I think it’s got a bright future ahead. It was the clear winner in the community voting, so I think it’s safe to say that everyone is as excited by it as I am.
Convening Sponsor Prize: The other $5000 prize is selected by the two Convening Sponsors — Microsoft & Planet. We decided to split this one up, with $2000 going to Vincent Sarago and Jeff Albrecht, for all they are doing with COG’s and STAC, and in particular, the promising work with Titiler and its support of Numpy Tiles, powering cogeo.xyz. The rest of the prize will be reserved to help STAC Browser, set aside as bounties to incentivize some key features, like upgrading its map to read numpy tiles and point clouds (likely using deck.gl).
The next set of prizes were made to be an alternative to our practice of sponsoring travel for new people to join our in-person sprints. These grants give winners $1000 to help them spend time in the community, and also reserve travel money for our next in-person sprint. We decided to give out two of these, and the other two will be awarded at the next sprint.
Aimee Barciauskas won one of the newcomer grants. She supported the NASA Space Apps COVID Challenge by putting up a number of great datasets as Cloud Optimized GeoTIFF’s on the AWS Registry of Open Data, and gave a great lightning talk on the work. She also gave a nice intro session on a new public Sea Surface Temperature dataset that is stored in Zarr, with lots of great detail on what Zarr is all about. SparkGeo and Planet sponsored this award.
Kyle Barron won the other newcomer award, for his work on deck.gl-raster which brings raster data from Cloud Optimized GeoTIFF’s to the great deck.gl library. You can see it in action in landsat8.earth, and I’m sure it’ll start to pop up in other places in the future.
The other two awards will be given out to the next newcomers to the community. Arturo is sponsoring one to recognize the most promising woman newcomer, and Digital Earth Africa is sponsoring one to recognize the best newcomer contributions from someone living in Africa.
The final set of awards are to recognize various contributions to the Cloud Native Geospatial community. Winners got $200, and some of them also received a Planet-tasked 50cm SkySat image.
We’ve still not sorted out exactly how the bounties will work, but I’ll aim to write up the details and post the information when I get a chance. As mentioned above we have two more ‘newcomer grants’ to give out. And then there were a number of community awards we will look to give out in the future. These include best contribution by a Canadian (sponsored by SparkGeo), best contribution or use of Sentinel 2 Africa STAC/COG data, best data contributions on both AWS and Azure, and best contribution with zarr.
We are also recognizing two groups of people with a STAC Hoodies. The first is everyone who made a contribution during the sprint. And the second is everyone who gave an ‘intro session’ at the CNG outreach day. And then everyone who gave a lightning talk will get a STAC T-shirt. I’ll be in touch soon to get people’s information to send them their hoodies.
Thanks again to everyone who participated and attended. Everyone I talked to really enjoyed it, and we’ll do our best to organize another one soon. And hopefully we’ll be at 1.0.0 for STAC, with an even richer ecosystem of Cloud Native Geospatial tools!
Earth Imagery for Impact
136 
136 claps
136 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/you-love-cloud-native-tech-but-do-you-know-what-it-advocates-8d0ea9f59727?source=search_post---------77,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dimitris Poulopoulos
May 27, 2021·5 min read
If you are a web or software developer, a DevOps engineer, or a data scientist, you have surely heard the term cloud-native. Sometimes, when we stumble upon a term all the time, we somehow develop a mental idea of what it…
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/cloud-native-ci-cd-with-tekton-laying-the-foundation-a377a1b59ac0?source=search_post---------78,NA
https://medium.datadriveninvestor.com/cloud-native-from-docker-to-kubernetes-and-to-service-meshes-a6105fdf23a?source=search_post---------79,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
By Lv Renqi.
Back last year around when I was doing some research into Service Meshes, I read all about Kubernetes, mostly looking through some presentations online. And around the same time, I also deployed and played around a bit with OpenShift, which is a Kubernetes application platform. Afterwards, I wrote two articles in which I discussed my experiences and thoughts about Service Mesh and Kubernetes.
Looking back on those previous articles now, though, I think that my understanding back then was a bit limited and one sided, as with most information you’ll find online about these topics. Now that my team are in the middle of considering using Dubbo for a cloud-native architecture, I spent some time brushing up on the basics reading some practical guides to Docker and Kubernetes in the hope of providing a better and more comprehensive understanding of all the relevant technology.
Now, to begin discussion, I’d like to consider what exactly Docker, Kubernetes, and Service Mesh are and what do they do to try to make better sense of what cloud native exactly is and means. Last in the blog, we’ll also tackle the concept and meaning of cloud native and take a stab at what exactly “cloud nativeness” is.
In this section, I’m going to present my thoughts after reading the practical guide to Docker.
Docker is a Linux container toolset that is designed for building, shipping, and running distributed applications. Docker helps with the process of containerization, which ensures application isolation and reusability, and also virtualization, which is a functional way to ensure the security of physical machines.
www.datadriveninvestor.com
In many ways, Docker was one of the first mainstream pieces of what would become the cloud native puzzle.
Below is a graphic that shows how Docker can be used to streamline things:
The points below sum up what exactly Docker is and what it does.
Docker is also well integrated with all the stages of the application lifecycle:
Docker is responsible for the following:
As I hinted at before, Kubernetes came after containerized applications and Docker became popular and were started to be used as a platform to manage containers. In this section, like the last, I’m going to present my thoughts on reflection after reading a practical guide to Kubernetes.
The popularity of Kubernetes is closely connected with Docker. The wide adaptation of Docker made Platform-as-a-Service (PaaS) a viable thing. Kubernetes is a great achievement that came after much practical experience in managing massive data centers at Google. It was also largely heralded in by the explosion of containerized applications that occurred at the time. Google’s goal was to establish a new industry standard, and they undoubtedly did-with Kubernetes being as wildly popular as it has come to be.
Kubernetes is really a big piece of the cloud native revolution.
Google started using containers in 2004 and released Control groups (typically referred to as cgroups) in 2006. At the same time, they were using cluster management platforms such as Borg and Omega internally in their cloud and IT infrastructure. Kubernetes was inspired by Borg and draws on the experience and lessons of container managers, including Omega.
You can read more about this fascinating bit of history on this blog.
Some things to consider moving forward is how did Kubernetes beat other early comers like Compose, Docker Swarm, or even Mesos? The answer to this question, in short, is Kubernetes’s superior abstraction model. Kubernetes in principle is different at the core of its design. To understand Kubernetes, we’ll need to go over all the concepts and terms that are involved in its underlying architecture.
Below are some of the concepts that are more or less unique to Kubernetes. From understanding the concepts behind Kubernetes, you can gain a better general understanding of how Kubernetes works:
If you’d like to learn more, you can get it from the source itself, read the official documentation directly, which you can find here.
Continuous integration and deployment is arguably the most distinctive features of Platform-as-a-Service (PaaS) models and cloud native as well, and most cloud vendors provide PaaS models that are based on Kubernetes. Therefore, general user experience and the specific Continuous integration and continuous delivery (CI/CD) pipelines used in these models are the things that distinguish different services. Generally speaking, CI/CD pipelines start when the project is created and permeate every stage of the application lifecycle. This is the core advantage of CI/CD pipelines. They can be integrated into all steps of a development workflow and provide an all-in-one application service experience.
Below are some mainstream CI/CD tools that can be used with Kubernetes:
Now, the last important thing to consider when it comes to Kubernetes are the API objects and metadata connected with Kubernetes:
In reality, all that metadata does really is it defines the basic information of an API object. It is represented by the metadata fields and has the following attributes:
In this section, I’m going to quickly look into Service Mesh, which is really the next big thing after Kubernetes and Docker. To me, the core strength of Service Mesh is its control capabilities, and that’s one place where the service mesh Istio in particular especially shines.
In fact, I’ll go as far to say that, if the Istio model could be continued to be standardized and expanded, it could easily become the de facto PaaS product for containerized applications. Though, I think that the service meshes of Apache’s Dubbo, Envoy, NGINX, and even Conduit are also viable integration choices.
Since I think that Istio really is a stellar option, let’s focus on it first.
To understand Service Meshes, you really need to understand the design principle of a service mesh. Let’s look at what’s the design principle behind Istio. In a nutshell, a service abstraction model comes first before implementation on a container scheduling platform. Consider the figure below.
If you’re interested in learning all the specifics about what exactly Istio is, you can check out its official explanation here.
Generally speaking, the Istio service model involves an abstract model of services and their instances. Istio is independent of the underlying platform, having platform specific adapters populating the model object with various fields from the metadata found in the platform. Within this, service is a unit of an application with a unique name that other services refer to, and service instances are pods, virtual machines and containers that implement the service. There can be multiple versions of a service.
Next there’s the services model. Each service has a fully qualified domain name (or FQDN) and one or more ports where the service is listening for connections. A service can have a single load balancer and virtual IP address associated with it. Also, involved with the service model are instances. Each service has one or more instances, which serve as actual manifestations of the service. An instance represents an entity such as a pod, with each one having a network endpoint.
Also involved in the design of Istio is service versions with each version of a service being differentiated by a unique set of labels associated with the particular version. Another concept is Labels, which are simple key value pairs assigned to the instances of a particular service version. All instances of a same version must have the same tag. Istio expects that the underlying platform provides a service registry and service discovery mechanism.
With the above discussion about Docker, Kubernetes, and Service Meshes like Istio, I left out one major thing to cover and that’s “cloud native”. So, what is “cloud native”? There are different interpretations of what it means or what it takes to be “cloud native”, but according to the Cloud Native Computing Foundation (CNCF), cloud native can be understood as follows:
Cloud-native technologies and solutions can be defined as technologies that allow you to build and run scalable applications in the cloud. Cloud native came with the development of container technology, like what we saw with Docker and then Kubernetes, and generally involves the characteristics of being stateless, having continuous delivery, and also having micro services.
Cloud-native technologies and solutions can ensure good fault tolerance and easy management and when combined with a robust system of automation thanks to CI/CD pipelines, among other things, they can provide a strong impact without too much hassle.
Nowadays, the concept of cloud native and “cloud nativeness” even according to the CNCF involving using Kubernetes, but Kubernetes is not the only piece of the puzzle but rather is only the beginning. With traditional microservice solutions Dubbo now being a part of the CNCF landscape, more and more people are attracted to cloud native due to the unique features and capabilities that these solutions have to offer.
In other words, in many ways, in today’s landscape of cloud native, cloud native may start with Kubernetes but end up going on to embrace Service Mesh solutions like Istio. It’s just like the progression we saw throughout this blog, first there was Docker, and then Kubernetes, and now there’s also Service Meshes.
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
193 
193 claps
193 
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://igokuz.com/%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B9%83%E0%B8%88%E0%B9%80%E0%B8%81%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%A7%E0%B8%81%E0%B8%B1%E0%B8%9A-cloud-native-app-d9acc332fb07?source=search_post---------80,NA
https://medium.com/radiant-earth-insights/join-the-cloud-native-geospatial-outreach-day-and-sprint-286f6fd553c3?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
On September 8th we will be continuing SpatioTemporal Asset Catalog (STAC) Sprint #6, but we decided to expand its scope to include more of the ‘Cloud Native Geospatial’ ecosystem. The core idea of Cloud Native Geospatial is articulated in this ‘blog series’, with the first post positing the question: ‘what would the geospatial world look like if we built everything from the ground up on the cloud?’. The second post introduced the Cloud Optimized GeoTIFF (COG), and it has since emerged as a default format for anyone doing geospatial on the cloud.
So the ‘Outreach Day’ will aim to introduce STAC, COG, and other emerging cloud-native geospatial formats and tools to new audiences. Our hope is to make the day accessible even to those who do not have deep geospatial or software development knowledge. We want to welcome new people into our community, as we believe that having all geospatial data in the world on the cloud, with tools that can process it, can help us tackle the largest problems that face us. But the technology won’t have a real impact on our planet unless we enable a diversity of users to learn it, contribute to it, and use it in their work.
The full agenda will be announced soon, but our plan is to have a number of ‘Introductory Sessions’ that run in various timezones through the whole day of September 8th. These will enable people who want to learn more about Cloud Native Geospatial topics and tools to learn and ask questions in smaller groups. We’ll even have some sessions that introduce the basics of geospatial and remote sensing, so please join us even if everything is new! We’ll have many people who can welcome you and answer your questions. We will also try to organize a ‘virtual job fair’ of some sort since there are many organizations looking to hire people to work in Cloud Native Geospatial.
Starting at 13:30 UTC (in the ‘middle’ of the day globally) We’ll do a couple of hours of ‘core’ sessions that everyone should try to join. And we will aim to record and post these as soon as possible for those who can’t join live. This will include a ‘State of Cloud Native Geospatial’ talk, with brief overviews of STAC & COG, along with a variety of ‘lightning talks’ from various community members. We’ll hear about companies large and small embracing Cloud Native Geospatial, dive into various datasets that are becoming available in the formats, and survey the ecosystem of tools that supports working with them.
After the core sessions, we’ll continue the beginner sessions. If you are interested in presenting a lightning talk or a beginner session please submit your ideas. After the talks, we’ll also hold the kick-off for the ‘Data Sprint’. This sprint runs for a week, and the goal is for everyone to team up and convert as much interesting data as possible to STAC and COG formats, and to do projects that show off the data. There will be a wide variety of tasks that people can do to help out — no matter what skills you come with we’ll be able to find a way for you to help out! And our hope is that people can take what they learn in the beginner sessions and actually apply it during the sprint, since the best way to learn is by actually doing.
We’re also going to run a Data Labeling contest as part of the sprint. This should be a great task for those who are new to geospatial, we’ll use Azavea’s Groundwork to provide a nice user interface to trace geospatial imagery, automatically creating geospatial labeled data for Machine Learning applications. The results of everyone’s work during the sprint will be a new STAC + COG dataset that we release to the world on Radiant Earth’s MLHub.
We’ve had really amazing sponsorship, with Planet and AI for Earth at Microsoft leading the charge, and Arturo also providing substantial support. Digital Earth Africa and the World Bank are first time sponsors and are helping push us to be globally inclusive in our outreach. Maxar, SparkGeo, Element84, Amazon Web Services, Pixel8, CosmiQ Works, TileDB, and Azavea round out our great sponsors. Normally we’d use the sponsorship money to fund travel grants to welcome new people to our community. But with a purely virtual event our plan is to give out a variety of prizes to recognize the work done and help encourage new contributions. We will post the list of prizes soon, but there will be several that are available to groups less represented in STAC+COG today.
There will be some nice prizes, including an openly licensed Planet SkySat SkySat 50cm image tasked to the location of your choosing, and $15,000 in Azure credits for top projects focused on sustainability. Anyone who makes a meaningful contribution during the sprint will get a t-shirt (with some limit if we get an unexpected amount of contribution), and there will be at least one top prize of $2500 or above.
If you are interested in attending please sign up at on this Google Form and you’ll receive email updates on all the details on the event. Thanks!
UPDATE: More information on how we are working to welcome new people and prize specifics is available here.
Earth Imagery for Impact
73 
2
73 claps
73 
2
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/radiant-earth-insights/cloud-native-geospatial-outreach-day-recap-9b873ce3788e?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
It’s been just over three weeks since the Cloud Native Geospatial Outreach Day. Everyone I’ve talked to felt it was an incredible event, and I definitely concur. Thankfully we managed to record almost all of it, so if you missed it you can still catch the content on youtube!
I wanted to give a recap of the event and share some of my favorite parts.
We opened with a welcome from Bruno Sánchez-Andrade Nuño and me, representing the Microsoft and Planet, the convening sponsors. Then Hamed, the new Executive Director of Radiant Earth, introduced the Data Labeling Contest (which was a great success). I then attempted to give an overview of ‘Cloud Native Geospatial’, to help explain it to new people and to chart our progress. It’s always fun to pull these together, as each time I get to survey all the new things happening, and there seems to always be great new stuff. This time I also tried to explain more of the ‘why’. I don’t think I made the strongest argument, but I hope to try again and go deeper in a blog post.
The two sessions of lightning talks truly demonstrated how the Cloud Native Geospatial movement is crystalizing in a big way. When I put out the call for lightning talks I knew there was some cool content out there, but I was blown away by the diversity and sheer awesomeness that came in. If you are to watch any of the recorded videos from outreach day then Lightning Talks Round 1 and Round 2 are definitely the place to start.
We heard how a number of different organization and datasets are embracing STAC & COG, from commercial satellite providers like Maxar and Planet to startups like Arturo, SparkGeo, and Astraea, to the huge public data catalogs of ESA in their FedEO Portal, NASA in their Common Metadata Repository and particular open data sets like CBERS and Sentinel 5P (those links all go direct to the lightning talks). And all three major public cloud providers are moving towards the formats — AWS shared all about their Registery of Open Data, Google showed how Earth Engine can read and write COG’s, their STAC interface to their Earth Engine Data Catalog, and their plans to embrace STAC more. And though Microsoft hasn’t embraced it quite as quickly they have the potential to be a real emerging force in providing Cloud Native Geospatial datasets, as part of their AI for Earth and Planetary Computer initiatives. Their Azure Open Datasets are already quite interesting, and they are building a team that looks set to really embrace COG and STAC as a differentiator. And I always love those who differentiate by being the best at open standards.
I was also really excited to see two international development organizations, the World Bank and Digital Earth Africa, join the movement. Both sponsored the event, but more importantly, both are seeing the potential of this new approach. Years ago I did a fellowship in Zambia, so it is now always in the back of my mind how the technology I build can work in less mature technological environments. Storing data as COG and STAC on the cloud takes much less ‘capacity building’ than previous geospatial approaches, cloud storage regions in Africa let the data live close to its users and moving the compute to the cloud obviates the need to download massive datasets on slow connections to do analysis. So it was awesome to learn about all the Digital Earth Africa from Fang Yuan — they’ve actually put up the first fully Cloud Native Geospatial Sentinel 2 dataset, converting the level 2 surface reflectance products from JPEG2000 to Cloud Optimized GeoTIFF and putting a STAC interface on top of it, stored in the Africa AWS region.
Unfortunately, the timing didn’t work for the World Bank to give a lightning talk, but they’ve been working on a release of NOAA nighttime lights archive in COG & STAC, and their Open Cities AI challenge is a great example of using the Label extension for STAC.
We heard about a number of great tools in the ecosystem. The new stacindex.org that Matthias shared was something he started just two weeks prior, but is a true game-changer for the STAC ecosystem.
Anyone can get a browser interface to their STAC catalog in seconds by just submitting the public URL, and the index should grow as the central place to go to find interesting data in STAC. And it also lists all the tools available in the ecosystem. The lightning talks also highlighted more STAC and COG tools, like DotNetStac, GDAL’s COG support, Intake-STAC, and ESRI crawling STAC with their GeoPortal Harvestor.
I loved Robin’s talk on how STAC is being applied to Planetary datasets at the USGS’s astrogeology science center. My favorite moment in the evolution of an open project is when the work is used in a way that you never even dreamed of. I’ve had lots of big aspirations for STAC & COG, but it didn’t occur to me that they would be useful for work with Mars or Venus. So seeing her talk was definitely that moment for me, and it feels like many more are likely to come with STAC.
There were a couple of other datasets presented that similarly pushed beyond STAC’s core use case of satellite imagery. Perhaps my favorite talk title was Radio Occultation & STAC: a match made in the ionosphere, describing Development Seed’s work for NASA to work with data from Spire and how they managed to bring it into STAC, even though it didn’t fit quite as seamlessly as other data. And then it was awesome to hear from Pixel8 about their work on point clouds and STAC. They have a really compelling vision to take terrestrial point clouds captured from camera phone photos and combine them with overhead reference data to create a single harmonized model of the world that is more accurate than either one alone.
One of the big reasons we expanded from just STAC to be Cloud Native Geospatial was to include Cloud Optimized Geotiff (COG) as well as other emerging formats. COG demos tend to be a bit more visual and flashy than STAC demos, and a few of the presenters delivered in spades.
Fabian from EOX showed COG Explorer, which has been one of my favorite projects for awhile — proving that browsers can talk directly to COG’s, with no tile server needed. He also showed more advanced visualization of COG’s for real analysis of CORINE Landcover. And then Daniel explored even more visualization of COG’s directly in the browser. Duck from Planet shared a slightly different approach, with a COG tile server that sends full band and full bit-depth information to the browser.
And then the flashiest demo of them all was pretty easily Kyle from Unfolded, showing off deck.gl-raster powering landsat8.earth:
He actually took an approach similar to Duck’s, using a tile server on top of COG’s. They used slightly different approaches, but are now actively working together to get an interoperable format.
We also heard about the new emerging Cloud Native Geospatial formats. Zarr excels at multidimensional data, a cloud native format for NetCDF type data. Anderson gave a good overview, Aimee showed off some interesting public Zarr data, and then she also gave a deeper dive into zarr in her intro session. And then Norman shared all about TileDB in his lightning talk and intro session. TileDB is a great new ‘universal data engine’, but the core of it is an array format that is truly cloud-native. And Javi from Carto gave a great articulation of the potential for cloud-based data warehouses (think Snowflake or BigQuery) to transform our industry, with a pitch for using a geospatial Avro for a cloud native vector format, as they’ve done in their new data observatory.
One of our main goals for the event was to try to expand our community, as Cloud Native Geospatial has been rapidly maturing and it’s time to bring more people in. And we wanted to welcome not just people who are new to STAC and COG, but also those who may be new to geospatial. The geospatial world can still be opaque for new people, and a flurry of talks is not the best format to really understand. So we decided to make ‘intro sessions’, that would be up to 40 minutes long, with enough time to take things slow and encourage questions and learning. My favorites of these were aimed at true beginners, aiming to explain many of the core concepts that experienced geospatial practitioners take for granted.
For those who are new to geospatial, I’d recommend starting with a pair of talks from a couple of awesome Planeteers: Sara’s ‘Intro to Geospatial Raster Data’ and Ash’s ‘Packing Your Geospatial Data Science Toolkit’. The first gives a great overview of what geospatial is all about, and the second provides a general approach for how to tackle (geospatial) problems and presents an array of great tools to help you do so. From there, the ‘Machine Learning and Satellite Imagery overview’ from Dave introduces one of the most interesting new trends in geospatial, explaining what ‘Machine Learning’ is and how it applies to imagery. And Data Labeling with Groundwork by Joe and Niki was focused on the Data Labeling Contest that was run, but provides a great introduction to a really interesting tool that is used to create ‘labeled data’ that powers the type of machine learning Dave talks about.
And the final set of truly introductory talks brings people in deeper to some awesome tools. The QGIS session shows how to use the leading open source desktop GIS tool with STAC, and the Sentinel Hub session introduced their awesome cloud GIS, and showed how it works with Cloud Optimized GeoTIFF’s. Alex also shared a practical introduction to a variety of tools working with Digital Earth Africa’s COG+STAC data.
Other intro sessions topics included Radiant Earth MLHub, Intake-STAC, PySTAC, Cirrus, TiTiler & Arturo STAC, Franklin, Multi-Scale Ultra High Resolution (MUR) Sea Surface Temperature (SST) Zarr. Matthias and I each hosted intro + Q&A sessions on STAC (his is better than mine). And Phil did a really great introduction to STAC API, using the Astraea STAC implementation.
To me, the most inspiring aspect of the day was seeing how all these individually cool projects all come together into a real movement. I don’t think doing geospatial natively on the cloud is in any way ‘new’, but I believe there will be a tipping point when geospatial is done primarily on the cloud. To me, the outreach day demonstrated that we are fast approaching the tipping point. The next step is to get the majority of the world’s geospatial data in COG, STAC, and other new cloud native formats, and to shift everyone to a norm of first publishing geospatial data to the cloud. It should open up a new level of global analysis that can actually handle this tsunami of data being generated, at a critical juncture for humanity.
I’ve still got to write up all that happened in the sprint portion of the event and to recognize the best contributions. So look for that soon. And if you’d like to see another Cloud Native Geospatial Outreach event happen and are up to help out please get in touch. I’d love to help make it happen and be a part of it, but I think next time I need to distribute the load so I don’t have two weeks of my life (including part of my vacation) taken over by organizing.
But this outreach event and the overall sprint were a great step forward for this emerging movement, so it was definitely worth it. It was great to make it about more than just ‘STAC’ and recognize all the related pieces coming together. I think we can be confident that STAC is ready to release 1.0.0 soon (with a few more minor evolutions), which was my personal goal for this event. We remain a mostly volunteer effort, so if you’re interested in helping out (with time or funds) don’t hesitate to get in touch.
And thanks once again to everyone who presented, the event would truly be nothing without you.
Earth Imagery for Impact
58 
58 claps
58 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/geekculture/part-1-building-a-java-microservices-e-commerce-app-587ff6df1bdc?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
In recent years, Java is not always considered the preferred programming language for the Cloud. Contrary to the popular belief, you can build and deploy production-grade enterprise application…
"
https://medium.com/lightspeed-venture-partners/vectorized-free-and-source-available-cloud-native-infrastructure-for-real-time-applications-6a419bf7a3e7?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
Over the past decade, we have seen an incredible shift to the “real-time economy.”
Need to go somewhere? Pick up your smartphone, get real-time pricing and availability and 5 mins later a car arrives to pick you up.
Bored? Fire up Netflix and based on your preferences, there are dozens of highly relevant recommendations immediately waiting for you.
Sluggish? Get on your Peloton to work out, beat your personal record and compete in real-time with riders around the world.
What’s amazing is that behind each of these seemingly simple actions, there’s a complex cloud of servers, networks, and storage powering, analyzing, and executing each of your interactions. And, in today’s world, we demand real-time performance. What’s worse than staring at that spinning ball?
While our world has changed dramatically over the past decade, a core piece of infrastructure has not. Underpinning a large portion of today’s real time applications is Kafka, an elegant piece of infrastructure software that was originally designed over a decade ago at LinkedIn. Kafka was written at a time when the cloud was still nascent, when storage was still based off spinning disks, and when multi-core processors were expensive and scarce. Consequently, although Kafka works well-enough, it is extremely complex, hard to manage and doesn’t take full advantage of today’s modern hardware.
Great entrepreneurs see an opportunity and have the courage to challenge the status quo. Alex Gallego, founder and CEO of Vectorized, is one such entrepreneur.
Eighteen months ago, when I first met Alex in a San Francisco coffee shop, he laid out an ambitious plan to modernize the real-time backbone for applications with a brand new architecture. Although I was initially skeptical, it quickly became clear that Alex was one of the few people in the world who could actually pull this off. Alex has dedicated his career to building real-time, data streaming infrastructure, and is a rare technical talent who can both build and explain what’s he building. He also embodies the immigrant mindset. He grew up in Colombia, immigrated to the U.S. in his teens and taught himself to program before going on to study Cryptography at NYU. He brings an incredible focus to his work and has a strong sense of purpose that emphasizes his drive and ambition.
We were excited to lead Vectorized’s initial seed financing with our friends at Haystack in 2019 to help bring his vision to reality.
Now, 18 months later, what Alex and the Vectorized team has accomplished is incredible. Their product, Redpanda, is a modern, free and source available streaming platform for mission critical workloads.
Written in C++, Redpanda squeezes the most out of today’s multi-core hardware, resulting in at least 10x better performance than existing solutions.
Architected for simplicity, Redpanda has removed the complexity of managing third party orchestration systems such as Zookeeper.
Built for the cloud, Redpanda takes native advantage of the compute and storage primitives resident in AWS, GCP and Azure, delivering out-of-the-box functionality such as disaster recovery and global high availability.
Designed for true real-time processing, Redpanda enables developers to ship code and execute that code as the data enters and traverses the pipeline. At its lowest level, Redpanda uses WebAssembly, an intermediary language that allows software engineers to write and edit code in their favorite language to perform data transformations at the streaming source.
And, perhaps the most exciting piece is that Redpanda is completely Kafka API compatible, which means that the millions of real time applications already leveraging Kafka can easily switch to Redpanda, without any code changes… and get an immediate boost in performance and simplicity.
Today also marks an exciting milestone, as we at Lightspeed are thrilled to announce that we have led Vectorized’s Series A financing, with participation from our friends at Google Ventures and Haystack. With over $15M in capital raised, Alex and the Vectorized team are now hardening the soon to be released Redpanda Cloud Offering and also extending enterprise-class features and functionality to power some of the world’s biggest streaming infrastructures. Me, my partner Nnamdi and the entire Lightspeed team are all excited to partner with Alex and the Vectorized team as they modernize real-time infrastructure for a cloud-first world.
-Arif and Nnamdi
Arif Janmohamed is a Partner at Lightspeed Venture Partners. He focuses on investments in enterprise infrastructure, security and SaaS and sits on the boards of a number of rapidly scaling companies, including Vectorized, TripActions, Netskope, Moveworks, Appzen, and CyCognito. In his free time, Arif plays ice hockey with his wife, who yells at him for never passing the puck to her.
Nnamdi Iregbulem is a Partner at Lightspeed Venture Partners, where he focuses on software investments across developer tools, application infrastructure, and machine learning. He works closely with Lightspeed portfolio companies Vectorized, Materialize, and several other unannounced investments. A self-taught programmer, Nnamdi loves to code in his free time and especially loves Python.
Lightspeed is a multi-stage VC firm focused on accelerating disruptive innovations and trends in the enterprise, consumer, and health sectors. Lightspeed has backed 400+ companies globally in the past two decades including Nutanix, Affirm, AppDynamics, MuleSoft, Snap and Nest.
Lightspeed is a multi-stage VC firm focused on accelerating…
135 
135 claps
135 
Written by
Venture Capitalist at Lightspeed Venture Partners. Canadian. Dad.
Lightspeed is a multi-stage VC firm focused on accelerating disruptive innovations and trends in the consumer, enterprise, and health sectors. In the past two decades, Lightspeed has backed 400+ companies and currently manages $10.5B across the global Lightspeed platform.
Written by
Venture Capitalist at Lightspeed Venture Partners. Canadian. Dad.
Lightspeed is a multi-stage VC firm focused on accelerating disruptive innovations and trends in the consumer, enterprise, and health sectors. In the past two decades, Lightspeed has backed 400+ companies and currently manages $10.5B across the global Lightspeed platform.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@datastax/the-future-of-cloud-native-databases-begins-with-apache-cassandra-4-0-87b92c8f695d?source=search_post---------85,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
Sep 9, 2021·5 min read
By Patrick McFadin
“Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust.”
This was the first line of the highly impactful paper titled “Dynamo: Amazon’s Highly Available Key-value Store.” Published in 2007, it was written at a time when the status quo of database systems was not working for the massive explosion of internet-based applications. A team of computer engineers and scientists at Amazon completely re-thought the idea of data storage in terms of what would be needed for the future, with a firm footing in the computer science of the past. They were trying to solve an immediate problem but they had unwittingly sparked a huge revolution with distributed databases and the eventual collision with cloud-native applications.
A year after the Dynamo paper, one of the authors, Avinash Lakshman, joined forces with Prashant Malik at Facebook and built one of the many implementations of Dynamo, called Cassandra. Because they worked at Facebook, they were facing scale problems very few companies were dealing with at the time. Another Facebook tenet in 2008: Move fast and break things. The reliability that was at the top of Amazon’s wish list for Dynamo? Facebook was challenging that daily with frenetic non-stop growth. Cassandra was built on the cloud-native principles of scale and self-healing — keeping the world’s most important workloads at close to 100% uptime and having been tempered in the hottest scale fires. Now, with the release of Cassandra 4.0, we are seeing the beginning of what’s next for a proven database and the cloud-native applications that will be built in the future. The stage is set for a wide range of innovation — all built on the shoulders of the Dynamo giant.
The previous generation of databases before the NoSQL revolution arguably drove a lot of innovation in the data center. It was typical to spend the most time and money on the “big iron” database server that was required to keep up with demand. We built some amazing palaces of data on bare metal, which made the pressure to virtualize database workloads difficult in the early 2000s. In most cases, database infrastructure sat on dedicated hardware next to the virtualized systems of the application. As cloud adoption grew, similar issues persisted. Ephemeral cloud instances worked great for web and app servers, but “commodity” was a terrible word for the precious database. The transition from virtualization to containerization only increased the cries of “never!” for database teams. Undaunted, Kubernetes moved forward with stateless workloads, and databases remained on the sidelines once again. Those days are now numbered. Technical debt can grow unbounded if left unchecked. Organizations don’t want multiple versions of infrastructure to manage — it requires hiring more people and keeping track of more stuff. When deploying virtual data centers with Kubernetes, the database has to be a part of it.
Some objections are valid when it comes to running a database in a container. The reasons we built specialized hardware for databases are the same reasons we need to pay attention to certain parts of a containerized database. High-performance file systems. Placement of the system away from other containers that could create possible contention and reduce performance. With distributed databases like Apache Cassandra, placement of individual nodes in a way that hardware failure doesn’t impact database uptime. Databases that have proven themselves before Kubernetes are trying to find ways to run on Kubernetes.
The future of databases and Kubernetes requires we replace the word “on” with “in” and the change has to happen on the database side. The current state of the art for “Runs on Kubernetes” is the use of operators to translate how databases want to work into what Kubernetes wants them to do. Our bright future of “Runs in Kubernetes” means databases use more of what Kubernetes offers with resource management and orchestration for basic operation of the database. Ironically, it means that many databases could remove entire parts of their code base as they hand that function to Kubernetes (reducing the surface area for bugs and potential security flaws).
The fact that Apache Cassandra 4.0 was recently released is a huge milestone for the project when it comes to stability and a mature codebase. The project is now looking forward to future Cassandra versions building on this solid foundation. Primarily, how can it support the larger ecosystem around it by becoming a rock-solid foundation for other data infrastructure? During the past decade, Cassandra has built a reputation as a highly performant and resilient database. With the types of modern cloud-native applications we need to write, we’ll only need more of that — interoperability will only become more important for Cassandra.
To think of what a cloud-native Cassandra would look like, we should look at how applications are deployed in Kubernetes. The notion of deploying a single monolith should be left rusting in the same pile that my old Sun E450 database server is in now. Cloud-native apps are modular and declarative and adhere to the principles of scale, elastic, and self-healing. They get their control and coordination from the Kubernetes cluster and participate with other parts of the application. The need for capacity is directly linked to the needs of the running application and everything is orchestrated with the total application. The virtual data center acts as a unit but can survive underlying hardware problems and works around them.
The future of Cassandra in Kubernetes isn’t about what it does alone. It’s about what new capabilities it brings to the system as a whole. Projects like Stargate create a gateway for developers to build API-based applications without interacting with the underlying data store. Data as a service deployed by you, in your own virtual data center using Kubernetes. Cassandra itself may be using enabling projects such as OpenEBS to manage database class storage. Or Prometheus to store metrics. You may even find yourself using Cassandra without it being a part of your application. Projects like Temporal use Cassandra as the underlying storage for their persistence. When you have a data service that deploys easily, scales across multiple regions, it’s an obvious choice.
From the spark of innovation that started with the Dynamo paper at Amazon to the recent release of 4.0, Cassandra was destined to be the cloud-native database we all need. The next ten years of data on Kubernetes will see even more innovation as we take the once ivory palace of the database server and make it an equal player as a data service in the application stack. Cassandra is built for that future and ready to go with what is possibly the most stable database release ever in 4.0. If you are interested in joining the data on Kubernetes revolution, you can find an amazing community of like-minded individuals at the Data on Kubernetes Community. If you want to help make Cassandra the default Kubernetes data store, you can join us at the Cassandra project or more specifically the Cassandra on Kubernetes project, K8ssandra.
This post originally appeared on the DataStax blog.
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
See all (229)
68 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
68 claps
68 
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
About
Write
Help
Legal
Get the Medium app
"
https://blog.getambassador.io/three-predictions-for-cloud-native-platforms-for-2020-72f28c22e788?source=search_post---------86,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
It is the time of the year when everyone starts to make predictions. At Datawire, we have recently been very focused on the early access launch of the Ambassador Edge Stack, and also presenting several sessions at KubeCon in San Diego, but we were keen to take some time out to reflect on the key trends we see emerging within the cloud native space over the next twelve months.
Broadly speaking, our top three predictions for the cloud native space in 2020 are:
The emerging next generation of application deployment platforms and DevOps tooling have been built on a new foundation that is cloud-agnostic: the Kubernetes framework. “GitOps” is a way to manage Kubernetes clusters and the associated delivery of software applications; although it is already popular, we predict that 2020 will be the year GitOps goes mainstream.
GitOps works by using the Git version control system as a single source of truth for declaratively configured infrastructure and applications. As Git is now at the center of the delivery process, engineers use a self-service process for modifying infrastructure, deploying applications, and releasing new features, simply by issuing pull requests.
On a related topic, there does appear to be a bit of a CLI vs UI workflow schism emerging within the cloud native ecosystem. At KubeCon NA, held in San Diego, many organisations were extolling the virtues of GitOps and declarative configuration, with the usual suspects from Weaveworks, and also user stories from Fidelity and CERN. However, there were many folks wandering around the expo hall asking how UI can be used to drive configuration, and there were vendors keen to meet this need.
One way this divergence could be addressed is by using UIs as an alternative way to generate declarative config that can be fed into a GitOps pipeline, while still supporting the creation of config manually or via other automated processes.
Broadly speaking, there are three definitions of the edge associated with cloud native technologies:
All three categories have seen increasing adoption in 2019, and as the definitions and requirements of these edges become clearer, this will open up potential for rapid innovation.
At Datawire we have recognised there are two primary challenges with datacenter edge technologies and API gateways when adopting Kubernetes, and we’ve presented several strategies for managing APIs and the Kubernetes network edge. Our conclusion is that everyone building a Kubernetes platform needs an effective edge stack that provides L4 load balancing, an API gateway, security, cross-cutting functional requirement management (rate limiting, QoS etc) and more.
The second half of our 2019 was spent working with and listening to customers, and in December we launched the early access version of the Ambassador Edge Stack. In 2020 we will continue to work on several innovations within managing and scaling edge technologies, and we expect the underlying proxy powering the Ambassador API gateway — Envoy Proxy — to also continue leading the field with innovations in this space.
The large public cloud vendors are focusing on extending the private data center into the cloud (and vice versa) via two main approaches: first, the compute abstraction, which consists of managing VMs, containers, and k8s via a common cloud control plane, such as GCP Anthos and Azure Arc; and second, via the use of a networking abstraction with common communication control planes, such as seen in AWS Outposts and AWS App Mesh. We believe this push towards hybrid clouds and supporting the integration of multiple clouds will only continue to get stronger in 2020, and accordingly more and more tools will focus on multi-cloud.
At Datawire we’ve shared our thoughts about migrating from VMs to Kubernetes, and also presented on hybrid cloud migration strategies using an API gateway, such as Ambassador, and a service mesh, such as HashiCorp’s Consul. We’ll be continuing this work in 2020, and talking more about how to build a cloud agnostic Kubernetes platform.
You can learn more about the Ambassador Edge Stack at www.getambassador.io, and if you have any questions, please join the team in the Datawire OSS Slack.
Developer-First Kubernetes.
48 
48 claps
48 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/doctor-alice-and-cloud-native-bob-my-favorite-machine-learning-users-5c2b36998e0d?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Julien Simon
Jul 20, 2019·8 min read
Alice and Bob are great, which is just as well because I meet them everywhere I go! They’re passionate, hardworking people who try their best to build great Machine Learning solutions. Unfortunately, a lot of things stand in their way, and slow them down.
It’s my job to listen to and help all the Alices and Bobs out there :) In this post, I’ll try to summarize the challenges that they’re facing, how they can start solving them with AWS services in general, and Amazon SageMaker in particular.
"
https://blog.heptio.com/cloud-native-part-2-d5c29e699caf?source=search_post---------88,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
This is the second part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
Like any area with active innovation, there is quite a bit of churn in the Cloud Native world. It isn’t always clear how best to apply the ideas laid out in the previous part. In addition, any project of significance will be too important and too large for a from scratch rewrite. Instead, I encourage you to experiment with these new structures for newer projects or for new parts of existing project. As older parts of the system are improved, take the time to apply new techniques and learnings as appropriate. Look for ways to break out new features or systems as microservices.
There are no hard and fast rules. Every organization is different and software development practices must be scaled to the team and project at hand. The map is not the territory. Some projects are amenable to experimentation while others are critical enough that they should be approached much more carefully. There are also situations in the middle where the techniques that were proven out need to be formalized and tested at scale before being applied to critical systems.
Cloud Native is defined by better tooling and systems. Without this tooling, each new service in production will have a high operational cost. It is a separate thing that has to be monitored, tracked, provisioned, etc. That overhead is one of the main reasons why sizing of microservices should be done in an appropriate way. The benefits in development team velocity must be weighed against the costs of running more things in production. Similarly, introducing new technologies and languages, while exciting, comes with cost and risk that must be weighed carefully. Charity Majors has a great talk about this.
Automation is the key to reducing the operational costs associated with building and running new services. Systems like Kubernetes, containers, CI/CD, monitoring, etc all have the same overarching goal of making application development and operations teams more efficient so they can move faster and build more reliable products.
The newest generation of tools and systems are better set up to deliver on the promise of cloud native over older traditional configuration management tools as they help to break the problem down so that it can easily be spread across teams. Newer tools generally empower individual development and ops teams to retain ownership and be more productive through self service IT.
Continue with part 3 as we look at how Cloud Native relates to DevOps. We also look a bit about how Google approaches this through the SRE role.
Heptio
28 
28 claps
28 
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/velotio-perspectives/jenkins-x-a-cloud-native-approach-to-ci-cd-69e06d367711?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
Jenkins X is a project which rethinks how developers should interact with CI/CD in the cloud with a focus on making development teams productive through automation, tooling and DevOps best practices.
Jenkins is a tool which was developed long before cloud become the norm and it’s definitely not a cloud-native tool, meaning it doesn’t have out of the box capability to survive outages, seamless scaling, etc.
Basically, Jenkins X is not just a CI/CD tool to run builds and deployments, it automates the whole development process end to end for containerized applications based on Docker and Kubernetes.
With Jenkins X, you get not only Kubernetes pipelines for your apps but also a very scalable CI/CD solution.
So all this stuff which developers have to do is kind of unnecessary heavy lifting. Jenkins X is designed to get the above stuff done and focus on delivering actual value to customers.
By using Jenkins X, the developer can type one command jx create or jx import and the following process: get the source code, the git repository and application created, automatically built and deployed to Kubernetes on each Pull Request or git push with full CI/CD complete with Environments and Promotion.
Developers and teams don’t have to spend time figuring out how to package software as docker images, create the Kubernetes YAML to run their application on kubernetes, create Preview environments or even learn how to implement CI/CD pipelines with declarative pipeline-as-code Jenkinsfiles. It’s all automated in Jenkins X.
If you want to see Dockerfile, Jenkinsfile or Helm charts for your apps or their environments then those are all available versioned in Git with the rest of your source code with full CI/CD on it all.
Create new projects or import existing source code quickly into Jenkins X via the jx command line tool and:
On each Pull Request, a CI pipeline is triggered to build your application and run all the tests ensuring you keep the master branch in a ready to release state. When a Pull Request is merged to the master branch the Release pipeline is triggered to create a new release:
JX is a command line tool for installing and using Jenkins X. Check out how to install jx. To create a new Kubernetes cluster with Jenkins X installed, use the jx create cluster command. The command initializes Pods with the following applications in jx namespace of Kubernetes:
Let’s have a look at how the JX processes correlate to Git actions. For simplicity, let’s use only master and one feature branch. The table below details the steps of the flow.
Note: Following steps are for Ubuntu 16.04
a. Download from: https://storage.googleapis.com/kubernetes-release/release/v1.10.3/bin/linux/amd64/kubectl
b. Move to /usr/local/bin for path binding:
a. Download from: https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz
b. Move to /usr/local/bin for path binding:
After triggering this command, we are prompted to authenticate Google Cloud account. It also prompts for some questions on how to create our cluster. It generates a random admin password which is used to access all the applications that we’ve installed.
We can now get logs for the build jobs which are actually provisioning the staging environment. Please find detailed logs of the above command displayed below.
Please find detailed logs of the above command from the Production environment displayed below.
Jenkins X has been successfully installed and default environments are created on our new GKE cluster.
The following steps creates a sample Spring Boot application and import it into Jenkins X.
It will prompt for some questions like the type of project, mvn group ID, artefact name, spring boot dependencies etc. It initializes this project as a Git repository. It adds some extra files to the repo i.e. Dockerfile, Jenkinsfile, charts etc. This will create a repository in GitHub. Also creates a project in Jenkins and registers webhooks in the Git repository.
2. Command to get build logs
3. Application deployment onto staging environment
Using CLI, we can see our application is now deployed to staging environment with version 0.0.1
4. Application URL
We can get application URL with following command
5. Start working on the project
Now we can start working on our new project. Go to your project location and from there you can actually create a Github issue.
6. Add HTML Page
Now switch to your working branch and add a simple HTML page.
7. Create a pull request
Then create a pull request from working branch to master branch. After that our CI pipeline is automatically triggered.
After some time, our CI checks pass.
8. There we can see a comment has been added to the pull request saying our application is been built and deployed into the preview environment.
9. Now our application is deployed in preview environment.
10. Now, you can see the application running on the given URL.
As you have seen, there was no direct interaction with Jenkins, but it is there, running the pipelines for continuous integration and continuous delivery of the repository, and orchestrating things with Kubernetes.
If you run jx get pipelines you can see URLs to the various pipelines that have been set up for you are part of interacting with Jenkins X.
jx open
jx activity
jx get environments
jx get apps
So this is how Jenkins X automates the installation, configuration of Jenkins and some other software components. It also automates CI/CD for your applications on Kubernetes alongwith promotions through GitHub.
*****************************************************************
This post was originally published on Velotio Blog.
Velotio Technologies is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering.
Interested in learning more about us? We would love to connect with you on ourWebsite, LinkedIn or Twitter.
*****************************************************************
Thoughts and ideas on startups, enterprise software &…
14 
14 claps
14 
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/the-how-of-cloud-native-people-and-process-perspective-a50852b50aa2?source=search_post---------90,"There are currently no responses for this story.
Be the first to respond.
Kyle Brown and Kim Clark
Note, this is part 2 of a multipart series. You can find part 1 here , or jump to Part 3, Part 4, Part 5.
In the previous article, where we discussed what cloud native actually means, we established that to achieve the desired benefits from a cloud native approach you needed to look at it from multiple perspectives. It is about not only what technology you use and where your infrastructure is located, but also how you architect your solutions. But perhaps most importantly, it is about how you organize your people and what processes you follow. In this and the next two articles we are going to walk through what we have seen to be the most important ingredients of successful cloud native initiatives, taking a different perspective in each. A summary of the themes in this series is shown in the diagram below:
Let’s begin by looking at perhaps the most overlooked perspective — how cloud native affects the people involved, and the processes they are part of.
The people component outweighs any of the other parts in getting to cloud native success. In order to achieve the business value of cloud native, teams need to be able to rapidly coordinate between business and IT, have a “low touch” way of getting their changes through to production, and be passionately accountable for what they deliver. No amount of new technology, or modern architecture approaches will accomplish this on their own. Teams need to invest in moving to agile methods, adopt DevOps principles and software lifecycle automation, adopt new roles (such as SREs), and organizations must give teams an appropriate level of autonomy. We show some of the most important people aspects of cloud native in the diagram below:
This list is by no means complete. We would also assert that there are other people based aspects that improve team resiliency and cut across all the ingredients below such as a move to a no-blame culture, and encouraging a growth mindset. In the next sections we’ll dive into each of the ingredients in the above diagram in depth.
Agile methods
Cloud native infrastructure and microservices-based design enable the development of fine grained components that can be rapidly changed and deployed. However, this would be pointless if we did not have development methods that can leverage and deliver on that promise. Agile methods enable empowered (decentralized) teams to achieve rapid change cycles that are more closely aligned with business needs. They are characterized by the following:
Agile methods are usually contrasted with older, “waterfall”, methodologies. In a traditional waterfall method, all requirements are gathered up front, and then the implementation team works in near isolation until they deliver the final product for acceptance. Although this method enables the implementation team to work with minimal hindrance from change requests, in today’s rapidly changing business environment the final delivery is likely to be out of sync with the current business needs.
Agile methodologies use iterative development cycles, regular engagement with the business, combined with meaningful data from consumer usage to ensure that projects stay focused on the business goals. The aim is to constantly correct the course of the project as measured against real business needs.
Work is broken up into relatively small business relevant features that can then be prioritized more directly by the business for each release cycle. The real benefit to the business comes when they accept that there cannot be a precise plan for what will be delivered over the long term but that they can prioritize what is built next.
Agile itself is becoming an “old” term and has suffered over time, as many terms do, from nearly two decades of mis-use. However, for the moment, is it perhaps still the most encompassing term we have for these approaches.
Lifecycle automation
You cannot achieve the level of agility that you want unless you reduce the time that it takes to move new code into production. It does not matter how agile your methods are, or how lightweight you have designed your components if the lifecycle processes are slow. Furthermore, if your feedback cycle is broken, you cannot react to changes in business needs in real time. Life cycle automation is centered around three key pipelines. These are:
We show the interaction of these in the diagram below:
Continuous Integration (CI) means that as changes that are committed to the source code repository often (“continuously”) and that they are instantly and automatically built, quality checked, integrated with dependent code, and tested. CI provides developers with instant feedback on whether their changes are compatible with the current codebase. We have found that Image-based deployment enables simpler and more consistent build pipelines. Furthermore, the creation of more modular, fine-grained, decoupled, and stateless components simplifies the automation of testing.
CD either stands for Continuous Delivery or Continuous Deployment (both are valid, although Jez Humble’s book popularized the term Continuous Delivery, which covers both). Continuous Delivery takes the output from CI and performs all the preparation that is necessary for it to be deployed into the target environment, but it does not deploy it, leaving this final step to be performed manually in controlled, approved conditions. When an environment allows the automation to deploy into the environment, that is Continuous Deployment, with advantages in agility balanced against potential risks.
Continuous Adoption (CA) is a less well known term for an increasingly common concept; keeping up to date with the underlying software runtimes and tools. This includes platforms such as Kubernetes, language runtimes and more. Most vendors and open source communities have moved to quarterly or even monthly upgrades. and failing to keep up with current software results in stale applications that are harder to change and support. Security updates as a minimum are often mandated by internal governance. Vendors can provide support for a minimal number of back versions, so support windows are getting shorter all the time. Kubernetes, for example, is released every three months and only the most recent three are supported by the community. CI/CD, as noted above, means code changes trigger builds, and potentially deployment. Enterprises should automate similar CA pipelines that are triggered when vendors or communities release new upgrades. For more information about CA, see Continuous Adoption.
Its worth noting that lifecycle automation is only as good as efficiency of the processes that surround it. There’s no value in working to bring your CI/CD cycle time down to minutes if your approval cycle for a release still takes weeks, or you are tied to a dependency that has a lifecycle measured in months.
DevOps and Site Reliability Engineering
As we can see from the figure above, lifecycle automation lays the groundwork for a more profound change to the way people work. As we simplify the mechanism between completion of code, and it’s deployment into production, we reduce the distance between the developer and the operations role, perhaps even combining them.
This is known as DevOps, and has some key themes:
In traditional environments there is strong separation between development and operations roles. Developers are not allowed near the production environment, and operations staff have little exposure to the process of software development. This can mean that code is not written with the realities of production environments in mind. The separation is compounded when operations teams, in an effort to protect their environments, independently attempt to introduce quality gates that further impede the path to production, and cyclically the gap increases.
DevOps takes the approach that we should constantly strive to reduce and possibly remove the gap between development and operations so that they become aligned in their objective. This encourages developers to “shift left” many of the operational considerations. In practice this comes down to asking a series of questions and then acting on the answers:
Platforms elements such as containers and Kubernetes can play an important role in this, as we will see from concepts such as image based deployment and infrastructure as code that we will discuss later.
Clearly, the shortening of the path between development and production by using CI/CD is a linked to DevOps as is the iterative- and business-focused nature of agile methods. It also means changing the type of work that people do. Software developers should play an active role in looking after production systems rather than just creating new functions. The operations staff should focus on ways to automate monotonous tasks so that they can move on to higher value activities, such as creating more autonomically self-healing environments. When these two are combined, this particular role is often referred to as a Site Reliability Engineer to highlight the fact that they too are software engineers. Key to succeeding with this is the need to accept that “failures are normal” in components and that we should therefore plan for how to manage failure rather than fruitlessly try to stop it from ever happening.
In a perfect world, software development and operations become one team, and each member of that team performs both development and operations roles interchangeably. The reality for most organizations has some level of compromise on this however, and roles still tend to become somewhat polarized toward one end of the spectrum or the other.
Team Autonomy
If we make the methods more agile, and the path to production more automated, we must no then stifle their ability to be productive and innovative. Each team is tackling a unique problem, and it will be better suited to particular languages and ways of working. We should give the teams as much autonomy as possible through:
If we’re going to rapidly iterate over more fine grained components, we need to decentralize “one-size-fits-all” policies and allow more local decision making. As we will discuss later a good cloud platform should naturally encourage standardization around build, deployment and operations, so long as components are delivered in a consistent way (e.g. container images). To be productive, teams then need to have freedom over how they implement those components; choosing their own technologies such as languages and frameworks. Equally important is to ensure the teams can rapidly self-provision the tools and resources they need, which of course aligns well with the very nature of cloud infrastructure.
There is still a need for a level of consistency in approach and technology across the enterprise. Approaches like the Spotify model, for example, often approach this need through “guilds”, groups made from individuals from the teams that focus on encouraging (rather than enforcing) common approaches and tools based on real world experiences in their own teams.
Of course, a caveat is that decentralization of control can’t typically be ubiquitously applied, nor can it be applied all at once. It might make sense for only certain parts of an enterprise, or certain types of initiative in an enterprise. Ultimately, seek a balance between enabling elements of a company to innovate and explore in order to retain market leadership, and ensuring that you do not compromise integrity of the core competencies of the business with constant change and increasing divergence.
In the next part of this series we’ll look at what architecture and design choices we need to make in order to best leverage the cloud environment. In the meantime, if you want to learn more about any of these issues, visit the IBM Garage Method website, where we cover many of these topics in more depth in the context of an end-to-end method.
Get smarter at building your thing. Join The Startup’s +750K followers.
67 
67 claps
67 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
IBM Fellow, CTO for Cloud Architecture for the IBM Garage, author and blogger
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://architecht.io/can-free-bandwidth-unbundle-the-cloud-can-ai-and-cloud-native-unbundle-silicon-valley-2b58c8c95d7b?source=search_post---------91,NA
https://medium.com/@dunith/alcobuddy-a-story-based-approach-to-understanding-cloud-native-applications-a5980c9438b3?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dunith Dhanushka
Jun 6, 2019·12 min read
Let me tell you this first. This is not going to be just another post explaining the concepts of Cloud Native Application development. There’s plenty of good material out there describing what is Cloud Native Computing and the problems it’s trying to solve these days.
But in this post, I’m gonna take a different approach — to be specific, a story based approach to teach you why should your organization strive to embrace the Cloud Native Application development practice. The story revolves around three friends who tried to do a mobile app based startup.
So let me tell you the story of “Alcobuddy”…
The story begins with Ivan. He’s an aspiring young entrepreneur who comes from a business background. He’s got proven track records of successfully running businesses and most importantly knows whom to meet to get the job done.
Ivan has an idea of creating a mobile app to drive people home who are not in the right condition to drive; especially after taking a few drinks ;) The app works in a way that the users would request for a chauffeur through the app and the chauffeur will drive the user back home in the comfort of his/her own vehicle. Ivan saw this as a very good opportunity to seize and there was a huge untapped market that already existed.
So Ivan approached Shenal; a young computer nerd who’ll be very resourceful in Ivan’s new venture. He’s gonna play the CTO role of the new startup. Shenal just said, “yes, I’m in” :)
Now they have the brains and muscles. But there’s a need for a friend with benefits ;) Having that in mind, they approached Dexter, a celebrity singer with deep pockets and owns a band. Ivan pitched his idea and Dexter carefully evaluated the horizon and finally said: “count on me man, I’ll invest my money in your venture”.
And just like that, the App Alcobuddy was conceived. Let’s see how they going to take this idea from the ground.
Having a good IT background under his belt, Shenal analyzed the whole project’s scope and came up with the initial architecture.
The solution comprises of a mobile app (obviously!) and a Java-based backend. The mobile app will communicate with the backend over REST APIs exposed by the backend app.
Shenal is a little rusty on mobile app development and Java development knowledge. So he hired two developers for the mobile app and the backend to work remotely. With the seed capital received from Dexter, Shenal bought a new server to host the backend and transformed his garage into a mini data center. He’ll be playing the operations guy role going forward.
So everyone teamed up with big dreams in their minds, rolled their sleeves up and started hacking.
The initial release took more time than they expected. They thought of rolling out the first cut of the app within one month. But things didn’t work out that way. Here’s why.
After spending 6 months in the development and testing, they finally launched the app to the market.
But a local ride-hailing company stole their idea and launched it as a feature to their already existing app just one month before Alcobuddy.
Alcobuddy was late to the market…
Despite being late to the market, Alcobuddy started getting some traction. There was a sizable market had been consuming the app.
We know nothing is perfect at first. So does Alcobuddy. One day, a lot of users complained that they couldn’t log into the app. Shenal had to call the mobile developer and then they did some diagnosis. Apparently, the backend database had crashed and none of the users were able to log in.
Shenal had to coordinate with two developers and debug the code for the app and the backend. Finally, they found a bug in the backend code that leaked the database connection. Fixing this bug took both developers a lot of time as they had to depend on each other (plus the time zones didn't help either). Also, they had to run the full integration test suite again, build the app, package it and finally deploy to production. During this window of time, u Alcobuddy users experienced a critical downtime and they took to the Alcobuddy FB page to vent it out. Ouch!!!
In the meantime, Alcobuddy’s investor, Dexter was having plans to do a concert with his band. So he wanted to offer a discount to any Alcobuddy user that requests a chauffeur to and from the concert. Dexter wanted to get this feature rolled out ASAP. So he communicated that to Shenal via Ivan.
This means that Shenal had to roll out a new feature into production since the current version of the app doesn’t support offering discounts. Both the mobile app and backend has to be changed and the timelines were not helping out since they only had 2 days left to roll out the feature. Shenal was like “Ohh, not again!”
Once again, Shenal rolled his sleeves up and sat down with the two developers. But it took more time than they thought. The backend developer had to apply major changes into almost all the API methods. Those changes were followed by the changes in value objects, service implementation and finally in the database schema. The mobile developer had to change app UI to meet the need. When both developers are done, Shenal had to run the tests, build the package and deploy.
Shenal’s team had spent sleepless nights and finally, when they were ready, the concert was over.
Dexter was not happy…
Despite the constant hiccups, it went through, Alcobuddy’s business was booming. Prices of booze were reduced so that people started crawling into pubs and called Alcobuddy to rescue them ;)
Ivan was happy, but definitely, Shenal was not. He spent most of the time in the garage taking care of outages and crashes. The backend had almost reached its max capacity. Shenal wanted to bring in a new server to balance the load and increase the availability. Shenal had to experience the agony of infrastructure provisioning again.
After the upgrade, Shenal’s workload increased. Now he has two servers in his fleet which requires duplicate efforts on configuring, applying patches, taking backups and monitoring.
Shenal was not so happy…
Ivan noticed that the operational cost of the business went through the roof, proportionate to the demand. He was expecting a fat margin from the increased demand. But apparently, Shenal’s IT expenses ate into that.
Since Alcobuddy was used by the users who are active at night time, the app hardly received traffic in the day time. Both servers were idling during the day time. But they had to be constantly powered up and bandwidth had to be allocated. This lead to unnecessary operational cost and business was not making enough money to run its operations.
So everyone was not happy with the current situation.
Now let’s do an analysis of the past events at Alcobuddy.
The Alcobuddy team got together and did some soul searching to figure out a way to fix the current situation.
Cloud Native Applications are the new kid in the block these days. Everybody seems religiously adopting that. But what has motivated them to go Cloud Native? Let’s find out.
Before we dive into the details, let’s look at the definition of Cloud Native Applications. I’d like to quote Pivotal here.
Cloud-native is an approach to building and running applications that exploits the advantages of the cloud computing delivery model. Cloud-native is about how applications are created and deployed, not where. — Pivotal
According to the above, Cloud Native applications reap the benefits of deploying them on cloud infrastructure. But what about the “cloud” in cloud-native applications?
If an app is “cloud-native,” it’s specifically designed to provide a consistent development and automated management experience across private, public, and hybrid clouds. — RedHat
By combining these two definitions, we can simply say that Cloud Native Application development is the practice of building applications without thinking about where they might get deployed. Cloud Native applications abstract away the infrastructure requirements for applications, allowing them to be deployed and migrated across multiple environments in a consistent manner. Those environments could be private, public, or hybrid clouds.
This allows application developers to focus more on their business problems rather than addressing infrastructure and logistical concerns.
In the next section, let’s learn why you should adopt the Cloud Native application methodologies with the lessons learned by Alcobuddy team.
After having a few rounds of discussions, the Alcobuddy team was convinced they need to move to the cloud. But they were not so confident that moving to the cloud would solve all of their existing problems. Also, they had no clue or guidance on how to initiate the Cloud Native journey.
Then Shenal stepped outside from his comfort zone and acted as a true CTO there. He mastered the principals of Cloud Native Application development and determined to instill a culture inside Alocobuddy to utilize the Cloud Native concepts for their day to day operations.
Along with Shenals guidance, Alcobuddy team took some radical approaches to restructure the way they used to deliver the applications to the market which were based on the key tenets of Cloud Native Application development as follows.
Let’s look at those approaches in detail.
Microservices architecture decomposes applications into small, loosely coupled independently operating services.
Along with his developers, Shenal took a lead on breaking up their monolithic Java-based backend application into a collection of Microservices with a bounded context. These services map to smaller, independent development teams and make possible frequent, independent updates, scaling, and failover/restart without impacting other services.
— Since Microservices are independently developed and communicated over standard interfaces, backend developer was able to develop and test his code in isolation without depending on the app developer. So the tight coupling between two developers was taken away.
— Hiring new backend developers and get them to up to the speed was easier and scalable. Earlier, the new developer had to go through the entire code base to get familiarise with the business domain. But now a developer has to own and know only a defined domain of knowledge and code.
Individual Microservices are packaged in containers. Containers offer a lower resource utilization, fast startup time, low distribution size and above all it abstracts away the underlying deployment platform complexities from the developer. Instead of configuring, patching, and maintaining operating systems, now developers focus on their application context.
— Since containers are portable across environments, developers were able to quickly troubleshoot the production issues in their local workstations without replicating the production environment. This has drastically reduced the time to attend critical bugs.
— Developers were able to verify fixes and features across multiple environments in a shorter cycle time. This has drastically cut down the time to needed to verify fixes.
Shenal took a radical decision to go with a container orchestration engine service offered by a cloud vendor. He selected a managed Kubernetes service so that he no longer had to spend time in front of hardware boxes, configuring them, installing software, patching and constantly monitoring them for outages.
— Operations were offloaded to the cloud vendor so that Shenal could focus more on his core business rather than tending the server farm in his garage.
— The operational risks were mitigated to the cloud vendor. High availability, monitoring, and disaster recovery will be handled by the cloud vendor.
— Cloud infrastructure offered an outstanding on-demand scale-out and scale-in facility. Most of the scaling scenarios were fully automated so that ops team no longer needed to manually provision/decommission compute units. That helped Alcobuddy to respond to varying demand much faster while providing a consistent user experience.
— With the cloud vendor’s Pay-As-You-Go scheme, Alcobuddy was able to cut down many infrastructure expenses. They were charged only for what they had consumed. Alcobuddy finally moved from CAPEX to OPEX which let them invest the saved expenses back into the business.
If done right, DevOps culture lays out a foundation for building, testing and releasing software rapidly, frequently, and more consistently.
Continuous Delivery makes it possible to continuously adapt software in line with user feedback, shifts in the market and changes to business strategy. Test, support, development, and operations work together as one delivery team to automate and streamline the build-test-release process.
After establishing solid DevOps practice and a CD process, software releases were no longer a painful or risky job. Shenal teamed up with the developers and implemented a fully cloud-based CI/CD pipeline that automates everything from building containers, run test suites on lower environments to finally deploy them on production in a reliable manner.
— Alcobuddy drastically cut down the time taken to ship new features and bug fixes to the users with this new practice. That eventually increased the release velocity and allowed them to respond to the shifts in the market in a more faster and reliable manner.
After embracing the Cloud Native movement in a meaningful way, Alcobuddy took off the ground fast, became agile, lean and mean. They streamlined their value delivery pipeline by making the right technology choice at the right time.
If you are in doubt of going Cloud Native, I suggest you consider the benefits that Alcobuddy enjoyed once they embraced going Cloud Native.
— Breaking the Java-based monolithic backend application into a collection of Microservices lead the development team to gain a better developer experience, reduced coupling among team members, and apply robust programming according to the Twelve-Factor App.
— DevOps culture and CI/CD process automated all manual and cumbersome tasks. This has reduced the time to move fixes and new features into production more frequently with less risk. Ultimately that helped them to respond to ever-changing business needs much faster.
— Containerising of application code abstracted away from the infrastructure concerns. That made the application components more portable across different environments and made the debugging easy.
— Moving to a cloud-based container orchestration platform reduced its operational costs. Also, they benefitted from the inherent on-demand scalability and reliability features offered by the cloud vendor. Moreover, the risk of owning their infrastructure has been successfully mitigated.
As a parting note, I’d like to extend my gratitude to my colleague Shenal Mendis for coming up with those awesome avatars for the characters.
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
255 
Thanks to Ivan Saverus. 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
255 claps
255 
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@fedakv/cloud-native-vs-lift-and-shift-which-way-to-choose-7dc89e1da906?source=search_post---------93,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Mar 21, 2018·5 min read
Cloud transition might seem a daunting task for many enterprises, given the complexity of their legacy software and architecture. Should you lift and shift your apps or go cloud-native?
Lifting and shifting the software is a process of moving the unchanged infrastructure and software from on-prem servers to the cloud. It’s like you go on a trailer from one trailer park to another. You obviously don’t need to change a trailer to a muscle car for that. The main reason is cutting the expenses, usually. However, if you change nothing, you carry the flaws of the old system with you to the new cloud location.
There are certain scenarios, however, when such a move can be feasible. For example, the company must meet a budget threshold and the on-prem infrastructures are always a cost center of IT operations. Here are some cases when the apps can be safely lifted and shifted to the cloud:
While the lift and shift approach can be feasible as a short-time solution for mission-critical systems (or a long-term allocation of auxiliary systems), many enterprise businesses had realized that in order to use the cloud efficiently, one has to use it to the fullest extent. There are simply bottlenecks and single points of failure (SPOF) like the vertically-built databases that cannot be removed without disassembling the system and reassembling it anew.
This also makes impossible using the autoscaling and serverless computing features the cloud platforms provide. Obviously, if the app was built without these features in mind, its architecture cannot support them, which leads us to a necessity of redesign of the app from scratch to become cloud-native. Or change your trailer to a muscle car and go to Las Vegas, NV to win a jackpot, if we continue our analogy.
On the contrary to lifting and shifting, redesigning the apps to become cloud-native takes quite a long time. It is an intricate process that consists of several main parts:
As compared to lifting and shifting the apps, building their cloud-native analogs is a time-consuming and resource-costly process, of course. However, it delivers several benefits that cannot be overestimated:
There is a point to consider, though: it all depends on the scale of operations and the tasks at hand. If the enterprise goes global and requires lots of resource-intensive tasks (like Big Data analytics or training Machine Learning models for predictive decision-making support), using bare-metal data centers with an added Infrastructure-as-a-Service layer is definitely more feasible. The public cloud serves as the medium to connect them in that case unless the company decides to go for a private cloud or hybrid cloud strategy.
To sum it up, both cloud-native and lift and shift approaches to cloud transition are feasible, depending on your business operational patterns and goals. However, we sincerely believe — and know for sure from our hands-on experience — the complete redesign of the app architecture and underlying infrastructure to be more rewarding in the long run.
Despite the larger initial investment and longer transition time, going for cloud-native software ecosystem ensures the utmost flexibility of operations. This also provides the ability to integrate the latest technology and practices in order to gain and retain the competitive edge over not-so-techy market players, optimize the resource allocation and deliver a steadier, bigger ROI.
This story was initially posted on my company’s blog — https://itsvit.com/blog/cloud-native-vs-lift-shift-way-choose/
DevOps & Big Data lover
14 
14 
14 
DevOps & Big Data lover
"
https://medium.com/built-to-adapt/for-cloud-native-data-is-the-new-black-544f26a45059?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
About two months ago I was called out to spend some time with a team from a customer of ours to talk about “Devops.” Some background: at Pivotal we’re rather opinionated on what Devops means and how you get there. For us it’s about process and cultural change, coupled with the right platform. This achieves a software development and delivery capability that eliminates the friction between development and operations, and allows for the continuous delivery of value to the customer.
Having spent the last three years focusing on an incredible application platform that is a tremendous enabler of Devops, I’ve had this conversation quite frequently in a variety of venues, from customer meetings to conference halls.
This meeting proceeded like many before it. When I talked about the need for environment parity across the entire software lifecycle in order to avoid “it works on my machine” finger-pointing, nervous laughter filled the room. My hosts had a great appreciation for developers who had self-service provisioning resources at their fingertips to do their work, and have those resources automatically reclaimed when the job was done.
When I described how immutable infrastructure — the stack you build once and run as many instances of it as needed — eliminated configuration drift and untouchable snowflakes, there were murmurs of “we need to do that.” The mood shifted to astonishment when I explained that at Pivotal we only did production deploys during normal business hours, something made possible because we derisk software upgrades with things like blue/green and parallel deployments.
When I talked about the need for environment parity across the entire software lifecycle in order to avoid “it works on my machine” finger-pointing, nervous laughter filled the room.
When I suggested that they needed to make a fundamental shift from treating change as the exception to treating it as a rule, there were nods of appreciation. When our two-hour meeting stretched into the fourth hour, we talked about how to get started, the practices that could be tried, and the tooling required to support it.
The team was open minded and frank in assessing their current state. They were charged up from our meeting and were brainstorming possible solutions. It was a whole lot of fun! And not that unusual.
Except for one thing.
The team I was meeting with that day were not developers, or IT operations and not tasked with bringing a new compute platform to developers. This was the data team. That is, members of IT who mind the data technologies, databases, data analytics, storage and more, across their enterprise. That afternoon I realized that this data team had goals that were very similar to the many other (non data-centric) client developer teams I had been working with over the last three years: to provide a platform that enables data scientists, developers and operators to get the resources they need to build, deploy and manage their software products at start-up speeds, without the friction that has historically plagued them.
For the last three years I have been part of the Cloud Foundry product team at Pivotal. My day job has been engaging with customers and partners to help them navigate their way onto the third platform. My focus, and the focus of the Cloud Platform organization as a whole, has been predominantly on compute for developers and operators — in particular, the Cloud Foundry Elastic Runtime and the Spring Framework, as well as, BOSH, the “cloud native workhorse” and toolchain for managing complex, distributed systems like Cloud Foundry.
Yes, the stateless, resilient, scale-out applications that would be deployed to Pivotal Cloud Foundry would bind to data-oriented services drawn from a rich marketplace that we’ve built together with our partners. And, yes, Pivotal also has the Big Data Suite of world-class data products. But the fact is, the cloud platform and those data products have largely been treated as two independent initiatives, more like two dancers who acknowledge that they are partners, but have been dancing on their own.
The experience I had with this customer a few months ago planted a seed, and in the weeks following that meeting, I talked with more customers and colleagues, and researched what was happening with data products in the industry.
After several months on a different assignment within Pivotal, I was delighted to rejoin the product team to focus on this challenge with like-minded peers. The astoundingly successful Pivotal Cloud Foundry platform forms the base into which we will deeply integrate data-centric capabilities, both from within Pivotal and through engagement with strategic partners. I see a Pivotal data team that partners with industry leaders who are pushing the envelope with the most innovative data products available in the market. And we look forward to taking the very logical next step continuing to shape the future compute platform with more data muscle.
The astoundingly successful Pivotal Cloud Foundry platform forms the base into which we will deeply integrate data-centric capabilities, both from within Pivotal and through engagement with strategic partners.
And it’s fantastic! It feels a great deal like the way “Platform-as-a-Service” felt three or four years ago: new and innovative technologies that are increasingly available, where IT leaders sense see the potential to disrupt and enable innovation. Enterprises will be looking for a path to evaluate and bring these data and other technologies in house, and learn how to leverage them to their advantage.
Businesses who have embraced cloud native application platforms continue to say that it is helping them unleash great value. Bringing data products into the platform in a deeply meaningful way will help businesses unlock even more value. Pivotal is not the only one to see that vision. Data is pretty much the new black for cloud native. But it’s an implementation issue that we can and are tackling.
I look forward to engaging with a great many of you in the broader tech community. Here are some conversations I’m looking to take part in:
I’m keen to hear from you.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
20 
20 claps
20 
How software is changing the way society and businesses are built.
Written by
Mom to Max, wife to Glen and Sr. Director of Technology at Pivotal, helping customers change the way they build software.
How software is changing the way society and businesses are built.
"
https://itnext.io/service-mesh-winning-ingredient-for-cloud-native-enterprises-10c714df7e8d?source=search_post---------95,NA
https://blog.heptio.com/cloud-native-part-4-79dc3875e03c?source=search_post---------96,"This is the fourth part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
There is quite a bit of excitement around containers. It is helpful to try to get to the root of why containers are exciting to so many folks. In my mind, there are three different reasons for this excitement:
Let’s look at each of these in turn.
First, containers provide a packaging mechanism. This allows the building of a system to be separated from the deployment of those systems. In addition, the artifacts/images that are built are much more portable across environments (dev, test, staging, prod) than more traditional approaches such as VM images. Finally, deployments become more atomic. Traditional configuration management systems (puppet, chef, salt, ansible) can easily leave systems in a half configured state that is hard to debug. It is also easy to have unintended version skew across machines without realizing it.
Second, containers can be lighter weight than full systems leading to increased resource utilization. This was the main driver when Google introduced cgroups — one of the core kernel technologies underlying containers. By sharing a kernel and allowing for much more fluid overcommit, containers can make it easier to “use every part of the cow.” Over time, expect to see much more sophisticated ways to balance the needs of containers cohabitating a single host without noisy neighbor issues.
Finally, many users view containers as a security boundary. While containers can be more secure than simple unix processes, care should be taken before viewing them as a hard security boundary. The security assurances provided by Linux namespaces may be appropriate for “soft” multi-tenancy where the workloads are semi-trusted but not appropriate for “hard” multi-tenancy where workloads are actively antagonistic.
There is ongoing work in multiple quarters to blur the lines between containers and VMs. Early research into systems like unikernels is interesting but won’t be ready for wide production for years yet.
While containers provide an easy way to achieve the goals above, they aren’t absolutely necessary. Netflix, for instance, has traditionally run a very modern stack (and is the AWS poster child) by packaging and using VM images similar to how others use containers.
While most of the original push around containers centered around managing the software on a single node in a more reliable and predictable way, the next step of this evolution is around clusters (also often known as orchestrators). Taking a number of nodes and binding them together with automated systems creates a new self service set of logical infrastructure for development and operations teams.
Clusters help eliminate ops drudgery.
Clusters help eliminate ops drudgery. With a container cluster we make computers take over the job of figuring out what workload should go on which machine. Clusters also silently fix things up when hardware fails in the middle of the night instead of paging someone.
The first thing that clusters do is enable the operations specialization (as described in part 3) that allows application ops to thrive as a separate discipline. By having a well defined cluster interface, application teams can concentrate on solving the problems that are immediate to the application itself.
The second benefit of clusters is that it makes it possible to launch and manage more services. This allows new architectures (via microsevices described in the next installment of this series) that can unlock velocity for development teams.
In the next part of this series we will look at how Cloud Native works to enable the microservices.
Heptio
15 
15 claps
15 
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://itnext.io/cloud-native-ci-cd-with-tekton-building-custom-tasks-663e63c1f4fb?source=search_post---------97,NA
https://medium.com/radiant-earth-insights/welcoming-new-collaborators-to-the-cloud-native-geospatial-ecosystem-c688d5f5f05?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
I wanted to share some background and more details about the Cloud Native Geospatial Outreach Day and Sprint (1 week away! September 8th — signup here), which has grown out of the SpatioTemporal Asset Catalog (STAC) community sprints. One of our goals for STAC has been to make it a truly collaborative community, and one that is welcoming as possible. We believe that the best standards are forged from diverse use cases and perspectives coming together and collaborating. We started with a gathering of 25 people from 14 organizations and have always sought to bring more people into the fold.
I think we’ve done well at bringing in diverse technology perspectives, but it’s bothered me that the majority of people who join our sprints all look the same. I’m not sure that we’re worse at inclusion than the general geospatial tech industry, but it is a goal of mine to do better in the communities I help lead and to play an active role in welcoming more truly diverse contributions to the broader geospatial & tech industries. I believe geospatial tech has the potential to help us tackle some of the biggest challenges that face us globally, and my experience has taught me we will not live up to that potential if the technology behind it is built from a narrow perspective.
So we are going to try out some experiments with this sprint, to try to do better. I’ll admit I’m a bit nervous about it not working out quite right, or getting something wrong, but I suppose that is the nature of an experiment, and that trying with a willingness to fail is the only way to get better.
One of the traditional ways we’ve used successfully to bring new people into our community is by using any extra sponsorship money to fund ‘travel grants’ to join our in-person sprints. These sprints have been the driving energy behind STAC. Every single new peak of activity on the spec occurred at a sprint:
And each brought in more contributors. But travel grants make little sense this time, as there’s no possibility of an in-person sprint during a global pandemic. So this is our latest thinking.
In each of the last couple of sprints, we’ve added more session to help people new to the STAC community get up to speed. These started as just an informal review of the spec with a long Q&A session that let people ask any question in a very friendly environment where ‘there are no bad questions’. But they have always presumed at least some background knowledge of geospatial and software. This time we are planning for some of our ‘Introductory Sessions’ to not require any background, introducing topics like Machine Learning on Imagery, and Remote Sensing, with lots of support for beginners. I wish I’d done more to organize this ahead of time to give people more time to prepare, but I’m really thankful for the great presenters who have signed up so far (and please let me know if you are interested in sharing your knowledge with new people, submit here or get in touch). Our plan is to record the content and make it available to all, and to iterate and improve in future sprints.
A big part of participating in open communities is actually contributing in a way that helps improve things. With STAC the bar to contribute has been pretty high, generally requiring deep geospatial and software development knowledge, to be able to edit the spec itself or to make software. But there is a myriad of ways that people can help the whole STAC and Cloud Native Geospatial Ecosystem, so for this sprint we’re working to try to make it much easier for new people to contribute in a meaningful way.
The first is that we are setting up Azavea’s Groundwork to enable anyone to help with data labeling of satellite imagery to create Machine Learning Labeled Training Data. The output is a STAC catalog, so everyone annotating clouds in the tool will be making meaningful contributions to the ecosystem. We’ll be organizing it into a ‘competition’, with awards for top contributors, and we’ll have more details about it soon, in a separate post.
Beyond that, we are aiming to write up all the ways that people can help transform geospatial data to be ‘Cloud Native’ so that it is easier to jump in and help out. Expanding beyond just STAC enables people to focus on Cloud Optimized GeoTIFF conversion, which has a more mature tool ecosystem. We’ll need help converting, upgrading, and creating STAC records, validating catalogs, testing with various tools, documenting where the data lives, creating tutorials, updating the website, and more. Our hope is that people will be able to use some of the skills they acquire in the Introductory Sessions to help out with further contributions.
The final experiment we’re doing in this sprint is to award a number of prizes to help encourage people to get involved and contribute, with some special categories to help welcome new contributions. This came about as usually we use any excess money for travel grants to bring new people into our community, but with a completely virtual event, we have no travel costs. This time we’ve had by far our largest response from sponsors and our core costs are much less. So we’ve been thinking about how to best use that money to further the Cloud Native Geospatial ecosystem, especially by bringing in more data, tools, and especially people into the fold. You can find the full details at on the Cloud Native Geospatial Award Details doc, but the overview is as follows:
Forward Funding Awards for Most Promising Contributions — We will have two $5000 awards that recognize the top projects moving the ecosystem forward during the sprint. One will be selected by the participants of the ‘software sprint’, and the other by a small panel of judges from the top sponsors. The goal will be for the money awarded to enable the project to continue on past the sprint, so the contributors can justify spending additional time on it.
‘Newcomer Grants’ — We’ve been brainstorming on how to best replace our traditional ‘travel grant’ with something that works during the pandemic. We’ve decided to award four ‘newcomer grants’ that include $1000 to help them continue their work in the community, as well as a full travel grant to the next in-person sprint. One is focused on new contributors from African countries, sponsored by Digital Earth Africa and the World Bank, and another two on newcomer women, one sponsored by Arturo and the other by SparkGeo & Planet.
Community Recognition — The next set of awards aims to recognize an array of contributions. We aim to have a nice custom STAC jacket (like Patagonia or Arcteryx) for top recognition. You can see the full list of awards; some of the highlights include:
We’ll also be making STAC-branded hooded sweatshirts for an additional 30 or so contributors, including everyone who delivers an ‘Intro Session’. And finally, anyone who makes a meaningful contribution to the Cloud Native Geospatial ecosystem will be awarded a t-shirt (with a soft cap of 200 to keep us within budget). See the award page for more information. And we do understand that not everyone values swag in the same way, so awardees will be able to select a gift card as an alternative ($200 for jackets, $50 for hoodies, $20 for t-shirts).
All of this is to say that we want you, yes you, to join us, even if you know nothing about STAC or Cloud Native Geospatial. We think there are some cool things happening, that have real potential to positively affect the world, and we want to share. The main day for new people is September 8th, and we hope you’ll join us to contribute during the sprint that follows — we promise to help anyone interested in contributing! Sign up at https://forms.gle/3Kx7cffMZov52MH88, and please share this with anyone you think might be interested. We can’t guarantee that everything in the sprint will be a smooth introduction to our world, but we’ll do our best to meet you more than halfway. And you will hopefully help teach us how to do this better in the future.
Earth Imagery for Impact
94 
Thanks to Sara Safavi. 
94 claps
94 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Helping the global development community navigate the Machine Learning and Earth observation marketplace and innovations taking place.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/top-9-cloud-native-startups-to-watch-in-2021-1aac4dfed577?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
"
https://itnext.io/why-cloud-native-3d1ef7cfa6bc?source=search_post---------100,NA
https://medium.com/none-size-fits-it-all/cloud-native-host-operating-systems-d61213141bbd?source=search_post---------101,"There are currently no responses for this story.
Be the first to respond.
Container orchestration systems, typically running on a couple of machines, require a local operating system that is responsible for managing a single machine.
Those local operating systems are usually called host operating system and the main requirement is that they support a container runtime. These days this means almost always Docker with increasing demand for rkt and runc. The container runtime can either be provided natively as the case with CoreOS Linux or Red Hat Atomic Linux or as a post-hoc.
The following matrix provides an overview of what host operating system can be used with which container orchestration system:
Note 1: I didn’t include Nomad in the matrix since it provides a generic Linux binary for installation. Also, I didn’t include Triton DataCenter with its OpenSolaris-based SmartOS host operating system since a place in the matrix doesn’t make sense.
Note 2: once Windows becomes a first-class, production-grade host operating system, I will include it here as well.
Note 3: the sources used for above matrix were the following:
Note 4: a couple of potential candidates for container orchestration systems I didn’t include were Shipyard (looks sorta hibernating to me), Panamax (I believe discontinued), and ContainerShip (not sure about its future but seems like a single-person effort).
If you have other or more up-to-date information I appreciate it if you leave a comment here and I’ll update the matrix.
Discussions of tools we use to explore, understand, and…
11 
2
Some rights reserved

11 claps
11 
2
Written by
Solution Engineering Lead in the AWS open source observability service team
Discussions of tools we use to explore, understand, and troubleshoot software
Written by
Solution Engineering Lead in the AWS open source observability service team
Discussions of tools we use to explore, understand, and troubleshoot software
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.openebs.io/cloud-native-storage-vs-marketers-doing-cloud-washing-c936089c2b58?source=search_post---------102,NA
https://medium.com/adobetech/meet-the-adobe-i-o-team-meryll-blanchet-on-cloud-native-applications-862b55a393ee?source=search_post---------103,"There are currently no responses for this story.
Be the first to respond.
Engineering manager Meryll Blanchet leads engineering on a new Adobe I/O project codenamed CNA, which stands for Cloud Native Applications. The CNA team is working on an SDK as part of I/O Runtime to enable developers to create fully-fledged UI applications on top of Adobe Experience Platform with all the benefits of serverless capabilities.
Here he gives us an exclusive insight into the project, the tools the SDK and sample apps are built on, his proudest achievements, and a sneak peek at some exciting developments that will be announced at Adobe Summit this month.
I started my career back in 2006 at Novartis, one of the worldwide leaders in the pharmaceutical industry, with a six-month internship that concluded my Master’s in Computer Sciences and Software Engineering. I was responsible for building Java portlets, which would integrate internal solutions into the knowledge portal for the drugs development business unit.
I then spent the next five years designing and implementing web solutions for big logos as a software engineer for Capgemini’s delivery services entity in Basel. I joined Adobe Switzerland in 2011, initially as an AEM consultant, and then evolved into an architect role. Until last year, I had been collaborating with many of our customers and partners on various projects: from good old technical implementations of CMSs to larger and more complex multi-solution digital transformation programs.
I then took the leadership of consulting engagements, also supporting license and services pre-sales phases, and finally had the privilege to co-lead the talented AEM consulting team in France. Last year, I accepted the challenge and joined the Adobe I/O team as engineering manager for the project, codenamed CNA.
The project started last December, and we are a team of seven engineers between Basel and Bangalore, who partner with our product manager Sarah Xu in the U.S.
We’re aiming to create a CNA developer community and provide them a best-in-class experience from various angles: extensibility of our solutions, development simplicity and velocity, tooling, and guidance via documented best practices and shared sample apps.
Our SDK will come with ‘all-batteries included’ to maximize the developer’s immediate productivity. This requires a lot of joint reflections, also involving the other I/O teams. We strive to put ourselves in the shoes of our future users: as a customer IT or integration partner team, what would I need to have to work on Adobe-related projects in the best conditions and deliver outstanding apps for my own customers in a minimal timeframe?
Our SDK comes with both back-end and front-end features. The back-end pieces are made of npm libraries that are used by I/O Runtime actions and sequences, so server-side JavaScript in both cases.
We have chosen to build our sample CNAs on top of React. But developers will be free to use their favorite front-end library for their own apps.
We have a few open source dependencies, such as Parcel to build the UI, and Express to facilitate local development.
Last but not least, our CI/CD is based on Jenkins, and we use Mocha for our unit tests. We also plan to integrate with the rest of the I/O ecosystem like CLI, Console, or Events.
To deliver incremental value with this I/O initiative, codenamed CNA, we need to understand and prioritize our efforts accordingly: what are the first building blocks that will be helpful for CNA developers? Where are the field priorities and can we identify recurring use-cases that we could help to solve?
We work in parallel on a broader vision in order to keep the SDK on track in terms of completeness and consistency with the roadmap of Adobe Experience Platform and its underlying solutions and technologies.
Another challenge is our heterogenous level of expertise with the Adobe solutions. For example, some of us have more experience with the Creative Cloud, while others know the Experience Cloud well. We organize regular knowledge sessions with SMEs to get comprehensive overviews of the solutions and build relationships with other internal engineering teams who can help us with topics requiring deep product expertise.
I am extremely proud of our great team spirit! We had our first hackathon in Bangalore in December, and it was amazing to see all of us help each other while keeping the fun. No one could have guessed that we had just met in person for the first time. I strongly believe that this spirit is the key to overcome our future challenges.
We have created two great demos showcasing CNA capabilities in both XC and CC worlds. On one hand, a multi-solution dashboard consolidating and displaying marketing campaign data from Analytics, Target and Campaign Standard. On the other, a Bridge plugin automating multiple CC services from Lightroom, Sensei and Photoshop, which can then be applied to a large bulk of raw images.
We are currently finalizing our Project Starter: a project skeleton combined to utilities for developers to create a new CNA from scratch, test it locally, and deploy it to Runtime in a matter of minutes.
“Leave your comfort zone.”
Behind any new challenge there’s always the opportunity to define new objectives, learn, and develop yourself. I did it myself by joining the I/O team after many years on the Experience Cloud consulting side and can’t believe how much I am learning, and how much I still have to learn from the whole I/O team.
This philosophy can also be followed on a daily basis: a back-end developer can try to build a front-end, while an XC solution specialist can write CC plugins.
That’s why for each sprint, I like to encourage another engineer to volunteer and run the whole demo. Our backlog stories are various: microservices, UI, research, etc.
Our team size and distribution perfectly allow daily exchanges and ongoing communication. I am convinced that everyone can get the right visibility and knowledge about the several aspects of our CNA project that way.
We are currently working on multiple IMS authentication scenarios for our SDK — an important milestone, as it will provide developers with the authentication support to then call our solutions APIs.
We will also focus on some specific solution extensibility topics in the coming months and collaborate with the I/O Tooling team to bring our CNA Project starter along with AIO CLI.
We’re excited that our team will be hosting an I/O Runtime lab, in which participants will get a first flavor of our work around CNAs.
We actually aim to open source everything. We will start with the CNA Project Starter and can’t wait to get in touch with developers from the field, hear their feedback, and receive their pull requests!
Adobe Summit takes place in Las Vegas from March 26–28. Follow the Adobe Tech Blog for more developer stories and resources, and check out Adobe I/O on Twitter for the latest news and developer products.
News, updates, and thoughts related to Adobe, developers…
37 
37 claps
37 
News, updates, and thoughts related to Adobe, developers, and technology.
Written by
Independent editor and content consultant. Founder and captain of @pixelpioneers. Co-founder and curator of GenerateConf. Former editor of @netmag.
News, updates, and thoughts related to Adobe, developers, and technology.
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-9cfabf2dd389?source=search_post---------104,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Nov 28, 2019·4 min read
So often the first task my customers ask me to work with them on is to ‘cloud-scale’ their computational complex tool or data pipeline. I find that this approach can be premature. I like to first ask…
“How could I try it out on the cloud?”
In this multi-part series, I’ll discuss how my team has collaborated with bioinformatics researchers world-wide to answer this question using cloud-native services, tools and patterns.
Because we have been working with bioinformatics researchers, we often are asked to scale open source tools and libraries which they have developed as a part of their work. However, this is not always the case — as the bioinformatics industry matures, standardized analysis tools, such as The Broad Institute’s open source GATK (Genome Analysis) toolkit are gaining more adoption. In the latter case, the significant ‘Hello World’ code is that of tool configuration values, rather than the tool source code.
Also researchers are usually working toward eventual publication, so a key goal is to create testable systems that are fully reproducible by other researchers. Those researchers are often working on different teams at different locations globally.
The first task my team performs in these situations is to attempt to use the tool or library locally for a ‘hello-world’ scale test analysis. This is often an arduous process, requiring installation of SDKs, languages, libraries, etc…
50%+ of the time we stop after 1+ hours of installation attempts
We also often discover issues with analysis data. These are not limited to, but have included the following:
We want to experience the potential pain of local installation first (and discover the ‘data situation’), so that we can be confident as we work to build a better alternative — Hello World using cloud tools and services.
We define this as an example that can be set up and run in less than 15 minutes. Note that we include setup time in the test goal time.
The ideal test time is less than 5 minutes.
In order to accomplish set up with the fewest steps, we recommend using known cloud services and patterns, such as vendor (or 3rd party) templates, rather than scripts. Also we setup suggested baseline security best practices, such as individual user accounts via IAM Roles for AWS testing, etc…
Often, we find there has been no time on the research team to consider using any of these tools or patterns — researchers are focused on conducting their research first and publishing it second. When the size of their analysis ‘no longer runs on my machine’ or ‘takes too long on the HPC cluster’, researchers start to look to the public cloud as a potential solution. Then, and only then, do they think about environment reproducibility.
This approach has had some unfortunate consequence. One study found that 70% of reviewed research was not reproduced computationally and was, therefore, not useful.
Building one or more cloud-native Hello World implementations early not only provides the advantage of enabling collaboration with other teams earlier in research, but also give the team important practice learning cloud services. Because the goal at this time is usability and not scalability, it’s a low cost situation, that allows the team to make mistakes cheaply (in terms of time to run and cloud service costs) as they learn.
This approach helps the team to ‘skill up’, so that when they want to scale up for actual research, they already have cloud basics (such as understanding security groups, IAM roles, etc…) in place. Also it allows them to share their scripts, tools and/or tool configurations outside of their group, thus facilitating more collaboration even earlier in their work.
Creating reproducible cloud infrastructure is a ‘solved problem’ in that all major cloud vendors, and also several third party companies [such as Terraform] provide templates to define and create cloud infrastructure.
Going beyond templates to use vendor services to enable even faster setup, for example by hosting templates with metadata using the AWS Marketplace is an advanced example of this type of pattern.
However, my team has found that the bioinformatics research industry, being mostly relatively new to using cloud services, has a rather large learning curve to get from clicking on the AWS console to start EC2 instances to get to using CloudFormation templates which are source controlled.
A key pattern is to work with the research teams to build Hello World-scale, cloud-native implementations first.
In the next part of this series, we’ll examine a couple of examples in depth. The first of which will be CSIRO’s VariantSpark library running on the AWS Marketplace. As a preview, the architecture is shown below.
Cloud Architect who codes, Angel Investor
72 
72 
72 
Cloud Architect who codes, Angel Investor
"
https://medium.com/geekculture/how-to-scan-for-vulnerabilities-in-cloud-native-applications-eeaf5b66e1de?source=search_post---------105,"There are currently no responses for this story.
Be the first to respond.
The article is originally published on ‘Container Journal’
On the road to embracing DevOps, many IT organizations still depend on traditional security practices, policies, and tools that were not built to withstand the modern cloud-native approaches of scaling and complexity. With less attention paid to security, organizations fail to transform themselves in this rapidly-changing digital world. Recent surveys and researchers have found how important security has become in the software development lifecycle that was may have been ignored for years as the “security team’s problem.”
The emergence of DevSecOps has helped organizations to shift security left, but is that enough? Organizations have barely started to understand the complexity and security threats associated with their cloud-native journey. It is highly recommended to use modern cloud-native best practices and tools to tackle vulnerabilities and threats found in the SDLC.
Approximately ten years ago, the word cloud-native was coined by businesses like Netflix and Amazon. They leveraged modern cloud practices, tools, and technologies. For many companies, cloud-native means innovating, reinventing, transforming the way we do software development.
Cloud-native software applications employ microservices deployed within lightweight containers that use low-overhead orchestration, runtime, and networking services. Cloud-native applications leverage cloud compute frameworks and infrastructures and encourage speeding time-to-market.
These applications use modern cloud practices like immutable infrastructure, Infrastructure as Code (IaC), containers and container registries, service meshes, declarative, and APIs.
Image source: DZone
There are multiple steps a company can take to begin and progress on this journey. Cloud-native’s fundamental principles include scalable apps, resilient architectures, and the ability to make frequent changes.
Three phases to mention in the journey.
Phase I > Developer Focus > Container Adoption
Phase II > DevOps Focus > Application Deployment
Phase III > Business Focus (end-to-end)> Intelligent Operations
Example of a modern cloud-native application stack
Image source: The Linux Foundation
Fundamentally at the lower layer, you will have your typical access aspects of it, the load balancers, either your network load balancers or application load balancers. Then you have a large number of subnets where you have deployed all your hosts, on top of which we can actually deploy either managed or self-hosted Kubernetes: The Kubernetes orchestrator for our container deployments.
We also need storage, whether it’s databases or cloud storage. So once we have all these artifacts, we deploy our container orchestrators. One of the main avenues through which we can leverage and configure and deploy applications on the orchestrator is using the orchestrator API.
The orchestrator exposes the API server and a very rich set of functionalities which the clients then leverage to perform various actions on the orchestrator. Now, the next aspect is that we want to make sure that we have isolation, and one of the aspects that are facilitated in a Kubernetes environment is using Name Spaces. These Namespaces will finally deploy the application pods. So, all these different artifacts are what comprise the new cloud-native application stack.
While shifting security “left” is getting huge attention these days, it is highly recommended to have security integrated throughout the software development life cycle, and having security checkpoints at each stage is considered more effective. Shift-left in security means prioritizing security very early in the development life cycle and making it (security) as everyone’s job. This way, any vulnerabilities that can impact the software delivery and bottlenecks in production can be prevented.
O’Reilly’s survey on “How Companies Adopt and Apply Cloud Native Infrastructure” report of 590 security practitioners, DevOps managers, and CxOs from across the world found that Security and compliance barriers among the top challenges for cloud-native adoption.
Image source: HELPNETSECURITY
Cloud-native security addresses the security concerns involved.
Cloud-native security forces the point of re-focusing on security and shifting the security left in the SDLC. Cloud-native applications must be secured for a successful digital transformation journey. Thus cloud-native security ensures that vulnerabilities are detected, identified, and remediated at the right time in the SDLC. This is where we recall the DevSecOps approach, baking security throughout the software development life cycle.
Image source: DZone
Cloud-native security acts as a gatekeeper and a guard for all the security vulnerabilities that might enter your software flow.
Image source: Kubernetes.io
The cloud is regarded as the base of the security layers. Steps must be taken at the cloud level since developers cannot configure application security at the code level. It is all about running secure workloads in the respective cloud provider’s environment.
After cloud, comes the cluster layer, and Kubernetes is considered as the de facto orchestration tool. When using Kubernetes, there are certain things to consider — RBAC, Pod security and network policies, secret management, logging, and monitoring.
This layer talks about container security management and best practices.
When the applications are built inside a container, there are certain security best practices to follow. First, avoid running privileged containers. Most applications don’t need root access to operate, besides for system containers like monitoring or logging agents. This should prevent an invader from getting root access to the container and access the host node.
The last C in the cloud-native security layer is code. Strengthening security into an application’s code is one of the best practices in DevSecOps.
It all starts with the source code. By catching security vulnerabilities early in the software development life cycle (SDLC), companies can save developers time, cost, and effort.
One best policy to restrict the vulnerabilities in your code is to use tools built just for this purpose — something like JFrog’s Xray.
Image source: Checkpoint
A typical software development flow will have the following steps: Developer develops the code/software, tests locally in his/her machine, then commits code to the version control system used; the CI/CD tool takes the code, builds it, and then pushes it to the Docker compose which also builds a container utilizing images and packages from public repositories. And then places it into the registries; after the step of successful staging, the container proceeds to production. All these steps pose security risks at each point and hence must be taken care of.
The workflow might have code with vulnerabilities, libraries, and images downloaded from unknown sources, license-related issues, etc. Hence, as a DevSecOps best practice, it is recommended to have regular checkpoints at each stage of the development workflow along with the cloud-native security tools.
Google cloud has put up a great table of implied requirements for security in moving to a cloud-native architecture.
Image source: Google Cloud
There will usually be many dependency layers in an application, from their code to production journey.
For example, when the application is written in Java, you may have Maven dependencies. The next step would be running this application on Linux, and then you will have dependencies in Debian repositories. The next is wrapping it in Docker because you want to have it on Kubernetes, and then you will have Docker repositories immediately. The Kubernetes universe will have repositories like Helm center and so on.
Cloud-native enterprises make use of a dedicated place to manage and store all these dependencies, binaries, and libraries, and it is Artifactory. It has all the dependencies and libraries from trusted sources. Using Artifactory makes it easy for developers to have a single source of truth while developing, and any dependencies can be easily scanned with this tool called Xray. Artifactory and Xray both are connected and hence feature a full-blown security tool when working with cloud-native applications.
Security has come a long way, and it is here to stay. While DevOps focuses on speed and agility, cloud-native security focuses on the security aspect throughout the cloud-based SDLC. Combining both speed and security, enterprises can easily achieve their desired digital transformation journey. With the increasing adoption of cloud-native principles, tools, and platforms, there is a chance for security risks, which can be mitigated using security tools and DevSecOps principles. Employ the best-in-class tools to detect and analyze vulnerabilities, have integrated security checkpoints across the SDLC, and make security everyone’s job. Let’s keep hackers and attackers away from our systems.
Hope you liked the article. For more such cloud-native articles, follow Pavan on Twitter.
Proud to geek out. Follow to join our +1.5M monthly readers.
26 
26 claps
26 
A new tech publication by Start it up (https://medium.com/swlh).
Written by
DevOps Enthusiast! Currently working at Harness as a developer advocate.
A new tech publication by Start it up (https://medium.com/swlh).
"
https://itnext.io/cloud-native-predictions-for-2020-20aa6a2fec01?source=search_post---------106,NA
https://medium.com/albaraka-tech-global/java-spring-boot-cloud-native-uygulama-microservice-a63694635ab9?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
Spring framework içersinde farkı java bileşenlerini bulunduran bir çatıdır. Java’nın open source olarak geliştirilmesi sebebi ile bir birinden ayrık bir çok tool, kütüphane ve araçlar türemiştir. Bunlardan biri de spring boot framework’dür.
Özellikle içinde bulundurduğu bileşenler sayesinde microservice yapılarında sıkça kullanılmaktadır.
Spring boot içinde bulunan ve sıkça kullanılan microservice bileşenlerinden bazıları şunlardır;
Config Server: Yazdığımız microservice’lerin config’lerini ortak bir yerden yönetmemizi sağlar.
Service Registry: Host ettiğimiz microservice’leri startup ‘da service registry’e kayıt ederek http akışlarını ve ayağa kaldırdığımız servislerin url ve port bilgilerini kullanarak daha rahat route etmemizi sağlayan yapıdır.
Api Gateway: Microservice mimari yapılarının olmazsa olmaz bileşenlerinden biridir. Microservice yapılarına gelen requestleri ilgili servislere route eden , auth kontrolü , loging v.s. kabiliyetleri sunan bir bileşendir.
Detaylı bilgiye resmi sitesinden ulaşlabilirsiniz: https://spring.io/microservices
Spring boot üzerinde örnek bir microservice oluşturmak için aşağıdaki siteye girelim.
https://start.spring.io/
Yeni bir spring blank projesi oluşturalım. Maven veya gradle paket yöneticilerinden birini seçerek proje group, artifact, name, description, package name bilgilerini girelim. Bir rest api projesi oluşturacağımız için dependencies bölümünden web paketini ekleyelim.
İndirdiğimiz zipin içinden çıkan maven projemizi Eclipse veya farklı bir ide açarak existing import edelim.
Ardından src/main/java folder altına iki tane paket ekleyelim com.demoaplication.Controller ve com.demoaplication.Model
Model paketimizin altına bir tane model tanımlayalım.
Ardından controller paketinin altında TicketController.java dosyası ekleyerek aşağıdaki kodu ekleyelim.
Bir tane ticket objesi oluşturarak return ediyoruz.
Evet uygulama bu kadar kısa şimdi projemizi çalıştırarak. http://localhost:8080/ticket adresine get isteği atalım ve aşağıdaki response çıktısını görelim.
Tabiki bu uygulamamızı çok basit bir şekilde kurguladık. Katmanlı mimariye uygun olarak controller, service, repostory, model, utils katmanlarını da araştırmanızı öneririm.
Bir sonraki yazımızda görüşmek dileğiyle. Hoşçakalın..
Repo:
github.com
Yenilikçi Finansal TeknolojilerΩ
30 
30 claps
30 
Yenilikçi teknolojiler ve finans teknolojileri
Written by
Full Stack Developer
Yenilikçi teknolojiler ve finans teknolojileri
"
https://faun.pub/what-is-a-cloud-native-database-24fe917eee67?source=search_post---------108,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/memory-leak/a-conversation-with-cloud-native-thought-leaders-f28aea0c736?source=search_post---------109,"There are currently no responses for this story.
Be the first to respond.
I recently had the pleasure of hosting Redpoint’s second annual Cloud Native Summit in partnership with the CNCF and our friends at Amplify Partners. During the event I moderated a panel with technical leaders including Edith Harbaugh (CEO of LaunchDarkly), Preeti Somal (VP of Engineering at Hashicorp), Matt Klein (creator of Envoy), and Austen Collins (CEO of Serverless Inc).
Themes during the discussion:
1) Serverless is early and growing in popularity
2) The industry is transitioning to workflows that tie together solutions that solve specific challenges
3) Feature flags and service meshes are complementary technologies along a continuum
4) Similarly, service meshes and functions interoperate and Solo.io’s Gloo functional-level gateway helps take advantage of both solutions
5) Devops positively impacts business outcomes but more work needs to be done around cultural best practices and bridging the infrastructure and product teams
I’m excited to share an edited transcript for those who were not able to attend the event.
Astasia: Austen, you’ve been working in the Serverless ecosystem for a while now, almost since the beginning. What’s real? What’s hype? What’s actually going on?
[laughter]
Austen: That’s a fantastic question. I was just looking at this, and I remember back in 2015 when I saw Amazon come out with Lambda, I thought, “Wow, as a developer, this is the compute service I’ve always wanted. Auto-scaling, rapid execution, event-driven microservices. Sounds like a dream. I was trying to go around telling people in my network this is going to be the future of compute on the cloud. This is everything. This is a convergence of all the great ideas of our time. I built this project called the Serverless framework to help other people think this way and build applications on this new computer service. I remember posting it to Hacker News and my first comment was, ‘This is a horrible idea.’
[laughter]
Astasia: That’s what they post for everything.
Austen: Yes. That’s certainly the default, like, ‘Thanks for posting to Hacker News. Welcome to the community. Your post is horrible.’
[laughter]
Now I’m looking at this and this is this is such a trip. It’s just wild seeing all this growth. Look, we work with a ton of companies that have built amazing stuff using a Serverless architecture. Is it hype to them? No. Is it hype to a lot of people who are really skeptical about all this stuff? It could be. As far as what’s real, what’s not real, some things that I’ve seen from the vantage point of the service framework are, when you reduce overhead, you liberate productivity and you liberate innovation, and that is really, really cool. We see small teams provisioning hundreds of these Lambda functions.
In fact, Nordstrom, they have a pretty awesome Serverless team over there. They’ve been one of the biggest contributors to our framework. I know that they provision like well over 100 functions or something, and they’re using them for all types of things. It’s not just the amount of productivity that they’ve been able to reach, but the use cases that they have. Serverless isn’t perfect for all types of things, of course. When you have that compute service that has all those qualities, people just want to stick a whole bunch of stuff in it because they want those qualities and they want to build systems that have lower overhead.
They’ve been building a whole bunch of stuff. They have projects to where you deploy Lambda function. They have like five other Lambda functions that will go interrogated with different tests, all types of cool use cases like that. That feels very real, liberating that productivity in that innovation.
Up next, enterprise usage. Seeing it being adopted by these companies like Nordstrom and Coke. FINRA processes 75 billion events a day, all stock trade, equity trades and option trades. They’re using a lot of Lambda for that now. I think that that’s pretty awesome, 75 billion events. I’m not sure what the percentages of that the processing of those, but when I chatted with them, it was significant, and that feels pretty real to me.
Also the fact that about 20%-25% of our Serverless framework users have never used a public cloud provider before. This is their first time using this stuff. I think the serverless stuff minimizes the operational burden. The surface area of concerns is also much smaller and it’s almost like we see this new diet version of the cloud, like a light version of the cloud. It’s like the more accessible flavor. What we see a lot of, is people adopting cloud like via Serverless architectures because it’s so much more accessible and a lot of enterprises that want to do cloud development, one of their big challenges is just getting their team up to speed on these new technologies, and Serverless makes that really easy.
I’m also a big fan of seeing people who aren’t engineers, aren’t traditional developers become developers through service architectures especially. There’s a whole bunch of people out there who have great ideas but they just they’re intimidated by software as we’ve been doing it.
All that stuff feels very real to me. The Serverless open source platforms on Kubernetes bringing portability to functions, I think that’s great. Bringing these options, this experience, this architectural pattern to people who can’t use public cloud, I think that that’s very real and that’s very awesome.
What’s not real? Portability is going to be a challenge in the Serverless world. We’re thinking about it the same way we have coming from the container era. But with Serverless, especially the architectural pattern, it’s not just about the compute, but it’s about the other infrastructure that you’re using with that compute to make some type of outcome whether it’s the database or some storage system. It needs to also have Serverless qualities, otherwise it’s not going to scale, it’s not going to fit nicely in that model.
As well as the event driven plumbing too, that’s also fairly complicated, and the idea that you’re going to be able to move this stuff really easily, is a big challenge. This is a huge opportunity for the CNCF here. We have the Serverless working group. I attend every single one of those calls and we’re working on standardizing a lot of these concepts, and we could chip away at this problem. We can make a lot of progress here. That’s definitely a challenge.
Migrations into Serverless architectures, this is also a bit of a challenge. It’s a lot for companies to take on once. First off, it’s micro-services. You’re deploying all your logic in these small independent units of deployment, and that alone is a big transition for a lot of companies. Moving microservices to writing things as functions thinking more about event-driven computing, all that stuff, that’s a bit of a challenge.
What’s not real is that this technology is going to take over everything. This is a not silver bullet. It is not going to win everything. That’s never real. That’s what I see.
Astasia: Awesome. Thank you. Preeti, at Hashi, you guys meet with a lot of enterprise customers. Are you seeing them use any Serverless functions today?
Preeti: I think I’m going to be the contrarian view on this. A little bit, but honestly as we are out there talking with customers, especially outside the valley, they’re still trying to figure out how to use the cloud, and how to think about applications in a way where it’s not just a lift and shift. The classic story we hear over and over again is, yes, there was a mandate to use public cloud and yes we moved this application but then we got this massive bill. The CFO stepped in and wait, just a lift and shift wasn’t the right approach. What we’re really seeing is certainly enterprises want to get that agility, want to get that operability. And of course, you have people who manage the service that developers don’t have to get exposed to. But they’re still trying to figure out how to get there. Increasingly, it’s about figuring out which projects are the right ones to start with.
Just take security, for instance. A lot of times, people don’t know which services are talking to which services, and somebody left the company, and this is the person that was pushing the firewall rules or the network requirements. How do you unwind all of this? There’s still a ton of work to do.
Astasia: It sounds like here in the Bay Area, we’re more on the bleeding edge and maybe we’re seeing more than some of the enterprise customers that you’re dealing with today at Hashi.
Preeti: Yes.
Astasia: Matt, as someone at one of these thought leader bleeding edge companies, Lyft, with a strong platform infrastructure team, are you guys doing Serverless? What’s the use case if you are?
Matt: No.
[laughter]
Astasia: You’re not? Dig into that. That’s great.
Matt: Let’s see. We have a couple of non-critical jobs that we run on Lambda. But I would say that any Serverless that we do is — it’s a rounding error on our bill. I’m a firm believer that if you look out like 5, 10, 15 years from now, I do believe that we’re going to move into the Serverless world. But what I typically tell people today, just to echo what was said before, is that we can barely run containers today. If you look at networking, if you look at observability, if you look at how people deploy all the tooling, it’s absolutely horrendous. If you look at what is required, in my opinion, to do a real-time system using Serverless, it’s probably an order of magnitude harder problem. You’re not just dealing with more rapid auto-scaling. You’re dealing with a typology that’s always changing.
I think that we will get there, but we are years and years out, and I think we are much better served by fixing what we have today. I think batch will probably move towards Serverless first because that’s something that isn’t latency sensitive. I think we’ll see that move. But I think Serverless is a buzz word distraction today. I think that we should move towards it, but I think we have a lot of problems that we need to solve first.
Astasia: Edith, it looks like you have something to say.
Edith: It’s funny. I wrote an article two years ago where it says Serverless is the new electricity. What the Pinterest speaker said was awesome where he’s like developers just really care about their own experience. The underpinnings of how this gets out, they just want to ship. I think I still believe in the promise of Serverless. But I also agree with you that there’s a lot of steps to get there first.
Matt: Totally.
Edith: I think it’s taken longer than we expected just because there’s still a lot of people that are just moving out of their own data centers.
Matt: Well, it’s that, but it’s also that just like the way that we run infrastructure today, it still is just too complicated. People are still doing YAML and all of these things. We have a very long way to go, I think.
Edith: I do believe in the long term. I think a developer just wants stuff to work.
Astasia: If we were going to ask the panel raise of hands, do you think Serverless and functions is going to be as big as containers in the next three years?
No.
[laughter]
Austen: It feels like a setup.
[laughter]
This question around the Serverless versus containers is a kind of a strange one to me, actually. I’ve always felt that it doesn’t make any sense to me. I feel like there are this kind of awkward collision course, actually. I wish that Lambda used containers. It would solve so many problems in kind of the development phase, the build phase, all that stuff. We’re breaking our back trying to emulate kind of the Lambda experience without containers. It’s a pain. To think of these things as separate, I think, no. I think that they’re kind of heading in the same direction, and the developers have spoken. It’s clear. They said we had this kind of cultural awakening a couple of years ago. They’re like, “This is what we want.” You’re right, it’s going to take a while to get there, right?
That fact that it has gotten so far. If you really look at the options that you have on Amazon, for example, to build a Serverless architecture, it’s very few. The fact that people are making so many use cases with these few options are like, I guess we’ll try and use DynamoDB for that. Again, even though this is like the worst database to use for it, they’re still trying to make it work because they want this stuff so bad. This is how it starts always, people are just using it for these one little, one small tasks or something like that. This is exactly how we see adoption happen. A developer brings it into their org and they’re like, “I’m just going to automate this one thing that I don’t want to do anymore with a function.” As soon as they do that and they feel that magic moment, then their mind expands and think, “What else can I do with this stuff?”
But you’re right, it’s early. We’ve got a long way to go here. But at the end of the day, I think developers have spoken, like this is what we want. I do think containers could help a lot of that experience. The idea of these things are antagonistic, I don’t quite understand. I’m looking forward for them to just kind of get along and get on with it.
Astasia: Well, I think it’s clear that there is appetite in the market. All the different open source solutions, on the landscape that’s provided, is an example of that. I think I really enjoyed the community survey piece that Serverless Inc. put out last week. It showed that businesses are most interested in operationalizing serverless. The main pain points were around debugging, testing, and observabilities. It sounds like there is momentum, and we’re starting to see that by the new pain points that are coming to fruition now.
Switching gears a little bit, one thing that’s been interesting to me is the co-evolution of HashiCorp and the Kubernetes community. As Kubernetes is being championed by the CNCF. Preeti, it’d be great to have you speak about the work HashiCorp is doing with its solutions suite, and reconcile that with the parallel work around Kubernetes.
Preeti: Yes, sure. I think it is definitely worth reiterating that HashiCorp’s roots are in open source. The co-founders are practitioners that have been part of the open source community and really for me coming in and being here for six months, the focus on solving customers’ problems today from a point of view of workflows, not technology, resonated.
If you look at our website and internally, what we live and breathe is we’re creating tools that solve workflows that customers need, and we deliver in a multi-cloud hybrid world. We know every single enterprise customer is multi-cloud, whether you define cloud as something that somebody else is running or something that you were running as a Kubernetes cluster or physical or whatever. For us, it’s about as customers are moving to this cloud operating model, cloud-native model, what are the challenges? How do you think about security in a dynamic world? You can no longer think about security as your perimeter and nobody enters it and my machines are all secure. You have to think about your applications moving and security being dynamic. How do you think about provisioning and being able to apply some of the policy and governance pieces as your team of provisioning?
Again, for us, it’s about tools that help solve those problems and work across a variety of technologies that exist within the customer’s base. More concretely with Kubernetes, for instance, we have a telephone provider that does provisioning for Kubernetes. We’re doing some work where Walt and Consul can be used with Kubernetes more seamlessly. So it’s another proof point of listening to the community, understanding what they need, and then being able to solve those problems.
Astasia: Great. Lots of work around workflows. Within the cloud-native community, we’re also hearing a lot about Gitops and Git-centric workflows. Matt, are there changes to the SDLC or processes that businesses need to go through to adopt cloud-native?
Matt: Yes, for sure. That’s a tough one. We’re moving towards this whole idea of doing “DevOps”. I do that in quotes because that means different things to different people. What it typically mostly means is that we have a lot of developers who aren’t quite as experienced operating systems who now are being told to go on and they have to deploy software and they have to monitor software and do that type of stuff. It’s been interesting to see that the whole Git-approach to software, and for those that don’t know that’s just the idea that you can keep your configs in Git and it’s version controlled and all that stuff.
To be perfectly honest, using Git in that way, it’s a hack because we don’t have better systems. By better systems, I mean, I’m a firm believer that we need custom built user experiences for people who are doing deploys and monitoring and all of the tools that people have to do for doing DevOps. We don’t have those today because historically in infrastructure, we have not hired or invested in the design skills to actually develop those tools. People are stuck using git and GitHub because it’s the second best thing that we have, which is something that’s a version control has a UI, I can go and do code review.
In the future, people’s experience with editing config files, whether be YAML or JSON or something else, is really quite bad. It’s quite error-prone. They’re just using GitHub because it has a code review UI and it has a version control and you can revert things because the only reason that’s being used. People are basically hacking around it. They’re using slack bots or they’re using GitHub. These are all hacks. In the future, we will hopefully have better custom UIs that people can use to do their actual DevOps experiences.
Astasia: In line with some of the advancements that we’ve seen in SDLC, one component of that is feature flagging. I’d love Edith to talk about feature flags, the motivation behind why people use them, and start to dig into whether feature flags are same as service meshes.
Edith: Yes, well, there’s a bunch of questions there. Let me start at the beginning. Feature flags are an industry best practice where you are turning sections of code on and off after deployment. Martin Fowler really popularized this. You can push out code and then selectively turn it on for people. The real gain then is that it opens up this whole new universe. You can ship something, if it’s doing poorly, you can just turn it off. I’ve talked to people all over the world about how fast their deployments are. I’ve heard everything from an agonizing 14 hours, which is awful. That’s the number they’ll meet to me. I know it’s actually longer so even like a super fast deploy takes a couple of minutes. That’s a lifetime if something’s going wrong in the field.
People love feature flags because they could say this isn’t working. Let’s turn it off. The alternative that I lived through without feature flags was this isn’t working. Let me figure out in a very tight time pressure how to fix it, test it, deploy it, and then find out that I actually made this far worse, which has happened to me many, many times when under stress.
Then there’s also this whole other world of, okay, once you have this freedom, you can selectively deploy something, you can then do a lot of really interesting in software development lifecycle things. We’re saying, like, okay, developer, you built it. Product manager, you get to pick who gets it, which takes away, I’d say 90% of fights in software development, not 100%, but 90%. Feature flagging lets developers focus on building. If somebody’s not working, give somebody else the control to do it. Then also let product and marketing and sales really control the access level controls.
We don’t see this as same as a service mesh at all. We see service mesh as a really complementary thing. A service mesh is much more about discovery and making sure that stuff is deployed correctly.
Matt: I think that’s right. I think the only thing I would add is that I think that the service mesh you can implement feature flags really in one of two ways. You can put them in your application code, or you can have the service mesh actually act on them. I think there’s cases for both. We had actually talked to you all I think at some point about having Envoy actually talk to LaunchDarkly so that we could do that directly. I definitely see that happening more in the future. Where again today it comes back to what I was saying previously that people are not using the right UI for doing what they are trying to do. What we see today is that people use Envoy and service mesh for feature flagging. But they’re doing that by committing something into a config file like running some CLI or something.
No, I would like to go to the LaunchDarkly UI and have an experiment like move things around and be able to revert it, and we can’t do that today. We just don’t have the right tool integrations that we actually need, I think.
Edith: Well, I’d love to work with you on that. I completely agree that they’re serving different steps of a continuum. You want to make sure that it’s getting out to the right boxes, at the right times, at the right Kubernetes cluster. Then you have business users who want to enable it and the UI are different.
Astasia: Preeti, Hashi, a few weeks ago, announced Consul Connect, which is kind of their take on the Service mesh. What are some key decision criteria practitioners that think about when picking a service mesh?
Preeti: Thank you. What we launched a couple of weeks ago was Consul Connect, which is essentially a number of features that are within the Consul product. The product is Consul and this is another use case. Essentially, the problem that we’re trying to solve is the problem of, how do you enable microservices to connect securely with each other and manage those intentions in a simple to use way? Today, and I lived this personally when I was at Yahoo, a lot of the policies are encoded in the network. When you want to change these things, you have to go talk to your network admin. Where do all the delays in terms of deployment happen? It’s not the fact that you can’t get the code on the box. It’s the fact that you can’t get traffic securely to the code of the box, and that code to talk to other downstream services that it needs to be functional.
What Consul Connect is trying to do is essentially solve this problem of being able to securely communicate between services, and delegate the response ability of who can talk to whom, to someone who is essentially at the application level. We call it intentions. We can layer on top off whatever network security you might have in place, and this is a key factor in terms of when you’re looking at service meshes, what is your decision criteria? One key aspect of a lot of the HashiCorp tooling is, we don’t require operators to rip and replace. Our focus is on how do we give you immediate value in the environment that you have running today, and then help you move to a model where you’re running more effectively? Consul Connect will, out of the box, be able to talk with legacy applications as well. It’s out in open source. I definitely encourage all of you to take a look at it, and it’s something that we’re really excited about and looking forward to getting more feedback on.
Astasia: Something that we’ve been thinking through is the co-evolution of serverless and service mesh. Austen, this is to you, is the work around functions actually compatible with what we’re seeing in service meshes? Can you run them together?
Austen: Is the work around service meshes compatible with functions on a public cloud provider? Yes, absolutely. Service discovery is pretty easy I’d say with a lambda function. You can put a simple end-point in front of it. With respect to the service mesh platforms that are being built on Kubernetes, I believe some of them are designed to have Envoy integrations kind of out of the box. I’m not sure if you’ve seen work on that.
Group: Yes, there are a couple solutions going after this right now like Gloo from Solo.io
Astasia: Have you seen anyone that’s running your framework on Lambda and have a front end of a service mesh or anything like that?
Austen: They’re not doing a lot of that right now, but we see it coming. Especially given the maturity of some of the new tools in this space. We’re hearing more and more customer conversations talk about this.
Astasia: Well, that’s pretty exciting. Great.
Preeti: If I may just add. The Solo folks behind Gloo.io actually have done an integration with Envoy and Consul Connect too.
Astasia: Maybe check out Gloo, the project, to see if you’re interested in doing that. Switching gears, one thing that I don’t think the DevOps and cloud-native ecosystem has focused enough on is really talking about the impact on business and culture. Edith, how does DevOps culture help the broader business and them being successful?
Edith: It’s an interesting question because in my opinion, it started with agile, which is all about moving faster. Then agile permeated back until like if you’re shipping more often, you need processes in place like DevOps to help you do that. The real business impact that I have seen to our customers is huge. We’ve seen customers move from deploying once a year to deploying once a month, which is actually a huge difference when you talk about how much value can deliver and how that shifts the way they think about features, like it just gives them more at-bats. I think and I go to a lot of DevOps conferences. Everybody has a different idea about what they mean from DevOps. I think the business drive is just like, how can we reduce risk? How can we move faster? How do we build a culture around that?
I think that’s much more than tools and just about a mindset. I visited a customer this week. They said speed is a habit. Not speed is an emergency death match, which I remember that the project I didn’t like it. But just, how can we just get to this culture where we ship all the time, we figure out what works? Not everything is going to work, but we’re going to ship something else very quickly after that, that will work. That’s much bigger than any tool, that’s more just a mindset.
Astasia: Interesting. Matt, being on one of these platform teams, are there any organizational changes that need to occur for DevOps or cloud-native.
Matt: In fact, I just wrote a blog post on this.
Astasia: Can you give us the cliff notes?
Matt: Sure. This is very top of mind for me right now. First, I think DevOps just means different things to two different people. I think for companies that used to deploy once per year and are learning some agile techniques, great, like let’s do DevOps. I think for hypergrowth companies like Lyft and companies in that space, I think the idea of DevOps is frankly it’s pretty broken right now. It’s broken in the sense that companies like Lyft are hiring so many product engineers without any real plan on how to educate them of how to do DevOps. We just expect people to know, and the tooling is still very immature. Really the crux of the post is that I think because a lot of our infrastructure tooling today is still quite primitive, I really believe that some of these hypergrowth companies, you need some team. Call them whatever you want, whether it’s production engineering, SRE something else. But you need some bridge that can bridge between the infrastructure team and the product team, and that’s kind of the only way of making it so that these two groups of people don’t end up just hating each other.
I think that’s what ends up happening in a lot of these companies, and it leads to a lot of burnout. I think from like the original question from a people perspective is, I think, especially for some of the hyper hypergrowth companies, I think really considering just the human costs and how to educate people, how to do documentation, like these tend to be afterthoughts and they’re really important that people don’t think about them enough.
Astasia: Sounds like there’s still a lot of work that can be done from a team and culture perspective. We touched on a few trends today, including serverless, service meshes and some of the workflow tooling that needs to go into place. Just as a parting question to each of you, is there any one big trend that the audience should be looking for in the upcoming year? Preeti, do you want to kick it off?
Preeti: What we’re seeing is really what’s playing out in enterprises. It’s not super trendy from the point of being fashionable, but I think for us what we’re seeing is multi-cloud is certainly real. Time and again, we’ll be talking with customers and they do not want to put all their workloads in one cloud. They definitely want to go multi-cloud.
How do we make sure that we can enable that developer agility, productivity with the appropriate security cost in a multi cloud environment? DevOps, call it whatever, but it’s really how does this play out in terms of real adoptions for enterprise customers this year.
Astasia: Multi-cloud. Edith?
Edith: It sounds like a cliche, but far more people use software than I think anybody including me ever realized. About 10 years ago, there was this thought that software was a cost center. Let’s make software as cheaply as possible. Let’s ship once a year, that’s fine.
The high growth startups are scaring the old line companies, everybody wants to be Warby Parker. Nobody wants to be Lens Crafter. There’s just this absolute tidal wave of we need to move faster.
Matt: I don’t think there’s any one thing over the next year. The general trend though that we’re seeing is that anytime that people are working on infrastructure for most companies, it’s basically overhead. It’s basically useless. Companies don’t care. They want people writing business logic. I think that what we’re really seeing now is a trend where the fewer people that companies can employ to do infrastructure, those solutions will end up becoming popular.
Those are better public cloud solutions that offer better container services or better Serverless platforms. Or startups that are able to do a SaaS service that can take over the work that three people were doing at a company that they don’t have to do anymore. There’s no one area but it’s all the things I have been talking about before, which is why we need to make things easier to use. More UI, more UX like less YAML, less GitHub, less Slack. I think that people that do that, they will sell product. Whether that’s a startup or that’s cloud, that’s a separate conversation, but that’s really the trend, is just make things simpler to use.
Austen: Great question. I might reiterate what all three of these panelists said. Number one, multi-cloud. Absolutely. We see this a lot, and it’s not for traditional reasons like optimizing cost or fail over scenarios. In the service community especially, we see vendor choice. We see Google has that new thing over there, can we just stick a function over there and bring that in architecture somehow. We see big public cloud providers bending over backwards coming out with more and more serverless options.
More and more managed services that are higher levels of abstraction that really focus on producing outcomes faster without having to think about the underlying infrastructure. We see more and more demand for vendor choice and ways to just take advantage of the tool that best solves the problem rather than think within the limitations of a specific platform. That’s on the rise, I’d say.
With respect to moving faster, absolutely. We’re still trying to solve all these ancient problems of getting stuff out to the market, finding product-market fit. All this stuff is so hard that is the thing that I think businesses especially should be focused on. First and foremost, it is such a challenge.
Then we have all these new problems that are coming in as a result of just technology invading our human reality in every regard. With IoT devices everywhere, we’ve got AI systems, autonomous systems integrating into our lives. How are companies going to build the integrations with these systems have public facing kind of assets or presences on all the systems? Anything that helps them move faster increase innovation I think absolutely it’s a must. I just don’t see how companies are going to keep up with all this. It’s a concentrated change out there. Then lastly, making these things simpler. Absolutely. Then just building in the safety right into that.
Developers, they should not think about infrastructure, and they shouldn’t have to think about organizational policies and general compliance. This should be baked in. The guardrail should be in there and they should just focus again on business logic, and then see some type of error or something when they’re doing it wrong. Then I love the idea of just making these tools easier because we see this all the time in the service communities. New people who are traditionally developers being able to take this stuff and build all types of code use cases and stuff. I think that’s a powerful thing, especially for companies who want to do more. They’re having a hard time finding talented people.
As these tools get more accessible and software developing has democratized, I think we’re going to see big changes there. Especially when you think about just the creative class in general. I think there’s a lot of people out there who would be great software developers who just a bit too intimidated today. I think that’s another big thing to look out for.
Astasia: I like that. Make it multi-cloud, make it fast, make it easy. If only it was that easy to build solutions that support these principles.
VC Astasia Myers’ perspectives on distributed computing…
20 
20 claps
20 
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
"
https://medium.com/@davsclaus/webinar-develop-cloud-native-microservices-using-apache-camel-7a0cc6859358?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Claus Ibsen
Jan 25, 2019·2 min read
On next thursday 31st January 2019 I am presenting a live webinar about developing cloud native microservices with Apache Camel.
The session is scheduled for a full hour including QA. The talk with be a mix of slides and live demos. It will be my first talk with revealing details about Apache Camel 3 and a peak and demo of Camel K (next-gen serverless Camel on Kubernetes).
The abstract of the talk is as follows
Apache Camel has fundamentally changed the way enterprise Java developers think about system-to-system integration by making enterprise integration patterns (EIP) a simple declaration in a lightweight application — wrapped and delivered as a single JAR. In this webinar, we’ll show you how to bring EIP best practices to containers running on top of Kubernetes and deployed as Spring Boot microservices, which are both cloud-native and cloud-portable.
We’ll discuss:
Registration
The webinar is scheduled on thursday 31st of January at 11am ET (5pm CET) and is of course free to attend. All you need to do is to register at the provided link.
More webinars
We have talked about continuing the webinar with a series of Camel and agile integration talks. So if you are interested to hear more webinars and have requests for topics to be covered then we are open for feedback.
Originally published at www.davsclaus.com on January 25, 2019.
Works for Red Hat on the open source integration project Apache Camel. Author of Camel in Action books. Java Champion. ASF member. International speaker.
24 
24 
24 
Works for Red Hat on the open source integration project Apache Camel. Author of Camel in Action books. Java Champion. ASF member. International speaker.
"
https://medium.com/@dataart/transforming-organizations-with-cloud-native-development-75898725cc7b?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataArt
Apr 18, 2019·11 min read
Adopting cloud-native development increases:
- speed of software development and deployment
- productivity of teams doing systems development, systems support and IT operations, and
- ability to use leading-edge, cloud-native third-party services
These benefits can transform organizations and their clients. New and/or significantly improved products and services are a common outcome, as is a move to continuous improvement of products and services. This means that the business case for cloud ranges from reduced infrastructure costs to increased commercial success. Given these results, you would be crazy not to take this step but adopting cloud-native development often requires a change of culture — it is more than using new processes and technologies. This article describes the ethos and elements of cloud-native development; barriers to and benefits of adoption; and how cloud-native development relates to cloud-native applications and services, especially those from major cloud providers.
Cloud-native development is an approach to building and running applications that exploits the advantages of the cloud computing delivery model. The Cloud Native Computing Foundation (CNFC) talks about using “… an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilization. Cloud native technologies enable software developers to build great products faster.” (CNCF, 2019). It is that last part that should cause executives to sit up and take notice. The ability to add new features on a daily basis and to launch new products and services in timeframes previously thought impossible can be the difference between being an upper quartile company and being stuck in the pack. This is why cloud-native is the development method of choice for Facebook, Amazon, Netflix, Google and other software-driven, technology-centric, cloud-based organizations.
Cloud-native development achieves its magic by linking and automating the work of systems development, systems support and IT operations teams. The speed of pushing new software to production increases dramatically, while quality improves as opportunities for error are removed from previously manual processes.
The four tenets of cloud-native development are: (1) micro-services; (2) containers; (3) continuous delivery; and, (4) DevOps. Each is discussed below:
1. Microservices in this context means building applications as collections of small, independent services that communicate over HTTP APIs. It allows autonomous teams to work in parallel on the same application, and results in code that is not tied to infrastructure instances; faster creation of new applications, products and services; and, applications that are easier to understand, develop, test and support.
2. Containerization is virtualization at the operating system level, as compared to the virtualization of hardware from ‘traditional’ Virtual Machines (VMs). It allows single servers to be divided dynamically into one or more isolated containers. By doing so, it provides improved efficiency in infrastructure utilization and speed of deployment compared with traditional VMs; an ideal compute vehicle for deploying and scaling individual microservices (such as low overhead for creating and destroying containers); the ability to manage and migrate application dependencies with the application; and the ability to abstract the operating system and the Cloud platform away from the application. It is achieved, using tools such as Docker, by separating the internal elements of the application instance from external elements such as host OS, libraries (and other binaries), and configuration files. Containerization allows development teams to focus on building business functionality without having to worry about server memory, provisioning, and other performance and resource restrictions — which results in significant improvements in speed and productivity.
3. Continuous delivery allows software to be generated frequently, reliably and ready to be deployed to production. It is achieved by automation, specifically continuous integration (automated build and test), and automated delivery processes. Continuous delivery results in development work being normalized and standardized for IT Operations; code always in a deployable state; the option of automated, continuous deployment; and faster feedback from customers. Faster, more reliable delivery and improved build quality are just two of many benefits.
4. DevOps is an approach to increasing speed and quality of software development, delivery and deployment that centers around increased collaboration between developers and IT operations personnel. The aim is achieved by linking and automating the processes of software delivery and making related infrastructure changes. DevOps results in shorter software delivery times; more responsive organizations; and quality assured from idea to design, build, deployment, daily use and support. You can do DevOps within traditional on-prem infrastructures, but it is much easier when cloud is providing a standard, centralized platform for automated testing, deployment, and production that is not available with the distributed systems model.
As well as the four tenets of cloud-native development described above, good practice also includes agile methods; wide scope; open culture; welcoming change; immutable infrastructure; container orchestration; service meshes; and loose coupling.
Agile. Agile methods are hugely compatible with the continuous change philosophy of cloud-native development. Teams building, deploying, running and managing applications through automated toolchains are part of this. Build automation, deployment automation, testing automation, static code analysis automation, and scaling automation together result in significantly increased speed of development and deployment, as well as increased productivity and responsiveness. They also improve quality by reducing the potential for human error.
Scope. There is always an outside world and other services with which systems need to communicate. This means integrating with cloud-services beyond your own containers and virtual machines for things like email notifications; setting up databases; managing DNS records; retrieving log files/metrics from applications; sending warnings and alerts, etc. At a more transformative level, cloud gives development teams access to so much more than they could ever build themselves. For example, the Google Transfer service can copy hundreds of GBs of data per second from S3 (Amazon’s Simple Storage Service). Google BigQuery is arguably the fastest technology ever for interactive analysis of massively large datasets. Networking in AWS is so fast that it is quicker to query data over the network than to read it from disk. Google TPUs act as AI accelerators while Cloud Spanner is a database service built specifically for cloud that combines the benefits of relational databases with the horizontal scalability of non-relational databases. Finally, Amazon SageMaker is a fully-managed comprehensive service providing algorithms for developers to create machine learning solutions. Meanwhile, systems development, DevOps and IT operations teams can use Serverless Computing’ models where responsibility for server processes and hardware is outsourced to BaaS (‘back-end as a service’) and FaaS (‘functions as a Service’) providers. They can also take advantage of ‘disposable infrastructure’ services such as pre-emptible VM’s that are provided to them cheaply by cloud providers, to utilize spare capacity at the provider (but can be taken back with almost no notice when non-discounted demand returns). The scope and power of what is available to development, DevOps and IT operations teams from cloud providers is growing by the day — and the increased scope means opportunities for vastly increased speed and depth of data analysis; use of AI; and speed of development and deployment.
It could be argued that many of the revolutionary cloud-based services described above are available to development teams regardless of whether they use cloud-native development practices or not. In theory, teams practicing waterfall development methods in document-heavy bureaucracies can still use leading edge technologies, but in reality it does not happen. A development team wedded to traditional approaches will not be adopting leading edge technologies. Instead, they will keep using the technologies they know. This leads us to…
Culture. An open culture welcoming of change across all areas involved in designing, developing and delivering services to customers is essential if cloud-native is to be adopted and used to the full extent. Examples of welcoming change include losing attachments to old ways of working such as slow processes for approvals, tests and packaging. You are not doing cloud-native development if you are scheduling software drops into production once every two weeks, and if there is a significant period of time (and work) between creating working software and it going into production. It simply does not work that way. Cloud-native is about achieving continuous integration, continuous delivery and even continuous (ie automated) deployment. If a change requires dual approval, a formal sign-off, a week-long test cycle and another week to prepare production packages, then that organization is practicing traditional on-prem development in a cloud environment. Failing to understand and accept that fact is not a technical issue, it is a cultural issue. This is a common problem — in fact ‘on-prem dev in cloud’ is more common than ‘cloud-native in cloud’ and is the main reason why so many organizations fail to achieve anything close to the full benefits of cloud.
Immutable infrastructure means never modifying servers once they are deployed, because modifying live servers always results in a certain percentage being broken in the process (the cause of many service outages in many industries). If changes are required, then new servers should be built from a common image instead of amending a previous build. Modifying servers is traditional IT and is best left in the past.
Container orchestration. The rise of lightweight, flexible containers has created new application architectures that have fundamentally changed how applications are deployed and visualized. The containerization approach is to package the different services that constitute an application into separate compute containers, and to deploy those containers across a cluster of physical or virtual machines. This has driven the need for efficient ways of defining, provisioning, scheduling and managing containers at scale. Tools such as Kubernetes and Mesos fulfil that need.
Service meshes are dedicated infrastructure layers for making service-to-service communication safe, fast, and reliable. If you are building a cloud-native application, you need a service mesh. Tools such as Istio or Linkerd provide a configurable infrastructure layer that makes communication between service instances flexible, reliable, and fast. They provide traffic management, security, monitoring and observability; support for a ‘circuit breaker pattern’ that isolates unhealthy instances; and, other capabilities essential for modern software architectures.
Loose coupling means developers breaking tasks down into separate services that can run on several servers in different locations, and not tying code to infrastructure instances. Because the infrastructure that supports a cloud-native app does not run locally, cloud-native applications must be planned with redundancy in mind so the application can withstand equipment failure and be able to re-map IP addresses automatically. This part of cloud design is massively cost-effective because services and resources for computation and storage can be scaled out horizontally as needed, which negates the need for overprovisioning hardware and having to plan for load balancing (virtual servers can always be added quickly).
Now that the components and best-practice for cloud-native have been described, can there be any doubt about its adoption? You might think not, but it happens, and the causal factors are people and culture not technology. Typically, the problem is a lack of prior experience in cloud computing generally or cloud-native development in particular. It may not be active resistance to cloud-native that is the problem, but instead people simply not knowing what they don’t know. No one can blame them for that. If no one in a development team has ever used cloud-native, then it is unrealistic to expect them to do so without a lot of help. It is down to the people leading the move to cloud infrastructure to include cloud-native development, cloud-native applications and third-party cloud services in the scope of migration. These things do not look after themselves. Those in positions of COO / CIO / CTO need to understand and be able to explain the nature and importance of cloud-native development and cloud-native applications and services to their colleagues. They will also need to act as coaches once the transition is underway. Some of the approaches and technologies of cloud-native involve a genuine paradigm-shift for people and organizations. This means that change management in general, and culture change in particular must be in the skill set of those leading the transition.
Unfortunately, but perhaps inevitably in certain circumstances, significant changes in personnel are sometimes needed in successful moves to the cloud. The ‘circumstances’ being in cases where the need for an injection of skills, knowledge and experience in cloud has been overlooked in the planning process. For example, the executive board of one important, highly-regarded financial institution instigated a move to cloud to satisfy strong requirements for systems modernization and improved security. Unfortunately, the move proved to be problematic and disappointing. Lack of previous cloud experience in the company was identified by the executive board as the main causal factor, so they initiated a 50% replacement of IT personnel (including the leader) with all new hires being required to have significant expertise and experience in cloud infrastructure and cloud-native development. Cloud subsequently became a significant success for the organization. Obviously, it would have been better if the firm had anticipated and addressed the skills and experience gap in advance so as to minimize the need for staff turnover, but the conclusion remains that getting people onboard who are experienced in cloud is critical to the endeavour. Do not go into cloud-native without hiring experts in the field.
Fast-forward to when your migration challenges have been overcome; your IT teams are doing real cloud-native development and, they are using leading edge cloud-native services to help transform the experience of your customers. How do you know that the transition has been successful? To have an objective picture, it is best to have continuous metrics, an automated way to collect data, and a defined way to make decisions based on those data. Comparing the outcome against the original business case is important as doing so will force you to scrutinize which parts of the transition still needs to be addressed. The full scope of the business case for cloud is large and complex and it is often the case that opportunities and issues are overlooked. Benefits realization always needs to be an active process — and data is essential to make it happen.
In summary, the benefits of adopting cloud-native development are:
• Speed: faster time to market for new features, products and services
• Productivity: more software released more often by same sized teams
• Efficiency: automated infrastructure management for the application layer
• Reliability: safer and easier to handle rapid releases, replace failed components and recover from unexpected events
• Culture: increased collaboration between software development and IT operations teams
• Scope: easy access to a range of third-party services that is transforming technology-based services in every aspect of our lives
• Commercial: switches cloud from being a cost-saving decision to a driver of organizational success.
Adopting cloud-native development, applications and services is essential if you are to realize the full benefits of cloud infrastructure. It can swing initially more on people and culture than on processes and technologies, but having a clear view of the processes, technologies and services that will be enabled by the move is important because it is those things that will transform the experience of your customers and the success of your company. Cloud is so much more than a cheaper way to operate IT infrastructure. Instead, it is the path to increased organisational success. Cloud-native is a critical part of achieving that success. Good luck with your own endeavors — it will be worth it!
By Cliff Moyce, Chairman of Advisory Board, Finance Practice at DataArt.
We design, develop & support unique software solutions. We write about Finance, Travel, Media, Music, Entertainment, Healthcare, Retail, Telecom, Gaming & more
8 
1
8 
8 
1
We design, develop & support unique software solutions. We write about Finance, Travel, Media, Music, Entertainment, Healthcare, Retail, Telecom, Gaming & more
"
https://medium.com/memory-leak/introducing-the-cloud-native-landscape-3cc59846a681?source=search_post---------112,"There are currently no responses for this story.
Be the first to respond.
This story originally posted here on November 8, 2016 by Lenny Pruss
Today, Redpoint Ventures and the Cloud Native Computing Foundation are proud to announce the Cloud Native Landscape. This project leverages the CNCF’s newly-announced reference architecture (below) to segment the stack and identify the prevailing projects, products and companies that enable organizations to build, test, deploy and scale cloud native applications. The goal, ultimately, is to provide clarity as to how different projects and companies fit together and to help ground the conversation for customers, practitioners and analysts as they embark on their cloud native journeys.
In the last decade, we’ve witnessed the emergence of a new computing paradigm characterized by open, elastic infrastructure, microservicesapplication architectures and DevOps development and deployment principles. These massive deflationary forces have eviscerated barriers to adoption, distribution and innovation for technology. Correspondingly, there has been a Cambrian explosion of new projects, products and companies while yesterday’s tech giants — minted in the client/server era — have seen their businesses come under pressure from the commoditizing power of open source and cloud.
The practical implication is that every organization is in the midst of a massive digital transformation where IT shifts from a cost center that is adjunct to core business to a profit center that is its lifeblood. Competitive advantage in this world is defined by a company’s ability to out-innovate its rivals, which most plainly translates into the ability to write better software faster.
Cloud native was a catchall term created to represent this new computing paradigm, one that is optimized for modern distributed systems capable of scaling to tens of thousands of self-healing multi-tenant nodes. It is informed by technical merit and end-user value, and inspired by Internet-scale computing born inside organizations like Google and Facebook.
If this sounds nebulous, it’s because it is. For the last several years, as the velocity of innovation has accelerated to breakneck speed, so too has confusion for end-users. Orchestration, scheduling, distributed coordination, service discovery? Suddenly a new computing vocabulary had emerged, but no clear taxonomy or reference guide existed to help customers along their path to become cloud-natives.
At Redpoint, we have been closely observing the radical innovation unfolding, remarking these changes were fundamental and would reshape and recast the entire IT value chain. Last year we began to map out the ecosystem, and as those efforts continued in 2016, we got in touch with Dan Kohn and Alexis Richardson of the Cloud Native Computing Foundation, which itself was in the midst of defining and voting on a reference architecture. Over the course of a few conversations it became evident that collaborating on a definitive industry landscape would benefit the entire ecosystem — from individual developers to customers to startups to large incumbents — which ultimately brought us to today.
Please note that this version 0.9 is a beta, and we will be working towards a 1.0 release in the coming months that will incorporate a more nuanced architecture and segmentation and feature more projects and companies which may be absent in the current version. The beta version is available at https://github.com/cncf/landscape and we’re eager to accept feedback. We encourage projects and companies that have not been highlighted in the current release to open an issue so they can be included in version 1.0.
We look forward to hearing from and working with you!
VC Astasia Myers’ perspectives on distributed computing…
20 
20 claps
20 
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Redpoint partners with visionary founders to create new markets or redefine existing ones at the seed, early and growth stages.
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
"
https://medium.com/built-to-adapt/hit-the-gas-to-stay-secure-in-a-cloud-native-world-ee6d6c1093bf?source=search_post---------113,"There are currently no responses for this story.
Be the first to respond.
Have you ever been zipping down the 101 when you come up on a car doing 35 mph or so? There’s a good chance they’re going so slow because they want to stay safe. It sounds like a reasonable approach. The slower I go the safer I’ll be. Except that’s wrong. A slow driver on the highway causes the rest of us to hit the breaks and swerve, increasing the chances of an accident. It might be counterintuitive, but it’s true: In order to stay safe on the highway, you need to keep your speed up.
There’s a similar phenomenon happening with cybersecurity. An increasing number of Fortune 500 enterprises have or are in the process of adopting modern, cloud-native practices like Agile and DevOps in order to build and release software faster so they can better respond to customer demand and stay a step ahead of the competition. But many of these same enterprises approach security the same way our driver approaches safety on the highway: by going slowly.
Taking a slow and steady approach to security makes intuitive sense. Let’s not do anything rash that may make us vulnerable to attack. But the slow and steady approach doesn’t work in a world where there are more users interacting with systems than ever before, where applications are created and updated daily or hourly, and where attacks against corporate networks are essentially continuous. A new report, created jointly by Thales eSecurity and 451 Research, puts it this way:
“This year we found that organizations are dealing with massive change as a result of digital transformation, but this change is creating new attack surfaces and new risks … “But while times have changed, security strategies have not … If security strategies aren’t equally as dynamic in this fast-changing threat environment, the rate of breaches will continue to increase.”
In fact, in order to improve security in cloud-native environments, enterprises need to speed up. When it comes to your infrastructure and applications, speed doesn’t kill. Speed secures.
But what does that actually look like? Justin Smith, Chief Security Officer for Product at Pivotal, boils it down to what he calls the Three R’s: Rotate, Repave, Repair. Let’s break these down.
By rotate, Justin means rotating datacenter credentials every few hours. Speaking at SpringOne Platform, Justin pointed to the 2016 SWIFT data breach, in which hackers broke into the Bangladesh central bank’s network and swiped over $80 million using stolen credentials. By frequently rotating datacenter credentials, so that each is only valid for a short period of time, stolen or leaked credentials soon become useless to hackers.
But no one tactic is enough. Even though rotating credentials frequently reduces the likelihood of a network breach, some will still succeed. That’s where repaving comes in. This refers to repaving each and every server and application in your environment from a known good state multiple times a day. Everytime you repave a server, the slate is wiped clean and everything on that server — including malware — is swept away. This effectively shortens the window of time an attacker has to act if he or she manages to get into the network in the first place.
Finally, repair means to apply security patches to operating systems as soon as possible after they are made available. The longer the time between patching, the longer the time hackers have to take advantage of existing OS vulnerabilities.
The three R’s are great, you’re thinking, but how can I possibly keep up this pace? My infrastructure team is stretched thin as it is and can’t possible rotate, repave and repair this quickly, day in and day out. And that’s true in traditional infrastructure environments. But if you’re using a secure, modern platform, such as Pivotal Cloud Foundry, it can be done. The key? Automation.
Consider the case of CSAA Insurance Group. The company is the insurance arm of AAA, offering car, home and other types of insurance to millions of AAA members. Prior to adopting Pivotal Cloud Foundry, the company typically applied patches on a quarterly basis, according to Kyle Campos, Technology Operations Manager for CSAA’s digital services organization.
How frequently does CSAA apply patches now that its running Pivotal Cloud Foundry? “Now we do that at least once a week,” Campos said, speaking at Cloud Foundry Summit in Basel, Switzerland last fall. “We just recently, with help from Pivotal, got our repaved pipelines down and continuous delivery for minor versions of PCF. We rotate every day and we have minor dot releases come out once every week.”
CSAA can repair its platform because Pivotal patches critical vulnerabilities found anywhere in the platform — from the operating system and middleware to specific Cloud Foundry components — within 48 hours of a fix becoming available. Operators use Concourse, a continuous integration/continuous delivery system remastered for teams that practice agile development and need to handle complex delivery permutations, to set up pipelines that detect and apply patches to their PCF foundations automatically, usually with zero downtime. Pivotal Cloud Foundry similarly automates the repave and rotate processes.
So even though it may sound counterintuitive, remember, speed — and automation — is the key to robust security. To learn more about the Three R’s, check out this on-demand webinar with my Pivotal colleague Kamala Dasika and Zane Lackey from Signal Sciences. Kamala and Zane explore strategies to overcome these and other security challenges unique to cloud-native apps. Stay safe out there!
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
25 
25 claps
25 
Written by
Senior Director of Product Marketing at Privacera.
How software is changing the way society and businesses are built.
Written by
Senior Director of Product Marketing at Privacera.
How software is changing the way society and businesses are built.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.getambassador.io/submitting-the-ambassador-api-gateway-to-the-cloud-native-computing-foundation-7bc2132a6a84?source=search_post---------114,"We’ve always been big fans of the Cloud Native Computing Foundation (CNCF), which has done a phenomenal job shepherding key cloud-native projects such as Kubernetes, Envoy Proxy, and Prometheus. As such, we’re excited to announce that we’ve begun the process of submitting the Ambassador API Gateway to the Cloud Native Computing Foundation as an incubation project.
Here at Datawire, we believe in the power of open source. Having built and contributed to multiple open source projects over the years, we have seen first hand that a growing, engaged community is critical to the wide-scale adoption of infrastructure software. Ambassador would not be where it is today without extensive contributions from members of our community. In order to take this to the next level, we believe that a vendor-neutral home for the Ambassador API Gateway is a natural step.
The ingress controller / API Gateway has become a critical part of the cloud-native infrastructure stack. With the CNCF hosting other key parts of the stack such as Kubernetes, Envoy Proxy, and Prometheus, we felt the CNCF was a logical choice.
We’ve worked with the CNCF on the Telepresence project, which we donated to the CNCF a few years ago. The CNCF has helped us in many of the governance matters associated with running a robust open source project, and we expect that partnership to extend to Ambassador.
We also think this marks a milestone for the Envoy Proxy, the Layer 7 proxy created by the team at Lyft that graduated from the CNCF in November 2018. Over the last few years, we have seen widespread community adoption of Envoy. Today, there are more than 13 open source projects built on top of Envoy, including Ambassador. With Ambassador’s submission to the CNCF, Envoy now has more CNCF open source projects than any other L4 or L7 proxy.
We’ve seen continued, rapid adoption of the Ambassador Edge Stack by our community. The Edge Stack will continue to be built on the Ambassador API Gateway. As such, the Edge Stack will continue to provide a full superset of the capabilities of the Ambassador API Gateway, with no limits on any functionality that is part of the core Ambassador API Gateway. Edge Stack provides revenue that we in turn use to hire engineers to build out both open source and commercial applications. This is key, as the world is littered with many open source companies that never got the balance right between commercial motions and open source. Our contribution to the CNCF shows our commitment to open source while maintaining a revenue stream to help both efforts.
We’re excited for the next chapter of the Ambassador API Gateway. If you are a happy user and support the donation to the CNCF, please feel free to leave a comment on our submission or reach out to us on Twitter @getambassadorio. As always, a big thanks to our community for the continued support.
Developer-First Kubernetes.
22 
22 claps
22 
Written by
CEO, Ambassador Labs. Makers of the Kubernetes Developer Control Plane, CNCF Telepresence, and CNCF Emissary Ingress.
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
CEO, Ambassador Labs. Makers of the Kubernetes Developer Control Plane, CNCF Telepresence, and CNCF Emissary Ingress.
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
"
https://medium.com/@lynnlangit/cloud-native-bioinformatics-hpc-to-gcp-21a5fb9921cd?source=search_post---------115,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
May 8, 2020·5 min read
Working with a team from the Imperial College in London, we started by reviewing a genomic analysis workflow which is currently running on their HPC cluster. The workflow is a complete analysis for single-cell/nuclei RNA-sequencing data. I started by reviewing the workflow process steps generated by the Nextflow script used to run it.
Next I reviewed the current method of running this workflow by connecting with the team and observing them run this analysis on their HPC cluster. Of note is that the workflow utilizes containerization via Singularity / Docker.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/wasm/cloud-native-webassembly-90b5ed782ea2?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
WebAssembly was originally created for the browser. But like Java and JavaScript before it, once it gained support from the community (esp standardization and toolchain support), WebAssembly has become increasingly popular on the server and on the edge as well. In 2021, WebAssembly won the prestigious Programming Languages Software Award from the ACM SIGPLAN.
In the age of cloud-native computing, the boundary of hardware and software is blurred. Everything is abstracted into “the cloud”. That has resulted in an explosion of microservices, messaging queues, data streams, and serverless functions. Those components are written by different developers, and potentially untrusted, but need to work together safely and seamlessly. The traditional way of using network APIs, or the more recent approach of using Docker containers, to isolate software components are slow and heavy. WebAssembly has emerged as a high-performance, cross-platform, and polyglot software sandbox for cloud-native software components. WebAssembly runtimes can be embedded into multiple programming languages and host environments. They can also be managed and orchestrated by container tools or data streaming frameworks.
The Cloud Native Computing Foundation (CNCF) is a proponent of WebAssembly in cloud-native infrastructure. It hosts several WebAssembly related projects and initiatives.
There are many other projects in the CNCF cloud-native landscape that utilize or integrate WebAssembly. If I have missed anyone (or you are interested in being included in this article), please reach out to me via email or Slack, or leave a comment in this medium article!
To learn more about WebAssembly in the cloud-native infrastructure and meet leaders in this industry, please join us at the Wasm Day event at the KubeCon North America 2021 (Disclaimer: I’m on the advisory board of Wasm Day). It is a virtual + in-person event. You can join as an attendee, speaker, or sponsor.
See you there!
Dedicated to curating the highest quality WebAssembly…
73 
73 claps
73 
Written by
Technologist and investor
Dedicated to curating the highest quality WebAssembly (Wasm) information, in an unofficial capacity. Encouraging writers, developers and researchers to share everything from innovative business ideas & Wasm use cases, right through to technical insights, documentation & code.
Written by
Technologist and investor
Dedicated to curating the highest quality WebAssembly (Wasm) information, in an unofficial capacity. Encouraging writers, developers and researchers to share everything from innovative business ideas & Wasm use cases, right through to technical insights, documentation & code.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/jay-kreps-talks-cloud-native-kafka-competitors-and-a-resurgence-of-enterprise-it-innovation-4011a483ed84?source=search_post---------117,"In this episode of the ARCHITECHT Show, Apache Kafka co-creator (and Confluent co-founder) Jay Kreps returns to talk about a wide range of topics, including an uptick in Kafka alternatives both open source and commercial. Among a range of other things, Kreps also shares his thoughts on “big data” IPOs, what type of open source project makes a good business, and why enterprise IT is becoming a driving force in software innovation.
For more from Jay, as well as his Confluent co-founder and fellow Kafka creator Neha Narkhede, check out their podcast interviews from last year:
architecht.io
architecht.io
Scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
This episode brought to you by:
news.architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
17 
17 claps
17 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/clouddon/waves-of-cloud-native-transformations-ef96a68ee238?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
Enterprise CIOs have been working on digitally transforming their IT infrastructure for a while now. Such digital transformations have traditionally been based on virtualization and Software-as-a-Service (or SaaS) offerings. With the development of cloud computing/ container technologies, transformational CIOs are looking into becoming cloud-native as well. But what is Cloud-Native?
The definition of cloud-native has evolved a lot in the past few years. It has evolved from a specific definition of applications to software architecture to design patterns to even team, technology, and culture. It also has evolved from being cloud-based (distributed, less stateful) to strictly container based (stateless, distributed, self-healing, micro-services based) to the higher level of abstractions such as Serverless. In this blog post, we will focus on design patterns and consumption patterns, not on culture.
The term ‘Digital Transformation’ informally refers to the application of digital technologies to transform how one runs business. It is an all-encompassing term that is applicable to Productivity Suites, data centers, to build pipelines to internal communications to customer experience, etc.. With developments in cloud computing/ container technologies, transformations based on such technologies have started influencing digital transformations so much that the term ‘Cloud-Native Transformations’ is increasingly being used to refer to ‘Digital Transformations’.
This blog post covers the evolution of cloud-native transformations and serves as a reference to future blog posts that discuss best paths to adopting such transformations from wherever one is at.
Waves of Digital/ Cloud-Native Transformations
The evolution of Cloud-native transformations can be best viewed as waves of successive abstractions rather than disconnected islands, with every abstraction developing upon the previous one. Even though these abstractions appear to be radically different from each other, each of them has also undeniably influenced and impacted the successive one. Every abstraction has resulted in a certain style of application design, thereby finding a niche set of applications more suitable for that abstraction. Following diagram captures these waves and shows their respective hype cycles/ hype peaks. One can also notice the increase in the shift towards application logic, away from the underlying infrastructure as the waves progress.
Virtualization
It is not hyperbolic to say that virtualization (CPU, storage, and network) set the tone for digital transformations. Through CPU virtualization technology, a virtualization-enabled physical server can ‘mimic’ multiple virtual servers, thereby increasing the amount of total number of resources it supports. With this came the promise of CapEx efficiency and cost savings.
Virtualization-based application design patterns largely followed patterns that were meant for physical servers — relying on highly available/ always available infrastructure, scaling up the infrastructure for heavier workloads, replicating physical network based isolation in virtual environments, etc. In essence, virtual machines were treated like physical servers (always on) and were expected to behave like physical ones. More often than not, enterprise IT organizations treated virtual machines like physical servers and followed same procedures for their life cycle management they employed for physical servers — for provisioning, installing/ updating operating systems, providing user access, securing through firewalls, network segregation, decommissioning, etc. Virtualization also enabled concepts of snapshots, live migration, backup & recovery, which went on to become core requirements of enterprise IT. Enterprises also standardized the delivery of such resources through processes such as ticketing, centralized control through a dedicated group (frequently referred to as the IT), etc.
Cloud Services
Virtualization, in turn, enabled on-demand delivery of infrastructure resources (compute, storage, and networking) — AWS being the pioneer here, offering such capabilities since more than a decade ago.
In this model — called Infrastructure-as-a-Service (IaaS), one can consume a resource such as a virtual machine, whenever one wanted. Google and Microsoft also pioneered another type of as-a-service delivery — Platform as a Service (PaaS), at a level of abstraction higher than IaaS. In this model, one can directly consume an application environment (say a web server), instead of worrying about the underlying infrastructure (servers)that would power this environment.
These as-a-service delivery mechanisms were broadly classified as ‘Cloud’, key characteristics being (assumption of) infinite capacity, on-demand availability, and elasticity. When such services were offered by a third party provider from a location outside of the customer’s location/ data-centers (off-premises), they were commonly referred to as ‘Public Cloud’ and when they were delivered in-house from inside of the customer’s location/ data-centers (on-premises), ‘Private Cloud’.
These as-a-service delivery models enabled new consumption patterns — self-service/on-demand provisioning, against what has then become a common practice among enterprises — ticketing based, centrally controlled provisioning. Such as-a-service offerings changed the discussions from CapEx to OpEx.
Containerization
A container (Linux or Windows) is an isolated, self-contained run-time environment which runs as a process on a host machine or virtual machine. A container contains necessary and sufficient dependencies and configuration files in order for an application to run. A container behaves just like a virtual machine in the sense that it appears to be a complete system, without the need for an entire operating system. Since a container doesn’t need the entire operating system, it operates much faster than a virtual machine.
Linux Container technology has been around for a decade now, but Docker Inc enabled packaging of the dependencies and configuration files more efficiently than ever before through ‘Docker Containers’. Such efficient packaging mechanism enabled developers to develop, deploy, and scale applications faster. This ease of use and efficiency captured the imagination of developers unlike ever before, so much that ‘Docker’ soon became synonymous with ‘Containers’.
Containers influenced design patterns that leveraged faster, efficient and smaller run-time environments. Faster startup/ shutdown times also meant applications no longer need to be worried about re-starts. In other words, an application could re-start in case of a failure/ crash with very less performance impact. This marked a considerable shift in the dependency on underlying infrastructure on ‘availability’. With physical servers and virtual machines, the applications expected them to be always available. With containers, the applications need not (or sometimes couldn’t) have such dependency. This meant that applications need to be more prepared in case of unavailability of the underlying infrastructure. In other words, they need to be more fault-tolerant. Lack of appropriate storage abstraction in the early days of this phase also meant applications tended to be almost state-less/ or less stateful.
Since each container is self-contained without any additional overhead, it could serve as a complete application or a part of an application, which could work with the rest of the application served by one or more containers. This provided a natural way to break down applications into smaller functions/ services — a pattern commonly referred to as ‘micro-services’.
These factors influenced a new application design pattern characterized by stateless (or less stateful), fault-tolerant, distributed, and micro-services based applications.
Containers, at scale, needed to be orchestrated together, bringing up the need for a container orchestration platform. While multiple such platforms came into existence, the industry appears to have standardized around Kubernetes.
Container-based application development introduced newer consumption pattern — CaaS (Containers as a Service), in both private and public cloud environments.
Serverless
As the waves progress, one can observe that focus gradually shifts away from the underlying infrastructure and more towards the application itself. Continuing this trend is the wave of ‘Serverless Computing’, which refers to a computing paradigm in which developers can focus on the application code/ logic without having to worry anything about the infrastructure required to support/ power this logic.
Serverless computing was enabled by the availability of services which enable executing a slice of code, triggered by an event of a predetermined type. This ability is broadly categorized as Function-as-a-Service (FaaS). Services such as AWS Lambda, Azure Functions fall under this category. Platforms such as OpenWhisk enable one to build a FaaS platform on on-premises infrastructure.
Serverless application design patterns are characterized by two key factors — Event-Driven and Micro-Services. They also don’t expect any native storage abstractions (such as mounted volumes, file systems, etc) to be present, but consume storage-as-a-service offerings (DBaaS, object storage as a service, etc.). In theory, one could also mount a volume on a network share and expose that to these serverless applications, but there are less compelling reasons to do so.
Some include ‘Serverless Containers’ (services such as Amazon Fargate) under the Serverless computing category because they enable one to focus on applications (containers) without having to worry about the infrastructure (cluster) below. One might argue then, that managed infrastructure services (like Amazon EKS, Rackspace Managed Cloud etc.) can also be called Serverless since they don’t require one to manage the underlying infrastructure. For our purposes of design patterns leveraging Serverless Containers are not included under the Serverless Computing pattern as they were discussed under Containers.
Application Design Patterns
The successive waves of cloud-native transformations have influenced distinct application design patterns that leveraged the strengths of each transformation. Design patterns also evolved as the waves progressed.
Compute
Applications during early days of digital transformations tended to be monolithic (for example, a 3 tier web application). A typical 3 tier web application had a presentation layer (front end), an application layer, and a database layer. Design patterns then focused on making these individual components highly available (through redundancy) and scalable (through vertical scaling). These patterns continued during virtualization phase as well.
As cloud-based services became available, compute design patterns evolved to leverage the elasticity and scale that cloud-based services provided by being distributed, elastic, and less stateful. This is when application patterns such as Twelve-Factor App found more adoption. This is also when design patterns evolved to consume storage options available as a service rather than relying on locally available storage devices.
As containerization gained momentum, compute design patterns evolved to leverage the agility that containers provided through micro-services. Ephemeral nature of containers (without the right level of storage abstraction during the early days) also influenced stateless or almost stateless design patterns.
With serverless computing, compute design patterns have evolved to be event-driven and micro-services based.
Continuing the evolution of compute design patterns, we expect that the next iteration of design pattern would be similar to a better version of PaaS (which we call as PaaS 2.0 in order not to introduce new jargon) where one can focus on the application of one’s choice without having to restrict to a certain type (micro-services, event-driven, stateless etc).
Storage
It is also interesting to observe how storage design patterns have evolved, along with compute design patterns. Both compute and storage patterns have influenced each other all along, but one can see that as waves progressed, storage is becoming more influential (Data Gravity!)
During the early days of digital transformations, applications relied heavily on the underlying infrastructure for their high availability. This requirement of high availability reflected in storage design patterns as highly available, special purpose, storage appliances with redundancy, recovery/ automatic failover, etc.. Developments in storage virtualization further enhanced these patterns so much that enterprise customers wouldn’t consider any storage options that didn’t have these capabilities.
As cloud-based services evolved, various storage capabilities also started to be available as a service (such as block store as a service, object store as a service, databases as a service, etc.). Compute design patterns evolved to leverage these storage capabilities available as a service.
Storage design patterns for containerized applications have taken an almost full circle. Due to the lack of proper storage abstractions and ephemeral nature of containers, application design patterns tried to be stateless. Soon, the industry realized that not all applications can be stateless and more importantly, applications that enterprise cared the most were not stateless. Storage abstractions to enable running such applications within containers evolved — such as Stateful Sets and Persistent Volumes. These abstractions provide a way to attach or detach a storage volume to a container and if needed, data written to such volumes can be persisted even after the lifetime of containers.
While we are not fans of such retrofitted designs, it is important to observe that they enable containerization of a certain type of applications that are limited by their storage layer/ backends.
Serverless applications, don’t expect the presence of native storage options (such as mounted volumes, C:\ etc.). They consume storage-as-a-service options such as Amazon S3, Azure Blob Storage etc. We don’t expect any service provider to provide block storage abstractions (like attach/ detach volumes) to Serverless applications as it would be anti-thesis of their design patterns.
What’s next — Transformation Best Practices
No matter where one stands in their digital transformation journey, more transformation is possible. In the following posts, we will cover transformation best practices that optimize individual transformations based on where one is at.
CloudDon - catalyzing modern enterprise IT transformations
10 
1
10 claps
10 
1
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
"
https://itnext.io/building-cloud-native-apps-intro-to-open-application-model-and-rudr-bd1b55df9bf3?source=search_post---------119,NA
https://medium.com/@mhausenblas/polyglot-cloud-native-compute-77f5fee69b00?source=search_post---------120,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Michael Hausenblas
Dec 6, 2019·3 min read
Where I’m contemplating about cloud native compute and how we’re moving more and more into a polyglot setup.
So, what is this about? What do I mean by cloud native compute, why is it polyglot and what are the challenges we’re facing?
It’s really horses for courses, applied to cloud native compute. Pick the “right” compute form for a given workload; in real-world setups, many of those compute forms, such as containers or Function-as-a-Service (FaaS), will co-exist. It’s not a zero-sum game.
I’ve been wanting to write about the topic of polyglot cloud native compute (PCNC) for some time now. This week, at re:Invent, it feels the time is ripe and I’d like to share some thoughts about PCNC, essentially raising awareness and helping you to navigate the space a little better.
Why is the time ripe? Now that we’ve launched EKS on Fargate—a launch I’m grateful and proud to have contributed a bit—and witnessing talks at re:Invent such as Using containers & serverless to accelerate application development (linked below), I believe we’ve enough data points at hand to make a case.
But how to pick the “right” compute form?
Maybe, you’re currently running your application on a VM, bare metal, or on a mainframe. With the decision to modernize your app or to move it to the cloud, you’re faced with a number of choices? Can the (monolithic) app be moved as a whole or will you need to break it up in smaller functional units, potentially causing re-design and re-writes?
An exemplary comparison of containers and FaaS, based on a talk I yet have to give in public (hit me up if you want me to keynote it, LOL) looks as follows:
As you can see from above, both cloud native compute forms have some conceptually similar properties, such as how artifacts. However, there are also areas, such as lifting & shifting a monolith, where they clearly differ.
Potential criteria to choose the one over the other for a specific workload— while acknowledging the fact any non-trivial, real-world system will end up using a combination—could be as follows:
Wrapping up, I believe that we’re seeing PCNC becoming a reality and the more you know about pros and cons of different compute forms and their implications, the more informed your decisions become. Good luck, and remember: PCNC ain’t no picnic.
To circle back to the title: if you squint at the abbreviation for Polyglot Cloud Native Compute (PCNC) you might actually hear yourself saying “picnic”, so that’s what the pun was trying to convey. Sorry not sorry, Massimo, couldn’t resist ;)
Solution Engineering Lead in the AWS open source observability service team
See all (22)
21 
Some rights reserved

21 claps
21 
Solution Engineering Lead in the AWS open source observability service team
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/micro/micro-3-0-m3o-is-a-platform-for-cloud-native-development-6173f1d9e755?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
This is the official announcement for the release of Micro 3.0 better known as M3O — a platform for cloud native development. Our 3.0 release is a major refactor and consolidation of the existing tooling into something that addresses the entire workflow of build, run, manage and consume all from the developers perspective.
Read on to learn more or go straight to the latest release. Head to m3o.com for the hosted offering.
Micro focuses on developer productivity for the backend. It’s clear that the Cloud has become infinitely more complex over the past few years. Micro attempts to create order out of that chaos by distilling it all down to a handful of primitives for distributed systems development.
Why should you care? If you’re reading this you’ve no doubt encountered the tedious nature of infrastructure management, wrangling a kubernetes cluster on AWS or the thousands of things you need to do to cobble together a platform before starting to build a product. We think we’ve nailed the solution for that just as Android did for Mobile. Keep reading if you want to find out more.
Micro started out as a toolkit for microservices development, incorporating an api gateway, web dashboard and cli to interact with services built using a Go RPC framework. Back then it felt like getting anyone to buy into PaaS again was going to be a losing battle. So we chose to write single purpose tools around an RPC framework thinking it might allow people to adopt it piece by piece until they saw the need for a platform. It was really straight forward right until it wasn’t.
There was a simple Go framework plus some surrounding components to query and interact with them, but like any long lived project, the complexity grew as we tried to solve for that platform experience that just couldn’t be done with a swiss army knife. The repo exploded with a number of independent libraries. To the creator its obvious what these are all for but to the user there is nothing but cognitive overload.
In 2019 we went through a consolidation of all those libraries which helped tremendously but there was still always one outstanding question. What’s the difference between micro and go-micro? It’s a good question and one we’ve covered before. We saw go-micro as a framework and micro as a toolkit but these words were basically empty and meaningless because multiple projects working in coordination really need a crisp story that makes sense and we didn’t have one.
In 2020 we’re looking to rectify that but let’s first let’s talk about platforms.
5 years ago the world exploded with a proliferation of “cloud native” tooling as containers and container orchestration took centre stage. More specifically, Docker and Kubernetes redefined the technology landscape along with a more conscious move towards building software in the cloud.
Micro took a forward looking view even as far back as 2015. It was clear distributed systems and cloud native was going to become the dominant model for backend services development over the coming years but, what wasn’t clear is just how long we’d spend wrangling all sorts tools like docker, kubernetes, grpc, istio and everything else. It felt like we were rebuilding the stack and weren’t really ready to talk about development aspects of it all.
In fact at that time, people mostly wanted to kick the tyres on all these tools and piece something together. Running kubernetes yourself became all the rage and even using service mesh as the holy grail for solving all your distributed systems problems. Many of us have come to realise while all of this tech is fun it’s not actually solving development problems.
We’ve gotten to the point of managed kubernetes and even things like Google Cloud Run or DigitalOcean App Platform, but none of these things are helping with a development model for a cloud native era. Our frustrations with the existing developer experience have grown and Micro felt like something that could solve for all that, but only if we took a drastic step to overhaul it.
We think PaaS 3.0 is not just about running your container or even your source code but something that encapsulates the entire developer experience including a model for writing code for the cloud. Based on that Micro 3.0 aka M3O is a platform for cloud native development.
What is cloud native? What does it mean to build for the cloud? What is a cloud service?
Cloud native is basically a descriptive term for something that was built to run in the cloud. That’s it. It’s not magic, it might sound like a buzzword, but the reality is it simply means, that piece of software was built to run in the cloud. How does that differ from the way we used to build before? Well the idea behind the cloud is that its ephemeral, scalable and everything can be accessed via an API.
Our expectation for services running in the cloud is that they’re mostly stateless, leveraging external services for the persistence, that they are identified by name rather than IP address and they themselves provide an API that can be consumed by multiple clients such as web, mobile and cli or other services.
Cloud native applications are horizontally scalable and operate within domain boundaries that divide them as separate apps which communicate over the network via their APIs rather than as one monolithic entity. We think cloud services require a fundamentally different approach to software creation and why Micro 3.0 was designed with this in mind.
Micro 3.0 (M3O) reimagines Micro as a platform for cloud native development. What does that mean? Well we think of it as PaaS 3.0, a complete solution for source to running and beyond. Micro has moved from just being a Go framework to incorporating a standalone server and hosted platform. Our hosted offering is called M3O, a hat tip to Micro 3.0 or M[icr]o, whichever way you want to see it.
Another way to think about it. What Git is to GitHub, Micro is to the M3O platform. Let’s dig into it.
Micro 3.0 includes the following.
The server is our abstraction for cloud infrastructure and underlying systems you might need for writing distributed systems. The server encapsulates all of these concerns as gRPC services which you can query via any language. The goal here is to say developers don’t really need to be thinking about infrastructure but what they do need is design patterns and primitives for building distributed systems.
The server includes the following:
The server provides inter-service communication and two means of external communication with a HTTP API and gRPC proxy but that experience is made much better when there’s user experience on the client side that works. Right now we’ve got two ways of doing this.
One thing we really understood from our time working on go-micro was that the developer experience really matters. We see Go as the dominant language for the cloud and believe most backend services in the cloud will be written in Go. For that reason we continue to include a Service Framework which acts as a framework for building your services and accessing the underlying systems of the server.
The Service Framework provides pre-initialised packages for all of the features of the server and creates a convenient initialiser for defining your own services starting with service.New. A Service has a name, endpoints, contains a server of its own and a client to query other services. The framework does enough for you but then attempts to get out of your way so the rest is up to you.
A main package for a Micro service looks something like this
When you want to make use of something like the Config service just import it like so.
You can find many more examples in github.com/micro/services.
From our experience writing software isn’t constrained to a single environment. Most of the time we’re doing some form of local development followed by a push to staging and then production. We don’t really see tools capturing that workflow effectively. Thinking about how to do this now we’ve built in environments as a first class system.
M3O offers 3 builtin environments; local, dev and platform.
Our goal here is to really direct the flow from local > dev > platform as the lifecycle for any backend service development. Start by running the server locally, writing your code and getting it to work. Ship it to the dev environment for further testing but also to collaborate with others and serve it publicly. Then if you’re interested in a scalable and supported production environment, pay for the platform environment. That’s it.
Interact with the environments like so.
Micro isn’t constrained to our built in environments. You can add others as you wish.
The local environment is just that, your local laptop. Its where development starts and normally this requires you to run all sorts of crazy infrastructure. Micro focuses on providing pluggable abstractions as gRPC services so your service just talks gRPC directly to Micro and we hide the details from you. Locally that means we’re using best effort stuff like mdns, file storage, etc.
We’ve almost made it drop dead simple to start locally. You just run one command.
This will boot all the services you need and let you build a service that will look identical in any cloud environment running Micro as a Service.
Set your environment to the local server when using it.
Curl localhost:8080 with your namespace
Get your namespace like so
This might be blank locally but you’ll get the idea for how namespace isolation works in a bit.
The ‘dev’ environment is a free cloud hosted environment that provides Micro 3.0 as a Service. What we’ve learned in the past few years is that open source is not enough. There’s some great open source tools out there but as soon as we get to deployment there’s so many hurdles to overcome. The dev enviroment provides everyone the ability to get up and running in minutes with the same tools you’d use for local development in the cloud.
All you have to do is set the env to ‘dev’ and use it like local.
If you’re using the dev environment URLs are *.m3o.dev. Find more details at m3o.dev
The ‘platform’ environment is a secure, scalable and supported production environment for where you’d likely run customer facing services and products. This is a paid tier with 2x the resource limits of dev to start including slack & email support along with SLAs. You can think of it as the equivalent of a production platform you’ve come to know at any work place.
Our goal with Local, Dev and Platform is to invoke that workflow we’ve all come to know and expect as a real product. These are totally separate environments and they’re managed exactly as that with M3O as well.
With the advent of a system like kubernetes and a push towards the cloud we can see that there’s really a need to move towards shared resource usage. The cloud isn’t cheap and we don’t all need to be running separate kubernetes clusters. In fact wouldn’t it be great if we could share that? Well Micro is doing it. We build in multi-tenancy using the same logic kubernetes does called Namespaces.
We’ve mapped this same experience locally so you get a rudimentary form of namespacing for local dev but mostly we’re making use of kubernetes namespaces in production along with a whole host of custom written isolation mechanisms for authentication, storage, configuration, event streaming, etc so Micro 3.0 can be used to host more than one tenant.
Whether you decide to self host and share your cluster for dev, staging and production we felt like multi-tenancy needs to become a defacto standard in 2020. How it works in practice. Each tenant get’s a namespace. That namespace has its own isolated set of users and resources in each subsystem. When you make any request as a user or service, a JWT token is passed with that so the underlying systems can route to the appropriate resources.
Once you’ve signed up to the dev environment your namespace will be set for you. You can get it using the command
When you’re using any sort of CLI commands, your namespace and auth token are automatically injected into request including refreshing those tokens. The same happens for any of your services running on Micro. If you want to use the http API or the public api url [api.m3o.dev] then go ahead and grab your namespace and set the header as Micro-Namespace.
Additionally each namespace gets its own custom domain so the foobar namespace becomes foobar.m3o.dev with say the helloworld service routing would be to foobar.m3o.dev/helloworld.
Micro was built out of a frustration with the existing tools out there. One of the things I’ve really been saying for a long time is that I wanted “source to running” in just one command. With Heroku we sort of got that but it really took too much away from us. Back in 2010 Heroku was focused on monolithic Rails development. Since then I’ve really said Heroku took too much away and AWS gave too much back. We needed something in between.
Micro can take your source code, from a local directory or a repo thats hosted on github, gitlab or bitbucket. In one command it will upload or pull from the relevant place, package it as a container and run it. That’s it. Source to running in just one command. No more need to deal with the pipeline, no more hacking away at containers and the container registries. Write some code and run it.
Source to running is cool. It’s what a PaaS is really for but one thing that’s really been lacking even with the new PaaS boom is a development model. As I eluded to, Heroku takes too much away and AWS gives too much back. We’re looking for a happy medium. One that doesn’t require us to rely on VMs or containers but on the other side doesn’t limit us to monolithic development.
Micro has always focused on the practice of distributed systems development or microservices. The idea of breaking down large monolithic apps into smaller separate services that do one thing well. To do this we think you really have to bake the development model into the platform.
What we include is the concept of a Service which contains a Client and Server for both handling requests and making queries to other services. We focus on standardisation around protobuf for API definitions and using gRPC for the networking layering.
Not only that we’re including pubsub for an event streaming architecture and other pieces like nosql key-value storage and dynamic config management. We believe there are specific primitives required to start building microservices and distributed systems and that’s what Micro looks to provide.
One of the key learnings we had from the development of a Go framework called go-micro was that we mostly use a single language for each platform we develop for such as web, mobile and so on. Cloud will be no different. We support Go for the Cloud, but think there needs to be an ecosystem for consumption of Go services and potentially extending beyond where there’s no way around using python, java, ruby, rust or javascript. Because Micro’s interface is gRPC we code generate gRPC clients and allow any language to leverage the Micro server.
In the past multi-language clients have been pain stakingly hand crafted and one thing we learned from building a framework, it’s incredibly hard to replicate this across languages also. With gRPC we’ve really found a happy medium of saying, there’s a built in service framework you can use to write code really elegantly with Go but gRPC allows us to reduce the scope of the surface area and provide strongly typed clients that can support a different model of development, one that might have more scope for pushing microservices to wide scale adoption in a way that wasn’t possible with frameworks.
We additionally include grpc-web generated clients which enable frontend to quickly and easily make use of typed javascript clients to leverage the same development as the backend. We’ve seen grpc-web slowly gain adoption internally at various companies and think this might extend to the public domain fairly rapidly as well.
See the micro/client/sdk directory for the generated clients. These will be pubished to their respective package managers in the near future.
Micro was built to make microservices development much easier and to increase developer productivity on the backend, beyond being able to consume those services using gRPC we think the world still really cares about HTTP/JSON based APIs and so Micro include an API gateway which translates http/json to grpc requests automatically. This means everyone is building API first services in the cloud without having to do anything.
Here’s a quick example.
Say you write helloworld on the backend with the following proto
Then expose this as the “helloworld” service on the M3O platform. You’ll instantly be able to access this as $namespace.m3o.dev/helloworld/message
We use path based resolution to map a http request to gRPC. So /[service]/[method] becomes [Service.Method]. If your microservice name doesn’t match the proto for whatever reason (you have multiple proto Services) then it works slightly differently e.g your service name is foobar then the endpoint becomes /foobar/helloworld/message.
One neat hack we’ve picked up from web browser is auto detecting an endpoint so we can shorthand something to something like /helloworld. With the web if an index.html page is found its served. In our case if we find the Call method in your proto we’ll automatically use it so /helloworld/call just shortens to /helloworld.
With Stripe, Twilio, Segment and others become huge API players, we think the world is going in that direction and you are probably building http apis too. So Micro builds in this in as a first class primitive. In future we’ll also look to include support for graphql.
Alright so we talk a good game, but how easy is it? Well lets show you.
Easy right? We see this as the common flow for most service development. Its a fast iterative loop from generating a new template to shipping it and querying to make sure it works. There’s additional stuff in the developer experience like actually writing the service but we think that’s a separate post.
Another thing we really learned from the past is nothing like this works without great documentation and tutorials. So we’ve written a whole suite of docs for Micro available at micro.mu and provide help for using the M3O on m3o.dev.
You can find other interesting resources at Awesome Micro.
Micro continues to remain open source but licensed using Polyform Shield which prevents the software for being picked up and run as a service. This is to contend with AWS and others running open source for profit without contributing back. It’s a longer conversation for another day.
We really believe that writing software for the cloud is too hard. That there’s far too much choice and time wasted focusing on how to piece everything together. There are tradeoffs to adopting a PaaS but ultimately our focus is developer productivity. By choosing one tool and one way we stop thinking about the how and just get down to what we’re trying to build.
M3O and Micro 3.0 look at the state of distributed systems development in the cloud native era and try to drastically simplify that experience with a platform that bakes in the development model so you can just get back to writing code.
We will now be ending support for go-micro. Having personally spent 6 years since inception on go-micro I feel as though its time to finally let it go. What started as a tiny library to help write Kubernetes-as-a-Service back in 2014 turned into a widely used open source framework for Go microservices development. Having now amassed more than 14k stars you might wonder why we leave it behind. The truth is, while it solved a problem for many it never became what it was intended for.
Go Micro was built on the premise that developers need a simpler way to build distributed systems. With strongly defined abstractions and a pluggable architecture it did that well but that became really unwieldy to manage. With an MxN matrix of complexity, Go Micro became the thing it was trying to fight against. As we attempted to hone on this platform effort, it just became very clear that to do that we’d need to start fresh.
Go Micro will live on as an independent library under my own personal account on GitHub but it will no longer be supported as an official Micro project. Hopefully it finds second life in some other ways but for now we say goodbye.
If you’d like to upgrade from Go Micro v2 to Micro v3 please see this upgrade guide.
You can use the Micro 3.O as a self-hosted open source solution locally, on a VPS or managed kubernetes, whatever works for you. Our goal is to facilitate a vastly superior developer experience for building services in the Cloud. Come join Discord or Slack to chat more about it. And lastly head to to m3o.com if you’re tired of the way you’re building software for today and want to learn of a better way that’s going to make you 10x more productive.
So to revisit. To get started for free in the cloud based dev environment just run the following commands.
If you want to test things out locally first
And that’s it! Please come chat with us in Discord or Slack and invite friends to test out the M3O platform.
To learn more about the M3O platform see the dev docs at m3o.dev. And for the open source docs check out micro.mu.
Written by Asim AslamFounder & CEO Micro
Programming the real world
8 
8 claps
8 
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
Written by
Working on Micro
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-3beb1ab820a?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Feb 12, 2020·5 min read
PART FIVE — Implementing scalable workflows on the public cloud with Terra
In part one, I presented the general rationale for building quick start examples (or ‘Hello World’) for bioinformatics tools.
In part two, I reviewed one such example for the CSIRO Bioinformatics VariantSpark library, runnable via smart templates in the AWS Marketplace.
In part three, I covered how to convert a locally runnable example to a reusable cloud example, by working with Google Cloud Platform custom Virtual Machine images…
"
https://medium.com/@datastax/the-search-for-a-cloud-native-database-8694f87b9f33?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
Aug 11, 2021·6 min read
By Cedrick Lunven and Jeff Carpenter, DataStax
The concept of “cloud-native” has come to stand for a collection of best practices for application logic and infrastructure, including databases. However, many of the databases supporting our applications have been around for decades, before the cloud or cloud-native was a thing. The data gravity associated with these legacy solutions has limited our ability to move applications and workloads. As we move to the cloud, how do we evolve our data storage approach? Do we need a cloud-native database? What would it even mean for a database to be cloud-native? Let’s take a look at these questions.
It’s helpful to start by defining terms. In unpacking “cloud-native,” let’s start with the word “native”. For individuals, the word may evoke thoughts of your first language, or your country or origin — things that feel natural to you. Or in nature itself, we might consider the native habitats inhabited by wildlife, and how each species is adapted to its environment. We can use this as a basis to understand the meaning of cloud-native.
Here’s how the Cloud Native Computing Foundation (CNCF) defines the term:
“Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds: Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.
These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.”
This is a rich definition, but it can be a challenge to use this to define what a cloud-native database is, as evidenced by the Database section of the CNCF Landscape Map:
Databases are just a small portion of a crowded cloud computing landscape
Look closely, and you’ll notice a wide range of offerings: both traditional relational databases and NoSQL databases, supporting a variety of different data models including key/value, document, and graph. You’ll also find technologies that layer clustering, querying or schema management capabilities on top of existing databases. And this doesn’t even consider related categories in the CNCF landscape such as streaming and messaging for data movement, or cloud native storage for persistence.
Which of these databases are cloud-native? Only those that are designed for the cloud, should we include those that can be adapted to work in the cloud? Bill Wilder provides an interesting perspective in his 2012 book, “Cloud Architecture Patterns”, defining “cloud-native” as:
“Any application that was architected to take full advantage of cloud platforms”
Or to ask a different way, what are the advantages of a cloud-native database? Consider the two main factors driving the popularity of the cloud: cost and time-to-market.
These goals apply to your database selection, just as they do to any other part of your stack.
Now we can revisit the CNCF definition and extract characteristics of a cloud-native database that will help achieve our cost and time-to-market goals:
Cloud-native databases are designed to embody these characteristics, which distinguish them from “cloud-ready” databases, that is, those that can be deployed to the cloud with some adaptation.
Let’s test this definition of a cloud-native database by applying it to Apache Cassandra™ as an example. While the term “cloud-native” was not yet widespread when Cassandra was developed, it bears many of the same architectural influences, since it was inspired by public cloud infrastructure such as Amazon’s Dynamo Paper and Google’s BigTable. Because of this lineage, Cassandra embodies the principles outlined above:
While automating the initial deployment of a Cassandra cluster is a relatively simple task, other tasks such as scaling up and down or upgrading can be time-consuming and difficult to automate. After all, even single-node database operations can be challenging, as many a DBA can testify. Fortunately, the K8ssandra project provides best practices for deploying Cassandra on Kubernetes, including major strides forward in automating “day 2” operations.
Speaking of Kubernetes … when we talk about databases in the cloud, we’re really talking about stateful workloads requiring some kind of storage. But in the cloud world, stateful is painful. Data gravity is a real challenge — data may be hard to move due to regulations and laws, and the cost can get quite expensive. This results in a premium on keeping applications close to their data.
The challenges only increase when we begin deploying containerized applications using Kubernetes, since it was not originally designed for stateful workloads. There’s an emerging push toward deploying databases to run on Kubernetes as well, in order to maximize development and operational efficiencies by running the entire stack on a single platform. What additional requirements does Kubernetes put on a cloud-native database?
First, the database must run in containers. This may sound obvious, but some work is required. Storage must be externalized, the memory and other computing resources must be tuned appropriately, and the application logs and metrics must be made available to infrastructure for monitoring and log aggregation.
Next, we need to map the database’s storage needs onto Kubernetes constructs. At a minimum, each database node will make a persistent volume claim that Kubernetes can use to allocate a storage volume with appropriate capacity and I/O characteristics. Databases are typically deployed using Kubernetes Stateful Sets, which help manage the mapping of storage volumes to pods and maintain consistent, predictable, identity.
Finally, we need tooling to manage and automate database operations, including installation and maintenance. This is typically implemented via the Kubernetes operator pattern. Operators are basically control loops that observe the state of Kubernetes resources and take actions to help achieve a desired state. In this way they are similar to Kubernetes built-in controllers, but with the key difference that they understand domain-specific state and thus help Kubernetes make better decisions.
For example, the K8ssandra project uses cass-operator, which defines a Kubernetes custom resource (CRD) called “CassandraDatacenter” to describe the desired state of each top-level failure domain of a Cassandra cluster. This provides a level of abstraction higher than dealing with Stateful Sets or individual pods.
Kubernetes database operators typically help to answer questions like:
A cloud-native database is one that is designed with cloud-native principles in mind, including scalability, elasticity, resiliency, observability, and automation. As we’ve seen with Cassandra, automation is often the final milestone to be achieved, but running databases in Kubernetes can actually help us progress toward this goal of automation.
What’s next in the maturation of cloud-native databases? We’d love to hear your input as we continue to invent the future of this technology together.
Learn about DataStax Astra, the multi-cloud DBaaS built on Cassandra
This post is based on Cedrick’s presentation “Databases in the Cloud-Native Era” from BluePrint London, March 11, 2021 (registration required).
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
14 
14 
14 
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
"
https://medium.com/@odsc/the-benefits-of-cloud-native-ml-and-ai-b88f6d71783?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
ODSC - Open Data Science
May 2, 2019·5 min read
As big data gets more complex, companies are struggling to accommodate the storage and computing needs of average organizations, much less massive enterprises. This is where cloud-native ML and AI comes into play.
Your computing power is limited. No matter what kind of hardware and software you buy, you’ll always be tiptoeing towards obsolescence. This is normal — and has been for years — but with the advent of big data and AI, we’re tiptoeing there just a little bit faster.
Enter cloud-native applications. The introduction of the cloud has democratized computing capability. Companies can deploy applications at scale using massive cloud computing and storage capabilities. Cloud native apps never settle into in-house systems. Instead, they run in the elastic computing environment, delivering reusable features through things like containers that can operate using agile.
Short story? You can deploy programs faster, progressively bigger, and with fewer taxes on your limited computing resources. Plus, the cloud can package your application into a container, allowing you to replicate the results across multiple platforms.
Machine Learning requires datasets for training models to perform tasks. The machine brain learns to identify a hotdog versus “not a hotdog,” for example, by consuming raw training in the form of pictures of hotdogs versus not hotdogs. Accessing the type of data you need for training more complex tasks can be frustrating when you don’t have the human or processing power you need.
Your sole data scientist can’t go through thousands of piece of data to train the machine to recognize the hot dog. Instead, by deploying those tools in the cloud, a company could automate machine learning and operate at scale.
The cloud allows organizations to use automated or managed machine learning to remove the tax on limited human resources. It removes limitations and access to data and machine learning, allowing all stakeholders to access the program and insights. In short, everyone gets to know which is a hotdog and which isn’t.
It also opens the door for citizen data scientists to deploy programs without having experience in code. The cloud uses automation to train and serve out a model. The user can evaluate the model, debug, and replicate results from the cloud directly.
[Related Article: Should You Build or Buy Your Data Science Platform?]
AI takes the same approach on the cloud as machine learning, but the focus is a bit broader. In fact, AI is becoming the driver of cloud computing. Companies can deploy AI models and deep learning to the elastic and scalable environment of the cloud.
Cloud Native AI is less a new technology and more a paradigm shift. Businesses can use the cloud to deploy AI micro-services, for example, or find data lakes for better, more in-depth training required for deep learning. It’s beyond hotdog/not a hotdog, and more like learning how to make the hotdog itself.
Cloud-based AI can be added to the three pillar services so desperately needed by business: sight, language, and conversation. Cloud AI gives businesses the ability to deploy these services without having to have native AI apps. Organizations can leverage cloud AI for their existing applications, access data lakes, and perform big data pulls.
The cloud environment is elastic. This is the most significant advantage for most businesses because the ability to customize how much data and where it’s stored without later implementing costly upgrades and system changes is an absolute game changer. The environment grows or pivots with your development. This leads to three critical things:
Reliable scale: Your growth isn’t hampered by your dependence on an established in-house repository. You can work with your existing solutions and keep one eye on expansion without the expensive and time-consuming software switch.
Microservice capability: Microservices are modular and easily follow an agile development timeline. Your organization can deploy microservices to automate a product or service for your customer base and relieve the burden on your human team, or you can make use of microservices for targeted development.
Data Lakes: Data lakes housed in the cloud give your organization access to bigger, better data for training without straining your in-house resources. As you deploy newer models and race towards the continuous innovation ideal, building on the resources available outside of your organization gives you a strategic advantage.
Kevin Wang of braze.com recently said, “The experience of effectively using AI is more like riding a horse, and less like having a super-intelligent horse steal your job.” For many people, this idea is comforting because no, in fact, AI is not coming for your job. There’s a flipside to this idea, however, because AI cannot do your job for you. Not really.
Deploying AI models or ML in the cloud doesn’t create a magic bullet for your product, service, or workflow. It enhances what you’ve already established. If you’ve got something great, those things make it better. If you’ve got something half put together, those things only amplify that confusion. If you don’t know where you’re going, your “horse” doesn’t either.
Other possible disadvantages center around the issue of control. Although you largely control your applications, services, and data, your back-end may reside with someone else. This possibility could be useful if you don’t have the infrastructure to handle true security and maintenance, but you’re more open to:
Downtime: Downtime is inevitable, and the good news is that your in-house IT team doesn’t have to work around the clock to figure out what’s going on. The bad news is that you have no control over when and where. If you don’t have a system in place to deal with downtime (expected and otherwise), you could lose out on your bottom line.
Service obstacles: If you can’t control your backend, you need to consider how your cloud provider will support your organization. Make sure you understand the service level agreements and that the SLA covers what you need.
[Related Article: 5 Mistakes You’re Making with DataOps]
Building a culture of continuous innovation can be difficult without proper infrastructure. For the first time, organizations have access to software and services that go far beyond their in-house capabilities. If you’ve got a scrappy startup, you could compete with giants in your industry quickly, as in right now. Consider the benefits and the downsides to see if you’re ready to make the switch to a cloud-native system.
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
15 
15 
15 
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
"
https://medium.com/buildpacks/cloud-native-buildpacks-2021-roadmap-e5ece588fc0?source=search_post---------126,"There are currently no responses for this story.
Be the first to respond.
As we turn the page on a new year, the Buildpacks project would like to share our assessment of last year, and our plans for the coming months. 2020 was challenging, but we’re proud to say that we still accomplished a great deal and positioned ourselves well for another productive year.
Our north star last year was coming of age, and we prioritized work that helped the project mature and become production ready. We established a well defined release cadence, improved reproducibility and distribution of Buildpacks, and the CNCF promoted the project to Incubation level.
Now that we’ve come of age, we want to make Buildpacks more accessible to the broader Cloud Native ecosystem. The 2021 roadmap theme is: Sustainable Growth.
We will focus on projects that help grow our community, contributors, and adopters. We’ll prioritize projects that make Buildpacks more accessible to a wider audience and we’ll try to solve problems that have prevented some users from adopting Buildpacks.
Last year, we sent a clear signal to the industry that Buildpacks are ready for production. Our technology has been adopted by a diverse set of companies and projects. Now we want to bring those adopters into the fold, and encourage them to join our community.
In the early part of 2021, we launched the Buildpack Registry, which allows adopters to publish their buildpacks and end users to discover them. We’ll continue making the registry better this year. We’ll also prioritize work to improve our documentation, and we’ll block releases if our documentation isn’t good enough.
We’re also aiming to define a better vision for how buildpacks fit into the broader Cloud Native ecosystem. For many in our community, it’s not obvious how Buildpacks work with Docker, Kubernetes, and other technologies that sit alongside our project in the CNCF landscape. One of our goals for the year is to deliver an out-of-the-box experience that works well with these tools.
The Buildpacks project has ten maintainers and more than a dozen contributors. We receive contributions from many different companies, but we think we can do better. The majority of Buildpacks adopters have not participated in our RFC process or joined in our design discussions, which may indicate that we haven’t been as welcoming as we could. This year, we want to diversify our contributor base in every dimension. We want to ensure that we remain inclusive, and we want to gain team members from different corners of the industry and from around the globe.
As part of this, we want to make it easier to understand the project and how to contribute to it, which is why we’re aiming to refactor our specification. The Buildpacks spec is our foundation, but after several years of iteration, it’s become confusing to those who read it for the first time. We hope that revisiting the spec will both encourage new adopters and make it easier to contribute.
Above all this year, we want to continue growing our user base. We’ll do this by shipping great features like Stack Buildpacks, and giving users more configuration “escape hatches” like Inline Buildpacks. These highly requested features will satisfy use cases that have hindered adopters in the past.
One of our more aggressive goals is to improve the development inner-loop for Buildpacks users. We’re aiming to define a Test Buildpack API, which would allow users to run their application tests in a buildpacks-based environment. Similarly, we hope to bring some of the existing patterns for developing in a buildpacks-based container into the buildpack specification.
We hope these goals will help the project, the people, and the technology behind Buildpacks to grow at a sustainable pace. And we believe we’re best poised to make progress when we gather together as a community. If you’re reading this, we want to hear from you!
To learn more about the 2021 Cloud Native Buildpacks roadmap and provide your input, see our Community Github repository. Or join the conversation on Slack. We’d love to hear your feedback about any of the items on our roadmap.
Buildpacks are pluggable, modular tools that translate…
36 
Thanks to Javier Romero, Natalie Arellano, David Freilich, and Terence Lee. 
36 claps
36 
Written by
I’m an architect at Salesforce.com who writes about software and related topics. I’m a co-founder of buildpacks.io and the author of The Healthy Programmer.
Buildpacks are pluggable, modular tools that translate source code into OCI images.
Written by
I’m an architect at Salesforce.com who writes about software and related topics. I’m a co-founder of buildpacks.io and the author of The Healthy Programmer.
Buildpacks are pluggable, modular tools that translate source code into OCI images.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/bouyant-ceo-on-why-cloud-native-why-now-and-why-doing-it-at-twitter-was-very-hard-91ced2b845c0?source=search_post---------127,"Buoyant co-founders William Morgan and Olivier Gould were infrastructure engineers at Twitter during its transformation from site that regularly displayed the fail whale to site that stayed up almost always, so it could serve its actual purpose of displaying tweets. A move to microservices was a big part of Twitter’s infrastructure evolution, which meant scaling yet-unproven tools such as Apache Mesos to new limits and building tools such as Finagle to let all those service communicate with each other.
In this episode of the ARCHITECHT Show podcast, Morgan, who’s also Buoyant’s CEO, discusses how Buoyant and its flagship linkerd “service mesh” technology emerged from the lessons he and Gould learned at Twitter. Morgan explains what linkerd is, how it works and how it connects to the broader cloud-native ecosystem that also includes Docker, Kubernetes and Mesos. He also touches a wide range of other topics, including: Buoyant’s recent $10.5 million series A round, led by Benchmark Capital; his take on successful open source business models; why scaling Twitter was such a unique experience; and why doing microservices today is a much easier proposition.
Keep reading for highlights from the podcast interview with Morgan, and scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
This week’s episode brought to you by:
In the news segment, co-hosts Derrick Harris (ARCHITECHT) and Barb Darrow (Fortune) discuss Microsoft’s new AI group and collection of AI efforts; Intel’s continued data center dominance; and IBM’s new Watson-powered approach to managing data center networks.
Here are some highlights from the interview with William Morgan, but you’ll definitely want to listen to the whole thing for more insights into the challenges of building Twitter’s infrastructure, the value of microservices and today’s cloud-native technologies, open source business models, and raising $10.5 million to scale Buoyant’s ambitions.
news.architecht.io
“[A] service mesh is … a dedicated infrastructure layer for managing service-to-service communication. What happens if you look at this historically is in the days of the three tiered apps, you’d have some web server in front … and you’d have some big, monolithic blob in the middle doing all the business logic, and then you’d have some database in the back. And you’d have those two layers of communication there: from web server to app code, and from app code to database. And that was the extent of your service, your internal communication. Then as we moved forward in time, as the applications got more complicated, we started breaking down that middle application layer. This is the microservices movement or this is what at Twitter, we didn’t have that word, so we called it SOA.
“And now you start having the systematic service-to-service communication that happens as part of that. … What the service mesh does is it gives you the same service-to-service communications management and visibility that we had [at Twitter, and that companies like Google, Facebook and Netflix built for themselves]. But it brings it as a user-space proxy, so it’s just an implementation detail, really, the way that a service mesh forces you to deploy it alongside your application code and you send requests … through it.”
architecht.io
“The value of the service mesh is primarily around reliability. The thing that the service mesh does, what linkerd does for you, is things like, ‘How do I handle retries and timeouts?’ ‘How do I handle deadlines?’ ‘How do I handle circuit-breaking?’
“Any time that service A wants to talk to service B, in our heads we imagine that as like, ‘Oh, there’s a box on the white board that’s labeled A, then we draw an arrow to B.’ But in practice, A is hundreds of instances, B is thousands of instances, those instances are all in various states of failure or latency or whatever, and Kubernetes or Mesos is rescheduling things all the time, so even where things are is changing all the time. It’s not just A talks to B, it’s A talks to B, talks to C, talks to D.
“So, the actual topology of what’s happening is very, very complicated. And that logic, traditionally, has been in the application code. We write our little retry loops and our timeouts. The problem with doing that is that you do something that seems very, very simple, but you open yourselves up to all sorts of terrible failure modes by doing it. So, Service A will talk to B and we’ll say, ‘Three retries and 500 millisecond timeouts.’ But the way you implement that, the way the services get chained together, you’re actually opening yourself up to catastrophic failures.
architecht.io
“Why are we talking about cloud-native at all? Well, it’s because we want to move on to cloud “hardware.” … We want to move onto the cloud. And what do we know about the cloud? Well, gosh, it’s a whole bunch of hardware that we don’t actually control, and that’s in various states of reliability, and that you’ve got other tenants in there that are doing things and your stuff gets affected.
“So the failure, or rather the reliability, guarantees we used to have when we were building on hardware—on our own hardware, on our own data centers—are no longer there. We have no guarantees from the hardware anymore, which means we have to build the reliability into the software. That’s the fundamental reason why people want to build cloud-native software, why this approach makes sense. …
…
“From my perspective, what do I care whether you actually want to run on Amazon’s hardware on your own? The way that you’re building the software is still a good way. And the way that you’re operating it with Docker and Kubernetes, Mesos and linkerd, it still gives you a lot of value. That thing that’s really changed is the cost of doing that has gone down. It used to be the case that, gosh, if you wanted to have multiple services, man, deploy time was going to be really painful. It’s already so difficult with one, how are we going to do this with 10, with 100? Things like Kubernetes and Docker have really reduced that cost.”
architecht.io
“The way I think about it is that any technical choice has a value and it has a cost. The value for Twitter doing this—adopting this cloud-native approach—was really, really high. The site was falling over. … That’s like a survival requirement for a company—the site has to be up—so the value was really high.
“But the cost that it paid was incredibly high. We had to bring Mesos from a grad-student project to a production-worthy system. We had to invent Finagle. We had to do a tremendous amount of core infrastructure engineering to make this work. …
“Now, what’s happened over the past [several years] … is the cost of companies adopting the cloud-native approach has gone down dramatically. Now, we have Docker. Now, we have Kubernetes. Now, we have these microservices patterns that we can rely on. And so now you can adopt this thing and you can get a lot of the value without having to pay the big, big costs.
“Now, we’re seeing adoption of things like linkerd, not just at companies that have tremendous scale, but at companies that are adopting it for the same reason they’re adopting Docker, the same reason they’re adopting Kubernetes—because there’s value to being cloud-native, and there’s value even at low scale or, at least by Twitter standards, what’s low scale.”
architecht.io
“One thing that I really feel confident about is the way that people are building these applications—the cloud-native approach where you’re splitting things down into … lots of different services and there’s interplay between those things. I think if you take that to a logical extreme, you end up with something like Lambda.
“… I think it’s the right approach for this world that we’re moving into where the hardware is further and further abstracted away. I feel good about that. I think the implementation details will be different. Five years from now, will we still be using Kubernetes? Will we still be using Docker? I don’t know. Will we be using some kind of orchestration system? Almost definitely. Will we be using some kind of packaging and isolation mechanism? Almost definitely.
“So these patterns we see, I think, are the right patterns. And it feels right based on our experience at Twitter and based on what we’re seeing companies using linkerd [for] today.”
architecht.io
“It hasn’t really changed it a whole lot because I think we knew going into this that the only way for an open source business to be successful is if the open source project is successful. And the only way that that project is successful is if it’s decoupled in a large way from the actual profit motive, or the business concerns, behind it. Obviously, there’s a relationship there — we are taking money and turning that into linkerd code and linkerd value — but the project itself has got to be an independent thing. … The users and the members of linkerd community have to trust us, and they have to trust the project to not have a real hidden ulterior motive in there.
“So the fact that it’s a CNCF project rather than a Buoyant project doesn’t really change anything because we already we’re conceding it as a community project.
…
“Now, there might have been a small period of time, which lasted about 30 seconds, where we we’re like, ‘Hmm, maybe linkerd should be a totally proprietary technology.’ But, like I said, that lasted about 30 seconds.”
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
7 
7 claps
7 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://digitizingpolaris.com/is-hortonworks-rewriting-hadoop-to-make-it-cloud-native-or-just-its-own-platform-be816ff3335e?source=search_post---------128,"Maybe Hortonworks (NASDAQ:HDP) wasn’t trying to upstage Cloudera at the start of the Strata Data conference it hosts in New York each year, but on the eve of the event they announced their Open Hybrid Architecture Initiative (OHAI) which intends to rewrite Apache Hadoop to make it cloud-native and container friendly.
Hortonworks’ co-founder and CPO Arun Murthy authored a lengthy blog post explaining the reasoning behind the move.
It pretty much boils down to this: Hadoop, as we know it, was architected for on-premises computing, with coupled storage and compute, before artificial intelligence and real time analytics were practical. As such, it fails to leverage many of the advantages of cloud computing or fit the needs of the modern enterprise. Or, in Murthy’s words:
“Organizations need a unified hybrid architecture for on-premises, multi-cloud and edge environments. The time has come to once again re-imagine the (Hadoop)data architecture, with hybrid (cloud+on-premises+edge) as a key requirement.”
Though many changes will be required, the primary ones are the containerization of Hadoop (in Hortonworks’ case, specifically Hortonworks Data Platform (HDP)) and Hortonworks Data Flow (HDF.)
The HDFS component of HDP (and Apache Hadoop?)will likely be replaced with Apache Hadoop Ozone, which leverages Hadoop Distributed Data Storage (HDDS) and is more suited to the cloud. Both Apache Hadoop Ozone and HDDS are being “actively developed on the trunk branch of Apache Hadoop.” Judging by the list of contributors on Github’s Ozone and HDDS branch pages, the contributors are all (there may be an exception) Hortonworks employees.
While this might have seemed disruptive a few years ago, as Gartner analyst Merv Adrian put it in his January 2018 Hadoop Tracker post, “This is normal product maturation as vendors differentiate their offerings from one another. The “disaggregation” of what was the “common Hadoop stack” continues, and many of the clients I talk to are adding other Apache and non-Apache pieces into their stacks as uses dictate and/or vendor offerings contain them.”
Hortonworks, which is a staunch evangelist and practitioner of the “Apache Way,” would, no doubt, welcome the employees of other independent commercial Hadoop providers (Cloudera and MapR)on its journey to build a “cloud native, container friendly” Hadoop — but they’re not likely to come. Just like they weren’t when Hortonworks and Pivotal started the Open Data Platform initiative (ODPI) in 2015.
At that time, the creation of ODPI clearly made Cloudera co-founder Mike Olson angry. “(It) is antithetical to the open source model and the Apache way. While the ASF is open to vendors, the ODP isn’t actually open at all,” he wrote in a blog post.
We invited Cloudera to comment on OHAI, but have not received a response. However, off of the record, insiders have suggested that Cloudera is re-architecting portions of Cloudera Enterprise to be cloud-native.
That won’t be necessary at MapR, according to Jack Norris, senior vice president of data and applications. That’s partly because MapR ‘s Converged Data Platform isn’t architected with HDFS. It evolved from MapR-FS, a clustered file system that supports both very large-scale and high-performance uses.
That is where MapR made the right bet, according to Constellation Research vice president and principal analyst Holger Mueller. “MapR sits pretty as the founders wanted to separate storage from day one,” he says.
Norris, for his part, points out that “MapR’s data fabric already includes a natively integrated Kubernetes volume driver to provide persistent storage volumes for access to any data ­–­ from databases, files and streaming — located on-premises, across clouds and to the edge.”
Regardless, instead of being snarky, he calls Hortonworks’ OHAI initiative “clever,” and a “good move,” noting, as did Ovum analyst Tony Baer, that it will take “time” (as in a long time) for Hortonworks to deliver on its aspirations.
Murthy said as much in the blog post. “Hortonworks has been on a multi-year journey toward cloud-first and cloud-native architectures. The Open Hybrid Architecture initiative is the final piece of the puzzle.”
It’s important to consider that Hortonworks won’t be working alone. While Cloudera and MapR are unlikely to join them in the effort, Red Hat and IBM have already committed to helping out with OHAI’s third phase and Hortonworks is, no doubt,wooing others.
Some of them may come along as cloud-native and containerization is the way to go. “ All vendors are moving to containers to scale compute parts of their offerings. For database vendors the tricky part is that they traditionally wanted to bring the compute to the data, for performance advantages. But this needs to be decoupled now, and Hortonworks is making the right step in the right direction,” says Mueller.
News and narratives on the trek to the digital enterprise.
9 
1
9 claps
9 
1
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
"
https://medium.com/@azeynalli1990/cloud-native-architecture-patterns-part-2-9704f160525f?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ali Zeynalli
Nov 22, 2021·3 min read
Anti-Corruption Layer, CQRS, Event Sourcing
Since Cloud-Native applications get extremely popular and almost all companies build their new systems on cloud nowadays, different patterns addressing cloud specific problems get popular as well. As I mentioned in first part: Software Architecting might take a slightly different approach in applications that are build in cloud-native…
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/cockroach-labs-ceo-on-building-a-database-for-a-cloud-native-world-ea52e1a6bedd?source=search_post---------130,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this episode of the ARCHITECHT Show, Cockroach Labs co-founder and CEO Spencer Kimball talks all about CockroachDB, his company’s open source take on the Google Spanner database, and the experience of building an enterprise software company in today’s IT landscape. Among other things, Kimball discusses how SQL keeps evolving to meet changing needs (such as geographically distributed transactions); taking advantage of Kubernetes, containers and cloud-native; figuring out the right open source license; and whether the company’s name scares people away.
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
16 
16 claps
16 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/wasm/cloud-native-webassembly-in-service-mesh-b19e3a96ccf8?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
A holy grail of modern enterprise architecture is the separation of concerns between infrastructure and business logic code. Ideally, application developers want to focus on business logic and are not concerned about how to manage and scale applications. That is the “serverless” paradigm.
In cloud-native environments, a commonly used approach is service mesh. The service mesh allows application units, or microservices, to run in distributed runtimes or containers. These runtimes are also called sidecars. The service mesh also provides a data plane, or API proxies, to manage data in and out of the sidecar microservices, as well as a control plane to configure policies for the data plane.
As microservices become more modular and lighter on one hand, and take over more complex application logic (eg AI inference) on the other hand, there is a growing need for lightweight runtimes for both sidecars and data plane proxies to improve system performance and reduce resource consumption by the growing infrastructure. After all, it is absurd to spin up a Docker container and guest OS just to execute a microservice function that has 10 lines of code.
The WasmEdge Runtime is a CNCF hosted WebAssembly runtime project. It is optimized for cloud-native applications and use cases. It could be a lightweight and high-performance runtime for service mesh sidecars and proxies. Based on the WebAssembly standard, WasmEdge supports multiple programming languages, including Rust and even JavaScript, and offers a safe runtime sandbox. It could be 100x faster and lighter than application containers like Docker, especially at startup time.
For sidecar frameworks that support multiple application runtimes, we could simply embed WasmEdge applications into the sidecar through its C, Go, Rust, or Node.js SDKs. A good example is Dapr. We have a template project that showcases how to run WasmEdge microservices as Dapr sidecars.
Similarly, you can also embed WasmEdge applications and functions in public clouds’ serverless services. Here are examples for AWS Lambda, Tencent Cloud, Vercel, and Netlify.
However, widely used service meshes, such as CNCF’s Linkerd, uses Kubernetes to manage sidecars. To that end, WasmEdge provides an OCI-compliant wrapper for its WebAssembly runtime and supports network sockets in WebAssembly to listen for incoming API requests directly. That allows WasmEdge applications to be managed directly by container tools and act as sidecar microservices.
The API proxy is another crucial component in the service mesh. It manages and directs API requests to sidecars in a manner that keeps the system scalable. Developers need to script those proxies to route traffic according to changing infrastructure and ops requirements.
Envoy Proxy, another CNCF project, is the first API proxy to support WebAssembly as an alternative to the LUA scripting language.
The Easegress proxy published an excellent hypothetical case study on how a WebAssembly-based extension can orchestrate traffic spikes for an e-commerce giant’s one-day flash sale.
Seeing widespread demand for this type of application, the community came together and created the proxy-wasm spec. It defines the host interface WebAssembly runtimes must support in order to plug into the proxy. WasmEdge is a WebAssembly runtime that supports proxy-wasm, enabling it to be used as extensions for Envoy, MOSN, and other proxies.
WebAssembly in service mesh is a fast-growing field in the cloud-native landscape. Just in the past 6 months, the CNCF accepted 3 WebAssembly projects into its sandbox — WasmEdge, wasmCloud, and Krustlet — with more on the way! If you are interested in these new technologies and their applications, meet us at the upcoming KubeCon and CloudNativeCon North America and Cloud Native Wasm Day. You can meet us in person in Los Angeles or online!
Dedicated to curating the highest quality WebAssembly…
41 
41 claps
41 
Dedicated to curating the highest quality WebAssembly (Wasm) information, in an unofficial capacity. Encouraging writers, developers and researchers to share everything from innovative business ideas & Wasm use cases, right through to technical insights, documentation & code.
Written by
Technologist and investor
Dedicated to curating the highest quality WebAssembly (Wasm) information, in an unofficial capacity. Encouraging writers, developers and researchers to share everything from innovative business ideas & Wasm use cases, right through to technical insights, documentation & code.
"
https://medium.com/angularventures/the-next-era-of-cloud-native-data-54e8a16afa14?source=search_post---------132,"There are currently no responses for this story.
Be the first to respond.
With today’s funding announcement (Techcrunch, VentureBeat), we are thrilled to be finally able to announce our investment in Firebolt, the world’s fastest cloud-native data warehouse built by the founding team that previously built SiSense. (We have been sitting on this investment in stealth mode since November 2018!) Eldad Farkash and the amazing team at Firebolt (Saar, Boaz, Asaph, Itzik, and others) are unlocking the next phase of cloud-native data analytics.
Angular Ventures was the first investor to commit to the company, and we are thrilled that Zeev Ventures, Bessemer Venture Partners, and TLV Partners — three of the sharpest investors in the Israeli eco-system — have joined us in supporting the company. For a more technical look at Firebolt, click here.
A long partnership. For me personally, this is particularly significant as it’s the second time I’ve been lucky enough to back Eldad at the seed stage, having backed him previously at Genesis to build SiSense which has turned into a massive success story in its own right. When Eldad showed up at Genesis to pitch SiSense, the conventional wisdom was that “BI is a four-letter word” and “only the biggest organizations in the world are going to need a data warehouse this powerful.”
Nope. Eldad saw the trends and built a product for where the puck was going — not where the puck was at that moment. SiSense has emerged a leading player for next-generation enterprise business analytics solutions, and recently crossed the $1B valuation threshold and — more importantly — joined the rarified group of companies generating over $100M in sales.
And today, with Firebolt, Eldad and his team are doing it again. Think current cloud-native data warehouses are fast? Think they are easy to use? Think the current pricing models make sense?
Nope.
We’re also thrilled to have had Eldad on board as a Venture Partner in Angular Ventures since day zero — long before the fund existed in any formal sense. Eldad is pretty busy with Firebolt, but he makes time to work with several of our portfolio companies across the data/AI/ML domain and is adding a ton of value to those companies and their founders.
Thank you, Eldad for your continued partnership across SiSense, Angular, and — now — Firebolt! 🔥🔩
Early stage. Enterprise Tech. Europe & Israel.
8 
8 claps
8 
Early stage. Enterprise Tech. Europe & Israel.
Written by
A global venture investor. Fascinated by the finance of innovation. Trying to help the few to do the impossible. Investing across Europe + Israel.
Early stage. Enterprise Tech. Europe & Israel.
"
https://blog.getambassador.io/situational-awareness-boost-your-cloud-native-developer-productivity-3389913f1f99?source=search_post---------133,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Declarative management of API gateways is de rigeur today, but the Ambassador API Gateway (now Emissary-Ingress) pioneered this concept with its initial release in 2017. By extending the Kubernetes Resource Model, Ambassador enabled a fundamentally different workflow from the REST or UI-centric management strategies of the day. This workflow, over time, has come to be known as GitOps. While the GitOps workflow has been transformative for accelerating velocity, we’ve also found that developers struggle to context switch between coding, shipping, and running code.
Today, we’re announcing a number of new enhancements to Ambassador Developer Control Plane (DCP), which boosts your situational awareness, reducing the cost of context switching. We’ve also thought carefully about how you want to interact with DCP, designing it to be a sidekick that augments your existing workflow instead of replacing it.
In the animation above, I’m opening a pull request against my repository to change my Emissary configuration. The DCP detects the change and adds a comment to the change that tells me what will happen when I merge the PR into your main deployment branch.
A developer’s situational awareness needs to span all phases of the cloud-native software development lifecycle, from development to production. The DCP today already provides a holistic view of your services across all your environments — development, staging, and production. Building on this core foundation, we’ve added several new capabilities to further improve your situational awareness, letting you code, ship, and run your services faster:
Using CRDs, different teams of developers can independently manage different routes in Emissary-Ingress. One of the major challenges of this approach, however, is that developers do not always have the context of their changes. For example, a developer may inadvertently overwrite the route to another service. Now, a pull request against the repository that changes Emissary-Ingress CRDs will automatically notify the Developer Control Plane. DCP analyzes the pull request, and posts its analysis as a comment on the pull request. Now, developers can make configuration changes to DCP with confidence!
Our on-premise developer portal has seen rapid adoption, with users using the developer portal to share their API documentation with end users. Now, with DCP, this information is part of the core Service Details page. Now, a developer can log into DCP and get a unified, real-time view of their services across environments — and then view the API documentation of the deployed service alongside the runtime information for a service.
It’s great to have situational awareness, but Kubernetes adoption is a team sport. From helping thousands of organizations along their Kubernetes adoption journey we’ve found that the migration and adoption of new cloud native tools is often led by a small group of experts, and the individual developers are left behind in the experimentation phases.
Today, we’re excited to announce new get started experiences across the cloud native development lifecycle that enables developers to see the developer control plane in action in a real Kubernetes cluster in less than 5 minutes without impacting your production environments.
Get started today:
To manage your services across environments with the Developer Control Plane, create a free cloud account today!
Developer-First Kubernetes.
61 
61 claps
61 
Written by
CEO, Ambassador Labs. Makers of the Kubernetes Developer Control Plane, CNCF Telepresence, and CNCF Emissary Ingress.
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
CEO, Ambassador Labs. Makers of the Kubernetes Developer Control Plane, CNCF Telepresence, and CNCF Emissary Ingress.
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/%EF%B8%8F-becoming-cloud-native-in-5-stories-faun-new-blockchain-newsletter-more-e4b2d7750f75?source=search_post---------134,"There are currently no responses for this story.
Be the first to respond.
This is an email from FAUN, a newsletter by FAUN Publication.
This content is part of / inspired by one of our online courses/training. We are offering up to 80% OFF on these materials, during the Black Friday 2019.
You can receive your discount here.
Hey Fauners,
In this letter, I am sharing five stories I wrote about the cloud-native ecosystem, including the best practices, the missing introduction to container and a Kubernetes workshop.
Let’s start with containers, you’re probably using this technology in your development and/or staging and production environments, but do you know how they evolved?
Docker is one of the most known containers platforms nowadays and it was released in 2013.
The use of isolation and containerization started before this date. Let’s go back to the year 1979 when we started using the Chroot Jail and see the most known containerization technologies that came after. This will help us understand new concepts not necessarily related to history but also to technology.
⏳ In The Missing Introduction To Containerization, you will discover how containers evolved.
With more than 1.2 claps, this story may interest you. It’s about DevOps.
This checklist is neither static nor unique, there is no manifesto that describes DevOps, but it should be adapted to the organization need, human interactions, and other specific criteria.
In other words, the checklist could help you proceed with setting up a DevOps culture but don’t consider it as a unique way to proceed with your organizational transformation.
🗣️ Read The 15-point DevOps Checklist.
DevOps changed the way we develop, build, deploy, secure and monitor software, however, there is nothing as a magic wand that solves all of modern IT problems.
Also, there is no unique way to approach DevOps and implement it within an organization since it depends on many factors like the legacy, a burden for most medium and large companies.
It also depends on other factors like business strategy, business priorities, and culture.
This article describes the technical patterns of cloud-native applications.
👣 Read The Road to Cloud Native: The Best Practices to Design and Build Cloud Native applications
In this story, you’re going to use learn how to deploy Kubernetes services and Ambassador API gateway.
You’re going to examine the difference between Kubernetes proxies and service mesh like Istio.
You will see how to access the Kubernetes API and discover some security pitfalls when building Docker images and many interesting things.
Since Kubernetes is a star in the cloud-native ecosystem, you will enjoy reading this article.
💼 Read A Gentle Introduction to Kubernetes.
👍 If you like this work consider giving a start to eon01/kubernetes-workshop repository, where you will find the code of the workshop.
This is the latest article I published in Faun community publication.
Well, there are two types of applications in this world: Cloud-native applications and non-cloud-native applications. In this story, we are going to discover the best practices to design and build the first type of applications.
👨‍🎓 Read 15 Best Practices to Design Cloud-Native Modern Applications.
We are working on a new newsletter:
Xenilla #Blockchain #BlockDev #BlockOps
In this newsletter, we will curate the most interesting stories from the blockchain ecosystem.
We will focus on the technical aspects of cryptocurrencies, blockchain technologies, and other related news and tutorials.
The newsletter is coming soon, but you can subscribe from now:
Visit https://www.faun.dev/join/ and check the topics (newsletters) you want to follow, then validate.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
67 
67 claps
67 
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devplanet/building-scalable-infrastructure-with-cloud-native-tools-107561e4106e?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
Have you ever wondered how big organizations like Microsoft, Google, Amazon, Facebook etc, serve billions of requests daily across a wide range of highly heterogeneous services, without any significant downtime? Everyday, Google servers have to deliver results to over 3.5 billion search requests and Twitter servers deliver over 500 million tweets every day on average (source: https://www.internetlivestats.com)
To achieve this, requests are routed and processed across clusters of servers in data centers spread around the globe. Managing software infrastructure across so many server clusters is no easy task and has necessitated a radical shift in the way we build, manage and deploy cloud applications.
To address the challenges of developing and deploying highly scalable services, a number of tools have emerged in the past decade, built by the best minds from the top software giants and made open source for every developer to use. Notable among these are docker, terraform, kubernetes and its associated tools such as helm, istio, prometheus, envoy etc.
This publication, is established to guide developers, both new and highly experienced ones to adopt and apply modern DevOps practises including cloud native computing, code versioning with git, continuous integration and continuous delivery, infrastructure as code tools as well as microservices. To this end, we shall be publishing several tutorial series, open source repositories and books on DevOps tools and programming practices.
In the traditional software development world, inspired by practices in the early days of the web, building cloud applications entails writing your entire application in one single stack such as Java, .NET, PHP, NodeJS, Python etc. After which your app is deployed to a single server with a VM that is often tailored to your specific stack. Scaling such applications simply means buying a more powerful server, this is called, vertical scaling. A large number of developers still build this way. This approach has a number of disadvantages; first you are limited by the functionalities in a single stack, second is that it is not very scalable. As your site grows, you will need clusters of servers, at which point horizontal scaling is the only way to go.
In the modern cloud native world, cloud applications are not a single code base built with one stack, rather, every application is built and deployed as a suite of microservices, each of which can be built with a separate stack that is suitable for its functionalities. This brings about clear separation of concerns and enables easy distribution of components across cluster of servers. To serve users, requests are efficiently distributed across these servers, and the microservices communicate over protocols such as REST or gRPC to perform tasks and return responses to the user.
Learning to build applications this way is an exciting ride and a rewarding experience that will transform you into a world class developer with all of the required knowledge to work and build in modern software organizations.
Even as a small team, you would benefit immensely from the automation offered by the use of cloud native tools, freeing you from costly human errors, enabling infrastructure versioning and rollbacks and reducing down times due to server faults. Overall, irrespective of the scale you are building for, the core tools and concepts we shall be treating in this publication would enhance your productivity and help you achieve your goals faster.
I am John Ishola Olafenwa, a software engineer and AI researcher with years of experience in building developer tools and libraries. I will be personally taking you on a ride and you can always reach to me personally through my email and on twitter. Cheers to a wonderful ride to the cloud native universe.
twitter: @johnolafenwa
email: johnolafenwa@gmail.com
Tutorials on DevOps, Cloud Native Computing, GitOps and…
61 
61 claps
61 
Written by
Software Engineer at Microsoft | Creator of TorchFusion (https://github.com/johnolafenwa/TorchFusion)
Tutorials on DevOps, Cloud Native Computing, GitOps and Programming
Written by
Software Engineer at Microsoft | Creator of TorchFusion (https://github.com/johnolafenwa/TorchFusion)
Tutorials on DevOps, Cloud Native Computing, GitOps and Programming
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/cloud-native-skaffold-book-review-49457242fadf?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
I’m a developer who cares deeply about production. But I’m not a devops and unfortunately I’m pretty bad at that. That’s why when I heard about Skaffold it instantly piqued my interest. Write Kubernetes cloud native apps without well… Writing Kubernetes native apps… Sign me up!
Unfortunately, as we all know. The time to pick up a new technology is when we actually need it and then it’s a rush to get something out. Few of us have time to take off from our busy day to study something and learn something new. That’s why when Ashish Choudhary, the author of the book “Effortless Cloud Native App Development Using Skaffold” asked for reviewers in the foojay.io slack group, I jumped on the opportunity. I can learn something interesting and be productive (this book review).
Before we go on to the book review, I’d like to get the following things out of the way:
Another important detail: I didn’t run the code or go through the samples. I just read the book. I also did that relatively quickly and used the PDF version. As such, I feel I lost some of the core experience in the book.
Skaffold is an open source project started by a Google engineer after suffering the pain of cloud native Kubernetes deployments. It’s effectively a command-line tool that automates the build, push, and deploy steps for Kubernetes applications.
That alone warrants an info sheet. The surrounding system integrates with Maven/Gradle/Spring etc. to create a smooth development experience. This makes building/debugging Kubernetes apps locally almost as easy as running a regular Spring Boot application.
There’s obviously additional complexity, and configuration, but the core idea is the same. Developers still need to understand basic Kubernetes ideas, there’s no way around it. But you can remove some of the hassle involved with it. Kubernetes is aimed at devops and a lot of its features are redundant for developers. This way we can build cloud native microservices without the hassle.
Packt books offer a quick and practical introduction to current technologies. They have the advantage of including a lot of current details that other books might miss. However, these details might become outdated by the time you need the book. It’s a tradeoff you need to make based on your experience with the publication.
Since the subject ‌is a very specific tool, I see the value of going into specific details and samples.
Choudhary divided the book into 3 sections:
In the first section, we get a glimpse into the “problem” of building Kubernetes applications with chapters such as “Developing Cloud-Native Applications with Kubernetes — A Developer’s Nightmare”.
Choudhary introduces Skaffold in the second section. The third section gets into the finer points of deployment, alternatives, etc.
The book focuses on the separation of the inner/outer loop of the development cycle. The inner loop is the local development environment, where the outer loop is the production/CI/CD environment. What makes this development style “Cloud Native” is the similarity between the inner and outer loops.
There’s a similar focus on the value delivered in integration tests during the CI cycle and finally in pushing to production via tools like GitOps. The cool thing is that we use an environment that’s pretty close to production in all stages of development.
Another aspect that makes Skaffold native to the cloud is the instant deployment via jib. As a result, we can instantly see changes in our local Kubernetes environment as we save changes in the IDE. That’s pretty cool, and this alone is worth the price of admission to Skaffold (which admittedly is free).
Choudhary is a talented writer who uses clear language and examples. The examples weren’t too verbose, as is sometimes the case with such books. He wrote the book targeting Kotlin/Java developers, which is great.
The core ideas and benefits of Skaffold are covered early on in the book and are pretty obvious. There are a few diagrams that illustrate the big set pieces.
The book covers competitors to Skaffold and seems to be reasonably objective with its treatment. This is at the end of the book, so if you have doubts about Skaffold, I suggest reading the ending first.
While somewhat wasteful (more on that later), the layout of the Packt books is as usual pretty great. Packt books have excellent layout, table of contents and index. This makes the reading/referencing process much smoother.
I feel the phrase cloud native was ‌co-opted by GraalVM and frameworks such as Quarkus. I understand what Choudhary is aiming at and think this makes sense for the book. But it’s one of those overused phrases that loses some of its meaning. A cloud native book is a pretty vague statement, Skaffold narrows it down but only after you know what that means.
There’s an entire chapter on GKE which I skipped entirely for that reason. It just isn’t relevant to me.
While I like Packt books, I have two grievances. I hope they improve on these:
Notice that both comments aren’t unique to Packt and apply to most publishers.
I liked the book. I still haven’t made up my mind about Skaffold though. As a latecomer to the cloud native realm, I feel I need to leapfrog some of the hassle. That makes it attractive. But I try to keep away from Google services due to past trauma . Skaffold doesn’t require GCP and can still be pretty useful without it, so it’s something I intend to try.
I like that Choudhary focused the book on Java/Kotlin as I’m a Java guy. But I’m doing a lot of cloud-native polyglot work recently, and this could be a limitation. I saw a mention in the book of NodeJS debugging, but that’s it. I honestly don’t know if I can use Skaffold for one of my Polyglot demos. It would be really cool for that.
I assume this isn’t discussed much because of a limitation in Skaffold, not because of an omission in the book.
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
78 
78 claps
78 
A humble place to learn Java and Programming better.
Written by
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
A humble place to learn Java and Programming better.
"
https://medium.com/memory-leak/pivotal-s-1-analysis-the-first-cloud-native-business-to-ipo-ec8dc74797ac?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Pivotal, the cloud software company that spun out of DellEMC and VMware, filed for an IPO last Friday to raise $100M. Pivotal is the first cloud native business to file to go public and closely resembles RedHat, the most successful open source software-based business. Pivotal’s IPO positively indicates cloud native businesses with open source offerings have the potential to go the distance. We believe open source is a crucial component of modern cloud infrastructure software companies and continue to be excited by cloud native startups that help developers and DevOps drive innovation at higher velocities. We expect to see more DevOps-focused open source software-based companies become unicorns. Pivotal’s IPO is just the beginning.
Pivotal’s revenue grew from $281M in FY16 to $509M FY18, a compound annual growth rate of 35%. Its revenue stems from two offerings: 1) Pivotal Cloud Foundry (“PCF”) that is a platform to help streamline software development and 2) Pivotal Labs that is strategic services. Pivotal’s history as a services-oriented business is highlight by Labs revenue representing 66% of revenue in FY16. Pivotal has done a good job transition to a subscription-oriented business over the past two years with PCF now representing over 50% of revenue.
Subscription customer YoY growth rates have slowed over the past three years from 140% YoY in FY16 to 16% YoY in FY18. Since new subscription customer additions have declined from a growth rate and absolute number perspective and first year subscription revenue by cohort has shrunk from the FY16 high, analysis suggests account expansion has driven subscription revenue growth. Pivotal notes that of the 73% increase in subscription revenue from FY17 to FY18, more than 85% of the increase was attributable to existing customers.
Open sources software adoption is bottoms up rather than top down. Adoption often isn’t coordinated across the company but rather sprouts organically based on individual teams’ needs. Hence, open source software adoption patterns facilitate expansion opportunities within the account as solutions are often already utilized across teams before a salesperson engages. Pivotal highlights that the dollar-based net expansion rate was approximately 163% at the end of FY17 and 158% at the end of FY18. In turn, customer success teams’ ability to provide additional lift is critical.
Pivotal has done an OK job managing COGS over the past two years as it has remained relatively stable, despite the company’s 73% YoY revenue growth in FY18. COGS stands for cost of goods sold. We expect COGS to continue to decrease as a percentage of total revenue as the revenue mix shifts more towards subscription revenue.
Gross margin has improved due to subscription revenue being a higher portion of total revenue and gross margin on the subscription business increasing to 88% in FY18 from 64% in FY16. Given the company’s significant services business Pivotal is significantly below the public SaaS gross margin median of ~71%.
Since Pivotal is still transitioning to a SaaS-first business model, we thought it was best to look at sales and marketing in terms of percent of revenue instead of sales efficiency ratios and magic numbers that are typically used to evaluate SaaS businesses. Sales and marketing margin of 43% has also approached public medians, now around ~47%. Most SaaS companies operate their businesses in this range.
Pivotal’s net income margin of -32% is below the public SaaS median of -13% and caused by the its higher COGs spend on services and R&D expenditure, which is 32% of revenue compared to the public median of ~20%.
The comparison to RedHat, a competitor with its OpenShift suite, is inevitable. The table summarizes some key metrics.
The company has raised $1.7B in venture funding since 2013, including from General Electric, Ford, and Microsoft. Pivotal’s last round post-money valuation was $3.3B.
Assuming the market values Pivotal similarly to RedHat, the implied market cap for Pivotal is roughly $4.8B. Given Pivotal’s YoY revenue growth rate is only slightly above RedHat, which is 25 years old, and Pivotal’s gross margins are worse due to the services mix, we believe Pivotal will receive a lower MC/Rev multiple so $4.8B market cap is likely an upper bound.
We also thought it would be interesting to identify the popularity of key words. We were surprised to see that Kubernetes was only mentioned 4 times and DevOps was used 5 times. The frequent usage of cloud native in the S-1 underscores Pivotal is the first of wave of these businesses, and we should expect more high flyers going forward.
VC Astasia Myers’ perspectives on distributed computing…
10 
10 claps
10 
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/lenny-for-your-thoughts/so-you-want-to-go-cloud-native-first-ask-why-79ba1b3ab8f0?source=search_post---------138,"There are currently no responses for this story.
Be the first to respond.
This post first appeared in The New Stack on February 5, 2016.
Over the last couple years the term “cloud native” has entered the collective consciousness of those designing and building applications and the infrastructure that supports them. At its heart, cloud native refers to a software architecture paradigm tailored for the cloud. It calls that applications 1) employ containers as the atomic unit for packaging and deployment, 2) be autonomic, that is centrally orchestrated and dynamically scheduled, and 3) be microservices-oriented, that is be built as loosely-coupled, modular services each running an independent process, most often communicating with one another through HTTP via an API.
Dissecting those characteristics further implies that modern applications need be platform-independent (e.g. decoupled from physical and/or virtual resources to work equally well across cloud and compute substrates), highly elastic, highly available and easily maintainable.
By the sound of that, it holds that building cloud native applications is a no-brainer for every organization, whether they consider writing software business-critical or not. In practice, however, going cloud native — much like adopting DevOps — requires putting into place a broad set of new technologies and practices which meaningfully shift around overhead costs associated with writing, deploying and managing software. So before considering going cloud native, it’s imperative to understand the motivations for this architectural transformation, both technically and organizationally.
A good place to start is with Google, the poster child for this highly distributed, autonomic computing paradigm. Google has been running on containerized infrastructure for nearly a decade and manages resource allocation, scheduling, orchestration and deployment through a proprietary system called Borg. In a research paper released in 2015, Large-scale cluster management at Google with Borg, Google elucidates its motivations:
Borg provides three main benefits: it (1) hides the details of resource management and failure handling so its users can focus on application development instead; (2) operates with very high reliability and availability, and supports applications that do the same; and (3) lets us run workloads across tens of thousands of machines effectively.
So Google’s rationale for going cloud-native is to achieve 1) agility, as defined by developer productivity and self-service, 2) fault-tolerance and 3) horizontal scalability. And while almost no organization has to operate at the massive scale of Google, every company in the world asks itself “how do I go faster” and “how do I minimize risk?”
Problems arise, however, when going cloud native becomes an end, not a means. While containers, autonomic scheduling and microservices-oriented design are all tools which can facilitate operational agility and reduce risk associated with shipping software, they are far from a panacea and involve shifting meaningful costs from dev to prod. Martin Fowler and others have termed this phenomenon the “microservices premium”
The [cloud native] approach is all about handling a complex system, but in order to do so the approach introduces its own set of complexities. When you [adopt cloud native architectures] you have to work on automated deployment, monitoring, dealing with failure, eventual consistency, and other [complexities] that a distributed system introduces.
The prevailing fallacy is to conflate using Docker as package format with the need to build an application as a complex distributed system from the get-go.
The first rule of the thumb is “if ain’t broke, don’t fix it,” so there’s no need for added complexity if your team is functioning at a high level, releases are on schedule and your app is resilient and scaling to meet the demand of users. Sustained high levels of developer productivity, continuous deployment and fault tolerant systems can be and are often achieved without so much as ever interacting with a Dockerfile (though it can radically simplify the development workflow). In fact, many of the most elegant delivery pipelines in high performance software organizations are AMI-based and deployed by Slackbots!
However, as your engineering organization balloons to 100+ devs, going cloud native — including stand up the entire distributed runtime — very well could begin to make sense. Just remember, all these decisions are tradeoffs, where complexity is merely shifted not reduced.
Perspectives on developer tools, distributed systems, cloud…
6 
6 claps
6 
Written by
VC @AmplifyPartners. Deep thoughts on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants.
Perspectives on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants
Written by
VC @AmplifyPartners. Deep thoughts on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants.
Perspectives on developer tools, distributed systems, cloud infrastructure and security with side helping of SJ Sharks rants
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/memory-leak/the-cloud-native-ecosystem-f0484fb3d57f?source=search_post---------139,NA
https://blog.devgenius.io/getting-started-with-openfunction-896e7b27b690?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
5-minute setup of OpenFunction on Minikube to run functions on Kubernetes
After more than three years of development, Knative recently announced the launch of Knative 1.0, marking its core components (Serving, Eventing) as generally available. This speaks to the maturity of the Kubernetes serverless frameworks ecosystem, including OpenFaaS, OpenWhisk…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/part-4-operations-and-the-cloud-native-stack-in-action-bb17d9f0ff5?source=search_post---------141,"An in-depth series on how to easily get centralized logging, better security, performance metrics, and authentication using a Kubernetes-based platform.
In this four-part blog series, we’ll give you a tutorial on how to create a microservice-oriented application from scratch, as well as how to deploy and run it using the cloud-native stack. In the first part, we talked about setting up the development environment. In the second part, we implemented our application in Golang. In the third part, we explained how we can use Helm and Helm charts to deploy the application.
Now, it’s time to see our cloud-native platform in action providing all the nice features we promised.
Last time we deployed our application. Now, let’s try to test it and check if it works as we expected. As we have no dedicated client for our ToDo application, we’ll use the curl command to talk to our HTTP API endpoint. But first, we need to start port-forwarding to the apiserver service:
Now let’s run:
We didn’t get any response, but that’s expected: we haven’t created any ToDo entries yet. Let’s create some:
Now, let’s fetch all ToDos again:
It works! 🎉 Now let’s see how the tools provided by the cloud-native stack we installed make our management of the application easier.
We’ll start with Linkerd. If you have terminated port-forwarding for Linkerd, start it again:
To see our tools in action, we need to have some live traffic hitting our ToDo service. You can either fire them manually like the GET request above repeated a few times or launch a simple bash loop that sends a GET request every 100 ms:
Next, head to http://localhost:8084/namespaces. What we should get immediately in the namespace dashboard are statistics for the number of total requests (both HTTP and gRPC) executed in our namespace:
We can see how many of the requests are successful, how many requests per second we are handling (RPS), and what are the 50th, 95th, and 99th percentile of the latency distribution. Let’s click the namespace name to get more specific data.
Here we can see the same statistics, but available on the per-deployment and per-pod level. Additionally, service discovery results in an automatically provided request routing topology is visible at the top of the screen.
The dashboards above show the “live” data about our application. But can we see historical data as well? Sure, just click the Grafana logo in the rightmost column of the entry you’re interested in.
For example, for the “todomanager” deployment it looks like this:
This way, you can already see historical data about the success rate, request rate, and latency distribution of your application. And the best thing is that we had to write no code to make it work, it’s all done by proxies injected by Linkerd. You can stay in the Grafana UI and explore our app some more, but we still don’t see any logs.
In Grafana’s left-side menu go to the “explore” menu we already visited when setting it up.
Like in the picture above, select a filter to show just the logs produced by “todomanager” service. We can see logs coming from all the “todomanager” pods and we can run text searches in them. Still, what Loki is expected to do is to let us apply different search criteria, indexes, and operators based on labels associated with log entries. This effectively allows us to use Loki’s query language LogQL to search our logs. But Loki doesn’t know how to turn our JSON formatted log entries into labels and the main log message. It’s the application’s responsibility to configure Loki to parse the application’s specific log format. Let’s do that now.
Configuring Loki
Our application provides structured JSON logs. Loki can turn this structure into a message with an associated set of labels, then build indexes on these labels, and allow us to use them for filtering. So, we have to tell Loki how to parse our log messages into labels. The configuration is pretty long and complex, so we’re providing a ready configuration file, but if you want to dig in, the reference documentation is available on Loki’s homepage.
Let’s apply the prepared configuration by fetching the current Loki’s configuration, appending changes specific to our application, then updating the config map and restarting the necessary pod:
What does it change? Let’s see:
As you can see, now we can create a filter using a new label called “todo_app” that allows us to easily select different components of our application. In the query above we’re using this label to select logs coming from the “apiserver” only and being requested by an HTTP agent different than “kube-probe” (this is the readiness check agent that is polling the endpoint we defined for our apiserver Deployment).
Additionally, we’re looking for entries having the text “request complete” in the message part. In the log listing shown in the picture above, you can see the different labels values each of the log entries has. Labels that are present, but have the same value for all the selected entries are shown in the grey between the graph and the log listing. So, our log querying capabilities are now much better!
Using Grafana, we can combine our metrics and log data sources and investigate them in a single UI. This already makes our application much easier to observe and debug — and we got all of that without writing anything related to that functionality in our application!
Are we done with our project? Well, there are many things we can improve, but let’s stop here and try to sum up what we did so far.
We created a simple ToDo application in a microservice architecture. It consists of an API server that exposes REST interface, then consists of an actual service with business logic that is accessible through gRPC. That service in turn stores data in a MySQL database.
Then, we used some cloud-native tools to provide observability to our application. The important fact is that the application itself doesn’t really contain any instrumentation code that is responsible for interacting with that cloud-native stack.
Our observability is provided by two components: Loki and Linkerd. Loki is responsible for fetching logs from our containers, storing and indexing them, and of course, making them searchable. Linkerd fulfills even more tasks: we use it to get request metrics and service discovery, but also automatic mTLS encryption between all involved services. On top of both Loki and Linkerd, we use Grafana — a data presentation tool.
We also use many Kubernetes features to make our application available and robust. This includes:
To be able to easily scale and roll out new versions of our application.
To be sure that actual processes in the container work (and not just the container is started). We use anti-affinity hints to make sure that the Kubernetes scheduler will distribute our pods across instances and availability zones, limiting the impact of a single instance or zone failure.
To define our application firewall and explicitly list the allowed traffic.
Which additionally guards our application during maintenance work in the cluster, preventing any management operations that would drive our pod count below a specific number.
Which limits our own pods in regard to extended privileges it can use, forcing us to provide more secure deployment.
Which we used to pack everything into a single deployment package containing all elements of the application.
As you might have heard, there are three main sources of data in the observability area. So far we covered metrics and logs, but we’re missing tracing capabilities. That’s what the next part of the series will target!
In the meantime, if you have any comments or questions, let’s connect: lukasz@giantswarm.io and @piontec on Twitter.
When running Kubernetes clusters in production, you will realize that you will need more, sometimes much more, than just one cluster. You will need to care not only about the deployment but also upgrades, security issues, and monitoring. That’s where Giant Swarm can help — we offer managed Kubernetes clusters so that you don’t have to worry about managing them yourself. We also offer managed applications — well-known cloud-native projects that we can run for you on top of your Kubernetes cluster, creating a fully managed cloud-native platform.
Written by Łukasz Piątkowski: Kubernetes Platform Architect @Giant Swarm
ITNEXT is a platform for IT developers & software engineers…
24 
24 claps
24 
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/@IBMDeveloper/introduction-to-eclipse-codewind-build-high-quality-cloud-native-applications-faster-3c486e8867fb?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Aug 23, 2019·1 min read
Eclipse Codewind is an open source project that makes it easier for developers to create cloud-native applications within their favorite IDE. Codewind initially supports Visual Studio Code, Eclipse IDE and Eclipse Che. We’re working on adding support for additional editors in the coming months.
Once you’ve installed Codewind, you can use common templates to quickly start using popular frameworks including Express (Node.js), Spring Boot (Java), Open Liberty (Java), and Kitura (SwiftLang). If you want to develop in other runtimes and frameworks, you can do that as well! Codewind enables you to bring your own templates to expand support to meet your own needs.
When you’re creating an application, Codewind immediately syncs and builds your application within its own container, pulling in application dependencies as appropriate. The best part? You don’t have to leave your editor to use dependent tools.
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
9 
9 
9 
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/lightspeed-venture-partners/taking-the-big-leap-in-cloud-application-security-or-cloud-native-security-8baa69f26ce7?source=search_post---------143,"There are currently no responses for this story.
Be the first to respond.
We’re very excited to be joining forces with Aqua just as the container market is emerging. While containers and Aqua specifically have shown tremendous growth, the market is still in its infancy — we see this as an exciting opportunity.
Containers are not just for web-scale companies like Facebook and Netflix anymore. Enterprise companies are adopting containers across industries and across geographies. This is happening via grass-roots, bottom-up projects driven by DevOps teams, as well as enterprise-wide, top-down design initiatives that recognize that containers can drive hybrid cloud strategies, agile application delivery, and cost savings.
Aqua is at the nexus of several forces that are shaping the present and future of enterprise IT and, as a result, of how enterprises approach security:
● The DevOps movement with its drive to create more fluid, dynamic application delivery has been around before containers, but containers are making it a lot easier to adopt and scale.
● Cloud has also been around for a while, and is seeing a surge in adoption with real competition emerging in what is a space still dominated by AWS. But containers make migration to the cloud easier, and make it possible to realize true hybrid-cloud strategies.
● Cloud-native, microservices-based architecture are changing how applications are built and deployed. This again is enabled by containers in a major way, driving the adoption of an entirely new stack of tools.
What does this mean for security? Well, for starters, security is becoming an enabler instead of an inhibitor. The kind of security provided by Aqua’s platform drives innovation forward, makes application delivery at DevOps speed possible in a secure way, removing obstacles instead of creating them.
As applications move to the cloud, the focus of security team fundamentally changes from focusing on infrastructure, network security and identity management to the higher layers of application security. This is because much of the basics are taken care of by the infrastructure and the cloud provider. If you’re running applications on Azure, it is Azure that ensures that no unauthorized users will have access to your VMs, and that no intruder will be able to traverse their network by invading another customer’s VM. But securing the application is still your responsibility — the cloud provider won’t touch that.
These trends will render many of the existing security tools and approaches obsolete. Aqua is bringing about a truly disruptive change. Aqua’s approach of providing full lifecycle security for containerized applications not only makes sense, it is necessary in order to manage and simplify the disruptive aspects of containers: the speed at which they are developed and deployed, the visibility required to understand their actions, the control points needed to prevent attacks, and the automation to handle it all.
We were impressed by Aqua’s technology and vision, but even more impressed by the team and its track record in working with some of the largest enterprise container deployments. Aqua’s roster of customers includes some of the world’s largest banks, insurers, retailers, software companies and manufacturers who they are partnering with to help transform those organizations’ application infrastructure.
This is why Lightspeed is looking forward to work with the Aqua team in driving market adoption of its solutions, realizing the vision of superior security and agility, and meeting the demand of this exciting market.
— Ravi Mhatre and Yoni Cheifetz, Lightspeed Venture Partners
Lightspeed is a multi-stage VC firm focused on accelerating…
107 
107 claps
107 
Lightspeed is a multi-stage VC firm focused on accelerating disruptive innovations and trends in the consumer, enterprise, and health sectors. In the past two decades, Lightspeed has backed 400+ companies and currently manages $10.5B across the global Lightspeed platform.
Written by
Investor and adviser to start-ups
Lightspeed is a multi-stage VC firm focused on accelerating disruptive innovations and trends in the consumer, enterprise, and health sectors. In the past two decades, Lightspeed has backed 400+ companies and currently manages $10.5B across the global Lightspeed platform.
"
https://medium.com/@alibaba-cloud/breaking-the-limits-of-relational-databases-an-analysis-of-cloud-native-database-middleware-2-d3e790de0673?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 12, 2019·12 min read
In Part 1 of this article, we covered the concept and implementation of sharding in NewSQL. In Part 2, we will discuss in further detail about distributed transactions and database governance.
As mentioned in the previous article, database transactions must meet the Atomicity, Consistency, Isolation, and Durability (ACID) standard:
On a single data node, transactions that can be accessed only by a single database resource are called local transactions. However, in the SOA-based distributed application environment, more and more applications require that the same transaction can access multiple database resources and services. To meet this requirement, distributed transactions have emerged.
Although relational databases provide perfect native ACID support for local transactions, they inhibit system performance in distribution scenarios. The top priority for distributed transactions is how to either enable databases to meet the ACID standard in distribution scenarios or find an alternative solution.
The earliest distributed transaction model was X/Open Distributed Transaction Processing (DTP), or the XA protocol for short.
In the DTP model, a global transaction manager (TM) is used to interact with multiple resource managers (RMs). The global TM manages the global transaction status and the resources involved in the transactions. The RM is responsible for specific resource operations. The following shows the relationship between the DTP model and the application:
DTP model
The XA protocol uses two-phase commit to ensure the atomicity of distributed transactions. The commit process is divided into the prepare phase and the commit phase.
The following figure shows the transaction process of the XA protocol:
XA transaction process
Two-phase commit is the standard implementation of the XA protocol. It divides the commit process of a distributed transaction into two phases: prepare and commit/rollback.
After an XA global transaction is started, all transaction branches lock the resources based on the local default isolation level and record the undo and redo logs. Then, the TM initiates a prepare vote to query all the transaction branches whether the transaction can be committed. If all the transaction branches reply “yes”, the TM initiates commit again. If a reply is “no”, TM initiates rollback. If all replies in the prepare phase are “yes” but an exception (such as system downtime) occurs in the commit process, after the node service is restarted, the transaction is committed again based on XA_recover to ensure data consistency.
The distributed transactions implemented based on the XA protocol are not intrusive for the business. Its greatest advantage is the transparency to users. Users can use XA-based distributed transactions the same way they use local transactions. The XA protocol can rigidly implement the ACID standard of transactions.
However, the rigid implementation of the ACID standard of transactions is a double-edged sword.
During transaction execution, all the required resources must be locked. Therefore, the XA protocol is more suitable for short transactions with a fixed execution time. In a long transaction, data must be used exclusively, resulting in a significant decline in the concurrent performance of the business systems that rely on hotspot data. Because of this, in scenarios where high-concurrency performance is the top concern, XA-based distributed transactions are not the best choice.
If the transaction that implements the ACID standard is a rigid transaction, then the BASE-based transaction is a soft transaction. BASE refers to the combination of Basic Availability, Soft State, and Eventual Consistency.
ACID transactions impose rigid isolation requirements. During transaction execution, all the resources must be locked. The idea of the soft transaction is to shift the mutex operation from the resource level to the business level through the business logic. The soft transaction can improve the system throughput by lowering the strong consistency requirements.
Given that a timeout retry may occur in a distributed system, operations in a soft transaction must be idempotent to prevent the problems caused by multiple requests. Models for implementing soft transactions include Best Efforts Delivery (BED), Saga, and Try-Confirm-Cancel (TCC).
BED is the simplest type of soft transaction. It is applicable to scenarios where database operations do succeed. NewSQL automatically records the failed SQL statements and reruns these statements until they are successfully executed. The rollback function is unavailable to BED soft transactions.The implementation of BED soft transactions is very simple but imposes rigid scenario requirements. On one hand, BED features unlocked resources and minimal performance loss. On the other hand, its disadvantage lies in the transaction not being possible to roll back when multiple commit attempts fail. BED is solely applicable to business scenarios where transactions do succeed. BED improves performance at the cost of the transaction rollback function.
Saga was derived from a paper published in 1987 by Hector Garcia-Molina and Kenneth Salem.
Saga transactions are more suitable for scenarios where long-running transactions are used. A saga transaction consists of several local transactions, each of which has a transaction module and compensation module. When any of these local transactions becomes faulty, the corresponding compensation method is called to ensure the final consistency of the transactions.
The saga model splits a distributed transaction into multiple local transactions. Each of these local transactions has its own transaction module and compensation module. When any of these local transactions fails, the corresponding compensation method is called to restore the original transaction, ensuring the final consistency of the transactions.
Assume that all saga transaction branches (T1, T2, …, Tn) have their corresponding compensations (C1, C2, …, Cn-1). Then, the saga system can ensure the following:
The saga model supports both forward recovery and reverse recovery. Forward recovery attempts to retry the transaction branch that currently fails. It can be implemented on the precondition that every transaction branch does succeed. In contrast, backward recovery compensates all the completed transaction branches when any transaction branch fails.
Obviously, providing compensation for transactions in forward recovery is unnecessary. If the transaction branches in the business will eventually succeed, forward recovery can lower the complexity of the saga model. In addition, forward recovery is also a good choice when compensation transactions are difficult to be implemented.
In theory, compensation transactions never fail. However, in a distributed world, servers may crash, networks may fail, and IDCs may experience power failures. Therefore, it is necessary to provide a rollback mechanism upon fault recovery, such as manual intervention.
The saga model removes the prepare phase, which exists for the XA protocol. Therefore, transactions are not isolated. For this reason, when two saga transactions simultaneously use the same resource, problems, such as missing updates and reading of dirty data, may occur. If this is the case, for an application that uses saga transactions, the logic of resource locking must be added to the application-level logic.
TCC implements distributed transactions by breaking down the business logic. As the name implies, the TCC transaction model requires the business system to provide the following sections of business logic:
The TCC model only provides a two-phase atomic commit protocol to ensure the atomicity of distributed transactions. The isolation of transactions is implemented by the business logic. In the TCC model, isolation is to shift locks from the database resource level to the business level by transforming the business. This releases underlying database resources, lowers the requirements of the distributed transaction lock protocol, and improves system concurrency.
Although the TCC model is the most ideal for implementing soft transactions, the application must provide three interfaces that can be called by the TM to implement the Try, Confirm, and Cancel operations. Therefore, the business transformation is relatively costly.
Assume that account A transfers 100 dollars to account B. The following figure shows the transformation of the business to support TCC:
TCC process
The Try, Confirm, and Cancel interfaces must be separately implemented for remittance and collection services. Meanwhile, they must be injected to the TCC TM in the service initialization phase.
Remittance Service
Try
Confirm
Cancel
Collection Service
Try
Confirm
Cancel
The preceding description indicates that the TCC model is intrusive for the business and the transformation difficulty is high.
The message consistency scheme is used to ensure the consistency of upstream and downstream application data operations through the message middleware. The basic idea is to place local operations and sending messages into a local transaction. Then, the downstream applications subscribe to a message from the messaging system and perform the corresponding operation after receiving the message. This essentially relies on the message retry mechanism to achieve final consistency. The following figure shows the message-driven transaction model:
Message-driven transaction model
The disadvantages of this model stem from the fact that the degree of coupling is high and the message middleware must be introduced to the business system, increasing the complexity of the system.
In general, neither ACID-based strong-consistency transactions nor BASE-based final-consistency transactions are “silver bullets”. Therefore, for either to be optimally useful, they must be used in their most appropriate scenarios. The following table provides a comparison of these models to help developers choose the most suitable model. Due to a high degree of coupling between message-driven transactions and business systems, this model is excluded from the table.
Comparison of transaction models
The pursuit of strong consistency will not necessarily lead to the most rational solution. For distributed systems, we recommend that you use the “soft outside and hard inside” design scheme. “Soft outside” means using soft transactions across data shards to ensure the final consistency of data in exchange for optimal performance. “Hard inside’ means using local transactions within the same data shard to achieve the ACID standard.
As described in the previous article, service governance is also applicable to the basic governance of the database. Basic governance includes the configuration center, registry, rate limiter, circuit breaker, failover, and tracker.
The major difference between database governance and service governance is that the database is stateful and each data node has its own persistent data. This makes it difficult to achieve auto scaling as a service does.
When the access traffic and data volume of the system exceeds the previously evaluated expectations, in most cases, the database needs to be resharded. When using policies such as date-based sharding, you can directly expand the capacity without migrating legacy data. However, in most scenarios, legacy data in a database cannot be directly mapped to a new sharding policy. To modify the sharding policy, you must migrate the data.
For traditional systems, the most feasible scheme is to stop services, migrate the data, and then restart the services. However, adopting this scheme imposes extremely high data migration costs on the business side, and engineers of the business side must accurately estimate the amount of data.
In Internet scenarios, the system availability requirement is quite demanding, and the possibility of explosive traffic growth is higher than that in traditional industries. In the cloud=native service architecture, auto scaling is a common requirement and can be easily implemented. This makes the auto scaling function of data, which is equivalent to that of services, an important capability for cloud-native databases.
In addition to system pre-sharding, another implementation of auto scaling is online data migration. Online data migration is often described as “changing the engines of an airplane in mid-flight”. Its biggest challenge is ensuring that the migration process is immune to services. Online data migration can be performed after the sharding policy of the database is modified. This may be, for example, changing the sharding mode of splitting the database into 4 smaller databases based on the result of ID mod 4 to the sharding mode of splitting the database into 16 smaller databases based on the result of ID mod 16. Meanwhile, a series of system operations can be performed to ensure that the data is correctly migrated to the new data node and that the database-dependent services are completely unaware of the migration process.
Online data migration can be implemented in four steps:
Online data migration can be implemented not only to expand the data capacity but also to support online DDL operations. Database-native DDL operations do not support transactions. In addition, tables will be locked for a long time when you perform DDL operations on a large number of tables. Therefore, you can perform online data migration to support online DDL operations. The online DDL operation is consistent with the data migration process, which simply requires you to create an empty table with the modified DDL and then complete the preceding four steps.
Reference:https://www.alibabacloud.com/blog/breaking-the-limits-of-relational-databases-an-analysis-of-cloud-native-database-middleware-2_594428?spm=a2c41.12548446.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
10 
10 claps
10 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibabatech/introducing-nacos-alibabas-open-source-solution-for-cloud-native-development-3826b1beb99f?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Nov 20, 2018·6 min read
What is Nacos? What problems can it help solve for cloud-based applications?
This article is part of the Alibaba Open Source series.
As a recent addition to the Alibaba ecosystem, open source platform Nacos (short for Naming Configuration Service) offers developers an easy-to-use framework for service discovery, management, and configuration in cloud-native applications. Since its July 2018 debut, the system has gained attention for its numerous advantages in a variety of key areas, generating interest in the fundamentals of what it is, how it works, and what kind of problems it can solve, as well as best practices.
In this series, we explore these themes in detail, highlighting takeaways developers and technical audiences can apply to their own work. This first article focuses on defining what Nacos is and the problems it can help solve, with focus on its configuration management capabilities.
In application development, configurations are an inseparable aspect of the codes they are deployed with, playing a key role throughout an application’s entire lifecycle. Traditionally, three types of configuration methods have offered competing advantages and drawbacks for developers to balance — namely hard coding, configuration files, and database configuration tables.
As a novel framework, Nacos avoids these inherent drawbacks with solutions designed for ease of use. The following sections examine the previous approaches and their problems individually before exploring the improvements introduced by Nacos.
Of conventional methods, hard coding generally presents the most problems for configuration. In hard-coded configurations, configuration items exist as the fields in a class, as shown in the following image.
This approach presents three major problems.
First, applications must expose their corresponding interfaces in order to achiever dynamic configuration modification, whether they are the Controller’s API or JMX.
Second, all configuration modifications are non-persistent, as they are done in memory. This means that an application’s configuration will be restored to default values whenever the application restarts, introducing a host of problems for developers.
Finally, hard coding introduces considerable maintenance cost issues. With this approach, machines must be reconfigured one by one, which becomes labor-intensive when a number of machines are involved.
An alternative but similarly flawed approach to configuration is the use of configuration files. In Spring — a framework Nacos is specifically designed to support — typical configuration files include properties, yml, and self-defined files with a *.conf suffix.
Using configuration files effectively avoids the problem of default value restoration that plagues hard-coded configurations, but leaves the other two issues unresolved. As with the hard-coding approach, configuration files allow for dynamic configuration modification by way of exposing the corresponding interfaces, with the one difference being that configuration files introduce a persistence logic. Nevertheless, reconfiguration remains possible by logging in to the machine, modifying its configuration, and then restarting the machine. To avoid restarting, a scheduled task can be added, such as reading the configuration file every 10 seconds.
Compared with hard coding, using configuration files presents a step forward in terms of its persistence logic and task scheduling aspects.
With database configuration methods — the last of the conventional methods used for configuration — the database can be a relational database such as MySQL or a non-relational one such as Redis. The following table shows one such example.
Database configuration tables manage configurations after separating them from their applications, which greatly reduces the costs of maintenance. Some may wonder how a database configuration table can realize dynamic configuration updates, for which there are two possible answers.
As with the use of configuration files, one solution using database configuration tables is to expose interfaces and add persistence logic. However, instead of creating a configuration file, database configuration tables use databases to store the configuration, which is then read by programs periodically. This allows for updating the configuration of an entire cluster of machines simply by calling the configuration management interfaces of one of the machines, which significantly reduces maintenance costs.
The second possible solution is to modify the configuration in the database directly. The modified configuration is then read by programs through a scheduled task.
While database configuration tables are able to address the main problems present in the hard-coding and configuration file approaches, it is nevertheless heavy and inelegant, leaving much room for improvement.
As an alternative to the above methods, Nacos offers an elegant solution to the issues of dynamic modification, persistence, and maintenance costs by managing configurations centrally after first separating them from their applications.
Nacos eliminates any need to add configuration management interfaces, realize configuration persistence, or introduce scheduled tasks, instead of handling these functions on its own. This is possible through configuration management features that bring together all of the logic related to configuration while providing an SDK for easy application management in Nacos.
Requiring just three steps, applying Nacos in the Spring framework offers a strong illustration of its advantages.
First, users should add the Nacos Spring dependency, as shown below.
Next, users should add the @EnableNacosConfig annotation to enable the configuration service. In the example code below, @NacosPropertySource is used to load the configuration source with “dataId” as “example”, and autorefresh is enabled.
Finally, users should specify the property value through Spring’s @Value annotation, as shown below. It is important to note that a Setter method is needed to enable autorefresh of configuration updates.
As well as being simple and brief, the above steps involve almost no intrusion into the application itself. With the addition of just one dependency and two annotations, Nacos can successfully assume configuration management.
For Nacos Spring users, achieving dynamic configuration updates simply requires setting “autoRefreshed” as a Boolean value in their applications. To modify configurations, they can call the Nacos API or use the Nacos console. After configurations have been updated, Nacos will then push the latest configuration to all machines involved with the application, vastly simplifying the entire process.
Nacos’ configuration management features are some of its most impressive, offering an efficient and elegant solution to labor-intensive issues at the heart of application development. As well as functions discussed in this article, technical audiences may wonder about gray release, version management, fast rollback, listening query, push track, access control, and encryption, among others. Some of these functions are already supported in the open source Nacos project, while others are provided in Alicloud’s Application Configuration Management (ACM) service, currently free of charge.
Ahead, this series will look more deeply into the technical fundamentals enabling the simple but powerful Nacos framework, as well as more of the problems it can help to solve and its developmental origins.
As an open source project, the Nacos community invites interested readers to learn more by becoming involved.
(Original story by Huang Xiaoyu黄晓禹)
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
See all (17)
10 
10 claps
10 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@codeamt/deploying-a-cloud-native-stx-mining-bot-on-kubernetes-with-okteto-cloud-7603ed937261?source=search_post---------146,"Sign in
There are currently no responses for this story.
Be the first to respond.
AnnMargaret Tutu
Jan 9, 2021·9 min read
I’m a proponent of blockchain-based technologies and enjoy studying new chains and corresponding protocols and mechanisms. Being a software engineer, this often means deploying blockchain testnets (“test [blockchain] networks”) to run decentralized applications (“dApps”). Motivated by the STX to 1 Million Mining Challenge (more…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-7831aecc8d1a?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Dec 12, 2019·11 min read
PART THREE — Working with custom cloud VM images for GCP
In the series to date, I’ve presented the general rationale for building quick start examples (or ‘Hello World’) for bioinformatics tools in part one.
In part two, I reviewed one such example for the VariantSpark library, runnable via smart templates in the AWS Marketplace.
In this part three, I’ll cover how to convert a locally runnable example to a reusable cloud example, by working with custom Google Cloud Platform custom Virtual Machine images running on Google Compute Engine.
One aspect of working in a cloud-native way is to START all technical work in the public cloud rather than on a local laptop. For example, building Hello World examples which can be quickly run on a public cloud infrastructure enables more researchers to try out tools and libraries with a minimum of effort as they won’t have to waste time locating, installing and configuring any software dependencies on their local laptop.
Many bioinformatics tools and libraries include several required dependencies. In order to use or run a hello world example on a local machine, the current assumption is that the researcher will install these dependencies on their local machine. This requirement can, at best, result in lost productivity as installations of SDKs, libraries, etc…can involve even more dependencies. Also configuration of the these dependencies, which can include setting of paths, variables and permissions takes even more time and effort. Worst case scenario, your Hello World example won’t be used, because the researcher is unwilling or unable to set up the test environment locally.
Working cloud-first can serve as an alternative.
Creating hello world examples which use custom cloud Virtual Machine images can improve the researcher experience substantially. For example, a researchers could select and use an appropriately pre-configured image file to easily start their Virtual Machine instance for testing.
After the instance is available, then they can just run the bioinformatics tool hello world example quickly and easily. When testing is complete, they simply shutdown and/or delete the VM instance. Another side benefit to this approach is that there is no disruption to functionality on their local machine — because they didn’t install anything there.
Nextflow is a bioinformatics pipelining library written in the Java-dependent Groovy programming language. Nextflow is designed to support quick set up of scalable workflows for bioinformatics analysis jobs. These workflows use chained bioinformatics tools and can be run in a number of environments, i.e. locally, HPC, public cloud, etc. The Nextflow website includes a number of Hello World (quickstart) examples. All of these examples are designed to be run locally.
One such example creates and runs a simple workflow which showcases the ability of Nextflow to parallelize tasks. This example workflow uses just one tool— blast. From the Nextflow site “the example splits a FASTA file into chunks and executes for each of them a BLAST query in a parallel manner. Then, all the sequences for the top hits are collected and merged to a single result file.”
The example is also designed to demonstrate how a docker container which includes a bioinformatics tool (in this case the blast tool) can be run in a Nextflow pipeline.
If the required dependencies (Java 8 and Docker) are installed locally, then the example can be run in two steps from the researcher’s local terminal. The steps to run the example locally are as follows:
When the first command is run the researcher will see output in their local terminal similar to that shown below. Note that at the end of download, the message “Nextflow installation completed.” will display.
2. Launch the Nextflow pipeline execution using the command shown below:
This command will automatically download the pipeline Github repository and the associated Docker images, because of this requirement, the first execution of the second command listed above can take a couple of minutes to complete.
This second command runs the blastn example using the included main.nf Nextflow script. The script first starts the a Docker container image which includes the blastn tool. The command then runs the blastn tool on the sample data and then prints the results to the terminal window. A sample output is shown below.
The Nextflow team has done a nice job writing a quick-to-execute hello world sample. End-to-end this takes less than 5 minutes to run — so long as the development environment is set up correctly.
Now let’s perform the steps to make this sample cloud-native. The first consideration is that the required dependencies are not trivial to locate, download, install and configure in general. As mentioned, the Nextflow scripting language is an extension of the Groovy programming language. Groovy is a language for the Java platform, and thus requires a JVM (Java Virtual Machine) to run in. The Nextflow team has noted that the availability of Java 8 is a requirement for the example in it’s source `travis.yml` file as shown below.
You’ll also note that the file above uses a docker pull command to pull a docker container image named `nextflow/examples`
So, in order to use this example in the cloud, the researcher needs a compute environment that includes Java 8 and Docker. Ideally, the environment would also be quickly reproducible, so that other researchers can also try out the example scripts.
Using Google Cloud Platform, what would be the easiest way for them to accomplish this? There are several ways to meet these requirements. These include the following:
As a starting point, I choose to work to create an example using the technique described the first option listed above, that is to create an instance of a Google Compute Engine (GCE) Virtual Machine based on an custom GCE image file. The image file must include an operating system AND both dependencies (Java and Docker).
Of course, the researcher could create a VM instance from a GCE default image and then install the dependencies — however this isn’t the most optimal solution because then they still have to locate, install and configure the correct version of both Java and Docker.
For this article, I created and tested an instance configured as described. I then created and exported a GCE VM image file and exported it to a public GCS bucket.
To use this custom image file to create a customized GCE VM instance, do the following:
After the instance is available, SSH to the GCE instance to open a terminal window. In that window, researchers can verify that required dependencies are installed using these two commands shown below. Both should return version numbers (for this image results should be `java 11` and `docker 19`).
Researchers can then use the Nextflow blast example on this configured instance using the two commands from the Nextflow site for ‘example 3’. First to get the Nextflow tool, run the command below in the terminal window of the GCE VM instance.
Expected output should look like screen shown below. Note the Nextflow version is 19 at the time I am writing this article.
Researchers can now run the Nextflow blast script example, which runs the bioinformatics blast tool inside the VM from a Docker contain image instance. The command is shown below.
The expected blast results appeared in less than a minute and an example is shown below. Voila! GCP Cloud-native Hello World for Nextflow.
Researchers can then close the SSH window, and stop the VM. They can restart the VM if they would like repeat the test. They can optionally delete the VM instance if their testing is complete.
Because I am writing not only for the users of Hello World examples (researchers), but also teams who are involved in building this type of sample, in the next sections of this article, I’ll cover how I quickly built and deployed this custom GCE image.
First I needed to find the best-fit base GCE image. In this case I simply used a standard Debian 9 Linux image on GCE. I needed to install both Java 8 and also Docker on this image.
NOTE: I tried to use a GCE container-optimized image (which had Docker tools pre-installed) but found that the lack of Linux tools (such as apt-get, dpkg and others) made it too time consuming to get those tools and then to install Java 8, so I abandoned this approach quickly.
I found it to be quite difficult to find accurate instructions on how to install both Java 8 and also Docker on a GCE image. I’ll add the steps I used to get this to work properly below:
To install Java 8 or above (I used Java 11):
To verify that Java is installed
To install Docker:
Then after the install completes, add your username to be able to run docker (as non-root)
Logout and then log back in to your GCE terminal to apply this update. Then run the command below to update docker
Verify that docker is running
I then tested the Nextflow blast hello world example on this configured instance using the two commands from the Nextflow site for ‘example 3’. First to get the Nextflow tool, I ran the command below in the terminal window of the GCE VM instance.
Then I ran the Nextflow script example, which uses Docker to containerize the bioinformatics blast tool, as shown below.
The expected blast results appeared in less than a minute. So my custom GCE image was working properly. The next step was to prepare a copy of the image for reuse.
To prepare the instance for imaging, I first stopped the VM instance and then created a GCE image from that stopped image using the GCP console. Because GCE VM images are available to users in the GCP project that they were created in, stopping here would not meet my need for creating a publicly available base VM image. Shown below from the GCP documentation is the workflow to create such an image.
To make my image publicly available , I next need to export the image as a file into a GCS bucket. This bucket need to have public permissions set on it. So I created a GCS bucket and set the permissions to `allUsers/ read` — which is public access for GCS. I then used the `gcloud` tool (example shown below) to create and export a zipped file from my custom VM image. This command puts that image file in the newly-created public GCS bucket.
Note that the line break is NOT shown for line 1 in the example below.
Now this image can be used by anyone with the URL as a basis for quickly creating a pre-configured GCE VM instance so that they can easily try out Nextflow.
As mentioned, in creating this quick example, I did NOT use two other common patterns for cloud-native Hello World examples. One option would’ve been to create a startup script, which would install and configure Java 8 and Docker on a base GCE VM, then put that startup script in a public GCS bucket. Next I would create a gcloud statement which creates a VM and uses that script and then just run the gcloud script to create an instance of a customized VM.
Here I am using what I call, ‘just enough automation’.
Because I wanted to make a copy of working VM image as quickly as possible, I just clicked in the console and used gcloud to create and export my VM image to a public bucket. I do expect, as I continue to test Nextflow, I’ll create this scripted version.
Creating a GCP deployment seemed like overkill in terms of time to write and test a configuration file (written in the Jinja/YAML or Python dialects). Although I do like the ‘install everything/remove everything’ aspect of deployments, because, at this stage, I am only working with a single VM, using Deployments just wasn’t time effective for this level of configuration automation.
Interestingly, Nextflow’s documentation references a pre-configured AWS VM image for Nextflow testing (in AWS these images are called AMIs — or Amazon Machine Images). Of note is that this AMI is in AWS EU/Ireland region. The ID is ‘ami-4b7daa32'.
As I continue to test various bioinformatics pipeline (DSL) scripting language and tools, I’ll continue to share my ‘just enough automation’ stories.
Cloud Architect who codes, Angel Investor
17 
17 claps
17 
Cloud Architect who codes, Angel Investor
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ankr-network/ankr-joins-the-linux-foundation-and-cloud-native-computing-foundation-686c88c695d4?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
We also join KubeCon Open Source Summit China as a Start-up Sponsor.
Ankr is now an official Silver member of the Linux Foundation and the Cloud Native Computing Foundation, and a start-up sponsor of KubeCon Open Source Summit China 2019.
Ankr has been working with Kubernetes and Docker since the early days, and with our current membership of the Linux Foundation and CNCF we show our commitment to keep contributing to the Kubernetes ecosystem.
Kubernetes is a container service in which software acts independently of other containers in the system. It facilitates the rollout and configuration of Ankr across large estate, such as data centers. Our DCCN system is deployed and distributed computing resources managed by Kubernetes in various geographic locations. In our specialized type of clusters, the Ankr Hub, resources are configured into clusters in Kubernetes and connected to each other.
The Linux Foundation acts as a neutral, independent organization which manages the infrastructure and sustains open-source communities long term.
Ankr is proud to join the Linux Foundation, which span across all industry sectors and work together to solve complex business and technology problems through open source collaboration.
The Cloud Native Computing Foundation is building a sustainable ecosystem and community of industry leading projects that orchestrate containers as part their architecture.
CNCF serves as the vendor-neutral home for many of the fastest-growing projects on GitHub, including Kubernetes, fostering collaborationbetween the industry’s top developers, end users, and vendors.
Ankr is an official start-up sponsor of the KubeCon & CloudNativeCon Open Source Summit China which takes place in Shanghai on June 24–26th 2019.
We are among industry giants in the open source and cloud computing space, such as Tencent Cloud, Alibaba Cloud, AWS, Hyperledger and many more.
The KubeCon forum brings CNCF projects, technologists and open source industry leaders together under one roof, to share information and learn about the newest and most interesting open source technologies, including Linux, IoT, blockchain, AI, networking, and more.
The official blog of Ankr
55 
55 claps
55 
The official blog of Ankr
Written by
Blockchain infrastructure and staking DeFi
The official blog of Ankr
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-d21458a0013f?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Jan 2, 2020·7 min read
PART FOUR — Toward Serverless Pipelines with GCP
In the series to date, I’ve presented the general rationale for building quick start examples (or ‘Hello World’) for bioinformatics tools in part one.
In part two, I reviewed one such example for the VariantSpark library, runnable via smart templates in the AWS Marketplace.
In part three, I covered how to convert a locally runnable example to a reusable cloud example, by working with Google Cloud Platform custom Virtual Machine images running on Google Compute Engine.
In this part four, I will explore current work I am doing with a research group. In particular, I’ll cover some recent work on preparing researchers to work ‘cloud-natively.’
In a word — it’s ‘serverless.’ Why? Because using scalable cloud services (via API endpoints) rather than creating, configuring and maintaining docker container or virtual machine images and their orchestration systems is simpler.
So, if this is the case, then why I have not covered serverless Hello World bioinformatics pipeline patterns yet in this series? To date I’ve only written about cloud-based Infrastructure as a Service (GCP GCE Virtual Machines) or Platform as a Service (AWS EMR). Why is that?
Serverless pipelining is indeed seductive, but also elusive…how do we get there from the current world of on-premise HPC clusters? What should the Hello World for cloud-native pipelining look like?
Of note is that despite work I’ve done with researchers to preview serverless pipeline examples — even using bioinformatics data — adoption of those serverless pipelining services remains low. In considering possible reasons for this, I was reminded [via work with my graduate students interns] that recent university Computer Science graduates are not taught the SQL query language (or anything about databases) in their CS courses at university. I assume this lack of familiarity with database concepts extends to bioinformatics researchers as well.
BigQuery, a serverless SQL cloud service, has been available since 2011. Its scalable, pay-by-data-scanned serverless model seems perfect for variable, bursting genomics analysis pipelines. So why isn’t BigQuery (or other comparable services, such as AWS Athena) being widely used in genomic analysis pipelines?
The answer appears to be the query language — SQL. Although in many (most?) verticals I had previously worked in (i.e. fintech, adtech, gov’t….) relational databases and associated SQL queries have been pervasive for many years, bioinformatics research appears to differ.
Researchers, in my experience, use files (not data tables) and programming languages, such as R, Python or even Java, to build their data pipelines. While patterns such as cloud-based VM images or customized docker container and pipeline languages (such as Nextflow, CWL or WDL) are being used to move on premise (mostly HPC) genomics workloads to the cloud, I wonder…
Could we move faster with serverless SQL?
Although I don’t have a large number of data points about the effectiveness of the serverless pattern for genomics, I do have some. In work that I’ve done with Dr. Denis Bauer and her team at CSIRO Bioinformatics in Sydney, Australia, they’ve had multiple successes using serverless design patterns.
However, even they aren’t using BigQuery yet. Why not? I wondered.
In summer 2019, I created an open source course on GitHub ‘gcp-for-bioinformatics’. Subsequently, I’ve received positive feedback from several research groups on this course. This course is based on my popular general introductory GCP course, hosted on Linked In Learning (‘GCP Essential Training.’)
The difference between my LinkedIn GCP course and my Open Source GCP course is that the open source course uses sample data and example solutions specific to bioinformatics/genomics. For example in the open source course, I use .fasta, .bam, .vcf, etc.. files, rather than .csv, etc..
I looked around for an example course which would explain SQL query concepts using bioinformatics data. I found this one. Although it’s a good start, I felt like there were a number of limitations in this course. The first of which was that the example queries were too complex, lumping too many concepts together. Also there was no implementation (database set up) provided.
As a next step I considered, what would it look like to use this bioinformatics SQL course in a cloud-native way. The natural answer was to modify the course to work with Google Big Query. I thought it would be trivial to ‘covert’ the course for this purpose. Actually doing this though, took a bit longer…
The sample includes 5 ‘tables’ populated with a very small amount of data. Shown below is a screen capture of a portion of the example data. It’s a point key that this example data is very domain-specific to genomics.
I wanted to use the Google BigQuery ‘auto detect schema’ feature to quickly convert this information to BigQuery dataset tables. First, I scraped and saved the data as local CSV files. Although I was eventually able to get this feature to work, I had to make a number of changes to the files. Changes included the following:
Once I made these changes, I was able to quickly upload the resulting CSV files to create tables in my BigQuery dataset. Next I wanted to make my dataset public so that it could be used in a tutorial. To do this, I clicked the ‘Share Dataset’ button and added `allUsers` and `allAuthenticatedUsers` to the BigQuery Dataset Viewer permissions for this dataset. The dataset is shown below in the BigQuery Web UI.
In the next step of the course conversion, I began to test the query “answers” provided in the original sample. The first change I needed to make was to alter queries to fully qualify all references to the example tables (or views) in my public BigQuery dataset. I did this by updating the FROM portion of each SQL statement. To fully qualify an object in BigQuery, use this pattern <projectName>.<datasetName>.<tableOrViewName>.
This update was required due to the need for students who use my tutorial to be able to access my (public) dataset from within their own GCP project. The required change to each query looked like this:
OLD: SELECT * FROM experimentsNEW: SELECT * FROM `gcp-for-bioinformatics.sql_genomics_examples.experiments` AS experiments
Next I added more intermediate queries to better level the learning by providing examples that added concepts more gradually than the original example queries. Sometimes I just ‘broke apart’ the original answer queries, other times I re-wrote for clarity. I also changed the source data to make the results more meaningful.
In teaching standard SQL syntax to hundreds of classroom students in the past, I found students benefited from visuals — particularly when they were learning JOIN queries. So I next created two quick reference charts. One for SQL Keywords and one for SQL JOIN queries. I tested the effectiveness of these charts, by posting them on Linked In. I got a quick positive response. For reference, these charts are shown below.
Next I added some screenshots of the tables involved in various types of joins — self-joins, two-table joins, etc.. You can see that in my screenshots, I took the time to highlight the table join columns. I’ll include one example below.
As I continued to test all of the original queries, I noticed that some of the potential answers either weren’t supported, weren’t recommended (correlated subqueries) or didn’t work as written (EXCEPT syntax slightly differed) in BigQuery. So I updated those queries.
Additionally, I updated a couple of data points and query questions to make the flow a bit more natural (and pattern-based). Also I removed some redundant (and needlessly complex) content.
Then I added a short ‘how-to-get-started-with-BigQuery’ section at the beginning of my tutorial with the goal of getting students running their first query within minutes of starting the tutorial. I also added some ‘learning more’ links at the end.
Finally, I pulled out all of the potential answers to the SQL Query questions in the document into a separate ‘answers’ file and linked the answers in the tutorial. I also added a ‘what could go wrong?’ section at the beginning, explaining and showing the function of the query validator in the BigQuery query window.
At this point, I’ll release v1 of my ‘cloud-native-SQL’ tutorial with the name “BigQuery Bioinformatics SQL Query Lessons” (beginning of course shown below). I am considering creating a Jupyter notebook as well, but want to get more feedback.
Over the years, I’ve had a number of people ask me about the process I use to create ‘Hello World’ content. In writing this short article, I wanted to provide a view into the type of work I do when updating/creating technical content at this level.
Also, and this is a key point — the testing, organization and clarity that I added to the original lessons are aimed to improve the usability of the tutorial. The entire point of HelloWorld is this, right?
If they can’t use it, it’s worthless
How did I do? Feedback welcomed.
Cloud Architect who codes, Angel Investor
6 
1
6 
6 
1
Cloud Architect who codes, Angel Investor
"
https://medium.com/unusual-ventures/styra-expanding-open-source-and-commercial-solutions-for-kubernetes-cloud-native-security-fc08cdbb165?source=search_post---------151,"There are currently no responses for this story.
Be the first to respond.
Today, Unusual Ventures is excited to announce that we will be continuing our partnership with Styra to expand open source and commercial solutions for Kubernetes/cloud-native security. We are thrilled to participate in the company’s $14M Series A financing in partnership with Accel and A Capital.
Styra, the founders of Open Policy Agent (OPA) and a leader in cloud-native authorization, was built to reinvent policy and authorization to meet today’s privacy demands in the new stack. The company also built OPA to solve authorization challenges that arise from the speed and scale of containerized application environments. Building on the proven success of OPA, Styra’s commercial Declarative Authorization Service provides a sophisticated management plane that provides context-based guardrails — built from a graphical policy library — to mitigate risks, reduce human error, and accelerate development.
We first met Styra’s co-founders, Tim Hinrichs and Teemu Koponen — both exceptional engineers and brilliant technologists — when they were at Nicira, a company that was focused on software-defined networking and network virtualization that was acquired by VMware in 2012. They observed early on that the existing technology for authorization was not designed for this new cloud-native era. Building on his work on an OpenStack project called Congress during his time at VMware, Tim recognized these shifts would have massive implications for companies in every industry and the problem would only become larger as more companies embraced the cloud. Both Tim and Teemu realized they needed to think from first principles and build this technology from the ground up in order to support this new reality. Early validation of Styra’s Open Source project OPA from companies like Netflix and Capital One made it clear they were onto something.
In a year, Styra has seen tremendous progress. We added a fantastic leader in Bill Mann to take the reigns as CEO and drive the go-to-market strategy. As the former CPO of Centrify, Bill has a long history of building great solutions in the access management and authorization space. The team also hired product, marketing, and sales leaders to help drive these key areas of the business. Styra has seen their open source project Open Policy Agent (OPA) graduate from early Sandbox to proven Incubating status in the Cloud Native Computing Foundation. They have also seen widespread adoption with well over 100 companies using OPA in production, and hundreds more using OPA in development and test environments. Additionally, Styra’s commercial solutions have been adopted in enterprises across verticals and sizes.
Styra’s early success comes down to two key ingredients: a disruptive, innovative product in a rapidly growing market and a world-class team. Tim has a PhD in declarative language from Stanford and truly is an expert in his domain. This type of expertise and understanding is exactly what you want at the heart of a disruptive company. Bill is a seasoned product and go-to-market executive with decades of experience taking innovative products to market, and building a team that is also world-class. We believe this combination is the foundation you want for any company and market leader. In short, we believe you’re just seeing the beginning of what’s to come from the Styra team.
On behalf of the Unusual team, we are excited to tackle the next leg of the journey with Team Styra. Onward!
Read more about Styra’s Series A announcement
To learn more about Unusual Ventures and what our portfolio companies are up to, follow us on Twitter
Your best partner for the journey ahead.
101 
101 claps
101 
Written by
Founder Unusual Ventures. Early stage investor Enterprise and Consumer IT.
Your best partner for the journey ahead.
Written by
Founder Unusual Ventures. Early stage investor Enterprise and Consumer IT.
Your best partner for the journey ahead.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devops-for-the-cloud/cloud-native-devops-requires-squad-autonomy-2aefdbade8ce?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
With the move to Cloud Native, many teams are accelerating their DevOps processes with a strong focus on Continuous Delivery. To realize the benefits of Continuous Delivery, many enterprise development organizations need to eliminate inhibitors to adoption including traditional governance models that disrupt agility.
The cloud is a significant disruption to the tech industry. Developers are demanding control of their ability to delivery quickly and receive instant feedback on what works and does not work. Adopters of the Lean Startup philosophy has resulted in a significant shift in how developers bring new technology to market. Developer responsibilities have expanded and now have to consider the impact of their code deliverables beyond the code commit to now having to support the live operational aspect of their applications that are running in the cloud with possibly 1,000s of tenants all leveraging a collection of highly scalable Cloud Native applications.
In this article, we will argue that team autonomy requires the removal of high touch enterprise governance. The shift in the role of DevOps requires a level of squad autonomy where development teams automate the entire DevOps workflow. These workflows typically are triggered with a single Git commit driving a pipeline that generates insights into the quality of their code based upon code quality and test coverage all with the intent to gauge readiness for promotion to production. These teams will demand dashboard style views of their deployments with real time metrics of their entire DevOps process. In shifting the power back to the squads, teams can react in a more agile fashion and adapt to industry disruptions on the turn of a dime.
Squad AutonomyIn Continuous Delivery, the notion of delivering small incremental changes is a basic table stake for high performing teams. To achieve high velocity, teams need to be prescriptive with the tools that will be used to deliver code to production with limited intervention. These tools when viewed in aggregate are considered a toolchain. Many teams view their toolchains similar to a production assembly line and often use the term pipeline to describe the path code takes to get released to production. Example of the type of tools range from source code repos (git commit triggering builds), to a CDI/CI tool such as Travis (build, package and test of the source code), to dependency management (using tools like NPM). Eventually, the packaged code gets deployed to various environments such a test, staging and then ultimately to production. With the focus on small incremental changes, each deployment is easily understood and verifiable for quality and stability to ensure the application continues to operate efficiently.
TransparencyIn the past, the role of Release Manager controlled the roll out of new code. However, this is changing with the move to the cloud. Gone are the days where a deployment consisted of a large batch of new function, the constant incremental deliveries by multiple teams asynchronously has shifted the paradigm. Dashboard summaries providing drill downs to specific micro services, their test coverage, versioning of their applications and their dependencies as well as real time health dashboards have become the new normal. Release Managers now work closely with teams to ensure that key performance indicators such as Service Level Agreements are met, teams are adopting a consistence cadence on releasing new function to production and that git branches are being integrated on a frequent basis. By having this transparency and open interaction model between the DevOps teams and Release Management, the need for traditional governance is removed from the discussion. Based upon these shifts, conventional roles such as Release Manager play a completely different role in a strong DevOps culture.
Now that we have talked about squad autonomy and transparency, lets go deeper into how Cloud Native Applications relates to DevOps and where containers play in this environment.
The Twelve-Factor AppThe leading methodology for building Cloud Native applications is a method called “The Twelve-Factor Methodology”. Adopters of these principles, in theory, allows for supreme agility and provides the foundation for realizing the benefits of Continuous Delivery. In this methodology, the developer is presented with guidelines ranging from the source control methodology using modern tools such as Github, to how you leverage dependencies in your application all the way through to how you build, test, deploy and manage your application throughout the DevOps lifecycle. In theory, if you follow these principles, your application will be resilient, scalable while remaining highly maintainable allowing your developers to focus on innovation and increasing your development velocity.
ContainerizationContainerization — also called container-based virtualization and application containerization — is an OS-level virtualization method for deploying and running distributed applications without launching an entire VM for each application. Instead, multiple isolated systems, called containers, are run on a single control host and access a single kernel. [1]
Containerization has gained significant adoption in the industry with the release of Docker. Docker allows developers to quickly package and deploy their code at an application level and provides a consistent ecosystem for developers to host and test their applications both locally and in the cloud. As the development environment is more closely aligned with their production environments, via containers, the DevOps cycle is able to be even further accelerated and maps squarely on the Twelve-Factor app category around “Dev/Prod Parity”.
SummaryContinuous Delivery is a real game changer for adopting Agile Principles. The concept of squads and autonomy really empowers the development teams to deliver high quality innovative solutions in a model that minimizes risk. The key is to ensure that teams continue to provide high value insights into their DevOps progress ranging from the rate and pace of code commits to ensuring that SLAs are continuing to be met. With a industry focused on the how many 9s your application is, your level of autonomy can easily be impacted. My hope is the days of big bang organizational level releases is a thing of the past and that we can continue to focus on building autonomous high performing engaging teams!!
[1] http://searchservervirtualization.techtarget.com/definition/container-based-virtualization-operating-system-level-virtualization
Discover more stories
Originally published at https://www.linkedin.com on March 27, 2017.
Articles focused on delivering software at startup speed at…
6 
6 claps
6 
Articles focused on delivering software at startup speed at the scale of the enterprise.
Written by
NCR Executive Director and Chief Architect for Retail Solutions. The opinions expressed here are my own. Follow me on Twitter @todkap
Articles focused on delivering software at startup speed at the scale of the enterprise.
"
https://medium.com/@lynnlangit/cloud-native-hello-world-for-bioinformatics-53ecbcb9631b?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lynn Langit
Dec 5, 2019·8 min read
In part one of this series, I discussed rationale for building a cloud-native ‘Hello World’ as an important first step in moving bioinformatics analysis successfully to the public cloud.
In this article, I’ll dig into the details of one (of many) examples of using this approach. This example is built on the AWS Cloud and it uses a number of AWS services. The most significant of which is the AWS Marketplace.
The AWS Marketplace is a set of services that are designed to make building reproducible cloud environments easier. The Marketplace includes a WebUI (shown below), a command-line tool (cli) and an API-based programming model.
The Marketplace is designed to allow builders to set up what I like to call ‘smarter templates’. These templates can be run with default service configuration values or customized, which allows users to quickly build up complex infrastructures. These infrastructures often include multiple cloud service instances and their associated configurations. As of this writing, there are over 7,000 of these smarter templates, which are called ‘solutions’, in the marketplace.
In addition to the large number of solutions available in the Marketplace, it’s important to note that AWS has been expanding the Marketplace capabilities to be able to host smart templates which support a number of delivery methods (including VMs, Containers and more).
The basis of operation for most of the items listed in the Marketplace is one or more AWS CloudFormation YAML or JSON templates. Templates define AWS service configurations (for S3 buckets, EC instances, etc…). In addition to templates, CloudFormation includes Stacks (defined as a set of related resources managed as a single unit) and Change Sets (defined as summaries of your proposed changes to a stack). The diagrams below show how the templates, stacks and change sets are designed to work.
First…shown below (from AWS documentation) is a diagram of the process of building cloud infrastructure by creating a stack from a template. When the user clicks ‘create stack’, then AWS CloudFormation uses the associated template to allocate and configure AWS service instances such as EC2 Virtual Machines, S3 storage buckets, IAM security roles, etc…
In addition to using stacks to create cloud infrastructure, they can be used manage and maintain it. Shown below (from AWS documentation) is the process for updating that infrastructure via a change set.
In addition to templates, stacks and change sets, the AWS Marketplace adds a the ability to include a number of types of metadata to stack. This metadata can include a description of the solution’s infrastructure. It can also include instructions and direction on how to configure that infrastructure. Additionally it can contain a selection of suggested configuration options (in our case EC2 instance size), and, importantly, a service-cost estimator.
Although the original purpose of the AWS Marketplace was to enable commercial vendors to sell (lease) configured versions of their software running on AWS, the CSIRO Bioinformatics team used the Marketplace a bit differently. This team decided to focus on the ease-of-use features, such as template metadata, so that they could make it simpler for bioinformatics researchers world-wide to try out their open source genomic-scale custom machine learning algorithm (VariantSpark) on AWS.
To start the team fully utilized the metadata tagging capabilities in the AWS Marketplace. When a user enters the search term ‘bioinformatics’, the CSIRO VariantSpark solution appears on the first AWS Marketplace results page, as shown below.
In working with a number of new-to-AWS-Marketplace customers over the years, I’ve found that they’ve often skipped this tagging step, which is unfortunate. As mentioned, with over 7,000 entries in the AWS Marketplace, focusing on discoverability is an important first step in creating a broadly usable solution.
If people can’t find it, they can’t use it
Another important aspect of this sample is that the team built a reference infrastructure that can be run using the AWS Free Tier. Although the ultimate goal of building analysis infrastructure on the public cloud is to improve speed and cost of running genome-sized workloads, taking this ‘try it fast and free first’ approach implements the cloud-native Hello World pattern elegantly.
Details matter too. Notice also that the team is using AWS Marketplace solution versioning, the current published version being 1.0.10.
In addition to following best practices of naming, tagging, versioning and pricing their solution clearly, the CSIRO team did a great job describing how their example works in the AWS Marketplace VariantSpark solution ‘Product Overview’ and ‘Highlights’ sections. They included both a time-to-run and cost estimate in their concluding sentence.
“VariantSpark can process 200 samples with 20M variables in 1 hour consuming $3 of AWS resources. VariantSpark compute time increases linearly with both variables and samples.”
Also, of note, is that the team took the time to record and link a short screencast of how to set the solution up (showing how to fill out the Cloud Formation template parameters) on the main AWS Marketplace page.
In the ‘Usage Information’ section, further enhancing the usability of the solution, the team included an example Jupyter notebook, sample data and also a summary diagram of the AWS infrastructure that will be built (shown below).
A series of large yellow buttons in the UI guide the user through the process to quickly build and test this example. First, the user must subscribe to this solution. This is one-time process and reflects the history of the marketplace, that is, many commercial vendors use the feature to lease their software to customer via the marketplace. Subscribing is one-time operation.
Next the user is presented with the yellow ‘Continue to Launch’ button shown below. Note the small number of configuration options presented at this phase of the process. This is by design — users need only to select the Region — and is done to make set up simpler. Screen shown below.
In the next screen users can select between a ‘Cloud Formation’ or ‘Service Catalog’ launch. They would normally choose the first option for testing. The second would be used in Enterprise scenarios (where the ability to launch solutions from the AWS Marketplace should be controlled by launch policies associated with the AWS Service Catalog).
Clicking ‘launch’ here will open the template in the CloudFormation UI. The AWS Marketplace VariantSpark solution has an associated CloudFormation template file which is stored in an AWS S3 bucket. Users click ‘next’ to go the main configuration screen, here they enter a name for their stack, the number of CPUs (default is 32), their local IP address (for the Jupyter notebook client) and then click ‘next’ a couple more times to accept all template defaults. Note that the last configuration page shows that some IAM resources will be created and is required to be manually checked for the solution to function.
Users review stack (instance creation) event notifications in the CloudFormation console to view progress as shown by example below.
Users will need to wait ~10 minutes, then they’ll see the stack status change to the green text message which says ‘CREATE_COMPLETE’. At that point, they can navigate to the ‘Outputs’ tab on the their stack page, then click on the Jupyter URL hyperlink to open and run the example notebook, a portion of which is shown below.
While the notebook is running, users can view the workload overhead using the Ganglia tool and associated visualizers, again shown below. AWS includes Ganglia as part of it’s EMR (Elastic Map Reduce or managed Hadoop/Spark) service, which this solution uses, because CSIRO’s VariantSpark library is written to run on top of a Spark cluster.
In addition the many advantages discussed already, yet another aspect of using the AWS Marketplace and associated CloudFormation stacks for ‘Hello World’ scenarios is that users can quickly and cleanly REMOVE all service instances when they are done testing with one action. In this case, the template created 24 service instance (called ‘Resources’) and a partial view is shown below.
In the CloudFormation UI, users can simply select the stack and then click ‘delete’ to delete all associated service instances when they are done testing. Additionally, if users want to make a copy of the stack for their own use (or further customization) they can easily do that by clicking on the ‘create stack’ button in the UI. After the termination process completes, the stack is removed from the CloudFormation console. Users can also verify that the associated EC2 instances (for example) have been terminated by viewing the EC2 console (example shown below).
CSIRO implemented a cloud-native ‘Hello World’ for their VariantSpark bioinformatics library using the following best practices:
Congratulations to Dr. Denis Bauer and her team on a job well done!
In the next article, I’ll be examining another approach in this space — patterns are similar, but not identical, this is part due to the choice of cloud vendors. The next article will cover a cloud-native Hello World bioinformatics example running on GCP (Google Cloud Platform).
Cloud Architect who codes, Angel Investor
19 
19 claps
19 
Cloud Architect who codes, Angel Investor
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/building-the-open-data-stack/the-future-of-apache-cassandra-is-cloud-native-1d2591b26bc5?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
Jun 2, 2021·5 min read
By Sam Ramji, Chief Strategy Officer, DataStax
Along with tens of thousands of developers and operators, and companies ranging from startups to titans like Apple and Netflix, we want to see Apache Cassandra™ become cloud-native.
We have done the work required to have an opinion: Astra is built on Kubernetes, Prometheus, Envoy, and participates in the GKE and EKS native control and management planes. We’ve reviewed work done by others, particularly those who have shared what they have learned in the form of open-source Kubernetes operators for Cassandra.
The opinion expressed in Astra is based on the technical work, changes, and trade-offs we’ve needed to make in order to operate Cassandra at scale on Kubernetes in the cloud. This opinion will continue to evolve as we continue to deliver and improve Astra.
We are sharing that opinion in contributions to the Cassandra open-source ecosystem — as OSS projects including a Kubernetes operator, a management sidecar, a metrics collector, a configuration builder, and a NoSQL testing system.
The corollary of sharing an opinion is listening to others. Through the Cassandra Enhancement Proposal process, we see a vibrant set of hard-won experiences that have led to many Kubernetes operators for Cassandra. Shared listening will lead us all to think and work together, improving our opinions with collective experience, and releasing better code for users.
We’ve learned about the structure and architecture of Cassandra as it relates to cloud deployment. This is helping us contribute to the community’s thinking on the architectural choices we will need to make together after the release of Cassandra 4.0.
We’ve put some of that into practice in our fork of Cassandra, and we expect to share this in the future. We’ve put as much as possible into code that complements Cassandra to solve for Kubernetes-specific and cloud-native needs.
There are four areas of work needed to run a cloud-native service with Apache Cassandra 3.11: gateway, operations, management, and deployment.
Astra clusters receive traffic through the gateway. Astra uses Envoy to route Cassandra binary port requests and simplify driver configuration. In cloud native configurations, having a single ingress IP is critical for managing connectivity with an elastic backend. This also reduces the number of open ports exposed. This enables Astra to support CQL in the cloud while being elastic and secure.
Astra also uses the gateway to expose REST and GraphQL APIs for easing developer burden when interacting with data assets. We believe that these better serve a new generation of developers with experience in full-stack application development.
Cluster operations need to be automated. Astra does this with a pair of components: Cass Operator and Management API for Apache Cassandra (MAAC). Cass Operator is a Kubernetes Operator for Apache Cassandra and the MAAC is a Kubernetes sidecar.
Cass Operator provides a StatefulSet of Cassandra nodes and automates the manual tasks normally performed by Cassandra administrators. This includes the start sequence for bringing up a cluster, draining nodes when they are deleted, and putting the right nodes in the right pods, such as preventing multiple nodes from deploying to the same host.
Participating gracefully in a Kubernetes environment requires providing insight into the cluster state. In practice, this means that some operations that were previously database internals, such as automated retries, or establishing Gossip links to track internal cluster state, are raised up to the application layer.
Kubernetes can then make decisions based on the health of the whole cluster, rather than each Cassandra node making independent decisions based on its own view. For example, during a rolling restart, instead of internal checks to see that a node is up, those signals are raised to Cass Operator, which makes sure that every Cassandra node in the cluster can achieve quorum.
MAAC provides a JSON interface to invoke nodetool commands and inject Vault secrets. This makes Cassandra feel more like Kubernetes while retaining operational consistency with standard Cassandra tooling. This component starts, stops, configures, and checks liveness and consistency levels. MAAC runs as a sidecar and communicates with Cassandra via CQL via a local Unix socket (optionally over mTLS). Each sidecar process is responsible for the local Cassandra instance only. This simplifies the topology and reinforces the role of Cass Operator as the orchestrator for the cluster.
Metrics are essential for running Cassandra at scale. Astra uses a Metric Collector for Apache Cassandra (MCAC) to provide management functions that integrate with Kubernetes and cloud environments.
MCAC simplifies metrics collection for Cassandra users. The traditional mechanism for metrics in Cassandra is JMX, which does not match the performance and scale needs for a deployment on Kubernetes. MCAC is built on collectd and is bundled into Cass Operator. It works for all open source Cassandra versions from 2.2 to 4.0 beta. Using collectd means that hundreds of thousands of series per node can be exported with minimal impact on Cassandra performance. It aligns with Prometheus, the standard in Kubernetes environments for monitoring, and MCAC metrics can be analyzed along with OS-level metrics such as context switches, disk performance, and network performance. Finally, MCAC creates a historical log of metrics and lifecycle events such as flushes, compactions, exceptions, and garbage collection.
MCAC is a critical part of Astra’s ability to give users visibility into the real-time running characteristics of their instance. Astra fully manages the operations of the underlying Cassandra nodes, but users need to understand the impacts of their data model and queries on the operational characteristics of the cluster.
Deployment is continuous in a Kubernetes environment. By relinquishing control of configuration changes to the operator, Astra is able to provide a more dynamic and trustworthy data layer. Astra uses cass-config-builder to drive configuration and NoSQLBench for continuous testing of the environment.
Cass-config-builder parametrically generates cassandra.yaml files based on environment requirements. When a Cassandra pod is started up, it ingests this configuration. IP addresses, network information, performance tuning, security, disk optimization, seed providers are all key components for running Cassandra correctly. As Kubernetes controls the overall environment while scaling up and down, reacting to hardware failure, or changing fleet-wide properties, Cassandra needs to adopt new configurations.
NoSQLBench gives users the ability to create arbitrarily sized synthetic datasets and use those datasets to execute large scale load tests based on real-world application workloads to ensure that the cluster is trustworthy. It works with any NoSQL database. As part of a continuous deployment environment, test automation for the database can drive billions of write and read operations, thrashing storage volumes and demonstrating the cloud environment’s performance characteristics empirically.
Our aim is to use our ten years of experience building and operating enterprise Cassandra deployments to build a world-class cloud-native service for Apache Cassandra.
Everything we see in feedback from Cassandra users tells us that Kubernetes needs Cassandra and Cassandra needs Kubernetes. We’re building our “Cassandra-as-a-Service” on Kubernetes so that we are in the thick of the fight to make Cassandra the best open-source, scale-out, cloud-native database in the world.
Cassandra is becoming cloud-native. We’re thrilled to be on that journey with all of you.
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
20 
20 
20 
We’re huge believers in modern, cloud native technologies like Kubernetes; we are making Cassandra ready for millions of developers through simple APIs; and we are committed to delivering the industry’s first and only open, multi-cloud serverless database: DataStax Astra DB.
"
https://medium.datadriveninvestor.com/how-to-monitor-and-autoscale-cloud-native-applications-in-kubernetes-a276ed971500?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
While an increasing number of developers continuously accept and recognize the design concept of the cloud-native applications, it is critical to note that Kubernetes has become the center of the entire cloud-native implementation stack. Cloud service capabilities are revealed from the standard Kubernetes interface to the service layer through Cloud Provider, CRD Controller, and Operator. Developers build their own cloud-native applications and platforms based on Kubernetes. Hence, Kubernetes is now the platform for building platforms. Let’s understand how a cloud-native application seamlessly integrates monitoring and autoscaling capabilities in Kubernetes.
www.datadriveninvestor.com
This article is a compilation of various excerpts from a speech titled “Cloud-native Application Monitoring and Autoscaling in Kubernetes” by Liu Zhongwei (Mo Yuan), at KubeCon. Liu is a technical expert at Alibaba Cloud Container Platform.
Alibaba Cloud container service for Kubernetes mainly supports the following two types of integrations.
Alibaba Cloud container service for Kubernetes integrates with four cloud monitoring services, including Simple Log Service (SLS), Application Real-Time Monitoring Service (ARMS), and Application High Availability Service (AHAS), and CloudMonitor.
SLS is mainly responsible for collecting and analyzing logs. In the Alibaba Cloud container service for Kubernetes, SLS collects three different types of logs:
In addition to the standard link for collecting logs, SLS also provides the upper-layer log analysis capability. By default, it provides the audit analysis capability based on APIServer, the observability display of the access layer, and log analysis of the application layer. In the Alibaba Cloud container service for Kubernetes, the log component is installed by default, and developers only need to check it while creating the cluster.
ARMS is mainly responsible for collecting, analyzing, and displaying performance metrics of the application. Currently, it mainly supports the integration of Java and PHP, and collects metrics at the JVM layer, such as GC times, slow SQL of applications, and call stacks. It plays a very important role in performance tuning.
AHAS is an architecture-aware monitoring service. Generally, most of the load types in the Kubernetes cluster are microservices, and calling topology of microservices is also complex. Therefore, when the network link of the cluster has problems, the biggest challenge is how to quickly locate, discover, and diagnose problems. AHAS shows the cluster topology through the traffic and trend of the network, providing a higher level of problem diagnosis.
The compatibility and integration of open-source solutions are also part of the monitoring capability of the Alibaba Cloud container service for Kubernetes. It mainly includes the following two parts.
In the Kubernetes community, heapster/metrics-server is a built-in monitoring solution, and core components, such as Dashboard and HPA, depend on the metrics provided by these built-in monitoring capabilities. Due to the inability to ensure the complete synchronization of the release cycle of components in the Kubernetes ecosystem and the release of Kubernetes, some consumers with monitoring capabilities have monitoring problems in Kubernetes. Therefore, Alibaba Cloud came up with enhancements on metrics-server to achieve version compatibility. In addition, for node diagnosis, the Alibaba Cloud container service enhances NPD coverage, supports the FD file handle monitoring, the NTP time synchronization verification, and the inbound/outbound network capability verification. Further, it makes the eventer open-source to support the offline transmission of Kubernetes event data to SLS, Kafka, and DingTalk, thus implementing ChatOps.
To support Prometheus, the standard third-party monitoring platform in the Kubernetes ecosystem, the Alibaba Cloud container service provides integrated charts for developers to integrate with one click. In addition, there are enhancements at the following three levels:
Alibaba Cloud container service for Kubernetes mainly includes the following two types of autoscaling components.
For scheduling layer autoscaling components, all autoscaling operations are pod-related, regardless of the specific resource situation.
Resource layer autoscaling components support operations regarding the relationship between pods and specific resources.
The preceding diagram shows a simple Demo, where the application subject is APIservice. The APIservice calls the database through the sub-APIservice, and the access layer is managed through Ingress. Use PTS to simulate the traffic generated by the upper layer, collect logs of the access layer through SLS, and collect application performance metrics through ARMS. Finally, expose external metrics through alibaba-cloud-metrics-adapster to trigger HPA for re-computing workload copies. When the scaled pod occupies all the cluster resources, virtual-kubelet-autoscaler triggers to generate ECI for hosting loads beyond the cluster capacity planning.
It is very simple to use monitoring and autoscaling capabilities on the Alibaba Cloud Container Service for Kubernetes. Developers only need to install the corresponding component chart in one click to get complete access. With multi-dimensional monitoring and autoscaling capabilities, cloud-native applications obtain higher stability and robustness at the lowest cost.
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
57 
57 claps
57 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.datadriveninvestor.com/new-thoughts-on-cloud-native-why-are-containers-everywhere-ada1b7264b64?source=search_post---------156,"There are currently no responses for this story.
Be the first to respond.
By Muhuan
On April 24, the first Cloud Native Industry Conference sponsored by the China Academy of Information and Communications Technology (CAICT) was held in Beijing. During his speech “Cloud Native Digital Leading Future”, Yi Li, Director of Alibaba Cloud Container Service, said “Cloud native technology can support Internet applications, and has a profound influence on new computing architectures and intelligent data applications. Represented by containers, service grids, microservices, and Serverless Runtime, cloud native technology has delivered a new method of application construction.”
This document is based on the speech given by Yi Li.
www.datadriveninvestor.com
Currently, most enterprises completely embrace cloud computing. The all-in-cloud era has witnessed three important changes: cloud-based infrastructure, Internet-based core technologies, and data-driven and intelligent services. In different fields and industries, many business applications were born in the cloud so that many enterprises are more and more similar to Internet companies. Therefore, technical capabilities are viewed as indispensable core competencies. At the 2019 Beijing Alibaba Cloud Summit, Zhang Jianfeng, President of Alibaba Cloud Intelligence, mentioned the significance of vigorous investments in the cloud native technology when talking about “Internet-based core technologies”.
Why should we embrace cloud native technology? On the one hand, cloud computing has rebuilt the entire software lifecycle, from architecture design to development, construction, delivery, and O&M. On the other hand, the IT architectures of enterprises have changed significantly and services deeply depend on IT capabilities. These two aspects contribute to complexity and challenges.
As the development of human society is followed by technology revolutions and changing divisions of labor, complexity can be reduced by using cloud native technology, which reflects IT progress.
First, Docker decouples applications from the runtime environments: The loads of many business applications can be containerized, and containerization makes applications agile, migratable, and standardized. Second, Kubernetes decouples resource orchestration and scheduling from underlying infrastructure: Application and resource control is simplified, and container orchestration improves the efficiency of resource orchestration and scheduling. Third, the service grid technology represented by Istio decouples service implementation from service governance capabilities. In addition, Alibaba Cloud provides diverse development tools (such as APIs and SDKs) for integrating third-party software. This opens up extensive possibilities for cloud ecosystem partners. Such layered technology architecture has advanced the division of labor and significantly accelerated technical and business innovation.
Alibaba Cloud believes that cloud native technology can support Internet-scale applications, accelerate innovation, allow low-cost trial and error, and avoid differences and complexity of underlying infrastructure. In addition, new computing approaches, such as service grids and serverless computing, make the entire IT architecture extremely flexible so that applications can better serve business purposes. You can build domain-oriented cloud native frameworks based on Alibaba Cloud Container Service, for example, Kubeflow for machine learning and Knative for Serverless Runtime.
Containers are everywhere. As a provider of Container Service, we believe that container technology will continue to develop and be applied to new computing forms, new application loads, and new physical boundaries. The following shares relevant insights and new thoughts.
The cloud native technology aims to make enterprises and developers focus only on application development rather than infrastructure and basic services. Similarly, serverless computing turns application services into resources and allows you to call resources from the client through APIs. More importantly, the Pay-As-You-Go mode can reduce your costs.
Serverless Runtime can be implemented for infrastructure containers, application service encapsulation, and event-driven function-oriented computing.
The cloud native serverless runtime can be implemented in multiple forms. Many manufacturers have designed different service solutions.
Containers were not considered suitable for traditional applications. However, they have now been significantly improved. Containers are supported by the Windows ecosystem. Most core capabilities of Kubernetes V1.14, such as pods, services, application orchestration, and Container Network Interfaces (CNIs), are now supported on Windows nodes. Windows systems still have a 60% market share. Traditional virtualization-based applications, such as Enterprise Resource Planning (ERP) software, ASP-based applications, and a large number of Windows databases, can be containerized without rewriting the code.
New architectures based on container technology will generate new business value for applications. As cloud native AI is an important application scenario, we need to quickly build an AI environment and efficiently use underlying resources to seamlessly adapt to the full lifecycle of deep learning. For AI engineering, the cloud-native system can improve efficiency in four aspects.
Distributed training of deep learning, for example, can be enhanced in three aspects through Alibaba Cloud Container Service. Resource optimization: Heterogeneous resources, such as CPU and GPU, can be scheduled in a centralized manner. Virtual Private Cloud (VPC) or Remote Direct Memory Access (RDMA) is used for network acceleration. Performance improvement: With GPU FP64 P100, the acceleration ratio is increased by 90%, and the performance is improved by 45% relative to native TensorFlow. Algorithm optimization: Message-Passing-Interfaces (MPIs) replace gRPC, ring-allreduce, computing-communication overlapping, and gradient convergence.
In other high-performance computing scenarios, such as gene data processing, an Alibaba Cloud user can process a WGS with 100 GB data within five hours. A complex process with more than 5,000 steps is supported, and 500 nodes can be scaled out within 90 seconds. In this way, we can make the most of the extreme elasticity of containers.
The best-known container infrastructure is an Internet Data Center (IDC). The extreme elasticity of containers allows IDCs to scale applications and resources in response to fluctuations in business traffic, and thereby ensure high utilization and cost-effectiveness.
With the advent of 5G technology and IoT, traditional cloud computing centers that store and compute data in a centralized way can no longer meet the needs of terminal devices for timeliness, capacity, and computing power. The extension of cloud computing capabilities to edges and devices and the centralized implementation, delivery, O&M, and control through a center will become an important trend in cloud computing. Based on Kubernetes, the cloud-native technology provides the same functions and experiences as in the cloud. It distributes applications in a cloud-edge-end integrated mode, supports application distribution and lifecycle management in different kinds of system architectures and network conditions, and optimizes access protocols, synchronization mechanisms, and security mechanisms for edges and devices.
As described in the preceding section, application containerization supports standard migration and achieves an agile and flexible cloud-native application architecture. In this way, multi-cloud or hybrid cloud deployment is significantly simplified, cost-effectiveness is optimized, and more options are available. For example, security compliance requirements are met, business agility is enhanced, and regional coverage is improved.
Containers are applicable to multiple kinds of infrastructure, such as IDCs, edge clouds, multiple clouds, or hybrid clouds. In this way, developers can concentrate on the applications themselves.
The era of cloud native technology is the best era for developers.
Cloud native technology can support Internet applications and has a profound influence on new computing architectures and intelligent data applications. Represented by containers, service grids, microservices, and Serverless Runtime, cloud native technology has delivered a new method of application construction. In addition, the cloud native technology is expanding the boundary of cloud computing by promoting borderless computing in multi-cloud and hybrid cloud mode and through cloud-edge-end integration.
In the era of cloud native technology, cloud manufacturers can play a larger role and create more value for customers.
Cloud manufacturers need to help users take full advantage of clouds and help enterprises create commercial value.
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
146 
146 claps
146 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@sebgoa/triggermesh-vision-for-cloud-native-integration-e844cc4cac9?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sebastien Goasguen
May 19, 2021·4 min read
Hi everyone! I know it’s been a while since I posted here. 2020 has been nuts for everyone, between COVID, working from home in what is absolutely not normal working from home, social distancing and the like, I think everyone has been stressed and in somewhat of a funk. I am no stranger to this funk, but let’s try to get out of it :)
Things have been quite busy at TriggerMesh during COVID, we closed our seed round 4 months before the lock downs, despite of that we got going. I want to start posting again with an update on our vision and direction for cloud native integration.
At TriggerMesh, we are building what we call a cloud native integration platform. I admit that at first glance ‘cloud native integration platform’ is a bit of a mouthful. But if you’ll indulge me, it’s actually a very simple concept that I think will make sense to cloud native developers and people experienced with Kubernetes.
I’m going to start from the end — ‘Integration platform.’ This is no different than what we’ve been doing in the enterprise for quite a while — namely, integrating different applications together, usually through some type of messaging system, point A to point B, through a messaging channel. It goes back to some of the basic principles of service oriented architecture (SOA), enterprise service bus (ESB), and so on. The enterprise integration patterns are well known.
In many ways, enterprises today are doing exactly the same thing — building and integrating applications. The big difference is that now, we need to start doing it in a cloud native manner and leverage our entire Cloud migration/adoption initiatives.
It means that the application building blocks are containerized and they frequently run on a platform like Kubernetes. Plus, being cloud native, it means that a lot of the end points we’re trying to integrate come from the cloud. They come from infrastructure clouds, or generic clouds like Google, Azure, AWS, etc. They come from ad hoc clouds like Salesforce, and Twilio, etc. So very much, the integrations we are now doing are multi-cloud.
That’s what we mean by cloud native. And this new mode of building apps creates a unique set of integration challenges to bring multiple clouds together with on-premises applications.
So, TriggerMesh focuses on integrating this multi-cloud and cloud native environment and, critically, we help AUTOMATE all these integration. Taken together, integration and automation can accelerate our customers’ time to market when you’re building a new app and bringing it to your customers. TriggerMesh gives our customers the platform they need to integrate, automate, and accelerate (the TriggerMesh moto by the way !)
I want to zoom in on the automation piece, because we believe this is critically important. Automation is key because we believe in building integration as code. We want integrations to be defined through a set of API objects, following the OpenAPI specification, and very friendly to the entire Kubernetes ecosystem. So you have to think about using Kubernetes API objects to declare integrations. And of course, when we talk about integration as code, this means that the integrations can be driven by your CI/CD system and continuously delivered to your setup.
Managing and deploying integrations with CI/CD means now all your integrations benefit from this powerful automation. You benefit from your CI/CD security, review process, testing, etc. It’s hard to overstate how significant this is. In our view, this makes it hard to justify an expensive and slow ESB-based integration approach. When the necessary controls are built into your CI/CD pipeline, adding TriggerMesh integration as code makes getting the integrations you need faster and easier without sacrificing the enterprise security you need.
To recap, Cloud Native Integration is defined by five core components:
With all of this in mind, the TriggerMesh Cloud-Native integration platform allows people to describe integration as code and run those integrations in Kubernetes. As a sneak peek, we are trying to make this as simple as possible with our own language using HCL. For instance sending GitHub events to a Kinesis stream is starting to look like this
I am very interested to hear your thoughts and I promise I am going to try to post more, in the meantime feel free to reach out @sebgoa or sebgoa@triggermesh.com
@sebgoa, Kubernetes lover, R&D enthusiast, co-Founder of TriggerMesh, O’Reilly author, sports fan, husband and father…
7 
7 
7 
@sebgoa, Kubernetes lover, R&D enthusiast, co-Founder of TriggerMesh, O’Reilly author, sports fan, husband and father…
"
https://medium.com/hackernoon/a-deep-dive-into-cloud-native-from-basics-to-applications-70d4d9f20e4?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
The cloud native concept emerges as cloud computing enjoys rapid development. Cloud-native has become extremely popular. You will be considered outdated if you do not understand Cloud-native as of this year.
Although many people are talking about cloud native, few have told you exactly what cloud native is. Even after finding and reading some cloud-native materials, most of you may still feel confused and lack a complete understanding of cloud native. At this point, you might start to doubt your own intelligence. In my case, I always tend to blame the authors’ stupidity for my incomprehensibility of certain articles, though this is not necessarily true. However, this way of thinking prevents me from being held back by my own self-doubt and I can try to remain positive.
The reason why cloud native cannot be explicitly described is the lack of a clear definition. Since cloud native is undergoing constant development and changes, no individuals or organizations have the absolute right to define cloud native.
Technical changes are always heralded by certain ideologies, just as the invincible Marxism leads to the prosperity of proletarian revolutions.
Cloud native is an approach to building and running applications. It is a set of systematized techniques and methodologies. Cloud native is a compound word made up of “cloud” and “native”. The word “cloud” represents applications residing in the cloud instead of in traditional data centers. The word “native” represents applications that are designed to run on the cloud and fully utilize the elasticity and the “distributed” advantage at the very beginning of application design.
The cloud native concept was first put forward by Matt Stine at Pivotal in 2013. In 2015 when cloud native was just becoming popular, Matt Stine defined several characteristics of cloud-native architectures in the book Migrating to Cloud Native Application Architectures: 12-factor applications, microservices, self-service agile infrastructure, API-based collaboration, and anti-fragility. At an InfoQ interview in 2017, Matt Stine made some changes and indicated six characteristics of cloud-native architectures: modularity, observability, deployability, testability, replaceability, and handleability. The latest description about cloud-native architectures on the official Pivotal website shows four key characteristics: DevOps, continuous delivery, microservices, and containers.
In 2015, the Cloud Native Computing Foundation (CNCF) was founded. CNCF originally defined four characteristics of cloud-native architectures: containerized encapsulation, automated management, and microservices. In 2018, CNCF updated the definition of cloud-native architectures with two new features: service meshes and declarative APIs.
As we can see, different individuals and organizations have different definitions for cloud-native architectures, and even the same individual or organization has different definitions for cloud-native architectures at different points in time. This complexity makes it hard for me to clearly understand cloud-native architectures. After a while, I came up with a simple solution: to choose only one definition that is easy to remember and understand (in my case, DevOps, continuous delivery, microservices, and containers).
In a word, cloud-native applications are required to meet the following: Implement containerization by using open-source stacks like K8s and Docker, improve flexibility and maintainability based on microservices architectures, adopt agile methods, allow DevOps to support continuous iteration and automated O&M, and implement elastic scaling, dynamic scheduling, and efficient resource usage optimization by using cloud platform facilities.
Cloud native supports simple and fast application building, easy deployment, and allows applications to be scaled as needed. Cloud-native architectures bring many advantages over traditional web frameworks and IT models, and have almost no disadvantages. Cloud-native architectures are definitely a powerful secret weapon in this industry.
Microservices: Almost all the definitions of the cloud native concept include microservices. Microservices are opposite to monolith applications and based on Conway’s law, which defines how to split services and is not easy to understand. In fact, I think that any theories or laws are not simple and easy to understand, otherwise they would not sound professional as theories or laws are. The main point is that system architectures determine product forms. I am not sure if this also results from Marx’s view on the relationship between the productive forces and relations of production.
In a microservices architecture, after split by function, services have stronger decoupling and cohesion and therefore become easier. It is said that DDD is another technique to split services. Unfortunately, I don’t know much about DDD.
Containers: Docker is the most widely used container engine. For example, it is used a lot in infrastructures of companies like Cisco and Google. Docker utilizes LXC. Containers provide guarantees to implement microservices and play the role of isolating applications. K8s is a container orchestration system built by Google to manage containers and balance loads between containers. Both Docker and K8s are developed in the Go language and are really good systems.
DevOps: DevOps is a clipped compound of “development” and “operations” and has a relationship different from that between development and production. In fact, DevOps also includes testing. DevOps is an agile thinking methodology, a communication culture, and an organizational form with the goal of enabling continuous delivery for cloud-native applications.
Continuous delivery: Continuous delivery enables undelayed development and updates without downtime and is different from the traditional waterfall development model. Continuous delivery requires the co-existence of development versions and stable versions. This needs many support processes and tools.
First, cloud native is a result of cloud computing. Cloud native would not have come into existence without the development of cloud computing. Cloud computing is the basis of cloud native.
With the growing maturity of virtualization technologies and the popularity of distributed frameworks, it is now an irreversible trend to migrate applications to the cloud. This trend is also driven by open-source communities like container technologies, continuous delivery, and orchestration systems as well as development ideas like microservices.
The three layers in cloud computing, which are IaaS, PaaS, and SaaS, provide a technical base and directional guidance for cloud native. True cloudification does not only involve changes in infrastructures and platforms, but also requires proper changes in applications. Applications are required to abandon traditional methods and be re-designed based on the characteristics cloud in different phases and aspects of architecture design, development models, deployment, and maintenance. This allows us to create new cloud-based applications, that is, cloud-native applications.
It is obvious that we need new and cloud-native development to implement cloud-native applications. Cloud native includes many aspects: infrastructure services, virtualization, containerization, container orchestration, and microservices. Fortunately, open-source communities have made many significant contributions to cloud-native applications, and many open-source frameworks and facilities are directly available. After released in 2013, Docker quickly becomes an actual container standard. Released in 2017, k8s stands out among many container orchestration systems. These technologies have significantly reduced the threshold of developing cloud-native applications.
Although the cloud native introduction document may seemingly show a trace of exaggeration, as nitpicky as I am, I feel totally amazed at the advantages listed in the document. Cloud-native architectures are perfect. Does this mean that applications should immediately switch to cloud-native architectures? The ideal is perfect and tempting, until you try to switch the reality to that ideal. My view on this is to make a decision based on actual needs and consider if the current problems really affect your business development and if you can afford to re-design your applications.
Software design comes with two critical goals: high cohesion and low coupling. These two critical goals further lead to many specific design principles, including the single responsibility principle, the open–closed principle, Liskov substitution, dependency inversion, interface segregation, and least knowledge.
Software engineers have always been striving to achieve the two goals and write clearer, more robust software that is also easier to scale and maintain.
Later, more needs are added. Software development is expected to be simpler and faster. Programmers want to write fewer lines of code, and non-professionals also want the ability to develop applications. Easier programming languages are developed for people who do not have programming skills. More programming technologies and ideas are developed, including libraries, components, and cloud infrastructures.
As a result, many technologies are of less practical value, although they themselves are highly advanced. Many software engineers have new roles as parameter adjustment engineers, API call experts, library masters, and component specialists. This is an inevitable result from efficient division of labor and technological development.
From nearly 20 years of Internet technological development, we can see that the mainstream trend is the application of technologies in specific fields. Especially with the development and popularity of cloud computing in recent years, infrastructures have become more solid and business development is increasingly easier and has less technology requirements. At the same time, small enterprises and teams are no longer plagued by problems related to aspects like performance, loads, security, and scalability. This situation worries many middle-aged people working in the Internet industry, who may feel as if they could soon be out of a job.
Although it is undeniable that the technology is becoming a less important threshold in this industry, we do not have to be that pessimistic. Similar arguments also occurred when VB, Delphi, and MFC appeared in the PC programming era: What you see is what you can use and you can develop PC programs simply by clicking your mouse. Isn’t this cool? At the time, programmers probably have more concerns. However, after the division of the backend development with the development of the Internet industry, they soon found their new battleground came up with many new ideas about networking, distributed services, databases, support for large amounts of services, and disaster recovery.
If the basic infrastructure of the PC programming era is the control library and the infrastructure of the Internet era is the cloud, what about the infrastructure of the AI era? What high-end technologies will emerge in the AI era?
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
#BlackLivesMatter
4 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
4 claps
4 
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/kelsey-hightower-on-keeping-up-with-cloud-native-and-helping-companies-make-smart-choices-19bd414f7021?source=search_post---------159,"In this episode of the ARCHITECHT Show, Kelsey Hightower — Google staff engineer, senior developer advocate and all-around cloud-native superstar — talks about his journey into cloud-native computing techniques and how he uses what he’s learned to help companies make the right decisions for their needs. Among many other topics, Hightower gives his thoughts on open source startups, Kubernetes, serverless computing, and the importance of spending time with technologies (even competitive ones) before offering opinions on them.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
8 
8 claps
8 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/building-the-open-data-stack/why-kubernetes-is-the-best-technology-for-running-a-cloud-native-database-d35ff842e6c5?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
Jun 28, 2021·6 min read
By Jeff Carpenter, Developer Advocate, DataStax
We’ve been talking about migrating workloads to the cloud for a long time, but a look at the application portfolios of many IT organizations demonstrates that there’s still a lot of work to be done. In many cases, challenges with persisting and moving data in clouds continue to be the key limiting factor slowing cloud adoption, despite the fact that databases in the cloud have been available for years.
For this reason, there has been a surge of recent interest in data infrastructure that is designed to take maximum advantage of the benefits that cloud computing provides. A cloud-native database is one that achieves the goals of scalability, elasticity, resiliency, observability, and automation; the K8ssandra project is a great example. It packages Apache Cassandra and supporting tools into a production-ready Kubernetes deployment.
This raises an interesting question: must a database run on Kubernetes to be considered cloud-native? While Kubernetes was originally designed for stateless workloads, recent improvements in Kubernetes such as StatefulSets and persistent volumes have made it possible to run stateful workloads as well. Even longtime DevOps practitioners skeptical of running databases on Kubernetes are beginning to come around, and best practices are starting to emerge.
But of course grudging acceptance of running databases on Kubernetes is not our goal. If we’re not pushing for greater maturity in cloud-native databases, we’re missing a big opportunity. To make databases the most “cloud-native” they can be, we need to embrace everything that Kubernetes has to offer. A truly cloud-native approach means adopting key elements of the Kubernetes design paradigm. A cloud-native database must be one that can run effectively on Kubernetes. Let’s explore a few Kubernetes design principles that point the way.
One of keys to the success of cloud computing is the commoditization of compute, networking, and storage as resources we can provision via simple APIs. Consider this sampling of AWS services:
Kubernetes offers its own APIs to provide similar services for a world of containerized applications:
Kubernetes resources promote portability of applications across Kubernetes distributions and service providers. What does this mean for databases? They are simply applications that leverage compute, networking, and storage resources to provide the services of data persistence and retrieval:
Thinking of databases in terms of their compute, network, and storage needs removes much of the complexity involved in deployment on Kubernetes.
Kubernetes promotes the separation of control and data planes. The Kubernetes API server is the key data plane interface used to request computing resources, while the control plane manages the details of mapping those requests onto an underlying IaaS platform.
We can apply this same pattern to databases. For example, Cassandra’s data plane consists of the port exposed by each node for clients to access Cassandra Query Language (CQL) and the port used for internode communication. The control plane includes the Java Management Extensions (JMX) interface provided by each Cassandra node. Although JMX is a standard that’s showing its age and has had some security vulnerabilities, it’s a relatively simple task to take a more cloud-native approach. In K8ssandra, Cassandra is deployed in a custom container image that adds a RESTful management API, bypassing the JMX interface.
The remainder of the control plane consists of logic that leverages the Management API to manage Cassandra nodes. This is implemented via the Kubernetes operator pattern. Operators define custom resources and provide control loops that observe the state of those resources and take actions to move them toward a desired state, helping extend Kubernetes with domain-specific logic.
The K8ssandra project uses cass-operator to automate Cassandra operations. Cass-operator defines a “CassandraDatatcenter” custom resource (CRD) to represent each top-level failure domain of a Cassandra cluster. This builds a higher-level abstraction based on Stateful Sets and Persistent Volumes.
A sample K8ssandra deployment including Apache Cassandra and cass-operator.
The three pillars of observable systems are logging, metrics, and tracing. Kubernetes provides a great starting point by exposing the logs of each container to third-party log aggregation solutions. Metrics and tracing require a bit more effort to implement, but there are multiple solutions available.
The K8ssandra project supports metrics collection using the kube-prometheus-stack. The Metrics Collector for Apache Cassandra (MCAC) is deployed as an agent on each Cassandra node, providing a dedicated metrics endpoint. A ServiceMonitor from the kube-prometheus-stack pulls metrics from each agent and stores them in Prometheus for use by Grafana or other visualization and analysis tools.
Kubernetes networking is secure by default: ports must be explicitly exposed in order to be accessed externally to a pod. This sets a useful precedent for database deployment, forcing us to think carefully about how each control plane and data plane interface will be exposed, and which interfaces should be exposed via a Kubernetes Service.
In Kassandra, CQL access is exposed as a service for each CassandraDatacenter resource, while APIs for management and metrics are accessed for individual Cassandra nodes by cass-operator and the Prometheus Service Monitor, respectively.
Kubernetes also provides facilities for secret management, including sharing encryption keys and configuring administrative accounts. K8ssandra deployments replace Cassandra’s default administrator account with a new administrator username and password.
In the Kubernetes declarative approach, you specify the desired state of resources and controllers manipulate the underlying infrastructure in order to achieve that state. Cass-operator enables you to specify the desired number of nodes in a cluster, and manages the details of placing new nodes to scale up, and selecting which nodes to remove to scale down.
The next generation of operators should enable us to specify rules for stored data size, number of transactions per second, or both. Perhaps we’ll be able to specify maximum and minimum cluster sizes, and when to move less-frequently used data to object storage.
Hopefully I’ve convinced you that Kubernetes is a great source of best practices for cloud-native database implementations, and the innovation continues. Solutions for federating Kubernetes clusters are still maturing, but will soon make it much simpler to manage multi-data center Cassandra clusters in Kubernetes. In the Cassandra community, we can work to make extensions for management and metrics a part of the core Apache project so that Cassandra is more naturally cloud-native for everyone, right out of the box.
If you’re excited at the prospect of cloud-native databases on Kubernetes, you’re not alone. A group of like-minded individuals and organizations has assembled as the Data on Kubernetes Community, which has hosted over 50 meetups in multiple languages since its inception last year. We’re grateful to MayaData for helping to start this community, and are excited to announce that DataStax has joined as a co-sponsor of the DoKC.
In more great news, the DoKC was accepted as an official CNCF community group, and hosted the first ever Data on Kubernetes Day as part of Kubecon/CloudNativeCon Europe on May 3. Rick Vasquez’s talk, “A Call for DBMS to Modernize on Kubernetes,” lays down a challenge to make the architectural changes required to become truly cloud native. Together, we’ll arrive at the best solutions through collaboration in open source communities like Kubernetes, Data on Kubernetes, Apache Cassandra, and K8ssandra. Let’s lead with code and keep talking!
This article originally appeared in Hacker Noon.
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
13 
13 
13 
We’re huge believers in modern, cloud native technologies like Kubernetes; we are making Cassandra ready for millions of developers through simple APIs; and we are committed to delivering the industry’s first and only open, multi-cloud serverless database: DataStax Astra DB.
"
https://itnext.io/creating-the-microservice-application-with-the-cloud-native-stack-111e7d19c5e8?source=search_post---------161,
https://medium.com/@jaychapel/should-you-use-the-cloud-native-instance-scheduler-tools-ab3d00b7b160?source=search_post---------162,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 15, 2019·4 min read
When adopting or optimizing your public cloud use, it’s important to eliminate wasted spend from idle resources — which is why you need to include an instance scheduler in your plan. An instance scheduler ensures that non-production resources — those used for development, staging, testing, and QA — are stopped when they’re not being used, so you aren’t charged for compute time you’re not actually using.
AWS, Azure, and Google Cloud each offer an instance scheduler option. Will these fit your needs — or will you need something more robust? Let’s take a look at the offerings and see the benefits and drawbacks of each.
AWS has a solution called the AWS Instance Scheduler. AWS provides a CloudFormation template that deploys all the infrastructure needed to schedule EC2 and RDS instances. This infrastructure includes DynamoDB tables, Lambda functions, and CloudWatch alarms and metrics, and relies on tagging of instances to shut down and turn on the resources.
The AWS Instance scheduler is fairly robust in that it allows you to have multiple schedules, override those schedules, connect to other AWS accounts, temporarily resize instances, and manage both EC2 instances and RDS databases. However, that management is done exclusively through editing DynamoDB table entries, which is not the most user-friendly experience. All of those settings in DynamoDB are applied via instance tags, which is good if your organization is tag-savvy, but can be a problem if not all users have access to change tags.
If you will have multiple users adding and updating schedules, the Instance Scheduler does not provide good auditing or multi-user capabilities. You’ll want to strongly consider an alternative.
Microsoft has a feature called Azure Automation, which includes multiple solutions for VM management. One of those solutions is “Start/Stop VMs during off-hours”, which deploys runbooks, schedules, and log analytics in your Azure subscription for managing instances. Configuration is done in the runbook parameters and variables, and email notifications can be sent for each schedule.
This solution steps you through the setup for timing of start and stop, along with email configuration and the target VMs. However, multiple schedules require multiple deployments of the solution, and connecting to additional Azure subscriptions requires even more deployments. They do include the ability to order or sequence your start/stop, which can be very helpful for multi-component applications, but there’s no option for temporary overrides and no UI for self-service management. One really nice feature is the ability to recognize when instances are idle, and automatically stop them after a set time period, which the other tools don’t provide.
Google also has packaged some of their Cloud components together into a Google Cloud Scheduler. This includes usage of Google Cloud Functions for running the scripts, Google Cloud Pub/Sub messages for driving the actions, and Google Cloud Scheduler Jobs to actually kick-off the start and stop for the VMs. Unlike AWS and Azure, this requires individual setup (instead of being packaged into a deployment), but the documentation takes you step-by-step through the process.
Google Cloud Scheduler relies on instance names instead of tags by default, though the functions are all made available for you to modify as you need. The settings are all built into those functions, which makes updating or modifying much more complicated than the other services. There’s also no real UI available, and the out-of-the-box experience is fairly limited in scope.
Each of the instance scheduler tools provided by the cloud providers has a few limitations. One possible dealbreaker is that none of these tools are multi-cloud capable, so if your organization uses multiple public clouds then you may need to go for a third-party tool. They also don’t provide a self-service UI, built-in RBAC capabilities, Single Sign-On, or reporting capabilities. When it comes to cost, all of these tools are “free”, but you end up paying for the deployed infrastructure and services that are used, so the cost can be very hard to pin down.
We built ParkMyCloud to solve the instance scheduler problem (now with rightsizing too). Here’s how the functionality stacks up against the cloud-native options:
Overall, the cloud-native instance scheduler tools can help you get started on your cost-saving journey, but may not fulfill your longer-term requirements due to their limitations.
Try ParkMyCloud with a free trial — we think you’ll find that it meets your needs in the long run.
Originally published at www.parkmycloud.com on January 10, 2019.
CEO of ParkMyCloud
See all (317)
9 
9 claps
9 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://blog.getambassador.io/a-leadership-view-on-cloud-native-development-focus-on-uptime-collaboration-and-developer-987bc66ed6a8?source=search_post---------163,
https://blog.getambassador.io/why-cloud-native-b78fa8607310?source=search_post---------164,
https://medium.com/@mohamed-ahmed/cloud-native-logging-and-monitoring-pattern-f9fc6fa0a97e?source=search_post---------165,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Ahmed
Dec 23, 2019·9 min read
This article was originally published at https://www.magalix.com/blog/cloud-native-logging-and-monitoring-pattern
Any system that runs without exposing the necessary information about its health, the status of the applications running inside it, and whether or not something has gone wrong is simply useless. But this has been said many times before, right? Since the early ages of mainframes and punch-card programming, there have always been gauges for monitoring the system’s temperature and power among other things.
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/the-graphic-guide-to-evolving-cloud-native-deployments-41bfce721fb6?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
This blog post was inspired by a talk given by Timo Derstappen, Giant Swarm Co-Founder and CTO.
Kubernetes was invented in order to make developers’ lives simple. Indeed, getting started with Kubernetes is as easy as installing Minikube, connecting to kubectl and you’re ready to go. Sure, this is the case if you are working on a pet project, but not if you want to take it into production and run Day 2 operations.
You’ll have multiple teams focused on their objectives and doing their best to get there. As it often happens in enterprises, these teams aren’t necessarily heading in the same direction at the same pace.
The change from an easy Kubernetes installation to a gridlocked environment happens gradually as you mature in your cloud-native journey.
Your ‘Day 1’ installation looks something like this:
For ‘Day 2’ and beyond you’ll find that you have started building a cloud-native stack.
The result will look something like this:
It’s bigger but it’s still manageable.
The thing is, best practice indicates the need for multiple environments to build robust software. So suddenly you need duplicates of the stack for different environments.
You’ll find it looks something like this:
Truth be told, this is just for a single development team. In cases where you have multiple development teams, the solution is copying this per team or department.
It’ll then look something like this:
But what about regulation, cost, high-availability, latency, and resilience? These considerations may push you towards a multi-region or even a multi-cloud solution.
As a result, you’ll end up with something like this:
So bit by bit, on your cloud-native journey, you discover that the business agility you are aiming for requires you to take care of multiple environments.
All of this is just to keep your teams independent when it comes to finding the best time to do an upgrade or a launch, etc. The problem is that all of this complexity needs to be managed.
Tools and teams to manage this are a solution. At this point, some people in your organization may start wondering if infrastructure has become your main business. We have a better solution.
A solution where we run your infrastructure, so you can run your business.
Giant Swarm is a team of highly experienced individuals. Our mission is to create and run the best platform for you. We run hundreds of clusters and are continually improving the platform on which they run.
Running so many clusters in production gives us an advantage. Once we identify a problem, we implement the solution to all of our customers. Regardless of whether they discovered this problem (yet) or not.
It’s not just latent knowledge I am talking about. Knowledge is continuously codified and implemented into our product. This is based on operators. These operators, built with Kubernetes, continually check the desired state of each cluster. They are a central tool for keeping the infrastructure in the desired state.
Less manual work = less human error
The bottom line is that complexity is not going away. Complexity is just shifting towards infrastructure to make developers faster. Building your own platform will just make you slow. If you would like to achieve business velocity, we can help by running your infrastructure for you.
Get in touch with a cloud-native expert today
Written by Oshrat Nir — Product Marketing Manager @Giant Swarm
twitter.com
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬.
To join our community Slack team chat 🗣️ read our weekly Faun topics 🗞️, and connect with the community 📣 click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
4 
4 claps
4 
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibabatech/what-is-real-cloud-native-967cfe8520f1?source=search_post---------167,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Feb 17, 2021·7 min read
Jiang Jiangwei, Senior Researcher at Alibaba Cloud Intelligence
“Cloud Native” has become such a buzzword that if you don’t talk about it, you are considered as being outdated or obsolete. Thus, cloud vendors increasingly see the value and technological benefit in joining CNCF.
There are several definitions of Cloud Native, such as “microservice + container + continuous delivery + DevOps”, as defined by CNCF, and other definitions by different cloud vendors. Alibaba Cloud established the Cloud Native Technology Committee in September 2020. Today, I would like to talk about what is real Cloud Native starting from the original aspiration of cloud computing.
Let’s first review the origin aspiration of Cloud Native.
With the rise of cloud computing, a container wave, represented by Docker, swept across the industry, and was widely favored by DevOps and standardized delivery. Google then entered this field and proposed the innovative concept of Cloud Native from the perspective of unified deployment and standardization, and then released the open-source container-orchestration tool — Kubernetes. Later, Google established the CNCF Foundation and successfully gained a strong foothold in this industry.
In December 2020, CNCF’s ambassador Ian Coldwater posted a message on social media: “Kubernetes has now deprecated Docker support. You need to keep this in mind and plan accordingly as this may have an adverse effect on your cluster.”
This was a historic step in the Cloud Native strategy, and shows that equating Cloud Native with containers and service mesh was a narrow view.
So, what is Cloud Native in a broad sense? The real Cloud Native is the software, hardware, and architecture born from the cloud.
We believe that Cloud Native should be viewed from the perspective of customer applications. Applications deployed on the cloud must use one or more of the three types of capabilities that only large-scale public clouds could provide, namely elasticity and API automated deployment, operation, and maintenance, service-oriented Cloud Native products such as RDS and EMR, and software and hardware integrated architecture born on the cloud. This is Cloud Native.
First, Cloud Native applications are compared to the offline traditional enterprise physical IT equipment environment. They need to be deployed and managed in the cloud and should take full advantage of the supply efficiency of the cloud. Furthermore, they should be deployed globally, available instantly, and charged on a pay-as-you-go model.
The ability of customers to access the cloud in their vicinity is called elasticity. We published a book last year — Elastic Computing the Ubiquitous Computing Capability, which elaborates the underlying principles. It is possible to be Cloud Native only when such extensive elasticity exists. Kubernetes on the cloud is Cloud Native due to its API standardization of automated deployment, operation and maintenance.
Secondly, the concepts that are popular nowadays such as Cloud Native database, Cloud Native Big Data, Cloud Native container, Cloud Native middleware, and Cloud Native security, are all service-oriented Cloud Native products that are easily accessible on the cloud. These services, which are not available in the traditional offline model, can help enhance performance and reduce costs. This is Cloud Native.
The virtualization of integrated software and hardware represented by the X-Dragon architecture, is worth mentioning. The field of virtualization has experienced pure software virtualization, general-purpose hardware virtualization, and now integrated software and hardware virtualization.
By accelerating virtualization via the separation of computing and storage and the use of customized X-Dragon chips, performance higher than physical servers can be achieved. Based on this bottom layer of the cloud computing architecture, all middleware and application layers can gain performance-wise bonus without any code and architecture modification.
In addition, since the X-Dragon architecture supports unified architecture resource pooling, including ECI (Serverless Container), VM, bare metal, etc., all resources can be uniformly scheduled to achieve in-depth elasticity. This is also the first time that the industry has implemented unified architecture and resource scheduling, which is considered as the major technological contribution of Chinese cloud vendors.
In this sense, the X-Dragon architecture is born out of the cloud and is typically Cloud Native. Simply put, the advantages of X-Dragon cannot be fully utilized without a large-scale cloud computing scenario deployment.
Therefore, the original aspiration of Cloud Native is to be native on the cloud. If it lacks the basic characteristics of the cloud, it is not Cloud Native. To be extreme, if you bought two servers, placed them in your office, deployed containers on them, and then used Kubernetes to manage them, it is not Cloud Native and is not even cloud, as it lacks elasticity and automated deployment, operation, and maintenance capability by API.
CNCF is more about advocating that a system should use different components in its architecture to achieve internet architecture, to further achieve rapid iteration, flexible expansion, and efficient operation and maintenance. The utilization of these capabilities requires that the architecture framework is based on cloud, and a combination of cloud-based architecture framework and cloud itself is Cloud Native.
Many people may ask if a specific open source software is Cloud Native or not. That depends on its architecture. Take an open source database as an example, if its architecture still targets at traditional stand-alone and small-scale deployments, then it is not Cloud Native. If we maintain its interface compatibility with open source projects, make Cloud Native transformation and redesign, and fully utilize the basic capabilities of cloud in operation, maintenance, and control, then such software and services are Cloud Native, for instance, Alibaba Cloud’s PolarDB database.
3. Multi-cloud and hybrid cloud have accelerated Cloud Native
History is full of ups and downs, although seeing comes from believing, the process is full of complications. When you are driving on a winding road, often you only have a limited field of view.
The industry has developed multi-cloud and hybrid cloud to achieve higher cost effictiveness, fault tolerance, and efficiency. Multi-cloud means that customers deploy their workloads on different public clouds, while hybrid cloud means that customers deploy their workloads across public and private clouds.
Multi-cloud is a result of competition, although Amazon and Alibaba Cloud believe that one cloud vendor is sufficient for users. Whereas, hybrid cloud emerges for different reason. It is a cloud architecture built to satisfy customers’ desire to gradually enter the cloud and to meet the regulatory compliance needs in some industries.
No matter it is multi-cloud or hybrid cloud, cloud vendors will push customers to make unit transformation of their business, to reduce the difficulty of distributed deployment and increase customer acceptance, because that the Kubernetes has solved the problem of standardized deployment and migration.
In addition to Kubernetes and containers, customer’s business often involves elasticity, acceleration, and other service capabilities provided by public cloud, as well as the Cloud Native transformation of databases, Big Data, and audio & video systems. From this perspective, Kubernetes has accelerated the capabilities of narrow sense Cloud Native.
Cloud computing has generated new application scenarios and demands for CPUs, that is, Cloud Native chips.
For example, Alibaba Cloud’s X-Dragon chip is a Cloud Native chip. It is the core of the entire device as it enables the X-Dragon server to have full virtual machine characteristics and interfaces.
The MOC card based on the X-Dragon chip plays an important role in the entire architecture. With a wealth of management interfaces and external data interfaces, it can help us realize the functions of the elastic bare metal servers.
Cloud Native CPUs show some common characteristics: the main frequency is not necessarily high, the power consumption is extremely low, the cores run independently, the multi-core architecture can better support service mesh applications, and there is a mature software ecology.
5. Use Cloud Native thinking to optimally utilize Cloud Native architecture
It is important for cloud computing users to understand the meaning of Cloud Native. Rather than simply using containers to define your own system, using the real Cloud Native system in the design, development, and deployment cycles, will lead to huge competitive advantages and ensure you enjoy the bonus of cloud computing.
Much more than practice, Cloud Native is a mindset which will lead to transformation of different eras. Development environment of engineers should be born and thrive on the cloud. Tools like Wuying will enable the full life cycle of software development to be accomplished solely on the cloud, including product design document, development and debugging, deployment tools, test, POC validation and resource consumption etc.
Therefore, Cloud Native brings about the change in mindset and culture, and the productivity of the new era, far beyond the standardized Kubernetes interface that CNCF defined. This is the standard way of using cloud in the future.
We are extremely lucky to experience a historic milestone of entering the cloud era.
Our future generations will be even more fortunate, as all the computing environments that they will touch, will be pure Cloud Native. They would live in a world where cloud computing resources are ubiquitous and inexhaustible, and they won’t care about where the cloud resources come from and how many resources exist, just like when we use tap water, we don’t bother thinking about where the water comes from.
Real Cloud Native is the software, hardware, and architecture born on the cloud. Technology born on the cloud is Cloud Native technology.
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
9 
9 
9 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/criteo-engineering/cloud-native-2019-took-place-in-london-and-gather-almost-200-hundred-cloud-practitioners-cc203965ea02?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Cloud Native 2019 took place in London and gather almost 200 hundred cloud practitioners. It has been an opportunity to exchange point of view on various topics, to extensively discuss testing in this context, and to share feedback on such technologies.
This question was the topic of the opening keynote. It was also a question we have been asked many times at our booth. The general consensus was that nowadays it makes more sense to use a public cloud (AWS, Google Cloud Platform, … ). There are some good reasons to be reluctant to use a public cloud (like the collection of sensitive data). But there are also several injustified reasons. For instance, several speakers mentioned the fear be locked-in within a cloud provider and then to experience a price increase. They dismissed this argument based on price evolution data from the last decade. Yet, this data shows that the situation never occured.. And there are several good reasons to use one. In particular not having to maintain a stack of technos not directly related to one’s business -and the subsequent security risks- as well as the velocity it brings to be able to benefit from an actual prod-proof cloud.
We had several questions on that matter since we’re managing our own private cloud:roughly 50.000 servers and several dozens SRE engineers to maintain it. Actually Criteo is a bit older than AWS so when we started, developing our private cloud was the only option. We started with bare metal and, as we grew, developed expertise on developing and maintaining our private cloud. Now, considering our scale, tooling and expertise, having reached a high level of optimization, it turns out to be cheaper and more efficient to manage our own infrastructure.. Yet we are open to other approaches, and we also use a public cloud for some use cases that make no sense to internalize, for example this is the case of some specific tests and caches.
At Criteo, performance is everything. It implies working smart with the best infra we can afford. It is sometimes public cloud, but as of today we’re mainly focused on our own private cloud.
Interestingly several talks broached this topic at some point. The cloud brings agility but if the testing strategy doesn’t evolve it can hinder this benefit. Also, with microservices new difficulties arise. For instance, we don’t want developers to have to spawn dozens of services on their own computers just to be able to run the regression tests locally.Several aspects were discussed, in particular:- consumer-driver contract tests, to be able to test a service without having to spawn the whole infra-structure- testing in prod by first deploying on a trafic-less prod server -after all the usual regression tests in pre-production have been done- in order to be able to test the integration with the other services in conditions… as close to prod as possible! A little regret on that though: the duration of this talk was a bit short and we didn’t have much time to talk about handling side effects in this context.- testing issues we could never have imagined could occur in prod, thanks to Disaster Recovery Testing games, chaos tests, and a solid monitoring
My team, Test Services is in the process of redesigning the way we’re doing end to end tests at Criteo. Historically we developed sandboxes: an isolated environment used to run tests in our build pipeline. Those environments replicate a lightweight Criteo datacenter on a few servers. It was quite useful back in the days when we had only a few services but it turned out to be more and more complex. And more and more painful to maintain. We’ve hence been in the past quarters, in a process to re-thinking the way we’re tackling this. Those talks have hence been an opportunity to step back on what we’re doing -and to give us new ideas to go one step farther.
One of our Criteo’s was also doing a Lightning Talk: “Discovery, Consul and Inversion of Control for the infrastructure” (slides are available here). During this talk, he explained how infrastructure is evolving every day faster and how mixing several kinds of architectures (real datacenter, containers and various Cloud providers) is hard to handle to an infrastructure point of view since everything is moving faster and faster.He also explained how Criteo implemented a new pattern of architecture for infrastructure called inversion of Control. Which such pattern, infrastructure becomes a live database of all workloads and by enriching the running services, so it becomes possible to decouple all tools, innovate faster and create additional value very easily to match business needs.
Criteo is using this pattern to perform all of its load-balancing, generated automatically alerts, track versions at large scale and with vendor-independent schedulers, meaning hybrid cloud/containers/legacy systems.
Being able to structure your data, bring semantics to infrastructure is probably one of the big challenges of next few years in the era of microservices and Criteo is pioneering it with interesting success.
Long story short: Cloud Native 2019 was a quite interesting place to be.See you next year!
Authors: Guillaume Turri, Pierre Souchay and Clément Boone
Tech stories from the R&D team
4 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
4 claps
4 
Written by
The engineers building the advertising platform for the open Internet.
Tech stories from the R&D team
Written by
The engineers building the advertising platform for the open Internet.
Tech stories from the R&D team
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-boosts-full-cloud-based-development-and-practices-dae4c2e1db42?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 19, 2021·10 min read
Today, thousands of industries are embracing cloud computing and cloud-native for digital innovation and upgrade, which enriches cloud-native and allows us to redefine it today. Cloud-native technologies help developers build loosely coupled systems that are scalable, easy to manage, observable with good fault tolerance. Technologies, such as Kubernetes, Container, DevOps, Microservice, Service Mesh, and Serverless, are a collection of application-layer technologies. The traditional advantage of cloud computing is the pooling of resources, and it will bring about elastic distribution and API-based automatic management capabilities. Some say only the combination of cloud-native and cloud computing can truly be powerful.
The combination of cloud-native technologies and cloud computing are the cloud-native products. Today’s cloud platforms provide a large number of cloud-native products, including big data, databases, container service, middleware, application PaaS, cloud-native security, developer tools, audio/video services, and ECS Bare Metal instances. Products, software, hardware, technology, and architecture deriving from the cloud are the true cloud-native.
Today, we believe cloud-native is another upgrade to cloud computing. For the cloud platform, the technologies represented by containers have become a new service community in cloud computing. It is developer-oriented and can encapsulate infrastructure downwards and shield differentiation from heterogeneous environments.
Let’s use Alibaba Cloud Container Service for Kubernetes (ACK) as an example. It can encapsulate 30 cloud products downwards for easy-to-use interfaces. It can also encapsulate over 30 cloud products upwards, supporting heterogeneous loads and architectures. For enterprises, cloud-native is accelerating their digital innovation in the direction of cloud-based infrastructure, Internet-based core technologies, modernization of application architectures, and data-driven and intelligent business to help enterprises innovate their business.
Today, cloud-native has become the shortest path and cornerstone for enterprises’ digital innovation. For developers, cloud-native is reshaping the software lifecycle. On the one hand, it implements optimization downwards to achieve software-hardware collaborative optimization, which reduces technical costs and improves technical efficiency. On the other hand, it supports multiple workloads upwards to enable more fine features to architectures. If you are concerned about cloud-native, CNCF already has hundreds of projects covering a range of things, including application development to specific development frameworks, IDE, CI/CD, entire project releases and launching, changes to O&M capacity management, and the upgrade of the overall monitoring. Cloud-native provides a new open standard solution for the entire lifecycle. We believe cloud-native has enabled the cloud development era.
Cloud-Native propels development mode innovation, providing developers with some advantages in the following terms.
1) Architecture: The cloud-native development model is a modular architecture that communicates through standardized interfaces and protocols.
2) Application Delivery and Update: Continuous and automated iteration, integration, and delivery are supported.
3) O&M: Its O&M mode is standardized and automated.
4) Extensibility: Auto-scaling is supported as needed.
5) Dependency: Its fine portability means manufacturer locking won’t be a problem, as it has no dependency on the system environment and hardware.
6) Enterprise Organization and Culture: It has smooth cross-functional communication and cooperation and a strong ability to cope with changes.
We believe cloud-native is driving a new development era, an era for developers.
Current industry research reports show that the use of containers is increasing continuously and rapidly. According to a CNCF survey, 68% of organizations and enterprises will use containers in their production environment in 2021, which is a 240% increase from two years ago. It’s fair to say that containers are ubiquitous. Market research shows that for frontend/backend development, web page/mobile/applets, logic/component/framework, the proportion of developers willing to develop on the cloud in 2021 also reached 68%. The use of Serverless also increased significantly. By the end of 2021, 25% of developers will begin to use Serverless technologies and products.
Alibaba Cloud has created a large number of products, technologies, and open-source projects to keep with the times and empower developers. Oriented to the entire technical community, Alibaba Cloud feeds back to the world’s top foundations with its technical achievements in cloud computing research and development, such as the OpenAtom Foundation and the Apache Foundation. All the efforts are made by Alibaba Cloud in hopes of creating an open, standard, healthy, and benign technology development ecology.
For microservice-oriented standards in China, Alibaba Cloud has incubated more than eight projects for the Cloud Native Foundation, such as the edge container-based platform OpenYurt, the chaos engineering tool in the field of distributed high availability ChaosBlade, the service registration and discovery tool Nacos, and other open-source projects. Any developer that wants to build an open-source architecture based on cloud-native and open-source technologies can find a solution.
Alibaba Cloud has served a large number of leading enterprise users, such as iQIYI, Huya Live and Ping An Technology. Alibaba Cloud also hopes to build an open and standard technology system to serve global developers. Alibaba Cloud ranks first in the contribution ranking of the open-source community GitHub, with over 2,600 open-source projects, over 30,000 contributors, and millions of followers.
The emergence of cloud-native technologies was first centered on resource management, not friendly enough to applications. Therefore, Alibaba Cloud and Microsoft jointly proposed the OAM open application model, a collaborative solution that makes the interfaces for developers, O&M personnel, and testers clear and standard. OAM provides a unified interface for application description and application delivery, a PaaS platform with various functions and strong integration capabilities, and the ability to manage and deliver applications in multiple environments and versions. Image downloads have surpassed 100,000, including over 20 enterprise users, such as Bytedance, 4Paradigm, and Youzan. Meanwhile, Alibaba Cloud also impels the standardization of OAM application management to become an industry standard. The OAM project was released by the CAICT as an industry standard in March 2021.
For application R&D and O&M, Alibaba Cloud provides the cloud-native all-in-one DevOps to make development and operation more efficient. The all-in-one tool platform covers the requirement management and the entire CI/CD change. It breaks the barriers of on-premises and cloud scenarios, enabling full cloud-based development to make the entire development more efficient. As shown in the figure above, the project management, requirement management, code repository, code management, image management, the release and testing of CI/CD testing, and the entire Developer Suite, including an external IDE, are all cloud-based development tool platforms.
An integrated platform with data and intelligence connects all the data in a link for full measurement to find the efficiency bottleneck in the entire production process for enterprises and developers to achieve valid optimization. Enterprise-level security assurance, seamless integration of cloud services, the integration of application management for Apsara DevOps products and ECS, ACK container service, and Function Compute are certified with the highest level of R&D capability by CAICT. Currently, DevOps has served 1,000,000 service developers and more than 100,000 enterprise customers.
Today, containers are a necessity for developers. Alibaba Cloud container services provide a wide range of infrastructure, including ACK, ASK, multi and hybrid cloud management, heterogeneous computing power scheduling, intelligent O&M systems, ASM, and container application market. It supports a wide range of architectures, such as microservices, stateful applications, big data intelligence applications, and innovative applications (blockchain IoT).
Based on this, Alibaba Cloud has formed technical product solutions for the industry, including microservice technology architecture solutions, cloud-native big data solutions, genetic computing solutions, DevOps solutions, X-Dragon container solutions combined with integration and optimization, together with hybrid cloud container management solutions. According to the Gartner report on public cloud container services, Alibaba Cloud has been the only Chinese enterprise selected for three consecutive years and has been rated as the world’s most complete cloud service provider for container products. Alibaba Cloud has served tens of thousands of enterprise customers and hundreds of thousands of enterprise developers.
With the development of cloud-native, the user interface of cloud computing is moving upwards, which leads to higher development efficiency. DevOps provides features, such as fully-hosted, O&M-free ultimate elasticity, and quick launch, which allows developers to focus more on the business logic. Today, Serverless has gradually become the mainstream technology in cloud computing and will become a major trend in the future.
The Serverless products provided by Alibaba Cloud formed a Serverless runtime pool based on Alibaba Cloud Serverless container 2.0, the third-generation X-Dragon architecture, Pangu storage, and the Luoshen network. It provides four features: function compute-oriented FC, application-oriented SAE, container orchestration-oriented ASK, and container instance-oriented ECI, in support of a wide range of application scenarios, including full-site and full-end development, applet development, audio/video development in online education, application packaging, data intelligence development, and mainstream microservice architectures.
Alibaba Cloud provides a complete set of development capabilities featuring the unification of development tools for developers, components, and the cloud. It also creates an application center and provides many experience optimizations, application templates, and classic case libraries, enabling more efficient development and better secondary development and innovation. At the same time, we also put Serverless into a white box, so we can understand what is going on in the technology stack and have better control over it. In the 2021 Forrester FaaS report, the Alibaba Cloud Serverless product capability ranked first in the world. According to a 2020 survey conducted by the CAICT for developers across China, Alibaba Cloud’s Serverless market share was 66%.
Alibaba Cloud built ARMS, an all-in-one application-oriented observable product, to make observability available for developers. It works at the infrastructure layer, container orchestration and scheduling layer, application architecture layer, application layer, and end-to-end testing experience layer to provide complete link metric analysis of log events, app monitoring capabilities, frontend monitoring capabilities, application monitoring, tracing analysis performance diagnosis, Prometheus monitoring, and cloud alert services. We hope productization can be output through the unified O&M and observability to realize automatic O&M. Alibaba Cloud has entered the 2021 Gartner’s APM Magic Quadrant as the only cloud vendor in China, serving tens of thousands of enterprise customers and hundreds of thousands of developers.
Alibaba Cloud has the most extensive cloud-native product families in China. It has nearly 300 cloud-native products and nearly 1,000 technical solutions, including the container layer, modern application architecture layer, aPaaS event-driven microservices, Serverless, cloud-native big data AI, and other product systems. Enterprises in the cloud-native era can build IT technology systems fully based on cloud products. Developers that want to enhance their values and create greater productivity can find a complete set of tools and product systems on Alibaba Cloud. Alibaba Cloud provides enterprises with five major technical values: resource elasticity, security and reliability, business intelligence, application agility, and system stability. It has become the technical backup for thousands of industries, including transportation, manufacturing, government affairs, media, Internet, finance, retail, and communications. It has also served over 80% of China’s technology companies, over 60% of A-share listed companies, more than 3,000,000 customers, and more than 5,000,000 developers from 200 countries and regions.
Cloud-native technologies are now calling for the cloud development era, making every developer a better developer. As a result of using cloud-native technologies, developers will create greater technical and commercial value to accelerate the digital economy development.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
31 
1
31 claps
31 
1
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/five-trends-in-cloud-native-de9f742b9df5?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 20, 2020·16 min read
By Li Xiang and Zhang Lei, Edited by Heyi
The future development trends in cloud-native can be summarized in five main points:
The core assumption of cloud-native is that the software of the future must be born and raised on the cloud. The concept of “cloud-native” defines the optimal approach to enable applications to make maximum use of cloud capabilities and the value of the cloud. Therefore, cloud-native is a set of ideas that guide the design of software architectures. First, cloud-native software must be born and mature on the cloud. Second, it must maximize the use of cloud capabilities. In this way, software and the cloud can be organically integrated to achieve the full potential of the cloud.
Cloud-native is familiar to many people in the industry. Many enterprises have implemented practices based on cloud-native architectures and technical concepts. Therefore, we must look at the future trends in the cloud-native field. This will show us how to adapt as cloud-native goes mainstream.
To this end, we reached out to Li Xiang, a senior technical expert from Alibaba’s cloud-native division, representative of the CNCF technical supervision committee, and an author of etcd, and Zhang Lei, a senior technical expert at Alibaba Cloud and co-chairman of the CNCF application delivery field. These experts will discuss the concept, development, and future trends of cloud-native, opening doors to new ideas and new horizons.
Their views are presented in the following article.
Kubernetes is a key project in cloud-native. The rapid development of Kubernetes has made it the cornerstone of the overall cloud-native system. Today, we will look at the development characteristics of Kubernetes. First, Kubernetes is ubiquitous. It is used in clouds, in user-built data centers, and will play a role in scenarios we haven’t imagined yet.
Second, all cloud-native users use Kubernetes to deliver and manage applications. Applications are a general concept. Applications can take the form of websites, large e-commerce platforms (like Taobao), AI jobs, computing tasks, functions, and virtual machines (VMs). In all these cases, users can use Kubernetes to deliver and manage applications.
Third, Kubernetes is currently seen as a connecting link. Kubernetes abstracts formatted data to expose infrastructure capabilities, such as service, ingress, pods, and deployment. These are the capabilities that Kubernetes native APIs expose to users. Currently, Kubernetes provides standard interfaces that allow users to access infrastructure capabilities, such as CNI, CSI, DevicePlugin, and CRD. This enables the cloud to act as a capability provider and integrate capabilities into the Kubernetes system through a standardized method.
Kubernetes plays a role similar to Android. Although Android is an operating system installed on devices, it can connect your hardware, mobile phone, TV, car, and other devices to a platform. Android provides a unified set of application management interfaces so you can program applications based on the Android system to access infrastructure capabilities. This is another similarity between Kubernetes and Android.
Finally, Kubernetes itself does not directly generate commercial value. Users do not directly purchase Kubernetes. This is also like Android, as you do not pay for the Android system itself. For both Android and Kubernetes, the real value lies in the upper-layer application ecosystem. Android already has a huge mobile or device application development ecosystem today. Kubernetes is similar, but it is in an earlier stage of its development. We can see that many of the business layers built on Kubernetes today are vertical solutions. It is user-oriented and application-oriented solutions that can really generate commercial value, not Kubernetes itself. That is why we say that Kubernetes’ development is similar to Android. Of course, this may also be due to the approach that Google has perfected: promote a free “operating system” and profit from the value of its upper-layer ecosystem, rather than from the operating system itself.
In light of this context, we can summarize the development trends of Kubernetes:
Users use Kubernetes to deliver and manage applications. If Kubernetes continues to grow, all data centers and infrastructure around the world will have a Kubernetes layer. Then, users will naturally start to program, deliver, and manage applications based on Kubernetes. This is similar to how we program mobile apps based on the Android operating system.
In this scenario, most software and cloud products on the cloud will be developed by third parties. Third-party development allows everyone to develop and deliver software on a standard interface. The software can be either proprietary software or cloud products. In the future, more and more third-party open-source projects, such as MongoDB and Elasticsearch, will be developed, deployed, and maintained cloud-natively. Ultimately, they will evolve into cloud services.
With the Kubernetes standard, developers are faced with an interface similar to an operating system. Many applications are designed for Kubernetes or delivered to Kubernetes. Therefore, a product similar to a “pea pod” is required. As a cloud-based app store or application distribution system, the pea pod can deliver applications to any Kubernetes system in the world without having to worry about compatibility. The principle is the same as using pea pods to deliver any Android application on any Android device.
Google has already tried out this type of product. One example is Anthos, an application delivery platform for hybrid clouds. Although this is a hybrid cloud product, it essentially works by delivering database services, big data services, and other Google Cloud services to any hybrid cloud environment based on Kubernetes. It is equivalent to a pea pod for clouds.
Since the entire application ecosystem of the future will be oriented to Kubernetes, open application platforms with Kubernetes-based scalability will gradually replace traditional PaaS as the mainstream approach. When you build an open application platform with Kubernetes-based scalability, its capabilities are pluggable, and it can deliver and manage various application types. This is more in line with Kubernetes trends and ecosystems. Therefore, we will see the emergence of many platform-layer projects with high scalability.
There is still a large gap between Kubernetes and an ideal cloud-native application ecosystem. This is what the Alibaba Cloud-Native team has been working on. Based on Kubernetes, we want to build a richer application ecosystem at the application layer to meet diverse requirements.
By observing the development of applications and cloud capabilities in the cloud-native era, you will find another trend: Operators. Operators are a concept in Kubernetes. They are basically entities delivered by Kubernetes. These entities have a basic model, which is divided into two parts: a Kubernetes API object (a custom resource definition or CRD for short) and a controller. This is shown in the following figure:
Here, we need to distinguish between two concepts: customization and automation. Many people believe they can use Operators for customization. When users need more than Kubernetes’ built-in capabilities, they use the scalability provided by Operators to write controllers that can meet their needs. However, customization is only a small part of what Operators do. The core motivation for today’s Operator-based applications and capabilities is automation. Moreover, only automation allows us to achieve cloud-native.
This is because the greatest benefit of cloud-native is that it allows us to efficiently use cloud capabilities to their full potential. This is something we cannot achieve manually. In other words, only through automated application development, O&M, and automated interaction with the cloud can the value of cloud-native be truly leveraged.
To automate interactions with the cloud, we must use a plug-in, such as a controller or an Operator, in the cloud-native ecosystem. Currently, the products Alibaba delivers on the cloud, such as PolarDB and OceanBase, use a controller connected to Kubernetes. The controller interacts with the infrastructure and cloud to embed cloud capabilities in these products.
In the future, a large number of cloud applications and their corresponding O&M and management capabilities will be delivered through Kubernetes Operators. In this context, Kubernetes serves as an access layer and standard interface for capabilities. The following figure shows a typical user-side Kubernetes cluster:
In a user’s Kubernetes system, Kubernetes native APIs only account for the section highlighted in red. Many other capabilities are implemented as plug-ins or Operators. For example, in the preceding figure, all the custom resources and capabilities are developed by third parties and delivered to end-users through Operators. This means, in future cloud-native ecosystems, most applications and capabilities will be based on CRD Operators, not Kubernetes native APIs.
As this trend continues, more and more software and capabilities will be described and defined by Kubernetes Operators. Cloud products will be based on Kubernetes and delivered through Operators.
As more and more Operators emerge, we will need to use a centralized method to solve their potential stability, discoverability, and performance problems. In other words, we will probably need a horizontal Operator management platform to manage all the Kubernetes Operator-based applications and capabilities in a unified manner. This will allow us to serve users in a better and more professional manner.
We will also need to program an Operator for each capability or application. Therefore, developer-friendly Operator programming frameworks are likely to be a major future trend. Such a programming framework would support different languages, including Go, Java, C, and Rust. It would allow programmers to focus on O&M logic, application management, and capability management, rather than the semantics and details of Kubernetes coding.
Finally, the popularization of the cloud-native ecosystem will promote the implementation of cloud services as Operators. Application-layer-oriented cloud services will be defined and abstracted in a standardized manner for use in multi-cluster or hybrid cloud environments. In addition, Operator-based methods will gradually replace infrastructure-as-code (IaC) projects, such as Terraform, in cloud service management and consumption in the cloud-native field.
As cloud-native and the overall ecosystem develop, we will see many changes in the application middleware field. Having gone from the original centralized enterprise service bus (ESB) architectures to fat clients, we will now see a gradual evolution to the sidecar-based service meshes that are a hot topic today.
Today, cloud capabilities and infrastructure capabilities are constantly expanding. Many things that could only be done through middleware can now be easily achieved through cloud services. Application middleware is no longer a capability provider, but a standard interface through which users can access capabilities. The standard interface is no longer built on fat clients but implemented through the common HTTP and gRPC protocols. The sidecar approach decouples the service access layer from the application business logic. This is the idea of service mesh.
Currently, service mesh only supports traffic governance, routing policies, and access control in traditional middleware. However, the sidecar model can be applied in all middleware scenarios to completely decouple the middleware logic from the application business logic. It sinks application middleware capabilities to the Kubernetes layer. In this way, applications can be more specialized, with more attention paid to the business logic.
In response to this trend, another trend will emerge at the Kubernetes layer: sidecar automation and large-scale O&M capabilities. Since there will be a huge number of sidecars and the application middleware is likely to evolve into sidecar clusters, sidecar management, and large-scale O&M capabilities will be a necessity for clusters and cloud products.
With the continuous development of the cloud-native ecosystem and popular adoption of the cloud-native concept, DevOps is likely to undergo an essential change that sees the emergence of a new generation of DevOps models and systems. As Kubernetes capabilities grow more powerful and the infrastructure becomes more complex, it will be easy to build application platforms based on this powerful infrastructure. Such application platforms will eventually replace traditional PaaS platforms.
We currently use DevOps because the infrastructure is still not sufficiently powerful, standardized, or practical. Therefore, we need a set of tools for business R&D that can connect developers and infrastructure. For example, since the infrastructure provides capabilities in the form of VMs, we need to turn these VMs into the blue-green release or progressive application delivery systems desired by R&D teams. This previously required a series of DevOps tools and a continuous integration and continuous delivery (CI/CD) pipeline, but now, the situation has changed. The capabilities of the Kubernetes infrastructure are expanding, allowing Kubernetes to provide a blue-green release and other capabilities itself. This will fundamentally alter DevOps in the following ways:
1. Separation of Concerns
In the Kubernetes context, “software” is no longer a single deliverable controlled by the application owner, but a collection of multiple Kubernetes objects. Among these Kubernetes objects, only a small number of objects concern the R&D team, and the application owner is not aware of many objects. This leads to the separation of concerns on the platform. The focus of the R&D team will be completely different from the O&M team or system team. This means that R&D personnel does not have to consider O&M details, such as how to implement blue-green release or horizontal scale-out. They simply need to write and deliver the business code.
As Kubernetes and its infrastructure become increasingly complex and the relevant concepts become more important, it will be impossible for developers to understand all the concepts involved in the platform layer. Therefore, the future cloud-native ecosystem will inevitably be abstracted and stratified. The roles of each layer will only interact with their own data abstractions. The R&D team will have its own declarative API objects, while the O&M team has a different set of declarative API objects. In addition, each layer will have a different focus. This situation will provide the context for the future development of the overall DevOps system.
2. Widespread Adoption of Serverless
Cloud-native focuses on applications. In this context, serverless is no longer an independent scenario and no longer limited to certain vertical fields. Serverless will become a general approach and an intrinsic component of cloud-native application management systems. Let us explain this from two perspectives. First, in terms of capabilities, lightweight O&M, NoOps, and self-service O&M will become mainstream application O&M capabilities. Application management in the cloud-native ecosystem is a lightweight O&M process. This means application O&M is no longer manual and complicated. Rather, it is an out-of-the-box process with very simple modular operations. Both Kubernetes and cloud-native are used to modularize the underlying infrastructure. This is similar to the NoOps concept advocated in the serverless field.
Second, in terms of applications, application descriptions are widely abstracted by users, and the event-driven and serverless concepts are split and generalized. This way, serverless capabilities can be applied in a wide range of scenarios instead of the narrow range of serverless scenarios we have today, such as FaaS and container instances. In the future, all applications will be scalable down to zero servers.
3. Application Layer Technology Based on IaD Will Go Mainstream
First, the idea based on infrastructure-as-data (IaD) will go mainstream. IaD is accomplished by declarative APIs in Kubernetes. The core idea of declarative APIs is to describe infrastructure, applications, and capabilities through declarative files or declarative objects. In this way, the file or object itself is “data.” As a result, Kubernetes or the infrastructure layer is driven by data, achieving IaD. This idea can be used to develop many cutting edge technologies, such as GitOps and pipeline YAML operation tools (including Kustomize and Kpt.) This sort of pipeline-based application management will become a mainstream application management method in the cloud-native ecosystem.
Second, declarative application definition models (such as Open Application Model OAM), declarative CI/CD systems, and pipelines will become a new method of application delivery. For example, traditional Jenkins is an imperative organization method. With the emergence of declarative pipelines and growing popularity of the cloud-native ecosystem and Kubernetes, IaD-based pipelines and next-generation CI/CD systems will also become the mainstream solutions in the industry. These new systems will be fundamentally different from previous CI/CD and pipelines because all operations in the new CI/CD systems will be declarative descriptions. Since they use declarative descriptions, all these operations and the processes in CI/CD can be hosted in GitHub. Even manual approval operations can be hosted in GitHub for auditing and version management.
The emergence of IaD tells us in future cloud-native systems, everything will be an object, and everything will be data. As the number of objects and data increases, object and data management, auditing, and verification will become increasingly complicated. Therefore, we must develop policy engines for these objects and data. Policy engines will be very important in future systems. In the future, all Kubernetes application platforms might need a policy engine to help users process data operation policies in different scenarios.
4. End-User Experience Layers Built on Top of IaD
Note: although IaD will become a mainstream technology at the application layer, this could have a detrimental impact on the end-user experience. The human brain is better at understanding things that follow set processes or rules than static data. Therefore, for ease of understanding, we need to construct an experience layer for end-users above the IaD layer. This means that Kubernetes will not present declarative data directly to end-users, but will first manipulate the data through a dynamic configuration language (DSL) that can understand the Kubernetes data model, an API-object-based CLI or dashboard, or an application-centric interaction and collaboration process. The end-user experience layer determines whether a product can attract and retain users. This will ultimately determine whether cloud-native systems are user-friendly.
5. DevSecOps
With the development of the next-generation DevOps systems described earlier, security will be incorporated into application delivery from the very beginning. The combination of development, security, and operations is called DevSecOps in the industry. This refers to the practice of considering security policies, security issues, and security configuration from the very start of application development. This is different from a traditional approach, which performs post-event security auditing and management after the application is delivered or has been launched.
With the development of cloud-native systems, the value of the cloud is gradually moving to the application layer and systems are evolving toward the use of declarative APIs and IaD. These trends will also change the underlying infrastructure in several ways. First, the infrastructure capabilities will be provided based on declarative APIs and self-help methods. Today, clouds are collections of infrastructure capabilities and can be thought of as infinite capability layers. We currently assume that all infrastructure capabilities can be provided by clouds. This is completely different from the previous view of infrastructure. In the past, both cloud and infrastructure capabilities were weak. Therefore, we had to develop a large middleware system and a sophisticated DevOps system to provide the glue that would connect the infrastructure with application, R&D, and O&M personnel.
In the future, applications will play the leading role in the cloud-native ecosystem. The cloud provides the capabilities that applications need through a standardized access layer, rather than by directly dealing with the infrastructure. The development of the cloud-native ecosystem will lead to a major change from the user perspective. The process will change from infrastructure-oriented to application-oriented. The infrastructure will now be able to provide any capability that users want. In the future, infrastructure will be centered on applications.
This concept is similar to serverless. We can call it the serverless-native for the underlying infrastructure. This means that the infrastructure will gradually evolve into declarative APIs, and as a direct result, become self-service infrastructure.
Building a more intelligent infrastructure will become an important means to achieve infrastructure based on declarative APIs and self-service. The conversion of the modular capabilities of the infrastructure system into a data-based definition method allows us to easily drive infrastructure operations through monitoring data and historical data. This is what we mean by a “self-driving infrastructure.” Data-driven intelligent infrastructure will become possible in the future, provided that the infrastructure itself implements declarative APIs and self-service capabilities.
At the same time, since the application layer generalizes serverless capabilities, functions, such as scale to 0 and pay-as-you-go, will become a basic assumption of applications. As a result, the resource layer will be able to achieve high scalability and implement unlimited resource pools. As part of intelligent infrastructure, we can implement intelligent scheduling and hybrid deployment to ensure optimal resource utilization and minimize costs.
At the same time, to achieve maximum resource efficiency, the underlying layer must be a Kubernetes-oriented architecture. This will allow for natural integration with Kubernetes. This is reflected in two areas. First, at the runtime layer, such an infrastructure would be better suited for container runtime based on hardware virtualization instead of traditional VMs, such as Kata Container. In this case, ECS Bare Metal Instances would be more suitable hosts. With the development of this technology, lightweight Virtual Machine Managers (VMMs) will become a key technology for the optimization of container runtime and the agility of the overall infrastructure.
Second, the multitenancy control plane would be physically isolated, not just logically isolated, for different tenants. The Kubernetes data model requires strong physical isolation between tenant control planes. This is why we believe that future architectures featuring strong multitenancy will be built based on Kubernetes. Alibaba has observed this trend and we are working to better adapt to the development of serverless-native infrastructure.
The next step in the evolution of cloud computing is cloud-native. The next step in the development of IT architectures is cloud-native architecture. This is why it is a great time to be a developer. Alibaba Cloud will release the Cloud-Native Architecture white paper in July to help developers, architects, and technical decision-makers come together to define and embrace cloud-native.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
4 
4 
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://itnext.io/top-gitops-tactics-to-build-secure-cloud-native-infrastructure-7c70f5d4cbf4?source=search_post---------171,"GitOps adoption is on the rise, driven by enterprise migration to the cloud. We can attribute its growing popularity to its efficiency in streamlined infrastructure management. As such, cloud security is an essential component that goes along with it.
While GitOps helps companies accelerate delivery and time to market, it also helps improve Continuous Integration (CI) and Continuous Delivery (CD) pipeline security. It can serve as a foundation where developers identify and resolve potential vulnerabilities throughout the…
"
https://medium.com/@pingcap/tidb-on-jd-cloud-a-cloud-native-distributed-database-service-204727c2067?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
Feb 18, 2021·7 min read
JD Cloud database team
In the past year, as most people practiced social distancing, they relied heavily on online services for shopping, remote working, and socializing. Online services surged, and as the requests and data flooded in, the database systems faced severe challenges.
After surveying hundreds of enterprises, we at JD Cloud developed a portrait of their ideal database:
To provide a cloud database service that meets these expectations, JD Cloud teamed up with PingCAP to provide a distributed database service on the cloud: Cloud-TiDB.
TiDB is an open-source, cloud-native distributed database. It features horizontal scalability and MySQL compatibility. As a distributed database, TiDB can be deployed across data centers, and it stores data in multiple replicas with strong consistency. It also supports automatic failover. In addition, TiDB performs remarkably for Online Analytical Processing (OLAP) workloads to provide a one-stop Hybrid Transactional/Analytical Processing (HTAP) solution.
TiDB’s architecture is shown below. It has a multi-layered architecture with three modules: TiDB, TiKV, and PD.
On JD cloud, we deploy TiDB in Kubernetes to make TiDB more flexible and reliable. The deployment architecture is as follows:
The user’s client accesses the TiDB cluster via the network load balancer (NLB). The NLB balances the load among multiple TiDB nodes in the cluster. The cluster can have three replicas in three availability zones (AZ). By setting affinity, the number of TiKV instances is roughly balanced in each AZ. PD instances are also distributed in each AZ.
In this architecture, each component in the cluster is highly available, and the database has no single point of failure. Even if one AZ fails, the database service is still available, and no data is lost. If an individual node fails, the TiDB cluster still provides services for the external application.
The TiDB cluster is also connected to the JD cloud monitoring and alert service and the logging system, which work together to supervise the status of TiDB instances. Users can manage the lifecycle of TiDB instances on the JD Cloud console.
Cloud-TiDB can automatically perform full backup and stores the backup data in object storage service (OSS). During the backup, TiDB doesn’t write data to the local disks directly, but caches it in memory first and then sends it to OSS. This write approach improves backup efficiency and reduces wear and tear on the disks.
During the restore, TiDB can restore data from OSS into a new cluster without overwriting the original instance. This greatly ensures data safety and avoids any damage caused by operator error.
Cloud-TiDB can scale out by adding nodes online dynamically. TiDB can horizontally scale its computing capacity and storage to an almost infinite extent.
When users first launch a new application, a small number of instances may be enough for the service. (The minimum configuration is 3 TiKV, 3 PD, and 2 TiDB.) As the data volume grows, they can add more TiKV or TiDB instances as needed.
Cloud-TiDB supports real-time data analytics by adding TiFlash nodes. TiFlash is a columnar storage engine, the key component that makes TiDB an HTAP database. As a columnar extension of TiKV, TiFlash replicates data from TiKV in real time and provides the same Snapshot Isolation level of consistency as TiKV does. By virtue of the Raft consensus algorithm, TiFlash makes sure that the application reads fresh data.
When TiDB processes a query, it decides whether to read from TiFlash or TiKV. It can even use both storage engines in a single query to boost the query performance.
You can import data in CSV format into TiDB clusters at a stunning speed of 500 GB per hour, several times faster than traditional SQL import. The import process is as follows:
After the import, TiDB compares the checksum of the data source and the target cluster to make sure that the data is correctly imported.
JD Cloud-TiDB supports deploying primary-secondary clusters. The primary-secondary clusters keep data in sync using TiDB Binlog, which replicates data from the primary cluster to the secondary cluster (deployed in any data center) in near real time.
The primary cluster connects with the application and performs read and write operations, while the secondary cluster replicates all the data from the primary cluster and takes over the service when the primary cluster fails. In addition, the secondary cluster can also provide data analytics services, such as generating reports. This is to offload the pressure from the primary cluster and consume resources more efficiently. This feature will be available soon.
TiDB has a comprehensive monitoring and alert service. There are two types of alerts:
The TiDB cluster provides bountiful monitoring metrics. You can access the monitoring dashboard via a browser and read hundreds of TiDB monitoring items as shown below:
TiDB is a good match for two types of scenarios:
TiDB is an elegant and ideal alternative for traditional database middleware and sharding solutions. Because TiDB clusters scale out almost unlimitedly and linearly, TiDB can process data with highly concurrent throughput. In this regard, you can use TiDB simply as an infinitely scalable MySQL alternative, which greatly reduces the maintenance complexity.
TiDB can also provide powerful data analytics. With the distributed query engine, TiDB processes complex queries ten times — sometimes even 100 times — faster than MySQL. You can use TiDB for various real-time data analytics, such as reporting and risk control.
Cloud-TiDB is now available on JD Cloud, and you can apply for a beta test for free. If you have any questions, join our community on Slack.
Originally published at www.pingcap.com on Feb 09, 2021
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
See all (22)
4 
4 claps
4 
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/how-shifting-left-helps-organizations-mitigate-cloud-native-security-risks-cff20bac950b?source=search_post---------173,"There are currently no responses for this story.
Be the first to respond.
This article was originally published at: https://www.magalix.com/blog/how-shifting-left-helps-organizations-mitigate-cloud-native-security-risks
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chroniclesec/today-you-really-want-cloud-native-siem-capabilities-cd96dc007f94?source=search_post---------174,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chronicle
Apr 21, 2021·3 min read
(By Anton Chuvakin and originally posted at Anton on Security)
One thing I did not expect to see in 2021 is a lot of people complaining about how difficult their SIEM is to operate.Let’s explore this topic for the (n+1)-th time. And let me tell you … that “n” is pretty damn large since my first involvement with SIEM in January 2002 (!) — examples, examples, examples.
Anton’s old SIEM presentation from 2012 (source, date: 2012)
Before we go, we need to separate the SIEM tool operation difficulties from the SIEM mission difficulties. To remind, the mission that the SIEM is aimed at is very difficult in today’s environments. The mission also evolved a lot over the years from alert aggregation to compliance and reporting to threat detection and response support. Note that even intelligently aggregating, cleaning and normalizing lots of logs coming from a broad range of systems, from mainframes to microservices is not that easy…With that out of the way, SIEM detection challenges definitely do not mean that you need to spend hours patching a SIEM appliance, for example. Or tuning the backend to make sure that the searches are fast (fast being a relative term, longer than a coffee break, but shorter than a vacation for many tools).Over the years — and recent years — mind you, I’ve heard people say things like (quotes are all fictitious, but all inspired by real examples; if you literally said the below, this is a coincidence):
Now, aren’t we all surprised that this is still an issue today in 2021? I recall the day when appliance “SEM” products have started replacing the old-style installable software SIM. The vendors were touting the fact that anybody with a screwdriver can install their SIEM right into a rack — and then magic happens.
But what happened instead was reality.
Anton’s old SIEM presentation from 2009 (source, date: 2009)
So, yes, even today’s SIEM tools produce the customer reactions I mentioned above. And open source — in this context — is occasionally worse, requiring even more work to keep it up and running, performing, and scaling.OK, now guess which ONE THING solves most of the SIEM operation challenges?
You got it: SaaS-based security analytics. Proper cloud-native security analytics, not the fake cloud variety (this does solve some of the challenges and creates others).
Now, how to decide if cloud native security analytics is for you? Here are some arguments:
Likely YES:
Possibly NO:
This is how I’d decide. For more details, go here (or here). Now, go and get “cloud smart” :-)
22 
22 
22 
"
https://medium.com/hackernoon/ask-me-anything-with-terence-lee-and-joe-kutner-of-heroku-and-cloud-native-buildpacks-on-july-ab9739170114?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
This is an email from Get Better Tech Emails via HackerNoon.com, a newsletter by HackerNoon.com.
Terence Lee and Joe Kutner are founding members of the Cloud Native Buildpacks project. Ask them anything at 5 pm EST on July 28th!
In their own words:
we are Terence Lee and Joe Kutner, both working on the Platform Engineering team at Salesforce Heroku. We are also founding members of the Cloud Native Buildpacks project, and today we will answer any questions you have around Buildpacks, what they are, use cases, and how we use them here at Salesforce Heroku. If you are new to the topic and want to have some context before the AMA you can take a look at the following resources:
Thank you for joining us on July 28th, at 2 pm PDT / 5 pm EDT.
Join the AMA.
#BlackLivesMatter
4 
4 claps
4 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Visit hackernoon.com. FYI: we publish rarely w/ Medium CMS b/c of pop up ads for account creation & paywall requesting $$$ from readers. Open Internet!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@krishnan/differing-cloud-native-platform-philosophies-6192dbcceaf0?source=search_post---------176,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krish
Jul 12, 2018·5 min read
Cloud Native platforms, driven in large part by Kubernetes, is at the peak of the hype cycle and almost every enterprise we talk to have embraced or in the process of embracing such platforms. At Rishidot Research, we focus on finding patterns emerging from the enterprise users and share it with a larger audience. One of the patterns we expect to gain traction in the coming years is to empower developers to handle the underlying distributed systems through code. Though the concept of infrastructure as code is nothing new, we are seeing a new trend that empowers developers to use familiar programming languages to handle the underlying infrastructure than using config management tools or complex YAML files. We are bucketing the application platforms, broadly, into three buckets. In this post, we will differentiate between these three platform philosophies so that enterprise users can find the right platform for their needs.
The traditional approach to solving the complexity for developers is to build an abstraction and offer developers an API to deploy their applications. The abstraction took away most of the underlying infrastructure complexity and made it easy for developers to deploy their applications. Typical examples include Red Hat OpenShift, Pivotal, Mesosphere, Docker EE, Rancher Labs, etc.. They range in focus from developer centricity to focus on IT operators. In the early cloud days, this bucket was categorized as PaaS.
Driven by public clouds and the self-service interface for consuming infrastructure services, a more DevOps-centric approach to operations gained traction in the last decade. With configuration management tools like Chef, Puppet and Ansible gaining traction and modern approaches like GitOps promoted by WeaveWorks, treating infrastructure as a code are widely used by startups and some of the cutting edge enterprises. Hashicorp’s Terraform is another example in this category. In the early cloud days, this was categorized as IaaS+.
Infrastructure as a code is nothing new. It has steadily gained adoption with the cloud, especially among the startups and some of the enterprises. Platform abstraction was attractive for many large enterprises wanting to modernize their infrastructure. In both cases, the configuration files are fast becoming a bottleneck with the ever-increasing complexity. A third new approach to application infrastructure is gaining attention and it could change the way enterprises are thinking about platforms. Driven by startups like Pulumi Inc and the Metaparticle project started by Brenden Burns of Microsoft, this new approach aims to bring abstractions to infrastructure components straight to developers and empower them to handle the underlying infrastructure in a language (as in programming languages) familiar to them. Metaparticle is focussed on Kubernetes for now and Pulumi takes an approach that is independent of the type of the cloud service or provider. The rationale behind this philosophy is that this abstraction will not only reduce the complexity of configuration files dramatically but also make it more palatable for developers who need not learn a brand new language specification for handling the underlying infrastructure.
These are three different approaches to application platforms that will compete to gain enterprise adoption. If the developer-centric approach to infrastructure gains momentum, expect to see vendors in the first two categories to bring in another layer of abstraction in front of their platforms. Very early days but an interesting approach that requires the attention of modern enterprise decision makers.
tl:dr version: Abstracted platforms keep the role of Devs and Ops separate; Infrastructure as code requires Devs to understand Operations; Infrastructure as developer friendly code puts a developer-friendly wrapper around infrastructure
Originally posted at StackSense.io
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
See all (529)
3 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
3 claps
3 
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/part-2-creating-the-microservice-application-with-the-cloud-native-stack-5a3f4ab412c9?source=search_post---------177,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
An in-depth series on how to easily get centralized logging, better security, performance metrics, and authentication using a Kubernetes-based platform.This is the second part of our tutorial. If you haven’t seen it, please check out Part 1, How the Cloud-Native Stack Helps Writing Minimal Microservices.
In this section, we’ll focus on creating the application. We’ll start with requirements and architecture, and then move to the actual code. Please note that there are hardly any changes in the code that integrates with the cloud-native platform we prepared in Part 1.
To demonstrate what our Kubernetes-based platform can provide to application developers, we’ll create a typical “todo list” application in a microservice architecture.
Let’s start with the requirements we have for it.
Create a todo list management application, that:
Now that we have our requirements, it’s time to switch our focus to the application’s architecture. We’ll create two main components and use a MySQL database as a persistent data storage for them.
The idea is shown in the diagram below:
Our application is going to offer an HTTP based REST interface. Exposing the API to the clients is a task of the green “API service” layer. This layer will be responsible for handling HTTP requests, validating them, logging all requests, and providing authentication. As we’re abstracting from the clients, this API can be used by anything that can send HTTP requests, which includes mobile, web, and even CLI applications.
To keep the scope of this blog entry limited, we won’t be implementing actual clients and we’ll just use the curl command for testing the API. We're also exposing the API over plain HTTP only, which is, of course, unacceptable in production, but again will let us limit the scope a little. This will also make testing easier and will free us from caring about HTTPS certificates.
In the architecture diagram above, API service is the gateway to our system. When a request passes this gateway, behind it lies the land of different microservices, that cooperate to provide the necessary service. In our case, the application is so simple, that the whole business logic is provided by a single service called “todo manager”. To better show how Kubernetes platforms allow you to deploy and manage multi-protocol multi-language solutions, we’ll switch the communication protocol to gRPC here. As we want our “todo service” to be stateless and easily scalable, it won’t keep any persistent data. This task will be delegated to the MySQL database that we inject into our solution.
If you’re not interested in implementation details and just want to build the application and run it, the code is already in the repository, so please skip to Building the Application.
We need to implement two services, one being an HTTP server and one gRPC. We’ll use Golang for both, as it’s a good match for our Kubernetes-based environment and we can use some existing tools and libraries to create our code in a very effective way. I won’t cover every line of code here, but only point out some more important parts.
Let’s start with something basic: our communication contract for both services.
We need to define a communication interface for our two microservices. First, let’s define JSON requests and responses our REST API gateway will serve to clients. Our only resource is called todo:
This object will be sent every time a todo or a list of todos is processed by our clients. Please note just 2 things. The ID is populated by the service and the client never sets the ID. Secondly, we don’t have any owner information here. Right now, we’re skipping authentication in the REST layer. Still, we want to already include owner information processing in our application, as stated in requirements. In the future, when authentication is added to the REST layer, we’ll get real username information from our HTTP request context. For now, we’ll use a simple placeholder in the form of a constant username in the code:
Our API endpoints use a typical REST approach, we prefix all of them with “v1/todo” and then expose:
Details of this REST API are in “ api-server/pkg/todo/routes.go” and “ api-server/pkg/todo/model.go “.
Our second interface is between the api-server and todo-manager services. This one is based on gRPC, so the whole interface is described using the protobuf syntax:
As you can see here (todo-manager/pkg/proto/todo.proto), we’re mapping our REST interface directly to the gRPC calls, just remembering to add “owner” information to every request, so the todo-manager can filter ToDo entries by ownership.
Our API server is really simple. Its main aim is to act as a gateway between REST endpoints and gRPC microservices. Typically, such gateways also integrate authentication and authorization, rate limiting, API keys etc. For now, our gateway will stay very simple and will just pass the requests to the microservices. We’ve already covered the api-server/pkg/todo/model.go” file when discussing interfaces. The “ todo-manager/pkg/proto/todo.pb.go” has a lot of code, but it’s all auto-generated. That’s just the way protobuf and gRPC work: they use the “ todo-manager/pkg/proto/todo.proto” file to generate server and client stubs for you. Here we’re using the generated client to pass our REST API calls to gRPC. The gRPC client connects to the gRPC server in “ api-server/pkg/todo/routes.go:L30” and REST API is mapped to gRPC in line 42. Note that we disable gRPC TLS security ( line 28). We’ll be relying on our Linkerd to provide encryption here.
The aim of this microservice is straightforward: get data from the gRPC calls, check for ownership and get the data to/from the database. To keep this last part as short and simple as possible, we’re using the gorm.io mapper here. The object that we use to map our data to the database is in “ todo-manager/pkg/server/model.go”. We just need to know where the database is — that’s the role of the Config struct in “ todo-manager/pkg/server/config.go”. It loads our configuration from environment variables and stores them. Actual requests-serving logic is in “ todo-manager/pkg/server/server.go”, starting in L46. One thing that might be strange there is how we send a response to the “ ListTodos() “ call. This is the so-called gRPC stream response. Using this, a server can start sending a collection of objects as soon as they become available, without waiting for the whole list to be available (which is not the case in our code). Also, using stream response a gRPC client can start processing elements on the list as soon as the first one is available.
For both our go projects, we’re providing a Makefile, which makes building the projects easier. It also injects version, commit, and build date in Go code before compiling into binaries, so we can better identify versions of software we’re deploying. So, to build our go binaries, we just have to run make in both ""api-server"" and ""todo-manager"" catalogs.
The next step is building docker images that we’ll run on our Kubernetes platform. The problem is that we need to make these images available to our minikube cluster later. Normally, this is done by using an image registry: we build the images, push them to the registry, then container runtime in the Kubernetes cluster pulls them from the registry when it starts the applications. Here, we’ll use a simpler solution that doesn’t need an external registry. Using minikube, our whole cluster has just a single docker instance as a container runtime, running in the virtual machine that minikube created. This docker is available over the network, so we can use it remotely, from our native in-system docker installation. To make this work, we just need to ask our local docker client to talk with a remote (minikube) docker server instead of the local server. To configure this, we need a few environment variables to point to the minikube’s docker.
Fortunately, minikube provides a ready command for that:
Now we can build Docker images and save them in the minikube’s docker directly. Again, in both “api-server” and “todo-manager” directories run make docker-build. When it's done, our images are ready and present in the Kubernetes cluster. Verify that by running docker images. Now it's time to work on the deployment of our application.
In Part 3 of our series, we’ll focus on how to deploy the application with Helm.
When running Kubernetes clusters in production, you will realize that you will need more, sometimes much more, than just one cluster. You will need to care not only about the deployment but also upgrades, security issues, and monitoring.
That’s where Giant Swarm can help — we offer managed Kubernetes clusters so that you don’t have to worry about managing them yourself. We also offer managed applications — well-known cloud-native projects that we can run for you on top of your Kubernetes cluster, creating a fully managed cloud-native platform.
Written by Łukasz Piątkowski: Kubernetes Platform Architect @Giant Swarm
ITNEXT is a platform for IT developers & software engineers…
23 
23 claps
23 
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/the-google-cisco-partnership-is-a-reminder-that-cloud-native-still-has-limits-in-hardware-a90b2c160ef7?source=search_post---------178,NA
https://medium.com/swlh/tidb-on-kubesphere-run-a-cloud-native-distributed-database-on-a-hybrid-cloud-kubernetes-platform-182008e2a045?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
Will Zhang (SRE at iSoftStone)
Kubernetes is the de facto standard for building application services that span multiple containers, so a scalable, reliable database that can run on Kubernetes is essential. TiDB, a cloud-native, open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads, meets that need admirably. Its architecture is suitable for Kubernetes, and it is MySQL compatible. TiDB also features horizontal scalability, strong consistency, and high availability.
As an SRE engineer responsible for cloud applications, I use TiDB as well as KubeSphere, an open source distributed operating system that manages cloud-native applications. KubeSphere uses Kubernetes as its kernel and provides a plug-and-play architecture so you can seamlessly integrate your applications into its ecosystem. You can run KubeSphere anywhere without changing any Kubernetes code.
By combining TiDB with KubeSphere, I can have Kubernetes-powered TiDB clusters and manage the clusters using a developer-friendly web UI. In this post, I will demonstrate how to deploy TiDB on KubeSphere from scratch.
Before you can deploy TiDB, you need a Kubernetes cluster. Installing the cluster might seem daunting: you need to prepare working machines, either physical or virtual, and you must configure network rules so that traffic can move smoothly among instances.
Fortunately, KubeSphere’s sponsor, QingCloud, provides a highly-functional platform that enables you to quickly deploy Kubernetes and KubeSphere at the same time with just a few clicks. (You can also deploy Kubernetes by itself.)
In this tutorial, I use QingCloud Kubernetes Engine (QKE) to prepare the environment, which creates virtual machines, configures networks, and sets storage for you. If you don’t use QKE, you may also directly use instances on QingCloud to install KubeSphere and Kubernetes by yourself.
Note: You can install KubeSphere on any infrastructure. We just use the QingCloud platform as an example. For more details, see the KubeSphere documentation.
To prepare the environment using QKE:
5. Install the TiDB Operator CustomResourceDefinition (CRD). TiDB Operator is an automatic operation system for TiDB clusters in Kubernetes. In the Tools section at the bottom of the page, click the Kubectl command line tool and enter the following command:
The output is shown below:
6. Create a new workspace. In the upper left corner of the current page, click Platform to display the Access Control page. In Workspaces, click Create to create a new workspace and give it a name; for example, dev-workspace.
In a workspace, users have various permissions to perform project tasks. Usually, a department-wide project requires a multi-tenant system so that everyone is responsible for their own part. In this tutorial, I use the admin account. Review the official KubeSphere documentation to learn more about how the multi-tenant system works.
7. Go to the workspace you just created and add a KubeSphere application repository. Click Add Repos:
KubeSphere provides two methods to add Helm charts: App Templates and App Repos. This article only talks about adding a KubeSphere application repository. You can also upload your own application templates and submit them to the KubeSphere App Store.
8. In the Add App Repository dialog box, and in the URL field add the PingCAP Helm repository (https://charts.pingcap.org):
TiDB Operator is an automatic operation system for TiDB clusters in Kubernetes. It provides a full management life-cycle for TiDB, including deployment, upgrades, scaling, backup, fail-over, and configuration changes. With TiDB Operator, TiDB can run seamlessly in Kubernetes clusters deployed on public or private clouds. Before you deploy a TiBD cluster, you should first deploy TiDB Operator on KubeSphere.
4. Deploy TiDB Operator. Switch to the PingCAP repository that stores multiple Helm charts and click tidb-operator.
Note: This article only demonstrates how to deploy TiDB Operator and TiDB clusters. You can also deploy other tools based on your needs.
5. On the tidb-operator page, select the Chart Files tab. You can view the configuration from the console directly or download the default values.yaml file. From the Versions drop-down menu on the right, select the version you want to install.
6. On the Basic Info page, confirm your app name, version, and deployment location.
7. On the App Config page, you can either edit the values.yaml file, or click Deploy directly with the default configurations.
8. On the Applications page, wait for TiDB Operator to be up and running.
9. Verify your deployment. In Workloads, you can see two deployments created for TiDB Operator.
After you deploy TiDB Operator, you can deploy a TiDB cluster on Kubernetes. The process of deploying a TiDB cluster is similar to deploying TiDB Operator.
Note: If you have trouble deploying TiDB on KubeSphere, contact us on GitHub.
Because I installed KubeSphere through QKE, all of these storage components were deployed automatically. The QingCloud CSI plugin implements an interface between the CSI-enabled Container Orchestrator and QingCloud storage. For more information on the QingCloud CSI, see their GitHub repository.
Note: In this tutorial, to provide external persistent storage, you only need to change the field storageClassName. If you want to deploy each TiDB component, such as TiKV and Placement Driver (PD), to individual nodes, specify the field nodeAffinity.
4. Click Deploy. The Applications list displays two apps:
Now that we have our apps ready, we may need to focus more on observability. KubeSphere dashboard lets us observe applications throughout their lifecycle.
2. Verify the stateful applications: TiDB, TiKV and PD. TiKV and TiDB will be created automatically. It may take a while before they are displayed in the list.
Click the StatefulSets tab. Click on a single StatefulSet item to display its details page. The page shows metrics in line charts over a period of time. The following is an example of TiDB metrics:
This page also lists relevant Pods. As you can see, the TiDB cluster contains three PD Pods, two TiDB Pods, and three TiKV Pods.
3. Go to the Storage section. You can see that TiKV and PD use persistent storage. This page also monitors Volume usage. Here is an example for TiKV:
4. On the Overview page, you can see the resource usage for the current project.
Kubernetes lists the running services and the port each service is exposed to, so you can easily access them.
3. Access the monitoring dashboard. TiDB also integrates Prometheus and Grafana to monitor database cluster performance. As shown in the first step, Grafana is exposed through NodePort.
After you configure necessary port forwarding rules and open the Grafana port in security groups on QingCloud Platform, you can view metrics from the Grafana UI.
Log in to the Grafana monitoring at ${Grafana-ip}:3000. The default username and password are both admin.
Note:
After you’re done with your cluster, remember to delete all its resources; otherwise, you may receive a large bill for services you didn’t use.
I hope you have successfully deployed TiDB. Because TiDB and KubeSphere are such powerful tools for cloud-native applications, I cannot showcase all their features in a single post. For example, the application deployment function (described in Deploy TiDB Operator has much to offer for cloud-native enthusiasts like me. I will post another article on how to deploy TiDB by uploading Helm charts to the KubeSphere App Store.
If you have trouble deploying TiDB on KubeSphere, contact us on GitHub.
Get smarter at building your thing. Join The Startup’s +750K followers.
37 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
37 claps
37 
Written by
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/inclavare-confidential-computing-container-technology-for-cloud-native-ae571fe421f4?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 28, 2020·10 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Qian Yue and Guo Chaohong.
The contents of this blog were taken from a larger virtual presentation from the 2020 Apsara Conference.
This speech is divided into two parts. Qian Yue (from Alibaba Cloud Intelligence) will be responsible for the first part, and Guo Chaohong (from Intel IAGS) will be responsible for the second part.
To judge from the current development trend, confidential computing has great potential, but it is still in the early stages. In the technical heat curve of the computing infrastructure released by Gartner last year, confidential computing was mentioned for the first time and is at the innovation trigger stage. Therefore, confidential computing still has many shortcomings in the engineering field. On the one hand, it is difficult to develop system software with SDK that is commonly used in the engineering field. On the other hand, the solutions provided by CSPs are generally implemented based on a single bare metal architecture, which sets a high threshold for use.
Since most attempts to promote confidential computing at the IaaS layer produced negative results, we found another way: we are trying to introduce confidential computing technology to the container ecosystem. To put it into practice, we initiated the Inclavare Containers project.
This project aims to provide the open-source community with the cloud-native-oriented confidential computing container technology and security architecture, giving full play to the characteristics of confidential computing. In terms of value, we wish to greatly reduce the cost of developing and using confidential computing by combining with container technology. This allows users to enjoy the high security brought by confidential computing and maintain the same user experience as common containers. Inclavare Containers will support a variety of enclave forms based on the hardware assistance of Intel processors to cover a wider range of user groups with different levels of security requirements. Inclavare Containers will provide users with more choices and flexibility to strike the balance between security and cost.
First, this open-source project is innovative. It integrates cloud-native container technology with confidential computing technology. The rune component has been added to the reference list of OCI runtime. In addition, Intel has confirmed its participation in the development of the open-source community.
Currently, the project supports the following core functions: It allows enclave containers to be started through Kubernetes and Docker. Container applications can run [directly] in the protected TEE environment. In terms of LibOS support, Inclavare Containers support Occlum by default and provides support for Graphene. In terms of runtime language, Inclavare Containers supports “three” advanced languages.
We will release a minor version at the end of each month for the community. Currently, three versions have been released. Community users can use and deploy Inclavare Containers on the mainstream Linux distributions by using the installation packages. For the self-developed Aliyun Linux 2 distribution by the Alibaba Operating System Team, we also provide dedicated user guides and technical support to help users deploy Inclavare Containers on Alibaba Cloud.
First, we integrate Intel SGX technology with the mature container ecosystem to deploy and run sensitive applications of users (in the form of enclave containers.) We want to free users from understanding the complexity brought by confidential computing when they create images, and to give them the same user experience as common containers.
Second, Intel SGX technology provides high-security protection, but it also brings some programming constraints. For example, the syscall command cannot be run in the SGX enclave. Therefore, we introduced LibOS technology to resolve the software compatibility issues and remove developers' burden of performing complex software adaptation in the process of porting software to the Intel SGX enclave.
Last but not least, Inclavare Containers supports multiple runtime languages, including Java. In this way, the application range of enclave containers is enlarged and improved. In addition, powerful language ecosystems, like Java, are also introduced to confidential computing scenarios, enriching the variety and quantity of confidential computing applications.
The project was established at the end of last year. The PoC was completed in March, and the v0.1 version was released in mid-May. After a series of development processes, we witnessed the establishment of the project from scratch, led it to engineering practice from innovative inspiration, and determined its evolution direction after directional exploration.
Currently, we have formed a systematic and phasic development strategy and determined the evolution direction. In the release of the v0.5 version, Inclavare Containers will provide a complete confidential computing cluster capability and support the implementation of Alibaba Cloud ACK-TEE 2.0 based on this capability. At the same time, we will also complete the pre-research and exploration and in new directions for Inclavare Containers 1.0.
To sum up, we want to continue to improve the confidential computing cluster product, ACK-TEE. It is more secure and easier to use based on the technical advantages of Alibaba Cloud container technology.
Relying on the Inclavare Containers technology and ecosystem, while supporting ACK-TEE products, we intend to provide open-source and standardized container technology.
From a strategic perspective, Alibaba Cloud is continuously committed to providing security protection that covers the entire lifecycle of data. Therefore, we are also making continuous progress towards the development of confidential computing technology and the continuous evolution of ACK-TEE products, making Alibaba Cloud a worthy cloud service provider in confidential computing technology.
In terms of data security, there are mature solutions for storage and transmission security. Data usage security is getting more attention.
There are many privileged codes in multi-user computer systems, such as BIOS, operating system, and Virtual Machine Manager (VMM.) If a malicious program controls the system by exploiting the vulnerability of privileged code, the data security of other users using this system cannot be guaranteed.
Especially in the multi-tenant public cloud environment, virtual machines of different tenants may run on the same physical server. Malicious programs can exploit the vulnerability of VMMs to gain system privileges and access other tenant’s data. Besides, the VMM has absolute control over the system and can access users’ data. Therefore, some users do not trust the public cloud service provider and refuse to use the public cloud.
Intel’s SGX technology resolves the issue of data security. Its basic idea is to divide the application into two parts: trusted memory and untrusted memory. SGX technology exclusively allocates private memory to store the trusted data for a specific process. The data in the private memory cannot be accessed even by running privileged code. In the Intel documentation, this private memory is called “enclave.”
The enclave acts like a black box. The code that runs inside provides some functions for external systems to call, but the code that runs outside cannot read the data in the enclave. Unless the enclave exits because of asynchronous events, such as hardware shutdown, programs must follow the request-response pattern and use SGX instructions to transfer data in and out of the enclave.
Since privileged code cannot be used to access the data in the enclave, even attackers that use privileged code to gain system management permissions cannot access the trusted data of other users.
SGX technology effectively resolves the trust issue between tenants and providers on the public cloud. For example, some users want to use Alibaba Cloud but worry that Alibaba Cloud may monitor their data. With SGX, this issue no longer exists.
This slide shows the four intentions that SGX has implemented.
The BIOS reserves a segment of physical memory for SGX and reports the memory to VMM or OS through the E820 table. Neither privileged code nor DMA can be used to access this memory.
OS or VMM uses the reserved physical memory to create the memory dedicated to a specific process, namely, the enclave.
By adding or removing a Memory Encryption Engine (MEE) from the memory controller, we can encrypt and decrypt the data that enters and exits the enclave. The key used by MEE encryption is randomly generated each time the machine is started or restarted.
When the memory controller receives the virtual address accessed by the CPU, if the corresponding physical memory belongs to SGX, the matter is transferred to MEE for processing. Access to non-SGX memory is still handled by MMU. Page table translation in the memory is still done in the untrusted part, so the traditional non-SGX code (OS or VMM) does not require any changes.
Through EENTER and ERESUME, the untrusted part calls the code in the enclave, which returns a response to the caller through EEXIT.
Like the management of normal memory, the management of the enclave memory also includes virtual addresses and physical memory. When you create an enclave, a data structure (SECS) is used to describe the enclave, which has two domains that specify the virtual address range of the enclave. Page table mapping is managed by an SGX driver and physical memory is allocated when a fault occurs in the page table mapping.
This is a flowchart of SGX memory access control.
To prove this, SGX provides a remote certification mechanism. The certification service provider can be the tenant’s local server or a trusted third party. Intel has published many papers and provided the fully open-source implementation code. Here is a simple schematic diagram. I will not elaborate on the details, but anyone interested in this can check Intel’s website for more information.
The tenant program running on the cloud uses EINIT to create an enclave. The CPU generates a digest for the content in the enclave based on the key and sends the digest to the remote certification service provider.
The remote certification service provider verifies that the application runs in the SGX trusted environment based on the digest. Then, the application establishes a secret channel and starts to deploy keys and private data.
During its running, SGX uses the sealing key to encrypt the enclave-related data and write the ciphertext to the disk. The enclave can be recovered if the machine restarts unexpectedly.
Both Linux and Microsoft Windows support SGX. Application developers can use the SGX SDK provided by Intel to develop SGX-based applications and ensure data security during use.
Anyone interested in this can visit this website.
With the development of technologies such as public cloud, edge computing, and blockchain, data security has attracted increasing attention. Many users who want to use remote hardware resources are worried about data leakage and do not trust remote hardware owners and public cloud service providers. SGX technology can resolve this trust issue and provide a solid hardware foundation for confidential computing. There are many scenarios where SGX can be used. Here are some examples.
To further promote confidential computing, eliminate users’ worries about data leakage, and encourage them to use the public cloud with confidence, Intel is currently cooperating with Alibaba to develop the infrastructure of confidential computing and provide users with confidential computing services on Alibaba Cloud.
Qian Yue is from Alibaba Cloud Intelligence and is the Director of Alibaba’s Operating System Security Innovation Team. Qian Yue mainly take charges of the work on confidential computing and trusted computing. Qian Yue is also the initiator of the open-source project, Inclavare Containers.
Guo Chaohong works for Intel IAGS and has worked closely with Intel SGX.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
3 
3 claps
3 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-best-practices-for-container-technology-implementation-f599b4dff66?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 18, 2020·19 min read
By Yili
Introduction: With the rapid development and extensive application of container technologies, cloud-native technologies are becoming the future of IT development. As the first Chinese company that deployed container technologies, Alibaba Cloud has made great achievements in both technologies and products. Yili, a senior technical expert at Alibaba Cloud, shares the best practices of container technology implementation through Alibaba Cloud Container Service. This article is meant to help you better understand the container technologies and cloud-native concepts, properly design cloud architecture, and make full use of the cloud.
A quote taken The Economist magazine said, “Without the container, there would be no globalization.”
Economic globalization is based on the modern transportation system and at its core are containers. The emergence of the shipping container enabled the standardization and automation of logistics, greatly reducing transportation costs and making it possible to integrate global supply chains. Therefore, without containers, there would be no globalization.
The standardization and modularization concepts of containers are promoting the supply chain reform in the construction industry. After the coronavirus (COVID-19) pandemic, Huoshenshan Hospital, a specialized hospital that can accommodate thousands of beds was built within 10 days in Wuhan, China. It assumed an important role during the fight against the pandemic, especially in the early days. The whole hospital was assembled from prefabricated container houses. The modular rooms were pre-equipped with air conditioners, disinfection stations, water supplies, and drainage, greatly accelerating the speed of construction of the hospital.
The software container technology is also reshaping the entire software supply chain. As a lightweight virtualization technology for operating systems, containers are different from traditional physical machines and virtualization technologies. Think about it like this:
A traditional physical machine is like a single-family detached home.
Virtual machines are like townhouses.
A container is like a container house.
In the past few years, container technologies have been widely used in the IT industry. Their most important values are:
Agility
Speed matters a lot. In the era of digital transformation, each enterprise is facing the impact of emerging business modes and numerous uncertainties. An enterprise’s continuous innovation ability, rather than its current large scale or past successful strategies, determines its success in the future. Container technologies improve the IT architecture agility of an enterprise, thereby enhancing its business agility and accelerating its business innovation. For example, during the COVID-19 pandemic, online businesses in the education, video, and public health industries experienced explosive growth. Container technologies help seize opportunities for rapid business growth. According to industrial statistics, container technologies increase delivery efficiency 3 to 10 times over, which allows enterprises to carry out fast iteration and low-cost trial and error.
Elasticity
In the Internet age, enterprise IT systems often encounter both predictable and unexpected traffic growth, such as e-commerce promotions and emergencies. Container technologies can give full play to the elasticity of cloud computing and reduce the computing cost by increasing deployment density and elasticity. For example, after the exponential growth of online traffic during the COVID-19 pandemic, container technologies can be used to alleviate the expansion pressure for online education, supporting online teaching for hundreds of thousands of teachers and online learning for millions of students.
Portability
Container technologies have promoted the standardization of cloud computing. Containers have become the standard for application distribution and delivery and can decouple applications from the underlying runtime environment. Kubernetes has become the standard for resource scheduling and orchestration. It shields differences of underlying architectures and allows applications to run smoothly on different infrastructures. The Cloud-Native Computing Foundation (CNCF) provides Certified Kubernetes Conformance Programs to ensure compatibility with different Kubernetes implementations. By using container technologies, it will be easier to build application infrastructures in the age of the cloud.
Kubernetes: Infrastructure in the Cloud-Native Era
Kubernetes has become a cloud application operating system. More and more applications are running on Kubernetes, such as stateless web applications, transactional applications (databases and message-oriented middleware), and data-based intelligent applications. The Alibaba economy also implements comprehensive cloud-native migration to the cloud base on container technologies.
Alibaba Cloud Container Service products provide an enterprise container platform within Alibaba Cloud, edge computing, and Apsara Stack environments. The core of Alibaba Cloud Container Service products is Alibaba Cloud Container Service for Kubernetes (ACK) and Serverless Kubernetes (ASK.) They are built on a foundation of a series of Alibaba Cloud infrastructure capabilities, such as computing, storage, networking, and security. In addition, they provide standardized APIs, optimized capabilities, and enhanced user experience. ACK is certified by the Certified Kubernetes Conformance Program and provides a series of core capabilities required by enterprises, such as security governance, end-to-end observability, multi-cloud, and hybrid cloud.
Alibaba Cloud Container Registry (ACR) is the core of asset management for enterprise cloud-native applications. It can manage application assets, such as Docker images and Helm charts, and can be integrated with continuous integration and continuous delivery (CI/CD) tools for a complete DevSecOps process.
Alibaba Cloud Service Mesh (ASM) is a platform for fully managing the traffic of microservice-oriented applications. It is compatible with Istio, supports unified traffic management of multiple Kubernetes clusters, and provides consistent communication, security, and observability for application services in containers and virtual machines.
This section describes the topology of a managed Kubernetes cluster. The Kubernetes cluster managed by ACK is based on the Kubernetes architecture. Master nodes of the Kubernetes cluster run on the control plane (a Kubernetes cluster) of the Virtual Private Cloud (VPC) network.
ACK adopts the default high-availability architecture design, where three etcd replicas run in three different zones, respectively. Two etcds are also provided according to scalability best practices. One etcd stores configuration information and the other stores system events. This improves the availability and scalability of etcds. Master nodes of the Kubernetes cluster, such as API Server and Scheduler, are deployed with multiple replicas and run in two different zones. Master nodes can be elastically expanded based on the workload, and worker nodes access the API Server through the Server Load Balancer (SLB.) This design ensures that the Kubernetes cluster runs properly, even if a zone becomes faulty.
Worker nodes run on the VPC network. You can run the nodes in different zones and use the zone-based anti-affinity feature of the application to ensure the high availability of the application.
Elasticity is a core capability of the cloud. Only the robust elastic computing power provided by the cloud can support the typical traffic pulse scenarios, such as the Double 11 Global Shopping Festival and the rapid growth of traffic for online education and collaborative office work after the COVID-19 pandemic. Kubernetes can maximize the elasticity of the cloud.
ACK provides various elasticity policies at the resource layer and application layer. The current mainstream solution at the resource layer is to scale nodes in or out by using cluster-autoscaler (CA.) When a pod fails to be scheduled due to insufficient resources, CA automatically creates nodes in the node pool based on the application workload.
Elastic Container Instance (ECI) provides a serverless container runtime environment based on lightweight virtual machines. You can schedule and run applications on instance groups in ACK. This is suitable for offline big data tasks, CI/CD jobs, and burst business scaling. On the Weibo app, 500 ECI pods can be scaled out in 30 seconds to easily respond to burst events.
At the application layer, Kubernetes provides Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Alibaba Cloud provides metrics-adapters to support more elasticity metrics. For example, you can adjust the number of pods for an application based on the queries per second (QPS) of the ingress. In addition, the resource profiles of many application workloads are periodic. For example, the business peak of the securities industry is the opening time of the stock market on weekdays. The resources required for the peak are 20 times those for the valley. To solve this problem, Alibaba Cloud Container Service provides a scheduled scaling component so developers can define a scheduled scaling policy to scale out resources in advance and reclaim resources regularly at the valley. This can balance the stability and resource costs of the system well.
Kubernetes provides powerful functions and flexibility, but it is extremely challenging to operate and maintain a Kubernetes production cluster. Even if a managed Kubernetes service is used, you need to retain the worker node resource pool and perform routine maintenance on the worker nodes, such as upgrading the operating system and installing security patches. You also need to plan the capacity at the resource layer based on your resource usage.
To address the complex O&M of Kubernetes clusters, Alibaba Cloud launched ASK. Compatible with Kubernetes applications, ASK enables Kubernetes O&M to be done on cloud infrastructures. This allows developers to focus on the applications.
For serverless containers, we provide two technical solutions: ACK on ECI and ASK.
ACK clusters are functional and flexible, which meets the demands of large Internet enterprises and traditional enterprises. You can run different applications and jobs in an ACK cluster. ACK clusters are intended for Site Reliability Engineering (SRE) teams in enterprises, allowing them to perform customized development and flexible control for Kubernetes.
ACK clusters support the following container runtime technologies.
ECI applies to Kubernetes clusters in the following scenarios:
ASK is a customized container for independent software vendors (ISVs), departments of large enterprises, and small- and medium-sized enterprises. You can create and deploy Kubernetes applications without the Kubernetes management and O&M capabilities, which greatly simplifies the management and is suitable for scenarios such as application hosting, CI/CD, AI, and data computing. For example, you can use the ASK and Graphics Processing Unit (GPU) instance groups to build an O&M-free AI platform. You can also create a machine learning environment on-demand. In either case, the overall architecture is very simple and efficient.
The cloud-native distributed application architecture has the following features: high availability, auto scaling, fault tolerance, easy management, high observability, standardization, and portability. We can build a cloud-native application reference architecture on Alibaba Cloud that includes:
End-to-End Elastic Application Architecture: You can containerize frontend applications and business logic, deploy them in a Kubernetes cluster, and configure HPA based on the application load.
At the backend data layer, you can use cloud-native databases such as Apsara PolarDB. Apsara PolarDB uses storage-computing separation architecture and supports scale-out. With the same specification, the performance of Apsara PolarDB is seven times that of the MySQL database, while the cost is half of the MySQL database.
Systematic High-Availability Design:
This ensures the zone-based availability of the entire system and can tolerate one failed zone.
Application High Availability Service (AHAS) provides the architecture awareness capability and can visualize the system topology. Moreover, AHAS provides the application inspection capability to detect availability issues, for example, whether the number of application replicas meets the availability requirements, and whether multi-zone disaster recovery is enabled for ApsaraDB for RDS (RDS) instances.
In a large-scale distributed system, various stability or performance problems may occur in infrastructures (networks, computing nodes, and operating systems) or applications. Observability helps you understand the status of the distributed system and make decisions accordingly. It also serves as the basis for auto scaling and automated O&M.
In general, observability consists of several important aspects:
Logging (Event Streams)
We provide a complete log solution based on Log Service (SLS) to collect and process application logs and provide capabilities such as ActionTrail and Kubernetes event centers.
Monitoring Metrics
Observability provides comprehensive monitoring of infrastructure services, such as ECS, storage, networking, and CloudMonitor. For business application performance metrics, such as the heap memory usage of Java applications, Application Real-Time Monitoring Service (ARMS) provides comprehensive performance monitoring for Java and PHP applications without modifying business code. For Kubernetes applications and components, ARMS provides managed Prometheus services, various OOTB preset monitoring dashboards, and APIs to facilitate third-party integration.
End-to-End Tracing
Tracing Analysis provides developers with comprehensive tools for distributed application trace statistics and topology analysis. It can help developers quickly locate and troubleshoot performance bottlenecks in distributed applications and improve the performance and stability of microservice-oriented applications.
From DevOps to DevSecOps
Security is an enterprises’ biggest concern about container technologies. To systematically improve the security of container platforms, we need to perform comprehensive security protection. First, we need to upgrade DevOps to DevSecOps, emphasizing the need to integrate security concepts into the entire software lifecycle and perform security protection in the development and delivery phases.
ACR Enterprise Edition provides a complete security software delivery chain. After you upload images, ACR can automatically scan the images to detect common vulnerabilities and exposures (CVEs.) You can then use the Key Management Service (KMS) to automatically add digital signatures to the images. You can configure automated security policies in ACK. For example, only the images that have been scanned and meet the launch requirements in the production environment can be released. This way, the entire software delivery process is observable, traceable, and policy-driven. This ensures security and improves delivery efficiency.
During runtime, applications face many risks, such as CVEs and virus attacks. Alibaba Cloud Security Center provides security monitoring and protection for applications during runtime.
Alibaba Cloud Security Center can monitor container application processes and networks, and detect application exceptions and vulnerabilities in real-time. When Alibaba Cloud Security Center detects a problem, it notifies you by email or SMS and automatically isolates and rectifies the problem. For example, a mining worm virus can exploit your configuration errors to launch attacks on container clusters. In this case, Alibaba Cloud Security Center can help you easily locate and clear the virus.
ASM
In February 2020, we released the first fully managed and Istio-compatible ASM in the industry. The control plane components of ASM are managed by Alibaba Cloud and independent of user clusters on the data plane. The hosting mode greatly simplifies the deployment and management of the Istio service mesh and decouples the lifecycle of the service mesh from the Kubernetes clusters. This makes the architecture simpler and more flexible and the system more stable and scalable. ASM integrates the Alibaba Cloud observability service and SLS based on Istio, which helps you manage applications in the service mesh more efficiently.
On the data plane, ASM supports various computing environments, including ACK Kubernetes clusters, ASK clusters, and ECS virtual machines. Cloud Enterprise Network (CEN) and ASM can implement service mesh between Kubernetes clusters across multiple regions and VPC networks. This enables ASM to implement traffic management and phased release for large-scale distributed applications in multiple regions. ASM will soon support multi-cloud and hybrid clouds.
Cloud migration has become inevitable. However, due to business data sovereignty and the security privacy of some businesses, enterprises can use the hybrid cloud architecture but cannot directly migrate their businesses to the cloud. Gartner predicts that 81% of enterprises will adopt multi-cloud or hybrid clouds. Hybrid cloud architecture has become a new norm for an enterprises’ cloud migration.
Traditional hybrid cloud architecture is designed to abstract and manage cloud resources. However, differences in infrastructures and security architecture capabilities between different cloud environments can separate an enterprise’s IT architecture from its O&M system. This makes hybrid cloud implementation more complex and increases O&M costs.
In the cloud-native era, technologies, such as Kubernetes, shields infrastructure differences for better centralized resource scheduling and application lifecycle management in hybrid cloud environments. Application-centric hybrid cloud architecture 2.0 is now available.
The following lists several typical scenarios:
Based on ACK, the hybrid cloud network, storage gateway, and database replication capabilities of Alibaba Cloud, we can help enterprises build a new hybrid cloud IT architecture.
Hybrid Cloud Architecture 2.0
ACK provides a centralized cluster management capability. In addition to Alibaba Cloud Kubernetes clusters, ACK can also manage your Kubernetes clusters in the on-premises Internet data center (IDC) and on other clouds. The centralized control plane enables unified security governance, observability, application management, backup, and recovery for multiple clusters. For example, SLS and managed Prometheus services can provide you with a unified observability dashboard for off-premises and on-premises clusters without code invasion. Security Center enables AHAS to help you detect and rectify security and stability risks in the hybrid cloud architecture.
ASM provides a unified service governance capability, which enables access to the nearest service, failover, and phased release with the multi-region and hybrid cloud network capabilities provided by CEN and Smart Access Gateway (SAG.) This compound solution can be used in scenarios, such as cloud disaster recovery and active geo-redundancy, to improve business continuity.
Cloud-Native Hybrid Cloud Solution
UniCareer is an e-learning career development platform that serves users in many regions around the world. Its applications are deployed in multiple Kubernetes clusters in four regions of Alibaba Cloud. In these clusters, CEN is used to connect multiple cross-region VPC networks. An ASM instance is used to manage the traffic of microservice-oriented applications in multiple Kubernetes clusters.
Service routing policies are centrally managed by the ASM control plane and delivered to multiple Kubernetes clusters. User requests are distributed to the ingress gateway in the nearest region through Domain Name System (DNS.) Then, the service endpoints are accessed in this region first through ASM. If services in this region are unavailable, the requests are automatically routed to other regions for traffic switching.
Cloud-Native Hybrid Cloud Management
The hybrid cloud solution of Alibaba Cloud has the following features:
Hitless Migration of Windows Containers to the Cloud
Now, let’s talk about support for Windows containers. As of 2020, the Windows operating system still dominates the market, with a market share of 60%. Enterprises use a large number of Windows apps, such as Enterprise Resource Planning (ERP), Customer Relationship Management (CRM), and ASP.Net. Windows containers and Kubernetes enable you to implement containerized delivery without rewriting the code of .Net applications. This maximizes the elasticity and agility of the cloud to achieve fast iteration and scaling of business applications.
ACK supports Windows 2019 in Kubernetes container clusters:
1) Provide a consistent user experience and unified capabilities for Linux and Windows applications.
2) Support hybrid deployment and interconnection of Linux and Windows applications in a cluster. For example, PHP applications running on Linux nodes can access the SQL Server database running on Windows nodes.
The following briefly introduces the cloud-native marketing strategy of Alibaba Cloud.
New cornerstone: Container technology allows users to use cloud resources. Cloud-native technology helps quickly deliver the value of the cloud.
New computing power: The innovation of the cloud-native-based software and hardware integration technology improves computing efficiency and accelerates intelligent business upgrades.
New ecosystem: We will provide the technology ecosystem and the Global Partner Program to enable more enterprises to enjoy the benefits of Alibaba’s technologies in the age of the cloud.
Get to know our core technologies and latest product updates from Alibaba’s top senior experts on our Tech Show series
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
2 
2 claps
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cote/private-cloud-all-up-in-my-grits-plus-cloud-native-enterprise-architecture-cot%C3%A9-memo-33-59deb139feab?source=search_post---------182,"Sign in
There are currently no responses for this story.
Be the first to respond.
Coté
Jul 15, 2017·4 min read
This week, news around Microsoft’s Azure Stack drove a lot of private cloud talk around these parts. Also, I’ve been trying to figure out what “Cloud-native Enterprise Architecture” means. Links and notes below.
Also, if you’re into Software Defined Talk, we’ve created a Patreon account. One, if you just want to give us cash, hey, we’ll take it. And two, we’re trying to figure out member only content. I think we’ll try with some bonus episodes where we do extended/extra analysis of white papers, surveys, and other tech industries studies and things. Go check it out and it’d be mighty fine if you helped us out.
Software Defined Talk: Private cloud is the Reuben sandwich of clouds, or, Shafer’s Theory of (Private) Cloud — www.softwaredefinedtalk.com
Microsoft will ship it’s private cloud stack, Azure Stack, in September. Will this work? Will people buy it? What could you even put in that cloud? You can feel that pull people have towards private cloud, so we’re looking forward to what happens. On a related topic, by our reckoning, kubernetes to small to have already fallen.
Pivotal Conversations: It’s private cloud all over again — soundcloud.com
There’s some exciting private cloud news on the horizon with Microsoft’s Azure Stack coming out in September. We discuss the brief history of private cloud and several models people have tried, along with some other news from the infrastructure software world. With no guest, Richard and I discuss some projects we’re working from cloud-native .Net, enterprise integration, and enterprise architecture.
Also, just for y’all paying close enough attention, here’s a link to a transcript.
That Moment: “It was essentially disrupting ourselves” — soundcloud.com
This isn’t a podcast that I do, but it’s a new Pivotal podcast that fits in the same topic area. Check it out!
I get asked a lot about “enterprise architecture” in a cloud-native/DevOps world. The stock answers are pretty terrible: “HAH-HAH-HAH!”
I’ve been looking at some old sources and talking with folks who work in large organizations on this topic to see what’s happening. Also, I’m putting together a talk on the topic (you know, once I figure it out).
My main theory is that enterprise architects only matter for large scale. “Cloud-native” (and DevOps) changes two things: faster release cycles and automating most all of the infrastructure. I’m not sure these “cancel” out the benefits of enterprise architects, so/but the question is: what does this role need to change about what it does to keep being helpful? Can 100’s of cloud-native teams just operate on their own with no EA support? Does some other team take on the EA function (with the help of automating everything)?
Is it just that the traditional tools and timelines of EAs are now crap, or that the whole function is now crap?
So…settling on a traditional definition of enterprise architecture (based on Enterprise Architecture as Strategy, 2006):
Continuous Lifecycle London, CFP open — continuouslifecycle.london
The Register’s DevOps-y conference is May 16th to 18th in London next year. I submitted a talk to it, and, of course, write a monthly column for them. Submit a talk yourself!
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
See all (153)
3 
3 claps
3 
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@IBMDeveloper/build-cloud-native-apps-faster-for-kubernetes-with-kabanero-a-new-open-source-project-from-ibm-e7b5afd70334?source=search_post---------183,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jul 23, 2019·1 min read
As companies modernize their infrastructure and adopt a hybrid cloud strategy, they’re increasingly turning to Kubernetes and containers. Choosing the right technology for building cloud-native apps and gaining the knowledge you need to effectively adopt Kubernetes is difficult. On top of that, enabling architects, developers, and operations to work together easily, while having their individual requirements met, is an additional challenge when moving to cloud.
To lower the barrier of entry for developers to use Kubernetes and to bring together different disciplines, IBM created new open source projects that make it faster and easier for you to develop and deploy applications for Kubernetes.
Today at OSCON 2019, we are excited to announce the creation of three new open source projects — Kabanero, Appsody, and Codewind — that developers can use to build cloud-native apps faster for Kubernetes.
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
3 
3 
3 
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/@GiantSwarm/your-guide-to-the-cloud-native-stack-1dfd5f779e8b?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Aug 28, 2019·2 min read
If you are in IT, then you couldn’t have failed to notice the buzz around ‘cloud-native’ in recent years. Everyone talks about it and whole conferences have popped up to discuss it. There’s even a prominent, vendor-neutral foundation in the Cloud-Native Computing Foundation (CNCF) whose purpose is to promote ‘cloud-native’ computing.
But what is it, and what benefits does it provide? At Giant Swarm we have put together a guide to explain just that and to walk you through some of the basics.
In this guide, we will explain what Cloud-Native is and we’ll be looking beyond the hype to highlight the main benefits your organization can gain from migrating to the cloud.
But as you know, there are no free lunches and benefits do not come without a cost. In our guide we will address common pitfalls to avoid when adopting a cloud-native architecture. The challenges which will be addressed in detail in the guide are:
Whether you are planning to join the cloud-native revolution, or have already started the journey. This guide will help you navigate through the wide range of technology and confusing choice. Download your guide here.
Written by Puja Abbassi — Developer Advocate @Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
See all (60)
12 
12 claps
12 
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/paving-the-way-in-cloud-native-alibaba-provides-enterprises-with-powerful-solutions-b345733b1ce1?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 29, 2019·6 min read
Watch Alibaba Cloud’s webinar on Bringing a Microservice App to Managed Kubernetes in 10 Minutes to learn more about setting up Kubernetes clusters on Alibaba Cloud with Container Service for Kubernetes.
In 2015, the first ever KubeCon + CloudNativeCon was held in San Francisco, with just over 200 attendees. But fast forward to the second KubeCon, this time held in China, over 3,000 attendees come from all over the globe — making KubeCon one of the biggest workshops of its kind. As predicted by Gartner, we are now entering the Cloud Native Era, where within the next three years, 75% of global enterprises will deploy containerized applications in production.
A forerunner in developing cloud native technologies and applications, Alibaba Cloud has shared 26 insightful presentations to the attendees of this year’s KubeCon + CloudNativeCon, of whom many are representatives of various global enterprises and others prominent developers. Ding Yu, the director of Alibaba Cloud Container Platform, pointed out:
“Cloud native is reshaping the entire software lifecycle. Containers, Kubernetes, and cloud native have become three key standards in the age of cloud. Alibaba Cloud will double down on its development of cloud native technologies and products, while maintaining its role in giving back to the open-source community. We are also collaborating with ecosystem partners to set cloud native standards and implement cloud native applications.”
Alibaba was one of the first companies in China to actively develop cloud native technologies. During his keynote presentation on June 26, Ding Yu emphasized that, “as early as 2011, Alibaba started to implement containerization processes, deploying and implementing cloud native technologies in such industries as e-commerce, finance, and manufacturing.”
The effort finally paid off after nine years, at which time the accumulation of technical knowledge gained from this entire process placed Alibaba Cloud as the only enterprises in China to be listed in Gartner’s Competitive Landscape: Public Cloud Container Services report.
Alibaba Cloud now has distinguished itself from its competition with four core advantages when it comes to cloud native technology and applications by having:
In March this year, Jianfeng Zhang, President of Alibaba Cloud Intelligence, said that within the next two years, Alibaba will run all its businesses on public cloud and increase investment in developing cloud native technologies.
Based on nine years of technical best practices within Alibaba Group, Alibaba Cloud now has the most extensive range of cloud native products in China. With over 20 different products of 8 categories built using cloud native, the technology has been implemented to build infrastructure, analyze data intelligence, and implement distributed applications in a wide range of industry scenarios.
Alibaba Cloud has applied its cloud native technology and applications to various core businesses of the Alibaba Group, especially in E-commerce and City Brain. During last year’s Double 11 global shopping festival, Alibaba was able to achieve the tremendous feat of deploying over 1,000 servers and millions of containers within 10 minutes. In just two short years, Alibaba has migrated the entirety of its business to the cloud, becoming the largest cloud native practice in the world.
Since embracing open-source standards, Alibaba Cloud has become an active contributor to the cloud-native community. The most prominent contribution of Alibaba Cloud is its contributions to open source projects that help advance cloud native services, such as resource orchestration and scheduling, job management, and serverless frameworks.
In January this year, Xiang Li, senior technical expert of Alibaba Cloud, became the first Chinese engineer to be selected as the CNCF TOC representative. He is dedicated to promoting the implementation of cloud native technologies.
Alibaba Cloud does not only provide cloud native technology and applications to Alibaba Group, but also provides these as public offerings to third-party groups. Alibaba Cloud Container for Kubernetes (ACK) is a container service offered in 18 regions worldwide. ACK is ranked as the largest public cloud container cluster and largest customer base in China of its kind.
During the conference, Alibaba Cloud unveiled two new services: its edge container service, ACK@Edge, and a management and delivery system for cloud native applications. Alibaba Cloud hopes that these services will be able to spur growth in cloud native scenarios by improving the development efficiency of cloud applications.
Ding Yu noted that: “With the advent of 5G and Internet of Things (IoT) technologies, centralized storage and computing for traditional cloud computing centers alone cannot meet the needs of the end devices using these services at least in terms of service immediacy, storage capacity, and computing power. Cloud native technologies will allow us to deliver cloud capabilities to end devices and edges, while also managing the delivery, O&M, and administrative control in a centralized fashion. This is the next step in the evolution of cloud computing.”
ACK@Edge distributes applications in a cloud-edge-end integrated mode, supports application distribution and lifecycle management in different system architectures and network conditions. It also optimizes access protocols, synchronization mechanisms, and security mechanisms for edges and devices. Using Terway, a self-developed and high performance network plug-in, ACK@Edge allocates ENIs to container instances to place them at the same network plane as ECS instances. As a result, the performance is 20% higher than that of traditional overlay container networks.
The management and delivery system for cloud native applications includes Cloud Native App Hub, the first cloud native application center in China, and OpenKruise, an automation engine for cloud native applications.
The open-source project OpenKruise is developed based on years of real world implementation in the large-scale deployment, release, and management of applications in the Alibaba Group ecosystem. This project also solves the challenges of automation management present in Kubernetes. OpenKruise will continue to explore more Kubernetes-based automation capabilities in processes such as deployment, upgrade, elastic scaling, QoS adjustment, health check, and migration and repair.
Comprehensive stress testing, rapid elastic scaling, and cloud native full-stack technology have been widely applied in Internet, finance, retail, manufacturing, industries. These practices lower the threshold for application development, enabling enterprises to make full use of the technical benefits of cloud native.
Cloud native architectures can streamline drills and practices in a cloud-based environment, improving efficiency and reliability even during unexpected traffic spikes. Alibaba Cloud ACK provides high elasticity of businesses at the container application layer. Furthermore, PolarDB offers capabilities of database scaling. PTS implements full-link stress testing by simulating actual business traffic.
Successful container-based applications include the following:
If you don’t have an Alibaba Cloud account, sign up for a New User Free Trial and get $300–1200 USD worth in free trial products.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
5 
5 
5 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/building-a-cloud-native-feed-streaming-system-with-apache-kafka-and-spark-on-alibaba-cloud-part-739932f5d65b?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 14, 2021·11 min read
By Chi Wai Chan, Product Development Solution Architect at Alibaba Cloud
The first part of the article series (Part A) is available here.
There are three parts to this series:
After reading Part A, you should have all of the components required (VPC, Kafka, HBase, and Spark virtual cluster) on Alibaba Cloud. Part B illustrates the building of the streaming processing solution on spark streaming with the serverless Data Lake Analytics (DLA) service for the “Feed Streams Processer” part. The source code from this demo is available here.
Alibaba Cloud’s Data Lake Analytics (DLA) is an end-to-end, on-demand, serverless data lake analytics and computing service. It offers a cost-effective platform to run Extract-Transform-Load (ETL), machine learning, streaming, and interactive analytics workloads. DLA can be used with various data sources, such as Kafka, HBase, MongoDB, Object Storage Service (OSS), and many other relational and non-relational databases. For more information, please visit the Alibaba Cloud DLA product introduction page.
DLA’s Serverless Spark is a data analytics and computing service that is developed based on the cloud-native, serverless architecture. To submit a Spark job, you can perform simple configurations after you activate DLA. When a Spark job is running, computing resources are dynamically assigned based on task loads. After the Spark job is completed, you are charged based on the resources consumed by the job. Serverless Spark helps you eliminate the workload for resource planning and cluster configurations. It is more cost-effective than the traditional mode. Serverless Spark uses the multi-tenant mode. The Spark process runs in an isolated and secure environment.
A virtual cluster is a secure unit for resource isolation. A virtual cluster does not have fixed computing resources. Therefore, you only need to allocate the resource quota based on your business requirements and configure the network environment to where the destination data that you want to access belongs. You do not need to configure or maintain computing nodes. You can also configure parameters for Spark jobs of a virtual cluster. This facilitates unified management of Spark jobs. A Compute Unit (CU) is the measurement unit of Serverless Spark. One CU equals one vCPU and 4 GB of memory. After a Spark job is completed, you are charged based on the CUs that are consumed by the Spark job and the duration for running the Spark job. The pay-as-you-go billing method is used. The computing hours are used as the unit for billing. If you use one CU for an hour, the resources consumed by a job are the sum of resources consumed by each computing unit.
To access resources in your VPC, you must configure the AccessKey ID and AccessKey secret. The resources include ApsaraDB RDS, AnalyticDB for MySQL, PolarDB, ApsaraDB for MongoDB, Elasticsearch, ApsaraDB for HBase, E-MapReduce Hadoop, and self-managed data services hosted on ECS instances. The Driver and Executor processes on the serverless Spark engine are running in a security container.
You can attach an Elastic Network Interface (ENI) of your VPC to the security container. This way, the security container can run in the VPC as an ECS instance. The lifecycle of an ENI is the same as that of a process on the serverless Spark engine. After a job succeeds, all ENIs are released. To attach an ENI of your VPC to the serverless Spark engine, you must configure the security group and VSwitch of your VPC in the job configuration of the serverless Spark engine. In addition, you must make sure that the network between the ENI and service that stores the destination data is interconnected. If your ECS instance can access the destination data, you only need to configure the security group and VSwitch of the ECS instance on the serverless Spark engine.
You can click this link to download the dla-spark-toolkit.tar.gz package or use the following wget command to download the package in a Linux environment. For more details, you can refer to the DLA serverless spark document.
After you download the package, decompress it.
The spark-defaults.conf file allows you to configure the following parameters:
You should replace the parameter in square bucket [xxxxx] for your environment.
For mappings between regions and regionIds, please see the Regions and Zones document here.
After configuring all of the requirement parameters, you can submit your spark job to the DLA virtual cluster by running the following command:
This demo will provide a step-by-step guide on building a simplified feed streams processing systems with the following tasks, as shown in Figure 1.
The simple Feed Streams Processor program with spark structured streaming on:
You can use any relational database management system (RDBMS) or integrated development environment (IDE) to access the HBase via Phoenix. I will use DBeaver for this demo. You can download the DBeaver Community Edition for free.
You can download the phoenix jar file from this link or any open-source compatible version. Unzip the gz and tar files. You have to extract the ali-phoenix-5.2.4.1-HBase-2.x-shaded-thin-client.jar for DBeaver.
After installing DBeaver, you can create a new connection to the database, and search for the “Apache Phoenix” database type. Then, click “Next.” Go to “Edit Driver Settings,” then “Add File,” and configure the phoenix jdbc driver to the previously extracted ali-phoenix-5.2.4.1-HBase-2.x-shaded-thin-client.jar file, as shown in Figure 2.
Click the “Connection Properties” tab in Figure 2, and configure phoenix.schema.isNamespaceMappingEnabled and phoenix.schema.mapSystemTablesToNamespace to false, as shown in Figure 3. Lastly, click ""OK"" to complete the driver setup, and provide the connection URL provided in the ""Server Load Balancer Connection"" from the SQL Service of the HBase provisioned in the previous section.
Now, you are connected to the HBase instance via the Phoenix Query Server. You can create the Phoenix table, which is mapped to the HBase table and corresponding secondary global index for a simple search using the supported SQL syntax. You can use the Phoenix syntax link for reference.
To create a table with mapping to HBase:
To create a global secondary index to the support filter query:
You can run the following select statement to verify the table creation, if there is a return with zero rows, then, you are finished with the Phoenix and HBase table creation.
Go to this link and copy the price and volume information for the Top 30 Components of Hang Seng Index. Convert them into .csv files, and then upload them to MongoDB using this tutorial.
There are many dependencies. It is recommended to set up a maven project with a maven-shade-plugin to compile an uber jar for simple deployment. You can refer to this POM file to set up your maven project.
The following sample code simulates publishing a very simple message to a particular Kafka Topic with a key:value pair. The key is the ""ip"" address of the sender, and the value is ""stock_code"" with the timestamp information.
The following sample code simulates:
We recommend using the Apache Spark Plugin to save the spark dataframe into HBase. The phoenix-spark plugin extends Phoenix’s MapReduce support to allow Spark to load Phoenix tables as DataFrames, and enables persisting them back to Phoenix. The phoenix-spark integration can leverage the underlying splits provided by Phoenix to retrieve and save data across multiple workers. Only a database URL and a table name are required. Optional SELECT columns can be given, as well as pushdown predicates for efficient filtering. You can refer to this document for more details.
In local mode, after running the program with the publisher program on sending messages to Kafka MQ, and also the program with subscriber program on receiving and enriching message from Kafka MQ, save the transformed records into HBase.
After a few seconds (e.g., 10 seconds), you can access the DBeaver (RDBMS IDE) to make the following queries:
You can retrieve the enriched records from HBase.
You can also access the DBeaver (RDBMS IDE) to make the following queries with filtering:
You can retrieve the enriched records from HBase with filtering.
You should compile and package the scala project into an uber jar while running “mvn clean install,” as shown in the figure below. The uber jar is now available under the ""[workspace_directory]\target\"".
Send the Uber jar to a Linux environment. Afterward, you can use the previously configured “Spark-SQL Command Line Tool” to send the task to DLA for large scale production deployment with the following command:
Go to the search bar under the “Products and Services” dropdown menu by clicking on the top left menu icon. Type “data lake analytics” in the search bar, as shown below, and click on “data lake analytics” under the filtered results. After that, you will be redirected to the DLA product landing page.
Click “Submit Job” on the menu bar on the left hand side. Select your virtual cluster (“test-hk”) and the spark tasking submitted will be listed on the data grid in the button of the window. For this particular task, you can click on the “Operation” dropdown button, and click “SparkUI.” You will be redirected to the corresponding spark history server for detailed task information and executor logs.
Congratulations, you have successfully deployed your first spark program to the Data Lake Analytics Serverless Virtual Cluster on Alibaba Cloud. Enjoy yourself! In the next section (Part C), I will show you how to deploy a simple machine learning model on Spark MLlib for building an intelligent distribution list and personalized content distribution.
Source: https://www.linkedin.com/pulse/building-simple-intelligence-cloud-native-feed-streaming-chi-wai-chan/
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
3 
3 
3 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.datadriveninvestor.com/fluid-an-important-piece-for-big-data-and-ai-to-embrace-cloud-native-5093f2ec9326?source=search_post---------187,"There are currently no responses for this story.
Be the first to respond.
By Gu Rong, Che Yang, Fan Bin
In recent years, more and more AI and big data applications are being deployed and run on cloud-native orchestration frameworks such as Kubernetes. This is because containers bring about efficient deployment and agile iteration, as well as exaggerate the natural advantages of cloud computing in terms of resource costs and elastic scaling. However, before Fluid occurs, the Cloud Native Computing Foundation (CNCF) landscape lacks a native component to help these data-intensive applications efficiently, securely, and conveniently access data in cloud-native scenarios.
Efficiently driving operations of big data and AI applications in cloud-native scenarios is a challenging issue that has both theoretical significance and application value. On the one hand, to solve this issue, we need to consider a series of theoretical and technical challenges in complex scenarios, such as application collaborative orchestration, scheduling optimization, and data caching. On the other hand, the solution to this issue can effectively promote big data and AI applications in a wide range of cloud-service scenarios.
To achieve systematic solutions, academics and industry experts need to cooperate closely. And that is why Dr. Gu Rong, an associate researcher at the Parallel Algorithms, Systems and Applications for Big Data Lab (PASALab) in Nanjing University, Che Yang, a senior technical expert at Alibaba Cloud container service, and Fan Bin, a founding member of the Alluxio project, jointly promoted the launch of the open source project, Fluid.
Fluid is an open-source cloud-native infrastructure project. Driven by the separation of computing and storage, Fluid aims to provide an efficient and convenient data abstraction for AI and cloud-native applications by abstracting data from storage, in order to achieve:
The data layer abstraction provided by Kubernetes enables data to be flexibly and efficiently moved, replicated, evicted, converted and managed between storage sources such as Hadoop Distributed File System (HDFS), Object Storage Service (OSS) and Ceph, and the computing of cloud-native applications. However, specific data operations are available to users, so users no longer have to worry about the efficiency of accessing remote data, the convenience of managing data sources, and how to help Kubernetes to make the O&M and scheduling decisions. Users only need to directly access the abstracted data through the most natural Kubernetes data volumes. The remaining tasks and underlying details are all submitted to Fluid.
Currently, the Fluid project focuses on two important scenarios: data set orchestration and application orchestration. The data set orchestration allows you to cache data of a specified data set to specific Kubernetes nodes. The application orchestration allows you to schedule an application to a node that can or has stored the specified data set. The two can also be combined to form a collaborative orchestration scenario, that is, node resource scheduling based on the data set and application requirements.
There are natural differences in the design concepts and mechanisms between the cloud-native environment and the big data processing frameworks. The Hadoop big data ecology, which was deeply influenced by Google’s three papers GFS, MapReduce, and BigTable, has believed and practiced the concept of “move computing instead of data” from the very beginning. Therefore, the data-intensive computing frameworks such as Spark, Hive, and MapReduce and their applications must consider the data locality architecture to reduce the data transmission. However, with the changes of the times, in order to give consideration to the flexibility and usage costs of resource expansion, the architecture of separation of computing and storage is widely used in the emerging cloud-native environment. Therefore, a component like Fluid is needed in the cloud-native environment to supplement the lack of data locality caused by the big data framework embracing the cloud-native technology.
In addition, in the cloud-native environment, applications are usually deployed in a stateless microservice manner and are not centered on data processing. Usually taking data abstraction as the center, the data-intensive frameworks and applications perform computing jobs and assign and execute tasks. After the data-intensive frameworks are integrated into the cloud-native environment, a scheduling framework centered on data abstraction is also needed, such as Fluid.
In view of Kubernetes’ lack of intelligent awareness and scheduling optimization for application data and the limitation that the data orchestration engine, taking Alluxio as an example, has difficulty in directly controlling the cloud-native infrastructure layer, Fluid proposes a series of innovative methods, such as the collaborative orchestration of data applications, intelligent awareness and joint optimization. It forms an efficient support platform for data-intensive applications in cloud-native scenarios.
The following figure shows the specific architecture:
We have provided a Demo video to show you how to use Fluid to speed up AI model training on the cloud. In this Demo, using the same ResNet50 test code, the Fluid acceleration has obvious advantages over the native ossfs direct access regardless of the training speed per second or the total training duration, with the training time shortened by 69%.
The Demo video
The Fluid must be running in Kubernetes v1.14 or later that supports Container Storage Interface (CSI). You can deploy and manage Fluid Operator by using Helm v3 on the Kubernetes platform. Before you run Fluid, make sure that Helm is correctly installed in the Kubernetes cluster. To install and use Fluid, please refer to the Getting Started Guide
Fluid allows Kubernetes to truly have the basic capabilities of the distributed data caching. Open source is only a starting point, and everyone’s participation is welcomed. If you find a bug or require a feature in the process of using it, you can directly submit the issue or Pull Request (PR) on GitHub to participate in the discussion.
Gu Rong, an associate researcher of the Department of Computer Science and Technology in Nanjing University, whose research direction is big data processing system, has published more than 20 papers in the frontier journals and conferences, such as the IEEE Transactions on Parallel and Distributed Systems (TPDS), IEEE International Conference on Data Engineering (ICDE), Parallel Computing, Journal of Parallel and Distributed Computing (JPDC), International Parallel and Distributed Processing Symposium (IPDPS) and International Conference on Parallel Processing (ICPP). His achievements have been applied to Sinopec, Baidu, Bytedance and other companies and Apache Spark. He won the first prize of Jiangsu Province Science and Technology in 2018 and the Youth Science and Technology Award of Jiangsu Computer Society in 2019. He was elected as the member of the Techinical Committee of Systems Software and the Task Force of Big Data, China Computer Federation, and the secretary general of the Big Data Committee of Jiangsu Computer Society.
Che Yang, a senior technical expert at Alibaba Cloud, specializes in the development of Kubernetes and container-related products. He focuses on how to use cloud-native technology to build machine learning platforms and systems and is the primary creator and maintainer of the GPU share scheduling.
Fan Bin is the member of Project Management Committee (PMC) of the open source project Alluxio and the maintainer of its source code. Before joining Alluxio, Fan Bin worked for Google and was engaged in the research and development of the next-generation large-scale distributed storage system. He received his doctorate from the Department of Computer Science at Carnegie Mellon University in 2013. During his doctorate, he was engaged in the design and implementation of distributed systems and was the creator of Cuckoo Filter.
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
62 
62 claps
62 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@alibaba-cloud/a-deep-dive-into-cloud-native-from-basics-to-applications-f1f6a44208d6?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 16, 2019·8 min read
The cloud native concept emerges as cloud computing enjoys rapid development. Cloud-native has become extremely popular. You will be considered outdated if you do not understand Cloud-native as of this year.
Although many people are talking about cloud native, few have told you exactly what cloud native is. Even after finding and reading some cloud-native materials, most of you may still feel confused and lack a complete understanding of cloud native. At this point, you might start to doubt your own intelligence. In my case, I always tend to blame the authors’ stupidity for my incomprehensibility of certain articles, though this is not necessarily true. However, this way of thinking prevents me from being held back by my own self-doubt and I can try to remain positive.
The reason why cloud native cannot be explicitly described is the lack of a clear definition. Since cloud native is undergoing constant development and changes, no individuals or organizations have the absolute right to define cloud native.
Technical changes are always heralded by certain ideologies, just as the invincible Marxism leads to the prosperity of proletarian revolutions.
Cloud native is an approach to building and running applications. It is a set of systematized techniques and methodologies. Cloud native is a compound word made up of “cloud” and “native”. The word “cloud” represents applications residing in the cloud instead of in traditional data centers. The word “native” represents applications that are designed to run on the cloud and fully utilize the elasticity and the “distributed” advantage at the very beginning of application design.
The cloud native concept was first put forward by Matt Stine at Pivotal in 2013. In 2015 when cloud native was just becoming popular, Matt Stine defined several characteristics of cloud-native architectures in the book Migrating to Cloud Native Application Architectures: 12-factor applications, microservices, self-service agile infrastructure, API-based collaboration, and anti-fragility. At an InfoQ interview in 2017, Matt Stine made some changes and indicated six characteristics of cloud-native architectures: modularity, observability, deployability, testability, replaceability, and handleability. The latest description about cloud-native architectures on the official Pivotal website shows four key characteristics: DevOps, continuous delivery, microservices, and containers.
In 2015, the Cloud Native Computing Foundation (CNCF) was founded. CNCF originally defined four characteristics of cloud-native architectures: containerized encapsulation, automated management, and microservices. In 2018, CNCF updated the definition of cloud-native architectures with two new features: service meshes and declarative APIs.
As we can see, different individuals and organizations have different definitions for cloud-native architectures, and even the same individual or organization has different definitions for cloud-native architectures at different points in time. This complexity makes it hard for me to clearly understand cloud-native architectures. After a while, I came up with a simple solution: to choose only one definition that is easy to remember and understand (in my case, DevOps, continuous delivery, microservices, and containers).
In a word, cloud-native applications are required to meet the following: Implement containerization by using open-source stacks like K8s and Docker, improve flexibility and maintainability based on microservices architectures, adopt agile methods, allow DevOps to support continuous iteration and automated O&M, and implement elastic scaling, dynamic scheduling, and efficient resource usage optimization by using cloud platform facilities.
Cloud native supports simple and fast application building, easy deployment, and allows applications to be scaled as needed. Cloud-native architectures bring many advantages over traditional web frameworks and IT models, and have almost no disadvantages. Cloud-native architectures are definitely a powerful secret weapon in this industry.
Microservices: Almost all the definitions of the cloud native concept include microservices. Microservices are opposite to monolith applications and based on Conway’s law, which defines how to split services and is not easy to understand. In fact, I think that any theories or laws are not simple and easy to understand, otherwise they would not sound professional as theories or laws are. The main point is that system architectures determine product forms. I am not sure if this also results from Marx’s view on the relationship between the productive forces and relations of production.
In a microservices architecture, after split by function, services have stronger decoupling and cohesion and therefore become easier. It is said that DDD is another technique to split services. Unfortunately, I don’t know much about DDD.
Containers: Docker is the most widely used container engine. For example, it is used a lot in infrastructures of companies like Cisco and Google. Docker utilizes LXC. Containers provide guarantees to implement microservices and play the role of isolating applications. K8s is a container orchestration system built by Google to manage containers and balance loads between containers. Both Docker and K8s are developed in the Go language and are really good systems.
DevOps: DevOps is a clipped compound of “development” and “operations” and has a relationship different from that between development and production. In fact, DevOps also includes testing. DevOps is an agile thinking methodology, a communication culture, and an organizational form with the goal of enabling continuous delivery for cloud-native applications.
Continuous delivery: Continuous delivery enables undelayed development and updates without downtime and is different from the traditional waterfall development model. Continuous delivery requires the co-existence of development versions and stable versions. This needs many support processes and tools.
First, cloud native is a result of cloud computing. Cloud native would not have come into existence without the development of cloud computing. Cloud computing is the basis of cloud native.
With the growing maturity of virtualization technologies and the popularity of distributed frameworks, it is now an irreversible trend to migrate applications to the cloud. This trend is also driven by open-source communities like container technologies, continuous delivery, and orchestration systems as well as development ideas like microservices.
The three layers in cloud computing, which are IaaS, PaaS, and SaaS, provide a technical base and directional guidance for cloud native. True cloudification does not only involve changes in infrastructures and platforms, but also requires proper changes in applications. Applications are required to abandon traditional methods and be re-designed based on the characteristics cloud in different phases and aspects of architecture design, development models, deployment, and maintenance. This allows us to create new cloud-based applications, that is, cloud-native applications.
It is obvious that we need new and cloud-native development to implement cloud-native applications. Cloud native includes many aspects: infrastructure services, virtualization, containerization, container orchestration, and microservices. Fortunately, open-source communities have made many significant contributions to cloud-native applications, and many open-source frameworks and facilities are directly available. After released in 2013, Docker quickly becomes an actual container standard. Released in 2017, k8s stands out among many container orchestration systems. These technologies have significantly reduced the threshold of developing cloud-native applications.
Although the cloud native introduction document may seemingly show a trace of exaggeration, as nitpicky as I am, I feel totally amazed at the advantages listed in the document. Cloud-native architectures are perfect. Does this mean that applications should immediately switch to cloud-native architectures? The ideal is perfect and tempting, until you try to switch the reality to that ideal. My view on this is to make a decision based on actual needs and consider if the current problems really affect your business development and if you can afford to re-design your applications.
Software design comes with two critical goals: high cohesion and low coupling. These two critical goals further lead to many specific design principles, including the single responsibility principle, the open–closed principle, Liskov substitution, dependency inversion, interface segregation, and least knowledge.
Software engineers have always been striving to achieve the two goals and write clearer, more robust software that is also easier to scale and maintain.
Later, more needs are added. Software development is expected to be simpler and faster. Programmers want to write fewer lines of code, and non-professionals also want the ability to develop applications. Easier programming languages are developed for people who do not have programming skills. More programming technologies and ideas are developed, including libraries, components, and cloud infrastructures.
As a result, many technologies are of less practical value, although they themselves are highly advanced. Many software engineers have new roles as parameter adjustment engineers, API call experts, library masters, and component specialists. This is an inevitable result from efficient division of labor and technological development.
From nearly 20 years of Internet technological development, we can see that the mainstream trend is the application of technologies in specific fields. Especially with the development and popularity of cloud computing in recent years, infrastructures have become more solid and business development is increasingly easier and has less technology requirements. At the same time, small enterprises and teams are no longer plagued by problems related to aspects like performance, loads, security, and scalability. This situation worries many middle-aged people working in the Internet industry, who may feel as if they could soon be out of a job.
Although it is undeniable that the technology is becoming a less important threshold in this industry, we do not have to be that pessimistic. Similar arguments also occurred when VB, Delphi, and MFC appeared in the PC programming era: What you see is what you can use and you can develop PC programs simply by clicking your mouse. Isn’t this cool? At the time, programmers probably have more concerns. However, after the division of the backend development with the development of the Internet industry, they soon found their new battleground came up with many new ideas about networking, distributed services, databases, support for large amounts of services, and disaster recovery.
If the basic infrastructure of the PC programming era is the control library and the infrastructure of the Internet era is the cloud, what about the infrastructure of the AI era? What high-end technologies will emerge in the AI era?
https://www.alibabacloud.com/blog/a-deep-dive-into-cloud-native-from-basics-to-applications_595025?spm=a2c41.13112025.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
2 
2 claps
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pingcap/tidb-on-kubesphere-release-a-cloud-native-distributed-database-to-the-kubesphere-app-store-cab26c99ccbc?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
Dec 23, 2020·6 min read
Author: Will Zhang (SRE at iSoftStone)
KubeSphere, an open-source, distributed operating system with Kubernetes as its kernel, helps you manage cloud-native applications on a GUI container platform. TiDB is an open-source, cloud-native database that runs smoothly on Kubernetes.
In my last blog post, I talked about how to deploy TiDB on KubeSphere. If you want TiDB to be available to tenants across the workspace, you can release the TiDB app to the KubeSphere public repository, also known as the KubeSphere App Store. In this way, all tenants can easily deploy TiDB in their project, without having to repeat the same steps.
In this article, I will walk you through how to deploy TiDB on KubeSphere by app templates and release TiDB to the App Store.
Before you try the steps in this post, make sure:
To deploy TiDB on KubeSphere, you need TiDB Helm charts. Helm helps you create, install, and manage Kubernetes applications. A Helm chart contains files that describe the necessary collection of Kubernetes resources. In this section, I’ll demonstrate how to download the required Helm charts and upload them to KubeSphere.
2. Add the PingCAP TiDB Helm repository:
3. View all the Helm charts in this repository:
Note that in this article, I use v1.1.6 charts. You can also get the latest version released by PingCAP.
4. Download the charts you need. For example:
5. Make sure they have been successfully pulled:
Now that you have Helm charts ready, you can upload them to KubeSphere as app templates.
I. In the upper left corner of the current page, click Platform to display the Access Control page.
II. In Workspaces, click Create to create a new workspace and give it a name; for example, dev-workspace as shown below.
2. Go to your workspace. From the navigation bar, select App Templates, and, on the right, click Upload Template.
My last blog post explained how to deploy TiDB using an app repository. This time, let’s upload Helm charts as app templates.
3. Select the Helm charts you want to upload to KubeSphere. After they are successfully uploaded, they appear in the list below.
2. After you create the project, navigate to Applications and click Deploy New Application.
3. In the Deploy New Application dialog box, select From App Templates.
4. Deploy TiDB Operator and the TiDB cluster. In the drop-down list, select From workspace and click tidb-cluster and tidb-operator respectively to deploy them. For more information about how to configure them, see my last post.
All Helm charts uploaded individually as app templates appear on the From workspace page. If you add an app repository to KubeSphere to provide app templates, they display in other repositories in the drop-down list.
App templates enable you to deploy and manage apps in a visual way. Internally, they play an important role as shared resources. Enterprises create these resources — which include databases, middleware, and operating systems — for the coordination and cooperation within teams.
You can release apps you have uploaded to KubeSphere to the public repository, also known as the App Store. In this way, all tenants on the platform can deploy these apps if they have the necessary permissions, regardless of the workspace they belong to.
3. From the navigation bar, click App Templates, and you can see the uploaded apps. To release it to the App Store, click tidb-operator.
4. On the Versions page, click the version number to expand the menu and click Submit Review.
KubeSphere allows you to manage an app across its entire lifecycle, including deleting the version, testing the deployment, or submitting the App for review. For an enterprise, this is very useful when different tenants need to be isolated from each other and are only responsible for their own part of the life cycle as they manage an app version.
5. Approve the app submitted for review. In the top left corner, click Platform and select App Store Management.
6. In the App Reviews page, click the app you just submitted.
7. In the App Info page that appears, review the app information and chart files. To approve the app, click Pass.
8. After the app is approved, you can release it to the App Store. In the top left corner, click Platform and select Access Control.
9. Go back to your workspace. From the navigation bar, select App Templates and click tidb-operator.
10. On the Versions page, click the version number again, and you can see that the status has reached Passed. The Submit Review button has changed to Release to Store.
11. Click Release to Store, and in the confirmation box, click OK.
12. To view the app released, in the top left corner, click App Store, and you can see it in the App Store. Likewise, you can use the same steps to deploy tidb-cluster to the App Store.
For more information about how to deploy an app from the App Store, see the KubeSphere documentation. You can also see Application Lifecycle Management to learn more about how an app is managed across its entire lifecycle.
Both TiDB and KubeSphere are powerful tools for us as we deploy containerized applications and use the distributed database on the cloud. As a big fan of open source, I hope both groups can continue to deliver efficient and effective cloud-native tools for us in production.
If you have any questions, don’t hesitate to contact us in Slack or GitHub.
Originally published at www.pingcap.com on Nov 17, 2020
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
4 
4 
4 
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
"
https://medium.com/@ledumjg/tidb-on-kubesphere-how-to-use-cloud-native-distributed-database-on-kubernetes-94532aae4aeb?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dr. Michael J. Garbade
Mar 31, 2021·3 min read
Introduction Kubernetes has, in the recent past, become the go to standard for building applications running multiple containers. Using Kubernetes comes with the need to run cloud-native distributed databases. One of the prominently emerging cloud-native, open-source NewSQL databases is TiDB.
By deploying TiDB on KubeSphere, this article demonstrates how you can have TiDB clusters powered by Kubernetes and how to manage the clusters using an easy to use web interface.
What is TiDB and KuberSphere? TiDB, an open-source NewSQL database, supports Hybrid Transactional and Analytical Processing (HTAP) workloads. TiDB is MySQL compatible and contains features such as; horizontal scalability, a strong consistency, and is highly available. TiDB provides users with a single, multi-purpose database solution, covering systems and services such as;
Online Transactional Processing (OLTP) Online Analytical Processing (OLAP), and Hybrid transactional/analytical processing (HTAP)
KubeSphere, on the other hand, is a distributed operating system managing cloud native applications with Kubernetes as its kernel, and provides plug-and-play architecture for the seamless integration of third-party applications to boost its ecosystem.
How do you deploy TiDB on KubeSphere? You will need a Kubernetes cluster before you deploy TiDB on KubeSphere. Installing the Kubernetes clusters require you to prepare either virtual or physical machines, and configuring network rules to enable smooth flow of traffic from one instance to the next.
How do you prepare the environments? KubeSphere can be installed on any infrastructure, including deploying Kubernetes by itself. Read the KubeSphere documentation for more insights. In this article, we use the QingCloud platform, provided by QingCloud, which sponsors KubeSphere. It is a highly-functional platform that enables fast deployment of Kubernetes and KubeSphere at the same time with just a few clicks.
Deploying the TiDB Operator The TiDB Operator is simply an automatic operation system used for TiDB clusters in Kubernetes. The operator provides a complete management life-cycle for TiDB, which includes deployment, scaling, upgrades, fail-over, backup, and any configuration changes. The TiDB Operator helps TiDB to run seamlessly in Kubernetes clusters that are deployed on public or private clouds. You should first deploy a TiDB Operator on KubeSphere, before you deploy a TiBD cluster.
Deploying a TiDB Cluster So, after you have deployed the TiDB Operator, you can move ahead and deploy a TiDB cluster on Kubernetes. This process of deploying a TiDB cluster is pretty much similar to deploying the TiDB operator.
How do you Access TiDB Clusters? Now that you have prepared your environment, deployed the TiDB operator and cluster, your apps are ready, and you may now need to focus on observability. The KubeSphere dashboard allows you to observe your applications throughout their lifecycle.
You then need to verify that the apps are up and running. Kubernetes lists all running services and the port in which each service is exposed to, for easy access.
Wrapping Up Many developers continue to move from cloud-hosted to cloud native environments. As this happens, the way DevOps builds applications and stores distributed data continues to change. This article describes how TiDB and KubeSphere are powerful tools for cloud-native applications. As Suenot describes in his project, “How to use Rancher (Kubernetes cluster) on Hetzner cloud” it is not possible to showcase all the features of TiDB and KubeSphere in a single post.
Originally published at https://dev.to on March 31, 2021.
Founder & CEO of Education Ecosystem. Serial entrepreneur with experience from Amazon, GE & Rebate Networks, https://www.education-ecosystem.com/
74 
74 
74 
Founder & CEO of Education Ecosystem. Serial entrepreneur with experience from Amazon, GE & Rebate Networks, https://www.education-ecosystem.com/
"
https://medium.com/@jystewart/cloud-native-organisations-7ef9953bba4f?source=search_post---------191,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Stewart
Mar 20, 2017·4 min read
“Cloud” is a nebulous term. At the start of both talks I explained that, while there are formal definitions that I’ve found useful, for me it’s just a useful term for starting a conversation about what’s currently happening at the collision point of “the internet” and “computing tools”. Most of the time for technology, I don’t draw a distinction between the impact of cloud, agile, devops and a number of other inter-related movements.
I talked in both settings about the work that we did in GDS and across government, of which the “Cloud First” policy was a significant part. They were both short talks and I was struggling for time. I should have gone into quite a bit more detail about GDS’ work when speaking in Manila.
For Cloud Expo Europe I drew out several lessons from the work in government. The primary lesson was that changing how we approach technology changes how we think about what government does.
I’ve talked before about how there’s a real opportunity to be much clearer about what needs to be bespoke and what we can consume. There are a set of areas where organisations think they have special requirements for their core technology. Most of the time that’s not true.
That is important, but the more transformative change is that as a set of technologies becomes more accessible it becomes part of our normal toolkit.
A much lower barrier to entry for technology lets cross-disciplinary teams incorporate technology development into policy and service design, and to iterate it in an operational setting. A set of traditional barriers are no longer necessary. Those cross-disciplinary teams–given freedom to meet their goals–are where we will get performance and innovation.
I went on to talk about some general trends in the technology world, once again citing Stephen’s Developers are the new Kingmakers. Top-down decision making isn’t enough to handle the pressure organisations are facing to change, or the scale of opportunities available through open source and cloud technologies.
What we see with the mature end of devops, and in much of the thinking behind “cloud native” architectures, is a strong focus on the team as the unit of delivery. And the team as the unit of responsibility. There are a set of architectural practices that support independent teams. They work where organisations set clear goals and principles.
Too often when we talk about “cloud” we either talk about infrastructure-as-a-service, or we talk about software-as-a-service. By and large they’re treated as very different things, but there are some common principles here that ought to be applicable to IT adoption more generally.
Software-as-a-service–particularly software that’s offered via a “freemium” model–allows a complete shift from a world where every new productivity tool has to be evaluated by a central IT department. People are expecting tools at work at least as good as those they have at home, and often will just adopt what they need whether the central department likes it or not.
As with many things we associate with cloud, that’s not new (though the scale has changed). It’s what we have been calling “Shadow IT”. With the shift to cloud we have the opportunity to change our approach.
In the talks, I cited the “Cloud Security Principles” as an example of how being more explicit about the things we care about lets us empower more people to make good decisions.
Those principles are an early illustration of how we should be thinking about cloud tools (IaaS or SaaS) in general. We should be clear about the principles that apply and help our people understand what we need to watch out for when choosing technology. Then we can help scale the use of the tools they find most useful.
There’s a lot more to do to take the thinking in those principles and really open it up to a wide audience, but they work very well as an indicator of direction.
“Software that treats people like people, not like cogs in the machine”
To illustrate what that should be letting us do, I stole a line I loved from a recent Netflix blog post. A goal of all “cloud adoption” efforts should be to give people more control of how they meet their goals.
As with almost everything, the real challenge for organisations is about leadership. Organisations face many challenges and it’s easy to fall back on locking things down as a way of reducing the perceived risks. That’s rarely the most productive approach.
Leaders need to focus on being open: about the real objectives, about the principles that guide how the organisation approaches issues, and about any particular risks.
Management needs a set of practices that understand what’s happening in the organisation, but that’s to keep things on course not to lock them down.
When thinking about cloud-adoption people seem to often get caught up in questions about catalogues of services and new lists of approved tools. Those things may be useful, but we would be better off spending more time explaining how we create a well-aligned, enabling environment before we get down into those sorts of details.
Update:Kush quite rightly pointed out that I didn’t go into anything specific about what to do next. In Manila, there were a set of roundtable conversations where we could begin to get into that. In London, I really just wanted to get people thinking. More blog posts to come…
Originally published at jystewart.net on March 20, 2017.
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
4 
4 
4 
Digital, tech and security consultant. Previously co-founder at Government Digital Service. East Londoner, cyclist, husband, dad.
"
https://medium.com/@mohamed-ahmed/breaking-down-the-complexity-of-cloud-native-security-for-leadership-7649a420437c?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Ahmed
Feb 18, 2021·7 min read
This article was originally published at: https://www.magalix.com/blog/breaking-down-the-complexity-of-cloud-native-security-for-leadership
One of the most challenging aspects of implementing a secure Cloud-Native environment is keeping up with the constant rate of change. As your teams gain familiarity and momentum with the basics, keeping track of everything going in and out of your stack becomes overwhelming and unmanageable. Besides, the Cloud-Native landscape itself is…
"
https://medium.com/@neuecc/microbatchframework-cloud-native-batch-framework-for-c-ad8ea79e09ca?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yoshifumi Kawai
Apr 6, 2019·3 min read
In my previous post, I made an announcement about the release of the MagicOnion library. In this post, I will discuss the release of the new open source C# CLI/Batch library.
https://github.com/Cysharp/MicroBatchFramework
There are many analysis tools for command-line arguments in C#. There are a lot, but when you use them, you are not really looking to analyze command-line arguments, what you actually want is parameter binding. That’s why I decided to make it able to call up methods like a web framework.
It’s made so you can call it up with SampleApp.exe -name “foo” -repeat 5.
It will take care of everything suitably with no annoying standard argument analysis, as long as you define the method.
With applications created with .NET Core, you can make an executable file for Windows/Linux/Mac without a need for installing a runtime. Also, you can use container bases like CircleCI with as much efficacy as other languages. For example, you can let CI create an executable file with input like that shown below.
With regards to distribution, things like NPM Global Tools, at .NET Core Global Tools , are also available.
Also, you can make the Daemon tool because MicroBatchFramework manages the lifecycle of the batch and can check by CancellationToken.
In order to be able to manage efficiently with a single console application, I have prepared a system that can be separated by just defining classes and methods.
You can change the method you are executing with the first argument.
SampleApp.exe Foo.Echo -msg “aaaaa”SampleApp.exe Foo.Sum -x 100 -y 200SampleApp.exe Bar.Hello2
Now, I recommend that you make any applications made like this into containers. This is easy even in C# if you are using .NET Core.
For example, if you bring it up with CircleCI in AWS ECR, it will probably come our in the following config.
It’s simple to write, with nothing special needed to get C# into a standard ecosystem.
With a container done up like this, you can now execute it simply as a container, but you can define both execution and scheduling (which you can set with the event triggers in CloudWatch) with relative ease with, for example, AWS Batch . You can then check the standard output in the log on CloudWatch.
In the case that you want to define a complicated work flow, you should be able to use Luigi or Apache Airflow.
If you use the managed kuberenetes services, you can use kuberenetes CronJob for batch scheduling.
Sometimes it can be a pain to type in a command every time you want to check something during development, etc., so I added a function to host it on the web. All it does is use RunBatchEngineWebHosting instead of RunBatchEngineAsync on startup.
This is hosted through Swagger, which makes it easier to confirm execution, etc.
MicroBatchFramework is new in that is constructed console applications on .NET Generic Host.
.NET Generic Host has standard support for logging, config loading, DI. It fully supports the standard methods of config mapping, logging, etc.
It would be great if anyone feels how easy it is to use CLI tools in C#! It really works on all platforms; writing CLI in C# is no problem, for real. I would be even more happy if I manage to get across the fact that C# will work, and is working in normal ecosystems, like CicleCI or Docker.
https://github.com/Cysharp/MicroBatchFramework
It is easy to try by NuGet. Please try it.
a.k.a. neuecc. Creator of UniRx, UniTask, MessagePack for C#, MagicOnion etc. Microsoft MVP for C#. CEO/CTO of Cysharp Inc. Live and work in Tokyo, Japan.
4 
4 
4 
a.k.a. neuecc. Creator of UniRx, UniTask, MessagePack for C#, MagicOnion etc. Microsoft MVP for C#. CEO/CTO of Cysharp Inc. Live and work in Tokyo, Japan.
"
https://medium.com/chronicle-blog/cloud-native-security-analytics-809c44d1739b?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
Here’s an excellent analysis of the impact that cloud-native security analytics systems such as Chronicle’s Backstory can have on the existing market. Rocky DeStefano of Visible Risk is a long-time security practitioner and implementation expert within the SIEM market. Check out his take: Cloud SIEM — Why All the Fuss?
Chronicle
2 
2 claps
2 
Chronicle
Written by

Chronicle
"
https://medium.com/@shot6/cloud-native-35edd3c33d6a?source=search_post---------195,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shinpei Ohtani
Feb 22, 2016·5 min read
Cloud Nativeというお題目で講演や発表を見かけることが多くなった。クラウドの良さを積極的に活かそう、そのノウハウを共有しようというスタンスは大変ありがたい。今後より深い議論や技術方式の確立などが望まれる分野だと思う。
個人として、Cloud Nativeを語る上で気にしなくてはいけない観点が3つあると思う。1) レジリエンシ、2) アプリケーションデベロッパーフレンドリかどうか、3) モニタリングと運用容易性のビルトインの3つ。1のレジリエンシは、耐障害性という言葉よりもクラウドにふさわしく、よりしなやかに障害に対応できる・対応しやすい、という特性のこと。Cloud上にサービスやアプリケーションを安全に生産的に作っていくためには、障害に強い状況を最初から組み込んでおくことが大事なので、プラットフォーム側で提供するか、またはサービスのコア部分に入れ込んでおくことが大事。
レジリエンをより概念としてきちんと知りたければRichard CookのResilience in Complex Adaptive Systemsあたりを見ると良いと思う。以下2つの動画はおすすめです。クラウドのプラットフォーム自体はもちろん兼ね備えている特性ですが、より効率的・効果的にサービスを提供するのであればその上で構築されるサービスも備えておくべき特性と思う。
2つ目は当然ですが、クラウドのメリットを十分に理解してアプリケーションデベロッパーだけで容易に開発できることが望ましいので、開発しやすい事が望ましい。そのためビルディングブロックをスタック上に下から積み上げるアプローチはあまり良い手とはいえないし、それはユーザが求めているものというよりは、どちらかというと提供者目線ではないかと感じる。自動化で部分的にはカバーできても、最初から組み上げないほうが望ましい。これから、こと日本に関しては開発・運用ともに人不足でまわらない状況がより深刻になり、ビジネスにも相当インパクトが出ることが予想される。なので、アプリケーション開発の少数名だけで開発できることが大事。もともとクラウドの良さは非常に限定された少数名がサービスを徹底活用することで、大人数のある程度のクオリティのところまでと同等かそれ以上を出力できることが良さなので、Cloud Nativeというならばまさにここが本筋と言っても差し支えない。
3つ目はモニタリングと運用容易性。どうしても人がいない中でになるので、できれば最初からモニタリング項目や運用のところはDay 1から既にカバーされていてほしい。これらがビルトインされて点は必須と言って良いと思う。運用のスキルがそこまで高くない開発者でもそこそこのところまでやれるようになっていてほしい。観測している範囲だと、運用をしっかりできる技術者の人は常に高負荷なことが多く、それも人数的・規模的に限界な事に見える。心身ともに疲労困憊なことも散見される（あくまで観測内）。運用容易性の先としては、オートヒーリングができると望ましい。何かしらサービスに問題があると思った時にスケールアウトやセルフリプレースで人手をかけずに自己修復してくれるのが良いし、その仕組が内包されているべきと思う。ちなみに最先端の企業だと、アプリケーションサーバごとのチューニングを人手ではなく、機械学習とプロビジョニングの仕組みを駆使してシステムが負荷状況をみて自動チューニングをする。その間に人が介在することはゼロだ。安定的に複雑なシステムを動かそうとすると、人手の介入を徹底的に減らすほうが安定する、その入り口くらいまでは来てそう。
余談としてCloud Nativeでロックインが危ない・危険だと語られるが、そういう人は言いたいだけである。利用するサービスや機能がロックインされることはどのプラットフォームでも起きていることだし、事実避けられない。避けようとすると、クラウドのもつ様々なメリットを失う。もちろん設計や実装するタイミングで適切な抽象化をすることで軽減できるがすべてのプラットフォームから自由になることはほぼ無理なので、費用対効果を見て現実的にどこまで使うか・依存関係のボーダーラインをどこにひくかが目利きとして大事だと思う。だからといって、100%クラウドが提供するサービスを使い切るのは大反対で、それはそれで極端すぎると思う。依存関係の管理については、どこかで自分の考えを書くつもり。
現状前職プラス少し触った限りではあるが、この分野で一番進んでいるのはPivotalのPivotal Cloud Foundry(PCF)だと思う。普通にCloud Foundryのプラットフォームとしても使えるが、Cloud Nativeなプラットフォームとして使うことを想定して記述する(Pivotal Web Servicesの方、多分普通のPCFでもできると思う)。アプリケーション・フレームワークのSpring Cloudやレジリエンシを高めるNetflixのオープンソースをビルトインしており、アプリケーション開発者の数名でかなりのところまでいけそうで、かつIaaSのプラットフォームは選ばない。インパクトが大きいのがSpring Cloud/Spring Bootを取り込んでいる点で、世界的に開発者が馴染みが多いので、始めやすさもある。次はAWSのBeanstalkかAPI Gateway/Lambdaの組み合わせでの構築。個人の意見として、ガチガチなPaaSというよりは、その少し下のレイヤを丁寧に提供してくれる方がサービスを作りやすい方し利用側の要求に答えやすいと思うので、そういう主義趣向も現れた結果だと思うが、事実、PCFやLambdaなどが利用され初めているのをみると、あながち間違いでもなかったようだ。
考え方の根本としては、Cloud Nativeというのであれば、クラウドのダイナミックな特性を理解して使いこなそうとするマインドで接してほしいし、それもないのに言葉先行なのであれば、何の意味もない、と思う。
Father/Architect/Programmer/Open source lover
3 
3 
3 
Father/Architect/Programmer/Open source lover
"
https://medium.com/@odsc/emerging-need-of-cloud-native-stream-processing-1be06adbd724?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
ODSC - Open Data Science
Oct 21, 2019·7 min read
Enterprise applications are evolved over the last few decades and sharpen itself with the evolution of IT infrastructure and user needs. Now it is the time for its next evolution, there are a lot of companies evaluating and revisiting their enterprise architecture and applications to move into the cloud. As the initial step, they have started converting their services and apps into micro-services. Since stream processing to be one of the key components in the enterprise architecture; we foresee stream processors will also follow the same trend. Emerging cloud-native stream processing software requires characteristics such as being lightweight, loosely coupled and adhering to the agile dev-ops process. However, most of the traditional stream processors are heavy and depends on bulky monolithic technologies which makes them harder moving to the cloud.
[Related Article: The Benefits of Cloud Native ML And AI]
In this blog, I will discuss on Siddhi Stream Processor (https://siddhi.io) which is a 100% open-source cloud-native stream processing agent that allows to implement stream processing applications and run them natively on cloud (i.e, Kubernetes). Siddhi also comes with a set of connectors which can be used to integrate with external systems such as NATS, Kafka, RDBMS, MongoDB, and Prometheus. Let’s understand more about Siddhi and its capabilities below.
Siddhi core is a Complex Event Processing and Stream Processing engine which took birth in 2011. In the Sinhala language [1], ‘Siddhi’ means ‘Events’. Overtime Siddhi engine is integrated with various other open-source and proprietary projects such as Apache Eagle [2], Apache Flink [3], UBER [4] and Punch Platform [5], and many others. Siddhi engine also plays the primary role in various WSO2 products such as WSO2 Stream Processor, WSO2 Data Analytics Server, WSO2 API Management (Traffic Manager component), and WSO2 Identity and Access Management (Adaptive Authentication component). With all the experiences and knowledge gained over the years on building Stream Processing applications, Siddhi team is now working in building distributed Cloud Native Stream Processor powered by the Siddhi engine.
Streaming applications help to express the streaming requirement/need. As per Forrester [6], Streaming applications can be used to achieve below things.
We could identify various patterns in stream processing based on its usages in numerous domains; these streaming processing patterns help to solve the commonly known stream processing problems and to provide some hints for the users on where to apply what stream processing pattern.
As shown in the above figure, Streaming patterns can be applied from the event collection phase itself. then it can spread across into event cleansing, event transformation, event enrichment, summarization, rule processing, event correlation, trend analysis, machine learning, data pipelining, event publishing, on-demand processing, etc. Each of these patterns requires specific streaming constructs such as windows, patterns, IO connectors and ML algorithms to achieve the requirement. Siddhi natively supports all of these streaming constructs which required for the user to achieve their requirements [7].
There are various ways followed over the past to write streaming applications such as code it by yourself, use a stream processing tool and streaming SQL based stream processor. Streaming SQL is the most used and handy way to write Streaming applications; it gives you the natural feeling of dealing with the events. Please check below example,
Consider the above example streaming application which is related to a healthcare use case where Glucose reading events are received through HTTP source and mapped into a stream called ‘GlucoseReadingStream’. There is a query is written to identify abnormal event occurrence over time; such identified abnormal events are pushed to a stream ‘AbnormalGlucoseReadingStream’. Those events are sent as email alerts to the respective authorities through email sink. If you check the query, you can understand that there is a window defined which accumulates events in-memory for some time (15 minutes) to identify the abnormal Glucose readings.
Cloud nativeness became one of the unavoidable requirements in the enterprise application world. As explained in the overview, organizations are looking for cloud-native technologies and applications for resilient, responsiveness and scalability. Kubernetes plays an important role in achieving cloud nativeness and various technologies are gathering around Kubernetes to build cloud-native applications; CNCF [8] is born to moderate it. Micro-services are the starting point of the Cloud-Native journey and a lot of applications; especially stateless applications have gone through promising miles in that. Stream Processing applications are so new to Cloud-Native journey and there is no much promising things happened around it; this is maintained because existing Stream Processors are not designed in cloud-native or friendly manner. Below are some of the areas that blocking existing Stream Processors to move in to cloud.
As given above, some design/architectural factors of the existing Stream Processors is not cloud friendly and it hinders to move into the cloud. Then, Stream Processors should have an architecture which supports cloud deployment. But stateful nature of the streaming applications is unavoidable in stream processing context then it is important to invest time/energy to achieve those requirements in cloud frameworks such as Kubernetes.
Siddhi Stream Processor is designed to build stream processing applications and run them in the cloud. It is built on top Siddhi stream processing library which is lightweight, it can boot up within a few seconds and able to process more than 100K events per second. Due to the micro nature of the Siddhi architecture, it allows you to deploy as containers. Siddhi provides native support to deploy a streaming application in Kubernetes with Siddhi K8s operator. Siddhi Kubernetes deployment patterns are built on top of features provided by Kubernetes and other cloud-native technologies such as NATS and Prometheus.
Siddhi deployment patterns are not based on master-slave architecture and there is no traffic involved between the nodes thus container-based deployment become easy and it leads to deploying Siddhi in Kubernetes natively. In Kubernetes, Siddhi supports to run stateful stream processing applications by involving a messaging system such as NATS and state snapshot persistence with a volume mount.
Now, let’s consider the sample Siddhi application that is discussed earlier. The specific Siddhi application is used to identify abnormal situations in patients and also it performs some stateful event processing to identify such abnormal event occurrences. If we are going to move such stream processing applications to the cloud then we have to consider the high availability of the deployment since it process sensitive data.
Siddhi provides native support to deploy such stateful streaming applications in a highly available manner. Below Kubernetes custom resource definition (CRD) which can be used to deploy a stateful stream processing application in Kubernetes.
Above CRD creates deployment shown in the below image in Kubernetes environment. As you can see, Siddhi streaming application is divided into two child applications; first child application exposes HTTP endpoint to consume Glucose readings related events and those events are pushed to the NATS messaging system. Second child application consumes events from NATS messaging system, continue the processing as per defined query and send email alerts. By following this approach, the deployment avoids event losses and achieves high availability. Please refer the Siddhi Kubernetes distributed deployment guide [9] for the in-depth details on this.
[Related Article: Why Value-Stream-As-A-Service Could Be Your Business’s Next Big Thing]
Siddhi-io is an open-source project powered by WSO2 Inc. Please check out the project source code in [10], tryout [11] and provide your feedback. You can also reach through the community channel [12] and contribute with your ideas or clarify your queries and doubts. You could try out the Siddhi cloud-native deployment patterns in Katakoda [13].
[1] https://en.wikipedia.org/wiki/Sinhala_language
[2] http://eagle.apache.org/docs/
[3] https://github.com/apache/bahir-flink
[4] https://www.youtube.com/watch?v=nncxYGD6m7E
[5] https://doc.punchplatform.com/4.0.2/src/doc/03_USER_GUIDE/Cep.html?highlight=siddhi
[6] https://www.forrester.com/report/The+Forrester+Wave+Streaming+Analytics+Q3+2017/-/E-RES136545
[7] https://siddhi.io/en/v5.1/docs/query-guide/
[8] https://www.cncf.io/
[9] https://siddhi.io/en/v5.1/docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-in-distributed-mode
[10] https://github.com/siddhi-io/siddhi
[11] https://siddhi.io/en/v5.1/download/
[12] https://siddhi.io/community/
[13] https://www.katacoda.com/siddhi/courses/siddhi-deployment
Original post here.
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday.
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
2 
2 
2 
Our passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.
"
https://architecht.io/microsofts-gabe-monroy-breaks-down-the-cloud-native-world-5744864b277d?source=search_post---------197,"In this episode of the ARCHITECHT Show, Gabe Monroy, head of product for cloud-native computing at Microsoft Azure, goes deep into the world of cloud-native computing. Monroy discusses his time in the PaaS space at Deis and EngineYard (and how that evolved into the container movement), and how the Kubernetes/CNCF communities are able to play nice with each other given all that’s at stake. He also discusses the art of building managed Kubernetes services, Microsoft’s GitHub acquisition, the role of serverless computing and more.
Scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
This episode brought to you by:
news.architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
2 
2 claps
2 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/geekculture/part-2-building-cloud-native-java-microservices-with-openjdk-746b053f4fb8?source=search_post---------198,"There are currently no responses for this story.
Be the first to respond.
In recent years, Java is not always considered the preferred programming language for the Cloud. Contrary to the popular belief, you can build and deploy production-grade enterprise application…
"
https://digitizingpolaris.com/kyligence-goes-cloud-native-with-olap-on-big-data-6c1a36352aa?source=search_post---------199,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
There are at least two things that will always be true in a data driven world; the volume, velocity and variability of data will keep growing, and, more and more users will keep expecting better insights sooner than they did yesterday.
Big data and analytics providers have been chasing their tails for years to deliver products that answer data workers’ and decision makers’…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/memory-leak/so-you-want-to-go-cloud-native-first-ask-why-59b1f5222c3d?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
This post first appeared in The New Stack on February 5, 2016 by Lenny Pruss
Over the last couple years the term “cloud native” has entered the collective consciousness of those designing and building applications and the infrastructure that supports them. At its heart, cloud native refers to a software architecture paradigm tailored for the cloud. It calls that applications 1) employ containers as the atomic unit for packaging and deployment, 2) be autonomic, that is centrally orchestrated and dynamically scheduled, and 3) be microservices-oriented, that is be built as loosely-coupled, modular services each running an independent process, most often communicating with one another through HTTP via an API.
Dissecting those characteristics further implies that modern applications need be platform-independent (e.g. decoupled from physical and/or virtual resources to work equally well across cloud and compute substrates), highly elastic, highly available and easily maintainable.
By the sound of that, it holds that building cloud native applications is a no-brainer for every organization, whether they consider writing software business-critical or not. In practice, however, going cloud native — much like adopting DevOps — requires putting into place a broad set of new technologies and practices which meaningfully shift around overhead costs associated with writing, deploying and managing software. So before considering going cloud native, it’s imperative to understand the motivations for this architectural transformation, both technically and organizationally.
A good place to start is with Google, the poster child for this highly distributed, autonomic computing paradigm. Google has been running on containerized infrastructure for nearly a decade and manages resource allocation, scheduling, orchestration and deployment through a proprietary system called Borg. In a research paper released in 2015, Large-scale cluster management at Google with Borg, Google elucidates its motivations:
Borg provides three main benefits: it (1) hides the details of resource management and failure handling so its users can focus on application development instead; (2) operates with very high reliability and availability, and supports applications that do the same; and (3) lets us run workloads across tens of thousands of machines effectively.
So Google’s rationale for going cloud-native is to achieve 1) agility, as defined by developer productivity and self-service, 2) fault-tolerance and 3) horizontal scalability. And while almost no organization has to operate at the massive scale of Google, every company in the world asks itself “how do I go faster” and “how do I minimize risk?”
Problems arise, however, when going cloud native becomes an end, not a means. While containers, autonomic scheduling and microservices-oriented design are all tools which can facilitate operational agility and reduce risk associated with shipping software, they are far from a panacea and involve shifting meaningful costs from dev to prod. Martin Fowler and others have termed this phenomenon the “microservices premium”
The [cloud native] approach is all about handling a complex system, but in order to do so the approach introduces its own set of complexities. When you [adopt cloud native architectures] you have to work on automated deployment, monitoring, dealing with failure, eventual consistency, and other [complexities] that a distributed system introduces.
The prevailing fallacy is to conflate using Docker as package format with the need to build an application as a complex distributed system from the get-go.
The first rule of the thumb is “if ain’t broke, don’t fix it,” so there’s no need for added complexity if your team is functioning at a high level, releases are on schedule and your app is resilient and scaling to meet the demand of users. Sustained high levels of developer productivity, continuous deployment and fault tolerant systems can be and are often achieved without so much as ever interacting with a Dockerfile (though it can radically simplify the development workflow). In fact, many of the most elegant delivery pipelines in high performance software organizations are AMI-based and deployed by Slackbots!
However, as your engineering organization balloons to 100+ devs, going cloud native — including stand up the entire distributed runtime — very well could begin to make sense. Just remember, all these decisions are tradeoffs, where complexity is merely shifted not reduced.
VC Astasia Myers’ perspectives on distributed computing…
6 
6 claps
6 
Written by
Redpoint partners with visionary founders to create new markets or redefine existing ones at the seed, early and growth stages.
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Redpoint partners with visionary founders to create new markets or redefine existing ones at the seed, early and growth stages.
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
"
https://medium.com/@rangleio/cloud-native-architecture-for-enterprise-758ed39bb7b4?source=search_post---------201,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rangle.io
Jun 28, 2019·9 min read
By: Suhanniyah Carpenter and Ditmar Haist
As every organization is different, adopting Cloud Native architectures has unique challenges depending on a variety of factors. In this post, we will not only highlight the benefits of adopting Cloud Native architectures but also clarify misconceptions of what it means to be Cloud Native. We’ll discuss the barriers that Cloud Native can help you overcome to addressing some challenges when adopting Cloud Native and how your organization can get started. On that note, let’s dive in!
As you begin learning about Cloud Native Architecture, start with a basic understanding and definition of what it is. But as you learn more about it’s rapidly growing list of capabilities, it becomes harder to succinctly define Cloud Native architecture because any single statement cannot capture the powerful capabilities of this new shift in architecture and operating model. It’s also complicated by the fact that, as a concept, it’s still very new and the path to successfully leveraging it is not just around architecture and design, but also depends on an organization’s maturity in Cloud adoption.
So, then, what is Cloud Native Architecture? In short, it means considering and leveraging the benefits of Cloud computation from the ground up rather than as an afterthought. It’s a shift from building and procuring everything yourself to understanding what services exist within the Cloud, leaving the Cloud vendors to do the heavy lifting while development teams focus on solving unique business problems.
So, is Cloud Native Architecture not just wrapping your monolithic application in a container and hosting it on the Cloud? Although this tends to be an initial step in an organization’s move to Cloud Native (usually termed as a “lift-and-shift” or “Cloud enabled maturity”), it is not Cloud Native. During the lift-and-shift migration, your application has just shifted it’s workload from an on-premise location to the Cloud. Here you are not yet making full use of the managed services that are available to you within the Cloud, with all of it’s auto-scaling, auto-provision, and auto-redundancy capabilities.
By this point, many of us have bought into the benefits of building microservices. We value an architectural mindset that favors loose coupling and decentralization over the traditional centralized monolithic applications. Given that the Cloud in itself is decentralized and distributed, it aligns well with enabling microservices. If you are already designing and following microservice architecture patterns you are on the right step in true Cloud Native Architecture development.
Some of the key building blocks of a loosely coupled, distributed system, are message queues and events. These are all managed services available through Cloud vendors, so you are out of the box set up with the building blocks that enable you to design and architect microservices. With the added benefits that these managed services are built to scale with your needs so you do not need to worry about getting your IT operations team to provision, host and scale these services. Imagine a world where these services were not available to you. Depending on your IT operations’ backlog, this could take weeks or even months to implement, at which point you’ve missed your sprint deadline that was allocated to complete any large spikes. Unfortunately, that means the product owner and engineering team would have to then make the unfortunate decision of putting off any new architectural changes until the next release. Your dev team will either design the architecture to account for loose coupling, or worse, continue creating technical debt by tightly coupling to the monolith because they feel that it is over engineering to create the anti-corruption layer when there is no need for it. Sound familiar?
As depicted below, with Cloud Native you are benefiting from the vendors managing the services you need. This drastically reduces what teams are required to manage, enabling them to make architecturally sound decisions that enable innovation. Go ahead, decouple that new feature.
We’ve already seen how a traditional barriers with IT operations’ ability to procure supporting infrastructure can be removed with the Cloud, and in doing so enables sound innovation. These are the other barriers that are removed when you start building Cloud Native:
When previously dealing with a large enterprise application and a new feature was needed in one module, all modules in the application would need to be deployed as well. Therefore, the product team that owns the change couldn’t release without first planning with other product teams to ensure sufficient QA cycles are available to test all impacted modules, ensuring that nothing is broken. This tight inter-team dependency meant that although the development team could have developed the feature in a couple sprints, the anxiety and sometimes complexity around testing was usually a factor in stifling the team’s ability to innovate.
Cloud native architecture, as an extension of microservices, has created a rise in cross-functional teams that own and operate the service that they are responsible for. Since the services are decoupled, the team is able to respond to changes and in effect, be more agile. They are able to develop, test, deploy and manage their service independently. Releasing changes faster.
Large monolithic applications were especially concerning for enterprise clients, who themselves needed time to not only accept new features coming in, but also needed ramp up time to train and document new modules for their user base, go through test cycles of their own to ensure that configurations and functionalities are in check, and procure any new infrastructure requirements if they were deploying on premise. This operational overhead usually meant that clients could only take a couple releases a year.
When you breakdown the monolith in favor of Cloud Native, teams are able to release at the pace of your ideas. Since the entire application doesn’t need to be redeployed and tested, changes can then be released faster. New infrastructure isn’t needed to enable a new feature, so there is no IT operations sign-off required. You’ve removed the cost of doing and can now simply get things done.
Instead of releasing once or twice a year, you can now continuously release as needed. This in turn opens up opportunities to get real user feedback on your new feature. You’re no longer waiting six months to release your idea, you can release as soon as you’re ready and make use of data analytics to monitor how your feature is doing. Learn, make changes, and deploy faster. That’s exciting.
When teams own their service, they are also able to pick the technology that is optimal for them and for getting the job done. This would not have been possible before. Aside from infrastructure to host the new technology, there would have been version incompatibility constraints. How many times have you wanted to upgrade to the latest version of a library or framework, but were unable to, because an upgrade meant a loss in functionality for another module X? The product team for X is so overwhelmed with their own backlog that a large scale upgrade isn’t feasible for a couple major releases (we’re talkin’ a year or two). This means the features we wanted in the new library we would have to implement ourselves.
With Cloud Native, we are no longer constrained by tightly coupled dependencies, better yet, by a single technology stack. Since our services are isolated, we are free to choose the right technology for the right job. It means not having to implement underlying capabilities of a particular library or feature within your existing technology stack, and rather focus on implementing business use cases using the right technology stack, the actual effort that provides ROI for your business.
There are design considerations to be mindful of when architecting for the Cloud. The ability for your operations to be transactional and atomic is not possible when an operation spans multiple independent services. Here, you’re most likely going to leverage message queues and events to facilitate communication between your services. This in turn can add latency and concepts of eventual consistency to your design which needs to be accounted for.
An emerging trend in Cloud Native development is the use of serverless computing or Functions as a Service (FaaS). Here you are running a stateless function, that only spins up when needed. You only pay for when the function is actually used, so you can save on cost, since your function is not constantly running as with traditional services. This is a powerful avenue to pursue when developing your distributed application. It does, however, mean that you do need consider another level of latency here. Since your function spins up when it is invoked, you do need to factor this in to your overall design.
Enterprise level organizations generally have the largest hurdle in adopting Cloud Native architecture. This isn’t from a lack of understanding the benefits, but rather looking at the shift to Cloud as means to finding a solution to all the traditional barriers mentioned earlier. Understandably, this can be a daunting task to solve in parallel with feature enhancements in the current backlog.
There is a tendency to look at Cloud migration from a re-platforming perspective. Making the decision to wipe the slate clean and start from scratch is simply too expensive and usually entails a large endeavor. Sometimes a lot of the core business logic is baked into the code and understanding how your software currently operates will need to be reverse-engineered because those that developed and wrote the business requirements may no longer be with the company. The risk of getting it wrong is far too great.
The other challenge with breaking down the monolith is the ability to start thinking about domains and defining proper bounded contexts. Do we have clear definitions of where the separations lie? This in itself can be a challenge. If developers are used to creating tight coupling and not defining proper isolations, it can take a few iterations to employ best practices.
From an operational perspective there is a lot to consider as well. Giving a team autonomy to own an entire service is hard to swallow, especially if they’re still new to defining proper boundaries. Setting expectations with clients in terms of expecting multiple releases is also a unique challenge that needs to be handled.
From an automation perspective, some legacy code bases are still being packaged and deployed manually. Companies are still looking for ways to get their feet wet with DevOps and formulating a solid CI/CD pipeline. This in itself is something that some companies require ramp-up time to solve.
Large enterprises can move to the Cloud without making large scale changes to their existing code bases. The key is to start small, so that you can learn the nuances of operating in the Cloud. Here some opportunities to consider for new or existing functionalities:
The objective is to find opportunities where you are able to minimize the amount of refactoring that needs to be done in order to start experimenting with your new functionality. These candidates will get your feet wet leveraging anywhere from serverless capabilities to services managed on the Cloud. This will ensure some learning on how your teams need to handle deployment and configuration of your services on the Cloud and how to familiarize themselves with monitoring and toolings necessary to scale your application when in the Cloud.
Also, starting small will ensure that you can begin looking at automating your processes using a CI/CD pipeline to form your initial DevOps practice. Getting a small team interested in experimenting here so that they can take their learnings for future enhancements.
These future enhancements may look like the figure below, where you begin to slowly carve out slices (a domain or proper bounded context) from your monolith and move it to the cloud.
New technology always sounds great, but when it comes time to adopt and integrate it into our own stack, that’s where our unique code bases and operating models pose a challenge not usually documented or widely talked about. By starting small, and taking a segment of work in order to gain familiarity with the Cloud, you will ensure that you can ramp up and learn the inner workings of Cloud Native in this new space. By creating an understanding of how your organization needs to approach Cloud Native and iteratively building up your experiences, you set up your organization to transform your innovation landscape.
The latest in Javascript, design, and innovation.
51 
51 
51 
The latest in Javascript, design, and innovation.
"
https://medium.com/@alibaba-cloud/how-cloud-native-is-reshaping-enterprise-it-architectures-c2b436de28d3?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 25, 2020·13 min read
By Yi Li, nicknamed Weiyuan at Alibaba.
Since the start of the 21st century, we have witnessed the enterprise-grade distributed application architecture (DAA) evolved from a service-oriented architecture (SOA) to a microservices architecture, and then to a cloud-native application architecture.
To explain the thinking behind the evolution of enterprise IT architectures, let’s first talk about metaphysics.
First, the complexity, which is measured by entropy, of enterprise IT systems is consistent with the second law of thermodynamics. As time goes by and services keep changing, enterprise IT systems will become increasingly complex.
Second, according to the famous law of conservation of complexity in human-computer interaction, the complexity of application interaction is constant, though it can exist in different ways. This principle also applies to software architectures. New software architectures cannot reduce the overall complexity of an IT system.
These laws may seem harsh, but there are ways we can work within them.
A core task of modern software architecture is to define the boundaries between infrastructure and applications, to properly divide the complexity and reduce the complexity to be faced by application developers. In other words, in the evolution we’ve seen up to now, what we want to do is to hand over certain problems to more appropriate persons and systems, to allow developers to focus on core value innovation.
Let’s begin with the following figure to explore the logic behind the evolution of the enterprise-grade DAA.
Image source: Bilgin Lbryam’s Twitter feed
In 2004, IBM built a global SOA design center. As an R&D team leader and architect, I participated in a series of pilot projects for global customers and assisted international enterprises such as Pep Boys and Office Depot in applying SOA to optimize intra-enterprise and inter-enterprise business processes and improve business agility.
At that time, as economic globalization further spread, the competition between enterprises became more intense, and the commercial revolution began to speed up. Since IT systems in large enterprises evolved for decades, their overall technical systems have become extremely complex with the coexistence of complex instruction set computing (CISC) and common business oriented language (COBOL) applications in host systems, report program generator (RPG) business systems in AS400 minicomputers, and applications written in C language, Java Enterprise Edition (JEE) language, or .NET framework in distributed systems such as X86 and Power. A large number of application systems are provided by third-party suppliers, but some systems had been left unmaintained. In addition, as businesses iterated, new business systems continued to emerge. However, due to the lack of proper methodological guidance and the lack of organic links between systems, these business systems became silos, which continuously increased the complexity of IT architectures and made it impossible to meet the demands for business development. These different business systems, although powerful in isolation, cannot work well with each other, leading to major problems down the road.
Therefore, the primary challenge faced by the enterprise IT architecture is to integrate a large number of siloed IT systems in the enterprises to support increasingly complex business processes, enable efficient decision-making, and easily adapt to rapid business changes. In this context, companies such as IBM proposed the SOA concept, which abstracts application systems into coarse-grained services to build a loosely coupled service architecture. In this architecture, services can be flexibly combined through business processes. This improves asset reuse in the enterprise IT architecture, improves system adaptability, flexibility, and scalability, and prevents information silos.
SOA proposed a series of principles for building a distributed system, which are still applicable today:
In the initial construction of SOA systems, point-to-point communication connections are primarily used, and service call and integration logic is embedded in application implementations. This development method is simple and efficient when only a few services are integrated into an SOA system. However, as services grow in scale, the communication between services becomes more complex and the connection paths and complexity increase sharply, posing major challenges to service governance.
To address these challenges, an enterprise service bus (ESB) was introduced. The ESB provides inter-service connection, transformation, and mediation capabilities. It can connect the internal system and various services of an enterprise to the service bus to implement a loosely coupled architecture between information systems. This simplifies system integration, makes the IT system architecture more flexible, and reduces the costs of information sharing within the enterprise.
The goal of the SOA methodology is to organize, gather, and integrate different capabilities. However, this is often difficult to achieve in practice. A large number of ambitious SOA projects failed to achieve their expected results.
This is because no IT architecture can succeed without the integration of business objectives, technological foundations, and organization capabilities.
Previously, SOA focused on dealing with inventory and marketing issues related to the enterprise IT architecture. This largely narrowed the SOA methodology to enterprise application integration (EAI). When applying the SOA concept, interconnecting information systems is only the first step. In order to maintain the agility and flexibility of the enterprise IT architecture and continuously support business growth and changes, enterprises also need to make great efforts to improve their capabilities and continuously reconstruct and iterate the enterprise IT architecture.
Previously, in most enterprises, the IT department was still a cost center and a business support department. Most enterprises lacked a long-term strategic IT plan, and their IT teams lacked growth recognition. As a result, SOA degenerated into project operation support, without organizational guarantees or continuous investments. Even then-successful projects gradually stagnated in the increasingly complex architecture. According to some photos sent to me last year by my friend who lived in the United States, the business system we built for our customer 15 years ago was still supporting the business of their stores around the country. This proves the success of the technical project, but reflects the lack of technical strategies on the part of the enterprise.
Technically, the ESB architecture decouples business logic from service integration, which allows better centralized service governance. However, severe issues are also exposed.
Along with the development of the Internet, and especially with the advent of mobile Internet, the economic pattern of the whole world changed dramatically. In particular, the focus of the enterprise IT architecture changed from the conventional systems of record, such as enterprise resource planning (ERP) and supply chain management (SCM), to systems of engagement, such as omnichannel marketing. These systems must cope with the rapid and large-scale growth of the Internet and support fast iteration and low-cost trial and error. Currently, the enterprise IT architecture has become an engine to drive innovations. The idea of using technology to expand business boundaries gave IT teams a new sense of mission and further accelerated the evolution of the enterprise IT architecture.
Internet companies led by Netflix and Alibaba oversaw a new revolution in enterprise architecture: microservices. Microservices frameworks such as Apache Dubbo and Spring Cloud were widely used.
The core idea of microservices is to simplify business system implementation by splitting and decoupling application functions. Microservices emphasize the division of application functions into a set of loosely coupled services, with each service compliant with the single responsibility principle. The microservices architecture solves several problems inherent in the conventional monolithic architecture. Each service can be deployed and delivered independently, which greatly improves business agility. In addition, each service can be independently scaled in or out to adapt to the scale of the Internet.
Image source: The definition of the microservices architecture by Martin Fowler
Certainly, the division of a large single application into multiple microservices makes the R&D collaboration, delivery, and maintenance of the IT system more complex. Fortunately, DevOps and containers are naturally integrated with the microservices architecture, forming the prototype of the cloud-native application architecture.
The microservices architecture draws on the principles of the SOA architecture. However, from the perspective of implementation, it tends to replace ESB by constructing a decentralized distributed architecture with smart endpoints and dumb pipes. Qiu Xiaoxia has analyzed these problems in detail in Those Things About Microservices, I will not go into detail here.
The microservices architecture must first face the internal complexity of distributed architectures. For more information, see Misunderstandings About Distributed Computing. Microservices frameworks need to overcome the complexity of service communication and governance, for example, the challenges of service discovery, fusion, throttling, and end-to-end tracking. Microservices frameworks, such as HSF, Dubbo, or Spring Cloud, package these capabilities as code libraries. These code libraries are built in applications and released and maintained with the applications.
Image source
In essence, service communication and governance are to horizontally connect systems of different departments, and therefore is orthogonal to the business logic. However, in the microservices architecture, the implementation and lifecycles are coupled with the business logic, and an upgrade of the microservices framework will cause the entire service application to be rebuilt and redeployed. In addition, code libraries are usually bound to specific languages, and therefore, it is difficult to support the polyglot implementation of enterprise applications.
With a centralized service bus architecture, SOA decouples the business logic from the service governance logic. The microservices architecture regresses to the decentralized point-to-point calling method, which improves agility and scalability at the expense of the flexibility brought about by the decoupling of business logic and service governance logic.
To solve these challenges, the community proposed the service mesh architecture. This architecture sinks service governance capabilities to the infrastructure and deploys them as independent processes for both service consumers and service providers. Therefore, decentralization not only ensures the scalability of the system, but also decouples service governance and business logic to allow independent evolution without mutual interference. This allows the overall architecture to evolve in a more flexible manner. In addition, the service mesh architecture reduces the intrusion into the business logic and simplifies the support for polyglot applications.
Image source
The Istio project led by Google, IBM, and Lyft is a typical implementation of the service mesh architecture and has become a new sensation in the industry.
The preceding picture shows the architecture of Istio, which is logically divided into a data plane and a control plane. The data plane consists of smart proxies deployed in sidecar mode. The sidecar intercepts the traffic of application networks, collects telemetry data, and implements service governance policies. On the control plane, the galley manages configurations, the pilot delivers configurations, the mixer checks policies and aggregates telemetry data, and the citadel manages security certificates during communication.
The Istio architecture provides a series of advanced service governance capabilities, such as service discovery and load balancing, progressive delivery (also called phased release), chaos injection and analysis, end-to-end tracking, and zero-trust network security. An upper-layer business system can orchestrate the Istio architecture into its own IT architecture and release system.
However, a service mesh is not a silver bullet. The flexibility of the architecture and the evolution of the system are ensured at the expense of the complexity of deployment in sidecar mode and the performance (two extra hops are added).
The community and cloud service providers are working together to address the deployment complexity. First, they seek to improve the automated maintenance of service meshes. For example, Alibaba Cloud greatly simplifies the upgrade and maintenance of the Istio architecture and simplifies cross-Kubernetes cluster deployment by using operators. In addition, they provide hosted service mesh services to help users focus on service governance at the business layer, instead of infrastructure implementation.
From the perspective of performance, service mesh must reduce the performance overhead of its own control plane and service plane. For example, you can offload the mixer to sink governance policies to the data plane. In addition, you need to reconsider the boundaries between applications and network infrastructure throughout the entire communication stack. To interconnect container applications, the Kubernetes community proposed the container network interface (CNI) to decouple the container network connectivity from the underlying network implementation. In addition, Kubernetes provides basic meta-languages such as services, ingress, and network policies to support service communication and access control at the application layer. However, these capabilities are far from enough to meet the requirements for application service governance. Service meshes provide new functions such as traffic management, end-to-end observability, and secure interconnection at L4 and L7. All these functions are implemented by the new Envoy proxy that runs in the userspace. This improves flexibility, but also inevitably increases performance overhead. To systematically resolve this problem, the community is conducting interesting explorations. For example, in the Cillium container network, capabilities of operating systems and underlying networks, such as extended Berkeley packet filter (eBPF) and express data path (XDP) can sink the service control capabilities of the application layer (such as services and network policies provided by Kube-Proxy) to the OS kernel and the network layer to solve this problem. In addition, the data links of the service mesh are optimized to reduce context switching and data copying, which effectively reduces performance overhead.
Currently, the service mesh technology is still in an early stage of development. It can provide flexible service communication at L4 and L7. The community is also exploring how to implement flexible networking at L2 and L3 by using the network service mesh. We believe that service mesh will become the communication infrastructure for distributed enterprise applications in the future.
In this process, new concepts and projects will be continuously created, and we need to rationally analyze their business value and technical limitations. We must avoid seeing service mesh as a magic bullet and sinking the business logic of application integration and application-side security to the service mesh to avoid high complexity. For more information, see Application Safety and Correctness Cannot Be Offloaded to Istio or Any Service Mesh.
It seems to be a universal law that unification inevitably follows prolonged division and division arises out of prolonged unification. Enterprise-grade DAAs have also gone through repeated unification and division. Today, with new technologies rapidly emerging, we not only need to embrace the architectural changes brought by new technologies, but also pay more attention to the evolution logic and core values behind them to systematically control complexity.
This article introduced the changes brought by the cloud-native computing architecture from the perspective of enterprise-grade DAA. In later articles, I will share my ideas about the research and development process and integrated architectures.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
11 
11 claps
11 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/geekculture/part-3-building-cloud-native-java-microservices-with-openjdk-91d1da14fd7b?source=search_post---------203,"There are currently no responses for this story.
Be the first to respond.
Bringing Java Microservices into Production with Custom Domain, HTTPS, Logging, Monitoring using AWS Cloud
In recent years, Java is not always considered the preferred programming language for the Cloud. Contrary to the popular belief, you can build and deploy…
"
https://medium.com/@alibabatech/kubevela-releases-1-1-reaching-new-peaks-in-cloud-native-continuous-delivery-82f4b49b1757?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Nov 17, 2021·6 min read
Tags: KubeVela, Kubernetes, DevOps, CI/CD, Continuous Delivery, PaaS
By KubeVela Maintainers
Initialized by Alibaba and currently a CNCF sandbox project, KubeVela is a modern application platform that focuses on modelling the delivery workflow of micro-services on top of Kubernetes, Terraform, Flux Helm controller and beyond. This brings strong value added to the existing GitOps and IaC primitives with battle tested application delivery practices including deployment pipeline, across-environment promotion, manual approval, canary rollout and notification, etc.
This is the first open source project in CNCF that focuses on the full lifecycle continuous delivery experience from abstraction, rendering, orchestration to deployment. This reminds us of Spinnaker, but designed to be simpler, cloud native, can be used with any CI pipeline and easily extended.
Kubernetes has made it easy to build application deployment infrastructure, either on cloud, on-prem, or on IoT environments. But there are still two problems for developers to manage micro-service applications. First, developers just want to deploy, but delivering application with lower level infrastructure/orchestrator primitives is too much for them. It’s very hard for developers to keep up with all these details and they need a simpler abstraction to “just deploy”. Second, application delivery workflow is a basic need for “just deploy”, but it is inherently out of scope of Kubernetes itself. The existing workflow addons/projects are too generic, they are way more than focusing on delivering applications only. These problems makes continuous delivery complex and unscalable even with the help of Kubernetes. GitOps can help in deploying phase, but lack the capabilities of abstracting, rendering, and orchestration. This results in low SDO (software delivery and operation) performance and burnout of DevOps engineers. In worst case, it could cause production outage if users make unsafe operations due to the complexity.
The latest DORA survey [1] shows that organizations adopting continuous delivery are more likely to have processes that are more high quality, low-risk, and cost-effective. Though the question is how we can make it more focused and easy to practice. Hence, KubeVela introduces Open Application Model (OAM), a higher level abstraction for modelling application delivery workflow with app-centric, consistent and declarative approach. This empowers developers to continuously verify and deploy their applications with confidence, standing on the shoulders of Kubernetes control theory, GitOps, IaC and beyond.
KubeVela latest 1.1 release is a major milestone bringing more continuous delivery features. It highlights: ●Multi-environment, multi-cluster rollout: KubeVela allows users to define the environments and the clusters which application components to deploy to or to promote. This makes it easier for users to manage multi-stage application rollout. For example, users can deploy applications to test environment and then promote to production environment.●Canary rollout and approval gate: Application delivery is a procedural workflow that takes multiple steps. KubeVela provides such workflow on top of Kubernetes. By default, Users can use KubeVela to build canary rollout, approval gate, notification pipelines to deliver applications confidently. Moreover, the workflow model is declarative and extensible. Workflow steps can be stored in Git to simplify management.●Addon management: All KubeVela capabilities (e.g. Helm chart deployment) are pluggable. They are managed as addons [2]. KubeVela provides simple experience via CLI/UI to discover, install, uninstall addons. There is an official addon registry. Users can also bring their own addon registries.●Cloud Resource: Users can enable Terraform addon on KubeVela to deploy cloud resources using the same abstraction to deploy applications. This enables cooperative delivery of application and its dependencies. That includes databases, redis, message queues, etc. By using KubeVela, users don’t need to switch over to another interface to manage middlewares. This provides unified experience and aligns better with the upcoming trends in CNCF Cooperative-Delivery Working Group [3]. That is the introduction about KubeVela 1.1 release. In the following, we will provide deep-dive and examples for the new features.
Users would need to deploy applications across clusters in different regions. Additionally, users would have test environment to run some automated tests first before deploying production to environment. However, it remains mysterious for many users how to do multi-environment, multi-cluster application rollout on Kubernetes. KubeVela 1.1 introduces multi-environment, multi-cluster rollout. It integrates Open Cluster Management and Karmada projects to handle multi-cluster management. Based on that, it provides EnvBinding Policy to define per-environment config patch and placement decisions. Here is an example of EnvBinding policy:
Below is a demo for a multi-stage application rollout from Staging to Production. The local cluster serves as the control plane and the rest two are the runtime clusters.
Note that all the resources and statuses are aggregated and abstracted in the KubeVela Applications. Did any problems happen, it will pinpoint the problematic resources for users. This results in faster recovery time and more manageable delivery.
Can you build a canary rollout pipeline in 5 minutes? Ask Kubernetes users and they would tell you it is not even enough to learn an Istio concept. We believe that as a developer you do not need to master Istio to build a canary rollout pipeline. KubeVela abstracts away the low level details and provides a simple solution as follows. First, installing Istio is made easy via KubeVela addons:
Then, users just need to define how many batches for the rollout:
Finally, define the workflow of canary, approval, and notification:
Here is a full demo:
In this KubeVela release we have built the cornerstone for continuous delivery on Kubernetes. For the upcoming release our major theme will be improving user experience. We will release a dashboard that takes the user experience to another level. Besides that, we will keep improving our CLI tools, debuggability, observability. This will ensure our users can self serve to not only deploy and manage applications, but also debug and analyze the delivery pipelines.
For more project roadmap information, please see Kubevela RoadMap.
KubeVela is a community-driven, open-source project. Dozens of leading enterprises have adopted KubeVela in production, including Alibaba, Tencent, ByteDance, XPeng Motors. You are welcome to join the community. Here are next steps:
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
5 
5 
5 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@alibaba-cloud/lindorm-reduce-storage-costs-by-80-with-cloud-native-time-series-and-spatial-temporal-database-c0ef54b57c85?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 21, 2020·4 min read
Catch the replay of the Apsara Conference 2020 at this link!
By ApsaraDB
At the Apsara Conference 2020 held on September 18, Alibaba Cloud launched Lindorm, a Time Series and Spatial-Temporal Database. It provides wide table engines, time series engines, search engines, and file engines to redefine the storage mode for multi-type data in the Internet era. Lindorm can simultaneously store, query, and retrieve various types of data, such as key-value data, wide table data, time series data, files, and images. It solves the problems, such as complex architecture and difficult maintenance, high data storage cost, and difficulty in coping with flexible business scale, when different types of databases are deployed respectively. It helps reduce massive data storage costs by 80% and is the first choice for scenarios, such as Internet of Vehicles (IOV), advertising, social networking, and gaming.
Reportedly, the Alibaba Cloud Lindorm database has been tested by the Alibaba economy for ten years and supports core businesses, such as Taobao, Tmall, Alipay, Cainiao Network, and IoT, with a throughput of tens of millions per second, hundreds of petabytes of storage capacity, and single-digit millisecond response latency. Based on the cloud-native architecture and with the separation of storage and computing, it provides geo-disaster recovery and global data synchronization. It can be flexibly scaled according to the business scale to match the rapid growth of the business.
The multi-modal technology has become the trend of the times, and traditional databases are carrying out the multi-model extension. However, this extension is based on the original architecture, and different data models need appropriate technical architectures to support them. Therefore, although this extension supports the storage of new data models, functionality, performance, and scalability are limited and cannot truly match the business needs. The Lindorm database is based on the self-developed cloud-native multi-modal engines and cloud-native storage engines. It is specially designed for multi-model data and has ultimate performance and scalability, making the system architecture simple and efficient. It greatly reduces system maintenance costs and massive data storage costs.
In traditional solutions, multiple databases are required to process different types of data separately. This leads to a complex architecture that is difficult to maintain and scale out in the case of dynamic and elastic services. The data is redundant and wasted.
Lindorm has built-in wide table engines, time series engines, search engines, and file engines. Each engine shares storage and supports independent auto scaling to replace these databases, enabling low-cost data storage and complex processing.
Data is one of the most valuable assets of an enterprise. However, due to the high costs of data storage, enterprises have to discard large amounts of valuable data. Based on self-developed low-cost elastic storage media, intelligent cold and hot separation technology, and adaptive compression algorithms that are transformed by zero application, Lindorm reduces the storage costs of massive data by 80%.
It is reported that Lindorm can provide services for users through Alibaba Cloud and Apsara Stack. By Lindorm, many users are building orders, logs, risk control, recommendations, social networking, IOV, intelligent buildings, Node Package Management (NPM), Application Performance Management (APM), which covers dozens of industries, such as IoT, internet, finance, gaming, and industrial production.
We kept broad-minded, accumulate experiences, and finally succeeded. After ten years of refinement and successful business validation, Lindorm maintains Alibaba’s understanding of the value of enterprise data. It is committed to building low-cost storage and processing solutions for data of any size and type, to make enterprise data “affordable and visible”.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@IBMDeveloper/build-a-cloud-native-microservices-application-in-java-2c95a1cb19ac?source=search_post---------206,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Aug 7, 2019·1 min read
In this series, IBM developer advocates Niklas Heidloff and Harald Uebele simplify the sometimes-intimidating process of building a full-featured microservices application running in Kubernetes. Follow along in easy-to-understand tutorials, with hands-on examples, as Niklas and Harald explain many aspects of modern application development. For additional details, see the project overview and design principles.
Learn how to put together a complete, secure back-end stack to run an enterprise application with a web front end that’s fault-tolerant and scalable using modern cloud-native technologies like:
Some tutorials rely on services in IBM Cloud, all of which are available in the free tier. However, you will need a credit card or promo code to use the free Kubernetes cluster. A Kubernetes lite cluster itself is free of charge, but it cannot be created in an IBM Cloud Lite account.
You can see the full series on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
11 
11 
11 
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/@alibaba-cloud/getting-started-with-kubernetes-a-brief-history-of-cloud-native-technologies-9b4f44af933a?source=search_post---------207,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 29, 2020·9 min read
By Zhang Lei, a senior technical expert for Alibaba Cloud Container Platform and an official ambassador of CNCF
“In the future, the software will definitely grow on the cloud.” This is the core assumption of the cloud-native concept. The so-called “cloud-native” actually defines the optimal path for enabling applications to exploit the capabilities and value of the cloud. On this path, “cloud-native” is out of the question without “applications,” which act as the carrier. In addition, container technology is one of the important approaches for implementing this concept and sustaining the revolution of software delivery.
Discussing the cloud-native ecosystem brings a huge set of technologies to the table. The CNCF cloud-native landscape includes more than 200 projects and products that fit within CNCF. Based on this landscape, today’s cloud-native actually covers the following aspects:
1) CNCF2) Cloud-native technology community. For example, more than 20 projects officially hosted by CNCF constitute the foundation of the modern cloud computing ecosystem. Among them, the Kubernetes project has become the fourth most dynamic open-source project in the world.3) Kubernetes. Currently, major public cloud vendors around the world support Kubernetes. In addition, more than 100 technology start-ups are making sustained investments in Kubernetes. Alibaba is also considering moving all its businesses to the cloud, and directly to cloud-native. This again reflects that major technology companies are embracing cloud-native.
2019 was the critical year of the cloud-native era. Why? Let’s explain it in a simple way.
In 2013, the Docker project was released. The Docker project popularized the sandbox technology that ran on the semantics of all operating systems. It allows users to conveniently and completely package their applications and allows developers to easily obtain the minimum executable unit of an application without relying on any Platform as a Service (PaaS) capabilities. This actually adversely affected the conventional PaaS industry.
In 2014, the Kubernetes project was launched. This was significant in the sense that Google “reproduced” its internal Borg and Omega systems based on the open-source community and proposed the concept of “container design patterns.” Actually, it is easy to understand why Google chose to indirectly make Kubernetes open-source instead of directly making the Borg project open-source. A system like Borg or Omega is too complex to be used by people outside Google. However, the design ideas of Borg and Omega are open to users through Kubernetes. This is also an important background for open-source Kubernetes.
The period from 2015 through 2016 was the era of the “Three Kingdoms” of container orchestration. Docker, Swarm, Mesos, and Kubernetes all competed against each other in the container orchestration field. The reason for the competition is obvious. Despite their own great value, Docker or containers lack value for commercialization or the cloud. To overcome this, they must play a favorable role in orchestration.
Swarm and Mesos both featured powerful ecosystems and technologies. Swarm is more powerful in terms of the ecosystem, whereas, Mesos is more sophisticated in terms of technology. In contrast, Kubernetes has both advantages. Kubernetes finally won the “Three Kingdoms” battle in 2017 and has become the standard for container orchestration since then. A typical event in this process was when Docker announced that it has embedded Kubernetes into its core products, and the Swarm project gradually fell out of maintenance.
In 2018, the concept of cloud-native technologies began to emerge. This occurred because Kubernetes and containers had become predetermined standards for cloud vendors, and the concept of “cloud-centric” software research and development gradually came into being.
In 2019, the situation saw another shift.
Many people are asking, “What exactly is cloud-native?”
Actually, cloud-native is the best path or practice. In more detail, cloud-native provides users with the best practice of exploiting the capabilities and value of cloud in a user-friendly, agile, scalable, and replicable way.
Cloud-native is a concept that provides guidance on software architecture design. Software designed around this concept has the following advantages:
The greatest value and vision of cloud-native are that future software is born and grows on the cloud and complies with a new model of software development, release, and O&M to ensure that the software maximizes the use of cloud capabilities. Now, let’s think about why container technology is revolutionary.
In fact, the revolutionary nature of container technology in IT is very similar to that of the container technology in transportation. To be specific, the container technology enables applications to be defined as “self-contained.” Only in this way can applications be released on the cloud in an agile, scalable, and replicable manner to exert cloud capabilities. This is also the revolutionary impact of the container technology on the cloud. Therefore, container technology is the cornerstone of cloud-native technologies.
Cloud-native technologies cover the following aspects:
After learning about the technological scope of cloud-native, it is concluded that cloud-native includes a lot of technologies, the essentials of which are similar. In essence, cloud-native technologies are based on two theories:
The concept of “immutable infrastructure” reflects that the infrastructure on which applications are running is evolving to the cloud. There is a contrast. Before evolution, the conventional application infrastructure is changeable in most cases. For example, to release or update software, you may often connect the software to the server through SSH, manually upgrade or downgrade the software package, adjust the configuration files on the server in order, or directly deploy the new code to the existing server. Therefore, in such cases, the infrastructure is constantly adjusted and modified.
In contrast, the “cloud-friendly” application infrastructure is immutable on the cloud.
On the cloud, the application’s infrastructure is permanent after the application is deployed. To update the application, change the public image to build a new service to directly replace the old one. The direct replacement is supported because containers provide a self-contained environment, which contains all the dependencies required for running the application. Therefore, the application does not need to learn about container changes, and there is only a need to modify container images. In conclusion, cloud-friendly infrastructure may be replaced at any time due to the fact that agility and consistency are ensured by containers for the application infrastructure in the cloud era.
In other words, the infrastructure in the cloud era is like a “draft animal” that may be replaced at any time, whereas the conventional infrastructure is a unique “pet” that can never be replaced but requires careful care. This is exactly the strength of immutable infrastructure in the cloud era.
The process in which the infrastructure evolves to be “immutable” provides us with two important benefits.
In addition, the cloud-native infrastructure allows applications to be deployed and maintained in a simple and predictable way. With images and self-contained applications, the entire container that runs based on images actually be self-maintained as operators in Kubernetes. Therefore, the entire application is self-contained, which allows it to be migrated to any location on the cloud. This also facilitates the automation of the entire process.
Furthermore, the immutable infrastructure allows an application to be scaled conveniently from 1 instance to 100 instances and even to 10,000 instances. This scale-out process is common for containerized applications. Finally, the immutable infrastructure allows to quickly replicate peripheral control systems and supporting components. This attributes to the fact that these components are also containerized and comply with the theory of immutable infrastructure.
Why was 2019 a critical year? We believe that 2019 was the year when the popularization of cloud-native technologies started.
In 2019, Alibaba announced that it would migrate all of its businesses to the cloud, and “directly to cloud-native.” The concept of “cloud” centric software R&D is gradually becoming the default choice for all developers. In addition, cloud-native technologies such as Kubernetes are becoming a required course for technicians, and a large number of related jobs are emerging.
In this context, “knowing Kubernetes” is far from enough. “Understanding Kubernetes” and “understanding cloud-native architecture” has become increasingly important. Since 2019, cloud-native technologies have been extensively used on a large scale. This is also an important reason why everyone wants to learn and invest in cloud-native technologies at this point in time.
You may be wondering what prerequisite knowledge is required to learn the basics of cloud-native. Generally, the required prerequisite knowledge is divided into the following parts:
1)Linux Operating System Knowledge: Mainly, general basic knowledge is required. Linux development experience is preferred.2) Computer and Program Design Basics: The knowledge required for an entry-level engineer or a senior undergraduate student is enough.3) Experience in Using Containers: Basic experience in using containers, such as Docker run and Docker build commands, is preferred. It is best to have some experience in developing Docker-based applications.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
51 
51 
51 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@BradAMurphy/the-apocalypse-of-cloud-native-is-already-here-and-your-agile-transformation-will-not-save-you-3bfbf61fa555?source=search_post---------208,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brad Murphy
Mar 29, 2017·9 min read
I originally posted this on LinkedIn in February 2017 and wanted to share this with my Medium readers.
Over the last eighteen months, I’ve been co-writing a book on the future of large companies and their pathway to reinvention in the age of cloud-native computing. The book is a candid examination of how agile adoption has distracted large companies from their single biggest opportunity and threat in the last thirty years — the paradigm shift to cloud-native computing and the exponential organizational models under which modern digital platform companies operate.
This article is about what we’ve learned from countless hours spent meeting with, listening to, and observing the work of individual contributors, managers, and executives working inside traditional firms as well as the early pioneers of cloud-native business models. This research points to an inescapable truth: agile transformation in traditional enterprise companies is complete nonsense. Don’t believe the hype or the survey data. Everyone is deceiving themselves, including the leaders of these companies, and particularly the agile cottage industry of consultants and tool makers.
Agile transformation hasn’t happened. All the talk, all the conferences, all the local meetup groups, the tweets, all the lists of agile leaders — it’s all hand waving; virtually none of it is real.
And, sadly, nothing has changed.
Companies are delivering virtually the same services they did in 1985, just with websites and customer portals (including web and mobile) instead of brick and mortar storefronts. Too many people are claiming that agile transformation is happening, when in reality, all companies are really doing is applying local, tactical, and incremental improvements to their legacy life-cycle approach to building digital platforms and solutions.
It’s improvement for sure. Less time between software releases, less waste in the process. But it’s not transformation, a word even more abused than innovation.
The Sober Truth, Your Agile Transformation is the Wrong Focus
Here’s what transformation should be focused on instead: Helping ALL your CxOs (CEO, CIO, CFO, COO, and CMO) understand, I MEAN REALLY UNDERSTAND, what cloud-native computing is really all about.
The business implications for future growth and new competitive threats are extraordinary. Cloud Native is NOT about hosting your apps in the cloud to save money. It’s not even about agile process frameworks like Scrum, Kanban, or SAFe. It’s a radical deconstruction of your business both organizationally and digitally into small functional building blocks that can be programmatically composed and infinitely recombined — not only with your own building blocks, but with literally millions of other third-party services as well. The result? The ability to authentically experiment, innovate, and create new digital products, services, and customer experiences in hours and days with — and here’s the kicker — one-tenth the digital workforce you operate with today.
An example of this kind of exponential organization is Monzo, a dynamic new company that has built a globally scalable banking platform in months, not years, and with 10% of the staff typical of traditional Enterprise.
In one of Monzo’s recent engineering blog posts, you can see how differently they think, organize, and execute with cloud-native architecture and open source frameworks.
Your fragile digital platforms built on the 1990s’ paradigm of client-server computing need to be flushed.
There I said it. Yep, your CFO will howl, your CEO will roll his eyes, and your board will question why you’ve spent billions over the last three decades only to suggest we need even more money to transform these systems from fragile monoliths (a fancy term for the way your software teams have been building software since 1990) to microservices and serverless computing (a new way of building software that looks more like interchangeable Lego blocks).
Guess what? Your CFO and CEO are going to howl either way. You can either step up to the plate and move your company to a strategic sustainable future, or you can opt for the incremental benefits of tactical agile and lean improvements only to find that cloud-native competitors are taking increasing market share away from your company.
By the way, there is very good news here: With the right strategy and leadership, you can harvest your cloud-native building blocks (something called microservices) from your existing software monolith platforms to avoid starting over. More importantly with the right analytics you can fund your transition to the future from the assets you already have.
With the right strategy and analytics, you can fund your transition to a cloud-native future from the software assets you already have
Spoiler alert: There are many technical and organizational reasons why monoliths are easier and less risky to operate than cloud-native microservices. Beware, however, of those suggesting that because microservices are harder and require more sophisticated infrastructure they should be reserved only for your newest and most strategic digital initiatives. This is the same logic used by companies to kick down the road nearly every other can that represents a disruption to long-standing routines.
Click here for an explanation of microservices that even your six year old can understand.
The current fad of scaling up agile across large numbers of teams by employing complex process coordination frameworks is nonsense. Yes, they do provide marginal relief to companies saddled with tightly-coupled monolithic software platforms. But this approach to scaling agility is the equivalent of giving someone Demerol when they suffer from an abscessed tooth. Sure, we can mask the pain, but we’ve done nothing to cure the patient.
If your goal is authentic, systemic company wide agility, then avoid use of elaborate Agile scaling frameworks
Companies should be pursuing what I call an “opposite” strategy that involves scaling down, not up. By making cloud-native your target state, you create a pathway to realizing this goal. Doing so results in displacing monoliths with more atomic, composable building blocks. It is this new digital DNA that unleashes freedom and autonomy and blows up dependencies between teams. Ask yourself this question: If monoliths and tight coupling between teams were such a good thing, why are today’s cloud-native leaders (Amazon, Google, Netflix, Zalando, etc.) doing everything they can to avoid working this way? They’ve learned that when you enable teams to work autonomously and asynchronously, they out-produce, out-innovate, and outmaneuver their competitors.
For these reasons, I urge you to avoid continuing any transformation effort that is not built on a strategy to scale down (not up) to small, T-Shaped, cross-disciplinary teams comprised of business, strategy, design, and engineering/ops professional’s. Each of these teams will own a complete business service, product, or capability (with economic, market, technical, and operational responsibility) supported by cloud-native compute platforms (Microservices, Container Orchestration like Kubernetes, Docker Containers and Serverless Architecture). This means avoiding reliance on scaling frameworks that are optimized around coordination, planning, and synchronization as a means by which to manage the dependencies between teams.
Scaling agility is NOT a process problem, it’s a problem of tight coupling and dependencies between teams.
Check out this 12 minute video on how Zalando used cloud-native API’s and microservices to free teams from the tyranny of tight coupling and provide the autonomy and scaled agility no process framework could ever match.
While we’re tackling the limitations of fragile monoliths, we need to also slay the dragon of hierarchy. Despite the protests of many in the agile community, hierarchies actually have value — the problem is we’ve been using them for the wrong things. So rather than kill the hierarchy, we should narrow and refocus the hierarchy on those few things it does really well — namely, establishing mission, communicating company wide goals, and dynamically allocating resources based on market performance. All other decision-making should be pushed to local full-stack digital teams.
So how do we distribute authority and decision-making that accelerates corporate performance while also advancing authentic agility and organizational speed? The good news is that Andy Grove, while at Intel, invented the ideal agile performance and distributed-management system for companies. Remarkably, large traditional companies have almost never heard of this replacement for command and control management. Its name? Three simple letters: OKRs, which stands for Objectives and Key Results. One heads-up though: Most companies implementing OKRs are doing so poorly. Forget how Google uses OKRs. You’re not Google. The secret to how OKRs work best in large traditional firms is to mobilize them with natural teaming and network structures. We devote two chapters in our upcoming book to why this approach works so well. I’ll also be writing about this approach in upcoming article — stay tuned.
If you want to learn more about the success companies are having with OKRs check out this video by Ben Ross of MYOB, one of world’s leading accounting software companies. In this video, Ross describes how MYOB is transforming a legacy software company into a modern agile enterprise by employing OKRs to push out decision-making and autonomy to teams — while simultaneously increasing corporate performance and alignment.
Automate your digital product development ecosystem everywhere, but here’s the catch: You must simultaneously and ruthlessly strip out the fragile digital infrastructure built up over the last twenty years. This means rescuing high-value services and functions from your current monoliths and recombining these services in a cloud infrastructure built for a serverless world. Will you do this in one fell swoop? Of course not. But the nonsense of bimodal IT (a really bad idea from our experts at Gartner) is not your friend. Bimodal is the equivalent of New York City taxis’ trying to build an app for you to hail a cab. Do you really think that a bimodal taxi system has any hope of slowing Uber? Not gonna happen. You’re either all in or not in at all.
The folks over at Ansible are crushing this problem with some amazing new #DevOps tooling innovations. Check out this Ansible Explainer Video to learn more about how ruthless automation radically reduces the overhead and cumbersome coordination of departmental silos.
If you want to know more about our book, or just need help engaging your organization in answering these tough questions around the transition to cloud-native agility, please drop me a line at brad.murphy@gearstream.com or chat with me on Twitter at BradAMurphy
CEO @gearstream | Business Agility & Digital Product Innovation Strategist | Hacked 50 Global 2000 into Lean Innovators | Intl. Speaker
5 
1
5 
5 
1
CEO @gearstream | Business Agility & Digital Product Innovation Strategist | Hacked 50 Global 2000 into Lean Innovators | Intl. Speaker
"
https://medium.com/@manningbooks/cloud-native-applications-3dc8e5062fc?source=search_post---------209,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Jan 31, 2019·2 min read
Cloud computing is a game changer. Being able to automate and constantly adjust infrastructure is one important reason to use cloud computing for your clients. And developing and operating systems that are able to recover from failure automatically has never been easier. Cloud providers offer you the needed infrastructure for a very reasonable price and technologies like messaging systems and load balancers allow you to decouple different parts of your system and plan for failure.
Cloud Native Applications is a collection of hand-picked chapters presenting five topics that will give you insights into the world of cloud computing. Michael and Andreas Wittig, authors of Amazon Web Services in Action, Second Edition and AWS in Motion, selected these specific topics to teach you how you can gain value from cloud computing. You’ll learn how to use Amazon Web Services, one of the most popular public cloud providers. You’ll get to know to Docker and Mesos to automate and manage your cloud infrastructure. Decoupling different parts of your system is possible by using a messaging infrastructure. And you’ll discover different use cases for Netty, a framework that helps you to build solid networking communication into your applications for the cloud.
Download this free eBook now!
___________________________________________________________________
For more free content, check out Manning’s Free Content Center.___________________________________________________________________
About the authors:
Andreas Wittig and Michael Wittig are software engineers and consultants focused on AWS and web development. They migrated the first bank in Germany to AWS along with other heavily regulated businesses with legacy applications.
Follow Manning Publications on Medium for free content and exclusive discounts.
2 
2 
2 
Follow Manning Publications on Medium for free content and exclusive discounts.
"
https://medium.com/@manningbooks/going-cloud-native-7dd2d6fbb86f?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Oct 28, 2019·5 min read
Six Questions for Cornelia Davis, author of Cloud Native Patterns
Cornelia Davis is Vice President of Technology at Pivotal Software. A teacher at heart, she’s spent the last 25 years making good software and great software developers.
__________________________________________________________________
You can get Cloud Native Patterns for 39% off by entering intdavis into the discount code box at checkout at manning.com.__________________________________________________________________
All of the major cloud providers, as well as other service providers, offer traditional infrastructure services for compute, storage and networks. That is, you can rent a virtual machine and some block storage, wire everything together using networking services, and host whatever application you want on that infrastructure. In fact, it’s not that hard to simply “lift and shift” an application that might be running in an on-prem data center to that cloud. So the where is now in the cloud-but just because it’s deployed into the cloud, the application is no more resilient to constant changes than before, in part because it’s likely not as highly distributed as it needs to be.
On the other hand, cloud-native software is designed to remain totally functional as the infrastructure it’s running on changes-and some of the secrets to making that happen are redundancy and distribution. Now, that software isn’t necessarily running in the cloud; it could be deployed in an on-prem data center or even on a set of servers sitting under desks. But how it works yields far better resiliency and manageability than that traditional application we lifted and shifted into the cloud.
The most immediately visible difference is in the level of coupling between components that make up the application, and most importantly WHEN those components are connected to one another to form a complete application. Let’s take a shopping web site as an example. Written in a traditional style, we might have a single application deployed that includes the product catalog and search as well as shopping cart and checkout capabilities. Even if under the covers there are different software packages for these different functions, the fact that they are bundled together before deployment allows them to communicate with one another in a fairly predictable manner.
A cloud-native e-commerce application is made up of independently deployed and managed components: there is a deployment of the catalog, separate from the deployment of a search service, separate from a deployment of a shopping cart service, separate from the deployment of the service that processes payments. Of course, those services still need to interact, but some of the predictability you had before is gone and therefore your software has to do things it didn’t before. Yes, it’s more work, but in return you get the highly resilient, scalable and agile systems that are giving users far superior experiences today.
Let’s carry on with the case of the e-commerce site deployed as a set of separate services. As I said, these components must communicate with one another. For example, the search service needs to present results from the product catalog, so those two components will need to connect on occasion. These components are independently deployed and managed, and the coordinates (say the IP addresses) of each can change, so when the search service needs to contact the catalog service it needs to go through a protocol to find its latest coordinates: this is the service discovery pattern.
Then having found where to make the call, if the search service does not get a response to a retry pattern request to the catalog service, what does it do? It may want to employ a circuit-breaker to make the overall system more resilient to the inevitable network blips. But then the catalog service may want to protect itself from being overwhelmed with load by standing up a in front of its service.
These are three of the more than two dozen patterns I cover in the book. I spend as much time, if not more, covering the context in which these patterns will be applied, and explaining why they are valuable, as I do on the patterns themselves. My aim is to not only to teach the patterns but to teach my readers when and how best to use them.
Simply put, I work on emerging technology and how it can be used to generate business value. What that means is that I look at emerging trends-cloud-native software has been one of those for the last five or so years-and I work with customers, partners and our own research and development teams to apply them in a way that it yields positive outcomes for the users of that tech.
I am passionate about one simple thing: I just want everyone-women, men, non-binary, people of color, caucasians, everyone-to have a chance to have as much fun as I do! This is a great field that offers tremendous financial stability and flexibility, and it’s just a total blast to boot. Plus, there are volumes of data showing that greater diversity in the workforce yields better business outcomes, larger profits, higher return on investment, and greater shareholder value.
Organizations such as Girls Who Code (GWC) and GapJumpers (I’m on their board) are addressing some of the many obstacles that keep a great many people from engaging in this space. GWC is one that I am most drawn to because it engages young women who are juniors and seniors in high school, teaching them to code in an intensive and very fun seven-week summer program. I discovered computing in the first week of my junior year in high school, and it changed my life.
My first preference when I arrive in a new city is public transportation. When I do need to take a car, I prefer sharing rides because if two or three of us are transported in one car, that generates fewer greenhouse gasses than three cars driving next to each other on the same freeway. Imagine my delight when I shared a ride with someone who works full time as an environmentalist. He worked for an organization that, among other things, “greened buildings,” that is, they did things to reduce the environmental impact of these complexes. He’s worked on buildings all over the world, including the White House and the Pentagon--imagine the impact of going to LED bulbs throughout the Pentagon--I’ve been there, and it is massive!
Originally published at https://freecontent.manning.com.
Follow Manning Publications on Medium for free content and exclusive discounts.
1 
1 
1 
Follow Manning Publications on Medium for free content and exclusive discounts.
"
https://medium.com/@alibaba-cloud/collaborative-cloud-native-application-distribution-across-tens-of-thousands-of-nodes-in-minutes-8c7bb782e9be?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 30, 2019·9 min read
By Xie Yuning, Luo Jing, and Deng Juan.
During the 2019 Double 11 Shopping Festival, all of Alibaba’s core systems were running completely on the cloud for the first time ever. During the event-the world’s busiest and biggest annual online shopping event-Alibaba Cloud withstood a peak of 544,000 transactions per second, proving once again that “cloud native” is the right solution for Double 11.
As an important piece of the infrastructure in the cloud-native domain of the Alibaba ecosystem and e-commerce machine, Alibaba Cloud’s Container Registry (ACR) satisfied all of Alibaba’s requirements in terms of large-scale distribution in the run-up to Double 11.
To handle these requirements, much planning was implemented, and ACR was updated in advance to provide the necessary performance improvements, as well as increases in terms of observability and stability in a large-scale distribution context. Moreover, to prepare for 2019’s big Double 11 event, several petabytes of image data were also added to the registry, which happened to pull hundreds of millions of images each month. The end result of all of these efforts is that Alibaba Cloud Container Registry was made fully geared to providing a cloud-native application delivery pipeline, in addition to several other features, which can meet the demands of Alibaba and its customers in the cloud-native era.
In this article, we are going to continue look at Alibaba Cloud Container Registry (ACR). In particular, we will discuss how this product can be used to address the new development needs and challenges that both Alibaba and its customers face in the cloud-native era. In particular, we will cover how this product can be used to improve your workflow and provide cloud-native application distribution across tens of thousands of nodes in minutes.
As cloud-native technologies prevail the market, growing rapidly in popularity, Kubernetes has become the de facto standard for containerized applications and a leader in the cloud-native space. It uses a declarative container orchestration and management system to standardize software delivery. Kubernetes provides a unified API mode that can define resources in Kubernetes clusters through using YAML-format files. These YMAL-format resource definitions allow Kubernetes to be integrated with upstream and downstream systems more easily, as well as allow you to be able to complete a series of operations more quickly, which would have been previously performed manually or by using non-standard scripts. At the same time, based on the application delivery scenarios and requirements, the Kubernetes community has also generated a series of cloud-native application delivery standards in addition to the resource definition files in the native YAML format, such as Helm Chart, Operator, and Open Application Model.
In addition to the new delivery standards for cloud-native applications, users now also have higher requirements for delivery methods. More and more users want and even require cloud-native applications to be delivered in a more secure, process-based, and automated manner. As a result, what was originally distribution across tens of thousands of nodes in minutes has progressed into what is now multi-stage collaborative distribution across tens of thousands of nodes in minutes. In addition, globalized business development means that, on top of the completion of each stage in minutes, global distribution is also a basic requirement. As such, higher requirements are placed on platforms that support the distribution of cloud-native applications.
By controlling container image sizes, using P2P for image layer distribution, and optimizing the Registry server, at Alibaba Cloud we have significantly improved the performance of large-scale distribution and can now complete distribution across tens of thousands of nodes in minutes. In particular we did the following:
To enable enterprise users to enjoy these distribution capabilities, ACR officially launched ACR Enterprise Edition in March 2019. ACR Enterprise Edition provides enterprise-level cloud-native asset management, as well as the global and large-scale distribution of cloud-native applications. This service is suitable for enterprise-level customer who require a high level of security, need to deploy services across multiple regions, and have a large numbers of clusters and nodes. In addition, ACR Enterprise Edition further improves collaboration in terms of the hosting, delivery, and distribution of cloud-native assets during the distribution of cloud-native applications across tens of thousands of nodes in minutes.
In the production stage of cloud-native applications, you can directly upload cloud-native assets such as managed container images and Helm charts. You can also use the build function to automatically upload your own cloud-native assets from source code from Github, Alibaba Cloud, and GitLab and intelligently build a container image. To meet the need for more secure, process-based, and automated delivery of cloud-native applications, ACR Enterprise Edition introduced the cloud-native application delivery pipeline. The cloud-native application delivery pipeline starts with the hosting of cloud-native applications and ends with the distribution of cloud-native applications. The delivery pipeline is observable, traceable, and customizable. It allows you to implement global, muli-scenario automated delivery for a single change to an application. This greatly improves the efficiency and security of distributing cloud-native applications across tens of thousands of nodes.
In the cloud-native application delivery stage, you can automatically initiate static security scans and customize blocking policies. Once a high-risk vulnerability is detected in a static application, the service automatically blocks subsequent deployment links. You can update and optimize the application based on suggestions in the vulnerability report to build a new image version and then re-deliver the image.
In the cloud-native application distribution stage, after the front-facing stage is completed without interruption, cloud-native applications officially enter the global and large-scale distribution stage. To ensure that distribution across tens of thousands of nodes can be accomplished in minutes, ACR works with other Alibaba Cloud products seamlessly, including Alibaba Cloud Container Service, Elastic Container Instance (ECI), to provide an exceptional peer-to-peer distribution experience. For global distribution, the global synchronization efficiency of cloud-native applications is seven times higher than that of manual synchronization due to optimizations such as fine-grained synchronization policy scheduling and synchronization link optimization.
To implement large-scale peer-to-peer distribution, Dragonfly-based distribution solutions were repeatedly optimized for cloud environments. Ultimately, at Alibaba we also incorporated multiple innovative technologies to resolve various file distribution issues in scenarios like large-scale file downloading and cross-network isolation, greatly improving the capability of large-scale container image distribution. ACR’s average large-scale image distribution efficiency is several times higher than that generated by normal methods, and it suits scenarios where an individual container cluster has 100 or more nodes.
In addition to large-scale peer-to-peer distribution, this product also supports large-scale distribution based on image snapshots to better meet the need for large-scale distribution in specific scenarios. The image snapshot-based distribution method can avoid or reduce image layer downloads, greatly accelerating the creation of container groups. When working with Container Service for Kubernetes (ACK) and Elastic Container Instance (ECI), ACR can pull images on 500 nodes in seconds, enabling rapid scaling in response to sudden business changes.
Specific improvements and optimizations in stability are being made in several aspects, including monitoring and alert, fault tolerance and disaster recovery, dependency management, throttling and degradation, and capacity planning.
Based on the rich integration capabilities provided by the Alibaba Cloud platform, you can use ACR Enterprise Edition as a piece of your infrastructure for cloud-native asset hosting and distribution so to be able to deliver cloud-native applications to your customers. ACR Enterprise Edition works to build a container application market in Alibaba Cloud Marketplace, supports container product hosting and commercial distribution in the container application market, and builds a closed-loop cloud-native ecosystem. Independent software vendors (ISVs), such as Intel, Fortinet, and Authine, have already released containerized products on the cloud marketplace in the form of container images or Helm charts, achieving standardized delivery and commercialization. Customers can also obtain high-quality official Alibaba Cloud and ISV-provided container images from the container application market and quickly deploy them to Container Service clusters. As such, they can enjoy the rich cloud-native ecosystem of Alibaba Cloud.
Having supported the large-scale distribution demands of Double 11, Alibaba Cloud Container Register (ACR) can also provide comprehensive solutions for the cloud-native asset hosting and the distribution needs of Alibaba and its consumers. ACR can support the construction of a closed-loop cloud container ecosystem, making it a core piece of the infrastructure of the cloud-native space. In the future, Alibaba Cloud will continue to enrich ACR to provide users with an exceptional cloud-native application distribution experience that also offers superior performance.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/6-key-technologies-enabled-by-cloud-native-d08ea7adae14?source=search_post---------212,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 2, 2021·4 min read
To learn more about the many features of a cloud-native architecture and the benefits it can bring to your business, download The Cloud-Native Architecture White Paper today.
A cloud-native architecture can realize many benefits for your business, helping you achieve high availability, scalability, and expedite your release cycles, while streamlining your operation and maintenance. But, under the hood, how does a cloud native architecture achieve this?
First, let’s explain what “cloud native” means. The word “cloud” simply refers to the application residing in the cloud instead of in a traditional, on-premises data center. Businesses have been using the cloud in this manner for some 15 years. The word “native” is where things get interesting. From a business perspective, cloud native represents applications that are designed to run on the cloud, fully utilizing the elasticity and distributed nature of the cloud.
A cloud-native architecture features that are not business-centric are moved out of the code base of applications. In other words, non-business features are offloaded from the code base of applications to the cloud infrastructure, automating cloud-native applications and making them more lightweight and agile. Let’s look into some of those key cloud native technologies now.
Containers are standardized software units that package an application and its dependencies so the application can run efficiently and reliably across different computing environments. This helps organizations achieve the agility to grow their business in the long- and the elasticity to cope with short-term peaks in demand. Containers also provide seamless application distribution and delivery, decoupling applications from the underlying runtime environment to provide portability.
Kubernetes is the standard container method to schedule distributed resources and implement automated O&M. At Alibaba Cloud, our Container Service for Kubernetes (ACK) provides a fully-managed service compatible with Kubernetes, where users can quickly secure and manage their applications, and create a large number of Kubernetes clusters within seconds.
Microservices represent an architectural style that breaks down an application into a loosely coupled collection of services. Microservices are not only highly maintainable and testable but can also be deployed independently and organized around your business capabilities, making them easy to understand, develop and maintain.
A Kubernetes cluster can provide excellent support for your microservice operations and our Microservices Solution simplifies your container management, providing a lightweight deployment.
Serverless computing is a software design pattern where applications are hosted by a third-party service. Also known as Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS), serverless eliminates the need for server software and hardware management, freeing developers from these tasks.
Alibaba Cloud provides a range of tools for developers to embrace serverless services, including Function Compute, which prepares computing resources and can runs your code. SAE can also integrate your serverless and microservice architectures, and our Serverless Workflow can further coordinate the execution of multiple distributed tasks.
Alibaba Cloud Service Mesh (ASM) is a new technology, which was developed in a microservice software architecture and applies to distributed applications. It is designed to integrate features, including connectivity, security, throttling, and observability, between microservices into the platform infrastructure; decouples applications from the platform infrastructure; and centrally manages the traffic of service applications that run on various computing infrastructures, including ACK.
This allows developers to focus on the business logic without worrying about the microservice governance issues, improving the application development efficiency and accelerating business exploration and innovation.
Message services can also help maintain and monitor your cloud-native architecture, where the Alibaba Cloud Message Service can seamlessly transfer messages between your applications.
A cloud-native database service allows you to store, manage and retrieve data from the cloud, providing features beyond those available with traditional database models.
PolarDB is a next-generation distributed cloud relational database service, which is compatible with MySQL and PostgreSQL and Oracle syntax. Each cluster can use more than 1,000 CPU cores to process data and store up to 100 TB of data. PolarDB-X is capable of storing even larger amounts of data, handling ultra-high concurrent throughput and large table performance bottlenecks, and improving the low efficiency of complex computing.
If you need to store, manage and access big data across your cloud-native architecture, you need a new-generation cloud-native data warehouse that features high concurrency and low latency for data queries.
Our AnalyticDB for MySQL, for example, is compatible with MySQL protocols and the SQL:2003 syntax and can instantly analyze large amounts of diverse data. Our AnalyticDB for PostgreSQL solution was also developed based on the open source Greenplum Database project and is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems.
A cloud-native architecture must combine a range of state-of-the-art cloud-based technologies to help you reap the full rewards of the cloud. Alibaba Cloud can help you set-up, deploy and manage a cloud-native architecture to innovate and optimize your business. To find out more download The Cloud-Native Architecture White Paper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/personal-observations-and-reflections-on-the-cloud-native-software-architecture-49f684038cce?source=search_post---------213,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 1, 2020·25 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Yi Li, Senior Technical Expert of Alibaba Cloud and Director of Alibaba Container Service
Cloud native computing generally includes the following three dimensions: cloud native infrastructure, software architecture, and delivery and O&M systems. This article will focus on software architecture.
“Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems.” — From Wikipedia.
In my understanding, the main goal of software architecture is to solve these challenges:
The cloud native application architecture aims to build a loosely coupled, elastic, and resilient distributed application architecture. This allows us to better adapt to the needs of changing and developing business and ensure system stability. In this article, I’d like to share my observations and reflections in this field.
In 2012, Adam Wiggins, founder of Heroku, issued “The Twelve-Factor App”. It defined some basic principles and methodologies to be followed in building an elegant Internet application. It has also influenced many microservices application architectures. The Twelve-Factor App focuses on the healthy growth of applications, effective collaboration among developers, and avoiding the decay of software architecture. Even today, The Twelve-Factor App is also worth learning and understanding.
The Twelve-Factor App provides good architecture guides and helps us:
The core idea of microservices is that each service in the system can be independently developed, deployed, and upgraded, and that each service is loosely coupled. The cloud native application architecture further emphasizes loose coupling in the architecture to reduce dependency between services.
In object-oriented software architectures, the most important thing is to define the object and its interface contract. The SOLID Principles is the most recognized design principle.
The five principles together are called SOLID Principles, which helps us build application architectures with high cohesion, low coupling, and flexibility. In the distributed microservices application architecture, API First design is the extension of Contract First.
API should be designed first. User requirements are complex and changeable. For example, the application presentation mode and operation process may be different from the client to the mobile app. However, the conceptual model and service interaction of the business logic are relatively stable. APIs are more stable, while their implementations can be iterated and continuously changed. A well-defined API can ensure the quality of the application system.
API should be declarative and describable/self-describable. With standardized descriptions, it is easy for developers to communicate with, understand, and verify API, and to simplify collaborated development. API should support concurrent development by service consumers and providers, which accelerates the development cycle. It also should support the implementation of different technology stacks. For example, for the same API, developers can use Java to implement service, JavaScript to support front-end application, and Golang for service calls in server-side applications. This allows development teams to flexibly choose the right technology based on their own technical stacks and system requirements.
API should be equipped with SLA. As the integration interface between services, APIs are closely related to the stability of the system. SLA should be considered as a part of API design instead of adding it into API after deployment. Stability risks are ubiquitous in distributed systems. In such circumstances, we can conduct stability architecture design and capacity planning for independent services by taking the API-first design approach. Apart from that, we can also perform fault injection and stability tests to eliminate systemic stability risks.
In the API field, the most important trend is the rise of standardization technology. gRPC is an open-source, high-performance, platform-independent, and general RPC framework developed by Google. Its design consists of multiple layers. The data exchange format is developed based on Protobuf (Protocol Buffers), which provides excellent serialization and deserialization efficiency and supports multiple development languages. In terms of transport layer protocol, gRPC uses HTTP/2, which greatly improves the transport efficiency compared with HTTP/1.1. In addition, as a mature open standard, HTTP/2 has various security and traffic control capabilities as well as good interoperability. In addition to calling server-side services, gRPC also supports interactions between back-end services and browsers, mobile apps, and IoT devices. gRPC already has complete RPC capabilities in functions and also provides an extension mechanism to support new functions.
In the trend of cloud native, the interoperability demand for cross-platform, cross-vendor, and cross-environment systems will inevitably lead to open-standards-based RPC technology. Conforming to the historical trend, gRPC has been widely applied. In the field of microservices, Dubbo 3.0 announced its support for the gRPC protocol. In the future, we will see more microservices architectures developed based on gRPC protocol with multiple programing languages support. In addition, gPRC has become an excellent choice in the data service field. For more information, read this article on Alluxio
In addition, in the API field, open standards like Swagger (an OpenAPI specification) and GraphQL deserves everyone’s attention. You can choose one of them based on your business needs, this article will not go into details of these standards.
Before talking about Event Driven Architecture (EDA), let’s understand what event is first. Event is the record of things occurred and status changes. Records are immutable, which means that they cannot be changed or deleted, and they are sorted creating time. Relevant parties can get notification of these status changes by subscribing to published events, and then use the selected business logic to take actions based on obtained information.
EDA is an architecture that builds loosely coupled microservices systems. Microservices interact with each other through asynchronous event communication.
EDA enables complete decoupling between event producers and consumers. Thus, producers do not need to pay attention to the way that an event is consumed, and consumers do not need to concern about how the event is produced. We can dynamically add more consumers without affecting producers. By adding more message-oriented middleware, we can dynamically route and convert events. This also means that event producers and consumers are not time-dependent. Even if messages cannot be processed in time due to application downtime, the program can continue to obtain and execute these events from the message queue after recovery. Such a loosely coupled architecture provides greater agility, flexibility, and robustness for the software architecture.
Another important advantage of EDA is the improvement on system scalability. Event producers will not be blocked when waiting for event consumption. They can also adopt the publish-subscribe method to allow parallel event processing by multiple consumers.
In addition, EDA can be perfectly integrated with Function as a Service (FaaS). Event-triggered functions execute business logics, and glue code that integrates multiple services can be written in functions. Thus, event-driven applications can be easily and efficiently constructed.
However, EDA still faces many challenges as follows:
With its own advantages, EDA has many outstanding prospects in scenarios such as Internet application architectures, data-driven and intelligent business, and IoT. The detail of EDA will not be discussed here.
In the cloud native software architecture, except for focusing on how the software is built, we also need to pay attention to proper design and implementation of software. By doing so, better delivery and O&M of software can be achieved.
In The Twelve-Factor App, the idea of decoupling applications from operating environments has been proposed. The emergence of Docker container further strengthens this idea. Container is a lightweight virtualization technology for applications. Docker containers share the operating system kernel with each other and support second-level boost. Docker image is a self-contained application packaging format. It packages the application and its related files, such as system libraries and configuration files, to ensure consistent deployment in different environments.
Container can serve as the foundation for Immutable Infrastructure to enhance the stability of application delivery. Immutable Infrastructure is put forward by Chad Fowler in 2013. In this mode, instances of any infrastructures, including various software and hardware like servers and containers, will become read-only upon creation. Namely, no modification can be made to those instances. To modify or upgrade certain instances, developers can only create new instances to replace them. This mode reduces the burden of configuration management, ensures that system configuration changes and upgrades can be reliably and repeatedly executed, and avoids troublesome configuration drift. It is easy for Immutable Infrastructure to solve differences between deployment environments, enabling a smoother process of continuous integration and deployment. It also supports better version management, and allows quick rollback in case of deployment errors.
As a distributed orchestration and scheduling system for containers, Kubernetes further improves the portability of container applications. With the help of Loadbalance Service, Ingress, CNI, and CSI, Kubernetes helps service applications to reconcile implementation differences of underlying infrastructures, so as to achieve flexible migration. This allows us to realize the dynamic migration of workloads among data centers, edge computing, and cloud environments.
In the application architecture, application logic should not be coupled with static environment information, such as IP and mac addresses. In the microservices architecture, Zookeeper and Nacos can be used for service discovery and registration. In Kubernetes, the dependence on the IP address of service endpoint can be reduced through Service and Service Mesh. In addition, the persistence of the application state should be implemented through distributed storage or cloud services, which can greatly improve the scalability and self-recovery capabilities of the application architecture.
Observability is one of the biggest challenges for distributed systems. Observability can help us understand the current state of the system, and can be the basis for application self-recovery, elastic scaling, and intelligent O&M.
In the cloud-native architecture, self-contained microservices applications should be observable so that they can be easily managed and explored by the system. First, an application should be observable on its own health status.
In Kubernetes, a liveness probe is provided to check application readiness through TCP, HTTP, or command lines. For the HTTP-type probe, Kubernetes regularly accesses this address. If the return code of this address is not between 200 and 400, the container is considered unhealthy, and the container will be forbidden for reconstruction.
For slow-start applications, Kubernetes supports the readiness probe provided by business container to avoid importing traffic before the application is started. For HTTP-type probe, Kubernetes regularly accesses this address. If the return code is not between 200 and 400, the container is considered to be unable to provide services, and then requests will not be scheduled to this container.
Meanwhile, the observable probe has been contained in new microservices architectures. For example, two actuator addresses, which are /actuator/health/liveness and /actuator/health/readiness, have been released by Spring Boot 2.3. The former is used as the liveness probe, whereas the latter is used as the readiness probe. Business applications can read, subscribe to, and modify the Liveness State and Readiness State through the system event mechanism of Spring. This allows Kubernetes to perform more accurate self-recovery and traffic management.
For more information, see this article
In addition, application observability consists of three key capabilities: logging, metrics, and tracing.
In the distributed system, stability, performance, and security problems can occur anywhere. Additionally, these problems require full-procedure observability assurance and the coverage of different layers, such as the infrastructure layer, PaaS layer, and application layer. In addition, the association, aggregation, query, and analysis of observability data should be realized among different systems.
The observability field of software architecture has broad prospects, and many technological innovations in this field have emerged. In September 2020, CNCF released the technology radar of cloud native observability
In the Technology Radar, Prometheus has become one of the preferred open-source monitoring tools for cloud native applications for enterprises. Prometheus has developed an active community of developers and users. In the Spring Boot application architecture, the introduction of the micrometer-registry-prometheus dependency allows Prometheus to collect application monitoring metrics. For more information, see this documentation
In the field of distributed tracing, OpenTracing is an open-source project of CNCF. It is a technology-neutral standard for distributed tracing. It provides a unified interface and is convenient for developers to integrate one or more types of distributed tracing implementations in their own services. Additionally, Jaeger is an open-source distributed tracing system from Uber. It is compatible with the OpenTracing standard and has been approved by CNCF. In addition, OpenTelemetry is a potential standard, trying to integrate OpenTracing and OpenCensus to form a unified technical standard.
For many remaining business systems, existing applications are not fully observable. The emerging Service Mesh technology can become a new way to improve the observability of these systems. Through request intercepting hosted in the data plane, the mesh can obtain performance indicators of inter-service calls. In addition, the service caller only needs to add the message header to be forwarded, and the complete tracing information can be obtained in the Service Mesh. This greatly simplifies the observability construction, allowing existing applications to integrate into cloud-native observability systems at a low cost.
Alibaba Cloud offers a wide range of observability capabilities. Among them, XTrace supports OpenTracing and OpenTelemetry standards. Application Real-time Monitoring Service (ARMS) provides the hosted Prometheus service, which allows developers to focus on other issues instead of the high availability and capacity challenges of systems. Observability is the foundation of Algorithmic IT Operations (AIOps) and will play an increasingly important role in enterprises’ IT application architectures in the future.
“Murphy’s Law” says that “Anything that can go wrong will go wrong”. The distributed system may be affected by factors, such as hardware and software, or be internally and externally damaged by human. Cloud computing provides infrastructure that is higher in SLA and security than self-built data centers. However, we still need to pay close attention to system availability and potential “Black Swan” risks during application architecture design.
To achieve systematic stability, developers need to take an overall consideration in several aspects, such as software architecture, O&M system, and organizational guarantee. In terms of architecture, the Alibaba economy has rich experience in defensive design, traffic limiting and degradation, and fault isolation. It has also provided excellent open-source projects such as Sentinel and ChaosBlade to the community.
In this article, I will talk about several aspects that can be further discussed in the cloud native era. I summarized my reflections as: “Failures can and will happen, anytime, anywhere. Fail fast, fail small, fail often and recover quickly.”
Firstly, “Failures can and will happen”. Therefore, we need to make servers more replaceable. There is a very popular metaphor in the industry, that is, “Pets vs. Cattle”. When facing architectures, should we treat servers as raising pets with carefulness to avoid downtime and even rescue it at all costs? Or should we treat servers as raising cattle, which means they can be abandoned and replaced in case of problems? The cloud native architecture suggests that each server and component can afford to fail. Provided that the failure will not affect the system and servers and components are capable of self-recovery. This principle is based on the decoupling of application configuration and persistence from specific operating environment. The automated O&M system of Kubernetes makes server replacement simpler.
Secondly, “Fail fast, fail small, and recover quickly”. “Fail fast” is a very counter-intuitive design principle. As failures cannot be avoided, the earlier problems are exposed, the easier it is to recover and the fewer problems occurring in the production environment there are. After adopting the Fail-fast policy, our focus will shift from how to exhaust problems in the system to how to quickly find and gracefully handle failures. In the R&D process, integration tests can be used to detect application problems as early as possible. At the application layer, modes, like Circuit Breaker, can be used to prevent overall problems caused by local faults of a dependent service. In addition, Kubernetes health monitoring and observability can detect application faults. The circuit breaker function of the Service Mesh can extend the fault discovery, traffic switching, and fast self-recovery capabilities out of the application implementation, which will be guaranteed by system capabilities. The essence of “Fail small” is to control the influence range of failures. This principle requires constant attention in terms of architecture design and service design.
Thirdly, “Fail often”. Chaos engineering is an idea that periodically introduces fault variables into the production environment to verify the effectiveness of the system in defending against unexpected faults. Netflix has introduced chaos engineering to solve stability challenges of microservices architecture. The chaos engineering is also widely used by many Internet companies. In the cloud native era, there are more new approaches available. For example, Kubernetes allows us to easily inject faults, shut down pods, and simulate application failure and self-recovery process. With Service Mesh, we can perform more complex fault injection for inter-service traffic. For example, Istio can simulate fault scenarios such as slow response and service call failure, helping us verify the coupling between services and improve the stability of systems.
For more stability discussions about architecture delivery and O&M, I will share them in the next article.
The cloud native software architecture aims to drive developers to focus on business logic and enable the platform to handle system complexity. Cloud native computing redefines the boundary between application and application infrastructure, further improving development efficiency and reducing the complexity of distributed application development.
In the microservices era, application frameworks such as Spring Cloud and Apache Dubbo have achieved great success. Through code libraries, they provide service communication, service discovery, and service governance, such as traffic shifting, blow, traffic limiting, and full-procedure tracing. These code libraries are built inside applications, and are released and maintained along with applications. Therefore, this architecture has some unavoidable challenges:
To solve these challenges, the community proposed Service Mesh architecture. It decouples business logic from service governance capabilities. By submerging architecture in infrastructure, service governance can be independently deployed on both service consumer and provider sides. In this way, decentralization is achieved, and the scalability of the system is guaranteed. Service governance can also be decoupled from business logic. Thus, service governance and business logic can evolve independently without mutual interference, which improves the flexibility of the overall architecture evolution. At the same time, the Service Mesh architecture lowers the intrusiveness towards business logic and the complexity of polyglot support.
The Istio project led by Google, IBM, and Lyft is a typical implementation of the Service Mesh architecture and has become a new phenomenal “influencer”.
The preceding picture shows the architecture of Istio, which is logically divided into the data plane and the control plane. The data plane is responsible for data communication between services. The application is paired with the intelligent proxy Envoy deployed in sidecar mode. The Envoy intercepts and forwards the network traffic of application, collects telemetry data, and executes service governance policies. In the latest architecture, istiod, as the control plane of Istio, is responsible for configuration management, delivery, and certificate management. Istio provides a series of general service governance capabilities, such as service discovery, load balancing, progressive delivery (gray release), chaos injection and analysis, full-procedure tracing, and zero-trust network security. These capabilities can be orchestrated into IT architectures and release systems of upper-layer business systems.
The Service Mesh achieves the separation of the data plane and the control plane in terms of architecture, which makes it a graceful architecture. Enterprise customers have diversified requirements for the data plane, such as various protocols support, like Dubbo, customized security policies, and observability access. The capabilities of the service control plane also change rapidly, including basic service governance, observability, security systems, and stability assurance. However, APIs between the control plane and the data plane are relatively stable.
CNCF established the Universal Data Plane API Working Group (UDPA-WG) in order to develop standard APIs in the data plane. Universal Data Plane API (UDPA) aims to provide standardized and implementation-independent APIs for L4 and L7 data plane configurations, which is similar to the role of OpenFlow for L2, L3, and L4 in SDN. UDPA covers service discovery, load balancing, route discovery, monitor configuration, security discovery, load reporting, and health check delegation.
UDPA is gradually developed based on existing Envoy xDS APIs. Currently, in addition to supporting Envoy, UDPA supports client-side load balancing, such as gRPC-LB, as well as more data plane proxies, hardware load balancing, and mobile apps.
We know that Service Mesh is not a silver bullet. Its architecture adds a service proxy in exchange for architecture flexibility and system evolvability. However, it also increases the deployment complexity (sidecar management) and performance loss (two forwarding added). The standardization and development of UDPA will bring new changes to the Service Mesh architecture.
In the latest version, gRPC starts to support UDPA load balancing.
The concept of “Proxyless Service Mesh” has been created. The following figure shows the diagram of the concept:
As shown above, gRPC applications obtain service governance policies directly from the control plane. gPRC applications can also directly communicate with each other without any additional proxy. This reflects the ambition of the open Service Mesh technology is to evolve into a cross-language service governance framework, which can give consideration to the standardization, flexibility, and operational efficiency. Google’s hosting Service Mesh products have taken the lead in providing support for “proxyless” gRPC applications.
For distributed applications, Bilgin Ibryam gives analysis and summary of four typical types of demands in the article, Multi-Runtime Microservices Architecture.
Those who are familiar with traditional enterprise architectures may find that the traditional Java EE (now renamed as Jakarta EE) application server also aims to solve similar problems. The architecture of a typical Java EE application server is shown in the following figure. The application lifecycle is managed by various application containers, such as Web container and EJB container. Application security management, transaction management, and connection management are all completed by the application server. The application can access external enterprise middleware, such as databases and message queues, through standard APIs, like JDBC and JMS.
Different external middleware can be pluggable from the application server by using the Java Connector Architecture specification. The application is dynamically bound to specific resources through JNDI at runtime. Java EE solves the cross-cutting concern of the system in the application server. Thus, Java EE allows developers to only focus on the business logic of the application, which improves the development efficiency. At the same time, the application’s dependence on the environment and middleware can be reduced. For example, ActiveMQ used in the development environment can be replaced by IBM MQ in the production environment, without modifying the application logic.
In terms of architecture, Java EE is a large monolith application platform. The iteration of its architecture is too slow that it cannot keep up with changes of architecture technologies. Due to its complexity and inflexibility, Java EE has been forgotten by most developers since the rise of microservices.
Microsoft gives a solution called Dapr. Dapr is an event-driven and portable runtime environment for building microservices applications. It supports cloud or edge deployment of applications, and the diversity of programing languages and frameworks. Dapr adopts the Sidecar mode to separate and abstract some cross-cutting requirements in the application logic. This decouples the application from the runtime environment and external dependencies, including dependencies among services.
The preceding figure shows the functions and positioning of Dapr:
Although Dapr is similar to Service Mesh in architecture and service governance, they are essentially quite different. For applications, Service Mesh is a transparent infrastructure, while Dapr provides abstractions for state management, service calling, fault handling, resource binding, publishing and subscription, and distributed tracing. To explicitly call Dapr, applications need support from SDK, HTTP, and gRPC. Dapr is a developer-oriented development framework.
Dapr is still very young and is undergoing rapid iteration. So, there is still a long way to go for Dapr to be supported by developers and third-party manufacturers. However, Dapr has revealed a new direction for us. By separating concerns, developers are allowed to focus only on the business logic, while concerns to distributed architectures submerges in infrastructures. Business logic should be decoupled from external services to avoid vendor binding. In addition, application and application runtime should be two independent processes that interact through standard APIs. The lifecycle should be decoupled to facilitate upgrades and iterations.
In the previous article, I have introduced Serverless application infrastructures, such as FaaS and Serverless container. In this article, I’d like to discuss some thoughts on the architecture of the FaaS application.
The core principle of FaaS is that developers do not have to focus on infrastructure O&M, capacity planning, or scaling. They only need to pay for cloud resources and services they used. By doing so, developers can focus on other issues rather than infrastructure O&M and reuse existing cloud service capabilities as much as possible. This will help developers to reallocate development time to things with more value and direct impacts on users, such as good business logic, user-attracting interfaces, and fast-responsive and reliable APIs.
At the software architecture level, FaaS splits complex business logic into a series of fine-grained functions, and calls these functions in event-driven mode. Since functions are loosely coupled, they can be combined and coordinated together in the following two modes:
Workflow Orchestration: Take Alibaba Cloud Serverless Workflow as an example. Tasks can be orchestrated through a declarative business process. This simplifies complex operations required in developing and running business, such as task coordination, state management, and fault handling. In this way, developers can only focus on business logic development.
Event Choreography: Function services exchange messages through events. Message middleware, such as EventBus, forward events and trigger function execution. The following is an example of a scenario where EventBridge connects several function-based business logics, including place-order, notify-user, notify-restaurant, accept-order, and complete-order. This mode is more flexible, and system performance is better. However, it lacks explicit modeling, and the development and maintenance are relatively complicated.
Serverless has many advantages, such as reducing O&M costs, improving system security and R&D efficiency, and accelerating business delivery. However, Serverless still has some unavoidable problems in following aspects:
Cost management: One of the weaknesses of the “pay-as-you-go” mode is that it is impossible to accurately predict the specific cost. It is different from budget management methods of many organizations.
Vendor targeting: Although Serverless applications are based on open languages and frameworks, most Serverless applications rely on some non-standard Backend as a Service (BaaS), such as object storage, key-value database, authentication, logging, and monitoring.
Debugging and monitoring: Compared with traditional application development, Serverless applications do not provide proper debugging and monitoring tools. Good observability is an important aid for Serverless computing.
Architecture complexity: Serverless developers do not need to focus on the complexity of underlying infrastructures, but the complexity of the application architecture requires special attention. Event-driven architecture and fine-grained function microservices are very different from traditional development methods. Developers need to apply them in appropriate scenarios based on business needs and technical capabilities, and then gradually expand their application scopes.
For more information about typical Serverless application architectures, read this article
The technical report, Cloud Programming Simplified: A Berkeley View on Serverless Computing, is also a good reference for further understanding of Serverless computing.
Faster, lighter, and more agile application runtime technologies are what cloud native computing is continuously pursuing.
Smaller size: For microservices distributed architectures, the smaller size means lower download bandwidth and faster distribution and download speed.
Faster booting: For traditional monolith applications, booting speed is not a key metric compared to operating efficiency. The reason is that these applications are rebooted and released at a relatively low frequency. For microservices applications that require rapid iteration and horizontal scaling, however, faster booting means higher delivery efficiency, quicker rollback, and faster fault recovery.
Fewer resources: Lower resource usage at runtime means higher deployment density and lower computing costs.
For those reasons, the number of developers using languages such as Golang, Node.js, and Python continues to climb. There are several technologies that deserve your attention:
In the Java field, GraalVM has gradually matured. It is an enhanced cross-language full-stack virtual machine based on HotSpot and supports multiple programing languages, including Java, Scala, Groovy, Kotlin, JavaScript, Ruby, Python, C, and C++. GraalVM allows developers to compile programs as local executable files in advance.
Compared with classic Java VM, the program generated by GraaIVM has lower booting time and runtime memory cost. As next-generation Java frameworks customized in the cloud native, Quarkus and Micronaut can achieve amazing booting time and resource cost. For more analysis, see cloud native evolution of Java.
WebAssembly is another exciting technology. WebAssembly is a secure, portable, and efficient virtual machine sandbox designed for modern CPU architectures. It can be used anywhere (like servers, browsers, and IoT devices), running applications safely on any platform with different operating systems or CPU architectures. WebAssembly System Interface (WASI) is used to standardize interaction abstractions of WebAssembly applications and system resources, such as file system access, memory management, and network connection. WASI provides standard APIs similar to POSIX.
Platform developers can adopt different WASI APIs for different implementations according to specific operating systems and operating environments. Cross-platform WebAssembly applications are allowed to be run on different devices and operating systems. This allows application operating to be decoupled from specific platform environment, gradually realizing “Building Once, Run Anywhere”. Although WebAssembly has surpassed the browser field, its development is still in the early stage. We are looking forward that the community will come together to further develop WebAssembly. If you are interested, check out this link for the combination of WebAssembly and Kubernetes:
Cloud native software architectures are developing rapidly, and they involve a wide range of content. The above-mentioned content is more of a personal summary, understanding, and judgment. So, I am looking forward to having an in-depth communication and discussion with everyone.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
4 
4 claps
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-storage-container-storage-and-kubernetes-storage-volumes-8ddd8c50b83c?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 23, 2021·22 min read
By Kan Junbao (Junbao), Alibaba Cloud Senior Technical Expert
This series of articles on cloud-native storage explains the concepts, features, requirements, principles, usage, and cases of cloud-native storage. It aims to explore the new opportunities and challenges of cloud-native storage technology. This article is the second in the series and explains the concepts of container storage. If you are not familiar with this concept, I suggest that you read the first article of this series, “Cloud-Native Storage: The Cornerstone of Cloud-Native Applications.”
Docker storage volumes and Kubernetes storage volumes are essential for cloud-native storage.
This container service is widely used because it provides an organization format for container images during container runtime. Multiple containers can share the same image resource, or more precisely, the same image layer, on the same node by using the container image reuse technology. This means image files are not copied or loaded each time a container is started. This reduces the storage space usage of hosts and improves the container startup efficiency.
The same image resource can be shared by different running containers, and data can be shared by different images. This improves the storage efficiency of nodes. An image is divided into multiple data layers, and the data of each layer is superimposed and overwritten. This structure enables image data sharing.
Each layer of a container image is read-only so that image data can be shared by multiple containers. In practice, when you start a container by using an image, you can read and write this image in the container. How is this done?
When a container uses an image, the container adds a read/write layer at the top of all image layers. Each running container mounts a read/write layer on top of all layers of the current image. All operations on the container are completed at this layer. When the container is released, the read/write layer is also released.
As shown in the preceding figure, three containers exist on the node. Container 1 and Container 2 run based on Image 1, and Container 3 runs based on Image 2.
The image storage layers are explained as follows:
The two images share Layer 3 and Layer 5.
Container storage is explained as follows:
Data sharing based on the layered structure of container images can significantly reduce the host storage usage by the container service.
In the container image structure with the read/write layer, data is read and written in the following way:
In the case of data read, when different layers contain duplicate data, data at the lower layer is overwritten by the same data at the upper layer.
Data is written at the uppermost read/write layer when you modify a file in a container. The technologies involved are copy-on-write (CoW) and allocate-on-demand.
CoW indicates that data is copied only when it is written. It is applicable to scenarios where existing files are modified. CoW allows all containers to share the file system of an image and read all data from this image. When you write a file, this file is copied to the uppermost read/write layer of the image for modification. For all the containers that share the same image, each container writes the file copies instead of the original files of the image. When multiple containers write the same file, each container creates a copy of this file in its file system and modifies this copy independently.
Storage space is allocated only when new files are written to images. This improves the utilization of storage resources. For example, disk space is allocated to a container only when new files are written to this container. Disk space is not pre-allocated during container startup.
Storage drivers are used to manage container data at each layer to enable image sharing among containers. Storage drivers support read and write operations on files. The storage drivers of containers store and manage data at the read/write layer. Common storage drivers include:
The following section explains how AUFS works.
AUFS is a type of union file system (UFS) and a file-level storage driver.
AUFS is a layered file system able to transparently superimpose one or more existing file systems to form a single layer. AUFS can mount different directories to the file systems under the same virtual file system.
You can superimpose and modify files layer by layer. Only the file system at the uppermost layer is writable, whereas the file systems at lower layers are read-only.
When you modify a file, AUFS creates a copy of this file and uses CoW to transfer this copy from the read-only layer to the writable layer for modification. The modified file is stored at the writable layer.
In Docker, the uppermost writable layer is the container runtime, and all the lower layers are image layers.
Any data read and write operations on applications that run inside a container are completed at the read/write layer of the container. The image layers and the read/write layer are mapped to the underlying structure, which is responsible for intra-container storage in the container’s internal file system. A container data volume allows applications inside a container to interact with external storage. This volume is similar to an external storage device, like a USB flash drive.
Containers store data temporarily. The stored data is deleted when containers are released. After you mount external storage to a container file system by using a data volume, the application can reference external data or persistently store its generated data in the data volume. Therefore, container data volumes provide a method for data persistence.
Container storage consists of multiple read-only layers (image layers), a read/write layer, and external storage (data volume).
Container data volumes can be divided into single-node data volumes and cluster data volumes. A single-node data volume is a data volume that the container service mounts to a node. Docker volumes are typical single-node data volumes. Cluster data volumes provide cluster-level data volume orchestration capabilities. Kubernetes data volumes are typical cluster data volumes.
A Docker volume is a directory that can be used by multiple containers simultaneously. It is independent of the UFS and provides the following features:
Bind: You can directly mount host directories and files to containers.
Volume: You can enable this mode when you use third-party data volumes.
Tmpfs is a non-persistent volume type, which stores data in the memory. Tmpfs data is easy to lose.
-v: src:dst:opts: This is only applicable to single-node data volumes.
Example:
-v: src:dst:opts: This is only applicable to single-node data volumes.
Example:
This section explains how to use Docker data volumes.
Anonymous data volumes: docker run –d -v /data3 nginx
By default, the directory /var/lib/docker/volumes/{volume-id}/_data is created on the host for mapping purposes.
Named data volumes: docker run –d -v nas1:/data3 nginx
If the nas1 volume cannot be found, a volume of the default type (local) is created.
If the host does not contain the /test directory, this directory is created by default.
A volume container is a running container. Other containers can inherit the data volumes mounted to this container. All the mounts of the container are reflected in the reference containers.
The preceding command is used to inherit all data volumes from a configured container, including custom volumes.
You can configure mount propagation for Docker volumes by using the propagation command.
Examples:
Mount visibility in Volume mode:
Mount visibility in Bind mode: This is determined by host directories.
You can use Docker data volumes to mount the external storage of containers to container file systems. To allow containers to support more external storage classes, Docker supports the mounting of different types of storage services through storage plug-ins. The extension plug-ins are also known as volume drivers. You can develop a storage plug-in for each storage class.
The Docker daemon communicates with volume drivers in the following ways:
Example:
Docker volume drivers can be used to manage data volumes in single-node container environments or on the Swarm platform. Currently, Docker volume drivers are less used because Kubernetes has become increasingly popular. For more information about Docker volume drivers, see https://docs.docker.com/engine/extend/plugins_volume/
As mentioned above, data volumes can be used to persistently store container data. Below, we will discuss how to define storage for loads or pods in a Kubernetes orchestration system during runtime. Kubernetes is a container orchestration system that is designed for the management and deployment of container applications throughout the cluster. Therefore, we need to define application storage in Kubernetes based on clusters. Kubernetes storage volumes define the relationship between applications and storage in a Kubernetes environment. The following sections explain related concepts.
A data volume defines the details of external storage and is embedded in a pod. In essence, a data volume records information about external storage for the Kubernetes system. When loads need external storage, the system queries related information in the data volume and mounts external storage.
Common types of Kubernetes volumes include:
Examples of volume templates:
PVCs are a type of abstract storage volume in Kubernetes and represents the data volume of a specific storage class. PVCs are designed to separate storage from application orchestration. A PVC object abstracts storage details and implements storage volume orchestration. This makes storage volume objects independent of application orchestration in Kubernetes and decouples applications from storage at the orchestration layer.
PVs are a specific type of storage volume in Kubernetes. A PV object defines a specific storage class and a set of volume parameters. All information about the target storage service is stored in a PV object. Kubernetes references the PV-stored information for mounting.
The following figure shows the relationships among loads, PVC objects, and PV objects.
A PV object can be used to separate storage from application orchestration and mount data volumes. So what is the purpose of combining PVC and PV objects? Through the combined use of PVC and PV objects, Kubernetes implements secondary abstraction of storage volumes. A PV object describes a specific storage class by defining storage details. Users do not want to have to study underlying details when they use storage services at the application layer. Therefore, it is not a user-friendly practice to define specific storage services at the application orchestration layer. To fix this problem, Kubernetes implements secondary abstraction of storage services. Kubernetes only extracts the parameters related to user relationships, and uses PVC objects to abstract underlying PV objects. Therefore, PVC and PV have different focuses. A PVC object focuses on users’ storage needs and provides a unified way to define storage. A PV object focuses on storage details, allowing users to define specific storage classes and storage mount parameters.
Specifically, the application layer declares a storage need (PVC), and Kubernetes selects the PV object that best fits this PVC object and binds them together. PVCs are a type of storage object used by applications and belong to the application domain. That means the PVC object resides in the same noun space as the application. PVs are a type of storage object that belongs to the storage domain instead of a noun space.
PVC and PV objects have the following attributes:
The PVC definition template is as follows:
The PVC-defined storage interfaces are related to the storage access mode, resource capacity, and volume mode. The main parameters are described as follows:
“accessModes” defines the mode of access to storage volumes. The options include ReadWriteOnce, ReadWriteMany, and ReadOnlyMany.
Note: The preceding access modes are only declared at the orchestration layer. Whether stored files are readable and writable is determined by specific storage plug-ins.
“storage” defines the storage capacity that the specified PVC object is expected to provide. The defined data size is only declared at the orchestration layer. The actual storage capacity is determined by the type of the underlying storage service.
“volumeMode” defines the mode of mounting storage volumes. The options include FileSystem and Block.
“FileSystem” specifies that data volumes are mounted as file systems for use by applications.
“Block” specifies that data volumes are mounted as block devices for use by applications.
The following example illustrates how to orchestrate the PV objects of data volumes in cloud disks.
Only PVC-bound PV objects can be consumed by pods. The PVC-PV binding process is the process of PV consumption. Only a PV object that meets the following requirements can be bound to a PVC object:
Only a PV object that meets the preceding requirements can be bound to the PVC object.
If multiple PV objects meet requirements, the most appropriate PV object is selected for binding. Generally, the PV object with the minimum capacity is selected. If multiple PV objects have the same minimum capacity, one of them is randomly selected.
If no PV objects meet the preceding requirements, the PVC object enters the pending state until a conforming PV object appears.
As we have learned earlier, PVCs are secondary storage abstractions for application services. A PVC object provides simple storage definition interfaces. PVs are storage abstractions with complex details. PV objects are generally defined and maintained by the cluster management personnel.
Storage volumes are divided into dynamic storage volumes and static storage volumes based on the PV creation method.
A cluster administrator analyzes the storage needs of the cluster and pre-allocates storage media. The administrator also creates PV objects to be consumed by PVC objects. If PVC needs are defined in loads, Kubernetes binds PVC and PV objects according to relevant rules. This allows applications to access storage services.
A cluster administrator configures a backend storage pool and creates a storage class template. When a PVC object needs to consume a PV object, the Provisioner plug-in dynamically creates a PV object based on the PVC needs and the details of the storage class.
Dynamic and static storage volumes are compared as follows:
Dynamic storage volumes provide the following advantages:
When you declare a PVC object, you can add the StorageClassName field to this PVC object. This allows the Provisioner plug-in to create a suitable PV object based on the definition of StorageClassName when no PV object in the cluster fits the declared PVC object. This process can be viewed as the creation of a dynamic data volume by the Provisioner plug-in. The created PV object is associated with the PVC object based on StorageClassName.
A storage class can be viewed as the template used to create a PV storage volume. When a PVC object triggers the automatic PV creation process, a PV object is created by using the content of a storage class. The content includes the name of the target Provisioner plug-in, a set of parameters used for PV creation, and the reclaim mode.
A storage class template is defined as follows:
When you create a PVC declaration, Kubernetes finds a suitable PV object in the cluster to be bound to the created PVC object. If no suitable PV object exists, the following process is triggered:
Certain types of storage, such as Alibaba Cloud disks, impose limitations on the mount attribute. For example, data volumes can only be mounted to nodes in the same zone as these volumes. This type of storage volume produces the following problems:
The storage class template provides the volumeBindingMode field to fix the preceding problems. When this field is set to WaitForFirstConsumer, the Provisioner plug-in delays data volume creation when it receives the PVC pending state. Instead, the Provisioner plug-in creates a data volume only after the PVC object is consumed by a pod.
The detailed process is as follows:
The delayed binding feature is used to schedule application loads to ensure that sufficient resources are available for use by pods before dynamic volumes are created. This also ensures that data volumes are created in zones with available resources and improves the accuracy of storage planning.
We recommend that you use the delayed binding feature when you create dynamic volumes in a multi-zone cluster. The preceding configuration process is supported by Alibaba Cloud Container Service for Kubernetes (ACK) clusters.
The following example illustrates how pods consume PVC and PV objects:
Template explanation:
According to the PVC-PV binding logic, this PV object meets the PVC consumption requirements. Therefore, the PVC object is bound to the PV object and mounted to a pod.
This article gives a detailed explanation of container storage, including single-node Docker data volumes and cluster-level Kubernetes data volumes. Kubernetes data volumes are designed for cluster-level storage orchestration and can be mounted to nodes. Kubernetes provides a sophisticated architecture to implement complex storage volume orchestration capabilities. The next article will explain the Kubernetes storage architecture and its implementation process.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
3 
3 
3 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@LawrenceHecht/cloud-native-ecosystem-proxy-technologies-on-docker-fee3c93ee4a9?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lawrence Hecht
Jun 24, 2017·1 min read
Originally published in The New Stack Update.
Since NGINX is deployed most often on Datadog customers’ Docker containers, we thought it appropriate to use the chart above as a “proxy” for the cloud-native world. From this perspective, all the top technologies are open source, and most of them support data and messaging infrastructure. Interestingly, every single technology is now offered as a service (aaS). Despite the effort needed to keep distributions up to date, most won’t let you choose to pay for supported elasticsearch, Redis or MySQL. Moving forward, we know companies are looking forward to selling you services with verified Docker images that contain many of these offerings. Alternatively, most major cloud providers are beginning to offer them, just as they also provide supported Apache servers with web hosting.
Edit/analyze/curate. Interest in #techpr/AR, #techpolicy, #blockchain #opendata, #analytics. Know about politics, econ, #enterpriseIT and surveys
1 
1 
1 
Edit/analyze/curate. Interest in #techpr/AR, #techpolicy, #blockchain #opendata, #analytics. Know about politics, econ, #enterpriseIT and surveys
"
https://medium.com/@cloud66/cloud-native-transformation-containers-in-the-enterprise-with-white-paper-ef421077932c?source=search_post---------216,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Nov 21, 2017·3 min read
A few weeks ago, I attended a talk about moving to microservices, in which the speakers, both of them DevOps program/delivery managers at a large online retailer, mentioned a rule they had. Tooling, they said, was not the answer to a Devops culture problem, but tools can be effective in slowing down or accelerating that very human effort.
This probably resonates well with anyone trying to drive technological change within an organization, and even more so with regards to cloud-native transformation in the enterprise, by which I mean the move to a microservices-based architecture and container-based infrastructure. It’s the right tools, that support the right human expertise, that get the job done.
Take delivering code to production as an example. In the old world (i.e., before containers…), CI tools took code from a repo, delivered an artefact, and then initiated some testing (essentially, using a framework-specific set of commands to execute a user-defined test). Then a set of scripts would configure servers, databases and load balancers would rev up, and off you went. “Simple”, said nobody, ever; that is, at least until containers came along!
Now though, your app is broken down into parts, and those parts move quickly, and by the way, your code still lives in your repo but also in a Docker repo, so they all need complex version control mechanisms, traceability/audit, and orchestration in order to be herded into production.
In delivery, many tools still deliver the same functionality as before, adapted for containers, but this isn’t enough anymore if you want to get from point C (images built) to point Z (production running). What about binding artefacts to the application as services? What about reducing runtime for efficiency but also IP protection? What about automated stop/start and redeployment hooks? What about observability for this complex system?
In deployment, there is a rising class of CaaS tools (see our post here for how that term can be mis-used), but they mostly focus on getting Kubernetes clusters up and running quicker than through a command line. But infrastructure is only as good as its support of the app layer: what about dynamic scaling? What about non-container workloads like databases? What about firewall management? What about control over what gets deployed where?
Cloud 66 has been running containers in production since 2012, so we know what the production issues are. We’ve written a white paper to encapsulate what we think you can consider along this journey to production. If you want to learn more about container-native CI/CD and pipelines, click below.
Download the white paper: ‘Continuous Delivery Pipelines for Container-Based Applications’
Lastly, if you’re an SI or MSP, come talk to us in person about your views and experiences at AWS Re:invent in Las Vegas, NV, and Kubecon in Austin, TX. Contact us to book a meeting.
Originally published at blog.cloud66.com on November 21, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
29 
29 
29 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://medium.com/@alibaba-cloud/alibaba-cloud-nas-the-one-container-solution-for-cloud-native-technology-35d3309968cf?source=search_post---------217,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 25, 2021·9 min read
By Meng Wei
Today, more applications are becoming cloud native, and storage solutions are following their lead. Containers are the infrastructure of the cloud-native era, but what is the infrastructure of the container technology?
“Cloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments, such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructures, and declarative APIs exemplify this approach. These techniques enable us to build loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, cloud-native technology allows engineers to make high-impact changes frequently, predictably, and effortlessly.” This is the definition of cloud native provided by the Cloud Native Computing Foundation (CNCF).
Kubernetes, a platform that orchestrates network, storage, and computing, has become the operating system for cloud-native technology. Featuring a novel interface, it simplifies operation and maintenance, improves the elasticity of resources, allows for use on demand, and lowers the costs for users. Cloud native has been embraced by enterprises and developers. Forrester predicts that the percentage of global organizations and companies that run containerized applications in production environments will increase significantly from less than 30% today to more than 75% by 2022. The trend of containerized applications in the business world is unstoppable.
The proportion of container applications in the production environment has been rising rapidly, from 23% in 2016 when the survey was first conducted, to 73% in 2018, and 84% in 2020.
Cloud-native applications are leading the transition to the cloud-native architecture in various application fields and have profoundly changed every aspect of application services. Essential to run any application, storage solutions are also faced with new requirements posed by cloud-native services. To suit the characteristics of cloud native, cloud-native storage has been substantially optimized in terms of its core capabilities, including availability, stability, scalability, and performance.
Alibaba Cloud, the cloud service provider in China, offers a wide variety of cloud-native services. Unlike network and computing, storage must be seamlessly connected to Kubernetes for orchestration and scheduling. To this end, Container Storage Interface (CSI) was released as the universal protocol to seamlessly integrate storage with Kubernetes. This article discusses the challenges cloud-native containers pose to storage. To keep pace with cloud-native technology and containers, Alibaba Cloud has been adapting and evolving its file storage solution Alibaba Cloud NAS. Now, it can effectively cope with the challenges of cloud-native storage and has become the natural choice for container storage.
To address the various performance, elasticity, high availability, security, and lifecycle challenges arising from containerization and the cloud migration of new workloads, we must not only improve storage services, but also improve cloud-native controls and data in a way that can promote the technological evolution of cloud-native storage. Now, let’s walk through these challenges.
Cloud-native applications are widely used in scenarios related to big data analysis and AI, which have demanding requirements for storage throughput and IOPS. In a scenario where the container clusters process massive data volume, launch thousands of pods at the same time, or add a large number of pods to read and write to the shared file system, the heavy workloads will increase latency, introduce high-latency glitches, and undermine read and write stability. In addition, the characteristics of cloud-native applications, such as rapid resizing and elastic scaling, will also test the ability of the storage service to cope with traffic peaks over a short span of time.
The elasticity of cloud-native technology poses new challenges to storage solutions. As a result of the diverse development of cloud-native services, databases and metadata management applications can be scaled out online, but local storage cannot be scaled out elastically.
In application and system O&M scenarios, a storage solution needs to meet stability and high availability requirements as it migrates along with containers.
In scenarios that require isolation among containers, a storage solution must cope with security challenges, such as multi-application sharing, capacity coordination of file systems in multi-tenant environments, permission control for shared access to cluster-level file systems, and end-to-end data encryption for user applications.
The storage of persistent data in massive container clusters needs to address the challenges arising from cold and hot data separation and storage costs.
In the preceding emerging computing scenarios, the challenges posed to storage in terms of performance, elasticity, high availability, security, and lifecycles must be addressed with improvements not only to storage services, but also to cloud-native applications, storage cloud services, and the adaption of underlying storage and the core storage layer. Then, an application-oriented cloud-native storage can be created with higher stability, greater security, and higher efficiency.
As cloud-native technology continues to develop, public cloud service providers compete to transform or adapt their cloud services to the cloud-native architecture and improve their service agility and efficiency to meet the needs of cloud-native applications. Alibaba Cloud Network Attached Storage (NAS) is also extensively optimized for adaptation to cloud-native applications. The solution supports the CSI protocol and the Flexvolume driver for seamless integration of data interfaces between cloud-native applications and storage services. Users can use the storage resources for service development, without having to worry about underlying storage services.
To meet the requirements of cloud native for elasticity, Alibaba Cloud NAS offers a fully elastic and shared file system that enables use on demand. Alibaba Cloud NAS optimizes and evolves its cloud-native storage to improve performance, elasticity, high availability, security, and lifecycle management.
To meet performance requirements in scenarios involving AI, big data analysis, and high-performance computing, Alibaba Cloud NAS can distribute I/O workloads among multiple file systems, storage clusters, and zones through container orchestration. The solution supports local read-only caching and distributed storage, which can reduce network latency, I/O latency, and GPU wait time. It can boost the computing power with rigid delivery of throughput in the dozens of GBs.
In terms of elasticity, Alibaba Cloud NAS, a fully managed file system, supports auto scaling and the pay-as-you-go billing method. Extreme NAS features a latency of hundreds of microseconds. To cope with cases where elasticity is urgently needed in industries such as finance and Internet, the solution can launch thousands of containers in a few minutes and rapidly load and unload data.
To meet high availability requirements, Alibaba Cloud NAS supports fast failover for containers and provides enterprise-level features such as storage snapshots and backup.
To ensure security, Alibaba Cloud NAS supports comprehensive AD/ACL permission management and quota management and provides a unified namespace with I/O isolation and management among large quantities of pods. It also supports features such as transmission encryption and disk encryption.
To address the challenges arising from massive data volumes, Alibaba Cloud NAS is capable of managing data lifecycles and automatically archiving cold data, which can reduce the costs to users by 90%.
Alibaba Cloud NAS provides storage services for unstructured data. As the result of rapidly evolving cloud-native technology, many companies choose containerized applications that use NAS to store data. Some NAS solutions are even used to store petabytes of data. Following its cloud-native strategy, Alibaba Cloud offers Container Service for Kubernetes (ACK) and Elastic Container Instance (ECI) with container instances that use the NAS file system for persistent storage. Shared file storage is indispensable for container storage.
Alibaba Cloud NAS is a fully managed cloud-native file system that is highly available and optimized for cost savings. Alibaba Cloud NAS offers three services, General-purpose NAS, Extreme NAS, and Cloud Paralleled File System (CPFS).
Alibaba Cloud NAS, as a fully managed service, is easy to configure, supports auto scaling when adding or deleting data, and provides the flexibility and convenience of container infrastructure, which makes it the natural choice for container storage.
Containers that share data with each other usually share file storage. Containers that must run for a long period of time can also use the shared file storage to cope with faults. Alibaba Cloud NAS can meet the requirements for auto scaling, flexible mounting, and high performance of persistent storage in container scenarios. In addition, the configuration files or initial loading data storage for container images can be shared in NAS and read in real time during batch container loading. Multiple pods share persistent data by using NAS and can switch over in the case of pod failure.
As new technologies continue to develop, applications including machine learning, AI, and gene processing make extensive use of shared file storage. Here are a few examples:
Rapidly adopted by corporate users as a container technology and a cloud-native computing solution, Kubernetes has gradually become an essential infrastructure in the era of cloud native, as have container services. New workloads drive the evolution of cloud-native storage and cloud storage. The cloud-native control plane ensures high efficiency, improves data storage stability, and reduces data security risks. To form a storage ecosystem in the cloud-native environment, it is imperative to consolidate the performance of cloud storage solutions, including fundamental capabilities such as capacity, elasticity, and density.
As the natural choice for container storage, Alibaba Cloud NAS can effectively cope with the challenges posed to cloud-native storage in terms of performance, elasticity, high availability, security, and lifecycles. The rapidly evolving cloud-native file storage technology from Alibaba Cloud will continue to empower the fast growth of cloud-native technology and container technology.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
47 
47 claps
47 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/cloud-native-transformation-containers-in-the-enterprise-21b24c453015?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Nov 21, 2017·3 min read
A few weeks ago, I attended a talk about moving to microservices, in which the speakers, both of them DevOps program/delivery managers at a large online retailer, mentioned a rule they had. Tooling, they said, was not the answer to a Devops culture problem, but tools can be effective in slowing down or accelerating that very human effort.
This probably resonates well with anyone trying to drive technological change within an organization, and even more so with regards to cloud-native transformation in the enterprise, by which I mean the move to a microservices-based architecture and container-based infrastructure. It’s the right tools, that support the right human expertise, that get the job done.
Take delivering code to production as an example. In the old world (i.e., before containers…), CI tools took code from a repo, delivered an artefact, and then initiated some testing (essentially, using a framework-specific set of commands to execute a user-defined test). Then a set of scripts would configure servers, databases and load balancers would rev up, and off you went. “Simple”, said nobody, ever; that is, at least until containers came along!
Now though, your app is broken down into parts, and those parts move quickly, and by the way, your code still lives in your repo but also in a Docker repo, so they all need complex version control mechanisms, traceability/audit, and orchestration in order to be herded into production.
In delivery, many tools still deliver the same functionality as before, adapted for containers, but this isn’t enough anymore if you want to get from point C (images built) to point Z (production running). What about binding artefacts to the application as services? What about reducing runtime for efficiency but also IP protection? What about automated stop/start and redeployment hooks? What about observability for this complex system?
In deployment, there is a rising class of CaaS tools (see our post here for how that term can be mis-used), but they mostly focus on getting Kubernetes clusters up and running quicker than through a command line. But infrastructure is only as good as its support of the app layer: what about dynamic scaling? What about non-container workloads like databases? What about firewall management? What about control over what gets deployed where?
Cloud 66 has been running containers in production since 2012, so we know what the production issues are. We’ve written a white paper to encapsulate what we think you can consider along this journey to production. If you want to learn more about container-native CI/CD and pipelines, click below.
Lastly, if you’re an SI or MSP, come talk to us in person about your views and experiences at AWS Re:invent in Las Vegas, NV, and Kubecon in Austin, TX. Contact us to book a meeting.
Originally published at blog.cloud66.com on November 21, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
17 
17 
17 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://medium.com/@alibaba-cloud/revealed-the-four-key-development-trends-for-cloud-native-a7162b3b7865?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 21, 2021·4 min read
To learn more about developing the right cloud-native architecture for your business, download The Cloud-Native Architecture White Paper today.
Cloud computing is an established solution, helping many businesses achieve the performance required to compete in today’s increasingly digitized market. It’s latest evolution, cloud-native, is helping businesses achieve ultimate elasticity. But cloud-native is developing at a fast pace and there are several key, emerging trends every developer must understand to create the right environment for your organization now, and in the years ahead.
Many businesses are choosing a hybrid cloud environment and distributed frameworks. These technologies use data center and cloud-based resources in a fine-grained and dynamic manner, allowing applications to make full use of these fragmented systems.
Within these distributed environments, edge computing is gathering momentum, providing businesses with low network latency and high bandwidth to support an increasingly broad range of internet-enabled devices, further driving borderless cloud computing.
If you don’t introduce the right solutions, such disparate solutions can increase your system complexity. This is where containers can help your software run reliably when they are moved from one computing environment to another, helping you implement your cloud environment to a distributed environment with ease. Kubernetes is the operating system for the cloud era and our Container Service for Kubernetes (ACK) is a high-performance, scalable, and fully-managed service to help you manage your container infrastructure.
Serverless computing is developing at a rapid pace and introducing a broad range of development tools to streamline application delivery pipelines, achieve improved observability, and integrate a broader range of services within today’s cloud-native architectures. As a result, serverless technology is supporting the ultimate elasticity, zero maintenance, enhanced security and efficient development that can be achieved with a cloud-native architecture.
But you can’t rely on serverless technologies in isolation. Serverless and container technologies are increasingly being integrated, where “serverless containers” can reduce the complexity of Kubernetes and free users from O&M tasks, such as capacity planning, security maintenance, and fault diagnosis. In addition, serverless containers can offload your security, availability, and scalability capabitilies to the underlying infrastructure layer.
Using event-driven connections to the cloud, serverless capabilities are now extending across the whole cloud ecosystem. Whether from an enterprise’s applications, partner-led services or hybrid environment, all events are increasingly processed in a serverless way. As a result, cloud services are now more closely connected to your serverless technologies, helping businesses build applications with scalability and high availability.
In the years ahead, serverless computing will also continue to improve computing density to ensure high performance with minimum resource usage and costs. End-to-end optimization will become important, based on the characteristics of the serverless workload to create a new computing environment for many businesses. Heterogenous hardware is another important growth area, helping businesses to ensure high performance with minimum resource usage costs.
These next-generation computing units will be secure, lightweight, efficient and optimized based on a range of computing scenarios. These changes will be implemented across areas including lifecycle management, O&M management, configuration extension and management, and language-independent frameworks to form a new programming interface between your applications and the cloud.
In particular, secure containers will be based on MicroVM, portable and lightweight containers will be based on WebAssembly, and OS virtualization innovations will continue. New container runtime technologies will also help businesses achieve security isolation, efficient executions and universality, with solutions including KataContainer, Firecracker, gVisor, and Unikernel taking center stage.
As non-business-related responsibilities are increasingly transferred to the cloud, new standards will be rolled out. This will allow application developers to take a consistent approach to their development efforts and optimize their O&M, regardless of whether a private, public, or hybrid cloud is being used.
This will lead to the creation of cloud-native operating systems, which have defined standards in openness, encapsulate resources, and support applications. This will allow for the efficient scheduling and orchestration of heterogenous computing power to process workloads of different types and a large number of computing tasks. As a result, developers can standardize and automate the delivery and management of applications in a portable, secure and controllable manner.
Alibaba Cloud can help you set-up, deploy and manage a cloud-native architecture to innovate and optimize your business. To find out more download The Cloud-Native Architecture White Paper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/github-actions-ack-a-powerful-combination-for-cloud-native-devops-implementation-1e28bb2a7ffe?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 15, 2021·9 min read
By Luo Jing (Yaojing)
According to the China Academy of Information and Communications Technology (CAICT), “The Investigation Report on the DevOps status quo in China (2020)”, 63% of the enterprises have put DevOps into practice and adopted a continuous delivery pipeline to enable links, such as development, testing, deployment, and O&M. However, 20% of the enterprises reported the DevOps practice was too complex to implement. Self-built Jenkins requires self-deployment and plug-in O&M, and the CI/CD tool of the SaaS model was complicated to configure. Therefore, more convenient and lightweight tools are necessary to accelerate implementation.
Alibaba Cloud and GitHub released GitHub Actions Workflow, a service that can be deployed quickly to Alibaba Cloud Container Service for Kubernetes (ACK). There is no need for self-deployment and CI/CD tool maintenance. Based on the out-of-the-box GitHub Actions and Alibaba Cloud deployment template, the process of packaging and uploading applications to the Alibaba Cloud Container Registry (ACR) and deploying applications to Alibaba Cloud Container Service for Kubernetes (ACK) can be implemented automatically and quickly after the GitHub code is changed. This article provides detailed descriptions from GitHub Actions and Alibaba Cloud Container Service and demos.
GitHub Action is a built-in continuous integration tool launched by GitHub in October 2018 to simplify automated build, test, and deployment processes. GitHub Actions encapsulates continuously integrated atomic operations into Actions and then assembles multiple Actions into reusable templates based on the Workflow process definition to trigger the Actions execution automatically after GitHub events are updated.
GitHub Actions has the following features:
The core concept of GitHub Actions is divided into the following four parts:
The following is a simple GitHub Actions Workflow example. A Job consisting of two Steps is defined below. The first Step reuses the community template actions/checkout@v2 to check out the current code repository. The second Step executes the Bash command directly.
The GitHub Actions Workflow released jointly by GitHub and Alibaba Cloud defines multiple Steps, supporting quick building and deployment to Alibaba Cloud Container Service for Kubernetes (ACK). For the detailed definition of a Workflow, please see this link.
If GitHub Actions is a powerful tool for DevOps practice, then Alibaba Cloud Container Service is the best interface for cloud-native DevOps implementation with various features and convenience. Alibaba Cloud Container Service for Kubernetes (ACK) is one of the first service platforms to pass the Kubernetes conformance certification worldwide. It provides a high-performance container application management service that supports lifecycle management for enterprise-level containerized applications. As the core containerization infrastructure in Alibaba Group, ACK has diverse application scenarios and experiences in e-commerce, real-time audio and video, database, message-oriented middleware (MOM), and AI. It supports a wide range of internal and external Double 11 activities. The container service is integrated with Alibaba’s experience and capabilities in various large-scale scenarios and is open to public cloud customers. It provides more functions and improved stability and has won the top market share in the Chinese container market for many years.
In terms of application management, with large-scale Alibaba practices and various enterprise production practices, Alibaba Cloud has further enhanced the reliability and safety and provided Kubernetes clusters with reimbursable SLA — ACK Pro. As an extension of the original ACK-hosted Kubernetes clusters, ACK Pro has all the benefits of the original hosted Kubernetes clusters. For example, Master node hosting and Master node high availability. In addition, ACK Pro improves the reliability, security, and schedulability with reimbursable SLA supported. Therefore, ACK Pro is suitable for enterprise users with large-scale businesses in production environments that require higher stability and security.
In terms of application product management, Alibaba Cloud launched Container Registry Enterprise Edition (ACR EE) for enterprises with high security and performance requirements. ACR EE provides the first enterprise-level service with a separate instance in a public cloud. In addition to multi-architecture container images, ACR EE supports the hosting of multi-version Helm Charts, Operators, and other OCI-compliant products.
In terms of security governance, ACR EE provides security protection in multiple dimensions, such as network access control, security scanning, image signing, and security audit, helping enterprises upgrade from DevOps to DevSecOps. In the global distribution acceleration scenario, ACR EE optimizes the network links and scheduling policies to ensure a stable rate of successful cross-sea synchronization. In the large-scale big image distribution scenario, ACR EE supports on-demand loading. Image data can be downloaded without fully downloading the content and decompressed online, reducing the average container startup time by 60%. ACR EE has been put into production environments by many enterprises to ensure the secure hosting of cloud-native application products and the efficient distribution of multiple scenarios.
The following GitHub Actions demonstrates how to package a simple NGINX application into a container image, host it to ACR, and then deploy it automatically to ACK. It helps implement the CI/CD process quickly and easily. For the GitHub Actions Demo, please see this link. Users can update the corresponding YAML file to customize business scenarios.
1) Create a Workflow
In the GitHub repository, click the Tab under Actions, and there will be Workflows recommended based on the current GitHub project. Select the Workflow template to be deployed on ACK.
GitHub Actions creates an alibabacloud.yml file in the github/workflows directory of the code repository by default. Listening code is defined to publish the Release event in the YAML file. Once the event occurs, the subsequent integration deployment process is automatically triggered. Users can also search for related Actions on the right-side of the marketplace to customize the Actions steps of the Job.
2) Update Variable Information in Workflow
Env environment variables are defined in Workflow. The corresponding information, such as the region, container image service, and container service cluster, should be updated as the practical conditions. ACCESS_KEY_ID and ACCESS_KEY_SECRET define the access key information of an Alibaba Cloud account, which must be set in ciphertext in the corresponding Secrets of the repository.
3) Automatic Deployment
After the configuration, when a Release event is published, the GitHub Actions is automatically triggered to execute the task by default. Click the Actions button to view the history and details of the corresponding tasks. The entire workflow is executed sequentially. Once one of the tasks fails, the entire workflow will be terminated. If high-risk security loopholes are detected after container images are pushed to ACR, the subsequent deployment of containers to ACK can be canceled immediately. After the construction, security scanning, and deployment are successfully completed, an NGINX service will be generated on the ACK server based on the new container image. The security risk identification and decision-making features are built during the whole procedure to implement the secure and efficient DevSecOps process.
4) Extension
Users can look for the required Actions task template on GitHub Actions Marketplace to customize the Workflow process. GitHub Actions has a variety of Actions templates, covering code dependency, code integration, code quality, and other scenarios in multiple languages. A Workflow matrix that supports a multi-operation system under multi-language frameworks can be built quickly based on the templates to test multiple versions of the project in parallel.
Compared to traditional Jenkins, GitHub Actions is a SaaS-based hosting service that does not require deployment or plug-in O&M. Convenient CI/CD scenarios can be implemented by defining or reusing the official Workflow. In contrast to Travis CI and Circle CI, GitHub Actions is a native tool by GitHub. With better integration experience and flexibility and more ecological support for the Actions Marketplace, it allows users to reuse and customize Workflows more conveniently.
GitHub Actions also provides built-in Workflows that support automatic building and pushing to ACR and automatically deploy ACK. It guides the way for DevOps implementation in the cloud-native era. Alibaba Cloud hopes to help more enterprises complete their digital transformation and architecture upgrades with cloud-native technology.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
2 
2 claps
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/automation-nation-3-steps-to-cloud-native-deployments-70d33c87a427?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Mar 2, 2016·4 min read
02 March 2016
Transitioning from a public PaaS involves more than just setting up a server. It involves two key functions: provisioning the infrastructure and implementing the deployment process. It may seem easy, especially with the abundance of DevOps tools that exist today. But what we fail to realize is building a complete deployment solution requires a long process of scripting and server automation to reach the customized environment your product needs. In order to better understand the steps involved, let’s look at the components required for automating the deployment process:
Before you can automate the deployment process, your infrastructure needs to be provisioned. Provisioning involves the full lifetime of the infrastructure, from initial setup to scaling up and down the resources required to support the current application load. It includes the following components:
Server provisioning and management: Provisioning servers to meet the application under current load is essential. This generally involves creating and monitoring auto scale groups that are connected to load balancers (below). This also includes operating system configuration and update management when security patches are released.
Container management: For those choosing to deploy their applications via containers, container instances must be managed and services deployed must be registered. This adds a layer of complexity on top of server management, especially when some servers offer specific capabilities that others don’t (e.g. servers offering GPU processing) resulting in specialized container deployment requirements.
SQL and NoSQL database support: Your infrastructure must support launching new database instances for your applications and services, including vendors such as MySQL, PostgreSQL, MongoDB, Redis and ElasticSearch. These services may involve just a single instance, or may require high availability and failover that’s managed internally or by the infrastructure provider.
Traffic load balancing: Web and inter-container traffic must be balanced across infrastructure through load balancers native to your cloud provider, or by using a solution such as HAProxy. They must be automatically configured and managed, as servers and containers are created and destroyed.
Networking and firewall configuration: For increased security, product infrastructure must provision and manage network security with fine grained access control (ACL) at the service level to prevent unauthorized access.
Network storage services: Managed storage (NAS) and block storage for shared filesystems offer distributed file storage to servers and containers and will need to be provisioned at the same time.
Once you have automated your infrastructure provisioning and management, the deployment process can then be customized through automation:
Deploy image creation: This includes building container images with your code, tarballs, .yum, and .deb packaging.
Multi-cloud support: To avoid vendor lock-in, multiple cloud vendor support will be required. You’ll need to be able to choose from two or more providers, including Amazon Web Services, Google, Azure, Rackspace, Digital Ocean, and Linode to name a few.
Continuous release support: To support a public PaaS-style deployment, it’s common to build scripts that initiate deployment processes based on a git commit hook. You’ll also need to determine if you want to support blue-green deployments, rolling deployments, or opt for scheduled downtime to deploy updates. Be sure to provide full visibility on every step, to help with troubleshooting failed deployments.
Multi-datacenter networking with service registry: From internal applications to container-based service discovery, your application will need a private DNS service with all registered resources. This often requires automation from the continuous release process to keep things in sync and to properly configure applications during deployment.
Backup and replication: Ensure your databases are backed up on-site and off-site or across cloud vendors. Automated restoration is necessary to prevent mistakes during crisis situations, such as a cloud provider outage.
Failover groups: What happens when a cloud vendor or zone goes down? You’ll need to automate failover to a new zone, region, or cloud provider. This will depend on infrastructure provisioning to rebuild the necessary resources, and a proper restoration process for data recovery.
Network, server, OS, and container monitoring: Implement resource monitoring to notify you about any security issues or outages. This will keep your team informed and ensure that problems are addressed before your customers have a chance to report the issue to you.
Centralized logging for troubleshooting: Adding centralized logging is necessary to view behavior across servers and containers that may be created and destroyed at any time. By centralizing the logging across all resources, troubleshooting becomes easier.
As you’ve probably realized, implementing a deployment process with continuous release support requires coordinating a large number of processes. Each step in the process requires automation scripting and oversight. According to a recent report, downtime can cost a company $100,000 or more during an outage. DevOps can help to avoid this loss through automated failover.
Automating the deployment process to support continuous releases can be very beneficial, with DevOps a key part of it. It’s important to note however the cost of implementing DevOps, particularly for smaller product teams who are just getting started. Depending on the needs of the company, it could result in an investment of $100–400k+ in expenses to fully automate the process as described above.
As a result, many teams are considering the move away from in-house DevOps to a DevOps-as-a-Service model. This is an approach, which allows companies to outsource much of the infrastructure and deployment automation, freeing them to focus on what really matters: developing the product offering.
Originally published at blog.cloud66.com on March 2, 2016.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
2 
2 
2 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://news.greylock.com/cloud-native-monitoring-at-scale-9a8bd96fcbea?source=search_post---------222,"This is an email from Greymatter from Greylock , a newsletter by Greylock Perspectives.
New episodes every week. Soundcloud | iTunes | Spotify | Google | Stitcher
Cloud native is the new order of business. To contend with the complex, dynamic and ephemeral nature of the modern cloud environment, enterprise organizations need monitoring technology at a level unmet by tools designed for the cloud transition era. And they need it at a scale larger than ever before. That’s why Martin Mao and Rob Skillington formed Chronosphere, a purpose-built platform to monitor cloud-native infrastructure and apps and enable organizations to make precise, data-driven decisions. In this episode of Greymatter, Greylock general partner and Chronosphere board member Jerry Chen sat down with Martin and Rob to discuss the company’s growth, the current state of cloud technology, and the challenges and advantages of starting a company just before the global crisis of 2020. (Greymatter)
Chronosphere, the scalable cloud native monitoring tool launched in 2019 by two former Uber engineers, announced a $43.4 million Series B today. The company also announced that their service was generally available starting today. Greylock, Lux Capital and venture capitalist Lee Fixel, all of whom participated in the startup’s $11 million Series A in 2019, led the round with participation from new investor General Atlantic. The company has raised $54.4 million. (TechCrunch)
Roblox raised $520 million at a $29.5 billion valuation in a financing round led by Altimeter Capital and Dragoneer Investment Group. The online gaming company also said it plans to go public via direct listing. (CNBC)
Check out our jobs page for all the available roles. Our core talent team is always looking to connect with engineers, product managers, and designers who are thinking about exploring early stage start-up as a next move in their career. Reach out: CoreTalent@greylock.com
Early partners of remarkable teams building companies that matter.
2 
2 claps
2 
Written by
Early partners of remarkable teams building companies that matter. www.greylock.com
Early partners of remarkable teams building companies that matter.
Written by
Early partners of remarkable teams building companies that matter. www.greylock.com
Early partners of remarkable teams building companies that matter.
"
https://medium.com/humio/humio-at-lunar-way-log-management-for-a-kubernetes-cloud-native-environment-bf06a5097ee9?source=search_post---------223,"There are currently no responses for this story.
Be the first to respond.
While the technology world moves pretty fast, there are still institutions lagging behind in the move to the modern. Lunar Way looks to handle one of these institutions, the banking industry, and bring things forward into the 21st Century.
Based in Denmark, Lunar Way is innovating the banking industry by bringing digital only transactions to the banking industry. The idea is to build out APIs that interact with brick and mortar banking organizations, making it easier for consumers to find and use more products and more focused features than might be available if they were forced to bank 9 am to 5 pm weekdays, as is common at most banks.
With a line of many different banking products, including credit cards and the Travel Card that make it easier on travelers, Lunar Way focuses on innovation. This means a fairly tight development and deployment schedule, which can bring about as many as ten to twenty deployments a day across their various mobile and web product lines. The need for analytics and log management is apparent every day.
The NoSQL of Log ManagementWhen Lunar Way began their journey, they started with a simple Elasticsearch and Kibana setup in AWS. Initially, the logs were small, so this was easy. As time passed and the products became more diverse and more complex, issues began to surface with usability, compatibility, and the day-to-day usefulness. The Elastic Query Language was a problem and Kibana seemed to always come with difficulties. It was obvious a change had to happen.
Around this time, some of the team members saw a presentation on Humio at GOTO Copenhagen. It seemed interesting, but the move to Humio wasn’t immediate. It was only as the applications grew, and the infrastructure supporting needed to grow that Elastic stack was no longer an option and it was time for Humio.
Security being a key part of what any bank does, Lunar Way is no different. They needed an On-Premises solution that would work with their private AWS cluster; something they wouldn’t need to send their logs to Humio, they needed everything to remain within their network. Luckily, Humio works well as an on-prem solution, which stands out from Elastic Cloud and other log management systems that require logs be sent out or exported to external services.
Another of the big differentiators for Lunar Way was that Humio did not require a schema for logs to start rolling in. As Bjørn Sørensen, Web Architect at Lunar Way, states, “Humio is the NoSQL of log management. We miss less in our logs because it’s not excluded by a schema”. It’s possible to use filters to see results directly without having to work through a detailed schema to get the desired results.
Using Humio to manage their logs to examine both application layer data and infrastructure level information, the folks at Lunar Way became aware of the need for a solution that gave the complete view, across all microservices — which is their chosen architecture. Humio gave them the collation of events both up and downstream, providing them with insights to prevent issues before they might occur.
Kubernetes and Humio Become Part of the Lunar Way
The interesting factor at Lunar Way is that they are big Kubernetes users. Using Kubernetes to manage their 35–40 different Microservices makes them unusual in Denmark and frankly unique in the Fintech industry. While creating a new way to look at Fintech, Lunar Way DevOps Engineer Kasper Nissen knew proper, modern DevOps ideals and infrastructure were needed to launch banking into the modern age. And Humio was at the heart of that launch.
Using things beyond their expected capabilities is part of what Lunar Way focuses on. Beyond taking kubernetes to the bank, Lunar Way also uses Humio in unexpected ways. A good example is their mobile team. When testing new application functionality on iOS or Android, the team takes advantage of their Humio installation to get instant feedback on what’s going right (or wrong) with a new piece of code being prepared to face the world. This allows for rapid turn around and a shorter development cycle, which in turn means more features and fixes in less time.
At this point, all teams at Lunar Way are using Humio as part of the daily development routine. With an extension into the dashboards used around the offices, everyone can keep an eye on logs and the graphs they produce to ensure all needs are being met and everything is running smoothly.
Humio helps Lunar Way to bring Fintech to the forefront of the tech world in Denmark, and soon around the world. To learn more about Humio and how you can gain the most of your logs, check out Humio.com or get started with a Free Trial.
Humio is a Log Management Platform that can scale to…
1 
1 clap
1 
Written by
Developer, writer, speaker, musician, and Community Advocate, PJ is the founder of DevRelate.io. He is known to travel the world speaking about programming.
Humio is a Log Management Platform that can scale to trillions of events. On our blog, we write about the technology we use, customer success stories, and new features and updates.
Written by
Developer, writer, speaker, musician, and Community Advocate, PJ is the founder of DevRelate.io. He is known to travel the world speaking about programming.
Humio is a Log Management Platform that can scale to trillions of events. On our blog, we write about the technology we use, customer success stories, and new features and updates.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@efipm/goldman-sachs-dares-to-launch-a-cloud-native-transaction-banking-service-7b73857889b5?source=search_post---------225,"Sign in
There are currently no responses for this story.
Be the first to respond.
Efi Pylarinou
Nov 12, 2020·3 min read
Goldman has been the one to watch for having the courage to reinvent its business and culture in all adversities. From the subprime crisis days, that Goldman chose to become a commercial bank and later returning the $10Billion bailout money; to offering free access to its analytical tools SecDB to all its clients; [1] to its infamous `You Can Marcus` ad for its…
"
https://medium.com/@ContinoHQ/what-is-cloud-native-architecture-and-why-is-it-so-important-e4b4a1f435ec?source=search_post---------226,"Sign in
There are currently no responses for this story.
Be the first to respond.
Contino
Jan 16, 2019·5 min read
So you’ve got a few EC2 instances powering your app, and that makes you cloud-based. Not so fast. Being cloud-native is more than just moving some of your workload to a public cloud vendor. It involves a completely different approach to infrastructure, application development, and team structure. Let’s dig deeper to understand what cloud-native architecture actually means and why it’s so important.
Cloud-native is a way of approaching the development and deployment of applications in such a way that takes account of the characteristics and nature of the cloud — resulting in processes and workflows that fully take advantage of the platform.
What does fully taking advantage of the cloud look like?
Firstly, a word on the cloud, as compared to traditional on-premises. On-premises infrastructure is a centralized system. Everything is (to all intents and purposes) in one place. In the cloud, however, servers and databases are distributed, i.e. not in one place.
In this context, if you simply port over your app from server hardware to the cloud, you won’t get the most out of your migration. This is because apps that were hosted on hardware servers were built as monoliths. The codebase included every feature and service that made up the app as a single, giant lump of code. Today, with microservices architecture, apps are being built as a distributed collection of services, which pairs up perfectly with the distributed nature of the cloud.
The advantages of this approach mean that, for individual microservices, you can now take advantage of automation in various ways to maximize efficiency and free time and money:
This is a much more granular means of deploying the minimum resources necessary to reliably maintain performance.
While it’s possible to just move your app with its legacy codebase to a cloud platform, that robs you of the benefits of being truly cloud-native.
Underlying the microservices architecture is the rise of Docker and the container ecosystem. Managing your app as distinct services has implications on infrastructure. Every service in a microservices app needs to be a self-contained unit. Services need their own allotment of resources for computing, memory, and networking. However, both from a cost and management standpoint, it’s not feasible to 10x or 100x the number of VMs to host each service of your app as you move to the cloud. This is where containers come in. They are extremely lightweight, and provide the right amount of isolation to make a great alternative to VMs for packaging microservices, enabling the benefits above.
This fundamental change in infrastructure calls for a change in toolset as well. Legacy tools that were built to manage tasks across a few hardware servers can’t hold up under the complexity of microservices in the cloud. Simple things like latency optimization, root cause analysis of the backend and end-to-end monitoring can become complex in a distributed microservices app. The resource consumption of each service needs to be metered to ensure compromised services don’t affect other services, and to keep costs under control.
The Cloud Native Computing Foundation hosts a number of open source projects that help run microservice apps. Here are a few of them:
This is a good sample list of the kind of tools cloud-native architecture requires.
Traditional apps require many teams for each stage of an app’s development. Dev, QA, IT, database admins, sysadmins, release management, and project management each have their own goals and priorities. Their agendas often clash, and it takes a toll on the application.
With microservices, team structures can be realigned to reflect the change in architecture. Modern software delivery teams are small and cross-functional. Teams are organized by the services they support. This keeps teams agile, improves collaboration and quickens decision-making. It also eliminates a host of accountability issues, whereby teams can relinquish responsibility for a feature after it is moves on from their team, but before it is released.
With the right infrastructure, the right tools to manage it, and the right team structure to bring it all together, the cumulative effect is that cloud-native releases become much more frequent — in fact, there aren’t any major releases. Every release affects a single service, or a couple of services at most. Because of this limited scope for every release, errors are easy to spot and fix. Rollback is easy with many tools capable of automatic rollbacks to the previous stable version when a new version fails.
Autonomous teams can ship updates to the services they own without being dependent on other teams. With multiple teams releasing updates to many services every day, the pace of deployment is blinding. The largest web companies deploy thousands of times a day.
This is the biggest advantage of a cloud-native architecture — that it takes you from idea to app in the quickest possible time.
To conclude, cloud-native architecture is more than just running your apps using cloud instances. It goes deeper to change the way you plan infrastructure. This is possible only after the rise of Docker. With the foundational infrastructure changing, you need a new toolset that is purpose-built for operating in a cloud-native world. Teams aren’t left untouched. They are now smaller, agile, multi-functional, and empowered to make decisions pertaining to the services they manage. All these efforts combine to create momentum that leads to much faster releases.
Cloud-native architecture is a lot of work to get started. However, it’s the obvious way forward if you want to grow and sustain your edge over the competition. Next time you hear someone bandying around the term cloud, stop to consider if they actually mean cloud-native.
After hearing the Gospel of Cloud, you’ve decided that you want to migrate to the magical wonderland that is the cloud.
How do you make sure that all the lofty promises come true?
A pattern that we often see emerge when enterprises begin to adopt the cloud is that they put the same processes and practices in place as they would have done in a traditional data centre scenario.
To secure the benefits of the cloud, you need to evolve your organisation and the processes by which you manage your infrastructure and applications. Check out our latest thought leadership that explores the key considerations of how to do just that.
Contino is a global DevOps and cloud transformation consultancy that enables highly-regulated organizations to accelerate innovation.
1 
1 
1 
Contino is a global DevOps and cloud transformation consultancy that enables highly-regulated organizations to accelerate innovation.
"
https://medium.com/@alibaba-cloud/breaking-the-limits-of-relational-databases-an-analysis-of-cloud-native-database-middleware-1-9c6bf5d9a5a7?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 12, 2019·14 min read
The development and transformation of database technology is on the rise. NewSQL has emerged to combine various technologies, and the core functions implemented by the combination of these technologies have promoted the development of the cloud-native database.
This article provides an in-depth insight into cloud-native database technology Among the three types of NewSQL, the new architecture and Database-as-a-Service types involve many underlying implementations related to the database, and thus will not be elaborated here. This article focuses on the core functions and implementation principles of transparent sharding middleware. The core functions of the other two NewSQL types are similar to those of sharding middleware but have different implementation principles.
Regarding performance and availability, traditional solutions that store data on a single data node in a centralized manner can no longer adapt to the massive data scenarios created by the Internet. Most relational database products use B+ tree indexes. When the data volume exceeds the threshold, the increase in the index depth leads to an increased disk I/O count, the substantially degrading query performance. In addition, highly concurrent access requests also turn the centralized database into the biggest bottleneck of the system.
Since traditional relational databases cannot meet the requirements of the Internet, increasing numbers of attempts have been made to store data in NoSQL databases that natively support data distribution. However, NoSQL is not compatible with SQL Server and its ecosystem is yet to be improved. Therefore, NoSQL cannot replace relational databases, and the position of the relational databases is secure.
Sharding refers to the distribution of the data stored in a single database to multiple databases or tables based on a certain dimension to improve the overall performance and availability. Effective sharding measures include database sharding and table sharding of relational databases. Both sharding methods can effectively prevent query bottlenecks caused by a huge data volume that exceeds the threshold.
In addition, database sharding can effectively distribute the access requests of a single database, while table sharding can convert distributed transactions into local transactions whenever possible. The multi-master-and-multi-slave sharding method can effectively prevent the occurrence of single-points-of-data and enhance the availability of the data architecture.
Vertical sharding is also known as vertical partitioning. Its key idea is the use of different databases for different purposes. Before sharding is performed, a database can consist of multiple data tables that correspond to different businesses. After sharding is performed, the tables are organized according to business and distributed to different databases, balancing the workload among different databases, as shown below:
Vertical sharding
Horizontal sharding is also known as horizontal partitioning. In contrast to vertical sharding, horizontal sharding does not organize data by business logic. Instead, it distributes data to multiple databases or tables according to a rule of a specific field, and each shard contains only part of the data.
For example, if the last digit of an ID mod 10 is 0, this ID is stored into database (table) 0; if the last digit of an ID mod 10 is 1, this ID is stored into database (table) 1, as shown below:
Horizontal sharding
Sharding is an effective solution to the performance problem of relational databases caused by massive data.
In this solution, data on a single node is split and stored into multiple databases or tables, that is, the data is sharded. Database sharding can effectively disperse the load on databases caused by highly concurrent access attempts. Although table sharding cannot mitigate the load of databases, you can still use database-native ACID transactions for the updates across table shards. Once cross-database updates are involved, the problem of distributed transactions becomes extremely complicated.
Database sharding and table sharding ensure that the data volume of each table is always below the threshold. Vertical sharding usually requires adjustments to the architecture and design, and for this reason fails to keep up with the rapidly changing business requirements on the Internet. Therefore, it cannot effectively remove the single-point bottleneck. Horizontal sharding theoretically removes the bottleneck in the data processing of a single host and supports flexible scaling, making it the standard sharding solution.
Database sharding and read/write separation are the two common measures for heavy access traffic. Although table sharding can resolve the performance problems caused by massive data, it cannot resolve the problem of slow responsiveness caused by excessive requests to the same database. For this reason, database sharding is often implemented in horizontal sharding to handle the huge data volume and heavy access traffic. Read/write separation is another way to distribute traffic. However, you must consider the latency between data reading and data writing when designing the architecture.
Although database sharding can resolve these problems, the distributed architecture introduces new problems. Because the data is widely dispersed after database sharding or table sharding, application development and O&M personnel have to face extremely heavy workloads when performing operations on the database. For example, they need to know the specific table shard and the home database for each kind of data.
NewSQL with a brand new architecture resolves this problem in a way that is different from that of the sharding middleware:
Cross-database transactions present a big challenge to distributed databases. With appropriate table sharding, you can reduce the amount of data stored in each table and use local transactions whenever possible. Proper use of different tables in the same database can effectively help to avoid the problem caused by distributed transactions. However, in scenarios where cross-database transactions are inevitable, some businesses still require the transactions to be consistent. On the other hand, Internet companies turned their back on XA-based distributed transactions due to their poor performance. Instead, most of these companies use soft transactions that ensure eventual consistency.
Database throughput is challenged by a huge bottleneck due to increasing system access traffic. For applications with a large number of concurrent reads and few writes, you can split a single database into primary and secondary databases. The primary database is used for the addition, deletion, and modification of transactions, while the secondary database is for queries. This effectively prevents the row locking problem caused by data updates and dramatically improves the query performance of the entire system.
If you configure one primary database and multiple secondary databases, query requests can be evenly distributed to multiple data copies, further enhancing the system’s processing capability.
If you configure multiple primary databases and multiple secondary databases, both the throughput and availability of the system can be improved. In this configuration, the system still can run normally when one of these databases is down or a disk is physically damaged.
Read/write separation is essentially a type of sharding. In horizontal sharding, data is dispersed to different data nodes. In read/write separation, however, read and write requests are respectively routed to the primary and secondary databases based on the results of SQL syntax analysis. Noticeably, data on different data nodes are consistent in read/write separation but are different in horizontal sharding. By using horizontal sharding in conjunction with read/write separation, you can further improve system performance, but system maintenance becomes complicated.
Although read/write separation can improve the throughput and availability of the system, it also results in data inconsistency, both between multiple primary databases and between the primary and secondary databases. Moreover, similar to sharding, read/write separation also increases database O&M complexity for the application development and O&M personnel.
As the key benefit of read/write separation, the impacts of read/write separation are transparent to users, allowing them to use the primary and secondary databases as common databases.
Sharding consists of the following processes: statement parsing, statement routing, statement modification, statement execution, and result aggregation. Database protocol adaptation is essential to ensure low-cost access by original applications.
In addition to SQL, NewSQL is compatible with the protocols for traditional relational databases, reducing access costs for users. Open source relational database products act as native relational databases by implementing the NewSQL protocol.
Due to the popularity of MySQL and PostgreSQL, many NewSQL databases implement the transport protocols for MySQL and PostgreSQL, allowing MySQL and PostgreSQL users to access NewSQL products without modifying their business codes.
MySQL Protocol
Currently, MySQL is the most popular open source database product. To learn about its protocol, you can start with the basic data types, protocol packet structures, connection phase, and command phase of MySQL.
Basic data types
A MySQL packet consists of the following basic data types defined by MySQL:
Basic MySQL data types
When binary data needs to be converted to the data that can be understood by MySQL, the MySQL packet is read based on the number of digits pre-defined by the data type and converted to the corresponding number or string. In turn, MySQL writes each field to the packet according to the length specified in the protocol.
Structure of a MySQL Packet
The MySQL protocol consists of one or more MySQL packets. Regardless of the type, a MySQL packet consists of the payload length, sequence ID, and payload.
Connection Phase
In the connection phase, a communication channel is established between the MySQL client and server. Then, three tasks are completed in this phase: exchanging the capabilities of the MySQL client and server (Capability Negotiation), setting up an SSL communication channel, and authenticating the client against the server. The following figure shows the connection setup flow from the MySQL server perspective:
Flowchart of the MySQL connection phase
The figure excludes the interaction between the MySQL server and client. In fact, MySQL connection is initiated by the client. When the MySQL server receives a connection request from the client, it exchanges the capabilities of the server and client, generates the initial handshake packet in different formats based on the negotiation result, and writes the packet to the client. The packet contains the connection ID, server’s capabilities, and ciphertext generated for authorization.
After receiving the handshake packet from the server, the MySQL client sends a handshake packet response. This packet contains the user name and encrypted password for accessing the database.
After receiving the handshake response, the MySQL server verifies the authentication information and returns the verification result to the client.
Command Phase
The command phase comes after the successful connection phase. In this phase, commands are executed. MySQL has a total of 32 command packets, whose specific types are listed below:
MySQL command packets
MySQL command packets are classified into four types: text protocol, binary protocol, stored procedure, and replication protocol.
The first bit of the payload is used to identify the command type. The functions of packets are indicated by their names. The following describes some important MySQL command packets:
COM_QUERY
COM_QUERY is an important command that MySQL uses for queries in plain text format. It corresponds to java.sql.Statement in JDBC. COM_QUERY itself is simple and consists of an ID and SQL statement:
1 [03] COM_QUERY
string[EOF] is the query the server will execute
The COM_QUERY response packet is complex, as shown below:
MySQL COM_QUERY flowchart
Depending on the scenario, four types of COM_QUERY responses may be returned. These are query result, update result, file execution result, and error.
If an error, such as network disconnection or incorrect SQL syntax, occurs during execution, the MySQL protocol sets the first bit of the packet to 0xff and encapsulates the error message into the ErrPacket and returns it.
Given that it is rare that files are used to execute COM_QUERY, this case is not elaborated here.
For an update request, the MySQL protocol sets the first bit of the packet to 0x00 and returns an OkPacket. The OkPacket must contain the number of row records affected by this update operation and the last inserted ID.
Query requests are most complex. For such requests, an independent FIELD_COUNT packet must be created based on the number of result set fields that the client obtains by reading int. Then, independent COLUMN_DEFINITION packets are sequentially generated based on the details of each column of the returned field. The metadata information of the query field ends with an EofPacket. Later, Text Protocol Resultset Rows of the packet will be generated row by row and be converted to the string format regardless of the data type. Finally, the packet still ends with an EofPacket.
The java.sql.PreparedStatement operation in JDBC consists of the following five MySQL binary protocol packets: COM_STMT_PREPARE, COM_STMT_EXECUTE, COM_STMT_ CLOSE, COM_STMT_RESET, and COM_ STMT_SEND_LONG_DATA. Among these packets, COM_STMT_PREPARE and COM_STMT_ EXECUTE are most important. They correspond to connection.prepareStatement() and connection.execute()&connection.executeQuery()&connection.executeUpdate() in JDBC, respectively.
COM_STMT_PREPARE
COM_STMT_PREPARE is similar to COM_QUERY, both of which consist of the command ID and the specific SQL statement:
1 [16] COM_STMT_PREPARE
string[EOF] the query to prepare
The returned value of COM_STMT_PREPARE is not a query result but a response packet that consists of the statement_id, the number of columns, and the number of parameters. Statement_id is the unique identifier that MySQL assigns to an SQL statement after the pre-compilation is completed. Based on the statement_id, you can retrieve the corresponding SQL statement from MySQL.
For an SQL statement registered by the COM_STMT_PREPARE command, only the statement_id (rather than the SQL statement itself) needs to be sent to the COM_STMT_EXECUTE command, eliminating the unnecessary consumption of the network bandwidth.
Moreover, MySQL can pre-compile the SQL statements passed in by COM_STMT_PREPARE into the abstract syntax tree for reuse, improving SQL execution efficiency. If COM_QUERY is used to execute the SQL statements, you must re-compile each of these statements. For this reason, PreparedStatement is more efficient than Statement.
COM_STMT_EXECUTE
COM_STMT_EXECUTE consists of the statement-id and the parameters for the SQL. It uses a data structure named NULL-bitmap to identify the null values of these parameters.
The response packet of the COM_STMT_EXECUTE command is similar to that of the COM_QUERY command. For both response packets, the field metadata and query result set are returned and separated by the EofPacket.
Their differences lie in that Text Protocol Resultset Row is replaced with Binary Protocol Resultset Row in the COM_STMT_EXECUTE response packet. Based on the type of the returned data, the format of the returned data is converted to the corresponding MySQL basic data type, further reducing the required network transfer bandwidth.
Other Protocols
In addition to MySQL, PostgreSQL and SQL Server are also open source protocols and can be implemented in the same way. In contrast, another frequently used database protocol, Oracle, is not open-source and cannot be implemented in the same way.
Although SQL is relatively simple compared to other programming languages, it is still a complete programming language. Therefore, it essentially works in the same way as other languages in terms of parsing SQL grammar and parsing other languages (such as Java, C, and Go).
The parsing process is divided into lexical parsing and syntactic parsing. First, the lexical parser splits the SQL statement into words that cannot be further divided. Then, the syntactic parser converts the SQL statement to an abstract syntax tree. Finally, the abstract syntax tree is accessed to extract the parsing context.
The parsing context includes tables, Select items, Order By items, Group By items, aggregate functions, pagination information, and query conditions. For a NewSQL statement of the sharding middleware type, the placeholders that may be changed are also included.
By using the following SQL statement as an example: select username, ismale from userinfo where age > 20 and level > 5 and 1 = 1, the post-parsing abstract syntax tree is as follows:
Abstract syntax tree
Many third-party tools can be used to generate abstract syntax trees, among which ANTLR is a good choice. ANTLR generates Java code for the abstract syntax tree based on the rules defined by developers and provides a visitor interface. Compared with code generation, the manually developed abstract syntax tree is more efficient in execution but the workload is relatively high. In scenarios where performance requirements are demanding, you can consider customizing the abstract syntax tree.
The sharding strategy is to match databases and tables according to the parsing context and generate the routing path. SQL routing with sharding keys can be divided into single-shard routing (where the equal mark is used as the sharding operator), multi-shard routing (where IN is used as the sharding operator), and range routing (where BETWEEN is used as the sharding operator). SQL statements without sharding keys adopt broadcast routing.
Normally, sharding policies can be incorporated by the database or be configured by users. Sharding policies incorporated in the database are relatively simple and can generally be divided into mantissa modulo, hash, range, tag, time, and so on. More flexible, sharding policies set by users can be customized according to their needs.
NewSQL with the new architecture does not require SQL statement rewriting, which is only required for NewSQL statements of the sharding middleware type. SQL statement rewriting is used to rewrite SQL statements into ones that can be correctly executed in actual databases. This includes replacing the logical table name with the actual table name, rewriting the start and end values of the pagination information, adding the columns that are used for sorting, grouping, and auto-increment keys, and rewriting AVG as SUM or COUNT.
Results merging refers to merging multiple execution result sets into one result set and returning it to the application. Results merging is divided into stream merging and memory merging.
In Part 2 of this article, we will discuss in further detail about distributed transactions and database governance.
Reference:https://www.alibabacloud.com/blog/breaking-the-limits-of-relational-databases-an-analysis-of-cloud-native-database-middleware-1_594427?spm=a2c41.12548085.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://architecht.io/aws-does-cloud-native-docker-is-still-a-unicorn-and-ai-is-still-too-hot-df167694bcad?source=search_post---------228,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is a reprint (more or less) of Monday’s ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
I’m back from break and ready to rock! Fair warning, though: This issue is long, not entirely chronological, and very limited comments on the items in the body (mostly to clarify vague headlines). Rather than ignore the last 2 weeks worth of news, I opted to read through what I missed and include the most interesting stuff. That did, however, cut into my writing and organizing time ;-)
There are several things really stuck out to me though, which I’ll highlight here:
AWS joins the CNCF, and announces a slew of new products
In some ways, Amazon Web Services is competing against itself when it comes to maintaining cloud market share. And one of its biggest weaknesses going forward is an open source reputation that lags behind competitors such as Microsoft and Google — especially when it comes to the Kubernetes container-orchestration platform. So it was a big deal on Wednesday when AWS announced (via a blog post by VP of Cloud Architecture Strategy and former Netflix open source leader Adrian Cockroft) that it has joined the Cloud Native Computing Foundation as a platinum member.
Cockroft’s post offers up some context as to why AWS joined, and James Governor at Redmonk has a nice analysis of the whys of this decision, too — including the notion that AWS already runs the lion’s share of Kubernetes deployments, so supporting a technology that drives resource consumption is a smart business decision.
But I think what’s most interesting about the AWS-CNCF announcement is what it doesn’t say. Specifically, both Cockroft’s post and the CNCF press release don’t say anything about AWS building a Kubernetes-based service (which has been rumored recently), and don’t actually talk too much about Kubernetes at all. And while Cockroft suggests plans to release open source projects to the CNCF, AWS has not released anything yet.
Considering its dominance in nearly all other facets of cloud computing (including, one could argue, serverless computing) I suspect Microsoft and Google were very happy to see AWS playing catch-up on containers and being branded as the less-open option. So I also suspect they’re waiting with bated breath to see how AWS will execute on its burgeoning open source strategy. The last thing anybody competing with AWS wants is for it to supplement its reputation for efficiency and scale with openness, as well.
AWS also made a bunch of non-container announcements on Monday, further proof that it’s still the cloud provider to beat, but also that even it can’t afford to rest on its laurels:
Docker is reportedly raising $75 million at a $1.3 billion valuation
Speaking of containers and open source …
As I’ve written before, Docker is in a weird position at the moment. It’s responsible for the container industry as we know it and Docker containers are all but ubiquitous at this point, but higher-level projects like Kubernetes will ultimately deliver much of the value from containers, and open source can still be a tough way to make a buck (or, in Docker’s ideal case, billions of them).
That being said, Docker is still very involved, very important, and has a huge user base from which it can mine customers or grow deal sizes. So a 30 percent increase from its $1 billion valuation in 2015 seems fair.
Andrew Ng announces a new deep learning specialization on Coursera
People have been wondering since March, when Andrew Ng left his chief scientist role at Baidu, what he would do next. He announced the first of his three new ventures last Tuesday: “deeplearning.ai, a project dedicated to disseminating AI knowledge.” A major part of this is a new series of deep learning courses on Coursera (which Ng also co-founded), which received good reviews from at least one student who successfully completed all of them. (There’s also a good explanation in the review about how the Coursera program differs from the fast.ai program for learning deep learning.)
Ng also released a series of video interviews on YouTube, under the heading Heroes of Deep Learning. His interviewees are Geoff Hinton (Google / University of Toronto), Yoshua Bengio (Element AI / University of Montreal), Ian Goodfellow (Google), Andrej Karpathy (Tesla), Pieter Abbeel (University of California, Berkeley), Ruslan Salakhutdinov (Apple) and Yuanqing Lin (Baidu). I haven’t had time to watch them yet, but I definitely plan to because these are some of the biggest names in the space — Ng included — and I suspect, based on my time chatting with Ng, that he’s a very good interviewer.
OpenAI “masters” Dota 2, or did it: A case study in AI hype and Elon Musk’s AI tweets
I openly admit that I have never heard of the online game Dota 2 until I read about how AI research organization OpenAI built a system that beat some of the world’s best players in a competition held last week. Nonetheless, it’s still a notable accomplishment because Dota 2, like other online strategy games, represents new challenges for AI systems over board games like chess and Go, or even simple video games. Mostly, this is because games like Dota 2 are fast-moving and neither party has access to all the information about the other player’s situation or how they might react.
When we think about AI systems that will help us solve truly complex business problems, or interact naturally with humans in homes or factories, these are the types of AI techniques that could help us get there. (In other news, both DeepMind and Facebook released tools and data last week to help researchers trying to crack the StarCraft games.)
Or, as OpenAI explains:
Dota 1v1 is a complex game with hidden information. Agents must learn to plan, attack, trick, and deceive their opponents. The correlation between player skill and actions-per-minute is not strong, and in fact, our AI’s actions-per-minute are comparable to that of an average human player.
Success in Dota requires players to develop intuitions about their opponents and plan accordingly. … [O]ur bot has learned — entirely via self-play — to predict where other players will move, to improvise in response to unfamiliar situations, and how to influence the other player’s allied units to help it succeed.
However, at least one expert explained in a blog post that a 1-on-1 game of Dota 2 is far less complex than a 5-on-5 game (which is a popular format) and that OpenAI’s system might have had access to more information that did the human players. In an interview with The Verge, one of the OpenAI researchers acknowledged there’s some truth to the criticism, but also defended the work and said the organization plans to tackle a 5-on-5 game at next year’s tournament.
This is a prime example of what happens when the media gets carried away hyping an AI research accomplishment without fully grokking its context — something that happens fairly frequently (see, for example, this about some recent Facebook research) and that tends to annoy AI practitioners. However, in defense of my media brethren, it can be difficult to separate fact from fiction, especially when some vendors always have their feet on the marketing gas pedal.
Also, a lot of this coverage might have been spurred by a series of tweets from Elon Musk (who, by the way, helped create and fund OpenAI in an attempt to keep AI research as open and safe as possible). In the middle a handful of tweets praising the accomplishment, Musk tweeted, “If you’re not concerned about AI safety, you should be. Vastly more risk than North Korea.”
Musk’s comments about AI and national/mankind security tend to get a lot of attention. This was no exception.
appleinsider.com
www.wired.com
www.microsoft.com
Microsoft’s AI system that can generate code and fix bugs in it.
www.infoworld.com
www.wired.com
www.nytimes.com
www.wired.com
research.googleblog.com
www.wired.co.uk
hbr.org
spectrum.ieee.org
code.facebook.com
www.recode.net
Here I will note that I interviewed the Ozlo founders in March.
www.youtube.com
If you want more information on how Google is applying AI, this is a great start.
www.fast.ai
blog.ycombinator.com
techcrunch.com
This interview with Hilary Mason is great for so many reasons. There’s her astute analysis of the state of AI and the connection between data and the real world, but also her takedown of the interviewer over questions on women in tech.
fortune.com
www.geekwire.com
www.zdnet.com
azure.microsoft.com
uk.reuters.com
I don’t envy cloud providers trying to do business internationally. The choices essentially are play ball with local laws or go home. The former is getting more complex (and sometimes ethically questionable), while the latter is not great for business.
www.nytimes.com
techcrunch.com
cloudplatform.googleblog.com
www.geekwire.com
www.datacenterknowledge.com
www.crn.com
www.fiercewireless.com
Edge computing, people!
techcrunch.com
highscalability.com
blog.ycombinator.com
This is a really interesting interview with Google’s cloud computing leader.
arstechnica.com
blog.heptio.com
Heptio was founded by two of the Kubernetes creators, and these are projects to make Kubernetes easier to run in production.
medium.com
A look at how Netflix handles DDoS attacks.
insights.hpe.com
It’s probably a good idea to test how our current technology holds up to space travel, because we’ll probably be wanting to put a lot more of it on satellites, rockets, etc.
redmonk.com
www.dispatch.com
If you enjoy the newsletter, please help spread the word via Twitter, or however else you see fit.
If you’re interested in sponsoring the newsletter and/or the ARCHITECHT Show podcast, please drop me a line.
Use Feedly? Get ARCHITECHT via RSS here:
blog.cleargraph.io
Tableau acquired the company to integrate its conversational query capabilities, which Tableau probably could use.
investors.hortonworks.com
www.wired.com
venturebeat.com
I always liked these guys, who were doing some cool things with publishing data and were very open about the infrastructure they needed to do it.
www.theregister.co.uk
engineering.linkedin.com
How LinkedIn moves a service from Oracle to its homemade database.
svds.com
www.datanami.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
2 
2 claps
2 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/cloud-native-report-2021-elucidates-kubernetes-challenges-k8s-lens-5-0-177f05b18d45?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/@alibaba-cloud/seizing-the-opportunity-in-the-new-cloud-native-battlefield-ca5659b7e061?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 18, 2019·16 min read
By Feifei Li
As the Cloud Computing tide strikes, the traditional database market is facing a situation of reshuffling. The rise of a group of new forces, including cloud databases, has shaken the monopoly of traditional databases, and the Cloud Native database led by cloud vendors pushes this change to the maximum.
What changes will databases face in the cloud era? What are the unique advantages of Cloud Native databases? At the 2019 DTCC (Database Technology Conference China 2019), Dr. Feifei Li, the vice president of Alibaba, gave a wonderful presentation on the Next Generation of Cloud Native Database Technology and Trend.
Feifei Li (nickname: Fei Dao) is the vice president of Alibaba Group, a senior researcher, the chief database scientist at Alibaba DAMO Academy, the head of Database Products Division of Alibaba Cloud Intelligent Business Group, and a distinguished expert of ACM.
The following figure shows a Gartner report on the global database market share. The report shows that the current global database market share is about $40 billion, of which the database market share of China accounts for 3.7%, about $1.4 billion.
In terms of database market distribution, the five traditional database manufacturers, which are Oracle, Microsoft, IBM, SAP, and Teradata, account for 80%, and the cloud database accounts for nearly 10% of the share now and is growing rapidly every year. Therefore, Oracle and MongoDB are also vigorously deploying their competitive positions in the cloud database market.
According to the DB-Engines database market analysis, the database system is developing towards diversification, from the traditional TP relational database to the multi-source heterogeneous database form today. Currently, the database systems we are familiar with, such as commercial databases (like Oracle and SQL Server), and open-source databases (like MySQL and PostgreSQL), are still in the mainstream. However, some newer database systems, such as MongoDB and Redis, have opened up a new track. The traditional sales method for database license is gradually going downhill, while the popularity of open-source and cloud database license is continuously increasing.
As Jeff Bezos, the founder of AWS, said: “The real battle will be in databases”. The cloud was first built from IaaS. Therefore, from virtual machines, storage, and networks, to popular intelligent applications nowadays (such as voice recognition, computer vision, and robotics), all are based on IaaS, and the database is the most critical link to connect IaaS with intelligent application SaaS. From data generation, to data storage, and to data consumption, databases are crucial at every step.
Databases are composed of four major types: OLTP, OLAP, NoSQL, along with database services and management tools. The four types are also the four directions for cloud database vendors. For OLTP, this technology has been developed for 40 years, and now everyone is still doing one thing: “Add 10 RMB and subtract 10 RMB”, often referred to as transaction processing. As the amount of data becomes larger and conflicts occur between reading and writing, OLAP is derived from the demand for online real-time analysis of data. Due to the demand for scaling, strong consistency of data cannot be guaranteed. Therefore, NoSQL appears. Recently, a new term “NewSQL” emerged, because NoSQL is also insufficient. Therefore, the ACID guarantee of traditional OLTP is integrated with the Scale-out capability of NoSQL are integrated as the NewSQL.
Throughout the development history of databases over the past 40 years: In the earliest relational database period, SQL, OLTP and other technologies have been derived; With rapidly increasing data volumes, it is necessary to avoid read/write conflicts, so OLAP is implemented through ETL, Data Warehouse, Data Cube, and other technologies; Today, in the face of heterogeneous and multi-source data structures (from graphs to time series, space-time to vectors), databases, such as NoSQL, and NewSQL have appeared, and some new technologies have also emerged, such as Multi-Model and HTAP.
The most popular database system architecture is Shared Memory: shared processor core, shared memory, and shared local disks. This standalone architecture is a popular architecture, and the traditional database vendors basically adopt the same architecture.
However, with the large-scale development of internet enterprises, such as Google, Amazon, and Alibaba, it is found that the original standalone architecture has many limitations, and its scalability and throughput cannot meet the business development needs. Therefore, the Shared Disk/Storage architecture is derived. That is to say, the bottom layer of the database may be distributed storage. By using a fast network, such as RDMA, the upper-layer database kernel appears to be using local disks, but it is actually distributed storage. The architecture can have multiple independent computing nodes. Generally, Single-Write-Multiple-Read is used, but Multiple-Write-Multiple-Read can also be realized. This is the Shared Storage architecture, and is typically represented by the Alibaba Cloud PolarDB database.
Another architecture is Shared Nothing. Although Shared Storage has many advantages and solves many problems, RDMA also has many limitations. For example, its performance may be compromised when it crosses Switches or even across availability zones (AZs) and Regions. After the Distributed Shared Storage reaches a certain number of nodes, the performance will suffer a certain amount of loss, so the performance of accessing remote data and accessing local data cannot be guaranteed to be exactly the same. Therefore, when the Shared Storage architecture reaches the upper limit of scaling when it is scaled to more than a dozen nodes. What if the application needs to be further scaled? Then, we need to implement a distributed architecture. A typical example is Google Spanner, which uses atomic clock technology to achieve data consistency and transaction consistency across data centers. In Alibaba Cloud, the distributed version PolarDB-X implemented based on PolarDB also uses the Shared Nothing architecture.
Note that Shared Nothing and Shared Storage can be integrated. Shared Nothing can be used on the upper layer, while Shared Storage architecture is used for shards on the lower layer. The advantage of such a hybrid architecture is that it can reduce the pain points of too many shards, and reduce the probability of distributed commits for distributed transactions, because the cost of distributed commit is very high.
To sum up the three architecture designs, if the Shared Storage architecture is implemented with Multiple-Write-Multiple-Read, instead of Single-Write-Multiple-Read, SharedEverything is actually implemented. The hybrid architecture that integrates the Shared Nothing and Shared Storage architecture should be an important breakthrough point in the future development of database systems.
The above analyzes the mainstream database architecture in the cloud era from the perspective of architecture. Technically speaking, apart from architecture differences, some differences still exist in the Cloud Native era.
The first is the Multi-model, which is mainly divided into two types, namely Northbound Multi-model and Southbound Multi-model. Southbound Multi-model indicates that the storage structure is diverse, and the data structure can be structured or unstructured, and can be graphs, vectors, documents and so on. However, for users, only a SQL query interface or a SQL-Like interface is provided. Typical representatives in the industry are various data lake services. While the Northbound Multi-model indicates that only one kind of storage is available. Generally, structured, semi-structured and unstructured data are supported through the KV storage data format, but it is expected to provide different query interfaces, such as SPARQL, SQL, and GQL. In the industry, this is typically represented by the Microsoft Azure CosmosDB.
The Self-Driving of the database is also a very important development direction. Many technical points are available from the perspectives of both the database kernel and the control platform. In terms of Database Self-Driving, Alibaba believes that self-perception, self-decision-making, self-recovery, and self-optimization are required. Self-optimization is relatively simple, that is, using the machine learning method in the kernel for optimization. However, self-perception, self-decision-making, and self-recovery are more aimed at the control platform, such as how to ensure the inspection of instances, and how to automatically and quickly fix or automatically switch when problems arise.
The third core of the Cloud Native database is the integrated design of software and hardware. The database itself is a system, and a system needs to be able to use limited hardware resources safely and efficiently. Therefore, the design and development of the database system must be closely related to the hardware performance and development. We cannot stick to the old database design without making changes when faced with changing hardware. For example, after NVM is released, it may have some impact on the traditional database design. The changes brought about by new hardware also need to be considered in database system design.
The emergence of new hardware or architectures, such as RDMA, NVM, and GPU/FPGA, provide new ideas for database design.
High availability is one of the most basic requirements of Cloud Native. Cloud users do not want service interruption. The simplest solution for high availability is redundancy, which can be Table-level redundancy or Partition-level redundancy. No matter which one is used, it is basically three replicas, and even more often four replicas or five replicas are required. For example, financial-level high availability may require three centers in two locations, or four centers in two locations.
For highly available multiple replicas, how to ensure data consistency between replicas? A classic CAP theory exists in the database. The result of this theory is that only two can be selected among Consistency, Availability and Partition Tolerant. Now, we generally choose C + P. For A, it can reach 99.9999% or 99.99999% through the three-replica technology and distributed consistency protocol. In this way, the 100% CAP is basically achieved.
PolarDB, a Cloud Native database with ultimate elasticity and compatibility, is created for massive data and massive concurrency. The background of the database market and the basic elements of the Cloud Native database are introduced above. Next, I will share the specific implementation of the above technologies with two Database Systems: Alibaba Cloud PolarDB and AnalyticDB. PolarDB is a Cloud Native database of Alibaba Cloud, and has a deep accumulation of technical knowledge. We have published papers, which mainly introduce technological innovations in storage engines, at international academic conferences, such as VLDB 2018 and SIGMOD 2019.
PolarDB adopts the Shared Storage architecture with Single-Write-Multiple-Read. The Shared Storage architecture has several advantages. First, the computing and storage are separated, and computing and storage nodes can be can be elastically scaled. Secondly, PolarDB breaks through the limitations of MySQL, PG and other databases on single-node specifications and scalability, and can achieve 100 TB storage capacity and 1 million QPS per node. In addition, PolarDB can provide ultimate elasticity, and greatly improve backup and recovery capabilities. At the storage layer, each data block uses a three-replica high availability technology, and the Raft protocol is modified at the same time. By implementing a parallel Raft protocol, the data consistency among the three-replica data blocks is ensured, thus providing financial-level high availability. PolarDB is 100% compatible with database ecosystems, such as MySQL and PG, and can help users achieve imperceptible application migration.
Because the bottom layer is shared distributed storage, PolarDB belongs to the Active-Active architecture. The primary node is responsible for writing data, while the standby node is responsible for reading data. Therefore, for transactions entering the database, both the primary and standby nodes are Active. The advantage is that continuous data synchronization between the primary and standby nodes can be prevented with a physical storage.
Specifically, PolarDB has a PolarProxy, that is, the gateway proxy in front, below it are the PolarDB kernel and PolarFS, and the bottom is the PolarStore, which uses the RDMA network to manage the underlying distributed shared storage. PolarProxy will judge customer requirements, to distribute write requests to the primary node and distribute read requests based on the server load balancing and the read node status, so as to maximize resource utilization and improve performance as much as possible.
PolarDB uses distributed and three-replica Shared Storage. The primary node is responsible for writing, while other nodes are responsible for reading. The lower layer is PolarStore. Each part is backed up with three replicas, and data consistency is ensured through distributed consistency protocol. The advantage of this design is that the storage and computing can be separated, and lockless backup can be achieved, so backup can be achieved in seconds.
In the case of Single-Write-Multiple-Read, PolarDB supports fast scaling. For example, upgrading from a 2-core vCPU to a 32-core vCPU, or scaling from 2 nodes to 4 nodes can take effect within five minutes. Another benefit of separation of storage and computing is cost reduction, because storage and computing nodes can be elastically scaled independently, which fully reflects the cost advantage.
The following figure shows how PolarDB uses physical logs for continuous recovery. On the left side, it is the architecture of traditional databases, while in PolarDB, due to the use of Shared Storage is used, the process of recovery by using physical logs, which is similar to that of traditional databases, can be basically retained, and continuous recovery can be achieved through Shared Storage to perform Snapshot recovery of transactions.
Let’s make a comparison. If MySQL uses the primary-standby architecture, first a logical log and a physical log are needed in the primary database. While in the standby database, the logical log of the primary database should be replayed, and then the logical log and the physical log are performed in the way of the primary database. However, in PolarDB, Shared Storage is used, so data can be recovered directly using a single log. The standby database can directly recover the required data without the need to replay the logic logs of the primary database.
Another major advantage of PolarDB Single-Write-Multiple-Read cluster is support for dynamic DDL. In the MySQL architecture, if you want to modify the data schema, you need to use Binlog to replay the data to the standby database. Therefore, the standby database will have a Blocking stage, and it takes some time to replay the dynamic DDL. While, in the PolarDB Shared Storage architecture, all schema information and metadata are directly stored in the storage engine in the form of tables. As long as the primary database is changed, the metadata of the standby database is also updated in real time, therefore, no Blocking process exists.
PolarDB Proxy is mainly used for read/write splitting, load balancing, high-availability switching, and security protection. PolarDB uses the Single-Write-Multiple-Read architecture. When a request comes in, read/write judgment is required to distribute the write request to the write node, and distribute the read request to the Read node. In addition, load balancing is performed for read requests. In this way, session consistency can be ensured, and the problem of not being able to read the latest data can be completely solved.
Lossless elasticity is one of the modules monitored by PolarDB. For distributed storage, it is necessary to know the amount of Chunks (/Chunk) to distribute, and PolarDB monitors the amount of unused Chunks. For example, when less than 30% is available, it is automatically scaled up on the back-end, which doesn’t have an obvious effect on the application and enables it to write data continuously.
For the cloud database PolarDB, the biggest advantage of the above technologies is the ultimate elasticity. Here, let’s take a specific customer case to illustrate. As shown in the following figure, the red line refers to the consumption of offline resources. These costs must be paid by the customer in any case, while the part above is the demand for computing resources.
For example, customers may have new products to be launched in March and April, and promotions in May. In these two periods, the computing demand will be very high. For the traditional architecture, it may be necessary to scale the capacity to a larger scale before the new product is launched, and maintain such a level. In the subsequent promotion phase, the capacity needs to be scaled to a further higher specification, which is costly. However, if ultimate elasticity can be achieved (for example, the storage and computing in PolarDB are separated to realize rapid elastic scaling), you only need to scale up the capacity before the Blue Square appears, and then scale it down, thus greatly reducing the cost.
In addition to the Cloud Native database PolarDB, the Alibaba Cloud ApsaraDB team has significantly explored other directions.
The distributed version PolarDB-X, which is highly concurrent and highly available across zones, supports horizontal scaling. If enterprises require extreme Scaling capabilities, such as Alibaba, and banks and power companies in traditional industries that require extremely high concurrency and massive data support, the Shared Storage architecture only supports scaling to a dozen nodes, which is definitely not enough. Therefore, the Alibaba Cloud ApsaraDB team also uses Shared Nothing for horizontal scaling. The Shared Nothing and Shared Storage are combined to form the PolarDB-X. PolarDB-X supports strong data consistency across availability zones at the financial level, and has excellent performance in supporting high concurrent transaction processing under massive data. At present, PolarDB-X has been put into use in Alibaba. It has successfully withstood the challenge of peak flow in all core business database links of Alibaba under the scenario of Double 11 Shopping Spree by using technologies, such as storage and computing separation, hardware acceleration, distributed transaction processing and distributed query optimization. We will launch the commercial version in the future. Stay tuned.
In addition, in the direction of OLAP analytical database, the Alibaba Cloud ApsaraDB team independently developed the database product — AnalyticDB, which is available on both the public and private clouds of Alibaba Cloud. AnalyticDB has several core architecture features:
Recently, AnalyticDB has ranked in TPC-DS, ranking the first in the world in terms of cost performance, and passing the TPC official strict certification. Moreover, the paper about the AnalyticDB system will be presented at the VLDB 2019 conference. The common application scenario of AnalyticDB is to transfer data from OLTP applications and synchronize the data to AnalyticDB through tool DTS for real-time data analysis.
Self-driving database platform: iBTune (individualized Buffer Tuning)
One of the features of Cloud Native databases is Self-Driving. Alibaba Cloud has an internal platform called SDDP (Self-Driving Database Platform), which collects performance data of each database instance in real time, and uses the machine learning method to model for real-time allocation.
The basic idea of iBTune is that each database instance contains a Buffer Size, which is allocated in advance in traditional databases and cannot be changed. In large enterprises, Buffer is a resource pool that consumes memory. Therefore, it is hoped to flexibly and automatically allocate the Buffer Size for each instance. For example, if the database instance of a Taobao commodity library does not need a large Buffer at night, the Buffer Size can be automatically scaled down, then automatically scaled up in the morning, without affecting the RT. To meet the above requirements and implement automatic Buffer optimization, the Alibaba Cloud ApsaraDB team has built an iBTune system. Currently, it monitors nearly 7000 database instances, and can save an average of 20 TB of memory in the long run. The core technical paper introducing the iBTune project will also be presented at the VLDB 2019 conference.
Data security on the cloud is very important. The Alibaba Cloud ApsaraDB team has also done a lot of work in data security. First, the data is encrypted when it is stored in the disk. In addition, the Alibaba Cloud ApsaraDB supports BYOK. Users can bring their own keys to the cloud to implement disk encryption and transmission-level encryption. In the future, the Alibaba Cloud ApsaraDB will implement full-process encryption during memory processing, and trusted verification of logs.
Alibaba Cloud ApsaraDB provides services according to tool products, engine products, and full-process database products under operation control. The following figure shows the common links of Alibaba Cloud ApsaraDB. Offline databases are migrated online through DTS, and distributed to relational databases, graph databases, and AnalyticDB based on data requirements and classifications.
Currently, the PolarDB database is growing rapidly, and have served leading enterprises in many fields, such as general industries, Internet finance, games, education, new retail, and multimedia.
In addition, AnalyticDB also has an outstanding performance in the analytical database market, supporting real-time analysis and visualization applications
Based on the Alibaba Cloud ApsaraDB technology, Alibaba supports a series of key projects, such as City Brain, and a large number of customers on and off the cloud. So far, the Alibaba Cloud ApsaraDB has supported a total of nearly 0.4 million database instances successfully migrated to the cloud.
Cloud Native is a new battlefield for databases. It has brought many exciting new challenges and opportunities to the database industry that has been developing for more than 40 years. Alibaba hopes to push the database technology to a higher level with all technical colleagues in the database industry at home and abroad.
https://www.alibabacloud.com/blog/seizing-the-opportunity-in-the-new-cloud-native-battlefield_595028?spm=a2c41.13112282.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://lab.wallarm.com/securing-cloud-native-applications-5e28ffbda3a9?source=search_post---------231,"Last week we were able to sit down with Randy Bias — a cloud pioneer and a technology visionary who currently oversees Juniper Networks cloud strategy.
We have asked Randy to share his thoughts on the security of private and public clouds and specifically cloud-native applications. Here are a few key points from that conversation:
Wallarm> Welcome Randy. Could you introduce yourself?
Randy> My name is Randy Bias. I currently work for Juniper Networks and am an advisor to Wallarm. I found myself almost fortuitously in this spot to be kind of one of the main thought leaders for the cloud when cloud got really big in the early days. At Juniper, I focused predominantly around being an internal agitator in a kind of “disrupt before you get disrupted”-type model.
Wallarm> When you say cloud-native applications, what do you mean?
Randy> If you’re thinking about cloud native, these are the applications that were born in the cloud. And so, the cloud tells you a whole bunch of things like it’s not 100 percent available. It will fail. You need to plan for your applications to route around failures. You need to plan for your applications to be tied into their APIs and other services. Cloud-native is a mindset. it’s a way of thinking about your application so that you are taking an approach that is not centered around any kind of infrastructure. That’s designed for failure, that’s designed for scale-out instead of scale up. That has that sort of more next-generation-type look to it.
Wallarm> What are some key security considerations when selecting which cloud to use?
Randy>Lean-forward folks have considerations that are really more about very practical things. “How much does it cost?” “What kind of services and technologies can I deploy on top of it?” “Is it friendly to my DevOps practices? and so on It’s pretty clear public cloud is our default model going forward.
Wallarm> As far as the security considerations, what is there to look for?
Randy> So, I guess what I want to say about security and public clouds first is that people have been outsourcing security even in IT since the beginning. If you look at the financial services industry, for example, it’s a big long extensive supply chain of folks who provide some kind of financial transactions for other folks. It’s not like everything that Wells Fargo does stays within its four walls. They outsource a lot of the types of financial services they have to other businesses, and then they have to go through a process, usually using a SAS 70 audit, to validate the outsourcer who they’re basically outsourcing to. And if you look outside IT, if you look at other utilities that are not say, public cloud, not IT utilities, more like electrical utilities. All of those utilities are effectively outsourcing security for our power, gas, and so on, to somebody else. We know how to trust other businesses, we know how to validate them, so that’s pretty straightforward. We want to see transparency, we want to know what kinds of control objectives people have in place so we can validate them. So the more transparency there is, the more information we have about how they run their security, the better shape we’re in.
Wallarm> I’m glad you brought up AWS because they’re actually pretty clear about what they feel is their responsibility in security, and what should be a responsibility of the clients who run the applications on AWS. They call it shared responsibility model.
Randy> This shared responsibility model they have makes a lot of sense. They’ve basically drawn this line and said: “hey, we do everything below the waterline, you’re responsible for everything over it.” So I Think that’s a great model. And if we look at folks like Amazon and Google, and even Microsoft, their security teams are at a whole other level than the average enterprises’ security team.
Wallarm> So that’s obviously about infrastructure security and general security practices. Let’s look at the application security process. There are tools that we typically use for applications security when we’re in our own data centers. As we move to the likes of Google Cloud and AWS, do the tools change? Do we still need the Nessuses and scanners and WAFs of this world?
Randy> It’s not public vs private clouds per say, but more on cloud-native versus non-cloud-native application architecture, whether it’s behind your firewall or not. Cloud-native applications are designed to be multitenant and scale up and down based on the load. There are multiple environments, devs, staging, QA, and so on, usually on the same infrastructure because you’re taking CICD pipelines, your DevOps practices and using those in concert with your private or public cloud. I think that the classic security system procedures are starting to break down in the face of this because a lot of the tooling and the way we went about it in the past is much more sort of “moats and castle”-type approach, right? We sort of build big walls around what we’re trying to protect and we pretend that the environment never changes, and the reality is that with cloud-native or in teh environments where the environment is changing all the time, your security model has to adjust so it’s more dynamic, and it’s more application and data-centric. You’re seeing some folks like Wallarm start to address those issues but I still think we’re a ways away because when I talk to classic enterprise security teams, they’re just still very focused on “now I need to build this castle, and then they cannot break in”, whereas if we look at the Googles of this world and again what would Google do, you know they’re not really focused on that, so much as let’s make sure we’ve got defense in depth, we’ve got security everywhere, we’ve got transparency and visibility with what’s going on with the system and we’ve got ways to respond very, very quickly to any kind of problem.
Wallarm> What can you share with us about security in the APIs, especially if you’re talking about microservices and applications that are highly distributed?
Randy> I think this is a whole new ball game that we are going to learn about together. I think it’s still early days. As we decompose these applications into lots of microservices and these services are talking to each other more and more over HTTP predominantly. Each time somebody is building their own internal API between their microservices I think that it’s getting trickier and trickier to implement security around that sort of decomposed model. One of the things that’s funny about security is that, again, in the perfect world, as a security person, you’d like the environment to not change. You’d like to have some understanding of what’s going on. But then, what you’d also like to do, is you’d like to learn over time about the particulars of your environment. Your particular APIs, your particular codebases, you know sort of what’s the normal behavior profile, what does it look like. That’s very difficult and so if you also aren’t really in sync with your developers who may be deploying new microservices and new APIs very rapidly say, every week every month. Then you get further and further away from that ideal because not only are things dynamic in terms of the way the environment moves around, but they are also dynamic in that the code velocity and the changes in the code and the changes in the API interfaces which makes it very hard for any kind of security personnel to stay on top of that manually.
Wallarm> I hear you. Here at Wallarm we’ve been working on making security more dynamic and this is one of the main drivers for us. So, last question for you. What to choose, private cloud or public cloud, and which is more secure?
Randy> Yeah, I hate to paint anything with a broad brush. I would say probably generally the public cloud is more secure, but I think it varies from business to business. The sophistication level and the maturity level of different security teams vary across the board quite a bit.
The complete conversation is available as a podcast on Wallarm SoundCloud channel: https://goo.gl/kogi5X
We are always looking for new topics and guests for our podcast. Please, share your ideas and feedback in the comments.
I agree to Wallarm Privacy Policy.
Webinars
More insights
Subscribe for the latest news
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/@alibaba-cloud/integrating-distributed-architecture-with-cloud-native-best-practices-by-ant-financial-2d6c171c1891?source=search_post---------232,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 7, 2020·14 min read
By Yu Renjie, Product Expert of SOFAStack at Ant Financial
From February 19 to February 26, Ant Financial live streamed digital classes with the topic of “Fight Against the “Epidemic” with a Breakthrough in Technologies”. Ant Financial invited senior experts to share their practical experiences of cloud native, development efficiency, and databases and answer questions online. They analyzed the implementation of the Platform-as-a-Service (PaaS) architecture in financial scenarios and the elastic and dynamic architecture of Alipay on mobile devices. In addition, they shared the features and practices of OceanBase 2.2. So far, we have compiled and posted a series of these speeches under the WeChat official account “Ant Financial Technology” (WeChat ID: Ant-Techfin). You are welcome to follow the account and read them.
In this blog, we’ll be recapping the presentation by Yu Renjie, product expert of SOFAStack at Ant Financial, on the practices of building the cloud-native application PaaS architecture at Ant Financial’s digital classroom.
Hello, everyone. Welcome to Ant Financial’s digital livestreaming classroom. In February, we commercially released our Scalable Open Financial Architecture Stack (SOFAStack) on Alibaba Cloud. To allow more friends to know about the capabilities, positioning, and design ideas of this financial-grade distributed architecture, we will conduct a series of livestreamed classes to share them later. Today, we are going to share the topic of Practices of Building the Cloud-native Application PaaS Architecture. This topic focuses on the ideas of employing PaaS product capabilities in some financial scenarios that require steady innovations, to help you better connect PaaS products to the cloud-native architecture.
Through the development history of IT, cloud computing has been classified into Infrastructure as a Service (IaaS), PaaS, and Software as a Service (SaaS) for more than a decade. During the development of the entire cloud computing industry, it is obvious that enterprises have experienced three stages of cloud computing policies, that is, Cloud-based, Cloud-ready, and Cloud-native. The three stages changed due to the fact that businesses are becoming increasingly agile, requiring enterprises to shift their focus to upper layers by investing more energy and talent in the business logic and turning over the unfamiliar and increasingly complex infrastructure and middleware at lower layers to cloud computing vendors. In this way, professionals are able to focus on their professions.
This essentially further refines the social division of labor and complies with the law of the development of human society. In the era of cloud native, the container technology, service mesh technology, and serverless technology proposed in the industry are all intended to decouple business R&D from base technologies to make it easier to drive innovations both in businesses and base technologies.
Cloud native is an inevitable trend in the technology industry where businesses change rapidly. What substantively drives this trend is the container technology represented by the so-called cloud native, Kubernetes, and Docker. These essentially revolutionize application delivery modes. To truly implement the new application delivery mode advocated by the industry and communities in the actual enterprise environments, we need an application-centric platform that works throughout the lifecycle of application O&M.
In fact, there have been a lot of exchanges and materials for the keyword “cloud native” in the community and the industry. They focus on best practices of Docker and Kubernetes, continuous integration and continuous delivery (CI/CD) of DevOps, the design of container network storage, optimizations on the integration with log monitoring, and so on. Today, we want to demonstrate the product value of building a PaaS platform on top of Kubernetes. Kubernetes is an excellent orchestration and scheduling framework, and the key contributor to standardize application orchestration and resource scheduling. In addition, Kubernetes provides a highly scalable architecture to help the upper layer customize various controllers and schedulers. However, Kubernetes is not a PaaS instance. The bottom layer of PaaS can be implemented based on Kubernetes. However, to truly use Kubernetes in production environments, you must supplement many capabilities at the upper layer, especially in the financial industry.
What are the challenges for implementing cloud native in production environments?
We have previously conducted some research and customer interviews. As of 2020, the vast majority of financial institutions had expressed a great interest in technologies such as Kubernetes and containers. Many institutions had built an open-source or commercial cluster for some non-critical businesses or in development and testing environments. Their motivations are simple. Financial institutions hope that this new delivery mode can help businesses evolve rapidly. However, in distinct contrast, it is evident that few of them dare to implement cloud-native architectures in core production environments. This is because financial business innovations are based on the premise of guaranteed stability.
Our team has summarized the preceding six challenges faced by Ant Financial in the process of serving internal businesses and external financial institutions. In fact, these challenges are also faced by our internal site reliability engineer (SRE) team. In the insights we will share today and in the future, we will gradually summarize and deepen product ideas to address these challenges.
One of the core ideas that we are going to share today is how we control application change risks at the product level. Around this topic, we will introduce the background of the “three measures” for application changes, the native deployment capability of Kubernetes, the extensions made to our products based on change requirements, and our open sourcing plans.
The so-called “three measures” indicate the requirements that application changes can be carried out in canary release mode, be monitored, and meet emergency needs. This is a redline rule for internal SREs at Ant Financial. All changes must comply with this rule, and this rule is mandatory even in the cases of minor changes or rigorous tests. To meet this rule, we have designed a variety of fine-grained release policies at the PaaS product layer, such as group release, beta release, canary release, and blue-green release. These release policies are similar to those used in conventional O&M. However, many container users find it difficult to implement them in Kubernetes.
Sometimes, due to demanding business continuity requirements, users are reluctant to accept the standard mode of the native Kubernetes model. For example, the canary release of a native deployment cannot be completely lossless or controlled on demand because we still lack control over pod changes and traffic governance by default. In view of this, we made customizations at the PaaS product level and extended custom resources at the Kubernetes level. In this way, we can still exercise fine-grained control over the entire release process in cloud-native scenarios, making the deployment, canary release, and rollback of applications in a large-scale cluster more graceful and in line with the “three measures” against technical risks.
Actually, the native deployment object of Kubernetes and the corresponding ReplicaSet have gradually been stabilized in several recent major versions. To put it simply, in the most common release scenario in Kubernetes, we declare the expected release mode and the definition of pod specifications by using the deployment object. When pods are running, we use the ReplicaSet object to manage the expected number of pods, in which case rolling release or recreation release is enabled by default.
The lower part of the preceding figure shows the rolling release of an application based on the deployment object. Here, we will not elaborate too much on that. Essentially, in the process, we specify a step according to O&M requirements, create a pod, and delete the old pod. In this way, we can ensure that there is always an available container that provides external services in the entire application version change and release process. In most scenarios, the deployment object is sufficient and the whole process is easy to understand. In reality, deployments are the most common in the Kubernetes system in addition to pods and nodes.
After reviewing the deployment object, let’s take a look at CafeDeployment, a CustomResourceDefinition (CRD) extension developed according to actual requirements. Cloud Application Fabric Engine (CAFE) is the name of our SOFAStack PaaS product line. We will briefly describe CAFE at the end of this article.
CafeDeployment has an important capability of perceiving the underlying topology. But, what does this topology mean? This topology knows the specific node to which we publish a pod and does not simply bind the pod to the node based on affinity rules, but can truly import relevant scenario information such as high availability, disaster recovery, and deployment policies into the entire release-centric domain model. To this end, we proposed a domain model called a deployment unit. It is a logical concept and is simply called a cell in the YAML file. In actual use, a cell can be used in different zones, different physical data centers, or different racks, all of which are centered on different levels of high availability topologies.
Let’s see the typical release process of a CafeDeployment that has perceived the underlying topology. We will demonstrate the process through the product console and command line later. This figure shows a fine-grained group release process that enables changes at the container instance level to be sufficiently controllable and support canary release. Specifically, each stage can be suspended, verified, resumed, or rolled back.
As shown in the preceding example, we want to release or change 10 pods and evenly distribute them into two zones to ensure high availability at the application level. In addition, we need to introduce the concept of group release in the release process. Specifically, each data center must release one instance first. After the verification is suspended, the data center can continue to release the next group. In this case, the beta group has 1 instance on each side, group 1 has 2 instances on each side, and group 2 has the remaining 2 instances on each side. In the actual production environment, we monitor the businesses and more dimensions when making major changes to a container, to ensure that every step meets expectations and passes verification. In this way, the fine-grained control at the application instance layer plays the important role of a break for online application release, allowing SREs to roll back an application in time, when needed.
Now that we’ve described the entire fine-grained release process, let’s talk about the fine-grained traffic removal process. To ensure lossless release, we must gracefully remove the network traffic in both the north-south and east-west directions so that online services are not affected when a container is stopped, restarted, or scaled in.
The preceding figure shows the standard control procedure when a pod is released after being changed. The time sequence diagram includes the pod and its associated component controllers. The CafeDeployment mainly works with network-related components such as the service controller and LoadBalancer controller to redirect traffic and check traffic recovery for lossless release.
According to our command-based O&M practices in conventional O&M scenarios, we can run commands sequentially to perform atomic operations on each component to ensure that all inbound traffic and inter-application traffic are removed before we make the actual change. In contrast, in the cloud-native Kubernetes scenario, these complex operations are performed by the platform and SREs only need to run a simple statement. During deployment, we pass through the traffic components associated with the application, including but not limited to the Service LoadBalancer, RPC, and DNS, to the CafeDeployment, add the corresponding finalizers, and use ReadinessGate to identify whether the pod can carry traffic.
For example, when we want to update a specified pod in place under the control of the InPlaceSet controller, the InPlaceSet controller sets the ReadinessGate parameter to false. After perceiving the change, the associated components deregister their respective IP addresses sequentially to trigger actual traffic removal. After all related finalizers are removed, the system automatically updates the pod. After the new version of pod is deployed, the InPlaceSet controller sets the ReadinessGate parameter to true to sequentially trigger the loading of actual traffic to the associated components. The pod has been released only when the detected traffic types of the finalizers are consistent with that actually declared in the CafeDeployment.
Let’s go back to the PPT file. In fact, the CafeDeployment we just mentioned is a commercial product of CAFE. We have also developed some open-source projects in the community when commercializing the CAFE. Here, I would like to introduce the OpenKruise project, which we developed based on massive cloud-native O&M practices in the Alibaba economy. We open-sourced many Kubernetes-based automated O&M operations through standard Kubernetes extensions to supplement the capabilities that native workloads cannot provide. The project has resolved application automation problems in scenarios such as deployment, upgrade, elastic scaling, QoS adjustment, health check, and failover and recovery in Kubernetes.
Currently, the OpenKruise project provides a set of controller components. In particular, the UnitedDeployment can be considered as the open-source version of CafeDeployment. In addition to the basic replica retention and release capabilities, the UnitedDeployment provides the capability of releasing pods to multiple deployment units, which is one of the main features of CafeDeployment. Additionally, UnitedDeployment manages pods based on various workloads, currently including StatefulSet and OpenKruise AdvancedStatefulSet provided by the community. Therefore, the UnitedDeployment can inherit features of the corresponding workloads.
Wu Ke (Haotian, GitHub ID: wu8685), the key contributor to UnitedDeployment, comes from the SOFAStack CAFE team and has led the entire design and development process of CafeDeployment. Currently, we are working to incorporate more capabilities into the open-source version through standardized methods after carrying out massive verification on the capabilities. By doing this, we can gradually minimize the difference between the two versions.
Due to time constraints, that’s it for our detailed discussion of these technical implementations today. According to the previous description about the entire release policy of CafeDeployment, our key value proposition for product design is to provide a steady evolution capability while helping integrate emerging technology architectures into applications and businesses. Both conventional O&M systems represented by virtual machines (VMs) and cloud-native architectures for large-scale container deployment require fine-grained technical risk control and evolution towards the most advanced architectures.
The following example shows the containerization evolution route of a certain Internet bank. Since its foundation, the Internet bank has determined a distributed system where microservices are built on top of the cloud computing infrastructure. However, from the perspective of the delivery mode, the PaaS management model based on conventional VMs was initially adopted. From 2014 through 2017, developers had been deploying application packages to VMs through Buildpack. This O&M mode lasted for three years, during which we helped upgrade the architecture from the zone active-active mode to the mode with three data centers across two zones, and then to the unitized active geo-redundancy mode.
In 2018, as Kubernetes became more mature, we built a base at the underlying layer based on physical machines and Kubernetes. Meanwhile, we used containers to simulate VMs to containerize the whole infrastructure. However, service providers are unaware of this. We provided services for upper-layer applications through “rich containers” by using pods that are located on top of the underlying Kubernetes. From 2019 to 2020, as the businesses developed, the requirements for O&M efficiency, scalability, migratability, and refined management drove us to evolve the infrastructure to a more cloud-native O&M system and gradually implement capabilities such as service mesh, serverless, and unitized federated cluster management.
Through productization and commercialization, we are open-sourcing the capabilities that we have accumulated for years. We hope to enable more financial institutions to quickly replicate the capabilities of the cloud-native architecture in the Internet financial business scenarios and create value for the businesses.
You may know about the unitized architecture and the elasticity and disaster recovery capabilities of the active geo-redundancy mode at Ant Financial from many channels. Here, I’ll show you a figure, which is the abstract architecture of a solution that we are currently working on and are going to implement within months for a large bank. At the PaaS level, we build federated capabilities on top of Kubernetes, hoping that each data center has an independent Kubernetes cluster. This is because disaster recovery requirements cannot be met if we deploy a Kubernetes cluster across data centers and regions. Furthermore, the multi-cloud federated management capability also requires that we extend Kubernetes capabilities to PaaS-layer products, such as defining logical units and federation-layer resources. Ultimately, this builds a unitized architecture that covers multiple data centers, regions, and clusters. We have made a large number of extensions, including some federated objects at the federation layer in addition to the aforementioned CafeDeployment and ReleasePipeline. The ultimate goal is to provide unified release management, disaster recovery, and emergency management for businesses in these complex scenarios.
Now, I can finally explain the meaning of CAFE, which was mentioned much earlier. CAFE stands for Cloud Application Fabric Engine. It is the PaaS for cloud-native applications at Ant Financial’s SOFAStack. It not only provides the cloud-native capabilities standardized by Kubernetes, but also open-sources production-proven financial-grade O&M capabilities at the upper layer, including application management, release and deployment, O&M and orchestration, monitoring and analysis, disaster recovery, and emergency management. In addition, CAFE is highly integrated with the SOFAStack middleware, service mesh, and Alibaba Cloud container service for Kubernetes (ACK).
The differentiated application lifecycle management capabilites provided by CAFE include release management, disaster recovery, and emergency management, plus the evolving path to the unitized hybrid cloud capabilities. CAFE is the key base for the implementation of distributed architectures, cloud-native architectures, and hybrid cloud architectures in financial scenarios.
The last slide is actually the core theme today. The CAFE we described today is a part of the financial distributed architecture product SOFAStack. At present, SOFAStack has been commercially available on Alibaba Cloud. So, we welcome you to apply for a trial and discuss it further with us. For more information, search us online, follow the product link provided in this article, or go to the official website of Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-story-behind-how-alibaba-cloud-developed-cloud-native-to-be-used-large-scale-926547179c29?source=search_post---------233,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 29, 2019·8 min read
Watch Alibaba Cloud’s webinar on Bringing a Microservice App to Managed Kubernetes in 10 Minutes to learn more about setting up Kubernetes clusters on Alibaba Cloud with Container Service for Kubernetes.
Alibaba Cloud has come to implemented cloud native technologies on a large scale. At the KubeCon + CloudNativeCon + Open Source Summit held on June 26, 2019, Xiang Li, a CNCF TOC representative and the senior staff engineer at Alibaba Cloud, delivered a keynote speech. Mr. Li shared Alibaba’s experience in scalability, reliability, development efficiency, and migration strategy, and discussed how to implement cloud native technologies and address technical challenges.
Why go cloud native? What benefits can cloud native technologies bring to us? From finding its way through the dark, to embracing open-source standards and contributing back to the community, what kind of challenges did Alibaba face to implement cloud native technologies? Does Alibaba have technical experiences to share?
Since 2011, Alibaba had begun to put the cloud native technology system into practice by leveraging containers. Alibaba, trailblazing in this industry, over time developed a containerized infrastructure architecture that is now today top-of-the-line among the global leading technology companies. This architecture is now the technological backbone of the entire Alibaba Group. Alibaba believes exploration is intrinsic to developing and discovering new technologies. Through much determination and exploration, Alibaba’s technical team has revolutionized many of the ways that technology is used today, becoming a leader in developing cloud native technologies in China.
Alibaba’s businesses are large and complex. A suitable starting point must be found to develop cloud native. Motivated by the cost pressure of the 11.11 global shopping festival, Alibaba chooses resource cost and efficiency optimization as the starting point of their journey into cloud native.
Alibaba leverages containers to develop low-cost virtualization and scheduling technologies. It provides flexible and standard deployment units, and changes the resource scheduling mode from static to dynamic and on demand. This improves deployment efficiency, solves the problem of resource fragmentation, and increases the deployment density. Employing technologies such as storage network virtualization and separation of storage and computing, Alibaba not only enhances the portability of tasks and improves resource reliability, but also reduces operating costs.
Motivated to reduce the cost of resources, Alibaba has completed overall containerization and replaced resource allocation with a highly efficient scheduling platform. Alibaba’s cloud native exploration is still ongoing, however. Increasing the research and development (R&D) efficiency and accelerating the iteration are key to boosting Alibaba’s business. Alibaba hopes to leverage cloud native technologies to improve the efficiency of developers.
To improve automation and simplify application deployment, Alibaba adopted Kubernetes as its container orchestration platform. Since then, Alibaba has dedicated its efforts to improve the performance and scalability of Kubernetes. Kubernetes also enables Alibaba to refine its R&D and deployment processes. To build more cloud-native Continuous Integration and Continuous Delivery (CI/CD) and further implement standardization and automation, Alibaba has introduced standardized application management tools such as Helm to manage the entire process, from R&D all the way to product launch. Alibaba also tries new and innovative deployment patterns like GitOps, and pushes forward the final state-oriented automated construction of the PaaS layer. Additionally, Alibaba is now beginning to explore service mesh, aiming to further improve the universality and standardization of service governance, lower the adoption threshold for developers, and further popularize microservices in multiple languages and environments.
In 2019, Alibaba launched the All-in-Cloud initiative. Through cloud native exploration and reconstruction, Alibaba builds a modern and standard infrastructure system. The container technology decouples applications from hosts when they are running. Kubernetes abstracts resources into Pods and volumes to unify the implementation of various resources. Intelligent scheduling on the PaaS layer makes it possible to automatically migrate applications and fix any instabilities. By using cloud native, Alibaba greatly simplifies the migration to cloud.
In the process of improving resource and personnel efficiency, Alibaba’s entire infrastructure has become more open and connected to multiple open-source ecosystems. Alibaba also integrates and shares beneficial concepts, technologies, and ideas with open-source communities. Now, Alibaba Cloud operates China’s largest cloud native application — the 11.11 global shopping festival. It also boasts the largest public cloud cluster and image repository in China. As the only vendor in China that was listed in Gartner’s Competitive Landscape: Public Cloud Container Services Market, Alibaba Cloud has accumulated the most extensive and valuable customer practices.
Scaling and performance optimization effectively helps Alibaba cope with traffic peaks encountered in various complex scenarios.
After many years of determination, Alibaba has made great achievements in the scaling and performance aspects of Kubernetes. Compared with the original iteration, the number of objects that can be stored has been increased by 25 times. At the same time, the number of nodes that can be supported has been increased from 5000 to tens of thousands, and the end-to-end latency has been reduced from 5s to 100 ms. Much of the R&D work is carried out as a collaboration between Alibaba and the open-source community. Most of these R&D achievements have been attributed to the community, and Alibaba hopes that other companies and developers can enjoy these technological benefits that have been brought about through large-scale optimization.
Alibaba has worked hard to constantly optimize the performance of Kubernetes in terms of workload tracking, performance analysis, customized scheduling, and large-scale image distribution. Alibaba provides a complete tracking and replay mechanism for workload scheduling, and analyzes all performance problems in detail to overcome technical bottlenecks. Additionally, using highly customizable Kubernetes, Alibaba has developed a customizable scheduling and image distribution system — Dragonfly, based on its business scenarios. The open-source Dragonfly project was initially launched to address the needs of the 11.11 global shopping festival, and is equipped with excellent image distribution capabilities. During the 11.11 global shopping festival, up to dozens of super clusters are used. Each of these have tens of thousands of nodes and millions of containers.
Alibaba implements Kubernetes in three stages. First, Alibaba supplies resources through Kubernetes but does not interfere much with the O&M process. Doing so allows the system container to be rich and at the same times brings such capabilities as image standardization and lightweight virtualization to the upper-layer PaaS platform. Second, Alibaba uses the Kubernetes controller to transform the O&M process of the PaaS platform. This ensures the PaaS platform has stronger capabilities during the final-state oriented automation. Finally, Alibaba changes the traditional heavyweight mode of operating environments to the lightweight mode of native containers and Pods. Additionally, Alibaba completely hands over the PaaS capabilities to the Kubernetes controller. In this way, Alibaba builds a complete cloud native architecture.
In the process of implementing cloud native technologies, Alibaba has transformed from using self-developed containers and scheduling systems, to embracing open-source standardized technologies. Alibaba recommends that developers use Kubernetes to directly build cloud native architectures. There are two reasons. The first of which is that Kubernetes is developed for platform builders and has become the mainstay in the cloud native ecosystem. In this way, Kubernetes not only shields the underlying details in the downstream direction, but also supports various peripheral business ecosystems in the upstream direction. The second of which is that increasing numbers of open-source projects developed by the community are built around Kubernetes, such as Service Mesh and Kubeflow.
The toughest challenge in the evolution to cloud native-based technical architecture lies in the management of Kubernetes. Kubernetes is still a relatively young system, and does not have a mature ecosystem for O&M and management. Managing tens of thousands of clusters is crucial for Alibaba to succeed. Through great determination, Alibaba decided on the following:
Multi-tenant management of Kubernetes is another key technical issue for Alibaba Cloud. Considering the limits of Namespaces such as poor scalability and naming conflicts, you can use Kubernetes to set up virtual clusters. In addition to high scalability, Kubernetes can implement strong API-layer isolation. Syncer is used to link virtual clusters and real clusters, and agents are added to nodes to improve multi-tenant management and resource utilization.
At the KubeCon + CloudNativeCon + Open Source Summit, Alibaba Cloud announced two major projects: App Hub and OpenKruise. App Hub is a Kubernetes application management center open to all developers. OpenKruise is a set of open source Kubernetes automation projects developed based on Internet scenarios worldwide.
Cloud Native App Hub can be considered as a mirror for Helm Hub in China. It allows users to easily obtain application resources and significantly simplifies the Kubernetes installation procedure. OpenKruise is committed to becoming a cloud native application automation engine that can solve most O&M pain points in large-scale application scenarios. At the conference, developers downloaded OpenKruise from Alibaba Cloud Container Registry by using Helm, and obtained a hands-on experience with various features in a Kruise application scenario, such as the in-place upgrade of stateful containers, sidecar container injections, and one-time broadcasting to all nodes. They also developed an initial understanding of the powerful automation capabilities of the OpenKruise project in large scale scenarios.
The Kruise project is developed based on the best practices of Alibaba’s large-scale application deployment, release, and management. The project is also developed based on the large-scale application O&M and website construction capabilities of the Alibaba Cloud container platform team, as well as the requirements of Alibaba Cloud on leveraging Kubernetes to serve thousands of customers. This project solves a number of Kubernetes’s automation issues in a number of different ways, from deployment, upgrade, elastic scaling, QoS scheduling, and health check to troubleshooting of migration errors.
If you don’t have an Alibaba Cloud account, sign up for a New User Free Trial and get $300–1200 USD worth in free trial products.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/from-containers-to-cloud-native-service-mesh-f5fbd8ecd5c1?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 9, 2019·12 min read
By Yili
This article summarizes the presentation at the Yunqi Community Director’s Course and shares some practical tips and development trends of the Cloud Native technology.
The development of container technologies preludes the cloud-native computing:
First, it is the containerization of applications. Many PaaS platforms such as Heroku/CloudFoundry are built based on the container technologies, significantly simplifying the deployment and maintenance of Web applications. Docker puts forward the container packaging specification — Docker Image and creates Docker Hub for global application distribution and collaboration, greatly facilitating the popularization of container technologies and applying container applications in more business scenarios. Communities such as Open Container Initiative and Containerd further facilitate the standardization and normalization of container technologies.
Then it is the orchestration and scheduling of containers. Developers want to optimize/combine and efficiently schedule underlying resources by using container technologies to improve the system usage, application SLAs and the automation level. Kubernetes stands out from among other competing products due to its excellent openness, high scalability, and active community. Google donating Kubernetes to CNCF accelerates the popularization of Kubernetes.
With the establishment of container and container orchestration standards, Kubernetes/Docker hides the differences between underlying infrastructures and provides excellent portability to support multiple clouds/hybrid clouds. The community has begun to build upper-layer business abstraction based on these features. Let’s look at the service governance layer. The year 2018 was the beginning of Service Mesh. Istio is a service governance platform launched by Google/IBM/Lyft. Similar to a network protocol stack between microservices in the cloud-native era, Istio can be used to implement a variety of tasks without the need to process them in the application layer, such as dynamic service invocation routing, flow limits, downgrading, end-to-end tracing, and secure transmission. Based on this, cloud-native frameworks targeting specific fields also emerge quickly, such as the cloud-native Kubeflow platform for machine learning and Knative for serverless workloads. These layered architectures allow developers to focus only on their own business logic instead of the complexity of underlying implementations.
We can see that the prototype of a cloud-native operating system begins to take shape. This is a good era for developers because cloud infrastructure and cloud-native computing technologies have significantly improved the business innovation speed.
To solve the challenges in large-scale Internet applications, we build applications in a distributed manner. When building distributed system applications, we often make some assumptions, which have been proven to be inaccurate or incorrect during the long-time process of running applications. Peter Deutsch summarized the common “misconceptions about distributed systems”. The main misconceptions are listed as follows:
To solve the challenges in the large-scale Internet applications, we have been discussing how to transform application architectures from monolithic architectures to distributed microservices architectures. Spring Cloud based on the Netflix OSS project and the open-source Apache Dubbo project from Alibaba Group are two excellent microservices frameworks.
Current microservices architecture implementations are usually built within applications by using code libraries, which include features such as service discovery, fusion, and flow limits. However, using code libraries may cause potential version conflicts. In addition, once these code libraries are changed, the entire application needs to be rebuilt and changed, even if the application logic has no changes at all.
On the other hand, microservices in enterprises/organizations and complex systems are usually implemented by using different programming languages and frameworks, and service governance code libraries for heterogeneous environments have many differences and cannot enable unified solutions for shared problems.
To solve the preceding issues, the community begins to adopt and popularize the Service Mesh technology.
Istio and Linkerd are two representative implementations of the Service Mesh technology. They intercepts all the network communication between microservices by using the sidecar proxy and implement a variety of features between services in a unified manner, such as load balancing, access control, and rate limiting. Applications don’t need to perceive the access details about underlying services. Sidecar and applications can be updated separately, implementing the decoupling between application logic and service governance.
We can understand the service connection infrastructure layer between microservices in analogy to the TCP/IP protocol stack. The TCP/IP protocol stack provides basic communication services for all the applications in an operating system. However, the TCP/IP protocol stack and applications don’t have a close coupling relationship. Applications only need to use the underlying communication feature provided by the TCP/IP protocol without focusing on specific implementations of TCP/IP, such as how IPs are routed and how TCP creates links and performs congestion management. The goal of Service Mesh is to provide basic service communication protocol stacks that are irrelevant to implementations.
The preceding picture shows the architecture of Istio, which is logically divided into the data plane and the control plane.
In the data plane, smart proxies deployed as sidecar deployment intercept and process inter-service communication.
The control plane includes the following components:
The year 2018 was the very beginning of Service Mesh and the first Istio version (V1.0) available for production was released in early August this year. We have contributed to the Istio community the guide on how to use Istio on Alibaba Cloud to help you get started quickly.
In the Alibaba Cloud Kubernetes (ACK) console, one-click deployment and out-of-the-box features are available for ease of use. You can decide whether or not to enable features such as log collection and metrics displaying simply by checking/unchecking different options.
The support for Istio in ACK helps implement the deep integration of Istio with Alibaba Cloud Log Service and CloudMonitor, improving the availability of Istio in the production environment.
The Istio service mesh enables the management of access traffic among pods in Kubernetes clusters as well as the integration with ECI/Serverless Add-on, implementing the network communication between classic k8s nodes and Serverless K8s pods.
With the Mesh Expansion hybrid deployment feature, you can also uniformly manage traffic among application loads on existing ECS and containers. This is a very important feature that is useful for gradually migrating application on existing virtual machines to container platforms.
Istio versions later than v1.0 provide an important feature that allows you to create a service mesh across k8s clusters. This means that a unified service governance layer can be created across different clusters and that requirements in multiple scenarios such as hybrid cloud and disaster recovery can be met.
The end-to-end observability is a very important feature in Istio. Directly using the open-source Jaeger solution will lead to many challenges in the production environment. The bottom-layer storage of Jaeger depends on Cassandra clusters and requires complicated maintenance, tuning, and capacity planning. Alibaba Cloud Container Service provides a solution that supports the end-to-end analysis of Jaeger based on SLS. No maintenance is required in the data persistence layer.
We also provide the adapter for Alibaba Cloud CMS in Istio’s official Mixer Adapters. With some simple Adapter configuration, you can easily send runtime metrics data (for example, number of requests) to the monitoring backend for unified monitoring and alarming.
The modular architecture of Istio enables easy extension, but this architecture has some impact on the performance. We used the performance testing tool Fortio to conduct a performance test targeting Istio 1.0.2 in Alibaba Cloud Container Service.
We can see that when Mixer is not enabled, the inter-service invocation QPS of Istio is around 8500 and that the response time is within 10ms in 90% of the cases. When Mixer is enabled, QPS is reduced to 3500 and the response latency has no change.
The Istio community is continuously optimizing Istio performance. For example, the execution of some governance policies has migrated from Mixer to Envoy in the data plane. If your applications have high performance requirements, we recommend that you use the traffic management feature of Pilot first and temporarily disable Mixer. Istio 1.1 will be released at the end of 2018, and we will perform a thorough performance test on it.
A major challenge in secure production is how to release applications without influencing online business. However good the tests we perform are, we can’t always find all of the potential problems. We must release versions in a safe and controllable way, limit influences of failures within an acceptable range and support quick rollback.
The simplest method to release newer applications is republishing, that is, to stop the older versions first and start newer applications. However, this method will cause business interruption, which is usually unacceptable in the production environment.
The built-in Deployment feature in Kubernetes supports rolling update. This is a progressive update process, which gradually removes and stops older instances in the load balancing service backend, starts newer application instances and adds them to the load balancing service backend at the same time. This method can implement application releases without business interruptions. Because the whole release process is uncontrollable, application rollback is very costly if problems occur later due to insufficient tests.
Therefore, most Internet companies adopt the phased release method, which is more controllable.
Let’s first look at the blue/green release. For the blue/green release, newer applications and older applications co-exist and newer versions have the same resources. Only a portion of traffic is split into newer versions by routing. This release method provides good stability and only requires route switching for rollback. However, this method requires twice the system resources and therefore is not suitable for releasing large-scale applications.
Another release method is the canary release. We gradually release a newer version of an application and keep an older version at the same time. First we split a small portion of traffic to the newer version (for example, 10%) by routing and if the newer version functions correctly, we gradually release the newer version, take the older version offline and split more traffic into the newer version. If problems occur, we only need to roll back parts of the application.
We can see that application releasing and routing management are two orthogonal tasks. We can implement controllable releases with k8s. ACK supports both the blue/green release and the canary release. Istio provides a route control layer independent of release methods. We can flexibly switch application versions according to the request type, mobile devices/PCs, and regions. Combining Istio and ACK can implement safe and controllable releases.
A distributed system is very complex. In a distributed system, stability risks in any of the infrastructure, application logic, and maintenance aspects can lead to business system failures.
We cannot simply “trust in Buddha that there will be no downtime” when approaching stability challenges. Instead, we should use systematic methods to proactively prevent downtime.
Netflix put forward the Chaos Engineering concept and released a series of tools as early as 2012. Chaos Monkey randomly kills instances in production and testing environments to test system durability, train standby services and ensure simpler, faster, and more automatic recovery; Latency Monkey purposefully increases the latency of requesting or returning on a machine to test system response; Chaos Gorilla simulates an outage of a data center to test business disaster tolerance and resiliency. Chaos engineering lets you conduct experiments in distributed systems in production and testing environments to validate and improve system durability and implement continuous process optimizations.
Behind Alibaba Group’s success of the Double 11 event each year are two indispensable factors: the systematic stability experiments and the end-to-end stress tests on the whole e-commerce platform. In order to support Double 11 events, it is required to comprehensively and thoroughly simulate the whole platform and ensure that performance, capacity, and stability in each aspect meet the requirements. The following considerations are every important:
Fault injection is a common technique in chaos engineering. Istio supports two fault injection types. The first type is interruption fault injection that terminates requests and returns error code to downstream services to simulate errors in upstream services. Istio also allows specifying the proportion of interrupted requests. The second type is latency fault injection that induces latency before the forwarding action in order to simulate faults such as network faults and upstream service overloads.
Traffic mirroring is a method of sending replicas of real-time traffic to the mirror service for validating tests. Traffic mirroring is performed outside the critical request path of the primary service. For example, when forwarding a HTTP request to an intended target, mirror the traffic and send the mirrored traffic to other targets at the same time. This allows us to record the real data and traffic models in a production system, which can be used to generate simulated test data and plans and perform end-to-end stress testing in a more real manner.
Note: For performance considerations, Sidecar/Gateway will not wait for the response from the mirroring targets before the response from the intended target is returned.
Now let’s look at an active geo-redundancy example solution that is based on containers and service mesh:
Consider a microservices application for redeeming reward points, which is deployed in two k8s clusters in Beijing and Zhangjiakou respectively. This application implements network interconnection with Express Connect and database replication with DTS. The services in the whole system are managed by using a highly available Istio service mesh across clusters.
In this example, we can use Alibaba Cloud Container Service to easily and uniformly deploy and manage applications in k8s clusters in the two regions. Meanwhile, we can use the Istio service mesh to implement the service management plane. Initially, the clusters in both Beijing and Zhangjiakou provide public-facing services. When the cluster in Beijing cannot be accessed, we switch all service invocations to the cluster in Zhangjiakou to ensure business continuity.
Kubeflow is a Kubernetes-based community project initially launched by Google to support machine learning. Kubeflow can support different cluster learning frameworks in a cloud-native manner and manage the entire lifecycle from data preparation and model training to model prediction.
Arena is an open-source project contributed by the Alibaba Cloud Container Service team to the Kubeflow community. This tool can hide all the underlying details and cascade deep learning pipelines. Today we mainly explain how to use Service Mesh to strengthen online inference services and support multiple models/versions, phased release, refined traffic flow control, and elasticity.
Continuously optimizing deep learning models is a great challenge in AI production applications. For example, an e-commerce platform needs to train models continuously based on user behavior records so that it can better predict user preferences and consumption hotspots. Meanwhile, before a new model goes online, a safe and controllable process is required to validate the improvements brought by that new model.
By using the Arena commands, we can easily and continuously optimize training models, release models, and traffic switching models. Data scientists use “arena submit” to submit distributed model training, for which GPUs or CPUs can be used. When a new model is trained, the “arena serve” command deploys the new model to a production environment, where the old model is also retained. However, this new model doesn’t provide public-facing services. At this point, data engineers can use the “traffic-split” command in Arena to implement traffic control, model version updates, and rollback.
Reference:https://community.alibabacloud.com/blog/from-containers-to-cloud-native---service-mesh_594644?spm=a2c41.12740885.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@kevinhoffman/kevins-cloud-native-world-tour-2017-e9e6cc1316ba?source=search_post---------235,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Hoffman
Sep 17, 2017·2 min read
This year has been a very “cloud native” year for me. While I’ve been knee deep in the concepts and implementations of cloud native patterns for quite some time now, momentum increased quite a bit with the publication of Beyond the 12 Factor Application — Exploring the DNA of Highly Scalable, Resilient Cloud Applications.
Since then I co-wrote and published Cloud Native Go, a book that goes into depth on cloud native as well as disciplines like CI/CD, testing, automation, and why that’s all important for cloud native applications.
Just recently another book I wrote — Building Microservices with ASP.NET Core — also went to print and I’ll be discussing that topic in depth in London in October at O’Reilly SACon.
I have been presenting to friends, family, coworkers, and anyone else who will listen about cloud native concepts, patterns, and implementations. My quest has brought me to RedisConf in San Francisco to talk about using Redis to support microservices, to QCon in New York to talk about Cloud Native Go, and most recently all the way to Beijing, China to chat about cloud native, the “15 factors”, and my personal philosophy on team perspective that Dan Nemeth and I call “The Way of the Cloud” (云道).
The next stop on my cloud native world tour is London in just a few days. I am excited to be a part of this year’s Cloud Native London where I will be sharing my thoughts on cloud native application development and all that entails.
The term cloud native is yet another overused and diluted buzzword. Everyone you ask seems to have a different interpretation of the phrase and what it means for them and their organization.
This creates the need for distillers like myself who go around and shave the hype and coverage down to the essence of the thing. What is it? Why should I care? What’s in it for me and my company? Most importantly, what does this look like in the real world and away from Twitter and blog posts and conferences?
If you’re curious about any of these things and looking for some guidance and hopefully some inspiration on cloud native, then I’d love to see you at my next presentation!
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
See all (236)
2 
1
2 claps
2 
1
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/from-containers-to-cloud-native-ultimate-elasticity-a649f033bf6?source=search_post---------236,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 18, 2019·13 min read
By Yili
We can understand the evolution of cloud computing from the process of enterprises’ migration to the cloud.
The first stage is mainly about moving to the cloud. The “Lift and shift” strategy is used to move applications running on physical machines to virtual environments. This process is mainly cost-driven. Application development and maintenance in this stage are not very different from the original development and maintenance model.
The second stage is what we call cloud-ready. In this stage, enterprises start to focus on the total cost of ownership and hope to improve the innovation efficiency with cloud computing. From the maintenance aspect, virtual machine images or other standardized and automated methods are used to deploy applications. Specialized cloud services such as RDS and SLB are used to improve the maintenance efficiency and SLAs. In the meantime, microservice architectures are used as application architectures. Each service can be deployed and scaled independently, improving the system scalability and availability.
The third stage is today’s cloud-native era. In the cloud-native era, innovation becomes the core competency of enterprises, and most enterprises begin to fully embrace cloud computing. Many applications are on the cloud from the very beginning of their development. Cloud computing reshapes software throughout the lifecycle, including architecture design, development, construction, and delivery.
First, application loads can be seamlessly migrated to platforms such as private clouds and public clouds and computing and intelligence can be extended to edge environments to implement boundaryless cloud computing. Second, as new computing paradigms continuously emerge (such as containers, service mesh, and serverless computing), ultimate agility and elasticity can support faster innovation, trial and error, and business growth and reduce the cost accordingly. In addition, the DevOps concept is also recognized and accepted by many developers, facilitating changes in IT organization and architectures as well as cultural reform.
In this era, we also thank CNCF and other open-source communities for their efforts to collaboratively push forward the development of the cloud-native ecology.
Alibaba Cloud Kubernetes (ACK) supports both the private and public clouds of Alibaba Cloud and provides the optimization and integration of the basic Alibaba Cloud features. Meanwhile, we also make Alibaba Group’s practices in large-scale distributed systems accessible to all users in a cloud-native manner to enable these practices to benefit the whole world.
First, we use Kubernetes to implement an infrastructure abstraction layer so that containerized applications can fully utilize the powerful abilities in the bottom layer of Alibaba Cloud, such as computing, storage, and network capacity. For example, in deep learning and high-performance computing scenarios that require every high efficiency, we can utilize heterogeneous computing power of bare-metal machines, GPUs, and FPGA instances. Using elastic network interfaces with the Alibaba Cloud Terway network driver can reach up to 9 GB network bandwidth with almost zero loss. We can also use the RoCE (25 GB) network technology. Parallel file systems like CPFS can be used to improve the processing efficiency and provide up to 100 million IOPS and 1 Tbit/s throughput.
We provide two different service types to simplify the cluster lifecycle management. With Managed Kubernetes, users can select node types according to their workloads, and cluster lifecycle management operations are performed by Alibaba Cloud, such as creating, updating, scaling, monitoring clusters and alarming. Users can host master nodes to further simplify maintenance and reduce the cost. With Serverless Kubernetes, users don’t need to focus on any underlying resources and cluster management, and applications can be deployed and scaled as needed.
Based on this, Container Service provides more features.
In the management of containerized content, container image management is supported and the support for Helm application graphs is also provided. According to a CNCF survey, Helm (68%) is the most popular tool for packaging container applications. In addition, Alibaba Cloud also supports the Open Service Broker API that integrates container applications and cloud services.
In addition to the familiar DevOps and microservice applications, more and more enterprise applications (such as .net and JEE applications) use containerization to accelerate the modernization of IT architectures and improve business agility. In the meantime, large amounts of innovative business for Alibaba Cloud and its customers is based on Alibaba Cloud Container Service. For example, Alibaba Cloud businesses like AI prediction service, IoT application platform, and BaaS is based on ACK.
Cloud computing uses the scale effect to balance between peaks and valleys of business traffic for different tenants, significantly decreasing IT costs on a macroscopic scale. In the cloud era, elasticity has become a new normal and is also one of the cloud-native application features that customers pay the most attention to according to a CNCF survey.
The total transaction amount on the Tmall Double 11 Shopping Carnival in 2018 reached 213.5 billion RMB. As the technical supporter of this Double 11 event, Alibaba Cloud set a new record in “surge computing.” During this Double 11 event, the accumulated number of ECS cores allocated by Alibaba Cloud exceeded 10 million, which is equal to the capabilities of 10 large data centers.
Of course, many business scenarios cannot be planned in advance and are unpredictable, for example, hot celebrity gossip may cause social websites to experience instantaneous overloads.
Zhou Hongyi joked at the World Internet Conference held at Wuzhen, “Cloud computing is truly awesome if it experiences no downtime in the case of massive amounts of network traffic from news and discussions about five young male celebrities’ marriage announcements and four celebrities’ affairs.”
To achieve that goal, elasticity is required for infrastructures so as to provide sufficient resources in a short time and elasticity is also required for application architectures so as to host more business loads by fully using scaled computing resources.
Different types of workloads in the application layer have different requirements regarding resource auto scaling.
In the cloud era, auto scaling solves the conflict between traffic peaks with sharp traffic increase and resource capacity planning as well as the trade-off between the resource cost and the system availability.
All elastic architectures consist of the following basic parts.
Kubernetes supports auto scaling from two dimensions:
Two types of scaling policies are available:
Kubernetes HPA supports the three following monitoring metrics:
HPA Controller uses Metrics Aggregator to aggregate the performance metrics collected by Metrics Server and the Prometheus adapter and calculates how many replicas a deployment task needs to support target workloads.
To prevent scaling oscillations, k8s provides default cooldown cycle settings: the cooldown period is 3 minutes for scaling out and 5 minutes for scaling in.
Cluster Autoscaler listens to all pods. When a pod is not scheduled due to insufficient resources, the configured Auto Scaling Group (ASG) is simulated as a virtual node; attempts to reschedule the unscheduled containers are performed; a compliant ASG is selected to perform node scaling. When no scaling tasks are to be performed, the request resource usage of each node is traversed. A node with the request resource usage lower than the threshold will be deleted.
A cluster may have different ASG, such as the ASG for CPU instances and the ASG for GPU instances.
In this example, a GPU ASG is selected to perform scaling when a GPU application cannot be scheduled.
Alibaba Cloud provides a series of enhancements in addition to the basic Cluster AutoScaler:
The open source code of relevant features is available:https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/alicloud?spm=a2c41.12761833.0.0.3ad9e7b0yGRxPF
Compared with scaling in nodes, scaling out nodes is more complicated. We need to avoid impact on the application stability due to scaling in nodes. For this purpose, k8s provides the preceding rules to ensure safe scaling.
A spot instance (also called preemptible instance) is an instance type in Alibaba Cloud ECS. When creating a spot instance, users are required to set an upper price limit for the specified instance type. If the current market price is lower than the bid price, this spot instance is successfully created and users are billed on the current market price. By default, a user can hold a spot instance without interruption for one hour. When the market price exceeds the bid price later or the supply and demand relationship changes, the instance is automatically released.
Using Alibaba Cloud ECS spot instances properly can decrease the operating cost by 50%-90% (compared with pay-as-you-go instances). Spot instances are of great significance in price-sensitive scenarios such as batch computing and offline tasks. Alibaba Cloud also provides spot GPU cloud servers, which considerably reduce the cost of deep learning and scientific computing and benefit more developers.
Kubernetes clusters have a variety of bidding policies available. You can select multiple availability zones and multiple instance types for bidding. Based on the price of the current instance type, instances with the optimal price will be created, significantly improving the instance creation success rate and reducing the cost.
Spot node instances after elastic scaling have the “workload_type=spot” tag. By using the node selector, application loads that match the features of spot instances can be dispatched to new scaling nodes.
Gene sequencing is one basic technology for precision medicine and requires large amounts of computing power and thousands of different frameworks for computing. Recently, the Alibaba Cloud container team and Annoroad Gene Technology worked together to improve computing power by using container technologies and allow researchers to customize data processing procedures. This implements the deployment and the unified scheduling of thousands of nodes on/off the cloud, increasing the genetic data processing efficiency by 200%-300%.
With Cluster Autoscaler, hundreds of nodes are scaled out within five minutes for each batch of gene sequencing tasks and are recycled after use. In addition, the speed of the next startup is optimized. Cluster Autoscaler can significantly reduce the resource cost and ensure fast scheduling.
HPA is very helpful for stateless services and deals with business growth by increasing replicas. However, some stateful applications (such as databases ad message queues) cannot implement scaling by using horizontal scaling. For these stateful applications, generally more computing resources should be assigned to implement scaling.
Kubernetes provides the “request/limits” resource scheduling and limit policy. However, it is very challenging to properly configure application resources based on specific business requirements. Too few resources will cause stability issues, such as OOM and CPU preemption, while too many resources will affect the system usage.
The mission of VPA is to automatically and vertically scale stateful service resources and improve the system maintenance automation efficiency and usage.
Let’s see a typical VPA execution process.
Currently VPA is still in its infancy and requires further maturity improvements. For example, its data monitoring model, predictive computing model and the implementation of the update action all have room for improvements. Meanwhile, to avoid the uncertainty of updates by VPA Updater, the community suggestion is to make Updater optional, let VPA only provide some resource allocation suggestions and let users decide how to perform actions like scaling.
In May 2018, Alibaba Cloud released Serverless Kubernetes Container Service, which does not require node management and capacity planning and allows users to pay based on resources required for applications and implement auto scaling.
The bottom layer of Serverless Kubernetes is built on top of Alibaba Cloud’s virtualization solutions optimized for containers and provides a lightweight, efficient and safe execution environment for container applications. Serverless Kubernetes also implements service discovery by fully utilizing Alibaba Cloud SLB and DNS-based Service Discovery. Serverless Kubernetes provides Ingress support by using the Layer 7 SLB routing and is compatible with most semantics of Kubernetes applications so that deployment can be performed without modifying semantics.
Currently the serverless HPA supports autoscaling/v1 and CPU scaling. We are planning the support for autoscaling/v2beta1 and the scaling of other pod metrics (memory and network) in early December.
Now let’s see how Serverless Kubernetes is implemented. The project code of core component is Viking, a tribute to ancient Vikings’ warships well-known for their agility and fast speed.
Viking: Viking registers itself as a virtual node in Kubernetes clusters and implements behaviors of components such as kubelet, kube-proxy, and kube-dns. It is responsible for tasks such as Sandbox lifecycle management, network configuration, and service discovery. Viking performs a variety of actions: It interacts with the cluster API Server, listens to changes of events in the current namespace (such as pods, service, and Ingresses), creates ECI instances for pods, creates SLB instances for services or registers DNS domains, and configures routing rules.
Alibaba Cloud Elastic Container Instance (ECI) is a container-oriented lightweight virtualization implementation. Its internal agent is responsible for a variety of features such as the pod lifecycle management, startup sequence of containers, health check, process guarding, reporting of monitoring information. The bottom layer of ECI can be based ECS virtual machines or X-Dragon + lightweight virtualization technologies.
Serverless Kubernetes provides cloud-oriented autoscaling design.
Listen to changes in resources such as pods, services, and Ingresses and synchronize cloud resource status bidirectionally.
For example, pods will be created/deleted when a Deployment is used to deploy an application.
When an Ingress is passed through, SLB will be automatically used and created, and the features of Alibaba Cloud SLB will be utilized to implement routing rules. In addition, the binding to the back-end ECI is also performed.
We can also manage ECI instances together with classic k8s clusters by using the Serverless add-on to implement fine elasticity granularity and optimize the cost. ECI can be used with Istio.
The Serverless add-on is built on the Virtual Kubelet framework released by Microsoft. Relevant Virtual Kubelet projects that support ECI have been published as open source so that all developers can use these features.
We are continuously making some enhancements. In the future, many features of Viking will be available in the Serverless add-on.
Reference:https://www.alibabacloud.com/blog/from-containers-to-cloud-native---ultimate-elasticity_594681?spm=a2c41.12761833.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@manningbooks/free-ebook-going-cloud-native-2da284d67530?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Oct 7, 2019·1 min read
Going Cloud Native is a collection of chapters from three Manning books picked by Docker expert Ian Miell. In the first chapter, you’ll explore Docker, the industry standard in container platforms, and go hands-on as you create a running To-Do application. Then, you’ll take a close look at Kubernetes, a container orchestration system, and its basic building blocks, pods, which contain all the necessary resources of an application. Finally, you’ll learn about serverless computing, another cloud native option, in which applications are kept on servers hosted by a third-party service. Using AWS services, including Simple Storage Service, Simple Notification Service, and AWS Elemental MediaConvert, you’ll build a serverless video-sharing website.
As you read, you’ll see how going cloud native sharply decreases overhead when compared with applications hosted on traditional machines. You’ll also appreciate how it empowers developers to be much more productive as they build highly flexible, easily scalable, and more manageable applications. If you’re thinking about going cloud native for your next application or moving your existing applications to the cloud, this guide is a great first step!
For more great free content, taken from our books and liveVideo courses, check out our Free Content Center.
Follow Manning Publications on Medium for free content and exclusive discounts.
1 
1 
1 
Follow Manning Publications on Medium for free content and exclusive discounts.
"
https://medium.com/@ab415/2019-is-shaping-up-to-be-a-monumental-year-for-cloud-native-technology-b9b2c11b801b?source=search_post---------238,"Sign in
There are currently no responses for this story.
Be the first to respond.
Abby Kearns
Jun 28, 2019·4 min read
When I sat down at the end of last year to catch my breath and take stock of the cloud ecosystem, I made a few predictions for 2019, many are happening more quickly than I expected. Now that we’re halfway through the year — collectively reflecting on the professional wins, losses, and progress made — I want to revisit my view of the trends defining the cloud native ecosystem in 2019. In particular: market consolidation.
The cloud native market is starting to consolidate. We’re starting to see more incumbent players acquiring cloud native technology companies — pulling these startups in to help shape the shift in their technology, culture and operations needed to support the future tech stack.
During a two-week span in May, we saw VMware acquire Bitnami and Palo Alto Networks acquired Twistlock — further solidifying the diverse and increasingly valuable future that cloud native technologies will foster. Larger incumbent companies see cloud native startups as an opportunity to help them evolve their company to meet the quickly changing needs of the technology stack and their customers.
Wave of Consolidation
My prediction that consolidation would increase this year is bearing out. In fact, I expect activity to pick up even more during the second half of this year, especially with the very early and widely adopted cloud native technologies. It’s a natural cycle in technology — a broad dispersion of new emerging technologies eventually leads to consolidation in the offerings as standardization occurs. This cycle is just happening faster in the cloud native space.
Packaged applications and container security are among many important developments in this space, and large, well-positioned enterprises are on the lookout, eager to buy more. There will be more blockbuster deals, like F5 Network’s acquisition of NGINX from March to support the desire to align future strategies around key technologies.
Consider Kubernetes, for example, it’s a super impactful technology and has changed the conversation around cloud native architecture, but it’s only a small piece of a much larger puzzle we’re all trying to solve. As these emerging technologies proliferate and become commoditized, a necessary period of consolidation will follow, particularly with respect to the most commoditizable aspects of that technology.
All open source software follows a cycle — early development efforts that include lots of hype and excitement, followed by standardization, and then the eventual maturity of the technology. One sign of any infrastructure technology’s success is that it stops being the main topic of conversation. Docker has seen that happen, to some degree. So have Cloud Foundry and OpenStack. Kubernetes is definitely next. That’s just the way these markets and ecosystems evolve. As technology matures and consensus is reached either through adoption or acquisition, a wave of consolidation follows in kind.
Take the Long View
For me, it’s important to maintain focus on the macro trends. Take a step back from the tech-focused hype and pay attention to the greater evolution that is happening. In technology, particularly in enterprise infrastructure, there is a massive shift happening. The entire middleware market is being disrupted and rewritten. In short, the technologies that have been developed over the last twenty to thirty years need to be revisited since they were not developed with the cloud native world in mind.
What does that look like for your organization? Spend less time thinking about the hot technology of the day and more time dedicated to embracing the challenges and opportunities that cloud native technology will enable for your organization. What are the business outcomes you would like to see? How will the new cloud native technologies help you achieve that?
Moving to the cloud is not a complete strategy. Changing your business to write better software faster is equal parts vision, strategy, and execution. Having a sustained vision on what you want your company to look like in the future is key. Followed by a cogent strategy on how to achieve that outcome, which will include a mix of appropriate technology choices as well as an understanding of how your people will fit into that vision.
Sure, the cloud is foundational, but these technologies are all components and tools to help businesses achieve specific goals. Building a digitized business that’s more successful, more agile and thereby more competitive in a rapidly changing landscape is the type of mission that teams will rally around and thrive.
As consolidation picks up this summer and fall, we should all take notice and strategize for what comes next. But more importantly, we should keep an eye on how these various technologies fit into the larger evolution that is happening. We’re all prone to getting wrapped up in our little slice of the world. And while there’s nothing wrong with fully engaging in our own sector, we will all see more clearly when we zoom out and consider the big picture.
CTO @ Puppet. Passionate about good food and all things tech. Always learning.
1 
1 
1 
CTO @ Puppet. Passionate about good food and all things tech. Always learning.
"
https://medium.com/js-weekly/js-weekly-58-cloud-native-is-serverless-first-5ce446340e3d?source=search_post---------239,"There are currently no responses for this story.
Be the first to respond.
In this article, the author is putting all the theory to the test by performing step-by-step migration of an application, following the recommendations from the previous part. To make things straightforward, reduce uncertainties, unknowns, and unnecessary guessing, for the practical example of migration, he decided to demonstrate the practice on a simple to-do application.
“Cloud Native is lean and one of the lean principles is to defer commitment as long as possible. I certainly won’t tell you that you can’t build your cloud-native system with containers. But in most cases you probably don’t need to. First, you may not even know for certain if your value proposition is even worth building. Second, you probably don’t know which services will actually benefit from containers and which will not. So let’s experiment with real users before we commit to a decision.”
“A lockfile injection could attack in a similar fashion, and so you should have proper practices in place to mitigate this issue, or at least reduce the risk. Consider the following:
We are adding new libraries to JavaScripting.com every week. Here is one worth checking out:
New way to see a web page with CSS3 3D transform
JS.weekly() is a weekly digest of the best JavaScript articles, hand-picked by our experts in the JavaScripting community, sponsored by Salsita Software. Don’t forget to follow us on Twitter.
Top 3 articles, 1 tweet and 1 library from JavaScript world…
1 
1 clap
1 
Top 3 articles, 1 tweet and 1 library from JavaScript world we found interesting during the week.
Written by
The best JavaScript, web development and general programming articles of the day. Sponsored by Salsita Software (http://www.salsitasoft.com)
Top 3 articles, 1 tweet and 1 library from JavaScript world we found interesting during the week.
"
https://medium.com/@alibabatech/2020-double-11-highlights-large-deployment-of-cloud-native-technology-for-higher-efficiency-and-a003d9b8e9ae?source=search_post---------240,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Dec 3, 2020·5 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Developer
Tmall has broken two records during the 2020 Double 11 Global Shopping Festival for consumption in GMV (US$74.1 billion) and peak orders per second (583,000). Alibaba Cloud has once again handled the world’s largest traffic peaks without any major issues. How did our technology support the entire event, providing a smooth experience for nearly one billion shoppers around the world?
Recently, Alibaba held the Technical Communication Meeting for Double 11. During the meeting, Ding Yu, Researcher of Alibaba Cloud and the Head of the Cloud-Native Application Platform of Alibaba Cloud, said, “This year, Alibaba Cloud has achieved major technical breakthroughs in the comprehensive cloud-native of the core system to implement major improvements in resource efficiency, R&D efficiency, and delivery efficiency. The resource cost of every 10,000 transactions has been reduced by 80% in four years. The R&D and O&M efficiency have been increased by more than 10% on average, and the delivery efficiency of scale applications has been improved by 100%. This means that Alibaba has successfully completed a comprehensive deployment of cloud-native technology during the 2020 Double 11 Global Shopping Festival.”
Ding Yu, Researcher of Alibaba Cloud and the Head of the Cloud-Native Application Platform of Alibaba Cloud
Compared with the comprehensive cloud migration in 2019, the comprehensive cloud-native in 2020 has revolutionarily reconstructed the “technology engine” for the Double 11 Global Shopping Festival. On the product side, there are dozens of cloud-native products, such as Alibaba Cloud Container Service for Kubernetes (ACK), ApsaraDB for PolarDB, ApsaraDB for Redis, RocketMQ, Enterprise Distributed Application Service (EDAS), Microservice Engine (MSE), and Application Real-Time Monitoring Service (ARMS).
By utilizing these products, Alibaba Cloud could fully support the Double 11 Global Shopping Festival. On the technical side, the four core technologies of cloud-native have made breakthroughs in the aspect of scale and innovation. They are demonstrating the way of transforming from technical capabilities to business values:
Cloud-native technologies have not only been extensively used in Alibaba, but they have also provided services for the Double 11 Global Shopping Festival through Alibaba Cloud. During the festival, cloud-native supported many customers, such as China Post, STO Express, Perfect Diary, and Century Mart, to help them handle the traffic stably and efficiently. We can use the logistics industry as an example. STO Express moved its core system onto the cloud and used Alibaba Cloud Container Service to obtain a stable system for millions of parcels in transit with 30% less IT costs. The large shopping malls and supermarkets have also gained benefits from cloud-native. For example, based on the elastic scaling of Alibaba Cloud’s Function Computing (FC), the QPS of Century Mart was 230% higher than that of the 2019 Double 11 Global Shopping Festival during its peak hours, the R&D efficiency and delivery efficiency increased by more than 30%, and the elastic resources cost reduced by more than 40%, respectively.
After Alibaba announced the establishment of the Cloud-Native Technical Committee at the Apsara Conference in September, cloud-native was upgraded to a new strategy for Alibaba technologies. The comprehensive cloud-native of the core system for 2020’s Double 11 Global Shopping Festival was an important milestone for the Cloud-Native Technical Committee to promote the comprehensive cloud-native development of Alibaba’s economy. Cheng Li, Chief Technology Officer of Alibaba Group, said, “The most significant difference brought by cloud-native is that Alibaba has truly realized achievements on proprietary research, open source, and commercial use, which can be directly used by customers. Doing this will eliminate the process of precipitation and output on the cloud and reduce the threshold and costs for customers to obtain “the same technology engine for Double 11.” As a result, customers can quickly enter the era of digital-native.
Cheng Li, Chief Technology Officer of Alibaba Group
Cloud-native is the quickest way to access the benefits of cloud computing and will become a new base for all-round cloud migration. Ding Yu pointed out that cloud-native is the true revolution of cloud technology that re-upgrade cloud computing. It has promoted the evolution of Cloud Hosting to cloud-native. It means Alibaba Cloud has walked from an old and closed technology system to an open and standard cloud technology system. Along with supporting the Double 11 Global Shopping Festival, these technologies will become the infrastructure of the new digital construction to support the whole society through Alibaba Cloud.
www.alibabacloud.com
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
1 
1 
1 
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@alibaba-cloud/ant-financials-going-all-in-on-cloud-native-cdf177d0cab9?source=search_post---------241,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 6, 2019·14 min read
By Miukeluhan.
11.11 Big Sale for Cloud. Get unbeatable offers with up to 90% off on cloud servers and up to a $300 rebate for all products! Click here to learn more.
Over the past 15 years, Ant Financial has completely reshaped how people make payments in China. However, these advancements in everyday finance wouldn’t have been possible without the support of several important technological innovations. During this year’s Apsara Conference held in Hangzhou, Ant Financial shared its experience in tech over the past 15 years and its plans for future innovations in the financial technology space. This article covers the announcements presented at the conference.
With the rapid development of Internet technology, we are moving into the cloud-native age. However, how should the financial industry embrace cloud native? During the past two years, Ant Financial implemented cloud native technology in the financial space, gaining some practical, real-world experience in this area. In this presentation, I will share what I think is the ideal cloud-native architecture for Ant Financial, the problems Ant Financial has encountered during the process of introducing cloud native, and some solutions to these problems.
Cloud computing has been a booming industry full of much growth and innovation for the past several years now. At present, it is easy to migrate to the cloud. But, in the future, we need to consider how we can use cloud in an even better and more efficient way. According to the latest statistics by RightScale in 2019, public cloud accounts for 22% of all cloud, with only 3% customers using private cloud. The vast majority of customers use a hybrid of public and private cloud solutions, in short hybrid cloud, to achieve a balance with their data privacy, security, efficiency, and elasticity priorities.
From the perspective of the global IT industry, public cloud only accounts for a 10% market share of the entire IT infrastructure market. In other words, the marketplace still has much room for growth in cloud, and many customers engaged in the market are traditional enterprises. An important reason that traditional industries cannot make full use of public cloud is that they have invested in, and have been building, their own IT systems over a long time, and many of them also have their own data centers. In addition, some enterprises for the stability and security of their businesses are not interested in using public cloud services. Therefore, these enterprises often tend to develop hybrid cloud policies. For example, they may store their core businesses in a private cloud, but migrate marginal or innovative businesses to a public cloud.
In addition to these distinctive characteristics, the financial industry has another two characteristics:
Therefore, hybrid cloud is more suitable for financial institutions. This conclusion is also backed up by research. As reported by Nutanix, the global financial industry has developed faster than other industries in terms of hybrid cloud application. Presently, 21% of financial institutions have deployed hybrid clouds, exceeding the global average of 18.5%.
So, what type of hybrid cloud is suitable for financial institutions? This article takes the evolution process of Ant Financial as an example.
The fourth-generation of Ant Financial’s IT architecture was transformed into a cloud-based one, specifically an elastic hybrid cloud architecture that can meet the elasticity requirements and burstable bandwidth of Ant Financial’s online services. At present, Ant Financial has already entered the fifth-generation of its IT architecture, which is now a hybrid cloud-based cloud-native architecture.
Here, let’s discuss the process of how Ant Financial moved from a hybrid cloud architecture to a cloud-native one. Throughout the process, one common thread was that Ant Financial had strong requirements and held high standards at all stages of research and development, allowing for a completely independent development process with good cost control, security, and stability. All of these standards can be seen in Ant Financial’s cloud-native architecture.
To establish a financial-grade online transaction system, the first step that Ant Financial needed to take was to implement a financial-grade distributed architecture. Ant Financial has two representative technologies in this area, SOFAStack and OceanBase. Both are now in widespread use after an initial beta phase. SOFAStack is used to build a scalable architecture at the entire application layer or the stateless services layer. OceanBase is used to deploy the storage infrastructure, which is typically a database, or a stateful service layer over the architecture. Both technologies have the following features:
These four key features are critical to financial businesses and must be implemented in applications and storage in end-to-end mode.
Consider consistency for example. Data consistency can be ensured in a single database. However, in large-scale applications, a single database will always be a bottleneck and data will need to vertically split based on finer granularity, such as transaction, payment, or account data, which is similar to the splitting of services or applications. When data is stored in different database clusters, consistency must be ensured at the application layer. To support mass data, a database cluster is also internally divided and stored in multiple replicas. OceanBase is one of such distributed database products that is used to implement distributed transactions internally. The consistency across a distributed architecture is exclusively dependent on this cooperation.
Now, consider scalability as another example. Some systems may be said to use a distributed architecture when, in reality, they use a micro-service framework for service optimization at the application layer. For these systems, horizontal scaling cannot be used at the database layer. And, in such systems, scalability is greatly limited by the shortcomings of the data layer.
With the above considerations, we know then that a real distributed system must take advantage of end-to-end distribution to achieve a high level of performance and scalability without these above limitations. In addition, a real financial-grade distributed system must provide end-to-end high availability and consistency.
Pictured above is the architecture behind Ant Financial’s Active Geo-redundancy Architecture that works in five data centers across three zones.
The main goal for the high-availability architecture at Ant Financial was to prevent data loss and ensure that service is uninterrupted. With this goal, we designed and implemented an active geo-redundancy architecture in five data centers across three zones. The core advantages of this architecture is city-level disaster recovery, cost-effective transactions, and unlimited scalability as well as an RPO of 0 and a PTO of less than 30 seconds. Ant Financial demonstrated network cable cutting at ATEC 2018. The demonstration showed how Ant Financial can achieve fast recovery in the cases of inter-city active geo-redundancy and disasters. In addition to meeting the goal of high availability, Ant Financial also worked hard to reduce risks. In summary, Ant Financial was able to ensure the security of capital, immunity to change, and fast fault recovery on top of high availability.
In addition to high availability, the most frequently mentioned topic in the finance industry is security. In the cloud-native age, we need to resolve end-to-end security risks through the entire process. The security issue consists of the following concerns:
In a speech called Cloud Native Security Architecture for Financial Services, the security advantages of this architecture were outlined. Click here to see the collated speech script. To sum up, the main capabilities are to provide financial-grade high availability and end-to-end security.
In the next section, let’s look at some of the problems that Ant Financial encountered when moving to cloud-native.
Pictured above is Ant Financial’s transition from a unit-based architecture to a cloud-native architecture that is elastic. This transition is done to help Ant Financial be able to more effectively respond to traffic spikes in online services.
To understand this transition, we need to first look into what exactly the unit-based architecture is. Part of this architecture is the database layer, which involves database sharding and table sharding. Sharding is used to improve the subpar computing performance of the centralized storage of the system. The core idea of unit-based architecture is to move data sharding to entry request sharding. At the network access layer of a data center, user requests are sharded based on a dimension such as a user ID. In this context, each data center is similar to a huge stateful database sharding.
For example, when your user ID ends with 007 or 008 and your request is sent to the data center from a mobile phone or a webpage domain name, the access layer can identify whether to route your request to China East or China South. When you are in the data center for a region, most requests can be processed inside the data center. Sometimes, services need to be called across data centers. For example, a user whose data is in data center A transfers to another user whose data is in data center B. In this case, stateful design must be completed in the data centers.
As we enter the cloud-native age, we can rethink the overall architecture based around Kubernetes. In a unit-based architecture, we can deploy a Kubernetes cluster in each unit and globally deploy logical federated API servers that can manage multiple Kubernetes clusters and deliver control commands. Control metadata is stored in an Etcd cluster to maintain the consistency of global data. However, Etcd clusters can only implement disaster recovery between data centers in the same city, but not among multiple data centers in different cities. Considering this, Ant Financial has moved Etcd to the key-value (KV) engine of OceanBase. The storage format and semantics of Etcd are maintained at the engine layer, and the storage layer can provide the high availability of five data centers across three zones.
Pictured above is the heterogeneous infrastructure of financial institutions.
With this architecture is applicable for Ant Financial’s technical architecture, we encountered several new challenges when opening our technology to external customers. For example, the data center of a customer used several different heterogeneous infrastructure systems, which meant that we had to implement multi-cloud adaptation based on the standards of a cloud provider.
Many financial institutions, including Ant Financial, were not the designers of their old systems as they are with these new cloud-native system. Many systems had stateful dependencies on infrastructure, such as IP address dependency, and as such, it was difficult for them completely use immutable infrastructure to support the system.
Following this, the high requirements for business continuity, the operation and maintenance mode of the native Kubernetes workload were simply unacceptable. For example, when native deployment was used for a phased or canary release, the processing of applications and traffic was rather primitive, which could in turn lead to exceptions and interruptions to business during operation and maintenance changes.
Ant Financial was able to resolve these problems, however, by expanding the native deployment into CAFEDeployment, which made the releases, phased releases, and rollback of large-scale clusters more refined, complying with the Ant Financial’s “three principles for handling technical risks”. This all made for a more suitable architecture for financial services.
So, to summarize things, Ant Financial discovered that a financial-grade hybrid cloud must be able to allow for elasticity, support system heterogeneity, and provide the stability needed for the operations and maintenance of large-scale architectures. And, we think, with new changes and the development of new businesses, the financial industry will need to focus on how to implement more sustainable development, while also retaining traditional development and operation and maintenance models to support existing business services and operations.
Pictured about is the multimode infrastructure Platform-as-a-Service (PaaS).
Cloud native is derived from a Platform as a Service (PaaS) model. When changing architectures one issue we needed to confront was how to convince customers to consider new delivery models when they want to stick to their original way of doing things. In the traditional model, customers were used to delivering code packages and to virtual machine-based operation and maintenance. However, in the cloud-native age, given that container images are used as delivery carriers, and running instances are container instances of images, we were able to simulate the running mode of virtual machines with containers, and simulate the operation and maintenance mode of virtual machines through expanding deployment when the container mode is supported.
In addition, the main operations of implementing a PaaS based on a traditional architecture and Kubernetes are the same. This is true for creating websites, as well as with launching, restarting, and scaling services, and even for going offline. It is clear that implementing two PaaSs wastes resources and increases maintenance costs. The functions for users are the same. Therefore, we used Kubernetes to implement the public part, including the unified metadata, unified operation and maintenance operations, and unified resource abstraction. In addition, we provided two interfaces, one at the product layer and the other at the operation and maintenance layer. We also supported the traditional application mode and technology stack mode for delivery as well as image-based delivery. In addition to applications, we can also support functions.
Pictured above is the SOFA bi-modal microservice platform.
The next step is the bi-modal microservice platform, which also happens to involve challenges that are related to transitioning from old and new systems. However, many of these problems can be mitigated with the help of mesh. In the cloud-native architecture, mesh is the Sidecar in a pod. However, old systems usually do not run on Kubernetes and do not support the operation and maintenance mode of the pod and Sidecar. Therefore, we must manage the mesh process in agent mode so that mesh can help to optimize services for applications in the old architecture and to manage services in the new and old architectures in a unified manner.
The data plane must be deployed in two modes and the control plane also needs to support both modes. Traditional SDK-based microservices register services with the old service registration service, but mesh register services are based on the control plane. Therefore, we can integrate the control plane with the old service registration service, and use the latter to provide the service registration and discovery service to achieve the visibility and routing of global services. For those who have learned about the service registration system of Ant Financial, they know how we implemented the high-availability design in an ultra-large scale and multi-data center environment. It is difficult to implement these capabilities on the community control plane shortly. We are gradually migrating these capabilities to the new architecture. Therefore, this bi-modal control plane is also applicable when the service architecture smoothly evolves to the cloud-native architecture in hybrid mode.
The last one is the Serverless architecture. Recently, the Serverless architecture is popular because it applies to many scenarios. However, this architecture imposes demanding requirements for performance and requires that each application can quickly start up. Otherwise, the architecture cannot be used in the production environment.
The internal node system of Ant Financial uses the Serverless architecture at a large scale and optimizes the startup speed. Currently, the average speed is around 4 seconds and will be accelerated to less than 1 second after further optimization. However, the Serverless architecture is not applicable for Java applications because common Java applications require 30 seconds to 1 minute to complete startup. Therefore, Java applications cannot share the benefits of the Serverless architecture due to slow startup.
We used the support vector machine (SVM) technology of Java Virtual Machine (JVM) to perform static compilation on applications and optimized the startup time of an application from 60 seconds to 4 seconds. However, this was achieved at the expense of some dynamic features such as reflection. In addition, we modified the SDKs of some middleware to keep the applications unchanged and mitigate the impact of the adaptation on the applications. When advanced technologies can support application startup within 1 second, the entire ecosystem of Java technology can more quickly migrate to this architecture, and more applicable scenarios will be supported by then. However, this process takes a long time and requires that more people in the community engage in the anti-dynamic transformation of open source class libraries. Therefore, we use the class isolation of our application containers to support different modules or different versions of the same module running in one Java runtime without interfering with each other. In addition, the fast cold startup and fast scaling in the Serverless architecture can be simulated.
The Java runtime that has the isolation capability and supports fast loading and unloading of modules is called an SOFA Serverless container. The smallest runtime module is called an SOFA function. These small code snippets are programmed with a series of Serverless APIs. In the transition phase, we deployed a Scalable Open Financial Architecture (SOFA) Serverless container as a cluster, in which multiple SOFA functions can be scheduled. In this case, the SOFA Serverless container and the SOFA functions are deployed in N:1 mode. In the future, if the Serverless architecture can support the fast startup of Java applications, these SOFA functions can be smoothly deployed in pod mode. Then, one SOFA function can run in one SOFA Serverless container.
To summarize, the financial-grade hybrid cloud must be evolutionary and iterative to ensure the smooth evolution of technologies. Therefore, we provide the bi-modal “hybrid” capability at the PaaS, microservices, and Serverless layers.
Above is a graph showing development trends of businesses and technologies in the financial industry.
Finally, we can see that both the development trends in banking and the entire financial field have a direct mapping to the evolution trends of the technical architecture, with different capabilities are required at different stages. I believe that many banks are undergoing digital and mobile transformation. However, after completing mobile transformation and fully incorporating themselves into the Internet, many financial institutions will also encounter the same problems as Alipay did.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://blog.geekyants.com/cloud-native-approach-for-software-engineering-two-part-article-d8499e88eb2a?source=search_post---------242,"This the 1st part of the two-part series talking about Cloud Native Apps and Containerization.
In recent days you would have heard people talking about building cloud native applications and if you were a bit more curious about it then you would have even heard of CNCF (Cloud Native Computing Foundation) certifications. So what exactly is this Cloud Native approach, you ask?
Read Part 1 here: https://geekyants.com/blog/cloud-native-approach-for-software-engineering-part-1---cloud-native-apps-177
These concepts blend into a foundation course for implementing Containerization which helps save resources and mount apps efficiently.
Read Part 2 here: https://geekyants.com/blog/cloud-native-approach-for-software-engineering-part-2---containerization
All credits to our newest geek, Vasuki Vardhan for taking the time out to talk about Cloud Infrastructure and write these articles.
The Official Blog of GeekyAnts, a colony of mad scientists…
1 
1 clap
1 
Written by
Challenges . Experiments . Research
The Official Blog of GeekyAnts, a colony of mad scientists who experiment for a living.
Written by
Challenges . Experiments . Research
The Official Blog of GeekyAnts, a colony of mad scientists who experiment for a living.
"
https://medium.com/@alexnadalin/book-review-cloud-native-infrastructure-e6547343073b?source=search_post---------243,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Nadalin
Sep 25, 2018·2 min read
This goes right into the list of books I really wanted to like but kind of disappointed me.
The book is structured extremely well, so it comes out as a very enjoyable read. One downside, though, becomes fairly evident a few chapters in: it isn’t practical at all, describes few patterns and instead focuses a lot on processes, advices and high-level description of approaches you should follow to embrace CN infrastructure.
I was probably expecting too much, but when I read the book’s description on the Kindle store:
This practical guide shows you how to design and maintain infrastructure capable of managing the full lifecycle of these implementations.
I got fairly excited and proceeded to buy.
Again, as I mentioned the book is really nice to read — I devoured it during my daily commute to work, and that’s something you don’t expect from most technical books. When it comes to content, though, I found it a tad repetitive and without many real-world examples / lessons, so it felt more of an “abstract” book around the Cloud Native ecosystem than anything else.
I would have definitely appreciated if the last chapter, which serves as an introduction to interesting patterns such as circuit breaking, would have gone more into details: it might have been out of the scope of the book but would have definitely made it feel more “hands-on”.
At the end of the day, Cloud Native infrastructure goes down as an introductory guide to the CN world. I’d recommend you to read it if you just started approaching the ecosystem.
Originally published at odino.org (1st September 2018).You can follow me on Twitter — rants are welcome! 😉
CTO at @NamshiDotCom, I like distributed systems, Golang, NodeJS, scalability, software design and µseconds. Writing https://leanpub.com/wasec :wq
1 
1 
1 
CTO at @NamshiDotCom, I like distributed systems, Golang, NodeJS, scalability, software design and µseconds. Writing https://leanpub.com/wasec :wq
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-kubernetes-and-cloud-natives-working-out-in-alibaba-s-systems-35e4ff40dccb?source=search_post---------244,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 15, 2020·10 min read
By Zhang Zhen (Shouchen), Senior Technical Expert for Alibaba Cloud’s Cloud-native Application Platform.
By in large, Kubernetes has become the de facto standard for container orchestration engines and nearly everything else that’s cloud native. Up to now, Kubernetes has been implemented at Alibaba Group for several years, but the way we used Kubernetes has changed a lot, having gone through four distinct stages:
Throughout the process, however, one problem still lingers on the minds of the architects. In such a massive and complex business as Alibaba Group, a large number of previous O&M methods remain along with the current O&M systems that support these methods. In this context, what methods are required for implementing Kubernetes, and what should be compromised or changed moving forward?
With all of this in mind, in this article, I’ll share, from the inside, Alibaba’s thoughts regarding these issues. In short, introducing Kubernetes wasn’t the ultimate goal of improving or advancing Alibaba’s systems. Rather, we implemented Kubernetes simply to drive the transformation of our business because, with the capabilities of Kubernetes, we can resolve the deep-seated problems in our old O&M system and unleash the elasticity of the cloud, and accelerate the delivery of business applications. However, as to whether to keep our old O&M systems and what changes should we do moving forward, there’s still a lot that can be discussed.
In Alibaba’s original O&M system, the platform-as-a-service (PaaS) system makes application changes by creating operation tickets, initiating workflows, and then making serial changes to the container platform.
When an application is released, PaaS system searches for all the containers related to the application in the database, and then makes a change to the container platform to modify the container image for each container. Each change is somewhat of a workflow that involves pulling images, stopping old containers, and creating new containers. And, if an error or timeout occurs during such a workflow, then the PaaS system will have to retry the failed operation to correct it. Generally speaking, then, to ensure that a ticket can be closed in a timely manner, the retry is performed only a few times. If all these retries fail, then manual processing is required at that point.
When an application is scaled in, PaaS performs deletion operations according to the container list that is specified based on the input from the O&M personnel. If a container fails to be deleted due to a host exception, or the deletion times out, then PaaS has to retry the operation repeatedly. So, to ensure the closing of the ticket, the container is considered to be deleted after a certain number of retries. However, if the host recovers later, then, in reality, the deleted container may still be running.
Therefore, the conclusion we can draw is that, with the older system, process-oriented container changes suffer from the following permanent problems:
1. A single failed change leads to final failure. For example, if a container image change fails, the PaaS system cannot guarantee the consistency of the final container image. And, if a container fails to be deleted, it is impossible to ensure that the container has been fully deleted. In both examples, inconsistent container problems must be handled through manual inspection. However, rarely executed inspection tasks cannot ensure an absolute lack of errors or promptness of action either.
2. Multiple changes may conflict with each other. For example, application release and application scale-out must be locked. Otherwise, a newly scaled container image may not be updated. Once a change is locked, the efficiency of the change will be greatly affected.
In Alibaba’s old O&M system, the container platform only produces resources. Application launch and service discovery are performed by the PaaS system after containers are launched. As such, this layered method provides the greatest freedom to the PaaS system. At the same time, it also promoted the prosperity of Alibaba’s initial container ecosystem after containerization first happened. However, this method has a serious problem. For example, the container platform cannot trigger container scaling alone, but needs to build a complex linkage with PaaS. In addition, the upper-layer PaaS has to complete lots of repetitive work. All of this will hinder the container platform from efficient self-recovery when the host fails or restarts, or when processes in the container become abnormal or stuck. Moreover, it makes elastic scaling extremely complex. This naturally could be a major problem.
With Kubernetes, container commands and lifecycle hooks can be used to build PaaS processes for application launch and application launch status check into a pod. In addition, by creating a service object, you can then associate containers with corresponding service discovery mechanisms to unify the lifecycles of containers, applications, and services. The container platform not only provides production resources, but also delivers services that can be directly used by the business. This greatly simplifies the implementation of fault recovery and automatic scaling after cloud migration, taking full advantage of the cloud’s elasticity.
In addition, when the host fails, the PaaS system needs to scale out the application before deleting the container on the host. However, in large-scale clusters, it is difficult to scale out applications in most cases. The application resource quota may be insufficient, or idle resources in the cluster that meet the application scheduling restrictions may be insufficient. Containers on the host cannot be evicted if scale-out is impossible. As a result, the abnormal host cannot be fixed, and over time, the entire cluster can contain a number of faulty servers that can neither be fixed nor removed.
In Kubernetes, the handling of faulty servers is much more simple and crude. Instead of first scaling out applications, the container on the faulty server is directly deleted, and the load controller performs scale-out only after the deletion is completed. At first glance, this solution seems daring. When it was implemented in Kubernetes, many PaaS followers showed strong objection to this method, because they believe it could seriously affect business stability. In fact, most core business applications reserve certain redundant capacity to implement global traffic switching or to deal with business traffic bursts. In other words, temporarily deleting a certain number of containers does not lead to insufficient capacity.
Instead, the key is to determine the available capacity of an application. This is a more difficult problem. However, accurate capacity assessment is not required for self-healing scenarios. In this case, a pessimistic estimate that can trigger self-recovery would be enough. In Kubernetes, you can use PodDisruptionBudget to quantitatively describe the number of migrated pods for an application. For example, you can set the number or proportion of pods to be simultaneously evicted for the application. You can set this value based on the proportion of each batch at the time of release. If an application is normally released in 10 batches, you can set maxUnavailable in PodDisruptionBudget to 10%. For the proportion, if only 10 or less pods exist for an application, Kubernetes still considers that one pod can be evicted. If the application even disallows the eviction of a single pod, such an application needs to be transformed before it can enjoy the benefits of migrating to the cloud. Generally, an application can automate application O&M operations by modifying its own architecture or by using operators to allow pod migration.
The emergence of Docker provided a unified application delivery pattern. The binary code, configurations, and dependencies of applications are packaged into an image during the building process, and application changes are made by using a new image to create a container and deleting the old container. The major difference between the delivery methods of Docker and traditional software package-or script-based delivery methods for delivering applications is that it forces the container to be immutable. To change the container, you must create a new container, with each new container being created from the same image of the application. This ensures consistency and avoids configuration drift or snowflake servers.
Kubernetes further strengthens the concept of immutable infrastructure. In the default rolling upgrade process, it neither changes containers nor changes pods. Each release is done by creating a new pod and deleting an old pod, which not only ensures the consistency of application images, but it also ensures that data volumes, resource specifications, and system parameter configurations remain consistent with the spec of the application template. In addition, many applications have complex structures. One pod may contain components that are individually developed by different teams. For example, an application may include business-related application servers, log collection processes developed by the infrastructure team, and even third-party middleware components. To release these processes and components separately, you cannot place them in the same application image. Therefore, Kubernetes provides the support for multi-container pods, allowing multiple containers to be orchestrated in a pod. In this way, you can publish a single component simply by modifying the container image.
However, Alibaba’s traditional containers are rich containers. That is, application servers and log collection processes are all deployed in the same large system container. As a result, the resource consumption of components such as log collection cannot be individually controlled, and independent upgrade is impossible. Therefore, during this cloud migration at Alibaba, all components in system containers, except business applications, were split into independent sidecar containers. We call this process a lightweight container transformation. After the transformation, a pod contains a master container that runs business, an O&M container that runs various infrastructure agents, and a sidecar that runs service meshes. After lightweight containerization, the master container can run a business service with a relatively low overhead, which facilitates serverless transformation.
However, the default rolling upgrade process of Kubernetes is too rigid for the implementation of the immutable infrastructure concept, which results in a serious lack of support for multi-container pods. Although multiple containers can be deployed in a single pod, releasing a container in the pod not only rebuilds the container to be released during the actual release, but also deletes and then reschedules and rebuilds the entire pod. This means that, to upgrade the log collection component of the infrastructure, all other components, especially application servers, will be deleted and restarted, which affects normal business operation. Therefore, changes to multiple components still have not been decoupled.
For an application, if a pod has a local caching component and the caching process is restarted every time this application is released, the cache hit rate drops greatly during the application release, which affects the performance and even the user experience. In addition, if component upgrades by the infrastructure, middleware, and other teams are tied with the component upgrade of this application, it is very difficult to achieve iterative technical upgrades. Assume that the middleware team has launched a new version of service mesh. The release of each application is required to update the mesh components. In this situation, the technical upgrade of the middleware is dramatically retarded.
Therefore, in contrast to pod-level immutability, we believe that adhering to immutability at the container level can give full play to the technical advantages of multi-container pods of Kubernetes. To this end, we have developed the ability of modifying part of the containers in place in the pod when releasing an application. In particular, we have built a workload controller that supports in-place container upgrade, and have replaced the default deployment and the StatefulSet controller of Kubernetes with the new workload controller to handle most internal workloads. Additionally, SidecarSet has been built to support the upgrade of sidecar containers across applications, facilitating the upgrade of infrastructure and middleware components. The support for in-place upgrade provides additional advantages such as cluster distribution determination and image download acceleration. These features have been made open-source through the OpenKruise project. For the name of OpenKruise, Kruise sounds like ""cruise"", and the ""K"" letter stands for Kubernetes. Altogether, OpenKruise represents the automatic cruising of applications on Kubernetes, which fully utilizes Alibaba's years of management experience in application deployment and the best practices of Alibaba's cloud-native economy. Currently, OpenKruise is planning to release more controllers to support more scenarios and functions, such as rich release policies, Canary release, blue-green release, and batch release.
This year, we implemented Kubernetes at a large scale and withstood the real-world test of Double 11. For scenarios with a large number of existing applications like Alibaba, there is no shortcut for implementing Kubernetes. We have quit the idea of rapid and large-scale implementation and chose not to make the implementation compatible with outdated O&M methods. Instead, we strove to lay a solid foundation in order to fully extract the value of cloud native. In the future, we will continue to promote the cloud-native transformation of more applications, especially stateful applications, to make the deployment and O&M of stateful applications more efficient. In addition, we will also promote the cloud-native transformation of the entire application delivery system to make application delivery more efficient and standardized.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/how-does-alibaba-cloud-build-high-performance-cloud-native-pod-networks-in-production-environments-31fe1cb27678?source=search_post---------245,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 21, 2020·16 min read
By Xiheng, Alibaba Cloud Technical Expert
On April 16, we livestreamed the second SIG Cloud-Provider-Alibaba seminar. This livestream demonstrated how Alibaba Cloud designs and builds high-performance pod networks in cloud-native environments. This article recaps the content in the livestream, provides download links, and answers questions asked during the livestream. I hope it will be useful to you.
First, I will introduce Cloud Provider SIG. This is a Kubernetes cloud provider group that is dedicated to making the Kubernetes ecosystem neutral for all cloud providers. It coordinates different cloud providers and tries to use a unified standard to meet developers’ requirements. As the first cloud provider in China that joined Cloud Provider SIG, Alibaba Cloud also promotes Kubernetes standardization. We coordinate on technical matters with other cloud providers, such as AWS, Google, and Azure, to optimize the connections between clouds and Kubernetes and unify modular and standard protocols of different components. We welcome you to join us in our efforts.
With the increasing popularity of cloud-native computing, more and more application loads are deployed on Kubernetes. Kubernetes has become the cornerstone of cloud-native computing and a new interaction interface between users and cloud computing. As one of the basic dependencies of applications, the network is a necessary basic component in cloud-native applications and also the biggest concern of many developers as they transition to cloud-native. Users must consider many network issues. For example, the pod network is not on the same plane as the original machine network, the overlay pod network causes packet encapsulation performance loss, and Kubernetes load balancing and service discovery are not sufficiently scalable. So, how do we build a cluster pod network?
This article will describe how Alibaba Cloud designs and builds high-performance, cloud-native pod networks in cloud-native environments.
The article is divided into three parts:
First, this article introduces the basic concepts of the Kubernetes pod network:
The following figure shows the Kubernetes pod network.
Pod network connectivity (CNI) involves the following factors:
To achieve these network capabilities, address assignment and network connectivity are required. These capabilities are implemented with CNI network plugins.
Container Network Interface (CNI) provides a series of network plugins that implement APIs allowing Kubernetes to configure pod networks. Common CNI plugins include Terway, Flannel, and Calico.
When we create a pod:
Typically, pods are not in the same plane as the host network, so how can the pods communicate with each other? Generally, the following two solutions are used to connect pods:
We need Kubernetes Service for the following reasons:
The cloud IaaS layer network is already virtualized. If network virtualization is further performed in pods, the performance loss is significant.
The cloud-native container network uses native cloud resources on the cloud to configure the pod network.
CNI calls the cloud network open APIs to allocate network resources.
Pod networks become first-class citizens in a VPC. Therefore, using cloud-native pod networks has the following advantages:
IaaS layer network resources (using Alibaba Cloud as an example):
ENIs or ENI secondary IP addresses are allocated to pods to implement the cloud-native pod network.
How to solve the gap between cloud resources and rapid pod scaling:
Terway uses an embedded resource pool to cache resources and accelerates startup.
You can configure, call, and batch apply for resources for concurrent pod networks, as shown in the following figure.
We must also consider many other resource management policies, such as how to select vSwitches for pods, to ensure sufficient IP addresses, and how to balance the number of queues and interrupts of the ENIs on each node to ensure minimal competition.
For more information, check the Terway documentation and code.
Exclusive ENI Mode
This mode is implemented in CNI:
This method has the following features and advantages:
This mode is implemented in CNI:
This method has the following features and advantages:
By default, Kubernetes Service implements kube-proxy and uses iptables to configure Service IP addresses and load balancing, as shown in the following figure.
Kubernetes NetworkPolicy controls whether to allow communication between pods. Currently, mainstream NetworkPolicy components are implemented based on iptables, which also have iptables scalability issues.
eBPF is described as:
As shown in the preceding figure, if you use the tc tool to inject the eBPF program to the ENI of a pod, Service and NetworkPolicy can be solved in the ENI. Then, network requests are sent to the ENI. This greatly reduces network complexity.
Note: We use Cilium as the BPF agent on nodes to configure the BPF rules for pod ENIs. For more information about Terway-related adaptation, please visit this website.
A Kubernetes pod must perform many searches when it resolves the DNS domain name. As shown in the preceding figure, when the pod requests aliyun.com, it will resolve the following DNS configuration in sequence:
CoreDNS is centrally deployed on a node. When a pod accesses CoreDNS, the resolution link is too long and the UDP is used. As a result, the failure rate is high.
Change a client-side search to a server-side search.
When the pod sends a request to CoreDNS to resolve the domain name:
This is how Alibaba Cloud designs and builds high-performance, cloud-native pod networks. With cloud-native development, more types of application loads will run on Kubernetes and more cloud services will be integrated into pod scenarios. We believe that more functions and application scenarios will be incubated in high-performance, cloud-native pod networks in the future.
Those who are interested in this topic are welcome to join us.
Q1: Is the veth pair used to connect the pod network namespace and the host when a pod network namespace is created?
A1:
Q2: How are the security audits performed when a pod’s IP address is not fixed?
A2:
Q3: Does IPVLAN have high requirements for the kernel?
A3: Yes. On Alibaba Cloud, we can use aliyunlinux2 kernel 4.19. In earlier kernels versions, Terway also supports the veth pair + policy routing method to share secondary IP addresses on the ENI. However, the performance may be low.
Q4: Will the pod startup speed be affected if eBPF is started in a pod? How long does it take to deploy eBPF in a pod?
A4: The eBPF program code is not too large. Currently, it increases the overall deployment time by several hundred milliseconds.
Q5: Is IPv6 supported? What are its implementation problems? Is there kernel or kube-proxy code issues?
A5: Currently, IPv6 addresses can be exposed through LoadBalancer. However, IPv6 addresses are converted to IPv4 addresses in LoadBalancer. Currently, pods do not support IPv6 addresses. IPv6 addresses are supported starting for kube-proxy starting from Kubernetes 1.16. We are actively tracking this issue and plan to implement native pod IPv4/IPv6 dual stack together with Alibaba Cloud IaaS this year.
Q6: The source IP address is used to obtain the accessed service during each CoreDNS resolution request. Is the Kubernetes API called to obtain the accessed service? Will this increase pressure on the API?
A6: No. The preceding shows the structure. CoreDNS AutoPath listens to pod and service changes from the API server using the watch&list mechanism and then updates the local cache.
Q7: Are Kubernetes Service requests sent to a group of pods in polling mode? Are the probabilities of requests to each pod the same?
A7: Yes. The probabilities are the same. It is similar to the round robin algorithm used in load balancing.
Q8: IPVLAN and eBPF seem to be supported only by later kernel versions. Do they have requirements for the host kernel?
A8: Yes. We can use aliyunlinux2 kernel 4.19 on Alibaba Cloud. In earlier kernel versions, Terway also supports the veth pair + policy routing method to share secondary IP addresses on the ENI. However, the performance may be low.
Q9: How does Cilium manage or assign IP addresses? Do other CNI plugins manage the IP address pool?
A9: Cilium has two ways to assign IP addresses. host-local: Each node is segmented and then assigned sequentially. CRD-backend: The IPAM plugin can assign IP addresses. Cilium in Terway only performed network policies and service hijacking and load balancing and did not assign or configure IP addresses.
Q10: Does Cilium inject BPF into the veth of the host instead of veth of the pod? Have you made any changes?
A10: Yes. Cilium modifies the peer veth of the pod. After testing, we found that the performance of IPVLAN is better than veth. Terway uses IPVLAN for a high-performance network without the peer veth. For more information about our modifications for adaptation, please visit this website. In addition, Terway uses Cilium only for NetworkPolicy and Service hijacking and load balancing.
Q11: How does a pod access the cluster IP address of a service after the Terway plugin is used?
A11: The eBPF program embedded into the pod ENI is used to load the service IP address to the backend pod.
Q12: Can you talk about Alibaba Cloud’s plans for service mesh?
A12: Alibaba Cloud provides the Alibaba Cloud Service Mesh (ASM) product. Subsequent developments will focus on ease-of-use, performance, and global cross-region integrated cloud, edge, and terminal connections.
Q13: Will the ARP cache be affected if a pod IP address is reused after the node’s network is connected? Will IP address conflicts exist if a node-level fault occurs?
A13: First, the cloud network does not have the ARP problem. Generally, layer-3 forwarding is adopted at the IaaS layer. The ARP problem does not exist even if IPVLAN is used locally. If Macvlan is used, the ARP cache is affected. Generally, macnat can be used (both ebtables and eBPF can be implemented.) Whether an IP conflict occurs depends on the IP management policy. In the current Terway solution, IPAM directly calls IaaS IPAM, which does not have this problem. If you build the pod network offline yourself, consider the DHCP strategy or static IP address assignment to avoid this problem.
Q14: After the link is simplified using eBPF, the performance is improved significantly (by 31%) compared to iptables and (by 62%) compared to IPVS. Why is the performance improvement relative to IPVS more significant? What if it is mainly for linear matching and update optimization for iptables?
A14: The comparison here is based on one service and concerns the impact of link simplification. In iptables mode, NAT tables are used for DNAT and only the forward process is involved. In IPVS mode, input and output are involved. Therefore, iptables may be better in the case of a single service. However, the linear matching of iptables causes significant performance deterioration when many services are involved. For example, IPVS is better when there are 5,000 services.
Q15: If I do not use this network solution and think that large-scale service use will affect performance, are there any other good solutions?
A15: The performance loss in kube-proxy IPVS mode is minimal in large-scale service scenarios. However, a very long link is introduced, so the latency will increase a little.
Xiheng is an Alibaba Cloud Technical Expert and maintainer of the Alibaba Cloud open-source CNI plugin Terway project. He is responsible for Container Service for Kubernetes (ACK) network design and R&D.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
2 
2 claps
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/exciting-news-openyurt-alibabas-first-open-source-cloud-native-project-for-edge-computing-e664bae78520?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 2, 2020·6 min read
By Guo Fei (Senior Technical Expert), Xiyuan (Senior Technical Expert), and Xinsheng (Technical Expert) from Alibaba Cloud
On May 29, Beijing time, during the first anniversary of ACK@Edge, the Edge Cluster Management Service of Alibaba Cloud Container Service for Kubernetes (ACK), Alibaba Group open-sourced its core capabilities of ACK@Edge and shared the cloud-native edge computing project, OpenYurt, to the community.
Edge computing is a form of cloud computing. It is built on edge infrastructure and is becoming a new focus in the industry. OpenYurt, Alibaba’s first open-source, cloud-native project for edge computing, brings together the deep technical accumulation from many edge computing teams in Alibaba Group and explores demands on cloud-native implementation of edge computing.
Since 2018, OpenYurt has become the core framework of ACK@Edge and has been applied to the Content Delivery Network (CDN), ApsaraVideo Live, IoT Platform, Logistics, Industrial Brain, and City Brain. It is also used in multiple Alibaba Cloud businesses or projects, such as Link Edge, Hema Fresh, YOUKU, and ApsaraVideo (ApsaraVideo VOD, ApsaraVideo Live, Real-Time Communication, Video Surveillance System, and IntelligentVision.)
Li Xiang, Head of Open-Source at Alibaba Cloud-Native and Senior Technical Expert at Cloud-Native Application Platforms, said, “As the number of scenarios and demands for edge computing continues to increase, technologies, such as ‘cloud-edge collaboration’ and ‘edge cloud-native’ are gradually becoming the new focuses. The OpenYurt open-source project practices the concept of ‘cloud-edge integration.’ It relies on the powerful container orchestration and scheduling capabilities of native Kubernetes to implement a full edge computing cloud-native infrastructure that helps developers easily deliver, operate and maintain, and control large-scale applications on massive edge and device resources. We hope the open-sourcing of OpenYurt will promote the collaborative development of the community in the cross-cutting areas of cloud-native and edge computing.”
A yurt is a circular tent consisting of a framework of poles covered with felt or skins that are used by Mongolians. The choice of OpenYurt as the open-source project name is expressing the “shape” of its edge computing, which focuses on creating a centrally managed but physically distributed infrastructure and supports automatic or autonomous operations.
OpenYurt focuses on cloud-edge integration. Backed by the powerful container orchestration and the scheduling capabilities of native Kubernetes and cumulative experiences in many edge computing scenarios, OpenYurt developed a zero-invasion edge cloud solution for native Kubernetes. This solution provides capabilities and features, such as edge autonomy, efficient O&M tunnels, edge unit management, edge traffic topology management, sandboxed containers, edge serverless and FaaS, and heterogeneous resource support. OpenYurt can help users solve the problem of delivery, O&M, and control of large-scale applications on massive edge and device resources. It also provides a deployment tunnel for core services to seamlessly integrate with edge computing applications.
Two years ago, with the development of the industry at that time, edge computing was becoming the new focus of cloud computing. However, the increasing scale and complexity have put forward higher requirements for the capabilities of edge computing, such as efficiency, reliability, and resource utilization. From the end of 2017, the Alibaba Cloud IoT Platform and CDN services, as typical edge computing businesses, were facing three major difficulties: explosive growth in service scale, sharp increases in O&M complexity, and low O&M efficiency. Therefore, the introduction of cloud-native and the comprehensive transformation of the O&M mode of edge applications have become an urgent problem to resolve.
In this context, OpenYurt was born in the Alibaba Cloud Container Service team. Over the next two years, as the core framework of ACK@Edge, OpenYurt has been applied to the Content Delivery Network (CDN), ApsaraVideo Live, IoT Platform, Logistics, Industrial Brain, and City Brain. It is also used in multiple Alibaba Cloud businesses and projects, such as Link Edge, Hema Fresh, YOUKU, and ApsaraVideo (ApsaraVideo VOD, ApsaraVideo Live, Real-Time Communication, Video Surveillance System, and IntelligentVision.)
OpenYurt follows the currently popular edge application management architecture of “central management and edge autonomy” and targets “cloud-edge-device integration and collaboration,” enabling cloud-native to expand to the edge. In terms of technical implementation, OpenYurt implements the core design concept of “Extending your native Kubernetes to Edge”. Its technical solution has the following characteristics:
With the preceding technical features, OpenYurt provides users with the following features:
OpenYurt has the following core open-source capabilities:
More advanced functions, such as edge traffic management, edge unit management, deployment, and regional autonomy, will gradually become open-source.
As the open-source version of ACK@Edge, OpenYurt will adopt the full-open-source community development model, and a new version will be released quarterly. This will include community upstream security, key bug fixes, new features, and new capabilities. Alibaba Cloud will gradually make the complete capabilities of OpenYurt open-source. OpenYurt 1.0 is expected to officially launch by the first quarter of 2021. The general roadmap is listed below:
The Alibaba Cloud-Native Application Platform Team that leads the OpenYurt project has already made significant projects open-source, such as OAM, OpenKruise, Dragonfly, Apache RocketMQ, and Apache Dubbo. The team is the most senior cloud-native, open-source contribution team in China. Based on the open-source design concept of “Extending your native Kubernetes to Edge,” the open-sourcing of OpenYurt has made great progress in the ecological construction and popularization of cloud-native technology in the field of edge computing. It also contributed to developers around the world in expanding cloud-native boundaries.
You can visit this link to find the open-source OpenYurt project.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GiantSwarm/the-giant-swarm-managed-cloud-native-stack-46d171ec2e24?source=search_post---------247,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Jul 3, 2018·5 min read
A lot of us at Giant Swarm were at KubeCon in Copenhagen back in May. As well as being 3 times the size of the previous edition in Berlin the atmosphere felt very different. Far more enterprises were present and it felt like Kubernetes has now gone mainstream.
As strong supporters of Kubernetes, it being the most widely deployed container orchestrator makes us happy. However, this poses the same question that James Governor from RedMonk wrote about Kubernetes won — so now what?
Part of this growth is that there are now a wide range of Cloud Native tools that provide rich functionality to help users develop and operate their applications. We already run many of these CNCF projects such as Prometheus, Helm, and CoreDNS as part of our stack. Rather than install and manage these tools themselves our customers want us to provide them, too.
At Giant Swarm our vision is to offer a wide range of managed services running on top of Kubernetes, as well as managed Kubernetes. These services will help our customers manage their applications, which is what they actually care about. We call this the Managed Cloud Native Stack and we’ll be launching it at the end of summer. This post is to give you a preview of what’s coming.
We’re expanding our focus to provide a managed Cloud Native Stack but managed Kubernetes will remain an essential part of our product. Not least because all our managed services will be running on Kubernetes. We continue to extend our Kubernetes offering and have recently linked up with Microsoft as Azure Partners. So we now have Azure support to add to AWS and KVM for running on-premise.
Our customers each have their own Giant Swarm Installation. This is an independent control plane that lets them create as many tenant clusters as they require. This gives flexibility as they can have development, staging, and production clusters. Teams new to Kubernetes can have their own clusters and later these workloads can be consolidated onto larger shared clusters to improve resource utilisation and reduce costs. A big benefit is, there are no pet clusters. All these clusters are vanilla Kubernetes running fixed versions. They are also all upgradeable.
This ability to easily create clusters also means we manage 1 to 2 orders of magnitude more clusters than a typical in-house team. When we find a problem it is identified and fixed for all our customers. This means in many cases that our customers never even see a problem because it was fixed for another customer already.
Each tenant cluster is managed 24/7 by our operations team. To do this a key part of our stack is Prometheus which we use for monitoring and alerting. Prometheus will also be in the first set of managed services we provide. It will be easy to install Prometheus and it will also be managed by us 24/7. Using our operational knowledge of running Prometheus in production at scale.
Helm is a key part of our stack. As its templating support makes it easy to manage the large number of YAML files needed to deploy complex applications on Kubernetes. Helm charts are also the most popular packaging format for Kubernetes. There are alternatives like ksonnet, but Helm is the tool we use at Giant Swarm.
We use automation wherever possible in our stack. A lot of this automation uses the Operator pattern originally proposed by CoreOS. This consists of a CRD (Custom Resource Definition) that extends the Kubernetes API and a custom controller which we develop in Go using our OperatorKit library.
To enable the Managed Cloud Native Stack we’ve developed chart-operator. This automates the deployment of Helm charts in our tenant clusters. We use Quay.io as our registry for container images and also for charts using their Application Registry. This approach lets us do continuous deployment of cluster components including managed services across our entire estate of tenant clusters.
chart-operator comes with support for release channels. As well as providing stable channels for production there can also be alpha and beta channels. This lets users try out new features on development clusters. It also lets us have incubation charts for new tools. This is important because of the pace of development of cloud native tools under the umbrella of the CNCF.
We’re already using chart-operator to manage cluster components like node-exporter and kube-state-metrics that support our Prometheus monitoring. We also use it for the Nginx Ingress Controller which we pre-install in all tenant clusters. With managed services, this will become an optional component but also configurable. For example, customers will be able to install separate public and private Ingress Controllers.
The charts we use are production grade based on our experience of operating these tools. This means they often have features not present in the community Helm charts. The charts are also designed to work with our monitoring stack. So for example, if an Ingress Controller breaks in the middle of the night our operations team will be alerted and resolve the problem.
As part of the Managed Cloud Native Stack we’re adding a Service Catalogue to our web UI, API, and gsctl our command line tool. This easily shows which components are running and lets users select which tools from the Cloud Native Stack they wish to install.
In the screenshot above you can see a typical cluster. The management and ingress components are all being managed by chart-operator. Additional tools can be selected from the service catalogue shown at the beginning of the post.
In the screenshot above you can see how components can be configured. In this case for the Istio service mesh, you can decide whether to inject the Istio sidecar pod into your applications.
We think as the Kubernetes and Cloud Native ecosystems continue to evolve, providing a wider set of services is essential. These services will be pre-installed or available later in the clusters we manage and also be managed by us. This helps our customers focus on running their applications and still take advantage of the rich functionality these tools provide. If this matches what you think a Managed Cloud Native Stack should provide, we’d like to hear about your use case. Request your free trial of the Giant Swarm Infrastructure here.
Written by Ross Fairbanks — Platform Developer @ Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
See all (60)
1 
1 clap
1 
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/next-gen-enterprise-security-based-on-cloud-native-technology-6a6c28a6011e?source=search_post---------248,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 31, 2019·4 min read
Relive the best moments of the Apsara Conference 2019 at https://www.alibabacloud.com/apsara-conference-2019.
“With development of the digital economy, an increasing number of enterprises are migrating their business to the cloud. With this move, enterprises are building next-generation enterprise-level security architecture designs based on cloud-native security technologies, upgrading from a flat architecture to a three-dimensional architecture. With this move, the benefits of cloud-native security technologies will be maximized,” said Xiao Li, Senior Director of Cloud Computing Security Department, Alibaba Cloud Intelligence, at the Cloud Security Summit on September 27, 2019 during the Apsara Conference.
During the summit, Xiao Li stressed that cloud-native security technologies will be embedded in every module of the enterprise security architecture, which will help to improve the overall security of the system.
Pictured above is Xiao Li, Senior Director of Cloud Computing Security Department.
At the summit, Xiao Li released the Alibaba Cloud Security White Paper 4.0, in which the core capabilities required for the next-generation enterprise security architecture designs from the perspective of five horizontal systems and two vertical systems were described.
From the horizontal perspective, users need to build five systems. These five systems form a bottom-up security architecture that is tailored towards users’ business requirements. The security of the Cloud platform serves as the base layer, upon which the basic security, data security, application security, and business security layers on the user side are stacked on top of each other. Cloud-native security technologies can be implemented in each module of this architecture in the form of product functions or security services, allowing enterprises to enjoy various advantages, such as higher performance, stronger scalability, and more intelligent defense, of cloud-native security technologies.
From the vertical perspective, users need to build two systems: account security and operations security systems. These two systems cover all aspects and nodes. In this case, enterprises can thoroughly transform the flat security architecture to a three-dimensional security architecture. The vertical systems are essential for the cloud security architectures of all enterprises.
Next-generation enterprise security architecture
In the whitepaper, Xiao Li demonstrated the cloud security best practices provided by Alibaba Cloud to users. His intention is to help enterprises build systems of higher-level security. These best practices include unified account authentication, network access control, data security, threat detection and response, and global security defense.
The growth of cloud-native security brings about boundless benefits to the public and enterprise users alike, making the “unified” security model an inescapable trend. In response to this trend, Alibaba Cloud has applied cloud-native security technologies in their products and services that are provided to cloud-based enterprises, and recommends such best practices to the industry.
In addition to introducing the next-generation enterprise security architecture, Xiao Li also introduced three flagship products based on cloud-native security technologies.
The first is the hybrid cloud security solution. This solution is based on cloud-native security capabilities such as cloud threat intelligence, big data analytics, and automated operations and maintenance. The solution effectively solves the operational challenges such as split security management, different underlying architectures, and inconsistent security tools in a hybrid cloud environment. It helps enterprises implement consistent security policies through one console. At present, it has been applied in fields such as government, tobacco, technology, and finance, and several best practices have been generated.
The second is a trusted solution for cloud platforms. This solution is applicable to both public and private clouds. Alibaba Cloud ensures that the key code and data of business systems and applications on the cloud cannot be tampered with, ensuring the systems and applications are running in a trusted environment. This way, cloud security is ensured at source. The solution has been applied in public clouds.
Data security is the core competency of cloud platforms. The protection of customer privacy has always been the top priority of Alibaba Cloud. The Internal Operation Transparency service was launched during the summit. This service enables users to view operation-related logs in a console. Internal Operation Transparency is currently available to Alibaba Cloud customers in China on an invitational basis. In addition, reliable end-to-end data encryption is provided, and the key is owned and completely controllable only by users, helping users to build a full stack data protection system on the cloud.
“In the future, security management and operations will no longer be burdens for enterprises, but rather capabilities built into infrastructure. Alibaba Cloud will join hands with its ecosystem partners to create a more complete security plan for end users so to bring higher level security capabilities to every cloud-based enterprise”, said Xiao Li, in closing the speech.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-benefits-of-cloud-native-for-your-business-f329d3aa596c?source=search_post---------249,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 2, 2021·4 min read
To learn more about the full range of benefits of a cloud-native architecture for your business, download The Cloud-Native Architecture White Paper today.
The cloud is an established tool for many businesses. However, if your applications go to the cloud without transforming their architecture, your organization may not reap the full range of rewards the cloud can offer.
This is where a cloud-native architecture can help your enterprise make better use of cloud computing to deliver increasingly agile, affordable, scalable and flexible services.
Cloud-native is, essentially, the next evolution of cloud computing where every digital solution is designed and developed to deliver the ultimate agility and elasticity for your business. Many applications are on the cloud from the very beginning of their development and, within a cloud-native architecture, the cloud effectively reshapes your software across its lifecycle, from design to development, construction, and delivery.
A cloud-native architecture uses containers, microservices, development and operations DevOps tools, and many third-party components to optimize your development, testing and O&M efforts. This reduces the complexity of managing distributed systems and accelerates your business iteration, freeing up your developers to focus on value-added tasks.
Let’s look an example a cloud-native architecture, which was used for a global e-commerce enterprise. This environment relies on our ApsaraDB for Redis solution, including cloud-native services such as our Container Service for Kubernetes (ACK) solution, our Spring Cloud microservice framework, Performance Testing (PTS), our Application High Availability Service (AHAS), and Tracing Analysis solution. It containerizes applications and optimizes the development process that includes testing, capacity evaluation, and scaling to improve the production and research efficiency and realize the following benefits.
The core applications included in an example cloud-native architecture
A cloud-native architecture can provide high availability in both your primary/secondary and cluster modes, and cold backup, with a switchover of a few seconds and providing services that feature more than 99.999% availability.
Within a cloud-native architecture, AHAS features throttling, degradation, and system protection to protect your company’s vital system resources and control resource usage across the whole system. Even during periods of peak demand, this ensures your system stability and provides a consistently high end user experience.
Our Application Real-Time Monitoring Service (ARMS) can evaluate both the capacity of a single machine and the overall capacity of your system. As a result, you can make accurate resource plans and cost estimates for both your future promotions and ongoing business, helping you autonomously scale with confidence.
If your organization holds regular promotions, this can cause peaks in demand across your network. Our cloud-native architecture recently helped the Double 11 2020 Shopping Festival reach a total transaction amount of 498.2 billion RMB in just 24 hours, handling 583,000 orders per second at its peak.
When working with similar organizations, the Alibaba Cloud Service Team conducts multiple tests and establishes standard procedures and emergency plans to ensure a company’s ongoing and continuous service stability.
Alibaba Cloud provides businesses with either a pay-as-you-go or subscription payment model. If you decide to subscribe, then you can realize economies of scale and, if you choose pay-as-you-go, then you do not need to pay a large amount of money in advance for your servers and racks.
Either way, you also reduce your O&M costs as you no longer have any on-premise hardware costs. A selection of our cloud-based services are also available for free through the Alibaba Cloud Free Trial
A cloud-native architecture also centralizes your O&M, using a continuous integration process that involves initializing projects based on business requirements, pulling code from a specified branch to develop applications, performing regression testing in the test environment, verifying applications in the staging environment, and publishing applications.
To troubleshoot issues, for example, your engineers can also find applications and retrieve the exception logs in our Log Service console with ease. As a result, your engineers do not need to physically log on to a machine to locate issues. Our Tracing Analysis service can also autonomously identify performance bottlenecks and Application Real-time Management Service (ARMS) can comprehensively monitor your applications.
Automation can help further reduce the complexity and scale of your software technology across your cloud-native architecture. For example, our Infrastructure as Code (IaC), GitOps, an Open Application Model (OAM), Kubernetes operators, and various automated delivery tools can be used to implement Continuous Integration (CI) and Continuous Deployment (CD)
Alibaba Cloud works with a range of organizations, working across multiple industries and geographical locations to deliver the right cloud-native architecture to optimize their business. We can help you set-up, deploy and manage a cloud-native architecture to innovate and optimize your business. To find out more download The Cloud-Native Architecture White Paper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-prometheus-solution-high-performance-high-availability-and-zero-o-m-77f64708eca?source=search_post---------250,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 7, 2021·11 min read
By Yuanyi
Alibaba Cloud Log Service (SLS) strives to develop itself into a DevOps data mid-end that provides rich capabilities including host data access, storage, analysis, and visualization. This article describes how SLS supports the Prometheus solution to provide a cloud-native Prometheus engine that features high performance, high availability, and zero O&M.
Cloud-native technologies have been booming and flourishing across the world in recent years, and Cloud Native Computing Foundation (CNCF), one of the most influential projects in the IT field, is the strong support behind cloud-native technologies. As a non-profit organization under Linux Foundation, CNCF manages a dozen projects related to cloud-native technologies, among which the best known is Kubernetes, the de-facto standard in the container orchestration field.
Prometheus is the second CNCF graduated project, and has become the most popular one apart from Kubernetes. It is no exaggeration to say that Prometheus has become a de-facto standard of cloud-native monitoring. If the first step of enabling cloud native is to build a Kubernetes environment, then Prometheus is the first step to implement cloud-native monitoring.
After you deploy apps in Kubernetes, you will find it necessary to check the running statuses of the cluster and the apps. However, some of the monitoring methods in the virtual machine (VM) environment are no longer applicable. Although there are several alternatives to Prometheus, it is the best choice for many applications due to these advantages:
When we first move an app and related monitoring methods from a test environment to an online cluster, everything goes smoothly, the app runs properly and the monitoring metrics looking normal. However, when more and more apps are deployed in the production environment and the access pressure gradually increases, we will gradually realize some of the pain points of Prometheus:
Alibaba Cloud Log Service (SLS) strives to develop itself into a DevOps data mid-end that provides rich capabilities such as host data access, storage, analysis, and visualization. It provides an all-in-one platform where you can easily handle data-related tasks in DevOps scenarios and quickly build your enterprise’s observable platform.
SLS provides a wide range of data access methods and supports many data access approaches related to cloud-native observability. The preceding figure shows the projects that are supported by SLS for data access in the CNCF landscape. The monitoring, logging, and tracing features all support CNCF graduated projects, such as Prometheus, Fluentd, and Jaeger. The main reasons for using SLS to store Prometheus monitoring data include:
The SLS MetricStore provides native support for PromQL. All data is distributed to multiple hosts for distributed storage as shards. The computing layer integrates a Prometheus QueryEngine module to separate storage and computing, so that massive data processing can be carried out easily.
Compared with the community-provided Prometheus distributed extensions, such as Cortex, Thanos, M3DB, FiloDB, and VictoriaMetrics, the SLS’s distributed implementation solution is closer to the community’s goal of solving the restrictions on the use of native Prometheus.
In addition to supporting these requirements of the community, SLS can provide the following advantages for Prometheus:
As cloud-native monitoring software, Prometheus provides sound native support for Kubernetes. In Kubernetes, almost all components provide Prometheus metrics interfaces. Therefore, Prometheus has become a de-facto Kubernetes’ monitoring implementation standard. The next section describes how to deploy Prometheus monitoring for Kubernetes and how to use SLS MetricStore as the storage backend.
We recommend that you register a cluster to connect an independently built Kubernetes to Alibaba Cloud. After the registration is complete, you can follow the Alibaba Cloud Kubernetes installation procedure to install the cluster.
If you opt for other connection approaches, see the official instructions of Helm package for installation. Before the installation, you need to create a secret and change the default configuration. For more information, see the following description of installing Alibaba Cloud Kubernetes.
If you use Alibaba Cloud Kubernetes, you can install and configure Prometheus in the app directory to store data to SLS. The configuration procedure is as follows:
SLS provides three time-series data modes. SQL plays a dominant role in time series data queries, and SQL’s support for calling PromQL ensures both easier syntax and powerful functionality. In addition, SLS supports directly calling PromQL to support the open-source ecosystem, such as the integration with Grafana.
SLS supports the Prometheus remote write protocol for data writes in MetricsStore implementation and supports PromQL queries by calling Prometheus APIs. This enables Prometheus to act as a data source of Grafana, so that Prometheus can be compatible with open-source ecosystems. If your data is written by using Prometheus, SLS will be very suitable for your scenarios.
Prometheus MetricsStore reuses the underlying architecture of SLS, so it is designed to support SQL queries. For example, the long SQL statements in the preceding example are pure SQL queries. Nevertheless, pure SQL queries require a lot of optimization to handle time series data with ease, which is time-consuming. In view of this, SLS offers a third solution.
SLS encapsulates PromQL into several functions, which can serve as subqueries to support nesting complete SQL statements at the outer layer. The following shows an example.
Pure PromQL queries:
PromQL as subqueries:
Complicated SQL queries with PromQL as subqueries:
Currently, SLS supports the following frequently used APIs in PromQL: query(varchar), query_range(varchar, varchar?), labels(,label_values(varchar), and series(varchar).
Specifically, query_range also supports an automatic step when the second parameter is empty.
SLS provides multiple visualization features for time series scenarios by default, and supports analysis in the standard SQL and the PromQL + SQL modes. For more information about SLS visualization, see Log Service Visualization Dashboard.
In addition to native visualization features, SLS also supports access to time series data in Grafana by connecting SLS to Grafana as a Prometheus data source. In this way, SLS is compatible with all Prometheus dashboard templates.
Prometheus has no authentication mode. Unlike Prometheus, the Prometheus interface provided by SLS supports the HTTPS protocol and requires BasicAuth authentication, making data more secure.
Note: Make sure you are using HTTPS.
1. Add a data source, and select Prometheus.
2. Configure the URL.
Enter the aforementioned URL.
3. Enable Basic Auth, and enter the AK information.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sifoundry/driving-customer-success-through-cloud-native-enterprise-software-architecture-a-conversation-f3a877c4d0a5?source=search_post---------251,"Sign in
There are currently no responses for this story.
Be the first to respond.
Silicon Foundry
Jul 21, 2021·3 min read
HCL Software doesn’t just acquire enduring software products — it is on a mission to drive customer success through relentless innovation of its products, particularly in cloud native architecture.
Since its inception five years ago, the software division of HCL Technologies has grown into a $1.1B business with over 4500 employees in 45 countries. It supports 30 product families across Customer Experience, Digital Solutions, Secure DevOps, and Security and Automation.
Silicon Foundry spoke to HCL Software’s Global Director of Strategy, Mark Edwards, about how the company invests in, develops, markets, and sells mission-critical enterprise software products in order to deliver value to its customers.
Let’s start with the HCL Software story. What do you do?
When we established HCL Software back in 2016, we aspired to build a world-class enterprise software business by driving customer success through product innovation. We started by assembling software products in our core focus areas. We’ve now made those products available as a cloud native platform where they can be easily deployed into public, private and hybrid clouds. This is available to over 20,000 of our customers, who have run these software products only on prem for many years.
The ability to develop, market, sell, and support over 30 product families requires a strong focus on technical expertise and operational discipline. That expertise and dedication underpins how we run our business and how we leverage the needs of our customers to deliver meaningful innovations that enable highly regulated industries.
Tell us about your products.
Our large-scale R&D team continues to advance HCL Software products and make them better with every single release, across all our core focus areas. Here’s a rundown of what we offer:
● HCL Customer Experience products deliver impactful end user experiences across all brand touch points and interactions.
● HCL Digital Solutions offerings deliver digital transformation and workplace solutions for connected enterprises.
● From Mainframe to Microservices, HCL Secure DevOps offerings provide the security, automation, visibility, governance, and control needed to deliver higher quality software, faster.
● HCL Security and Automation offerings provide capabilities to secure enterprise endpoints and applications and fully automate discovery, management, automation, and remediation, whether it’s on–prem or cloud, regardless of location or connectivity.
Tell us about HCL SoFy.
HCL (Solution Factory) SoFy, our crown jewel, is a cloud native platform that transformed over 50 traditional on-prem products to support the latest cloud standards. 2,000+ REST API endpoints make it easier for developers to extend, integrate, and customize enterprise systems. We also created self-service, cloud delivered demos and trials to provide our customers with fast hands-on experiences. SoFy transforms the way our customers think about their software and their perception of the software vendor that truly helps them achieve success.
What’s next for HCL Software?
Our customers, partners, and people will always inspire our drive to excel at what we do, and how we do it. We understand that delivering on our mission is a journey that requires strategic investments, product harmonization strategies, and enhanced cloud native offerings. We continue to organically grow and explore portfolio expansion and software partnership opportunities that empower global businesses and organizations to create tangible long-term value for everyone.
For more information about HCL Software, please visit: https://www.hcltechsw.com/cloud-native
Silicon Foundry is an innovation advisory platform that builds bridges between leading multi-national corporations and global startup ecosystems.
8 
8 
8 
Silicon Foundry is an innovation advisory platform that builds bridges between leading multi-national corporations and global startup ecosystems.
"
https://medium.com/@alibaba-cloud/embarking-into-the-cloud-native-era-9695b043219b?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 29, 2019·2 min read
Watch Alibaba Cloud’s webinar on Bringing a Microservice App to Managed Kubernetes in 10 Minutes to learn more about setting up Kubernetes clusters on Alibaba Cloud with Container Service for Kubernetes.
Cloud computing has forever changed the IT landscape of the world, with the world’s enterprise and institutions all migrating to the cloud. One important development in Cloud is that of Cloud-native applications, which can fully utilize the essential advantages of cloud computing. Alibaba Cloud, being at the cutting-edge of this technology, has embarked into this exciting new frontier in the cloud computing space from the very beginning.
Over the past few decades, enterprise IT architectures have gone through three major stages: standalone, distributed, and cloud computing. Within a decade of the birth of cloud computing, most Internet companies have launched their applications in the cloud. Today, enterprises and institutions in the financial, manufacturing, medical, as well as governmental sectors are gradually migrating to the cloud.
The cloud migration of enterprises is not only about upgrading their infrastructure and platforms, but also about shedding the traditional approaches of designing applications. Cloud-native applications must be designed based on the characteristics of the cloud, from their architecture and development, to their deployment and maintenance. Cloud-oriented design must be applied throughout the entire lifecycle of applications. Only in this way will applications run efficiently in the cloud and fully utilize the advantages brought on by the distributed architecture and elasticity of the cloud.
Cloud native is a set of groundbreaking ideas that stimulate the development of brand-new technologies, such as containers, microservices, and service meshes.
Alibaba Cloud is the first technology company in China to deploy cloud-native technologies. Alibaba Cloud attended 2019 KubeCon + CloudNativeCon + Open Source Summit (June 24 to June 26) and shared its cutting-edge technologies and applications based on the cloud native initiative with users worldwide. During the event, Alibaba Cloud also launched products and services such as ACK Edge Kubernetes and a cloud-native application management and delivery system.
If you don’t have an Alibaba Cloud account, sign up for a New User Free Trial and get $300–1200 USD worth in free trial products.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/unlock-cloud-native-ai-skills-develop-your-machine-learning-workflow-e493cc6613a6?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 29, 2020·10 min read
By Che Yang, nicknamed Biran at Alibaba.
In the previous article in this series, Unlock Cloud-native AI Skills | Build a Machine Learning System on Kubernetes, we set up a Kubeflow Pipelines platform. Now, we can try it out with a real case. Let’s learn how to develop a Kubeflow Pipelines-based machine learning workflow.
A machine learning workflow is a task-driven and data-driven process. In this process, we import and prepare data, export and evaluate model training checkpoints, and export the final model. To do this, we need to use a distributed storage system as the transmission medium. In this example, we use a network-attached storage (NAS) as the distributed storage. To do this follow these steps:
Note that, in this tutorial, we use a NAS as the distributed storage, and we need to replace NFS_SERVER_IP with the real NAS server address.
1. Create an Alibaba Cloud NAS service. For more information, click Here.
2. Create /data in the network file system (NFS) server.
3. Create a corresponding persistent volume.
4. Create a persistent volume claim (PVC).
The examples provided on Kubeflow Pipelines depend on Google’s storage service. As Google is inaccessible in China, users in China cannot truly experience the capabilities of Kubeflow Pipelines. For this reason, an example of training the Modified National Institute of Standards and Technology (MNIST) model based on Alibaba Cloud NAS was provided to help you get started with and learn about Kubeflow Pipelines on Alibaba Cloud. The example includes the following steps:
Each of the three steps depends on the previous step.
You can use Python code to describe this process on Kubeflow Pipelines. For the complete code, see standalone_pipeline.py.
In this example, we use arena_op, which is based on the Arena open-source project. This API is obtained by packaging default container_op in Kubeflow. It can seamlessly connect to the message passing interface (MPI) and parameter server (PS) modes of distributed training. In addition, it allows you to easily gain access to distributed storage by using heterogeneous devices such as a graphics processing unit (GPU) or remote direct memory access (RDMA). You can also conveniently synchronize code from Git sources. It is really a useful API tool.
Kubeflow Pipelines converts the preceding code into a directed acyclic graph (DAG). Each node in the DAG is a component, and the lines connecting the components represent the dependencies between them. You can see the DAG on the Pipelines UI.
First, let’s talk about data preparation. We have provided the Python API arena.standalone_job_op. Now we need to specify the following parameters: name (which is the name of this step), image (the container image to be used), and data (the data to be used and the directory to which the data is mounted within the container).
Here, data is in the array format. For example, data = [""user-susan:/training""] indicates multiple pieces of data that can be mounted, where user-susan is the previously created PVC, and /training is the directory to which the data is mounted within the container.
Actually, the preceding step uses curl to download the data from the specified address to the /training/dataset/mnist directory in the distributed storage. Note that /training is the root directory, which is similar to a root mounting point, and /training/dataset/mnist is a sub-directory of the distributed storage. In fact, in the following steps, the same root mounting point can be used to read data and perform operations.
The second step is to download code and train the model based on the data downloaded to the distributed storage. Before downloading the code, use Git to specify a fixed commit ID.
As you can see, this step is more complex than data preparation. In addition to specifying the name, image, data, and command parameters as in the first step, we need to specify the following parameters in this model training step:
After setting the data parameter, which is the same as the prepare_data parameter, to [""user-susan:/training""], you can read the corresponding data in the training code, for example, --data_dir /training/dataset/mnist.
This step depends on the prepare_data parameter. You can specify prepare_data.output to indicate the dependency between the two steps.
The export_model parameter is to generate a training model based on a checkpoint obtained through the train parameter.
The export_model parameter is similar to and simpler than the train parameter. It simply exports code from the Git synchronization model and then uses the checkpoints in the shared directory /training/output/mnist to export the model.
The entire workflow is much more intuitive. Now, let’s define a Python method to integrate the entire process:
The @dsl.pipeline parameter is a decorator that indicates the workflow. It defines two attributes: name and description.
The entry point method sample_pipeline defines four parameters: learning_rate, dropout, model_version, and commit. These parameters can be used at the preceding train and export_model stages. The parameter values are in the format of dsl.PipelineParam, so that they can be converted into input forms through the native UI of Kubeflow Pipelines. The keyword of an input form is the parameter's name, and the default value of the input form is the parameter's value. Note that the value of dsl.PipelineParam can only be a string or numerical value. Arrays, maps, and custom values cannot be converted through transformation.
In fact, you can overwrite these parameters when submitting a workflow. The following figure shows the UI where you can submit a workflow.
The Python domain-specific language (DSL) that you previously used to develop the workflow can be submitted to the Kubeflow Pipelines service in your own Kubernetes. Actually, it is easy to submit code.
First, run compiler.compile to compile the Python code into a DAG configuration file that can be identified by the execution engine Argo.
Second, on the Kubeflow Pipelines client, create an experiment or find an existing experiment and submit the previously compiled DAG configuration file.
Third, prepare a Python 3 environment in the cluster and install a Kubeflow Pipelines software development kit (SDK).
Fourth, log on to the Python 3 environment and run the following commands to successively submit two tasks with different parameters.
Log on to the Kubeflow Pipelines UI: [https://](){pipeline address}/pipeline/#/experiments. For example:
To compare metrics such as the input, duration, and accuracy of the two experiments, you can click the Compare runs button. Making an experiment traceable is the first step to make the experiment reproducible. Leveraging the experiment management capabilities of Kubeflow Pipelines is the first step to enable experiment reproducibility.
To implement a runnable Kubeflow Pipeline, do as follows:
The native dsl.container_ops API improves flexibility. It enables the interface that interacts with Pipelines. This allows users to do a lot of things through container_ops. However, it also has many drawbacks:
Alternatively, you can choose to use the reusable component API arena_op. The universal runtime code used by this API spares you from repeatedly constructing runtime code. In addition, the universal arena_op API is user-friendly and supports both PS and MPI scenarios. We recommend that you use this method to compile pipelines.
Are you eager to know the latest tech trends in Alibaba Cloud? Hear it from our top experts in our newly launched series, Tech Show!
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
6 
6 
6 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/cloud-native-is-here-but-not-evenly-distributed-ba20d84c4420?source=search_post---------254,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 9, 2021·18 min read
By Youjing
What is cloud-native? Everyone has his or her own interpretation of this term. Drawing on extensive discussions and practical experience from various projects, this article presents the interpretation of cloud-native technologies Alibaba’s delivery experts. It discusses how to build cloud-native applications, key cloud-native technologies, and ideas about cloud-native implementation.
The Internet has changed the way people live, work, study, and entertain themselves. The rapid development of technologies has driven the evolution of the cloud computing market from the early physical machines to virtual machines (Bare Metal Instance) and then to containers, while the Internet architecture evolved from centralized architectures to distributed architectures, and then to cloud-native architectures. Nowadays, the term “cloud-native” has been elevated by enterprises and developers to the status of an industry standard and the future of cloud computing. If I were to describe cloud-native technologies in one sentence, it would be “the future is here, but not evenly distributed.”
Cloud-native technologies (architectures) have seen a sharp increase in popularity, but the concept is still interpreted differently by different people, despite the wide-ranging articles and discussions on this topic in the online community and inside Alibaba. In my opinion, we are exploring what it means to be cloud native and trying to understand and put cloud-native technologies into practice. Therefore, there is still no clear or overarching standard definition.
A cloud migration project I took part in recently involved many cloud-native technologies. I would like to take this opportunity to share my insights while drawing on the discussions and practical experience from the project.
Before getting into this topic, let’s see how industry influencers define “cloud native,” namely Pivotal Software and CNCF.
Pivotal Software is a leader in the field of agile development (previously contracted with Google) and has an impressive pedigree (it was founded by EMC and VMware). It launched Pivotal Cloud Foundry (a big hit in the field of PaaS between 2011 and 2013) and the Spring Framework and is a pioneer in cloud-native technologies. The following figure shows how Pivotal defines “cloud native”:
Matt Stine at Pivotal Software first proposed the concept of cloud-native in 2013. In 2015, in his book “Migrating to Cloud-Native Application Architectures”, Matt Stine defined the key characteristics of cloud-native application architectures, including twelve-factor application, microservice architecture, self-service agile infrastructure, API-based collaboration, and antifragility. Matt Stine revised his definition in 2017 and indicated six characteristics of cloud-native architecture: modularity, observability, deployability, testability, handleability, and replaceability. The latest piece published on Pivotal Software’s official website characterizes cloud-native applications and services as an integration of the concepts of DevOps, continuous delivery, microservices, and containers.
Cloud Native Computing Foundation (CNCF) is a well-known organization in the industry. It is a foundation co-sponsored by leading open-source infrastructure companies such as Google and RedHat. The mission of CNCF was to compete in the container market dominated by the then-prominent platform Docker. Through the Kubernetes project, CNCF has maintained undisputed leadership in the field of orchestration in the open-source community and is the champion in defining and promoting cloud-native architectures. Here is how CNCF defines “cloud-native”:
In 2015, CNCF originally defined three characteristics of cloud-native architectures: containerized encapsulation, automated management, and microservices. In 2018, CNCF updated its definition of cloud-native architectures to include two new features, declarative API and service mesh (a new technology that emerged in the open-source community in 2017; it is a parallel technology to microservices). These technologies are used to build loosely coupled systems that are highly fault-tolerant and easy to manage and observe.
As the community continues to grow the ecosystem and push the boundary of cloud-native architectures, the definition of cloud native is constantly changing. Companies (like Pivotal and CNCF) define this concept differently, and one company may use different definitions at different times. Following Moore’s Law, we can expect the definition of cloud native to continue to shift in the future.
As for the two different definitions given by Pivotal and CNCF, I believe the distinction is caused by the respective organizational structures and perspectives adopted by the two industry influencers:
Pivotal Software is a pioneer in the concepts and methodologies of cloud-native architectures, while CNCF contributes to best practices.
However, it seems Pivotal Software advocates the concept of container technology, while CNCF implements its technology through the microservices content. So are they really all that different? We welcome you to tell us your opinion in the comment section below.
From the birth of the Internet to the present, we have adopted Internet thinking and then Internet+ thinking (which is essentially Internet native). When enterprises reach a certain stage, they need to develop value thinking (or, value-native thinking). Therefore, it is necessary for cloud computing practitioners to develop cloud-native thinking. Abstract paradigms always preceded tangible solutions in any technological reform or widespread adoption of new methods.
Drawing on the definitions given by Pivotal Software and CNCF, I came to the following understanding of what it means to be cloud-native:
Being cloud-native means building an application system that runs on the cloud through both a methodology (such as that from Pivotal Software) and a technical framework (such as that from CNCF). Such an application system breaks away from traditional system building methods and makes full use of the native capabilities of the cloud to maximize its value. It adopts the characteristics of cloud-native architectures in order to rapidly empower businesses.
This abstract interpretation can be broken down into four questions:
The emergence of cloud computing is closely related to the development and maturity of virtualization technology. It is an emerging IT infrastructure delivery method. that relies on virtualization technology to standardize, abstract, and scale IT hardware resources and software components into product-like services that allow users to “pay as they go”. In a sense, this reconstructs the IT industry’s supply chain. Its models of service delivery include Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Function as a Service (FaaS), and Data as a Service (DaaS):
IaaS
IaaS indicates the fundamental and underlying capabilities of cloud computing, such as computing, storage, network, and security.
PaaS
PaaS generally refers to the high-level domain- or scenario-oriented services that are built on top of the underlying cloud capabilities, such as cloud databases, cloud object storage, middleware (including caches, message queues, load balancing, service mesh, and container platforms), and application services.
Serverless
This is a serverless computing architecture, through which users can run applications without purchasing or concerning themselves with infrastructure and elastically scale services using the pay-as-you-go billing method. This is also an extreme form of evolution from PaaS. Currently, three types of solutions are available under this architecture:
DaaS
Using data as a service, the architecture extends to upper-layer applications and, when used with AI and cloud services, can deliver various high-value services. These services include big data-based decision making, video and facial recognition, deep learning, and scenario-based semantic understanding, among others. This is also the core strength of the cloud of the future.
As technologies and open-source solutions continue to develop and cloud service providers provide more products and capabilities, every layer of today’s technology architecture, from physical machines, virtual machines, and containers to middleware and then to the serverless architecture, has been gradually standardized. The more standardized the layers are, the greater the added value they can contribute. Relatively common technologies that are not directly related to business (such as service mesh) have also been standardized and incorporated into the underlying infrastructure. Every time a layer of the technology architecture becomes standardized, it will eliminate some of the inefficient and tedious tasks. In addition, the application layer provides emerging technologies, such as AI, to help enterprises reduce the costs incurred during the exploration of suitable solutions, speed up the verification and delivery of new technologies, and truly empower the business.
Meanwhile, users can choose the cloud products that best fit their needs just like building with LEGO blocks, using readily-available resources to avoid repetitive work. This greatly improves the efficiency in each stage of software and service development and accelerates the implementation of various applications and architectures. Users who are already on the cloud can realize huge cost-savings by consuming resources as needed and scaling out at any time.
The preceding section discusses the strong capabilities of the cloud. In comparison with traditional applications, new cloud applications, need to be adapted to these capabilities in each stage of the entire application lifecycle. This involved adaption during the design of software architecture, development, construction, deployment, delivery, monitoring, and O&M. I will discuss this process in terms of various issues users must face.
Great architectures come into being after evolving and progressing over time. They are not created all at once. Therefore, it is meaningless to talk about architectural design. The purpose of architectural evolution must be to solve a certain problem. We can address the problems listed below to better understand the design the cloud-native architecture:
Single-microservice applications are adapted to the cloud-native architecture due to their low complexity and comprehensive set of functions for monitoring, governance, deployment, and scheduling supported by the strong underlying system. However, from the perspective of the overall system, the complexity does not decrease. Instead, enterprises must bear the high costs of building a robust underlying system with strong architectural and O&M capabilities.
In addition, the technology stacks and middleware systems used by enterprises to achieve these functions are closed and highly private, making it difficult to meet all business needs (as is the case with Alibaba). Cloud hosting can reduce the overall complexity of such a project. The cloud service provider can take over the complex underlying system and provide attentive services. Projects will eventually evolve into an infrastructure-free design and use YAML or JSON declarative code to orchestrate the underlying infrastructure, middleware, and other resources. In this way, the cloud can meet every need of an application. Eventually, enterprises will embrace an open and standard cloud technology system.
We introduced DevOps to address the problem of the continuous delivery of applications.
DevOps is a concept everyone is familiar with. I see it as a series of values, principles, methods, practices, and tools designed to achieve fast delivery and continuous optimization. Its core advantage is to close the gap between R&D and O&M, expedite the software delivery process, and improve software quality. The chart below shows a DevOps pipeline:
The platforms involved in this process include: GitHub, Travis, Artifactory, Spinnaker, FIAAS, Kubernetes, Prometheus, Datadog, Sumology, and ELK.
The key to truly implementing and practicing DevOps lies in the answers to the following questions:
In essence, DevOps supports O&M services. By introducing a series of automation tools for new technologies and development into O&M, it brings development closer to the production environment and manages the entire development and O&M processes, ensuring freedom and innovation. When monitoring and fault prevention and control tools are used together with function switches, they can help reach achieve a balance between the user experience and fast delivery.
If technology professionals only need to consider business solutions and business code in the future, it would be necessary to quickly integrate the abundant technical products and cloud vendor platforms already available on the market. This would allow technical professionals to focus on finding solutions and connecting business and technology in a bid to satisfy increasingly diversified and complicated business needs. In terms of O&M, the cloud hides the complexity of the infrastructure and shifts to the O&M mid-end and large-scale O&M for toolchain development. This allows practitioners to focus on cost, efficiency, and stability while ensuring the steady progress of application development.
The earliest container, known as Chroot Jail, was developed in 1979. It was re-defined in 2008 as LXC (Linux Container) and combined the resource management of cgroups with the view isolation of namespace to achieve process-level isolation. However, the greatest innovation in container technology is the container image (or Docker container). This container contained the complete environment (the file system of the entire operating system) required to run an application. Additionally, it was consistent, lightweight, portable, and language independent. It allowed users to achieve “build once, run anywhere” (that is, in development, testing, and production environments) and completely standardize building, distribution, and delivery activities. It also supplies the foundation of immutable infrastructure.
Kubernetes is a Linux system for cloud computing and cloud-native architectures.
As Google’s open-source container orchestration and scheduling system built on Borg, Kubernetes makes it possible to use container applications in large-scale industrial production environments.
Relying on declarative APIs, scalable programming interfaces (by using CRD and controllers), and an advanced design philosophy, Kubernetes dominated the field of container orchestration (beating out Docker Swarm and Apache Mesos) and has become the de facto standard for container orchestration systems.
The Kubernetes platform frees users from resource management, further standardizes the infrastructure, reduces complexity, and improves resource utilization. In addition, Kubernetes reduces the cost of cross-data center deployment of hybrid clouds, multiple clouds, and edge clouds.
Service mesh aims to decouple the business logic from the non-business logic, allowing developers to focus solely on the business logic. The solution separates a number of client SDKs unrelated to the business logic (such as service discovery, routing, load balancing, and traffic shaping and degradation) from business applications and puts them into a separate proxy (Sidecar) process that is pushed down to the infrastructure middleware mesh (similar to the shift from TDDL to DRDS). With this solution, an application will face fewer risks from changes in the system framework, become more streamlined and lightweight, and enjoy a faster startup speed. This makes it easier to migrate the application to the serverless architecture. The meshes can implement automatic iteration and upgrade based on their own needs. This facilitates global service governance, phased release, and monitoring. In addition, the mesh boundary can be extended to the database mesh, cache mesh, and message mesh. In this way, service communication can be truly standardized by adopting the TCP/IP protocol for inter-service communication.
The infrastructure and its complete life cycle (creation, destruction, scaling, and replacement) are described in code and orchestrated, executed, and managed with appropriate tools, such as terraform, ROS, and CloudFormation. For example, users only need to define the code and then easily create all the basic resources needed by applications (such as Elastic Compute Service (ECS), Virtual Private Cloud (VPC), ApsaraDB for RDS, Server Load Balancer (SLB), and ApsaraDB for Redis), without the need to frequently switch between pages in the console to apply for and purchase resources. With this approach, the infrastructure code is version-controlled, reviewable, testable, and traceable and can be rolled back, maintain consistency, and prevent configuration drift. It is also easy to share, create templates for, and scale the infrastructure code. In addition to improvement in the overall O&M efficiency and quality, IaC allows users to easily see the full picture of the infrastructure.
The entire lifecycle of cloud-based IDE research provides a complete experience that integrates development, debugging, pre-release, production environment, and CI/CD release. The cloud platform also offers a variety of code library templates to improve the compilation speed through distributed computing and intelligently realize code recommendation and optimization, automatic bug scanning, and identification of logical and systematic risks. It is conceivable that the development models of the cloud era, completely different from those of the local development environment, will feature higher development efficiency, faster iteration speed, and better quality control.
As a member of the GTS delivery team that was tasked with empowering enterprises to succeed in their digital transformations, I have been thinking about the ways to help traditional enterprises transform themselves and embrace cloud-native architectures by drawing on the experience of the Internet industry. Here is a roadmap for the implementation of cloud-native architectures.
The Y-axis in this figure indicates business agility. To achieve cloud-native business agility, you need to:
The X-axis in the figure indicates business robustness. To achieve business robustness, you need to:
Cloud-native architectures seem to be extremely appealing, but once you go deep into the stage of implementation, you will find that they are very complicated. The complexity is not only reflected in the wide range of new concepts and technical features, but also in the huge gap between customers’ expectations and the value created by cloud-native technologies and the uncertainty about the future. In the future, I will continue to share and discuss my thoughts, experiences, and practices. This is the first of a series of articles. I hope my writings can contribute to the digital transformation of enterprises.
In the cloud era, we require novel thinking and concepts to properly understand application architectures and IT infrastructure in order to correctly answer the question “what does it mean to be cloud-native.” The future is undoubtedly cloud-native. Therefore, in addition to tools, enterprises seeking to transform themselves need a complete philosophy that progresses from concepts to methodologies and then to tools. Only in this way can we better embrace the arrival of the cloud era and maximize the value of cloud-native architectures.
This is the best of times for developers. This is the best of times for cloud vendors. In addition, this is the best of times for professionals specializing in cloud service delivery.
The future is here, but not evenly distributed. Let work together to understand, embrace, and deliver cloud native.
Disclaimer: The views expressed herein are for reference only and don’t necessarily represent the official views of Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-launches-the-cloud-native-devops-solution-entering-the-era-of-cloud-r-d-8f9a97319df2?source=search_post---------255,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 24, 2020·8 min read
By the Alibaba Apsara DevOps Team
On October 21, 2020, the Alibaba Cloud Apsara DevOps Team and the Cloud-Native Team held a cloud announcement conference to announce “the cloud-native DevOps solution.” This conference launched the cloud-native DevOps solution and its typical application scenarios based on Alibaba’s best R&D practices.
Zhang Yu, a Senior Solution Architect at Alibaba Cloud Intelligence, pointed out that we are in the era of cloud R&D, and summarized three “requirements” in the cloud R&D era. First, the IT infrastructure needs to be reliable, low-cost, and highly elastic. Second, the services provided to users need to be stable, secure, and high-performance. Third, software delivery must be continuous, fast, high-quality, and low-risk.
There is a Chinese saying, “ideas are very plump, but the reality is very bony.” Contrary to the preceding “ideal state,” software developers face many challenges in the cloud R&D era.
The first challenge is the increasing cost of the IT infrastructure. In the initial period of an enterprise, it may only take a few servers to meet its business requirements. However, as the business develops and the user scale expands, it may take several rows of cabinets or a data center to meet its requirements. Moreover, the growth rate of IT infrastructure cost is often greater than the growth rate of business scale, which makes developers feel that the cost of the infrastructure is getting higher.
The second challenge is that applications face the issues of “failed launches, frequent errors, and long duration.” The preceding figure shows the launch progress of Application A and Application B for six months. The vertical axis indicates the time for launching an application, and the horizontal axis indicates the launch date. Each green dot indicates a successful launch, and each red dot indicates a failed launch. We can see that Application A was launched 13 times in half a year, but seven times were HotFix launches with emergency bug fixes. Moreover, the launch duration varies significantly, from minutes to days. Application B was launched frequently, but the success rate was less than 30%. Each launch duration was longer than 24 hours, but in some cases, it was not launched for several days.
The third challenge is that less time is spent on the development of new functions. In the early stage of software development, almost all our manpower can be used for software development. However, with the richness of application functions, more manpower is used for the maintenance of existing functions, and there is little time for new function development.
How should we approach the cloud R&D era? We need cloud-native continuous delivery practices, including cloud-native infrastructures, end-to-end continuous delivery pipelines, premium quality protection, and a low cost, efficient service governance system.
In the 1950s and 1960s, the adoption of standardized containers established a set of standardized transportation systems. This effectively reduces the cost of transporting goods, and ultimately promotes economic globalization. Traditional “containers” used in shipping and virtual “containers” used in cloud-native technologies have something in common. Zhang Yu said, “The cloud-native infrastructure has two characteristics: “immutability” and “standardization.” The “immutability” can be used to eliminate uncertainty caused by inconsistency and reduce the risk of inconsistency. This can reduce the maintenance costs. The “standardization” can be used to simplify deployment and lower environment maintenance, toolchain development, and learning costs.”
A standard end-to-end continuous delivery pipeline consists of requirements analysis, code submission, construction, integration verification, pre-launch, and online. It needs to be describable, observable, and automated.
With the “cloud-native infrastructure” and “end-to-end continuous delivery pipeline” in place, the “premium quality protection” is also needed to improve the quality of software launches. Quality protection is a matter for all people in the development, testing, and operation and maintenance fields. Everyone is in the same boat. Working together and helping one another ensures the high-quality delivery of software.
Finally, we need a low-cost and efficient service governance system. When a microservice is launched, the service platform provides a series of service governance systems, including gateways, service monitoring, and auto scaling. This allows developers to focus solely on code development without worrying too much about service governance.
To help more enterprises and developers enjoy the R&D benefits of technology upgrading with high quality and low-cost, the Apsara DevOps Team worked with the Cloud-Native Team to build an all-in-one cloud-native DevOps solution. DevOps can easily cope with scenarios, such as general Kubernetes scenarios, Spring Cloud or Dubbo microservice scenarios, and lightweight Function Compute (FC) scenarios.
As shown in the preceding figure, the “Apsara DevOps kanban” is at the top left. Product managers can use it to manage requirements. After “requirements” are clarified and planned, they are split into “tasks” and allocated to a team or a developer for “execution.” During the development process, developers use the Apsara DevOps code management platform to create feature change branches. When the code is submitted, the feature branch listener will be triggered. During this period, Apsara DevOps automatically scans and reviews code and scans security. After the code is developed, developers can use the DevOps pipeline to perform operations, such as compilation, development verification, online review, production, and launch. The pipeline depends on multiple Alibaba Cloud services. For example, the pipeline relies on “mirror service” during compilation. In the development verification, production launch, and other links, it relies on Alibaba Cloud Container Service for Kubernetes (ACK) cluster services. After the application is officially launched, it depends on microservice governance services, including configuration center, service monitoring, and capacity adjustment. All of this information will eventually be fed back to developers through DingTalk and other means. When an issue occurs, it is reflected in the form of a “defect” in the Apsara DevOps kanban.
In summary, the DevOps cloud-native continuous delivery solution consists of four aspects: First, the cloud-native infrastructure supports ACK, Function Compute, and Serverless App Engine (SAE). Second, the end-to-end continuous delivery pipeline is realized through the kanban, code management platform, and pipeline of Apsara DevOps. Third, through the automatic scanning of its code management and the detection and verification of its pipeline, premium quality protection can be achieved. Fourth, the microservice governance of Alibaba Cloud has achieved a low-cost, high-quality service governance system.
The Apsara DevOps solution applies to three scenarios: the continuous delivery of Function Compute, continuous microservice delivery, and general-purpose cloud-native continuous delivery.
The continuous delivery of “Apsara DevOps + Function Compute” is more suitable for start-up teams with fewer developers. Since their businesses are often in the rapid verification and development stage, they hope their businesses can be launched and updated quickly, and they do not need to be concerned about their work outside the business.
This “continuous delivery scenario of Function Compute” has three advantages: First, developers can focus on business logic development. They do not need to pay attention to underlying details, resource conditions, service maintenance, and governance. Second, we need to pay for the resources that our services use. We need to reduce resource costs and bring our services online in a matter of minutes. Third, the entire R&D process is based on the Apsara DevOps platform, which provides automatic security guarding. The operating environment is stable, based on the infrastructure provided by Alibaba Cloud, and has undergone large-scale commercial practices. The function platform features high elasticity and can easily cope with sudden business traffic bursts.
For small and medium-sized developer teams that have adopted or are preparing to adopt microservice architectures, the continuous delivery method of “Apsara DevOps + SAE” is recommended. This kind of “continuous microservice delivery” has the following characteristics: First, SAE is deeply integrated with microservice frameworks, such as Spring Cloud and Dubbo, and the built-in micro service governance capability can effectively reduce the cost of using microservices. Second, the second-level elastic scaling capability enables fast service scaling and high elasticity. The service can cope with traffic bursts and ensure service stability. Third, the built-in microservice launch and O & M capabilities can effectively improve the efficiency of microservice testing, launch, and operation and maintenance.
Medium or large R&D teams, which have their own service governance systems and are expected to have sufficient R&D flexibility and enjoy the technical benefits of cloud-native and continuous delivery, can use the “general cloud-native continuous delivery” solution. What are the advantages of this delivery method? First, Apsara DevOps provides an all-in-one research and development process support from requirements to online operation and maintenance. Second, Apsara DevOps provides end-to-end security protection from the infrastructure to the DevOps toolchain. Third, it is deeply integrated with the Alibaba Cloud infrastructure and cloud services with the characteristics of no hosting and high performance. Meanwhile, the Alibaba Cloud infrastructure fully complies with the Kubernetes (k8s) open-source standards, so migration costs are saved.
Alibaba Cloud Apsara DevOps has served thousands of enterprises and millions of developers. It has helped many enterprises successfully complete the DevOps transformation, including ZhongAn Insurance, China Everbright Bank, Tianhong Asset Management, China Southern Airlines, SAIC-GM, Bank of Nanjing, Vanke, Cathay Insurance, Shanghai Boka, and Shijiazhuang Ai-mobile. With the launch of this cloud-native DevOps solution, Apsara DevOps hopes to help more enterprises move into the cloud R&D era through the DevOps transformation process.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/2020-double-11-highlights-large-deployment-of-cloud-native-technology-for-higher-efficiency-and-5dd08f94c1bd?source=search_post---------256,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 26, 2020·5 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Developer
Tmall has broken two records during the 2020 Double 11 Global Shopping Festival for consumption in GMV (US$74.1 billion) and peak orders per second (583,000). Alibaba Cloud has once again handled the world’s largest traffic peaks without any major issues. How did our technology support the entire event, providing a smooth experience for nearly one billion shoppers around the world?
Recently, Alibaba held the Technical Communication Meeting for Double 11. During the meeting, Ding Yu, Researcher of Alibaba Cloud and the Head of the Cloud-Native Application Platform of Alibaba Cloud, said, “This year, Alibaba Cloud has achieved major technical breakthroughs in the comprehensive cloud-native of the core system to implement major improvements in resource efficiency, R&D efficiency, and delivery efficiency. The resource cost of every 10,000 transactions has been reduced by 80% in four years. The R&D and O&M efficiency have been increased by more than 10% on average, and the delivery efficiency of scale applications has been improved by 100%. This means that Alibaba has successfully completed a comprehensive deployment of cloud-native technology during the 2020 Double 11 Global Shopping Festival.”
Compared with the comprehensive cloud migration in 2019, the comprehensive cloud-native in 2020 has revolutionarily reconstructed the “technology engine” for the Double 11 Global Shopping Festival. On the product side, there are dozens of cloud-native products, such as Alibaba Cloud Container Service for Kubernetes (ACK), ApsaraDB for PolarDB, ApsaraDB for Redis, RocketMQ, Enterprise Distributed Application Service (EDAS), Microservice Engine (MSE), and Application Real-Time Monitoring Service (ARMS).
By utilizing these products, Alibaba Cloud could fully support the Double 11 Global Shopping Festival. On the technical side, the four core technologies of cloud-native have made breakthroughs in the aspect of scale and innovation. They are demonstrating the way of transforming from technical capabilities to business values:
Cloud-native technologies have not only been extensively used in Alibaba, but they have also provided services for the Double 11 Global Shopping Festival through Alibaba Cloud. During the festival, cloud-native supported many customers, such as China Post, STO Express, Perfect Diary, and Century Mart, to help them handle the traffic stably and efficiently. We can use the logistics industry as an example. STO Express moved its core system onto the cloud and used Alibaba Cloud Container Service to obtain a stable system for millions of parcels in transit with 30% less IT costs. The large shopping malls and supermarkets have also gained benefits from cloud-native. For example, based on the elastic scaling of Alibaba Cloud’s Function Computing (FC), the QPS of Century Mart was 230% higher than that of the 2019 Double 11 Global Shopping Festival during its peak hours, the R&D efficiency and delivery efficiency increased by more than 30%, and the elastic resources cost reduced by more than 40%, respectively.
After Alibaba announced the establishment of the Cloud-Native Technical Committee at the Apsara Conference in September, cloud-native was upgraded to a new strategy for Alibaba technologies. The comprehensive cloud-native of the core system for 2020’s Double 11 Global Shopping Festival was an important milestone for the Cloud-Native Technical Committee to promote the comprehensive cloud-native development of Alibaba’s economy. Cheng Li, Chief Technology Officer of Alibaba Group, said, “The most significant difference brought by cloud-native is that Alibaba has truly realized achievements on proprietary research, open source, and commercial use, which can be directly used by customers. Doing this will eliminate the process of precipitation and output on the cloud and reduce the threshold and costs for customers to obtain “the same technology engine for Double 11.” As a result, customers can quickly enter the era of digital-native.
Cloud-native is the quickest way to access the benefits of cloud computing and will become a new base for all-round cloud migration. Ding Yu pointed out that cloud-native is the true revolution of cloud technology that re-upgrade cloud computing. It has promoted the evolution of Cloud Hosting to cloud-native. It means Alibaba Cloud has walked from an old and closed technology system to an open and standard cloud technology system. Along with supporting the Double 11 Global Shopping Festival, these technologies will become the infrastructure of the new digital construction to support the whole society through Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/openkruise-the-cloud-native-platform-for-the-comprehensive-process-of-alibabas-double-11-3bfd05741f33?source=search_post---------257,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 8, 2020·8 min read
Step up the digitalization of your business during the Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Jiuzhu (Siyu Wang) from the Alibaba Developer Community
OpenKruise is an open source cloud-native application automation engine provided by Alibaba Cloud since June 2019. Essentially, it is an extended workload project based on the Kubernetes standard. OpenKruise can be used with native Kubernetes to provide more powerful and efficient capabilities to manage application containers, Sidecar, and image distribution. By realizing automation in different dimensions, OpenKruise can overcome the problems concerning the large-scale O&M and website building of applications on Kubernetes, including deployment, upgrading, elastic scaling, Quality of Service (QoS) adjustment, health check, migration, and repair.
Kruise is a homophone of the word “cruise.” The “K” stands for Kubernetes, indicating the applications of navigation and automatic cruise on Kubernetes. Kruise is fully equipped with Alibaba’s best practices in deploying, publishing, and managing large-scale applications over the years. It also carries the demands of thousands of customers for Alibaba Cloud Kubernetes services.
During the process of fully connecting the Alibaba economy to cloud-native, the Alibaba Technical Team gradually developed a set of technical concepts and best practices that are consistent with the standards of the upstream community and suitable for large-scale Internet scenarios. Among those, the most important things are the ways to have applications published, run, and managed automatically. The Alibaba Cloud container team feeds back these capabilities to the community through OpenKruise to guide the industry to the best practice of cloud-native applications.
During the 2020 Double 11 Global Shopping Festival, Alibaba made its core systems fully cloud-native. Alibaba has been running nearly 100,000 OpenKruise workloads and managing millions of containers.
The following figure shows the relationship between the OpenKruise running internally for Alibaba and the open source OpenKruise.
The preceding figure shows that OpenKruise on GitHub is the upstream repository of the main body, and the internal downstream repository only implements a few internal coupling features based on public interfaces. Running codes for internal OpenKruise accounts for less than 5%. In other words, more than 95% of the codes for OpenKruise that runs internally for Alibaba comes from the community repository.
There are two notable points:
Personnel that is responsible for upper-layer business may not be familiar with the concept of “workload.” Have you ever wondered how to scale an application up or down or how to publish an application? In the cloud-native environment, application deployment requirements, such as the required number of machines and image versions, are described in a final state-oriented way, as shown in the following figures:
The workload mainly refers to the Yet Another Markup Language (YAML) definitions and corresponding controllers.
When an application is scaling up or down, Platform as a Service (PaaS), which is an O&M platform, will modify the required number of machines in the preceding YAML object. For example, if you scale up the number of machines from 10 to 110 and scale down by 5, the final number of machines is 105. Then, the controller adjusts the number of pods, also known as containers, based on the expected workload.
When there is a request for publishing or rolling back, PaaS modifies the image version and publishing policy in the YAML object. Then, the controller rebuilds all managed pods into the desired version based on the specified publishing policy. The working mechanism is much more complicated than the preceding statement.
In short, the workload manages the lifecycle of all containers in the application. Scaling and publishing applications depend on the workload. The workload can also constantly maintain the number of pods in the running application to ensure the expected numbers of instances are running continuously. If a host fails and the instances on the host are evicted, the workload immediately creates new containers for the applications.
After the core systems of Alibaba are migrated to the cloud, services, such as e-commerce, that are related to Double 11 and applications, such as middleware, are migrated to the cloud-native environment. These services and applications are deployed based on the workload of OpenKruise. OpenKruise provides several different types of workloads to support different methods of deployment.
Therefore, from e-commerce business on the upper-layer to middleware, and then to the O&M containers and basic components, the entire upstream and downstream process is deployed and operated based on the workloads provided by OpenKruise. For all the time, OpenKruise is maintaining various kinds of operations, such as the number of machines and version management, urgent scaling up, and publishing during the application runtime.
Imagine what would happen if OpenKruise failed?
Confronted with these high-risk problems, Alibaba has taken multiple protective measures to ensure the stability and availability of services.
The solution is OpenKruise, a platform for Alibaba’s deployment on the cloud, which has nearly taken over the full O&M services for the Double 11 Global Shopping Festival.
What brought about OpenKruise? Why was OpenKruise designed?
As the cloud becomes a major trend and cloud-native turns into a standard, the workload provided by native Kubernetes can hardly work to tackle the challenges faced by Alibaba in its ultra-large-scale business scenarios:
In this context, OpenKruise was created. Whether it is through new development, such as CloneSet and SidecarSet, or enhanced compatibility, including Advanced StatefulSet and Advanced DaemonSet, Alibaba finally has the upper-layer business implemented in cloud-native.
The most important feature of OpenKruise is its “in-place upgrade.” When customers need to upgrade an application, this feature only upgrades the images in the original pod without migrating or rebuilding the container. Some of the benefits are listed below:
Furthermore, Alibaba provides many other advanced features of OpenKruise to meet many business demands in large scale scenarios. The following figure compares OpenKruise with a Kubernetes-native workload in terms of their features in stateless and stateful applications.
On November 11, 2020, after the votes from all of the members of the Cloud Native Computing Foundation (CNCF) Technical Oversight Committee, the open source OpenKruise from Alibaba Cloud officially became a CNCF hosting project.
OpenKruise became a community open source project, and its internal and external versions are almost the same. Moreover, Alibaba included OpenKruise into the application directory of Alibaba Cloud Container Service for Kubernetes (ACK). All customers on the public cloud can install and use OpenKruise. Therefore, OpenKruise has realized its “trinity” system of application in Alibaba’s internal business, cloud services, and open source community. Currently, Douyu TV, STO Express, and Youzan are the main customers that use OpenKruise on ACK. As for the open source community, enterprises, such as Ctrip and Lyft, are both customers and contributors to OpenKruise.
The cloud-native capability of the workload that is released by OpenKruise, based on Alibaba’s ultra-large-scale scenarios, is significant in many aspects. It not only complements the missing section of application workload in the cloud-native community but also provides cloud users with Alibaba’s years of experience concerning the deployment and management of applications and the best practices of cloud-native. Since OpenKruise became an open source project, it has set several milestones:
Afterward, OpenKruise will focus on the following objectives.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/dubbos-cloud-native-transformation-analysis-of-application-level-service-discovery-d48212e940e7?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 2, 2020·17 min read
By Liu Jun (Lugui), Apache Dubbo PMC
Starting from version 2.7.5, we have introduced a new service discovery mechanism based on the instance (application) granularity for the community edition of Dubbo. This marks an important step for us in exploring Dubbo’s adaptation to the cloud-native infrastructure. It has been about half a year since the release of version 2.7.5. With the exploration and summary in this period, we have had a comprehensive and in-depth understanding of the feasibility and stability of this mechanism. Meanwhile, the planning of Dubbo 3.0 is in full progress. Then, how to make application-level service discovery the basic service model of Dubbo 3.0 the next-generation service framework in the future and how to solve the expansion and scalability problems of cloud-native and large-scale microservice clusters will become the focuses of our current work.
Since this new mechanism is so important, how does it work? Today, I will explain it in detail. In the initial community version, we gave this mechanism a mysterious name, that is, service introspection. I will further explain the origin of this name in the following sections and use “service introspection” to refer to this new application-level service discovery mechanism.
Developers familiar with Dubbo know that services have been defined by using Remote Procedure Call (RPC)-oriented methods. This is also the basis for Dubbo’s development of friendly and powerful governance features. So, why do we need to additionally define an application-level service discovery mechanism? How does this mechanism work? What is the difference between this mechanism and the existing one? What benefits can we seek from it? What are the benefits of cloud-native adaptation and performance improvement?
With all these questions in mind, let’s begin.
First, let me answer the questions mentioned at the beginning of this article:
The so-called “application and instance granularity” or “RPC service granularity” stresses a data organization format for address discovery.
Take Dubbo’s current address discovery data format as an example, which is an “RPC service granularity” format. It uses an RPC service as the key and the instance list as the value to organize data:
The new “application granularity-based service discovery” mechanism uses an application name as the key and the list of instances deployed by the application as the value. As a result, this introduces two differences:
To further understand the changes brought about by the new model, let’s take a look at the relationship between applications and RPC services. Typically, multiple RPC services may be defined in one application. Therefore, Dubbo’s previous service discovery granularity is more delicate, and more data entries are generated in the registration center, which is proportional to the RPC service. Meanwhile, this results in data redundancy to some extent.
After we briefly go through the basic working mechanism of application-level service discovery, let’s see why it is called “service introspection.”
To this end, let’s also begin with its working principle. As previously mentioned, the data model of application-level service discovery introduced the following changes: the data volume of the data center declines, RPC service-related data is removed from the registration center, and only application-level data and instance-level data are retained. To ensure that the absent RPC service data can still be correctly perceived by the consumer side, we have established a separate communication channel between the consumer and the provider. In this channel, the consumer and the provider exchange information through specific ports. Here, we regard the behavior that the provider actively exposes its own information as an introspection mechanism. Therefore, from this perspective, we name the entire mechanism “service introspection”.
When I talked about the working principle of service introspection, I also mentioned several differences it had introduced to the registration center. These differences are reflected in the Dubbo framework and the entire microservice system, and have the following benefits:
Automatic and transparent address discovery (load balancing) is a common task for all microservice frameworks. It turns the backend deployment structure to be transparent to upstream microservices. In this way, the upstream service only needs to select one address from the received address list to initiate a call. To achieve this purpose, automatic synchronization is required for two points:
For data synchronization between RPC instances with the help of the registration center, the REST mode has defined an interesting maturity model. If you are interested, click this link for reference.
According to the definition of 4-level maturity in the referenced article, Dubbo’s current interface-level model corresponds to level 4.
Next, let’s see how Dubbo, Spring Cloud, and Kubernetes are designed around the goal of automated instance address discovery.
Spring Cloud only synchronizes application and instance addresses through the registration center. The consumer can establish a connection with the service provider based on the instance address, but the consumer has no idea about initiating HTTP calls because Spring Cloud is based on rest communication. For example, the consumer has no idea what HTTP endpoints the service provider has, and which parameters need to be passed in.
Currently, RPC service information is negotiated by offline agreement or offline management systems. The pros and cons of this schema are summarized below:
Dubbo simultaneously synchronizes the instance address and RPC method through the registration center, so it can achieve automatic synchronization of the RPC process, the orientation to RPC programming and RPC governance, and the imperceptibility of the consumer for the split of the backend application. The disadvantage is that the number of address pushes increases, which is proportional to the used RPC method.
To support Kubernetes’ native services, compared with the service discovery system that builds the registration center on its own, Dubbo has two major changes in the working mechanism:
As an abstract concept, how to map a Kubernetes service to Dubbo is worth discussing.
In the case of mapping Service Name to Application Name, Dubbo applications have a one-to-one correspondence to Kubernetes services. Moreover, these applications are transparent to microservice O&M and construction and are decoupled from the development stage.
In the case of mapping Service Name to Dubbo RPC Service, Kubernetes maintains the binding of the scheduled service and the application’s built-in RPC service, so the number of services to be maintained increases.
Based on the analysis of the preceding different microservice framework models, we can find that in the abstract definition of microservices, Dubbo is quite different from other products, such as Spring Cloud and Kubernetes. Spring Cloud and Kubernetes adopt similar microservice model abstraction methods. The two products only care about the synchronization of instance addresses. If we look into some other service framework products, we will find that most of them are designed in the same way, that is, at level 3 in the REST maturity model.
In contrast, Dubbo is special as its design aims at the granularity of RPC services. It corresponds to level 4 in the REST maturity model.
As shown in the detailed analysis of each model, each model has its pros and cons. The reason why we believed that Dubbo had to make changes and align itself with other microservice discovery models was that when we first determined Dubbo’s cloud-native solution, we found that Dubbo needed to support Kubernetes Native Service, which requires model alignment as a prerequisite. Another reason is the demand from the user side for Dubbo’s scenario-based engineering practices. Thanks to Dubbo’s support for multi-registration and multi-protocol capabilities, Dubbo can connect different microservice systems. However, the inconsistency of service discovery models has become one of the obstacles.
This section talks about the interaction with the registration center and configuration center. As for the changes in the data of the registration center under different models, we have briefly analyzed them in the earlier working principle section. To more intuitively compare the push efficiency improvements brought about by the service model changes, let’s take a look at a comparison between registration centers of different models:
The left side of the figure shows the typical workflow of a microservices framework. In this framework, the provider and consumer implement automated address notification through the registration center. The table in the figure shows the provider instance information.
The application DEMO contains three interfaces, DemoService 1, 2, and 3. The IP address of the current instance is 10.210.134.30.
We can conclude that the amount of data stored and pushed by the application granularity-based model is proportional to the number of applications and instances. Only when the number of applications or the number of application instances increases will the pressure of address push increase.
For an interface granularity-based model, the amount of data is positively correlated with the number of interfaces. Given that an application usually carries multiple interfaces, the order of magnitudes of the interface-level model needs to time a multiplier to compare to that of the application-level model. Another key point is that interface granularity leads to an opaque evaluation of the cluster size. Compared with the growth in the number of instances and applications, which is usually included in O&M planning, the definition of the interface is more of the internal behavior of the service side, which can bypass the evaluation and impose pressure on the cluster.
Take a consumer-side service subscription as an example. According to my rough statistics on some medium- to large-scale Dubbo head users in the community and based on the actual scenario of the target companies, a consumer application needs to consume (subscribe to) more than 10 provider applications, or specifically, the number of interfaces to be consumed (subscribed) reaches 30. On average, the three interfaces subscribed by the consumer come from the same provider application. In this way, if the application granularity is used as the basic unit of address notification and addressing, the average address push and calculation volume will drop by more than 60%.
In extreme cases, when more consumer-side consumption interfaces come from the same application, the address push and memory consumption volume will be further reduced, with a potential reduction of even more than 80%.
A typical scenario is the gateway application in the Dubbo system. Some gateway applications consume (subscribe to) more than 100 applications, while the number of the consumption (subscription) services is more than 1,000. On average, 10 interfaces come from the same application. If we change the granularity of address push and calculation to the application level, the amount of address push will change from n 1000 to n 100, with a reduction by nearly 90%.
In the previous section, I described the benefits or reasons for Dubbo’s orientation to application-level service discovery from the perspective of the service model and supporting large-scale clusters. On the other hand, the service governance capabilities of interface granularity must also be retained, as it is the foundation of the ease of use of the programming model and the benefits of service governance capabilities of Dubbo’s framework.
In my opinion, we must continue to adhere to the following design guidelines during service model migration:
As a new service discovery mechanism, application-level service discovery is almost identical to Dubbo’s previous RPC service granularity-based service discovery in terms of the core process. That is, the service provider registers the address information with the registration center, and the service consumer pulls and subscribes to the address information from the registration center.
The main differences are listed below:
The following example shows the metadata of each instance. The general principle is that the metadata contains only information related to the current instance node and excludes RPC service-level information.
The general information mainly contains these items: the instance address, instance environment variables, the metadata of metadata services, and several other necessary properties.
After the registration center no longer synchronizes the RPC service information, service introspection sets up a built-in RPC service information negotiation mechanism between the service consumer and the provider. This reflects the origin of the name “service introspection”. The server-side instance exposes a predefined MetadataService RPC service, and the consumer obtains the configuration information related to the RPC method of each instance by calling MetadataService.
Currently, the format of data returned by MetadataService is:
For developers that are familiar with Dubbo’s RPC service granularity-based service discovery model, they can find that the service introspection mechanism splits the uniform resource locator (URL) used to be transmitted by the registration center into two parts:
Ideally, a URL can be strictly divided into instance-related data and RPC service-related data. However, you can clearly see that data redundancy occurs in the implemented version, and some data failed to be rationally divided. This issue is especially true for MetadataService. As you can see, the returned data is a URL list assembly, which contains full data.
The following figure shows the complete workflow for service introspection, which details the collaboration process among service registration, service discovery, MetadataService, and RPC calls.
The preceding workflow only considered a case where everything went smoothly. However, in a more specific design or coding implementation, we need to strictly stipulate framework behavior for certain unexpected cases. For example, if the consumer fails to call MetadataService, it will not be able to receive external traffic until the retry is known to be successful.
Configuration synchronization between the client and the server after they receive the address push request is the key phase of service introspection. Currently, there are two options for metadata synchronization, the built-in MetadataService, and an independent metadata center, which coordinates data through a moderately refined metadata cluster.
Note: The timing for the consumer to query the metadata center is after the notification of the address update of the registration center is received. Through the data issued by the registration center, we can know when the metadata of an instance has been updated. Only at this point will you need to query the metadata center.
Now, let’s recall that registration center data is organized in the format of the “application-instance list.” Currently, this change is not fully transparent to developers. That means service developers will be aware of changes in the mechanism for querying or subscribing to the address list. Specifically, compared with the past when addresses were retrieved based on RPC services, the consumer now needs to specify the provider application name to implement address query or subscription.
This is sample code from the legacy consumer development and configuration practice:
This is sample code from the new consumer development and configuration practice:
The method of specifying the provider application name in the preceding example is the current practice of Spring Cloud. It requires the developer on the consumer side to explicitly specify the provider application to be consumed.
The root cause of the preceding problem is that the registration center does not know any information related to the RPC service. As a result, it can only query the application by the application name.
To make the entire development process more transparent to legacy Dubbo users while avoiding the impact of the specified provider on scalability (see below for details), we have designed a set of mapping relationships between RPC services and application names to automatically complete the conversion from RPC services to provider application names on the consumer side.
The reason for establishing a mapping relationship between interfaces and applications in Dubbo is that the mapping relationship between services and applications is not definite. A typical scenario is application-service splitting. For example, the preceding configuration defines PC Service 2 as a service in provider-app-x. In the future, the service may be split by developers into another application, such as provider-app-x-1. This split needs to be perceived by all PC Service 2 consumers, and the application needs to be modified and upgraded accordingly, which is costly.
Whether to use the Dubbo framework to help developers solve this problem transparently or leave the problem to developers is only a matter of strategic choice. Currently, both options are available in Dubbo 2.7.5 and later versions. I prefer to leave it to service developers by leveraging organizational constraints. This approach can further reduce the complexity of the Dubbo framework and improve runtime stability.
The application-level service discovery mechanism is an important step for Dubbo’s transformation to cloud-native. It bridges the gap between Dubbo and other microservice systems at the address discovery level and also becomes the foundation for Dubbo to adapt to Kubernetes’ native services and other infrastructure.
We hope that Dubbo will retain its strengths in simple programming and service governance based on the new model. Meanwhile, we must note that the application granularity-based model increases complexity and requires further optimization and enhancement. On the other hand, in addition to the address storage and push, further application granularity exploration is required as it can potentially help Dubbo in addressing.
Liu Jun, whose GitHub ID is Chickenlj, is a core developer of Apache Dubbo PMC. He has witnessed the whole Dubbo process, from the renewed popularity of Dubbo in the open-source community to the rise of Apache Dubbo. Currently, he works on the Alibaba Cloud Cloud-Native Application Platform Team and is engaged in the development of service frameworks and microservices. He is currently responsible for promoting the Dubbo cloud-native version, Dubbo 3.0.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/interruptdev/containers-serverless-cloud-native-bimodal-interrupt-by-senzo-5875640d1914?source=search_post---------259,"There are currently no responses for this story.
Be the first to respond.
Around 2015, Gartner started to promote the idea of a bimodal IT organisation, with mode 1 effectively being ‘as is’ operations for existing infrastructure, and mode 2 being newer, modern, agile techniques and platforms.
A bimodal approach has the effect of promoting the idea that legacy infrastructure and operations are sufficient for existing systems, whilst supporting the innovation lab movement, which for most corporations is innovation theatre, at a hefty cost with little return. This approach prevents any real change in organisations, and in a world where the majority of IT budgets are spent on maintenance of existing infrastructure, it does little reduce that.
In the Gartner definition of Bimodal, the use of container and serverless technologies would be in the mode 2 operations bucket. That’s where new agile platforms and associated operational practices are used for new initiatives. Real world use tells a different story. The organisations reaping the most benefit from cloud platforms and agile operations are those that go all in.
Nordstrom, the 119 year old, $15 billion-a-year revenue retailer publicly stated they were going all in on AWS in 2015. Initially, they started down the route of many organisations moving to AWS, implementing automation and tools like Chef to automate their VMs in the cloud. At the same time container and serverless technologies were emerging. Nordstrom adopted both. Their first foray into containers was their homegrown container management solution, which has since been superseded by Kubernetes. They’ve achieved significant agility, with faster deploys and greater developer independence, whilst reducing their EC2 bill with AWS. Containers helped drive efficiency in their existing applications, without a major refactor. At the same time they were pioneering event driving architectures using serverless technologies, bringing new capabilities to the business. Doing all of this with a singular operational model of small teams, and developers pushing code to production via git-based workflows.
By using a singular operational model, container based platforms to drive efficiency and agility in existing applications, and serverless platforms to take advantage of new event driven paradigms, Nordstrom is able to be more responsive to its business needs whilst reducing costs, improving reliability and agility.
Nordstrom is far from alone in this. Vanguard, the 45 year old fund manager with $6.2 trillion assets under management, got up on stage at re:Invent 2019 to talk about their migration to AWS. The migration enabled them to “reduce the cost of computing 30 percent and deploy workloads up to 20 times faster, as well as improve resiliency and innovate quickly”. Like Nordstrom, Vanguard adopted a singular operational model, using containers (in the form of AWS Fargate) to drive efficiency in their existing applications, and leverage the event-driven nature of serverless platforms for stream processing.
These organisations are hardly alone. DataDog’s State of Serverless report found that 80% of AWS customers they surveyed who are using containers also used AWS Lambda. Containers + serverless is the norm, not the exception.
This is the cloud native pattern that works. A singular operational model with small empowered teams. Containers to drive efficiency in your existing applications, and Serverless architectures to become event driven.
Why is this the model? Why not use containers for everything? Why not serverless all the things? Why not different operational models for the different areas?
A singular operational model is key. The DevOps Research Agency (DORA) in its annual State of DevOps report identifies organisations into four clusters of operational practices: low, medium, high and elite. The organisations that are successfully achieving the outcomes of improved efficiency and agility whilst reducing costs share the same characteristics of high and elite performers. The 2018 DORA State of DevOps report found that an elite performer’s use of cloud was 23x more likely to meet the five cloud characteristics of on-demand self-service, broad network access, resource pooling, rapid elasticity, and measured service as identified in the NIST definition of cloud. The 2019 report found this had grown to 24x more likely. Essentially, elite performers were significantly more likely to fully utilise the cloud.
By adopting the practices of an elite performing organisation you will be in a better place to leverage container and serverless cloud native platforms.
The decision when to use a container or a serverless platform has some nuance to it. Containers are a great place to replatform existing applications. They give you isolation, a portable image format, and great resource efficiency; all of which can be achieved without rewriting any application code.
Serverless applications on the other hand give you all those same benefits, but to a greater extent, combined with the advantage of being truly event driven and pay per use, with the caveat that the application model will require a refactor in most cases.
Serverless applications have a lower cost and operational overhead than any other model, but they come with the cost of significant change. When it comes to migration, the tension between these factors comes into play with containers often winning because the cost of change to serverless outweighs the benefit. Serverless migrations are most common when there is another driver to refactor an application beyond replatforming for efficiency.
Cloud Native Bimodal is a strategy to leverage containers and serverless cloud platforms under a singular operational model, to drive agility and efficiency to better meet the needs of the organisation.
Containers allow you to migrate quickly, serverless allows you to push your architecture further. Cloud Native Bimodal will dramatically improve the ability of your technology investment to meet your business needs.
Originally published at https://interrupt.dev.
Interrupt and reset. Thoughts for modern developer teams.
50 
50 claps
50 
Written by
Figuring it out…
Exploring the world of the modern developer. Cloud Native, Observability, Serverless, Containers, Infrastructure as Code, and Shifting Left, has changed the scope and role of the developer in a modern tech organisation. We look to explore all these trends.
Written by
Figuring it out…
Exploring the world of the modern developer. Cloud Native, Observability, Serverless, Containers, Infrastructure as Code, and Shifting Left, has changed the scope and role of the developer in a modern tech organisation. We look to explore all these trends.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mohamed-ahmed/how-to-on-using-cloud-native-buildpacks-for-operational-efficiency-a3f5773be062?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Ahmed
Oct 26, 2021·7 min read
This article was originally published at Magalix
https://www.magalix.com/blog/how-to-on-using-cloud-native-buildpacks-for-operational-efficiency
Containers have changed the way of application development. Developers are not only building applications, but they often perform the task of application containerization by using Docker files. Building containers from the start often helps the team package the application and its dependencies, resulting in a more reliable product and improved…
"
https://medium.com/@bluefug/kubecon-2021-reconstituting-the-cloud-native-community-in-la-d42e44bcd264?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jason English
Oct 22, 2021·5 min read
Event Retrospective from Jason English, Principal Analyst, Intellyx
The concept of another big-bang software conference caused me some trepidation — after all, many well-known technology shows have had to pull back on plans to get together in person due to continuing COVID concerns.
But I was happy to find that as the date of KubeCon / CloudNativeCon NA 2021 approached, clear vaccination requirements and on-site mask and health check protocols were in place in the Los Angeles Convention Center to make sure that the only things spreading at the live conference were cloud native knowledge and camaraderie.
Breaking the ice with the normalcy of collaboration
While in-person attendance was nowhere near 2019 levels, the activity level within the Cloud Native Computing Foundation (CNCF) ecosystem has only accelerated, now that there are more than 140+ projects underway and nearly a thousand member and end user orgs in the fold.
And for the dozens of disruptive vendors with presences there, while end customer traffic might have been rather light on the show floor, the spirit of contribution to a common open source foundation meant there was a lot of partnership activity to talk about.
“CNCF has been really good at facilitating collaboration between vendors, contributors and practitioners,” said Shahar Fogel, CEO, Rookout. “We found that this event was about quality over quantity, and a virtual session would just not be the same as the partnership of finally meeting the cloud native community face-to-face.”
Kubernetes matures into a de facto reference architecture
For anyone who thinks of Kubernetes as simply another layer of container orchestration, there were plenty of use cases on display to set doubters right about how it can change the paradigm of how applications are developed and delivered.
Opsani demonstrated an automated optimization solution that discovers application configurations, generates canary Kubernetes instances and YAML files, and compares multi-cloud deployment options against SLOs of cost, scalability and performance as measured in leading observability tools.
Replicated was there in force, making a splashy picnic appearance on the show floor. They support an open source airgap install utility as well as a subscription-based installation management solution that doesn’t require cloud infrastructure at all — it uses K8s to package up all the system resources needed for secure on-prem (or ‘any-prem’) app delivery.
There’s a lot more to Cloud Native than Kubernetes
Companies are getting huge value out of much more than K8s, so the side-show conferences of EnvoyCon, FluentCon and other project focus sessions ended up drawing much of the attention of in-person and online attendees.
“Big companies like Chick-fil-A and Intuit demonstrated the benefits of transitioning to a GitOps workflow — and it doesn’t always take a huge team. State Farm managed their transition with just 4 engineers,” said Ken Ahrens, CEO of Speedscale. “I also appreciated the Day 0 content of EnvoyCon, where Envoy flexed its muscles as cloud-ready with the likes of Twitter and Adobe relying upon it at scale.”
Velero project backups and cloud native data migration were also hot topics, as practitioners seek to preserve state in a stateless microservices world. Trilio was there announcing the release of Velero data backup, monitoring and restore services in the context of ransomware prevention, which is top of mind for many CISOs today.
“As a program committee member — I reviewed 150 submissions and there were a lot of submissions about GitOps, Flux and more,” said Jason Yee, Director of Developer Advocacy at Gremlin. “People really want to establish an end-to-end platform, so while most companies here are investing heavily in Kubernetes, there’s also a heavy movement toward GitOps.”
Observability and control
With Prometheus a standard part of most K8s installs, Kafka on the rise as a common event streaming service and OpenTelemetry really starting to come into its own, contributions to open source observability tools from most leading vendors like Splunk, New Relic and Datadog continued to grow.
Cribl is a fast-growing vendor filling gaps in this space by acting as a ‘universal translator’ which filters those volatile and chatty sources of streaming log event data, routing them to any of the SIEM, observability and storage backends available to cloud native developers.
Down at the roots of code-level observability, Lightrun was dressed up in a sort-of Sherlock Holmes retro theme, highlighting how their platform allows developers to shift-left issue resolution by sleuthing out runtime bugs directly inside the IDE.
Filling the skills gap is still the greatest challenge
Enterprises are notoriously understaffed (as much as 40% by estimates here) on skilled cloud native engineers — and this talent and training gap is only growing. That’s why CNCF announced the addition of a business-level certification offering known as Kubernetes and Cloud Native Associate (KCNA) to compliment its wildly popular CK*-certification courses for operators.
The recently acquired Kasten by Veeam team with their cloud-native backup and data management solution had a huge presence at the show. But one interesting aspect of their appearance was to promote their new free self-directed Kubernetes learning platform which they said has already logged more than 30 thousand courseware users.
Clearly there’s a lot of hunger for learning in this ecosystem, since it seems we’ll never be able to recruit enough cloud native engineers.
The Intellyx Take
Importantly, I found a huge emphasis on developer experience at this show. The more comfortable developers are — whatever their walks of life — the more in the flow they can be to innovate and advance the ecosystem further.
Much of the progress we see in the cloud native movement has less to do with code and infrastructure, and more to do with mutual respect and inclusion.
A cool general session highlight celebrated Indigenous People’s Day with local tribal music and this thought from VMware’s Tim Pepper (@pythomit): “You have heard the saying ‘inclusion is not just inviting somebody to the dance but inviting somebody to dance — the difference between being invited to a dance and dancing.’”
With this kind of event breaking the ice for the next ones, I can see the open source cloud-native movement attracting far more diverse and talented people into its fold as users and contributors in the future.
Jason English (@bluefug) is a Principal Analyst of Intellyx, which advises business leaders and technology vendors on their digital transformation strategies. At the time of writing, Replicated is a current customer, and Gremlin and Rookout are former Intellyx customers. None of the other vendors mentioned in this article are Intellyx customers. The author is an advisor to Speedscale. CNCF covered English’s attendance costs at KubeCon, a standard industry practice.
Agile Digital Transformation analyst & CMO for Intellyx. Brewer, Bassist, Writer. DevOps, cloud, cybersecurity, supply chain focus.
2 
2 
2 
Agile Digital Transformation analyst & CMO for Intellyx. Brewer, Bassist, Writer. DevOps, cloud, cybersecurity, supply chain focus.
"
https://medium.com/digital-leaders-uk/getting-your-head-round-cloud-first-and-cloud-native-18d61df7946d?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
Written by Bill Mew, Strategist at UKCloud
Government technology in the UK was once blighted by slow, expensive and poor IT systems with 80% of public sector IT spend being allocated to an oligopoly of just 8 multi-national service providers. This led to a lack of competition, choice and indeed to situations of lock-in — which we’re still paying for today.
Austerity, and the 2010 coalition galvanised one of the greatest revolutionary advances in government. Lord Maude of Horsham (Francis Maude) deserves a great deal of credit for using his five years as Minister for the Cabinet Office to drive the creation of the Government Digital Service, launch and champion of G-Cloud, a revolutionary procurement framework, and launch of the single GOV.UK website. Policies like ‘cloud first’ and ‘digital by default’ provided a focus and purpose that enabled the government to ramp up its digital capability and move workloads to the cloud — ultimately leading to the UK being recognised by the United Nations as the most digitally advanced government in the world.
This was a massive achievement by any measure. It has also provided a platform for reinvestment and a further wave of transformation moving to cloud native applications and shared services (like GOV.UK Pay, Notify and Verify). However, when you consider that only about 10% of central government workloads are now in the cloud and in areas like local government, education and the NHS is as low as 1% or 2%. There is still much to be done, sometimes the real challenge is knowing where to start!
The Journey to Digital
Parts of the public sector are still blighted with inflexible legacy IT systems that create waste, inefficiencies and impede the delivery of services. Part of the vision for the future is to move to become truly digital — to allow citizens to consume services online, at their convenience (like they do with retailers or banks), and improve the systems for staff in the public sector at the same time.
It is not easy to move from areas of “broken IT” and dis-jointed government towards more efficient, agile and dynamic public services. The journey cannot be completed in a single step. We find it useful to track progress with what we call the 4 step digital progression index.
The first step from stage 1 to 2 is all about supporting workloads that were designed and built before cloud become accepted. These workloads typically already run in legacy data centres and so need to be moved into the cloud. This then allows you to stop focusing on running data centres and hardware — and start focusing on higher components of the stack.
The next step from stage 2 to 3 enables you to really benefit from cloud, with workloads that are dynamic and elastic. Applications need to be programmatically driven to scale-out and self-heal. Workloads that have these characteristics are called cloud native. They are built on open-source technologies in which they need a very different type of platform than the Enterprise and pre-cloud workloads.
These different stages require different solutions and tend to require a different mind-set:
There are perceptions however that enterprise or cloud native environments can be costly or difficult to integrate and manage. At UKCloud we provide price competitive, award-winning platforms, with a unique Cross Domain Security Zone.
Not only does this allow you to devise a multi-cloud strategy in which you can optimise your use of different cloud platforms to match the specific requirements of each workload, but you also have the opportunity to optimise the level of assurance you require as well. From its secure UK data centres, UKCloud provides services across two security domains, Assured OFFICIAL (historically PGA IL2) and Elevated OFFICIAL (historically PGA IL3), and four service levels, offering unrivalled levels of sovereignty, assurance and choice.
Obviously, this means that you can enjoy the best of both worlds with industry leading service and security, for a lot less than you’d think. Clearly with security and cost addressed, you are free to choose the platform right for each and every one of your workloads.
Originally published at digileaders.com on July 20, 2017.
Thoughts on leadership, strategy and digital transformation…
1 
Digital Leaders is a global initiative that has created a shared professional space for senior leadership from different sectors promoting effective, long-term digital transformation.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-storage-the-cornerstone-of-cloud-native-applications-f8b8182657b1?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 22, 2020·18 min read
By Junbao, Alibaba Cloud Technical Expert
It is impossible to ignore the significance of storage services as they are an important part of the computer system and the foundation on which we run all applications, maintain the states of applications, and support data persistence. As storage services have evolved, each new business model or technology put forward new requirements for storage architecture, performance, availability, and stability. With the current ascendance of cloud-native technology, we must consider the types of storage services that will be needed to support cloud-native applications?
This is the first in a series of articles that explore the concepts, characteristics, requirements, principles, and usage of cloud-native storage services. These articles will also introduce case studies and discuss the new opportunities and challenges posed by cloud-native storage technology.
“There is no such thing as a ‘stateless’ architecture” — Jonas Boner
This article focuses on the basic concepts of cloud-native storage and common storage solutions.
To understand cloud-native storage, we must first familiarize ourselves with cloud-native technology. According to the definition given by the Cloud Native Computing Foundation (CNCF):
Cloud-native technology empowers organizations to build and run scalable applications in emerging dynamic environments, such as the public, private, and hybrid clouds. Examples of cloud-native technology include containers, service meshes, microservices, immutable infrastructures, and declarative APIs.
These techniques enable us to build loosely coupled systems that are highly fault-tolerant and easy to manage and observe. When used together with robust automation solutions, cloud-native technology allows engineers to make high-impact changes frequently, predictably, and effortlessly.
In a nutshell, there is no clear dividing line between cloud-native applications and traditional applications. Cloud-native technology describes a technological tendency. Basically, the more of the features below an application offers, the more cloud-native it is:
A cloud-native application is a collection of application features and capabilities. These features and capabilities, once realized, can greatly optimize the application’s core capabilities, such as availability, stability, scalability, and performance. Outstanding capabilities are the current technological trend. As part of this process, cloud-native applications are transforming various application fields while profoundly changing all aspects of application services. As one of the most basic conditions necessary to run an application, storage is an area where many new requirements have been raised as services have become cloud native.
The concept of cloud-native storage originated from cloud-native applications. Cloud-native applications require storage with matching cloud-native capabilities, or in other words, storage with cloud-native tendencies.
The availability of a storage system represents users’ ability to access data in the event of system failure caused by storage media, transmission, controllers, or other system components. Availability defines how data can still be accessed when a system fails and how access can be rerouted through other accessible nodes when some nodes become unavailable.
It also defines the recovery time objective (RTO) in the event of failure, which is the amount of time between the occurrence of a failure and the recovery of services. Availability is typically expressed as the percentage, which represents an application’s available time as a proportion of its total running time (for example, 99.9%), with measurements made in the time unit of MTTF (mean time to failure) or MTTR (mean time to repair).
Storage scalability primarily represents the ability to:
Storage performance is usually measured by two metrics:
Cloud-native applications are widely used in scenarios such as big data analysis and AI. These high-throughput and large I/O scenarios pose have demanding requirements for storage. In addition, the features of cloud-native applications, such as rapid resizing and extreme scaling test the ability of storage services to cope with sudden traffic peaks.
For storage services, consistency refers to the ability to access new data after it is submitted or existing data is updated. Based on the latency of data consistency, storage consistency can be divided into two types: eventual consistency and strong consistency.
Services vary in their sensitivity to storage consistency. Applications such as databases, which have strict requirements for the accuracy and timeliness of underlying data, require strong consistency.
Multiple factors can influence data persistence:
In cloud-native application systems, applications can access storage services in various ways. Storage access can be divided into two modes based on interface type: data volume and API.
Data volume: The storage services are mapped to blocks or a file system for direct access by applications. They can be consumed in a way similar to how applications directly read and write files in a local directory of the operating system. For example, you can mount a block storage or file system storage to the local host and access data volumes just like local files.
API: Some types of storage cannot be mounted and accessed as data volumes, but instead, have to be accessed through API operations. For example, database storage, KV storage, and Object Storage Service (OSS) perform read/write operations through APIs.
Note: OSS is generally used to externally provide file read/write capabilities through RESTful APIs. However, it can also be used by mounting the storage as a user-state file system, just like mounting block or file storage as data volumes.
The following table lists the advantages and disadvantages of each storage interface:
This layer defines the interfaces for external access to the stored data, namely, the form in which storage is presented when applications access data. As described in the preceding section, access methods can be divided into data volume access and API access. This layer is the key to storage orchestration when developing container services. The agility, operability, and scalability of cloud-native storage are implemented on this layer.
This layer defines the topology and architectural design of the storage system. It indicated how different parts of the system, such as storage devices, computing nodes, and data, are related and connected. The construction of the topology affects many properties of the storage system, so much thought must be put into its design.
The storage topology can be centralized, distributed, or hyper-converged.
The layer defines how to protect data through redundancy. It is essential for a storage system to protect data through redundancy so that it can recover data in the event of a failure. Generally, users can choose from several data protection solutions:
Data services supplement core storage capabilities with additional storage services, such as storage snapshots, data recovery, and data encryption.
The supplementary capabilities provided by data services are precisely what cloud-native storage requires. Cloud-native storage achieves agility, stability, and scalability by integrating a variety of data services.
This layer defines the actual physical hardware that stores the data. The choice of physical hardware affects the overall system performance and the continuity of stored data.
The cloud-native nature of the storage means it must be containerized. In container service scenarios, a certain management system or application orchestration system is usually required. The interaction between the orchestration system and storage system establishes the correlation between workloads and stored data.
In the above diagram:
The orchestration system prepares the storage resources defined by the application load. The orchestration system uses the control plane of the storage system by calling the control plane interfaces and implements access and egress operations application loads perform on the storage services. After accessing the storage system, an application load can directly access its data plane, giving it direct access to data.
Every public cloud service provider offers a variety of cloud resources, including various cloud storage services. Take Alibaba Cloud for example. It provides basically any type of storage required by business applications, including object storage, block storage, file storage, and databases, just to name a few. Public cloud storage has the advantages of scale. Public cloud providers operate on a large scale, allowing them to keep prices low while making gigantic investments in R&D and O&M. In addition, public cloud storage can easily meet the needs of businesses for stability, performance, and scalability.
With the development of cloud-native technology, public cloud providers are competing to transform and adapt their cloud services for the cloud-native environment and offer more agile and efficient services to meet the needs of cloud-native applications. Alibaba Cloud’s storage services are also optimized in many ways for compatibility with cloud-native applications. The CSI storage driver implemented by Alibaba Cloud’s container service team seamlessly connects data interfaces across cloud-native applications and the storage services. The underlying storage is imperceptible to users when they consume the storage resources, which allows them to focus on business development.
Advantages:
Disadvantage:
In many private cloud environments, business users purchase commercial storage services to achieve high data reliability. These solutions provide users with highly available, highly efficient, and convenient storage services, and guarantee O&M services and post-production support. Private cloud storage providers, as they become gradually aware of the popularity of cloud-native applications, are providing users with comprehensive and mature cloud-native storage interface implementations.
Advantages:
Disadvantages:
Many companies choose to build their own storage services for business data with low service level requirements. Business users can choose among currently available open-source storage solutions based on their business needs.
Advantages:
Disadvantages:
Some businesses do not need highly available distributed storage services. Instead, they prefer local storage solutions due to their high performance.
Database services: If users need to achieve high storage I/O performance and low access latency, common block storage services cannot effectively satisfy their needs. In addition, if their applications are designed for high data availability and do not need to retain multiple replicas at the underlying layer, the multi-replica design of distributed storage is a waste of resources.
Storage as a cache: Some users expect their applications to save unimportant data, which can be discarded after programs are executed. This also requires high storage performance. Essentially storage is being used as a cache. The high availability of cloud disks does not make much sense for such services. In addition, cloud disks don’t have advantages over local storage in terms of performance and cost.
Therefore, although local disk storage is much weaker than distributed block storage in many key aspects, it still maintains a competitive edge in specific scenarios. Alibaba Cloud provides a local disk storage solution based on NVMe. Its superior performance and lower pricing make it popular among users for specific scenarios.
Alibaba Cloud CSI drivers can be used by cloud-native applications to access local storage and support multiple access methods, such as lvm volumes, raw device access to local disks, and local directory mapping. CSI drivers can be used to implement data access adapted for high performance access, quota operations, and IOPS configuration, among others.
Advantages:
Disadvantages:
With the development of cloud-native technology, the developer community has released some open-source cloud-native storage solutions.
Rook, as the first CNCF storage project, is a cloud-native storage solution that integrates distributed storage systems, such as Ceph and Minio. It is designed for simple deployment and management, deeply integrated with the container service ecosystem, and provides various features for adaption to cloud-native applications. In terms of implementation, Rook can be seen as an operator that provides Ceph cluster management capabilities. It uses CRD to deploy and manage storage resources such as Ceph and Minio.
Rook components:
Rook deploys the Ceph storage service as a service in Kubernetes. The daemon processes, such as MON, OSD, and MGR, are deployed as pods in Kubernetes, while the core components of Rook perform O&M and management operations on the Ceph cluster.
Through Ceph, Rook externally provides comprehensive storage capabilities and supports object, block, and file storage services, allowing users to use multiple storage services with one system. Finally, the default implementation of the cloud-native storage interface in Rook uses the CSI/Flexvolume driver to connect application services to the underlying storage. As Rook was initially designed to serve the needs of the Kubernetes ecosystem, it features strong adaption for containerized applications.
Official Rook documentation: https://rook.io/
OpenEBS is an open-source implementation that emulates the functions of block storage such as AWS EBS and Alibaba Cloud disks. OpenEBS is a container solution based on container attached storage (CAS). It adopts a microservices model as storage and applications do and orchestrates resources through Kubernetes. In terms of its architecture, the controller of each volume is a separate pod and shares the same node with the application pod. The volume data is managed through multiple pods.
The architecture comprises the data plane and control plane.
Data plane:
OpenEBS persistence volumes (PV) are created based on the PVs in Kubernetes and implemented through iSCSI. Data is stored on nodes or in cloud storage. OpenEBS volumes are managed independent of the application life cycle, similar to the PVs in Kubernetes.
OpenEBS volumes provide persistent storage for containers, elasticity to cope with system failures, and faster access to storage, snapshots, and backups. It also provides a mechanism to monitor usage and execute QoS policies.
Control plane:
The OpenEBS control plane, Maya, has implemented hyper-converged OpenEBS, which can extend the storage functions provided by specific container orchestration systems, such as when it is mounted to the Kubernetes scheduling engine.
The OpenEBS control plane is also based on microservices and implements different features, such as storage management, monitoring, and container orchestration plug-ins, through different components.
For more information on OpenEBS, see: https://openebs.io/
Similar to Rook, Heketi is an implementation of the Ceph open-source storage system on the cloud-native orchestration platform (Kubernetes). Glusterfs also has a cloud-native implementation. Heketi provides a Restful API for managing the life cycles of Gluster volumes. With Heketi, Kubernetes can dynamically provide any type of persistence supported by Gluster volumes. Heketi will automatically determine the location of a brick in the cluster and ensure that the brick and its copies are placed in different failure domains. Heketi also supports any number of Gluster storage clusters to provide network file storage for cloud services.
With Heketi, administrators no longer need to manage or configure block, disk, or storage pools. Heketi services manage all the system hardware and allocate storage as needed. Any physical storage registered with Heketi must be provided through raw devices. Heketi can manage these disks by using LVM on the provided disks.
For more details, see: https://github.com/heketi/heketi
Cloud-native application scenarios impose rigorous requirements for service agility and flexibility. Many expect fast container startup and flexible scheduling. To meet these requirements, the volumes must be adjusted agilely as the pods change.
These requirements demand the following improvements:
Most storage services already have monitoring capabilities at the underlying file system level. However, monitoring for cloud-native data volumes needs to be enhanced. Currently, PV monitoring is inadequate in terms of data dimensions and intensity.
Specific requirements:
Provide more fine-grained monitoring capabilities at the directory level.
Provide more monitoring metrics, including read/write latency, read/write frequency, and I/O distribution.
Big data computing scenarios usually involve a large number of applications accessing storage concurrently, which makes the performance of storage services a key bottleneck that determines how the efficiency of application operations.
Specific requirements:
Shared storage allows multiple pods to share data, which ensures unified data management and access by different applications. However, in multi-tenant scenarios, it is imperative to isolate the storage of different tenants.
The underlying storage provides strong isolation between directories, making it possible for isolation at the file system level among different tenants of a shared file system.
The container orchestration layer implements orchestration isolation based on namespaces and PSP policies. This prevents tenants from accessing the volume services of other tenants during application deployment.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/ensuring-high-availability-for-cloud-native-business-systems-e34f0783b9f1?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 1, 2021·10 min read
By Zhang Chunmei (Niutu)
This article is intended to help you ensure the high availability of your business systems under cloud native through the following three approaches:
The concept of “high availability” for a system can often be divided into business availability and service availability. A high-availability system is designed with features to ensure both business stability and service availability, as well as to accommodate for frequent code and functional testing.
High-availability systems can be divided into the following types by feature or business implementation:
Resources and business services are two major considerations for satisfying business needs or completing business maintenance. More importantly, we should ensure high availability of the business service system architecture in every aspect by using tools and techniques. A high-availability system provides PTS and AHAS, both of which are commercially available. AHAS includes online traffic protection and fault drills.
The following figure shows the evolution of PTS.
Alibaba initiated performance testing and distributed development in 2008 and started capacity planning through tools such as Cryptographic Service Provider (CSP) and Autoload in 2010. Since then, Alibaba has been conducting offline performance testing on a variety of platforms, such as the Password Authentication Protocol (PAP)-based distributed stress testing platform. However, offline testing resulted in a series of problems, including inaccurate test results due to the differences between the production environment and the offline environment in scale and code configuration. This may cause stress testing to output meaningless results.
To solve these problems, Alibaba tried to conduct online testing by using CSP. This involved analyzing logs based on online traffic to identify the APIs used and their general proportions through log playback. However, log playback has a disadvantage. The simplest POST method rarely ships data to forms. Even if data is logged, it may not be used properly. Alibaba released PTS 1.0 in 2013. PTS 1.0 supports comprehensive stress testing, including basic data construction and inter-link API configuration. The constructed data can all be read during the stress test process. In 2014, Alibaba used the independent software vendor (ISV) platform for data output, but this platform was used for offline testing in a way similar to the previous PAP-based platform.
In 2015, Alibaba released PTS Basic Edition, which requires advance data writing and item stress testing in the form of scripts. In the same year, Alibaba made PTS into a platform based on third-party components, such as payments. Alibaba explored a series of approaches to platformization, such as mocking and link streamlining. By 2016, Alibaba had built a variety of business systems to support more ecosystem businesses. Alibaba began to steer in the direction of intelligence. In 2017, Alibaba released PTS Platinum Edition. In 2018, PTS was made open source and made to support stress testing through Apache JMeter. In 2019, Alibaba deeply integrated performance testing with high-availability modules.
After more than a decade of development, PTS has gradually become a mature platform.
2. The traffic to be simulated must be real traffic; and
3. Operations must form a feedback loop.
Problems occur in every phase of stress testing no matter what the test scale is. The following model was created to solve these problems:
In short, a non-production environment is prone to code-related problems, such as garbage collection (GC) problems, memory leaks, and improper configurations. These problems can lead to other problems in a production environment, which are related to systems, traces, and generic layers, such as load balancing problems.
We can summarize the following four drivers:
Capacity evaluation is divided into three steps:
Step 1: Select a stress testing method
(1) Organize the related architecture; (2) Set a goal and determine the approach to achieve the goal; (3) Make test preparations, including data preparation and model preparation; and (4) Develop a checklist to record the important things to do.
Step 2: Select tools
(1) Open source tools and (2) Software as a service (SaaS) products
Step 3: Conduct scenario-based stress testing
(1) Construction method; (2) Stress testing approach; and (3) Locating method
The following section explains how to select open-source tools and SaaS products. JMeter is used as an example.
SaaS tools are more cost-effective than open-source tools.
The following figure shows some logs in a stress testing report.
The preceding section discusses JMeter. PTS provides a proprietary engine as one of its core capabilities. This engine assumes an important role in Alibaba’s Double 11 Shopping Festival. At present, two engines are commonly used: the proprietary engine and the native JMeter engine. The proprietary engine uses a pure-UI edit mode and requires no code maintenance or local maintenance. You only need to maintain data files.
The following figures show the capabilities of PTS in a flowchart.
The following figure shows the capabilities of PTS based on different phases of stress testing.
Recording from the cloud: After you configure a proxy, you can record ongoing operations on a PC.
The following figure shows the features of PTS.
Service level agreement (SLA): You can determine an SLA for stress testing. For example, the response time (RT) cannot exceed 500 ms, and the success rate cannot be less than 99.99%. If the success rate is less than 99.99%, you can trigger an alert or stop stress testing. This helps you monitor the accuracy of a stress test in progress.
Scheduled stress testing: This is commonly used by scheduled activities, such as monthly promotions and weekly iterations. You can perform an iteration over a period of several minutes and then analyze the iteration results the next day. You can also conduct unmanned stress testing by developing an SLA and setting a status success rate.
The following figures show some problems that may occur during the stress testing process.
The difference between predictions and reality is also a problem that deserves attention. For example, we usually conduct scale-out for online education before Chinese New Year to cope with the rising number of online education users during the holiday. In the case of an unexpected incident, such as the current epidemic, we must conduct scale-out again to meet the needs of more users. If this happens, we must determine the target capacity and take a series of protective measures in case more problems occur.
Problems must be handled in an effective, multi-level, and multidimensional manner.
The following figure shows statistics from the Double 11 Shopping Festival in 2018.
These statistics indicate two points:
(1) The volume is very large and (2) This volume occurs in a short period of time.
Under such circumstances, it is necessary to handle problems promptly to avoid any impact on customers. Otherwise, they may leave the purchase page.
Creation of Sentinel: The following figure shows a classic interface from Double 11, which indicates throttling in progress. This is intended to avoid system avalanche due to traffic peaks and ensure most customers enjoy a good experience. Therefore, we developed traffic protection tools.
Sentinel is a lightweight control framework based on a distributed architecture. It ensures the stability of systems and services when faced with traffic peaks through the following measures:
(1) Throttling; (2) Circuit breaking; (3) Traffic shaping; and (4) System protection
The following figure shows the Sentinel architecture.
Throttling can be implemented on the gateway. Applications in a distributed architecture are clustered and different applications can call each other. This allows us to implement application-level traffic shaping for staggered traffic management.
Common application scenarios include:
The following metrics are considerations for traffic protection:
The following section explains how circuit breaking works.
The probability of failed order placement is proportional to the number of links.
Some of the protected applications are unavailable and one of them is abnormal. Therefore, this application is downgraded to ensure the normal operation of other services. In other words, when a resource on a link is unstable, calls to this resource are restricted.
Traditional system load protection is implemented based on inflexible metrics. However, such metrics have latency and waste the processing capabilities of the system. This slows down system recovery and further delays adjustments.
A new overload protection algorithm is developed to solve these problems, as shown in the following figure.
The following figure shows the algorithm verification results.
The following figure shows the performance statistics for the new overload protection algorithm.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@RadwareBlog/anatomy-of-a-cloud-native-data-breach-92e39bc402d3?source=search_post---------266,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radware
Apr 10, 2019·1 min read
Migrating computing resources to cloud environments opens up new attack surfaces previously unknown in the world of premise-based data centers. As a result, cloud-native data breaches frequently have different characteristics and follow a different progression than physical data breaches. Here is a real-life example of a cloud-native data breach, how it evolved and how it possibly could have been avoided.
The company is a photo-sharing social media application, with over 20 million users. It stores over 1PB of user data within Amazon Web Services (AWS), and in 2018, it was the victim of a massive data breach that exposed nearly 20 million user records. This is how it happened.
Read more: http://bit.ly/2Usr68e
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
1 
1 
1 
A leading provider of application delivery & cybersecurity solutions ensuring optimal service level for applications in virtual, cloud and SDDCs.
"
https://medium.com/@manningbooks/a-few-words-about-cloud-native-patterns-9ef98322b101?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Oct 23, 2019·3 min read
From the book’s foreword by Gene Kim
We think Cloud Native Patterns is a pretty special book — one that is going to become a fixture on the desks of many developers. So we wanted to share the wonderful things that Gene Kim had to say in the book’s foreword, both about the book itself and its exceptional author: Cornelia Davis.
For six years, I’ve had the privilege of working with Nicole Forsgren and Jez Humble on the “State of DevOps Report,” which has collected data from more than 30,000 respondents. One of the biggest revelations for me was the importance of software architecture: high-performing teams had architectures that enabled developers to quickly and independently develop, test, and deploy value to customers, safely and reliably.
Decades ago, we could joke that software architects were expert only at using Visio, creating UML diagrams, and generating PowerPoint slides that no one ever looked at. If that were ever true, that is certainly not the case now. These days, businesses win and lose in the marketplace from the software they create. And nothing impacts the daily work of developers more than the architecture that they must work within.
This book fills a gap, spanning theory and practice. In fact, I think only a handful of people could have written it. Cornelia Davis is uniquely qualified, having spent years as a PhD student studying programming languages, having developed a love of functional programming and immutability, working for decades within large software systems, and helping large software organizations achieve greatness.
Over the past five years, I’ve reached out to her for help and advice many times, often about topics such as CQRS and event sourcing, LISP and Clojure (my favorite programming language), the perils of imperative programming and state, and even simple things like recursion.
What makes this book so rewarding to read is that Cornelia doesn’t just start with patterns. She starts with first principles, and then proves their validity through argumentation, sometimes through logic, and sometimes through flowcharts. Not satisfied with just theory, she then implements those patterns in Java Spring, iteration after iteration, incorporating what you’ve learned.
I found this book entertaining and educational, and learned an incredible amount on topics that I formerly had only a cursory understanding of. I am now committed to implementing her examples in Clojure, out of a desire to prove that I can put this knowledge into practice.
I suspect you’ll connect concepts that will delight and maybe even startle you. For me, one of those concepts was the need to centralize cross-cutting concerns, whether through aspect-oriented programming, Kubernetes sidecars, or Spring Retry injections.
I hope you find this book as rewarding to read as I did!
— Gene Kim, researcher and coauthor of The Phoenix Project, The Devops Handbook, and Accelerate
Interested in seeing some of the book’s contents? Go check it out on our browser-based liveBook reader here!
Follow Manning Publications on Medium for free content and exclusive discounts.
Follow Manning Publications on Medium for free content and exclusive discounts.
"
https://medium.com/@alibaba-cloud/alibaba-clouds-container-service-upgraded-cloud-native-just-got-a-bit-more-powerful-dfad4fe53e3d?source=search_post---------269,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 31, 2019·18 min read
By Tang Zhimin, director of research and development at Alibaba Cloud Container Service.
Relive the best moments of the Apsara Conference 2019 at https://www.alibabacloud.com/apsara-conference-2019.
Kubernetes, in short, both functions as a major operating system on the cloud and is also an extremely important infrastructure when it comes to everything cloud native. Alibaba Cloud has been an important player in bringing Kubernetes to customers in China, providing the most popular containerization service in China.
Director of research and development at Alibaba Cloud’s Container Service, Tang Zhimin (pictured above), in his keynote speech during this year’s Apsara Conference, reiterated this point. “Alibaba Cloud Container Service is easily China’s largest container cluster among public cloud service providers. And, according to the rankings put out by several major international evaluation agencies, the service has occupied the largest market share of any service of its kind in the Chinese market, with its performance and capabilities consistently ranking as number one.”
The latest release of Alibaba Cloud Container Service for Kubernetes (ACK), version 2.0, makes for a major upgrade in terms of capacity, performance, and elasticity, and should make Kubernetes even more accessible to even more customers and businesses alike. In this new version, each cluster can contain up to 10,000 nodes, retain up to 90% of the performance of native sandboxed containers, and scale to 1,000 nodes in a few minutes.
So far, Alibaba Cloud Container Service has been deployed in 20 regions around the globe. Moreover, Alibaba Cloud has also launched the cloud-native hybrid cloud 2.0 architecture and ACK@Edge to provide secure, intelligent, and cloud computing services.
In the early days, most of the applications running in Kubernetes were stateless. However, nowadays, an increasing number of enterprises are migrating their core business systems, data intelligence workloads, and innovation-related workloads to Kubernetes. Currently, at Alibaba, cloud services like Enterprise Distributed Application Service (EDAS), Microservice Engine (MSE), Dataphin, and Data Lake Analytics are all deployed on Alibaba Cloud’s Container Service for Kubernetes.
Alibaba Cloud’s Realtime Compute has launched a cloud native-based version of Apache Flink that allows users to deploy Flink on their Kubernetes clusters. This change in many ways can also help to streamline the workflows of several enterprises with online businesses and stream computing services being able to be deployed on the same Kubernetes cluster. As such, these enterprises can reduce related operation and maintenance costs while also taking advantage of the elasticity brought by the Kubernetes cloud-native infrastructure.
Again, this is the power of this all-new upgrade. And this also shows how Kubernetes and cloud native is changing the cloud computing landscape of today. Kubernetes can help many enterprises turn multiple platforms into one single, unified platform, making cloud that much more agile, elastic, accessible and flexible.
But, you may ask, then why is this particular time a defining moment for cloud native? Well, consider these points noted in the biennial statement issued by the Cloud Native Computing Foundation (CNCF) from back in August:
So, as you can see from current trends, cloud native and Kubernetes is clearly on the horizon and is clearly part of the future landscape of cloud computing.
Now let’s look back at the development of Alibaba Cloud’s own Container Service. In 2011, Alibaba Cloud became the first cloud service provider in China to offer container technologies. And at the end of 2015, Container became available for public beta testing. Then, over the past four years, Container Service eventually became fully serviceable in 20 regions around the world, including Asia Pacific, North America, and Europe.
So far, this service has served tens of thousands of customers and enterprises alike, being in several different industries, including the Internet, finance, public service, and manufacturing industries. Container Service occupies the largest market share of any service of its kind in the China. In fact, Alibaba Cloud Container Service has witnessed a growth of more than 400% for three consecutive years. And, as of August this year, the number of image downloads monthly has exceeded 300 million. Container Service has slowly become the first choice for enterprises deploying cloud-native applications.
Alibaba Cloud’s Container Service has received approval from some of the most influential research and advisory agencies. Alibaba Cloud was also the only cloud vendor in China to be listed in a report on the public cloud container services released by Gartner in June, 2019. According to the container report issued by Forrester in July 2019, Alibaba Cloud is one of the strongest competitors among cloud service providers in the global market and it ranks first in the Chinese market for having both the largest market share and boast some of the best performance.
An increasing number of enterprises, both in China and abroad, are benefiting from the advantages and capabilities of cloud-native technologies. Sanweijia, a home furnishing design company based in Guangdong, used Alibaba Cloud’s Container Service to achieve a quick and seamless cloud migration process. China-based Minsheng Bank, also a customer of Alibaba Cloud, optimized its architecture for core applications based on Kubernetes to accelerate its business iterations. Major Microblogging platform, Weibo used Kubernetes to manage heterogeneous resources, accelerate AI-powered computing operations, and enhance application data intelligence. While Siemens deployed its open IoT operating system, named MindSphere, they nevertheless also went for a multi-cloud strategy and chose Alibaba Cloud’s Container Service for Kubernetes as a platform for connect all of the different underlying infrastructure together.
Sanweijia (三维家, literally “3D home”) is a home furnishing design company located in Guangdong, China. The company, as their name implies, provide 3D panoramic view technology that allows the customer to see the full effect of the interior designs they offer. In many ways, they are leading a new evolution of home furnishing and interior design in the Internet age. Sanweijia used to use on-premises datacenters. The O&M team had to do all the work, which was a time-consuming effort. The team was weighed down with workloads and could barely keep up with the growing demands for computing capabilities.
In 2018, Sanweijia migrated a part of their workloads to the cloud, and started balancing the load of their home furnishing visualization rendering computing operations between Alibaba Cloud’s Container Service for Kubernetes and about 1,000 ECS Bare Metal instances. Sanweijia used containerization technologies to quickly migrate their workloads to the cloud in batches, which allowed them to complete the migration process in only three days flat. If they used traditional migration methods, the time needed for their resource scaling efforts could have easily multiplied. The auto scaling feature of Container Service can start 100 Bare Metal ECS instances in three minutes to handle workload bursts. In addition, with the canary release feature of Kubernetes, rendering technologies and services can be iterated based on customer levels and billing methods.
Weibo is a social media giant in China, similar to Twitter, with several major influencers in China posting regularly on the platform. As of now, Weibo has more than 200 million daily active users. To deliver tailored content to users with vastly different interests, Weibo used algorithms powered by machine learning. Alibaba Cloud’s Machine Learning Platform for AI can apply real-time computing and online learning capabilities to many different scenarios.
The entire online learning pipeline that Weibo deployed was both long and complex, having sky-high requirements on the validity and stability of both offline and online services. Weibo adopted Alibaba Cloud’s all-on-Kubernetes solution to maximize the benefits of offline-online hybrid deployment, improve the efficiency and stability of service operation management, and allow them to be able to dynamically scale resources.
When it comes to applying Kubernetes on a large scale, there are many challenges to overcome. For example, how can one ensure the security and compliance of Kubernetes and its applications? How can one manage online and offline Kubernetes clusters in a unified manner? And how can one make full use of the top and underlying Kubernetes ecosystems? Well, in this latest upgrade to the service, Alibaba Cloud’s Container Service team has worked hard to make sure that any and all customers can easily handle these issues.
To handle these issues, the Alibaba Cloud Container Service team has introduced a fleet of several powerful features. These features were developed based on several years of working in enterprise-level production environments and can assist users and enterprises alike to apply Kubernetes to their businesses with ease.
First, let’s discuss security and see how you can guarantee end-to-end security in the cloud-native era.
Compared with traditional security solutions, what are the new challenges for Container Service in the cloud-native era?
To handle these security risks, Container Service has implemented an end-to-end upgrade to enhance the security of the native-cloud architecture in the three following ways:
Now that we have discussed these security features, let’s focus on secure application supply chains and sandboxed-containers.
With the quick iteration rates of applications and new applications continuously being launched, at Alibaba we have higher standards for the security of the entire application development procedure. We understand the importance of predicting potential security risks and eliminate them at the beginning of the development lifecycle. The cloud-native secure software supply chain developed based on Alibaba Cloud’s Container Registry enables you to secure the entire application development lifecycle and guarantee the security of application releases. The secure software supply chain has the following benefits:
With the DevSecOps solution, we also aim to eliminate potential risks at the beginning of the entire application development lifecycle.
If you have an open-source or untrusted third-party application deployed in a Kubernetes cluster, you can use our sandboxed-Container Service. Unlike normal pods, each sandboxed-container has a kernel for security isolation. This is a step to ensure that you achieve security, compatibility, and performance at the same time-something not easy to do by yourself.
Sandboxed-containers have been thoroughly optimized by Alibaba Cloud, with a performance close to 90% of that provided by a native runC. You can deploy normal pods and sandboxed-pods on the same cloud server to achieve hybrid deployment. This allows you to choose between these two types of pods as needed. We also provide features such as logging to enhance the performance of sandboxed-containers.
Let’s go back to the second question now: how can you manage on-premises Kubernetes clusters and cloud-based Kubernetes clusters at the same time. Well, our ACK provides a cloud computing solution without borders to resolve this issue.
Considering the ownership and security compliance requirements of data, many companies will migrate only some of their workloads to the cloud. For example, when there are large online activities on Weibo and Bilibili, the applications will be migrated from on-premises data centers to the cloud to cope with traffic spikes. Some bank and government institutions have also chosen Alibaba Cloud given its cost-effective solutions for cloud-based disaster recovery and active geo-redundancy. Hybrid deployment has become a common choice for enterprises to migrate their workloads to the cloud. However, the adoption of hybrid cloud brings to mind a new challenge: There is a huge margin in terms of capabilities and security requirements between on-premises and cloud-based infrastructures. And so we arrive at the question: how can you manage both of them effectively at the same time?
To address this issue, Alibaba Cloud’s Container Service for Kubernetes has provided the application-centric hybrid cloud 2.0 architecture.
With this all-new architecture, you can install an agent on a Kubernetes cluster running in an on-premises data center to enable Container Service to manage the cluster on the cloud. Of course, in the case that you do not want to use Container Service to manage on-premises Kubernetes clusters, you can alternatively choose to use the Agility Edition of Container Service. With this Edition, after all your clusters have been registered, you can use the federation feature of Alibaba Cloud’s Container Service for Kubernetes to implement unified application deployment, security governance, and monitoring.
If you want to customize your load balancing, network traffic distribution, and application release policy configurations for your Kubernetes clusters, you can use the grid feature of Alibaba Cloud Container Service.
The cloud-native hybrid cloud architecture provided by Container Service also has the following advantages:
If you want to use our cloud-native hybrid cloud but have not migrated your workloads to the cloud, at Alibaba we can offer you a set of cloud-native migration tools to ensure smooth migration and reduce the migration costs. These migration tools simplify your migration work in three ways: application images, application configurations, and application status and data. You can use Packer to create custom ECS images from your OS images. With the Docker image migration tool, container images can be automatically migrated to container image repositories on Alibaba Cloud. With the help of Velero, your Kubernetes application configurations can be seamlessly migrated to Alibaba Cloud Container Service for Kubernetes. In addition, Data Transmission Service (DTS) can help you transfer data seamlessly.
With the advancement of 5G and IoT technologies, using traditional cloud and on-premises data centers for centralized storage and computing can no longer meet current demands for validity, capacity, and computing power. Cloud-native technologies, however, can meet these demands, and deliver cloud computing capabilities to user clients and edges, and implement unified release, O&M, and management from a governance center. This is next step in the evolution of cloud computing.
To achieve all of this, Container Service launched ACK@Edge to support the unified management of clouds and edges. This product also supports unified application releases, which can help improve the release efficiency by up to 300%. Moreover, edge deployment can efficiently shorten network latency by 75%. ACK@Edge supports unitized isolation and automatic reconnection. It also provides sandboxed-containers for you to deploy untrusted third-party applications on edges.
Now let’s learn how Youku completed its architecture evolution based on ACK@Edge. Youku is one of biggest online video hubs in China. As Youku expanded its businesses to hundreds of cities, the centralized architecture in its original on-premises data center could no longer keep pace with the fast growth of its business. Youku needed to upgrade its centralized architecture to an edge architecture to be able to cope.
Youku needed to find a new approach to manage on-premises data centers deployed in tens of Alibaba Cloud regions and near 1,000 edge nodes. Youku chose ACK@Edge to centrally manage ECS instances and edge nodes, release applications, and perform auto scaling. Elastic scaling has reduced the server costs by 50%. Moreover, after the new architecture was adopted, the video playback chain was removed from the public network. A new chain from the backbone network, through edge nodes, and to clients was created. This reduced network delay by 75%.
Now let’s go back to the third question: how can you manage work of upgrading and maintaining large amounts of nodes in Kubernetes clusters. At Alibaba Cloud, we think that the serverless architecture can be used to resolve this issue and help enterprise reduce operations and maintenance costs.
In 2018, Container Service released Serverless Kubernetes version 1.0. Users no longer needed to manage Kubernetes workers. Nor did they need to focus on the environment configuration of nodes, server management, maintenance, or upgrades. This change meant that customers could drastically simplify the operations and maintenance of Kubernetes clusters, while also improving their overall application development efficiency. In this way, no capacity management is required and no security risk is involved in the process either.
Today, Alibaba Cloud’s Container Service has already launched Serverless Kubernetes version 2.0, which means that the public preview of Container Service is already over with and the service is now a paid service. Serverless Kubernetes 2.0 provide major upgrades in terms of the compatibility, security, and elasticity of Kubernetes. In terms of security, solutions such as multi-namespace, role-based access control security models, and frameworks such as Istio and Knative are supported. Serverless Kubernetes 2.0 can be defined as being a serverless architecture that can provide the best compatibilities that you can achieve with a Kubernetes deployment in the industry. And, in terms of elasticity, Serverless Kubernetes 2.0 supports GPU instances and can start 500 pods within less than 50 seconds.
Currently, Serverless Kubernetes is widely used in several different scenarios across a variety of industries, such as job management and online scalability to help users embrace the application-centric nature of the cloud-native architecture.
What’s more? We have built a Serverless Framework based on Serverless Kubernetes. This Serverless Framework simplifies the work of handling events, compiling code, and deploying services. It also seamlessly integrates with other Alibaba Cloud application services, such as Message Service and Log Service, and provides improved observability. Enterprises can build their own serverless products for a variety of workflows including application, container, and function development. All of this aims to help enterprises build the next-generation serverless applications.
As cloud-native architecture continues mature, at Alibaba we are hoping to partner with other enterprises and service providers to help contribute towards building an open cloud-native ecosystem.
In the past, the team at Container Service has actively participated in and contributed to several cloud-native communities, and it still does today. Contributions include Moby and Kubernetes. Container Service has become a platinum member of Cloud Native Computing Foundation (CNCF). Alibaba engineer, Li Xiang is the only Chinese member of the CNCF Technical Oversight Committee. Alibaba Cloud Container Service is a member of Open Container Initiative (OCI) and a board member of Cloud Native Industry Alliance (CNIA). Alibaba Cloud Container Service is qualified by Certified Kubernetes Conformance Program, and also certified as Kubernetes Certified Service Provider (KCSP).
In addition to the open-source and cloud-native communities, we are also committed to building an ecosystem for global partnership. In 2019, some new members joined this ecosystem. Based on the open-source project Gardener, SAP Cloud Platform now supports Alibaba Cloud Container Service for Kubernetes and empowers enterprises by enabling them to manage a large number of clusters in hybrid cloud.
As running AI applications on Container Services becomes increasingly popular, Seldon, an open-source machine learning service provider from the UK, has also started to provide cloud-native AI model inference services. Container Service for Kubernetes is now supported in Cloud Brain, a technology developed by Click2Cloud in India. Their service provides a complete solution for enterprises to transfer traditional applications to cloud-native applications. Banzai Cloud, a container platform vendor from Europe, has a set of hybrid cloud and Istio products. Their pipeline products already support Container Service, enabling customers to create and manage container clusters from different cloud service providers with the lowest costs.
This year, we launched our level-1 container applications in the Alibaba Cloud market. We hope that we can empower enterprises with cloud-native technologies. Developers of cloud-native products can easily find Alibaba Cloud-certified and standard container ecosystem products, including open-source and free-of-charge container products and also for-purchase container products. These products can be quickly used on clusters to meet the business requirements of several different industries. For our independent software vendors, they can use standardized transaction procedure and a myriad of customer resources to simplify the pre-sales, transactions, delivery, and after-sales processes.
The following vendors and enterprises will join the container application market as Alibaba Cloud’s partners:
Last but not least, let’s review the evolution of cloud-native Container Service for Kubernetes version 2.0 and its future. At Alibaba Cloud, it is our vision to work together to build the new foundation, new computing compatibility, and new ecosystem in this cloud-native era.
To build the new foundation, Container Service will serve as an infrastructure in all scenarios to provide an end-to-end security architecture and support global deployment. A single cluster can support up to 10,000 nodes. ACK 2.0 also launched the application-centric hybrid cloud 2.0 architecture. It can reduce network latency by 75% and improve the release efficiency by three times.
To forge the new computing capability, ACK 2.0 supports extremely fast elastic scaling, which can expand a cluster to 1,000 nodes within several minutes. It also supports heterogeneous computing. And with enhancements to task scheduling, your utilization of resources can increase by 500%. Sanboxed-containers will be used to enhance application and container isolation. Meanwhile, ACK 2.0 can still maintain a performance equivalent to 90% of that by using RunC.
To build a new ecosystem, we intend to have the Container Service team work with cloud-native developers and other enterprise partners to continue to explore more of the future of cloud native technologies.
Of course, we would also like to thank our customers and various enterprise-level partners. We couldn’t have achieved what we have already without your help.
The defining moment has come. Let’s make cloud native technologies lead us to a new generation of digital transformation.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/clouddon/cloud-native-transformations-sling-tv-11273fe92705?source=search_post---------270,"There are currently no responses for this story.
Be the first to respond.
About the Podcast Series
We hear from end users on their journey through their cloud native transformations — their approach, goals, choices they made, challenges they faced and more. If you would like to participate in the podcast series, please contact us.
About this Episode
In this episode, we hear from Brad Linder, Director of Cloud Native Engineering, Sling TV on their journey of cloud native transformations. Thank you for your time and insights, Brad!
Summary
Sling TV is the current leader in the live streaming television market and has around 2.3 Million customers. Their key challenge is to provide live TV content over the internet to millions of customers across variety of devices which are customer provided and owned. They were able to enable CI/ CD at scale for this primary use case and one click deployment by adopting cloud native technologies over a period of 18 months.
Cloud Native Transformations at Sling TV
1. Primary use case is to deliver a personalized, highly available meta data service layer to an extremely elastic user population over the Internet at web scale.
2. Journey of over a period of 18 months, meeting the key goals of CI/CD at scale, common cloud native tools across the organization and one click deployment.
3. Leveraged existing co-lo data centers and skill sets on infrastructure and developed skill sets needed for cloud native technologies while setting the stage for hybrid cloud.
4. Re-architected monolithic applications in to cloud native patterns / microservices on a case by case basis. Some applications were not re-architected to be cloud native as they were better suited to run in their current architecture.
5. Leverage a common set of tools and processes to provide web scale for “free” for application development teams while maintaining operational efficiency.
Approach
1. Work across the organization to get to a cloud native world.
2. Sling TV considered different platforms before standardizing on a Rancher Kubernetes implementation in order to provide consistency across different environments while providing the best experience possible to their customers.
3. Partner with key stakeholders from the C level to the individual contributors to help them along the journey to become cloud native. Folks had to be trained up on new technology and paradigms in order to transform.
Recommendations
1. Use the right technology for the use case in front of you, not everything should become “cloud native”
2. Allow folks to understand the new tech in their own way, people will be your largest challenge in this journey
3. Do what is right for your use cases and business, do not just implement a technology or paradigm to say you did.
4. Copy-Pasting reference architectures do not work; apply cloud native best practices to your specific use case and organization. Whatever you implement must make sense to your culture, business and key strategies.
5. Enable a technology for the entire organization to use in a common way. This will enable a more robust and resilient solution for the organization as a whole and save you a lot of time, pain and money in the long run.
You can read more about Sling TV’s use case here.
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-does-oam-benefit-cloud-native-applications-d09c12bdafe0?source=search_post---------271,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 14, 2020·7 min read
By Sun Jianbo (nicknamed Tianyuan), Alibaba technical expert
Open Application Model (OAM) is a standard specification for building and delivering cloud native applications co-launched by Alibaba and Microsoft in the community. It aims to use a new application definition, O&M, distribution, and delivery model to promote the progress of application management technology towards lightweight O&M. Also, it strives to kick off a technical revolution in next-generation cloud native DevOps.
Today, I will discuss the value of OAM, that is, why we should use OAM as a standard model for cloud native applications.
In December 2019, AWS released ECS CLI v2, the first major version update in the four years since the release of v1 in 2015. The release of the v2 CLI focuses more on the end-to-end application experience, that is, managing the entire application delivery process from source code development to application deployment.
Based on user feedback received over the years, they summarized four CLI development principles:
As you may notice, these principles are not so much the development principles of ECS CLI as the demands of users:
Now, let’s look at how OAM meets each of these demands and the value it provides.
As shown in the following figure, an application configuration that is running OAM uses Kubernetes API specifications, which contain all the resources of an application.
OAM application definitions do not limit your underlying platform and actual runtime. You can run the application on a platform other than Kubernetes, including ECS, Docker, and FaaS (Serverless). This means you are not locked into a single vendor. If your application meets the conditions of a serverless application, the OAM description of the application can naturally run in a serverless runtime environment that supports OAM specifications.
In different environments that support OAM, you can use a unified application description to achieve identical application delivery. As shown in the following figure, users only need to describe the unified application configuration to achieve the same application experience in different environments.
The popularity of cloud native has pushed forward the implementation of infrastructure as code. As an infrastructure platform, Kubernetes uses declarative APIs. As a result, users are accustomed to describing required resources through YAML files. This is actually the implementation of infrastructure as code. In addition, OAM provides unified definitions of infrastructure resources that are not included in the native Kubernetes and uses the infrastructure by configuring the OAM standard YAML (code).
Today, the OAM implementation of the Alibaba Cloud resource orchestration product ROS provides a classic example. You can use OAM configurations to pull up infrastructure resources in the cloud.
Let’s look at a real-world example. Assume that we want to pull up a NAS persistent storage, which contains two ROS resources, which are NAS FileSystem and NAS MountTarget.
In the definition, you can see that NAS MountTarget uses FileSystemId that is output by NAS FileSystem, and ultimately, this YAML file is translated by ROS OAM Controller into the template file of the ROS resource stack. In this way, the cloud resources are pulled up.
The OAM implementation of ROS will soon be made open source, so stay tuned!
User requirements concern the application architecture, rather than the specific infrastructure resources that are used.
OAM addresses such requirements by describing the WorkloadType of an application to define the application architecture. This WorkloadType can be a simple stateless application “server”, which means that the application can be replicated, accessed, and persistently run as a daemon. Also, the WorkloadType can be a database-type application “RDS”, which starts an RDS instance in the cloud.
The “Component” user component selects a specific architecture template by specifying “WorkloadType”, and multiple Components constitute a complete architecture.
When using OAM application definitions, users are concerned with the architecture, not the specific infrastructure.
As shown in the following figure, an OAM application description allows you to specify that the application requires a public network access capability instead of a Server Load Balancer (SLB). In this case, you can specify that the components of the application are database components.
Users want O&M capabilities to be part of the application lifecycle, and OAM does exactly this. To define the O&M capabilities used by a Component, you can bind a Trait. In this way, O&M capabilities are added to the application description to facilitate the central management of the underlying infrastructure components.
As shown in the following figure, an application contains two components: a web service and a database. The database component is able to back up data, while the web service can be accessed and elastically scaled. These capabilities are managed by the OAM interpreter (the OAM implementation layer) in a centralized manner. This eliminates the need to worry about any O&M capability binding conflicts.
Just as Docker images resolve long-standing inconsistencies between development, testing, and production environments, unified and standardized OAM application descriptions make the integration of different systems transparent and standardized.
OAM also decouples the originally complex Kubernetes All-in-One APIs at a certain level and divides them into three main roles: application R&D (management Component), application O&M (combines Components and binds Traits to the AppConfig file), and infrastructure provider (mapping OAM interpretation capabilities to actual infrastructure components). With these three roles, workload is divided among them to simplify the duties of the individual roles. This gives different roles greater focus and specialization.
OAM application definitions are elastic and scalable. You can define different types of workloads by extending “Workload”. You can also describe your O&M capabilities by using custom Traits. In addition, both of these objects can be perfectly integrated with the CRD + Operator extension methods in existing Kubernetes systems.
Through this separation of focus, OAM divides applications into three layers: R&D, O&M, and infrastructure. It also provides modular collaboration for R&D Workloads and O&M Traits, significantly improving reusability.
As the number of modular Workloads and Traits increases, a market for these components will be formed. On this market, users will be able to quickly find architectures (Workloads) suitable for their applications and the required O&M capabilities (Traits) in CRD Registry and other similar registries. This makes building applications effortless.
I believe that, in the future, application construction will become increasingly simple and infrastructure choices will be automatically matched to users’ architecture requirements. In this way, users will be able to truly enjoy the benefits of cloud platforms and quickly reuse existing modular capabilities. Going forward, OAM will become the inevitable choice for cloud native applications.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/what-does-the-future-hold-for-next-generation-cloud-database-technology-in-the-cloud-native-era-c92e9075b9f4?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 28, 2019·10 min read
We are now in an all-cloud era, full of new technologies, innovations, and challenges. More importantly, we now face some important questions that can redefine the way we deal with database technologies. What reforms will be made in the database market in this era? How can cloud service providers offer more efficient and cost-effective database solutions to help more enterprise users seize opportunities presented by cloud migration?
In the database session of 2019 Alibaba Cloud Summit held in Beijing, Feifei Li, Vice President of Alibaba Group, Chief Database Scientist of Alibaba DAMO Academy, and Head of Database Business Group of Alibaba Cloud Intelligence, gave an insightful presentation on the next-generation cloud-native database technologies and the challenges they face.
According to a database market analysis report released by DB-Engine in January 2019, relational database products are still dominant in the database market. Meanwhile, more market segments, such as the graph database, document database, and NoSQL database segments, are forming in the database market. Another trend in this market is the continuous decline of the market shares of the traditional commercial database giants Oracle, DB2, and Microsoft SQL Server. By contrast, open-source and third-party database market shares keep expanding.
After over 40 years of evolution, database technology is still developing vigorously. Cloud computing vendors have reached a consensus that databases are an important component in the connection of IaaS and intelligent cloud applications. Therefore, the vendors need to improve their capabilities throughout the entire data lifecycle, including data production, storage, and consumption, enabling users to connect IaaS and intelligent applications.
Thanks to the constantly developing database technology, we now have online transaction processing (OLTP) systems to record real-time transaction data and online analytical processing (OLAP) systems to analyze massive amounts of data in real time. OLTP and OLAP systems require the support of database services and management tools. Given these circumstances, NoSQL database solutions have been developed to store semi-structured and non-structured data.
From the late 1970s to early 1980s, relational databases came into being, and later the SQL query language and OLTP systems were developed. The explosive growth in data volumes and the demand for complex data analysis gave rise to data warehousing, as well as OLAP, extract-transform-load (ETL), and other data processing technologies. With the continuous increase of multi-source heterogeneous data, such as graphs, documents, spatial-temporal data, and time series data, non-relational NoSQL and NewSQL database systems have also emerged.
Traditional databases typically use a single-node architecture, whereas cloud-native databases usually use a shared storage architecture. Alibaba Cloud PolarDB establishes a shared storage architecture over a high-speed network. This architecture separates storage from computing to enable the fast scale-out of computing nodes. In addition, PolarDB allows for the rapid scaling of storage and computing capabilities based on the actual needs of customers. Customers can use this shared storage database to complete a non-intrusive data migration without any change to the original business logic.
In addition to the cloud-native shared storage technology, a distributed architecture is required to handle highly concurrent access to massive amounts of data. For example, Alibaba is exploring the use of a distributed architecture to cope with the challenges posed by Double Eleven every year. Also, Alibaba Cloud wants to provide different query interfaces, such as SQL, to support queries of data in multiple models and states. Concerning the storage system, Alibaba Cloud hopes to allow users to store their data in different locations and use a unified interface like SQL to query all types of data. Alibaba Cloud Data Lake Analytics (DLA) is a cloud-native technology developed for this application scenario.
Traditional solutions isolate read and write conflicts by using the OLTP system to process transactions and the OLAP system to analyze huge volumes of data. In the cloud native era, Alibaba Cloud will minimize the cost of data migration by taking advantage of the technical benefits delivered by new hardware devices. This can be done by integrating transaction processing and data analytics features in one engine so that these two needs can be addressed seamlessly by one system.
Alibaba Cloud serves a large number of enterprises, which use our cloud resource pools based on a virtualized architecture that separates storage from computing. Therefore, we need to monitor and schedule all off-premises resources in an intelligent way to quickly respond to customer requirements and deliver optimal service quality. To achieve the necessary intelligence, we need to use machine learning and AI to enable automatic sensing, decision-making, recovery, and optimization in all sectors, including data migration, data protection, and elastic scheduling.
Another important technology required in the cloud native era is integrated hardware and software design. New hardware devices are providing many new technologies that will benefit database systems, such as RDMA networking, SSD, NVM, GPU, and IPG acceleration. The Alibaba Cloud PolarDB shared storage service uses a remote direct memory access (RDMA) network to provide access to remote database nodes as fast as local node access.
Many Alibaba Cloud customers require high availability for their financial business systems. Alibaba Cloud databases run high-availability protocols and use a three-replica architecture to implement seamless real-time switching among local databases. In addition, disaster recovery can be implemented by using remote databases based on different customer requirements. The remote databases synchronize data by using Binlog technology to guarantee high availability of financial business. As cloud users attach great importance to data security, the Alibaba Cloud database service provides data encryption as soon as data is written onto disks, safeguarding data throughout its entire lifecycle.
Alibaba Cloud provides a series of database service tools, including data backup, data migration, data management, hybrid cloud data management, and intelligent diagnosis and optimization systems. These tools help customers quickly migrate data to the cloud and offer them a hybrid cloud solution.
Core engine products of the Alibaba Cloud database service not only include our proprietary products but also third-party and open-source products. We hope to offer more options with commercial databases and open-source products. Also, we will make use of the technical benefits of cloud computing in our proprietary database products and do more research to help customers eliminate the difficulties and problems they encounter in the application of third-party or open-source databases.
The core OLTP products of the Alibaba Cloud database service are PolarDB and its distributed edition, PolarDB X. In addition, Alibaba Cloud offers mainstream MySQL, PostgreSQL, and SQL Server database systems, along with a series of Oracle-compatible services such as PPAS.
Our core OLAP products include AnalyticDB, DLA for multi-source heterogeneous data, and Time Series and Spatial-Temporal Database (TSDB) for IoT applications. For NoSQL users, Alibaba Cloud offers a wide range of third-party database products, such as HBase, Redis, and MongoDB, as well as Alibaba Cloud Graph Database (GDB).
The Alibaba Cloud database management platform and full-link monitoring service provide intelligent data monitoring and analysis during the entire lifecycle, enabling Alibaba Cloud databases to provide optimal service. Alibaba Cloud builds a hybrid cloud data storage link that integrates online and offline data storage. During migration to the cloud, customers can choose Alibaba Cloud Data Transmission Service (DTS) for real-time data uploading and synchronization. After the migration, customers can use PolarDB or other cloud-native database products to store data, or use DLA, AnalyticDB, or other analytics products to analyze data. Data analysis for specific scenarios can be implemented by using the document database, graph database, or TSDB solution. Customers can use the DTS system for data synchronization and backup between online and offline databases, and use the Hybrid Cloud Database Management (HDM) system to manage all databases. In addition, the Alibaba Cloud database service provides a database management suite that allows customers to manage and develop databases, improving the efficiency of database management and development.
Here, we will introduce two cloud-native database products: PolarDB and AnalyticDB.
PolarDB provides efficient shared distributed storage over an RDMA network. The shared storage technology establishes a one-write-multiple-read model among multiple computing nodes. In this model, multiple read-only nodes can appear quickly, adapting to the actual workload to complete computing during peak hours. The shared storage technology also allows for the rapid scaling of storage nodes. Customers can pay for an appropriate amount of PolarDB resources based on their application scenarios and the peak and trough data volumes of their business. This billing model significantly improves the database use efficiency and reduces the cost. In general, PolarDB is a super MySQL database. We will release PolarDB versions compatible with PostgreSQL and Oracle in the future.
In some scenarios, users are faced with the challenges of high concurrency and massive data access needs that exceed the capacity of shared storage. The distributed PolarDB X system uses the sharding partition solution to achieve unlimited scale-out for the storage capacity. PolarDB X will undergo public beta testing later and is currently available for trial use.
Read and write conflicts may occur during the analysis of a massive amount of data. Reading and analyzing a huge amount of data is a complex process. Therefore, we recommend that you use AnalyticDB, the real-time interactive analytical database system of Alibaba Cloud. AnalyticDB features a high writing throughput and provides a storage engine for row- and column-based storage. Therefore, it is capable of real-time interactive analysis and shows excellent performance in its quick response to highly concurrent access to massive amounts of data. Due to its compatibility with MySQL, AnalyticDB can import data directly from MySQL databases, allowing it to process a query of tens of billions of data entries in a matter of milliseconds and write data at millions of TPS.
In addition to core cloud-native database products, we also offer multiple database tools, such as DTS. DTS solves data transmission issues for non-cloud customers. After these customers migrate to the cloud, DTS implements real-time data synchronization between their databases off-premises and on-premises or from TP to AP systems. DTS provides efficient synchronization of incremental data to guarantee real-time data consistency. In addition, DTS provides a data subscription capability and can connect to more data sources through different protocols and interfaces.
Alibaba Cloud GDB is a new database product currently in public beta testing on the Alibaba Cloud website. It is a real-time reliable online database that supports property graph models and processes highly linked data query and storage using many cloud-native technologies, such as separated storage and computing. Similar to mainstream graph database products, GDB supports standard graph query languages and is compatible with Gremlin syntax. Another key feature of GDB is its support for real-time updates and OLTP data consistency. This allows GDB to guarantee data consistency during the analysis and storage of huge amounts of property graphs. GDB has all the features of a cloud-native database product, such as high service availability and easy maintenance. Its typical scenarios include social networking, financial fraud detection, and real-time recommendations. In addition, GDB supports knowledge graphs, neural networks, and other network models.
The Alibaba Cloud database team strives to offer enterprise-grade cloud-native database services. Based on a full range of controllable proprietary technologies, our team offers various services, such as quick cloud migration, centralized management of off-premises and on-premises data, and data security. For example, we have deployed Alibaba Cloud databases in Hangzhou and other cities to support complex applications such as City Brain. Such applications require the storage of both structural and non-structural data and pose major challenges to OLTP, OLAP, and tool products. Alibaba Cloud AnalyticDB, PolarDB, and DTS use cloud-native technologies to seamlessly support these applications.
To give an example, PolarDB went through public beta testing in August 2018 and was put into commercial use at the end of 2018. So far, PolarDB has seen rapid growth on public cloud platforms. The cause of this rapid growth is Alibaba Cloud’s desire to help customers eliminate their difficulties. We do not use new technology to create new needs. Instead, we develop technology to address the current needs of our customers. PolarDB features cloud native elastic storage and computing in minutes, high cost-effectiveness, flexible Pay-as-You-Go billing, and highly concurrent request processing capability. It supports fast scale-out of read-only nodes, provides a large capacity, and uses a distributed shared storage architecture to create an experience similar to single-node database access. In addition, it does not intrude into the business logic of customers and is highly compatible with MySQL.
AnalyticDB is a real-time interactive analytical system. Data produced locally or obtained from a big data system can all be migrated to an AnalyticDB cluster by DTS. Then, the data can be used for business analysis, visual presentation, and interactive query. AnalyticDB supports join queries on several hundred tables, delivering millisecond-level query services to users. Today, many users are using cloud-native databases like PolarDB and AnalyticDB on Alibaba Cloud. Cloud-native databases are helping them eliminate difficulties in their applications and realize greater business value.
In summary, a series of new technologies and challenges have emerged in the cloud native era. Facing these challenges, we must consolidate database kernel products, management platforms, and database tools to deliver the most efficient solutions with the highest commercial value. Alibaba Cloud sincerely invites you to experience our products and technologies and hopes to help more customers solve their problems. We also hope more developers and partners in the ecosystem will develop in-depth solutions for specific industries and fields based on our database services and products. Let’s work together to increase the prosperity of the database market in the cloud native era.
Reference:https://www.alibabacloud.com/blog/what-does-the-future-hold-for-next-generation-cloud-database-technology-in-the-cloud-native-era_594715?spm=a2c41.12820564.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@chrisshort/devopsish-039-cloud-native-football-kubernetes-serverless-and-more-a92efc6911aa?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Short
Sep 3, 2017·4 min read
Football is back and I couldn’t be happier about it (despite my Gators losing to Michigan). I know that sports and DevOps-types don’t always go together. However, there are a lot of things we tech folks can learn from football. Systems and process are what great football teams are built on. Take a good look at Nick Saban’s process focus at Alabama for an idea of how to build a perennial championship contender (tl;dr: it’s a one step at a time approach). When you take a look at Bill Belichick’s “Do Your Job” mantra, you realize that is what DevOps is all about; owning your assignments and executing them. Many times in DevOps the things that traditional development and operations teams can’t execute well on are done without question and executed quite well. Then they are iterated upon mightily like it’s second nature. Ownership of the pipeline and process is a big deal in DevOps. It’s a truly worthwhile comparison to explore when you have the time. Football and DevOps go together more than you realize.
GoCD is a continuous delivery tool specializing in advanced workflow modeling and dependency management. It lets you track a change from commit to deploy at a glance, providing superior visibility into your workflow. It’s open source, free to use and download. SPONSORED
September 7 and 8, Raleigh, NC USAWhat do Ashley McNamara, Nimal Mehta, Chris Short, and the legendary John Willis all have in common? They will all be speaking at DevOpsDays Raleigh this year! Receive $5 off with discount code MEETUP5 when registering. Website | Registration
October 23 and 24, Raleigh, NC USA2,500–3,000+ technologists will descend upon the City of Oaks to attend 200+ sessions from nearly as many speakers. Representative from nearly every major technology company in the U.S. will be in attendance as well.To get 20% off enter code DevOpsish20 when registering to attend.Website: https://allthingsopen.orgTo Register: https://allthingsopen.org/register-now
DevOpsDays Raleigh 2017 Attracts Top Talent: Some of DevOps’ elite will journey to Raleigh to share their wisdom and experiences from the trenches
Most TV computer scientists are still white men. Google wants to change that. Women of color are practically invisible in these roles. You would have to watch more than 85 hours of popular TV shows and movies to see a single instance of a Latina, black or mixed race female character discussing or engaging in computer science and, even then, you wouldn’t see a single Middle Eastern girl or woman, USC said.
For diversity in tech, leaders must be intentional, tech community says: Changes will not happen by chance.
Conquer your infrastructure in the cloud: Friend of DevOps’ish, Kris Nova, along with Justin Garrison have published an O’Reilly book; Cloud Native Infrastructure.
Why I’m Not Ditching Cloudflare: It wasn’t an easy decision but time, bugs, unprecedented events, and reason all played a factor.
The Web in 2050: A Orwellian tale of what the web will look like if we don’t do something soon.
Tech companies declare war on hate speech — and conservatives are worried: In light of Charlottesville, Silicon Valley revisits its absolute approach to free speech.
Mapping The Journey Episode 8: Interview with Ryan Dahl, Creator of Node.js
how kubernetes stole docker swarm’s share of voice: CNCF Backing, Domain Expertise, and Strong Developer Advocacy.
Get real on container security: 4 rules DevOps teams should follow
Google Shuts Down the Internet in Japan… by Mistake
A Cloud Native Series From Joe Beda: Joe Beda dives into the definition of Cloud Native, practical considerations of applying Cloud Native, how Cloud Native relates to DevOps, Containers and Container Clusters, how Cloud Native enables microservices, and how Cloud Native creates both problems and opportunities in the security domain.
The evolution of DevOps: Understanding the impact and expanding influence of DevOps culture, and how to apply DevOps principles to make your digital operations more performant and productive.
Distributed Systems Are Hard: Forget Conway’s Law, distributed systems at scale follow Murphy’s Law: “Anything that can go wrong, will go wrong.”
The 7 habits of highly effective DevOps: Most people struggle to communicate about DevOps culture and desired results.
LogDevice: a distributed data store for logs from Facebook
Building a Custom DataDog Agent Check
Managing Kubernetes Applications with HashiCorp Terraform
Are Containers and Serverless Computing Set to Battle for Cloud Control?
Kelsey Hightower has added CRI-O containers to Kubernetes the Hard Way
The Cloudcast #309: Secrets Management for Microservices
Four Scalability and Performance Wins after etcd3 Migration
DevOps/Cloud Systems Engineer — StockX — Detroit, MIStockX is looking for an experienced Cloud Systems Engineer, or DevOps Engineer, to accelerate the efficiency of our engineering operations and implement scalable infrastructure. You must be passionate about automating all the things that go into software engineering and technical operations.
Director, Software Engineering — Bankrate — Detroit, MIA manager in an agile environment with strong technical skills. Collaborative leader who will understand our people, business, technology, and customers’ needs. Leader who helps drive our culture of empowerment, ownership, and accountability to help develop our team technically and professionally. Identify and successfully implement improvements in our engineering processes, tools, and architecture to enable us to deliver on our key initiatives.
Red Hat Ansible | CNCF Ambassador | DevOps | opensource.com Community Moderator | Writes devopsish.com | Partially Disabled USAF Veteran | He/Him
Some rights reserved

Red Hat Ansible | CNCF Ambassador | DevOps | opensource.com Community Moderator | Writes devopsish.com | Partially Disabled USAF Veteran | He/Him
"
https://medium.com/@alibaba-cloud/zero-trust-security-part-3-zero-trust-security-with-cloud-native-microservices-and-containers-75961aa2e65e?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 20, 2021·5 min read
By Raghav K.
Enterprise networks have their share of complexities and advantages. In a distributed setting, there are different factors to counter and leverage. Networks have faced infiltration attempts for years. Barricading as a preventive measure helped keep networks and resources secure. While that was not a comprehensive approach to maintaining security, infrastructure integrity, and solutions, it was the only line of defense against malicious attacks.
After the introduction of cloud-native, containers based on Kubernetes have helped enterprises achieve their true potential. Cloud-native microservices have provided many advantages for building large applications that require a faster development to deployment cycle alongside innovation. Microservices swapped the traditional monolithic application approach for a simpler and more agile one.
Zero-trust security helps ensure a highly-secure environment and a scalable security solution that can ease your way out while managing the growing number of microservices. It can also help simplify the complexities related to SDLC. Microservices may sound like a security hassle, but it provides a much easier approach to secure an entire application. You can achieve superior system security after implementing the zero-trust security model with microservices.
There is a representation of the microservices classification on the Alibaba Cloud platform listed below:
Traditionally, the security created fencing for resources, and everything inside was trusted automatically. In a traditional approach, resources can access each other without much resistance since they share the same primary codebase. However, microservices architecture is based on a more loosely coupled approach. This ensures that different resources can only be called using APIs, making the system more secure.
I am not insinuating that the traditional methods of putting up barriers like firewalls should be retired. These security solutions are still useful, but relying on the traditional security approach is an outdated idea. Everything is evolving, and security has evolved using models like zero-trust.
Whether it is the microservices architecture or containers based on Kubernetes, the zero-trust security model should be implemented for an up-to-date and comprehensive security structure. Service Mesh has emerged as an option to provide architectural stability when using Kubernetes for containerization.
The Alibaba Cloud Service Mesh (ASM) architecture is listed below:
Alibaba Cloud Service Mesh is a fully-managed service that is highly compatible with the open-source lstio service mesh. Service Mesh is designed to manage services easily.
You can use Service Mesh with microservices applications since it is deeply integrated with the Container Service for Kubernetes (ACK). You can manage traffic for microservices across multiple Kubernetes clusters using Service Mesh. It provides consistent communication control over containerized applications.
The deep integration with Alibaba Cloud ACK, networks, and security capabilities helps Alibaba Cloud ASM enable zero-trust security with containers and microservices. Alibaba Cloud ASM can extend superior traffic shaping and observability for each microservice while creating a consistent service mesh on the cloud. This enables a deeper analysis of services and protocols to ensure no entity is left out when implementing the zero-trust security architecture.
Implementing security capabilities using Service Mesh is the introduction to zero-trust capabilities with your microservices architecture and containers. It is highly recommended to introduce the zero-trust security model as a design parameter rather than just retro-fitting it to existing systems. However, for environments that are pre-cloud native, migration might be an overload. Alibaba Cloud Service Mesh offers great migration scenarios to counter that scenario.
You can use Alibaba Cloud ASM to deploy and configure ASM instances to migrate existing applications to Alibaba Cloud. ASM offers a seamless migration scenario based on real-world transmission capabilities. Migration from a data center to the cloud is not only about application migration. Alibaba Cloud ASM allows you to dynamically route the incoming traffic to data centers or the cloud resources to migrate stateless services.
The Alibaba Cloud ASM migration architecture is listed below:
Implementing the zero-trust security architecture is all about controlling access using identity. If we talk about real-world scenarios, trust is the only thing that can be leveraged to gain access to an entire system. The most critical and sophisticated systems can be crippled in minutes due to the monolithic security approach. The zero-trust security model cuts down on automatic trust, ensuring greater security and stability for your microservices and containers.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@cote/devops-hustlers-cloud-native-ops-amazon-whole-foods-cot%C3%A9-memo-issue-31-88009c12ec98?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
Coté
Jun 30, 2017·4 min read
We had a two week vacation over at Gulf Shores, Alabama. Despite three days of rain, it was great. If you’re looking for a Gulf-beach vacation, it’s an excellent place to go — just about 10 hours from Austin if you drive fast and don’t have to stop a lot for kids to go pee. Well, and adults with tiny bladders too.
How to avoid getting hoodwinked by a DevOps hustler — www.theregister.co.uk
My column at The Register this month, how to find and rate a good consultant. Also, I finally get labeled a pill-popping stoner in the comments (FAKE NEWS!)
Software Defined Talk Episode 97: The novel strategy of making money, and investing to do so — Amazon + Whole Foods — www.softwaredefinedtalk.com
Looks like we’ll be getting cheaper organic food what with Amazon buying Whole Foods. What exactly is the strategy at play here, though? Other than the obvious thing of doing online groceries, how is Amazon advantaged here such that others (like Wal-mart), can’t simply do this themselves. We go over these questions and how they related to M&A in general. Plus recommendations and some podcast meta talk.
Amazon buying Whole Foods — Notebook — cote.io
If you want more highlights, context, and commentary on Amazon buying Whole Foods, check out my notebook on the topic.
Pivotal Conversations: Cloud-native Ops, with Tony Hansmann — soundcloud.com
Operating IT in a cloud-native mindset requires changes up and down the stack, especially in operations. The degree of automation in the stack changes the need for much of the manual work and process-driven checks and balances in IT ops as we know it. In this episode, we talk with Pivotal’s Tony Hansmann on what those changes are, how the technology pushes these changes, and some of the barriers.
New poll of rural Americans shows deep cultural divide with urban residents — www.washingtonpost.com  Big-city and small-town Americans have vastly different views on issues like immigration and religion, helping to explain President Trump’s election
Provençal Tian — www.seriouseats.com
I made this recently and it’s fantastic. It’s got very little in it, and while it takes a long time to brown all the stuff (Eggplant, Zucchini, Squash, and Tomato Casserole), it’s not tedious and well worth it.
Digital Magic: Disney’s DevOps Transformation — The New Stack — thenewstack.io
Summary of Disney’s experience doing the DevOps, with some good management-level ways of thinking listed at the end.
Podcast market estimated at over $220m — cote.io
A little round-up of podcast market-sizing.
Good intentions & indolent portfolio management lead to legacy quicksand — cote.io
A nice overview of the slippery slope of good intentions that lead to legacy holding you back.
In finance, large banks seem to be fast followers, not disruption victims — cote.io
More for the file: banks, generally know what they’re doing with all that digital stuff.
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
See all (153)
I work at Pivotal. I’ve worked in strategy & M&A at Dell, as an analyst at 451 Research & RedMonk, & was a developer before that. See @cote and http://cote.wtf
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/7-major-trends-for-cloud-native-in-2020-introduction-f34b7b4c476d?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 19, 2020·5 min read
Over the past few years, cloud native technologies and concepts have received wide acceptance. In this rapidly developing field, predicting the future is particularly difficult. However, we firmly believe that the cloud native field, supported by open innovation, will continue to reshape the software lifecycle and provide ever-increasing value.
In 2019, among many popular technology trends, cloud native attracted a high degree of attention. Many developers are highly interested in this serious of emerging technologies, and many enterprises have begun to explore cloud native architectural transformations. In China, developers have transitioned from a focus on the concept of cloud native to a focus on cloud native implementation practices.
While preparing for Alibaba Cloud’s first cloud native practice summit (page in Chinese), we explored the applications of and research into cloud native technology. As part of it, we invited 17 cloud native technical experts to review the developments in the cloud native field in 2019 and to describe the state of cloud native technology as we enter the new decade.
Looking ahead to 2020, we expect these landmark events to take place in the application and research of cloud native technologies.
First, the focus of cloud native technology will move up to serverless and application management.
Over the past few years, we have seen that the focus of cloud native technology has been on containers and container orchestration. The success of Docker and Kubernetes is almost synonymous with cloud native. Many people even say that Kubernetes is becoming boring. In this context, we can see that the focus of cloud native technology is about to move up:
Second, cloud-native technology will become the base from which cloud service providers will pursue innovation and competitiveness.
As container-based cloud native technology becomes widely accepted by users, containers will soon be the basic interface between clouds and users. Therefore, for cloud service providers, containers, microservices, serverless, service mesh, and other new cloud native technology fields will definitely serve as the bases from which cloud providers will innovate and compete in the future.
Virtualization will be the source of incremental cloud resource growth in the next three years. However, the combination of bare metal and security sandbox containers accelerated by hardware virtualization is accelerating enterprise cloud migration and containerization. The key to the future technological competitiveness of cloud providers is that, as the traditional advantages of cloud, including the scale, stability, and costs, reach their maximum extent, cloud service providers must serve their customers through continuous innovation in cloud native technologies and products to achieve a high customer stickiness. The field of cloud native products will become a battleground of fierce competition among cloud providers.
Third, cloud native will become ubiquitous, moving from data centers to cloud-edge-terminal integration.
Cloud native technology originated from applications and services in data centers. Over the past few years, it has gradually expanded to edge computing and even terminal computing. We believe that, with the rapid development of 5G and Internet of Things (IoT), cloud-edge-terminal integrated cloud native technology will be adopted by more enterprises and in more scenarios, becoming ubiquitous in the end.
Fourth, cloud native will prove difficult to implement in enterprises, so cloud native migration to the cloud will become the trend.
Cloud technology will develop faster than enterprises’ ability to implement it. Although cloud native technologies have been widely accepted, enterprises must spend a great deal of time overcoming difficulties to implement cloud native in their technology stacks. For example, the O&M habits for traditional virtual machines (VMs) must be changed during containerization and the conversion of traditional applications to distributed microservices involves re-architecting and other factors.
After cloud native is accepted by enterprises, these challenges need to be solved during implementation. The cloud native infrastructure, which contains many components and evolves quickly, also poses higher requirements on the technical skills of enterprise IT personnel. However, we believe that cloud native technology will be significantly valuable in reducing resource costs and improving R&D and O&M efficiency, which will spur enterprises on to conquer these challenges.
In this process, the use of cloud native cloud migration, standard interfaces, and hybrid cloud solutions such as containers and service mesh, will greatly reduce the complexity of cloud migration. This allows enterprises to quickly migrate to standard cloud services. We also believe that most enterprises will choose to migrate cloud native to the cloud to maximize cloud capabilities and achieve an efficient division of labor, allowing them to focus on their own business development.
With these factors in mind, let us now analyze the developments of the cloud native field in the past year, and analyze its trends for the coming decade through these seven directions:
List of authors (in no particular order): Ding Yu (Shutong), Zhang Liping (Gupu), Yang Haoran (Buchen), Yin Boxue (Yurui), Xu Xiaobin, Li Yun (Zhijian), Ceng Bin (Dianwei), Huang Ting (Luzhi), Yi Hongwei (Gaizhi), Xiaojian, Tang Zhimin, Hu Weiqi (Baimu), Wang Xu (Xunhuan), Liu Zheng (Wenqing), Yao Jie (Louge), Chen Tianzhou (Shuiniao), and Chen Xin (Shenxiu)
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-releases-eventbridge-to-support-cloud-native-architecture-2b304f2bcede?source=search_post---------277,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 12, 2021·13 min read
By Alibaba Cloud Serverless
This article introduces the latest development of Alibaba Cloud Serverless, including Function Compute (FC), Serverless App Engine (SAE), and EventBridge, a serverless event bus service.
In recent years, as the concepts of cloud native and Serverless become increasingly popular; “event-driven” has once again become a hot topic in the field of cloud application architecture. In 2018, the evaluation report of Gartner listed the event-driven model as one of the top 10 strategic technology trends. The report predicted that the event-driven architecture (EDA) would become the mainstream of microservices in the future. In addition, the report also made the following predictions:
By 2022, event notification model will be the solution for over 60% of new digital business.
By 2022, more than 50% of business organizations will participate in the event-driven digital business ecosystem.
In May 2018, the Cloud Native Computing Foundation (CNCF) began to host the open-source CloudEvents project. This project is aimed at describing events in a unified and standardized format to enhance the interoperability among different services, platforms, and systems. Therefore, the importance of events in the cloud native field is self-evident.
However, it is not easy to implement EDA on Alibaba Cloud. Alibaba Cloud’s Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) offering generate hundreds of millions of events every day. However, there is no simple or unified method to reach these events.
Many cloud services have self-built event centers, but they do not adopt unified standards or specifications to describe these events. As a result, users cannot interpret these events in the same way.
Currently, the events on the cloud are too independent to form a scale effect, which makes it difficult to obtain the business value of those events. To better discover the value of data, we must give full play to the scale effect of data and establish the relationship among data.
At present, events are mostly used in offline data analysis. Due to the lack of out-of-the-box capabilities to process the centralized events, those events cannot be applied to online business scenarios.
To solve these problems, Alibaba Cloud officially released the latest Serverless event bus service, called EventBridge. EventBridge is designed to serve as the hub of cloud events, connecting to cloud services and cloud applications with the standardized CloudEvents 1.0 protocol. It also provides the centralized capacities to govern and motivate events, helping users easily build a loosely coupled and distributed event-driven architecture. In addition to Alibaba Cloud, there are a large number of vertical SaaS services in the cloud market. Being excellent in cross-service, cross-organization, and cross-cloud integration, EventBridge will help users build a new on-cloud interface, which is comprehensive, event-driven, efficient, and controllable.
EventBridge is a brand-new cloud service completely designed for cloud native designing and architecture. The core capabilities provided by EventBridge include integration, choreography and drive, and data tunnel.
The ability and degree of integration are the key points of a service. EventBridge will connect cloud services, cloud applications, and SaaS applications in a cross-organization and cross-cloud manner. The connection requires low cost or even no cost, with low code or even no code.
Nowadays, Alibaba Cloud provides hundreds of mature cloud services and millions of computing instances, with hundreds of millions of cloud events generated every day. However, these events are currently out of control. They are a treasure of data waiting to be mined. Under this circumstance, EventBridge connects most of Alibaba Cloud services. Serving as an event source or event target, EventBridge improves the centralized governance capability for cloud events of Alibaba Cloud, and fully discovers the business value of cloud events. At the same time, EventBridge provides all-in-one connectivity services to ensure a better experience of cloud migration and cloud services utilization.
The ultimate goal of going to the cloud is to fully enjoy the technical benefits of cloud computing. Therefore, the process of cloud migration needs not only to change the host, but also to reconstruct the platform and systems. EventBridge provides abundant integration capabilities to make applications better connect to and use cloud services. At present, users can access Alibaba Cloud EventBridge ecosystem through the official HTTP interface of EventBridge, multi-language clients (Java, Golang, Python, C#, and PHP), and the open-source client of the CloudEvents community.
Alibaba Cloud adheres to the strategy of being integrated for SaaS. It is expected that a number of excellent SaaS providers will grow on Alibaba Cloud. EventBridge will provide a convenient way for SaaS to be deeply integrated with the Alibaba Cloud ecosystem and services.
The best practice of Serverless application architectures is the event-driven design. Whether it is traditional microservice or function computing, EventBridge greatly simplifies the development of event-driven architectures. Massive amounts of functions and microservices will be choreographed orderly in the form of events.
Gartner’s report compares the differences between architectures of orchestration and choreography. It is found that the way of combining and orchestrating microservices and functions through request-driven architecture brings a lot of unnecessary strong coupling. However, choreographing microservices and functions by event-driven architecture can be more thorough in decoupling, thus improving the resilience of the program and making business development more flexible and efficient.
Another core capability of EventBridge is to serve as a tunnel for streaming data. As a data tunnel, EventBridge uses the CloudEvents specification and the Schema registry (coming soon) to uniformly describe these data, and provides basic filtering and conversion capabilities. By doing so, EventBridge is able to implement data routing between different data warehouses, data processing programs, and between data analysis and processing systems.
EventBridge is about to launch the Connect capability. By using a large number of source and sink connectors, EventBridge allows user data to flow on the cloud.
EventBridge has several basic concepts: event, event bus, event source, event rule, and event target.
As is shown in the preceding figure, events are delivered from event sources to event buses. After being filtered and converted based on specified event rules, events are finally delivered to multiple event targets to be processed.
Event represents the occurrence, rules, and status changes of things. In the cloud era, events are ubiquitous. Any service, application, or even resource on the cloud is generating events all the time. These services, applications, and resources are called event sources. Event sources come from different organizations and environments. They do not have any expectations on how events will be responded to. Meanwhile, event targets subscribe to events through centralized event buses. They rely on the self-description capability of events to understand and process events at low cost.
Events in EventBridge are described by the cloud native event standard, called CloudEvents. CloudEvents is the first-class component in the EventBridge ecosystem. The main reasons for EventBridge to adopt CloudEvents are as follows:
Adopting a unified standard for cloud native events helps to uniformly express events from different event sources, thus improving the interoperability of event-driven programs.
When a standard is adopted, CloudEvents-based event-driven programs can be migrated between different clouds, without the need to worry about the vendor binding.
CloudEvents is very simple in structure. The following JSON text is a CloudEvents event.
Currently, Alibaba Cloud EventBridge supports two types of events: Alibaba Cloud service events and customized events.
Alibaba Cloud service events: Alibaba Cloud service events have a pre-defined schema registry. These events are generated from status changes of user resources on various Alibaba Cloud services. For example, Cloud Video Conferencing events, which include events of the start, end, and member changes of the meeting.
Customized events: Users can deliver customized events to EventBridge in multiple ways, such as using the open-source SDK of the CloudEvents community, the official multi-language lightweight SDK of EventBridge, EventBridge Connect, and Webhook. EventBridge Connect and Webhook will soon be available. With a wide range of methods to access event sources, clients can quickly build event-driven Serverless applications.
The concept of event bus is abstract. In short, event bus is a carrier of events. The event bus of Alibaba Cloud EventBridge has the multi-tenant capacity in the user side. Each bus has a unique Amazon Resource Name (ARN). Events are sent to event buses and then routed to event targets according to event rules.
As is shown in the preceding figure, Alibaba Cloud’s event bus consists of the following two types:
Default event bus: The default event bus is automatically created when the user activate the EventBridge service. All connected cloud service events are automatically delivered to the default bus. For users, cloud service events are out-of-the-box.
Customized event bus: Customized event buses are created by users to receive customized events. Customized event bus is an essential resource for developing event-driven architectures and programs.
Rules are used to filter the events in the event buses. After certain conversion, the successfully filtered events are routed to the Alibaba Cloud target services or HTTP gateways specified in the rules.
The rules in EventBridge have two ends. One end is connected to the event bus, and the other end to the event target. The two ends are in a one-to-many relationship, with each rule connected with a maximum of five event targets. In addition, the filtering and conversion components in rules provide users with lightweight event filtering and conversion capabilities.
The filtering capability of a rule is provided by event pattern. The following filtering patterns are now supported:
• Specified value matching• Prefix matching• Suffix matching• Exception matching• Numeric matching• Array matching• Complex combinatorial logic matching
The conversion in the rules is used to convert the CloudEvents event into the format that can be accepted by the event target. EventBridge provides four converters:
• Complete event: Directly deliver events to the native CloudEvents without conversion.• Partial event: Use JSONPath syntax to extract part of the content from the CloudEvents event and deliver it to the event targets.• Constant: Events only serve as a trigger and the delivered content is a constant.• Template converter: Render the customized content to be delivered to event templates by defining templates.
As a brand new cloud service, EventBridge fully adopts the cloud native technology stack. As is shown in the following figure, EventBridge is built on the Kubernetes cluster provided by Container Service for Kubernetes. A complete set of Kubernetes-based DevOps R&D system has also been designed. In the R&D phase, GitOps is practiced to improve the delivery and iteration efficiency. In the testing phase, a large number of automated tests are deployed. In the publishing phase, a complete gray release mechanism is provided. In the O&M phase, Kubernetes’s self-healing capability greatly reduces the O&M costs. Besides, a cloud-native monitoring system is established based on services such as Prometheus Service and Log Service (SLS).
In addition, EventBridge relies on the event storage capabilities of Message Queue for Apache RocketMQ (RocketMQ). As a message-oriented middleware developed by Alibaba, RocketMQ has been tested by several years of Double 11 and countless Alibaba’s internal business scenarios. It is proven to be excellent in providing EventBridge with high SLA and high-performance event transmission services.
This section lists three typical cases based on the available capabilities of EventBridge. As the EventBridge ecosystem becomes increasingly rich, more business scenarios will be able to apply EventBridge in the future. A series of model projects will be offered in the near future for users to reference, so that they can quickly adopt the event-driven approach to their own business scenarios.
The larger the enterprises grow, the more business stability they demand. To prevent more failures in the increasingly complex scenarios, it is particularly important to build an all-round observation and monitoring system for applications. For traditional applications, after their reconstruction based on cloud native, they can enjoy the benefits of cloud native technologies. At the same time, however, they are also confronted with more complexity in how to manage the stability of the applications. The most challenging issue is that it is difficult to control changes. The business relies on the entire cloud infrastructure, physical and network resources in the IaaS layer, cloud services in the PaaS layer, and even the upstream and downstream services. These services are so constantly changing that it is difficult for users to immediately perceive the occurrence and corresponding impact of these changes. In fact, 95% of faults are caused by changes.
To solve this problem, a 360-degree business panorama is created based on EventBridge. With this achievement, users can clearly perceive changes and exceptions in the entire business procedure, and whether these exceptions are related to the recent changes. What’s more, when special problems show up, self-O&M method can be used to help services recover faster and minimize the impact.
All these capabilities can hardly be realized without EventBridge to integrate multiple events of cloud services or trigger the responses of multiple cloud services through events. At the same time, through EventBridge, events, as an important carrier of information, are able to coordinate various cloud services to work in an orderly manner.
Scenario 1 is relatively complex, so let’s take a look at another scenario that can be easily implemented. Use EventBridge-driven Function Compute to take webpage screenshots, and send them through email to make daily briefings.
• First, through analysis, the business system calculates out the most valuable and mostly searched news of the day, and sends the URL addresses of the news as an event to EventBridge.
• After receiving the event, EventBridge activates the webpage screenshot program of FC based on the pre-configured rules.
• Then, FC automatically creates resources after receiving the event and runs the webpage screenshot program. Screenshots are saved to Object Storage Service (OSS) at the same time when FC notifies EventBridge to release the resources.
• Next, after receiving the event from FC, EventBridge activates the email service to send the screenshots to users based on the pre-configured rules.
This example demonstrates that Serverless and EventBridge are a very powerful combination in the cloud native era. Driving FC through events allows services to schedule resources on demand. By doing so, stability risks caused by burst traffic can be prevented, and costs can be reduced to the greatest extent through pay-as-you-go mode.
How many scenarios can EventBridge be applied to in the future? Let’s look at the scenario of smart furniture store of new retail.
• In this scenario, the warehousing events of furniture and the events of customers entering into the store are sent to the online analysis system in real time through EventBridge. By doing so, users are informed of which furniture is now available in the store and what kind of furniture customers prefer. The system also sends the analysis results to the shopping guide or advertising screen of the store, helping ensure a better ordering process.
• Customers first pay online, and then the orders are sent to EventBridge. EventBridge informs third party logistics companies to deliver the furniture.
• Third-party logistics companies push the location information of the furniture that customers order to the mobile apps through EventBridge in real time. Customers can easily check where their furniture is and when it will be delivered in real time through apps.
• All the data of these online business events are transferred through EventBridge. They will eventually be transferred again to the offline analysis system through EventBridge. Business reports will be automatically generated for management to make performance appraisal or operational decisions.
In this scenario, EventBridge plays a key role in efficiently transferring event information to help achieve business goals. For scenarios such as IoT, online business, and big data, the efficient transferring of EventBridge is very critical. Events are both online business data and offline analysis data, which reduces costs and improves efficiency.
As an event hub on the cloud, the core capability of EventBridge is to provide connectivity. Therefore, EventBridge will focus on building an ecological network in the future. Applications in online business, IoT, or big data scenarios can all be integrated to EventBridge with low code or even no code. If the applications are deployed in private Internet data centers (IDCs) or in cloud environments that are not provided by Alibaba Cloud, a secure and reliable integration methods will also be provided.
Certainly, to build such a huge central nervous system in the cloud era needs not only time, but also joint efforts. In the future, EventBridge will be committed to the construction of an open source community. It is hoped that more friends can participate in the construction and promote the event-driven model as pioneers in the cloud native era.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibabatech/aiming-for-corporate-level-cloud-native-alibaba-cloud-cnfs-solving-the-container-persistent-32285f76a70f?source=search_post---------278,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Aug 31, 2021·11 min read
Introduction: Driven by the cloud-native trend, applications are increasingly moving toward the containerized model. Meanwhile, Kubernetes is evolving into a new infrastructure for the cloud-native era. Forrester believes that enterprises and organizations worldwide will run their applications in containerized production environments by 2022. Two general phenomena can be observed through today’s container and Kubernetes applications. Firstly, cloud-based Kubernetes has become enterprises’ preference when they move their businesses to the cloud and containerize their applications. Additionally, users use containers in new ways, including stateless applications, core enterprise applications, and data intelligence applications. More and more enterprises use containers to deploy complex, stateful production applications that require high computing performance, such as web services, content libraries, databases, and even DevOps, AI, and big data applications.
Driven by the cloud-native trend, applications are increasingly moving toward the containerized model. Meanwhile, Kubernetes is evolving into a new infrastructure for the cloud-native era.
Forrester believes that enterprises and organizations worldwide will run their applications in containerized production environments by 2022. Two general phenomena can be observed through today’s container and Kubernetes applications. First, cloud-based Kubernetes has become the preferred choice of enterprises when moving their businesses to the cloud and containerizing their applications. Additionally, users apply containers in new ways, including stateless applications, core enterprise applications, and data intelligence applications. More and more enterprises use containers to deploy complex, stateful production applications that require high computing performance, such as web services, content libraries, databases, and even DevOps, AI, and big data applications.
How do we find solutions to implement data orchestration and storage for large numbers of containers in the cloud-native era? How shall we improve the performance and stability of containerized storage?
Today’s computing and application landscape are undergoing tremendous changes as our infrastructure evolves from physical servers to virtual servers, containerized environments such as Kubernetes, and even serverless computing models. The most significant difference is that applications, which in the past would exclusively occupy a CPU memory partition on a virtual server, have evolved to use function-based service provision under the serverless model.
The resulting technical system calls for new types of storage systems with the following features.
In the virtual machine era, one virtual machine had one independent storage space to store all the specific application data. However, the Kubernetes serverless environment uses a shared storage space. Every container needs to access a vast resource pool, resulting in high storage density and higher storage capacity requirements.
After creating a physical or virtual server, its storage media is generally used in relatively stable cycles. However, in today’s containerized environment, the needs of front-end computing services change dramatically and unpredictably. They may need dozens of servers at one time and then several hundred servers soon after, requiring highly elastic storage resources.
In a Kubernetes serverless environment, it is hard to guarantee the exclusivity of memory or storage resources. The reason for such difficulty is that the storage and computing resources, including the operating system and some underlying dependencies packages, are all shared in a containerized environment. We need to implement secure isolation at the infrastructure level and data isolation at the application level, the upper level, via well-structured security policies and solutions: a considerable change and challenge.
Block storage, file storage, and object storage are standard containerized storage solutions. What kind of file storage capabilities does an enterprise need in a containerized environment?
We may find it hard to rapidly transform the general application modes of enterprises, which usually adopt shared storage or distributed storage clusters in many scenarios. The situation highlights the importance of storage service compatibility with applications. We must ensure data consistency between containerized environments and non-containerized environments to minimize or eliminate the work required to adapt applications.
A significant feature of containerized deployment is the elasticity that responds to the rapid changes in resource needs during business peaks and troughs. As the computing resources at the upper level change elastically, the storage resources at the lower level are also required to respond quickly. They must not take a long time to sync underlying data.
The datasets used by big data and high-performance computing (HPC) applications are enormous, often in the terabytes and sometimes in the hundreds of terabytes. If this massive amount of data can’t be shared but instead copied and transferred in an elastic containerized environment, this will result in increased costs and lost efficiency.
No matter how abstract and regardless of its underlying infrastructure being physical servers, virtual servers, Kubernetes containers or a serverless environment, the most fundamental requirement of a business is security. There can be no data contamination between applications. The storage systems must be built on top of the sharing layer to ensure data security.
Enterprises are constantly working to lower their cost in all application scenarios. Cost control remains a topic in the most critical application scenarios of an enterprise. As a service grows and changes, its data storage requirements may multiply rapidly. Enterprises must support the rapid growth of data storage capacity while optimizing the storage costs. It is a great challenge.
To leverage the advantages and meet the challenges of container file storage, Alibaba Cloud has released Container Network File System (CNFS), which is built-in to Alibaba Cloud Container Service for Kubernetes (ACK). CNFS abstracts the Apsara File Storage NAS (NAS) into a Kubernetes Custom Resource Definition (CRD) for the independent management of O&M operations, including creation, deletion, description, mounting, monitoring, and scaling. It is then more accessible for users to use containers to store files, improves the performance and data security, and ensures inter-container consistency via declarative management.
CNFS thoroughly optimizes containerized storage in terms of elasticity and scalability, performance optimization, accessibility, observability, data protection, and declarative mechanisms. This product outperforms its peers in the following areas:
Many applications face sudden spikes in request volumes. These scenarios require a quick expansion of the number of containers and highly elastic resources. Container storage provides the general elasticity and rapid scalability needed to meet these requirements. Typical applications of this type include media, entertainment, live-streaming, web services, content management, financial services, gaming, continuous integration, machine learning, and HPC.
For such applications, pods must be able to mount and unmount storage PVs flexibly. Mounting a storage PV requires a quick boot of the container and may produce a high file I/O load. As the massive amount of persistent storage data proliferates, the storage costs and stress also surge. Given this, we recommend the ACK + CNFS + NAS combo for the following advantages:
· The built-in file storage class enables the launch of several thousand containers within a short period and the mounting of file storage PVs within milliseconds.
· The built-in NAS provides shared reads and writes for the massive amount of containers to quickly achieve containerized applications and high data availability.
· With the low latency and small file optimization, the solution enables data reads and writes within microseconds to satisfy file storage performance requirements in the event of highly concurrent container access.
· The solution supports file storage lifecycle management and automatic hot and cold data classification to reduce storage costs.
More and more AI services have moved their data training and reasoning processes to containers. The massive cloud infrastructure capacity and its integration with IDCs also enable more flexible computing power scheduling for AI applications. When an AI service performs training and reasoning on the cloud, the application will generate massive datasets. In the autonomous driving field, for example, some datasets can reach 10 PB or exceeding 100 PB. Therefore, we need to ensure that AI applications can be efficiently trained on such large datasets, which poses the following challenges:
· AI data flows are complicated and may be restricted by data I/O bottlenecks in the storage system.
· AI training and reasoning require high-performance computing and storage.
· In AI computing power collaboration, cloud and IDC resources/applications require unified scheduling.
We recommend the ACK clusters + CNFS + NAS/CPFS combo for the following advantages:
· The optimized NAS read and write performance improves shared storage performance to meet the needs of AI scenarios. This solution supports access to a massive amount of small files and faster AI training and reasoning.
· Computing clusters adapted to the containerized environment such as GPU cloud drives and bare-metal servers (X-Dragon) ensure super-high throughput and IOPS. CPFS also supports on-cloud/off-cloud hybrid deployment.
· ACK clusters support IDC-built Kubernetes clusters managed via ACK to form a uniform resource pool on and off the cloud. It enables uniform scheduling of heterogeneous resources/applications to maximize the computing advantages of massive cloud-based infrastructure.
Today, genetic testing technology is rapidly advancing. Already, many hospitals use this technology to treat complex diseases more quickly and accurately. The data size of a genetic sample from one person is already enormous, usually dozens of gigabytes. Moreover, the targeted genetic analysis generally requires samples from hundreds of thousands or millions of people, resulting in complex challenges for container storage solutions:
· Mining data from a large set of samples requires massive computing and storage resources. In addition, the data size snowballs, storage costs are high, and management is challenging.
· Massive data volumes need to be quickly and securely distributed to multiple locations in China for shared access from multiple IDCs.
· Batch processing of samples takes a long time and requires high computing performance with significant request peaks and troughs, which makes planning difficult.
Because of the features of genetic computing scenarios, we recommend the ACK + AGS + CNFS + File Storage NAS + OSS combo for the following advantages:
· The NFS’ built-in file storage class enables a fast, inexpensive, and precise genetic computing container environment.
· CNFS supports OSS PVs to save lower computer and post-mounting data and analyze result data for distribution, archive, and delivery. This way, a large number of users can upload and download data simultaneously, improving the data delivery efficiency. At the same time, CNFS also provides massive storage capacity and lifecycle management capabilities to archive and store cold data at a lower cost.
· AGS supports GPU accelerated computing of hot data for genetic computation, offering performance 100x better than traditional modes for significantly faster and cheaper gene sequencing.
Besides the typical scenarios above, CNFS offers optimized container and storage integrated solutions for services in many other applications. For more information, see https://help.aliyun.com/document_detail/264678.html.
Leveraging its deep integration with CNFS, Apsara File Storage NAS (NAS) has become the best-containerized storage solution. Several case studies are described below to help you better understand how to leverage Alibaba Cloud ACK and file storage service to build a modernized enterprise application.
Video service
BAIJIAYUN is a leading Chinese comprehensive video service provider. During the COVID-19 pandemic, their traffic increased dozens of times over, and they had to expand in a way that was imperceptible to users rapidly. Additionally, BAIJIAYUN’s service scenarios feature massive read and write volumes and its computing clusters horizontally expanded to four in number. During the recording transcoding process, its previous storage system created an I/O bottleneck, placing significant limitations on BAIJIAYUN’s ability to deal with high traffic volumes and high concurrency.
applications and allow data access immediately after scaling. Ultimately, BAIJIAYUN chose Alibaba Cloud’s container service ACK and file storage service NAS, allowing it to optimize its container cluster architecture and successfully expand its capacity ten times within three days.
NAS supports automated scaling as needed based on ACK. As a result, several thousand containers can be launched quickly to help the elasticity of containerized applications ideally. NAS provides standard APIs that are compatible with powerful transcoding software to mount video editing workstations easily. BAIJIAYUN has exceptionally high requirements for Kubernetes cluster performance. Thanks to the high-performance NAS service, the solution ensured a high throughput of up to 10 GB, which cracked the I/O bottleneck while meeting the high traffic volume and concurrency requirements of BAIJIAYUN’s scenarios. As a result, the company’s live-streaming and recording services were launched smoothly during the pandemic.
Autonomous driving
The second case is a well-known customer engaged in the auto industry. As a leading Chinese smart car manufacturer, the customer is also an emerging tech firm at the forefront of internet + AI integration. Its products are powered by multiple AI technologies and services, such as speech assistants and autonomous driving.
The company’s challenge was the hundreds of millions of small images (100 KB each and more than 100 terabytes in total) used as training resources for its autonomous driving system. During training, the GPU must repeatedly and randomly access these images, which requires the file system to provide high file access IOPS to accelerate the training process. However, as the storage system grew in prominence, its stability and performance failed to increase at the same pace as its size. In addition, other issues, including high costs and complex operations, maintenance, and management, also arose with the increase in storage resources.
support for the customer’s HPC platform for smart driving, speeding up the random access to small files during training by 60%. Data is stored at multiple data nodes in the cluster to support simultaneous access from multiple clients and parallel extension. Moreover, NAS supports multi-level storage data flows, which significantly streamlines autonomous driving data acquisition, transmission, and storage processes.
Genomic computing
The last case study is from the genomic computing sector. The customer is a world-leading life science institution focusing on cutting-edge science. Challenges: The customer’s data grew fast, and their current storage system could not meet their capacity and linear expansion requirements, creating an I/O bottleneck in their genomic computing system. The high sample data size resulted in increased storage costs and complex management.
With Apsara File Storage NAS (NAS) mounted on a containerized cluster, the solution enabled high performance in genomic data computing and data analysis using shared storage. The storage stored the lower computer data and the post-mounting data and intermediate data generated during the process, ensuring low latency and high IOPS for the containerized storage. The storage performance increased from 1 GB/s to 10 GB/s, with end-to-end data processing, including data migration to the cloud and result distribution from the cloud, taking less than 12 hours.
NAS provides elastic bandwidth with high throughput. NAS divides the capacity as needed and allocates appropriate bandwidth to ensure service elasticity at a lower total cost of ownership (TCO) based on the service scale. NAS ensures efficient genomic computing at a lower cost with a uniform process and uniform scheduling of on-cloud and off-cloud heterogeneous computing resources.
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@alibaba-cloud/flexible-and-efficient-cloud-native-cluster-management-experience-with-kubernetes-3fefbf159991?source=search_post---------279,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 14, 2020·9 min read
By Huaiyou, Linshi
A single Kubernetes cluster provides users with namespace-level isolation. Theoretically, this type of Kubernetes cluster supports no more than 5,000 nodes and 150,000 pods. Multi-Kubernetes clusters address the difficulties in resource isolation and fault isolation of single clusters. The number of nodes and pods usually supported is no longer limited. However, the complexity of cluster management increases. Especially in Apsara Stack scenarios, Kubernetes engineers cannot reach customer environments as quickly as in public cloud scenarios, so the operation and maintenance (O&M) costs are further increased. Therefore, how to manage multiple Kubernetes clusters in an automated, highly efficient manner and at low cost has become a common challenge in the industry.
Multi-cluster application scenarios are as follows:
1) The product itself requires multi-cluster capabilities. Product control needs to be deployed in a Kubernetes cluster. This product also needs to provide Kubernetes clusters to users. From the perspective of fault isolation, stability, and security, the control and business of container services must be deployed in different clusters.2) Users also hope that, when they use Kubernetes, it allows them to produce multiple clusters for different businesses, to isolate resources and faults.3) A user may need the capabilities of multiple types of clusters. Take the edge computing IoT as an example. It requires a customized edge Kubernetes cluster. If you create this edge cluster based on a common independent Kubernetes cluster, there will be a waste of resources. In addition, independent clusters increase O&M costs for users.
The following sections summarize the difficulties in operating and maintaining the Kubernetes cluster into two parts.
As a complex automated O&M system, Kubernetes supports the release, upgrade, and lifecycle management of upper-layer businesses. However, in the industry, Kubernetes O&M is usually performed by a workflow such as Ansible or Argo. The O&M process is not highly automated and requires O&M personnel to have professional Kubernetes knowledge. If multiple Kubernetes clusters require O&M, the O&M costs increase linearly even under ideal conditions, and the costs will significantly increase in Apsara stack scenarios.
Alibaba Group encountered the challenges of managing many Kubernetes clusters a long time ago. Therefore, we abandoned the traditional workflow model and explored another solution: Use Kubernetes to manage Kubernetes clusters. For more information, see the CNCF article Demystifying Kubernetes as a Service — How Alibaba Cloud Manages 10,000s of Kubernetes Clusters, which introduced Alibaba Cloud’s exploration and experience in managing large-scale Kubernetes clusters.
Of course, an Apsara stack scenario is quite different from Alibaba Group’s internal scenario. The scale effect of Kube-On-Kube (KOK) is the crucial reason why it is used in Alibaba Group’s internal scenario. You may use one Meta Kubernetes cluster to manage thousands of business Kubernetes clusters. The scale effect in the Apsara stack scenarios is small. Apsara stack uses KOK to automate the O&M of Kubernetes clusters, and KOK is compatible with various Kubernetes clusters. This improves stability and enriches application scenarios.
The declarative API of Kubernetes changes the traditional procedural O&M mode. It corresponds to the final-state-oriented O&M mode: Users define their desired states in the Spec, and the Kubernetes controller performs a series of operations to help users achieve the desired state. If the requirements are not met, the controller keeps trying.
For Kubernetes native resources such as Deployment, the Kubernetes controller is responsible for maintaining the final states. For user-defined resources, such as a cluster, Kubernetes provides a powerful and easy-to-use CRD + Operator mechanism. Customize final-state-oriented O&M tasks in a few simple steps:
1) Define your own resource type (CRD) and implement your own Operator, which contains a custom Controller.2) Submit a CR file in yaml or json format.3) The Operator detects the CR’s change, and the Controller starts to execute the corresponding O&M logic.4) During the running process, if the final state does not meet the requirements, the Operator monitors the change and performs corresponding recovery operations.
The Operator is one of the best practices that use code to perform O&M on applications. Of course, it is just a framework that eliminates some repetitive work, such as event listening and RESTful API listening. However, the core O&M logic still needs to be written case by case.
KOK is not a new concept. There are many excellent solutions in the industry, including by Ant Financial and other community projects.
However, the preceding solutions are highly dependent on Alibaba Cloud infrastructure. Apsara stack has the following features we need to consider:
After considering these three factors, we have designed a more general KOK solution as shown below.
The etcd Operator creates, destroys, upgrades, and recovers etcd clusters. It also monitors the status of etcd clusters, including the cluster health status, member health status, and storage data volume.
SRE is the cloud-native application platform of Alibaba Cloud. The SRE team improved the open-source version of etcd Operator and enhanced its O&M and stability. The etcd Operator is responsible for O&M and management of a large number of etcd clusters in Alibaba Cloud. The operator is stable and has been proven to be successful.
The Cluster Operator is responsible for creating and maintaining the Kubernetes management and control components, including Apiserver, Controller Manager, and Scheduler. It is also responsible for generating corresponding certificates and kubeconfig.
We worked with the PaaS engine and Apsara stack product team from Ant Financial group to create the Cluster Operator, which provides features such as custom rendering, version tracking, and dynamic addition of supported versions.
The Kubernetes management components of a business cluster are deployed in a meta cluster. From a meta cluster perspective, these components are common resources, including Deployment, Secret, Pod, and PVC. Therefore, business clusters do not have the concept of Master nodes:
However, Kubernetes is not available if only the preceding three components are deployed. The following requirements must be met:
1) In addition to etcd and the three major components, coredns, kube-proxy, and other components must be deployed for an available service Kubernetes.2) Some components must be deployed on the same meta cluster as etcd and the three major components. For example, a Machine Operator is responsible for starting nodes, and it must be ready to run before nodes are available in the business cluster. Therefore, it cannot be deployed in a business cluster.3) Components need to be upgraded.
To meet the need for scalability, we designed the Addons plug-and-play function, which allows importing all Addons components through only one command. In addition, Addons supports dynamic rendering and allows customizing Addons configuration items. The details will not be elaborated here.
The Machine Operator performs necessary initialization operations on a node; creates node components such as docker, Kubernetes, and NVIDIA; maintains the final state, and adds the node to a business cluster.
We use the KubeNode component maintained by the Serverless node management team for the cloud-native application platform of Alibaba Cloud. The Operator is responsible for connecting and disconnecting nodes in the Alibaba Group to implement a final-state-oriented O&M framework. Customize the O&M CRD for different Arch databases or operating systems (OSs), which are more suitable for Apsara stack environments that change constantly.
In short, KubeNode provides a final-state-oriented O&M framework, which consists of two concepts: the Component and the Machine.
1) You provide the O&M script based on the template to generate the Component CR.2) If you want to bring online a node, generate a Machine CR, which specifies the components that need to be deployed.3) When KubeNode detects the Machine CR, it performs O&M operations on the corresponding node.
In theory, this design allows applications to be extended based on different Arch databases or OSs, without modifying the Operator source code. Therefore, it is highly flexible. At the same time, we are also exploring how to integrate IaaS providers to ultimately achieve the goal of RunAnyWhere.
After using the automation tool (also known as a cloud-native Operator) to connect to the preceding processes, it’s possible to reduce a cluster’s production time to minutes.
The following table compares the costs of a tiled multi-cluster solution (direct deployment of multiple sets) and a KOK multi-cluster solution.
T is the deployment time of a single Kubernetes cluster, t is the deployment time of a single business cluster, K is the number of clusters, G is the number of sites, U is the time of meta cluster upgrade, u is the time of business cluster upgrade, and P is the number of upgrades.
According to our practical experience, T and U are about 1 hour, and t and u are about 10 minutes under normal circumstances. We predict that:
The tiled multi-cluster solution increases O&M complexity linearly. However, the KOK multi-cluster solution treats the Kubernetes cluster as a Kubernetes resource. By using the powerful CRD and Operator capabilities of Kubernetes, the O&M of Kubernetes clusters is upgraded from the traditional procedural type to the declarative type. In this way, the O&M complexity of Kubernetes clusters is significantly reduced.
In addition to years of O&M experience in Alibaba Group, the multi-cluster design described in this article adopts the cloud-native architecture to implement RunAnyWhere, without relying on differentiated infrastructures. Just provide common IaaS to implement easy-to-use, stable, and lightweight Kubernetes multi-cluster capabilities.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@pingcap/the-kost-stack-an-open-source-cloud-native-hybrid-database-stack-613dc71a69cb?source=search_post---------280,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
Nov 13, 2019·6 min read
Kevin Xu
Many technology stacks have emerged over the years that made software developers’ lives easier and more productive — better development efficiency, clearer software choices. The most notable among them is the LAMP stack (Linux, Apache HTTP server, MySQL, PHP), which helped countless developers turn their big ideas into big companies, none bigger perhaps than Mark Zuckerberg creating Facebook. More recently, popular web development patterns that leverage Javascript, like the MEAN stack (MongoDB, Express.js, Angular, Node.js) and its close cousins, MERN (swap Angular for React) and MEVN (swap Angujar for Vue.js), have shrunk idea-to-market time dramatically.
However, as cloud computing becomes the default way to consume infrastructure resources, just getting an app up and running is no longer enough for the next wave of innovation — not only does an idea need to reach market quickly, it needs to be architected for scale from the get go to handle rapid growth and ideally without vendor lock-in of any sort.
To meet this demand, a new infrastructure pattern is emerging called the KOST stack (Kubernetes, Operator, Spark, TiDB). This set of open-source technologies and framework provides a pattern that lets a developer consume cloud computing resources of any mix (public, private or hybrid) with a relational database management system that can performantly handle a hybrid of transactional and analytical processing workloads, so called HTAP. The native integration between these technologies makes spinning up a “KOST cluster” doable during any weekend hackathon — whether you are part of a legacy IT department looking to be “cloud-native” or a single developer looking to launch the “next big thing”.
Kubernetes is a set of open-source container orchestration APIs that has become the defacto toolset for deploying, running, and managing containerized workloads. Since being open-sourced by Google in 2014, its evolution, popularity, and rate of adoption have increased at a breathtaking pace — some even label it the Operating System of the cloud. Redmonk co-founder and analyst Stephen O’Grady said it most succinctly, “…it’s Kubernetes’s world and we’re all just living in it…”
While other container orchestration layers do exist (e.g. Docker Swarm, Apache Mesos, OpenStack), the cloud computing world has coalesced around Kubernetes. As a case in point, all major public cloud platforms offer their own managed Kubernetes engines (EKS in AWS, AKS in Azure, GKE in GCP, ACK in Alibaba Cloud), along with a large array of vendors providing managed Kubernetes solutions either on-premises or across different clouds — OpenShift, VMware PKS, Rancher, Giant Swarm, to name just a few.
Kubernetes is the common denominator of all cloud computing workloads and the foundation for enabling a private cloud or a multi-cloud architecture, which is especially appealing for large enterprises who either don’t want to be locked-in to one public cloud vendor, or must manage their own cloud infrastructure for security, compliance, or data governance reasons.
An Operator is a method of packaging, deploying, and managing a Kubernetes application. It is designed for running stateful workloads easier to manage and operate, which is a relative weakness of vanilla Kubernetes (most Kuberentes production workloads to date are stateless). The Operator pattern was pioneered by CoreOS, championed and fostered by Red Hat after its acquisition of CoreOS, and has gained popularity among different database technologies looking for ways to deliver its solution in a so-called “cloud-native” way, which has become somewhat synonymous with “Kubernetes-native”. Both Spark and TiDB have their own Operator implementation that are open-sourced for this very reason. Percona, a leading enterprise support, services, and consulting provider for open-source databases, also announced two operators for its XtraDB Cluster and MongoDB offering respectively.
It’s a critical layer in the KOST stack, because running an HTAP database stack in the cloud requires careful management of the state of data, to ensure that data is both strongly consistent and made available for analysis in near real-time. Additionally, there are other routines but important operational tasks that must happen, like horizontal scaling for capacity, backup, and recovery, during the course of running a database. All these tasks can be encoded and semi-automated within an Operator. It’s essentially an encoded extension of your SRE team.
Apache Spark, first created at UC Berkeley’s AMPLab in 2009, is a widely used distributed cluster computing framework that excels in big data analytics — classic online analytical processing (OLAP) workloads. It has since evolved, with components that support data streaming with Spark Streaming, querying structured and semi-structured data with Spark SQL, and conducting Machine Learning with Spark MLlib.
Spark has its own Operator, and can interface with a variety of distributed storage systems to aggregate data, including Alluxio, HDFS, Cassandra, AWS S3, OpenStack Swift, and TiKV, which is a distributed transactional key-value store and also a Cloud Native Computing Foundation (CNCF) member project.
While Spark provides a strong analytical engine, TiDB completes in the transaction side of the puzzle to make HTAP a reality in the cloud. TiDB is an open-source NewSQL database solution that speaks the MySQL protocol and guarantees strong data consistency. The project has significant traction in China, where it first started, by taking advantage of the massive scale of the country’s Internet economy to battle-test itself. TiDB is also experiencing strong growth in Southeast Asia and India, as well as traction in Europe and North America, noted recently by the industry analyst firm, 451 Research.
TiDB has its own Operator implementation, called TiDB Operator, and works natively with TiKV to provide a relational MySQL-like database that scales horizontally without giving up transactional consistency. (TiDB and TiKV were both initially created and maintained by the commercial company, PingCAP, before TiKV became a member project of CNCF. There is also an open-source Spark plug-in called TiSpark that’s maintained by PingCAP.)
Traditionally, transactions and analytics are run on two different databases (often designed to be well-suited for one or the other), and often managed by two different teams, even residing in two different buildings if you are a large enough enterprise. The architecture is monolithic (as opposed to microservices-based), difficult to scale horizontally, and data is only made available for analysis by managing a separate ETL (Extract, Transform and Load) pipeline that runs once or a few times a day, likely done by a third team.
While this legacy way can still work in certain industries, highly competitive verticals like retail/eCommerce is adopting the cloud-native HTAP way with the KOST stack, in order to deliver features like “smart pricing”, better inventory management, fraud detection, and new experiences like “buy online, pickup in store”. All these innovative approaches to shopping require a flexible digital infrastructure, as enabled by the KOST stack, that can scale quickly, operate easily, and enforce consistency of data while making it available to be aggregated and analyzed without requiring an ETL pipeline.
The KOST stack is by no means static, because each component is open-sourced, extendable, and integrates well with other software when the right use case beckons. Quite often, there’s a Redis-based cache layered on top to add more speed and performance gain. An open-source service mesh layer, like Istio, Kong, or Linkerd, also tends to sit on top of KOST to manage a set of microservices to power your applications.
Because KOST is made up of only open-source software, it’s entirely free to use if your team is willing to operate it — allowing large enterprises to evolve digitally while sunsetting the expensive licenses of proprietary alternatives. If DIY isn’t your style, each component has at least one commercial vendor ready to support it.
While stack patterns may sometimes inadvertently pick technology winners, irking certain vendors who have vested interests in the popularity of specific software, developers generally don’t mind. As the new Kingmakers of technology choices, developers are becoming increasingly sophisticated in filtering out vendor hype in favor of open-source technologies that have a vibrant community, play well with other software, and solve real technical pain points. Just like how LAMP lighted the way for the first wave of web development and MEAN/MERN/MEVN pushed the boundary of mobile development, KOST will ease the pain of using cloud computing to scale your database operation with real-time data insights — empowering developers and entrepreneurs to do what they do best: build.
This article was first published on The New Stack.
Originally published at www.pingcap.com on Jul 4, 2019
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
"
https://medium.com/mayfield-perspectives/expanding-mayfield-investment-team-with-a-devops-and-cloud-native-product-leadernavin-chaddha-8f8c3369b9bb?source=search_post---------281,"There are currently no responses for this story.
Be the first to respond.
I’m excited to share that Vivek Saraswat has joined the Mayfield investment team. Vivek is a great fit for Mayfield, as we believe that investors with operating experience can serve as effective advocates and provide relevant advice to entrepreneurs. In addition, he expands our network to reach next generation entrepreneurs, especially those in the open source and DevOps community and beyond. He shares our belief that this is an exciting time for enterprise software, with revolutionary advancements in areas like microservices, DevOps tooling, and cloud-native technologies.
Vivek brings deep product experience from leading companies such as Docker, VMWare and Amazon. As a product management leader at Docker, Vivek launched their primary enterprise product and steered it through advancements in container orchestration, networking, storage, and security. Prior to that, as a product manager, he helped launch the Cloud Native Applications Storage initiative at VMware, and evaluated new storage features at Amazon Web Services. In a previous life, Vivek was an early engineer at Solexel, a venture-backed solar power startup, and managed partnerships for semiconductor thin films at Applied Materials.
As a Bay Area native, Vivek grew up surrounded by the entrepreneurial ecosystem in Silicon Valley. He is a graduate of Stanford University with a BS and MS in Materials Science and Engineering as well as an MBA.
Vivek explains his decision to join the Mayfield team and pursue a career in venture capital in this way:
“I found my calling in creating the building blocks that others use to build bigger and better things. I’ve worked towards this purpose in product leadership roles at emerging companies. Now in VC, I hope to help entrepreneurs build companies that will create the building blocks that change the world. I am excited to join a close-knit team of experienced investors with a people-first philosophy, who are fiercely committed to the success of entrepreneurs everywhere.”
Some fun facts about Vivek include that he is an accomplished a capella singer and has built solar-powered crop dryers for hazelnut farmers in Bhutan.
Please join me in welcoming Vivek to the Mayfield family.
Originally published at www.mayfield.com on June 6, 2018.
People First.
Mayfield is a global venture capital firm with a 50+ year history of investing in relationships with entrepreneurs from inception to iconic.
Written by
Mayfield is a global venture capital firm with over $2.7 billion under management and over 49 years of championing people.
Mayfield is a global venture capital firm with a 50+ year history of investing in relationships with entrepreneurs from inception to iconic.
"
https://medium.com/@alibaba-cloud/mihoyo-a-first-generation-cloud-native-enterprise-realizes-imagination-533021f24163?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 6, 2021·10 min read
By Jia Ningyu
In the office area of miHoYo, there is a meeting room reserved for Alibaba Cloud engineers.
This ACGN company was established nine years ago. The friendship between miHoYo and Alibaba Cloud has lasted for eight years.
Liu Wei, President of miHoYo, still remembers the day when Dr. Wang Jian arrived at miHoYo’s office with eight company executives and a team from Alibaba Cloud. Back then, miHoYo only had about 30 seats in a small rented office. miHoYo had no meeting room for the visitors.
At that time, miHoYo was a newly founded company, and Alibaba Cloud was in the initial stage of external services. The two new companies came together by coincidence and began working together.
On that day, Dr. Wang said, “If the customer flies in the sky by airplane while we just look on the ground, breakdowns are easy to happen. To avoid this, we will fly in the sky with our customers.”
When Dr. Wang left miHoYo, he gave his phone number to Liu Wei and said, “If you have any questions, call me directly.”
Many years passed. Fortunately, Liu did not make that call. Alibaba’s core system had already been fully migrated to the cloud, realizing the goal of “flying with customers.”
Time passed quickly. In September 2020, after eight years of collaborations between miHoYo and Alibaba Cloud, miHoYo launched Genshin Impact. The Alibaba Cloud cloud sytem carries all of the servers of this game.
At the end of 2020, Genshin Impact’s popularity in the gaming industry rose continuously.
On November 30, 2020, Google awarded the “Google Play — Best Game of 2020” to Genshin Impact. One later, Apple awarded the “Annual Best Game of 2020 in the App Store” to Genshin Impact.
This year, Alibaba Cloud became the leader in the cloud computing industry. It serves as the digital base of many industries and provides services for many Chinese gaming enterprises.
Back in 2012, miHoYo developed an animation game called Guns Girl.
In 2012, the mobile gaming market was still in its infancy, and there were only a few developers of animation games. In the App Store, there were few mature mobile animation games. miHoYo’s leaders believed that there must be a place for a Chinese animation game in the future game market.
Coincidentally, the development process of cloud computing was somewhat similar to that of animation games. As a new technology, China’s cloud computing industry was also in its infancy at that time. However, miHoYo and Alibaba Cloud both had a dream about the future.
No one knows how to arrive in the future; the only thing they can do is believe.
In the traditional IT era, gaming companies had to purchase servers, build server rooms, and allocate O&M personnel. The mode, which launches a game by investing a large amount of money, has stopped many entrepreneurs with ideas from entering the gaming industry. At that time, only large companies could afford a game’s R&D.
Cloud computing brings new opportunities. Companies can skip all of the previous IT infrastructure investments and directly use cloud computing power with one click on the web page. This is a perfect product tailored for entrepreneurs like miHoYo.
When the first version of Guns Girl launched, miHoYo used two Alibaba Cloud cloud servers. This attempt started the miHoYo “cloud tour.”
Looking back, miHoYo was a representative of a generation of Internet venture enterprises. miHoYo deployed all of its businesses on the cloud. It enjoys the benefits of cloud computing, such as high elasticity, high concurrency, and low-cost. It can be called a “cloud-native enterprise.”
It is the rapid growth of this generation of “cloud-native enterprises” that has promoted the development and popularization of cloud computing in China.
In 2016, miHoYo launched its third game, Honkai Impact 3rd. However, miHoYo still had only two O&M personnel at that time, which was unimaginable in the era of traditional self-built server rooms.
As an adopter of early cloud computing, miHoYo encountered problems. Liu Xiao, the Technical Director of miHoYo, claimed, “Problems are difficult to avoid. The key is whether the problems can be solved.”
As the years have passed, miHoYo’s Team has gradually grown from three people to thousands of people. They have come a long way from the original small team without a meeting room.
miHoYo also reserved a meeting room for Alibaba Cloud engineers to use at any time.
At first, the business scale of miHoYo was small. miHoYo encountered some twists and turns in the process of expanding markets outside of China.
“Fortunately, the negative impact of this incident could be controlled because our business scale was small and there were not many users. If it happened on Honkai Impact 3rd or Genshin Impact, the loss would be immeasurable.” When talking about that, Liu Xiao still has some fear. He said, “For most game companies, the cloud is the best choice, and there is no other option.”
In 2016, miHoYo officially began expanding to markets outside of China. In these service areas, the cloud service provided by the foreign cloud vendor for Honkai Impact 3rd suffered several DDoS attacks by hackers. Once, the attacks even affected users’ normal access.
A DDoS attack is a very malicious network attack. Hackers send a large number of requests to the server within a short period, resulting in network congestion. Thus, users cannot access the server as normal.
For example, a restaurant that accommodates 100 customers opens. The owner of the restaurant next door feels unhappy and hires 150 hooligans to swarm the new restaurant. They occupy the seats but don’t order anything. As a result, ordinary guests that want to eat can’t be seated.
miHoYo also purchased additional security protection services to defend against attacks. Due to technical support and communication efficiency issues, the network congestion did not ease off in time.
At last, the miHoYo Team thought of Alibaba Cloud and came up with a method together with Alibaba Cloud engineers. Requests from players were sent to Alibaba Cloud Anti-DDoS Premium first and then were transferred back to the original server room after traffic cleansing. Thus, hundreds of GB of DDoS attacks were blocked. A problem that plagued miHoYo for a long time was solved once and for all.
By 2017, Alibaba Cloud had built more infrastructure worldwide. At the end of that year, Alibaba Cloud had 17 regions and 53 availability zones across Asia, Europe, and the USA. It was the best time for miHoYo to fully-migrate Honkai Impact 3rd to Alibaba Cloud.
Later, when planning Genshin Impact, miHoYo did not hesitate. The global resources of Genshin Impact were all provided by Alibaba Cloud. To date, the hacker attacks outside of China that miHoYo encountered previously have not appeared on Genshin Impact.
Genshin Impact is a great challenge faced by both miHoYo and Alibaba Cloud.
How grand is the plan of Genshin Impact? This is an open-world game, which means a huge map, various character images, complex level designs, and thorough storylines. miHoYo wants to make it a multi-platform and global-synchronous public beta game. This requires going online in five global service zones and simultaneous support on gaming platforms, including PS4, iOS, Android, and PC. Data in the same service zone are interconnected seamlessly.
For example, if you play Genshin Impact on your mobile phone and then switch to the computer, the gameplay and progress are seamlessly connected.
Moreover, miHoYo adopted a “general server” for a better online game experience.
Most game companies use the “server division” where one region has many servers. When the number of players reaches the limits of servers, more servers will be added. However, “general server” puts all users in a region on the same server cluster. The number of online players increases sharply in the same server.
After Genshin Impact launched, users from all over the world would pour in, the momentary peak concurrent users (PCU) could far exceed the estimated daily PCU.
If miHoYo designed such a product, can Alibaba Cloud support this high-concurrency, high-performance, and high-elasticity scenario?
Some werebe hesitant, but miHoYo chose Alibaba Cloud as always. This trust stems from years of cooperation. Over the years, miHoYo has witnessed Alibaba Cloud’s support for Taobao and Tmall during Double 11. Each year, the computing peak value supported by Alibaba Cloud is higher than the previous year.
“Although miHoYo is not an e-commerce company, technologies and games have very similar requirements for high concurrency. From the perspective of practices during Double 11, we believe that Alibaba Cloud’s technical capabilities are adequate.”
miHoYo also saw how Alibaba Cloud was making rapid progress. In 2017, Alibaba Cloud launched its proprietary database PolarDB. It provides services, including minute-level configuration upgrades and downgrades, second-level fault recovery, global data consistency, and data backup for disaster recovery.
For miHoYo, it took several hours to back up the data, and the game had to stop running for more than an hour when using the old database. However, with PolarDB, the backup duration can be shortened to seconds. This significantly reduces the time required for data backup and improves the efficiency of version updates.
In July 2020, Genshin Impact started the final closed beta on PS4, iOS, Android, and PC simultaneously.
Alibaba Cloud engineers came to the familiar meeting room again. In the last two months of closed beta, they became residents once again. They said, “Let’s take Genshin Impact as a project of Alibaba Cloud.” The accumulated capabilities of Alibaba Cloud over the years have become the technical basis for realizing the grand vision of Genshin Impact.
For example, the resource assurance capability is used, which engineers call “One Architecture, Global Deployment.” On different service nodes worldwide, Alibaba Cloud provides a set of architecture products from the foundation layer to the database. These products include cloud servers, network resources, storage, security, and databases. All of them adopt the same architecture and provide the same performance experience.
This provides a basic guarantee for Genshin Impact to open servers worldwide simultaneously. miHoYo only needs to build a single architecture globally. They do not need to change the architecture according to the local conditions of every service area.
The same architecture performance ensures that players in all five regions worldwide have a unified game experience. Players do not need to worry about the function inconsistency in different regions.
PolarDB reduces the investment of miHoYo in IT O&M with its perfect product packaging form while meeting the requirements of Genshin Impact’s high-performance gaming database. Thus, miHoYo personnel can focus on the game. “We focus on game design, and all basic IT works are left to Alibaba Cloud,” Liu Xiao concluded.
This is Alibaba Cloud’s biggest comprehensive practice in the game field. The requirements of Genshin Impact for Alibaba Cloud technologies have been met before in some other fields. However, this is the first time Alibaba Cloud has challenged all of the difficult tasks at once.
Judging from the global deployment effect and reservation amount, Genshin Impact would become the world’s largest game on the cloud after its launch. A few months later, the prediction became a fact.
Therefore, even the “hot migration” frequently used by Alibaba Cloud needs to be upgraded further in Genshin Impact.
Hot migration is used to completely save the running state of the entire virtual machine and restore the state to its original (or other) hardware platforms. Users will not be aware of any differences during usage.
It is similar to driving on the highway. When the engine alarms, it needs to be replaced without stopping the car’s functionality.
If changing the engine of a car is a normal state, it would be comparable to changing the engine of a large airplane during a flight for Genshin Impact. Due to the high stability requirements of the game, even the slightest jitter or offset of the airplane during replacement is not allowed.
Requirements are sent from the meeting room in miHoYo to the Apsara Park at Alibaba Cloud. Engineers from different departments, such as the gaming business and virtualization, gather to figure out solutions. They developed an intelligent O&M system to adjust the underlying performance. This can achieve lossless “AI hot migration” under ultra-high QPS, high CPU usage, and memory loads.
“AI hot migration” is applied to Genshin Impact with a very obvious effect.
In the last two months, the reservation amount of Genshin Impact and the server requirements have increased. Finally, after dozens of times of server scale-out and stress testing, Alibaba Cloud engineers felt that they finally made it!
On September 28, the globally synchronous public beta of Genshin Impact started at 10:00 am.
In that meeting room, more than 20 Alibaba Cloud engineers stared at the indexes on the dashboard with one peak after another.
In the end, the PCU peak reached a certain value, which was higher than everyone expected, but the cloud steadily withstood the pressure. The public beta was smooth and steady, just like countless previous stress tests that had been carried out before.
On the door of this meeting room, there was a name, Teyvat.
On the Teyvat continent, those selected by God will be awarded the “Eye of God.” They are called Genshin.
miHoYo is a first-generation cloud-native enterprise and is a perfect example of enterprises that implement cloud-native technologies. The way miHoYo flourished on the cloud can be a reference for enterprises that want to implement cloud-native. The cloud-native era has quietly arrived. More enterprises have seen its shining points, but implementing cloud-native and generating business value have become common problems that enterprises need to solve.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/quickly-build-a-cloud-native-e-commerce-platform-55070f456b6f?source=search_post---------283,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 19, 2019·8 min read
This article is based on the content of the SOFAStack Cloud Native Workshop event in KubeCon China 2019. This article will first go into the details of how cloud native emerged as a new and powerful alternative to a traditional centralized architecture, and then we will discuss how Ant Financial switched to cloud native for their architecture solution, Last, we will look at how you can quickly build a cloud native e-commerce platform like the one demoed at the SOFAStack Cloud Native Workshop at KubeCon.
On June 25, 2019, at KubeCon China 2019, Cloud Native Computing Foundation (CNCF), a world-renowned open-source organization, announced that Alibaba’s Ant Financial has now become an official CNCF gold member, being at the second top tier. On hearing the news, representatives from Ant Financial said that they would continue to increase support for open-source projects, including Kubernetes, Service Mesh, Serverless, and secure containers, and would exert their own strength.
At this conference, Ant Financial developers also worked to build a Cloud Native e-commerce platform with hundreds of Cloud Native enthusiasts in just five hours. So you may be wondering how did they do it so quickly anyway?
In this article, we’re going to see just how they managed to build this cloud native e-commerce platform so quickly. At the same time, we will also discuss some highlights of the discussions and topics covered at the workshop.
Over the past decade or so, great changes have taken place in technology. In a quick summary, we at see that at the beginning of the 21st century, most enterprises were still using a centralized architecture. However, already at this stage, enterprises began to do some information construction work, such as cluster deployment (with Tomcat and Weblogic clusters) to ensure higher levels of system availability, and the procurement of commercial software and hardware products, such as those of IOE (which stands for IBM, Oracle, and EMC), to meet the demands of business growth through higher configurations, better performance, among other things.
However, gradually as more companies expanded and scaled outwards, centralized architectures could no longer sufficiently support the complex business systems that developed. At this point in time, many enterprises began to make some system splitting transformations, such as using the service-oriented architecture (SOA). After the system splits, the previously expensive minicomputer was no longer needed to deploy services, and the deployment of virtual machines gradually became more and more mainstream. Similarly, service-oriented databases and storage were no longer required to adopt commercial software and hardware solutions, and enterprises are turning to open source solutions, such as replacing Oracle with MySQL.
System splitting has many advantages, for example, it delivers business cohesion, loose coupling between systems, and convenient and fast iteration. However, the problems that come with it are also pretty obvious. For one, as the number of systems split increases, the interaction between systems becomes more complex and long call links may cause performance problems. Next, systems of distributed data storage and other data consistency also pose many challenges in addition to other problems such as resource allocation and isolation after being servitization. At this time, some virtualization and containerization technologies began to emerge, which were at the time most often OpenStack and Docker. OpenStack helps to solve the construction and management problems of the Infrastructure-as-a-Service (IaaS) layer, while Docker provides best practices for resource isolation. But these do not solve complex O&M problems.
In recent years, some new Cloud Native products and concepts have emerged, such as Kubernetes, Service Mesh, and Serverless, which can solve some practical problems of complex application deployment and O&M.
Alibaba’s Ant Financial began to shift from a centralized architecture to a distributed architecture in 2007. The team at Ant Financial integrated a set of financial-level distributed architecture developed independently during the process of technological evolution over the past decade into the SOFAStack™ (Scalable Open Financial Architecture Stack).
From 2007 to 2012, Ant Financial has completed modular and service-oriented swift of all business systems. The TCC model solved data consistency problems brought about by servitization and data splitting, and the registration center solved the problem of a single point of service.
After the service-oriented transformation, as the service cluster scale increased, system scalability ran into a bottleneck. In addition, to meet financial industry requirements, Ant Financial has put forward higher requirements on system availability and data consistency. Since 2013, Ant Financial has explored a set of unitization ideas. Based on this, capabilities, such as zone active-active redundancy, active geo-redundancy, and auto scheduling, have been launched to ensure that the business continues operation and data is not lost.
After that, with the rise of internet finance in China and the internationalization of Ant Financial, Ant Financial has opened up its own capabilities and technologies to the financial cloud in the form of cloud products, and developers can quickly build a financial-level distributed system based on this. At the same time, some internal practices are also made open-source.
Since 2017, the concept of Cloud Native is developing rapidly. In the face of the opportunities and changes brought by Cloud Native, Ant Financial’s strategy was to actively “embrace” Cloud Native. The ideas and concepts brought by Cloud Native can be used to solve some internal scenarios and problems encountered by Ant Financial.
For example, Service Mesh can solve the problems at the lower level of basic capabilities (such as middleware), while at the same time Serverless can solve the problem of R&D efficiency, and ensure business development focuses on the business. These new technologies and concepts will be explored internally and implemented in production by Ant Financial. Recently, we have shared the practical summary on large-scale implementation for the first time at the [Global Internet Architecture Conference (GIAC))( http://www.conference.cn/gntc/2017/en/), Shenzhen. At the same time, we will also make these Cloud Native practices open-source, and work with the community to promote and build financial-level Native Cloud standards.
On April 19, 2018, it was officially announced that SOFAStack would be gradually open-source. The strategy is Open Core, that is, the core interfaces and implementations are all open-source, and old compatible code is kept internally. Up to now, in almost one year and two months, more than a dozen projects are already open-source, with a total of more than 25,600 stars, more than 120 contributors, and more than 30 production users. Recently, Ant Financial has also certified two community committers. Ant Financial plans to continue to optimize and expand the open source map.
In the figure below, it can be seen that many technical components related to microservices are open-source under the SOFAStack system, such as SOFABoot and SOFARPC. In addition, we have also been compatible or integrated with other excellent open-source products in the community. Using these components, we can quickly build a financial-level distributed architecture system. Open source code can be found at the GitHub address below this figure. In this Workshop, we will use some open-source technical components.
Ant Financial also integrated SOFAStack capabilities into cloud products, as shown in the figure below:
The figure above shows the architecture of SOFAStack on the cloud. We can see that SOFAStack provides a complete solution for the external market. What supports the solution is the capabilities, such as distributed middleware and cloud application engine, to be experienced this time. In addition, we also provide the well-developed R&D efficiency platform service and the technical risk prevention and control platform. As for this part, more detailed introduction and experience will be covered in the afternoon.
After all this discussion, now it’s time to show you how you can quickly build a cloud native e-commerce platform as demoed during the conference. In this demo, you will learn how to use the open-source version of SOFAStack and various related cloud products to build an online e-commerce platform within five hours or so.
The following is a brief introduction to the content of this Workshop, as shown in the figure below:
In the first half of the tutorial, you will do the following:
Then, in the latter half of the tutorial, you will do this:
Above is shown the system architecture diagram of the online bookstore. At the top is the deployed infrastructure, including the registration center SOFARegistry, the service console SOFADashboard, and the monitoring metric SOFALookout. We have prepared this part in advance.
Under this infrastructure is the content of the service. For convenience, we will not perform separate front-end and back-end deployments. This time, we only need to operate 2 applications: (1) the Web page system and the inventory system on the left to provide inventory operation services and (2) the accounting system on the right to provide balance-related services. When a user’s purchase request comes in, the inventory system needs to call it to the accounting system remotely through RPC.
In addition, the inventory service and the balance service correspond to independent databases respectively, and the Seata distributed transaction framework will be used to solve the problem of data consistency under distributed environment.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-native-image-distribution-system-dragonfly-announces-to-join-cncf-c3759f52954?source=search_post---------284,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 20, 2018·2 min read
Today Cloud Native Computing Foundation( CNCF ) announces: cloud native image distribution system Dragonfly joins CNCF as a sandbox level project. Dragonfly’s joining CNCF is a milestone event for both. Dragonfly’s value has been recognized by industry, and it will collaborate with community and the whole ecosystem to work on solving enterprises’ problems in digital transformation. For CNCF, it firstly adopts distribution as a new layout to speed up business delivery efficiency.
Dragonfly is an open source project from Alibaba Group which aims to tackle image distribution problems in distributed orchestration systems based on Kubernetes. In the era of digital transformation, industry enterprises are all evolving to adopt micro-services, and try to take advantage of cloud platform to optimize business management. Dragonfly is sourced from Alibaba, trained in actual scenarios, and solves three aspects problems in cloud native image distribution proactively:
efficiency, flow control and security:
“Dragonfly’s joining CNCF is one of the most convincing evidence that Alibaba embraces cloud native actively,” Lin Hao (nick: Bixuan), the principle of Alibaba Research Efficiency, says, “We are honored to cooperate with CNCF globally to promote Dragonfly’s adoption all around the world.”
Dragonfly started in June 2015 from Alibaba, and becomes one of Alibaba’s core fundamental infrastructure technologies in 2017. In November of that year, Dragonfly is announced open source with Alibaba’s container engine PouchContainer. With being open source for one year, Drgaonfly is favored by industry. Currently, Dragonfly has been used in fields including e-commerce, telecom, financial, internet and so on. And featured companies include Alibaba, China Mobile, Ant Financial, Huya, Didi, iFLYTEK, qunar.com and so on. Just because wide adoption in ecosystem, Dragonfly has a smooth journey to join CNCF as a sandbox level project.
Embracing CNCF as a sandbox project is far away from the finish time. Dragonfly is committed to continue working with technical roadmap, shaping bright future of image management in cloud native area. Dragonfly aims to providing general services in Kubernetes out of box. In aspect of community, Dragonfly will work with ecosystem end-users and developers to make it successful. For more details about Dragonfly, please follow @dragonfly_oss and see Github https://github.com/alibaba/Dragonfly.
Reference:https://www.alibabacloud.com/blog/cloud-native-image-distribution-system-dragonfly-announces-to-join-cncf_594177
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/introducing-data-lake-analytics-serverless-cloud-native-and-zero-upfront-cost-c076b429a0b2?source=search_post---------285,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 20, 2018·3 min read
Alibaba Cloud Data Lake Analytics (DLA) is a serverless, high performance interactive query service that requires zero infrastructure setup or upfront cost. Customers can use DLA to query data stored in a number of Alibaba Cloud services, such as objects in OSS, key-value pairs in Table Store, and relational data in RDS. Customers can explore a single dataset at a time or analyze data across multiple sources simultaneously, and DLA delivers fast, interactive responses with a Massive Parallel Processing (MPP) architecture. DLA is fully compatible with SQL syntax which customers are already familiar with and therefore have an extremely low learning curve. In addition, DLA works with popular BI tools natively so that customers can easily visualize their data, derive insights, and accelerate decision-making.
Fig.1: Data Lake Analytics in Alibaba Cloud
DLA is designed for those who need cloud-native analytics and are seeking a cost-effective solution that can turn raw data into real-time insights. Compared with conventional analytics platforms which require upfront hardware provisioning and setup, DLA allows customers to run queries on demand with no upfront costs and quickly explore ideas on new data while only paying for the resources consumed. Designed for developers, business analysts, and data scientist where a self-service operational model is highly preferred, DLA provides built-in ETL (Extract, Transform and Load) functionality that reduces the heavy lifting of pre-processing data of different formats and origins so customers can focus on analysis and insights.
Serverless: For enterprise users, the service is made available on a Pay-As-You-Go basis with ZERO maintenance effort. Key benefits include but not limited to instant startup, transparent upgrade, and elastic QoS.
Database-like User Experience: DLA offers standard SQL interface, with outstanding SQL compatibility and comprehensive built-in functions. Based on JDBC/ODBC connectivity, users are granted with fast and convenient service access, as well as low migration cost. Meanwhile, integration with BI product enables DLA to turn big data into consumable insights and visualizations. With database experience, DLA helps customers accelerate their cloud migration process.
Deep Ecosystem Integration: DLA enables complex analytics for data coming from different sources with various formats. Not only user can leverage DLA to analyze data stored on Alibaba cloud OSS and Table Store respectively, but also, they can join the results and thus generate new insights.
Optimized Performance: DLA fully leverages Massive Parallel Processing (MPP) architecture, vectorized execution optimization, multi-tenancy resource allocation and priority scheduling to achieve optimized performance.
For raw data (logs, CSV, JSON, Avro, etc.) persisted on a given storage, OSS for example, you can query specified objects/files/folders without having to go thru a complex ETL process, and get the results back instantly
Fig.2: Query OSS Data
When dealing with multiple data sources, OSS and Table Store for example, DLA enables JOIN operation across heterogeneous data sources, turning your big data into consolidated insights.
Fig.3: Joint Analytics Across OSS and Table Store
Data Lake Analytics is now available in the Singapore, China (Hangzhou, Shanghai, Beijing), and the UK regions, and would be coming soon to US (West) and Australia regions. To get started on Data Lake Analytics, watch the DLA webinar here or visit the product page here.
Check out the latest offers for Data Lake Analytics at www.alibabacloud.com/campaign/data-lake-analytics-2018
Reference:https://www.alibabacloud.com/blog/introducing-data-lake-analytics%3A--serverless%2C-cloud-native%2C-and-zero-upfront-cost_594182
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/building-the-open-data-stack/2-questions-to-ask-when-developing-cloud-native-apps-e58e4480ce68?source=search_post---------286,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataStax
May 25, 2021·2 min read
Check out our new eBook on working with Kubernetes and Cassandra
The cloud has transformed enterprise technology in ways that nobody could have imagined a decade ago. Enterprise development teams are trying to adapt at lightning speed to building, deploying, and running cloud-native applications. They’ve had to learn new ways to work with distributed application architectures and containerized environments and are mastering the challenges of designing applications that run flawlessly at a massive scale.
Success with cloud-native applications requires enterprises to make weighty platform choices that will shape everything they do — for better or worse — for years to come. Organizations face two critical questions when considering how to successfully build apps in the cloud:
How can development teams ensure that the applications they run in the cloud will perform at scale, minimize complexity and cost, and adapt easily to multi-cloud and hybrid cloud environments?
How can they deploy cloud-native technology stacks that keep applications and data management systems fully aligned and seamlessly integrated?
We dive into the answers in a new eBook for enterprise IT leaders, “Working and Winning with Kubernetes and Cassandra.” But here’s the TL;DR: Kubernetes and Apache Cassandra™ have emerged as ideal platforms for cloud-native application development and cloud-native data management. And now, forward-thinking enterprise development teams are taking the next step: integrating Kubernetes and Cassandra into a single, comprehensive platform for building and running cloud-native, data-driven applications.
The eBook explores why Kubernetes and Cassandra are so well-suited to the roles they play today and why they’re even more powerful when they work together.
Check out “Working and Winning with Kubernetes and Cassandra” to learn more.
DataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra®.
See all (229)
1 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
We’re huge believers in modern, cloud native technologies like Kubernetes; we are making Cassandra ready for millions of developers through simple APIs; and we are committed to delivering the industry’s first and only open, multi-cloud serverless database: DataStax Astra DB.
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/cloud-native-compute-engine-challenges-and-solutions-f62aec2fa6c4?source=search_post---------287,"There are currently no responses for this story.
Be the first to respond.
Figure 1 shows E-MapReduce (EMR) architecture based on Elastic Compute Service (ECS). As an open-source big data ecosystem, the EMR architecture provides an indispensable open-source big data solution for every digital enterprise in the last decade. It has the following layers:
Each layer has many open-source components that constitute the classic big data solution — the EMR architecture. Here are our further considerations about it:
Based on the considerations above, let’s focus on the concept of cloud-native. Kubernetes is a more promising implementation for cloud-native. Therefore, when we mention cloud-native, it is actually about Kubernetes. As Kubernetes becomes increasingly popular, many customers are showing interest in this technology. Most of the customers have moved their online business to Kubernetes too. They want to build a unified and complete big data architecture on a similar operating system with such a migration plan. Therefore, we summarize the following features:
Figure 2 shows the architecture of the EMR compute engine on Container Service for Kubernetes (ACK). As a Kubernetes solution of Alibaba Cloud, ACK is compatible with APIs of the Kubernetes community version. ACK and Kubernetes are not differentiated in this article and represent the same concept.
Based on the initial discussion, we believe that the more promising batch processing engines of big data are Spark and Presto. We will add some more promising engines gradually as we iterate the ACK version.
The EMR compute engine provides products based on Kubernetes. This is essentially a combination of Custom Resource Definition (CRD) and Operator, which is also the basic cloud-native philosophy. We classify components into service components and batch components. According to this classification, Hive metastore is the service component, and Spark is the batch component.
The green parts in the following figure are operators, which are improved in many aspects based on open-source architecture. They are also compatible with the basic module of ACK on the product layer. This helps to perform control operations in the cluster conveniently. The right section in the figure contains components of log, monitoring, data analytics, and ECM control, which are the infrastructure components of Alibaba Cloud. Let’s discuss the features of the bottom part:
It is relatively easy to deploy Presto in ACK as it is a stateless MPP architecture. Thus, this article mainly discusses the solution of Spark on ACK.
Generally, Spark on Kubernetes faces the following challenges:
Let’s look at the solutions that address these problems:
To solve the Spark Shuffle problem, we designed the Shuffle R/W separation architecture called Remote Shuffle Service. First, let’s explore the possible reasons for refusing to use cloud disk in Kubernetes:
Therefore, the remote Shuffle architecture can significantly optimize the existing Shuffle mechanism to solve this problem. It shows a lot of control flows in figure 3, which we will not discuss in detail here. For more information about the architecture, see the article EMR Shuffle Service — a Powerful Elastic Tool of Serverless Spark. The focus here is the data flow All Mappers and Reducers of executor marked in blue are running in the Kubernetes container. In the figure, the middle architecture is the Remote Shuffle Service. The blue part of Mapper writes Shuffle data remotely into service, eliminating the dependency of the executor’s task on the local disk. Shuffle service performs merge operation on data in the same partition from different Mappers and then writes the data into the distributed file system. In the Reduce stage, the Reducer can improve the performance by reading files sequentially. The major implementation difficulties of this system are the control flow design, fault tolerance in all aspects, data deduplication, and metadata management.
In short, we have summarized the difficulties in the following three points:
Regarding performance, figure 4 shows the Benchmark score of TeraSort. The reason for choosing the TeraSort workload for testing is that it is a large Shuffle task with only three stages. Therefore, it is very easy to observe the changes in Shuffle performance. In the left part of the figure, the blue bars show the runtime of the Shuffle service, and the orange ones show the runtime of the original Shuffle. With data volumes of 2 TB, 4 TB, and 10 TB, it is clear that the larger the data volume is, the more obvious the advantage of Shuffle service is.
In the right part of the figure, the performance improvement is reflected at the Reduce stage. The duration of Reduce with 10 TB of data is reduced from 1.6 hours to 1 hour. Earlier, we have explained the reason clearly. Those familiar with the Spark Shuffle mechanism know that the original sort Shuffle is M*N times of random IO. In this example, M is 12000 and N is 8000. Remote Shuffle has only N times of sequential IO. Therefore, a remote Shuffle with 8000 times of sequential IO is the fundamental reason for performance improvement.
Other aspects of EMR optimization are as follows:
In general, the EMR version of Spark on ACK has greatly improved in architecture, performance, stability, and usability.
From our perspective, the direction for cloud-native containerization of Spark is to achieve unified O&M with cost-effectiveness. These are our summaries:
Implementing native-cloud big data has many challenges. To solve these challenges, the EMR team collaborates with communities and its partners to develop better technologies and ecosystems.
Here are our visions:
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@ghaff/idc-survey-says-go-cloud-native-but-modernize-too-a39028d9ade4?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gordon Haff
Jan 7, 2016·3 min read
IDC’s recent survey of “cloud native” early adopters tells us that existing applications and infrastructure aren’t going away. 83 percent expect to continue to support existing applications and infrastructure for the next three years. In fact, those who are furthest along in shifting to distributed, scale-out, microservices-based applications are twice as likely to say that they are going to take their time migrating than those who are less experienced with implementing cloud native applications and infrastructure. It’s easier to be optimistic when you haven’t been bloodied yet!
IDC conducted this survey of 301 North America and EMEA enterprises on Red Hat’s behalf; the results are published in an December 2015 IDC InfoBrief entitled Blending Cloud Native & Conventional Applications: 10 Lessons Learned from Early Adopters.
It’s worth noting that even these cloud native early adopters plan to also modernize their existing conventional infrastructure. For example, in addition to the 51 percent continuing with their virtualization plans, 42 percent plan to migrate to software-defined storage/networking and to containerize applications currently running on virtual or physical servers.
This is an important point. The bimodal IT concept — originally a Gartnerism but now used pretty widely to connote two speeds or two modes of IT delivery — is sometimes critiqued for a variety of reasons. (To be covered in a future post.) However, perhaps the most common criticism is that Mode 1 is a Get Out of Jail Free card for IT organizations wanting to just continue down a business as usual path. This survey shows that those furthest along in transitioning to cloud-native don’t see things that way at all. (It should be mentioned that Gartner doesn’t either and sees modernization as a key component of Mode 1.)
Open source was almost universally seen as playing a key role in any such strategy with 96 percent viewing open source as an enabler of cloud native integration and conventional app modernization. No surprises there. An earlier IDC survey on DevOps early adopters found a similar view of open source with respect to DevOps tooling.
The study also found that security and integration were important elements of a cloud native transition strategy. For example, 51 percent identified security, user access control, and compliance policies as a technical factor that would have have the greatest impact on their organization’s decisions about whether applications are best supported by conventional or cloud native architectures.
The #2 factor (42 percent) was the ability to support/integrate with existing databases and conventional applications — highlighting the need for management tools and process integration between new applications and existing workflows, apps, and data stores. Business Process Optimization was identified as an important integration element. Strategies included structured and unstructured data integration, business process automation, model-driven process management, and the use of an enterprise service bus and cloud APIs.
If I had to choose one word to convey the overall gestalt of the survey, I think I’d choose “pragmatic.” IDC surveyed cloud native early adopters, so these are relatively leading edge IT organizations. Yet, these same organizations also emphasized SLAs and minimizing business risks. They stress avoiding business and end-user disruption. They plan to transition gradually.
Originally published at bitmason.blogspot.com on January 7, 2016.
Red Hat cloud guy, photographer, traveler, writer. Opinions are mine alone.
Red Hat cloud guy, photographer, traveler, writer. Opinions are mine alone.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chrischinchilla/the-weekly-squeak-ebpf-cloud-native-computing-with-isovalant-47d478e27080?source=search_post---------289,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Chinchilla
Feb 12, 2021·3 min read
I speak with Neela Jacque about eBPF and what does Cilium and Isovalent offer to cloud-native networking and security.
I also cover the rise of VSCode, the re-rise of co-working, Telegram v Signal, and more!
xx Chinch
"
https://medium.com/altoros-blog/cloud-native-buildpacks-how-to-create-a-custom-buildpack-63b8aca4ae8f?source=search_post---------290,"There are currently no responses for this story.
Be the first to respond.
In previous blog posts, we explored the benefits of Cloud Native Buildpacks (CNB) and explained how to create a custom CNB stack. With code samples, this tutorial demonstrates how to create a CNB buildpack consisting of config files and custom scripts. This allows for added flexibility of CNB, while being able to operate in complex technology environments.
For more, read the full article on our blog.
www.altoros.com
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-attraction-of-cloud-native-to-the-cloud-computing-ecosystem-2a13459a5a70?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 25, 2021·9 min read
On February 2, 2021, the Cloud Native Computing Foundation (CNCF), the world’s top open-source community, officially announced its new term of Technical Oversight Committee (TOC). Zhang Lei, a Senior Technical Expert of Alibaba Cloud, was elected as the only representative from a Chinese enterprise among the current nine TOC members.
CNCF explained in its official announcement that Zhang Lei was elected because of his outstanding contributions to Kubernetes. “Zhang Lei is a Co-Maintainer of the Kubernetes community and a Co-Chair of CNCF App Delivery SIG. He is in charge of the Kubernetes and large cluster management systems at Alibaba.”
Founded in July 2015, CNCF is an affiliate of the Linux Foundation. CNCF devotes itself to maintaining and integrating open-source technologies while focusing on cloud-native to serve cloud computing. It also supports application orchestration based on the containerized microservice architecture. Currently, CNCF has more than 300 member companies, including AWS, Azure, Google, Alibaba Cloud, and other mainstream global cloud computing vendors. CNCF TOC is composed of nine representatives with rich technical knowledge and industry background. Those representatives provide technical support and guidance for the cloud-native community.
Since 2017, Alibaba has invested a lot in cloud-native technologies and participated in the development and maintenance of many top-level open-source projects, such as ETCD, Kubernetes, and ContainerD. It has also completed the self-upgrade of the overall infrastructure through the cloud-native technology stack. Some star projects, such as the exclusive open-source Dubbo and RocketMQ, are donated to the Apache Foundation and graduated as top projects. Since the release of open-source Spring Cloud Alibaba, it has become the most active Spring Cloud implementation of Spring Cloud with the best development experience. Dragonfly has become a CNCF incubation project, and OpenYurt and OpenKruise have entered the CNCF Sandbox. What’s more, Alibaba has released the world’s first Open Application Model (OAM) with Microsoft Azure to lead the cloud-native standard application delivery ecosystem. After the release of OAM, Alibaba launched KubeVela, a core engine of the cloud-native platform based on OAM. KubeVela is the implementation of the Kubernetes ecosystem in constructing standard application delivery systems. Together with Nanjing University, Alibaba has opened Fluid, a cloud-native infrastructure project. Fluid is an important component for big data and AI to embrace the cloud-native. In addition, Alibaba Serverless Devs has become the first Serverless Tool from China among CNCF Landscape Tools.
By the end of 2020, over ten Alibaba projects entered the CNCF Landscape, which ranks among the top ten in the world in terms of the number of projects in the Kubernetes community.
Currently, Zhang Lei is the only member of CNCF TOC from a Chinese enterprise. Born in 1989, he is one of the youngest early maintainers of the Kubernetes community. He initiated and participated in the design of multiple Kubernetes features, such as Container Runtime Interface (CRI), equivalent class scheduling, and topology resource management. With his continuous influence in the Kubernetes community, Zhang Lei was elected as the CNCF official ambassador in 2016 and served as the KubeCon reviewer and Keynote Speaker for several consecutive years. In 2019, Zhang Lei was elected as the Co-Chair of the CNCF Application Delivery Team. So far, he is the only Chinese Co-Chair among the seven CNCF teams.
At Alibaba, Zhang Lei has participated in the design of the cloud-native application infrastructure of Alibaba Cloud and the maintenance of the largest public cloud container cluster in China. His “application-centered” standard application delivery system gives birth to a series of prospective and leading open-source technologies for cloud-native application management.
Zhang Lei and his team jointly developed OAM with CTO Office of Microsoft Azure. OAM is the first standard model and framework for cloud-native application delivery and management in the industry. It has quickly become the model for several enterprises worldwide to build their cloud-native application platforms, such as MasterCard and 4Paradigm. It has also become a standard project of the China Academy of Information and Communications Technology (CAICT) and the Ministry of Industry and Information Technology (MIIT) called “General Requirements and Reference Framework for Cloud Computing Open Architecture .” OAM has been listed as one of the “Top Cloud-Native Technology Trends of 2020” by TheNewStack and “Top 10 New Open-Source Projects in 2020” by InfoQ.
Today, no one will question the rationality of a platform team’s adoption of Kubernetes as its infrastructure. In 2020, Kubernetes projects have almost achieved the final goal. They aim to bring platform-layer abstractions to the cloud computing infrastructure, which allows the platform team to construct “everything” based on these abstractions.
However, what is “cloud-native” exactly? Why is it so attractive to the cloud computing ecosystem? What is the future of cloud-native in China? Let’s learn about the opinions of Zhang Lei through a Q&A session:
Q: Congratulations! You are now one of the nine CNCF TOC members! Let’s have a brief self-introduction first, shall we?
Zhang Lei: Currently, I am responsible for technical work related to the infrastructure of the cloud-native application platform at Alibaba Cloud. I am also involved in promoting the construction of Alibaba’s core open-source projects, such as OAM/KubeVela, OpenKruise, and OpenYurt. Before working for Alibaba Cloud, I mainly worked for the upstream part of the Kubernetes community. I was one of the early initiators and maintainers of multiple core features, such as CRI and scheduler. I was also a member of the KataContainers project team. Recently, my team and I have worked with CNCF TOC and over 40 companies to promote the establishment of a vendor-neutral working group for GitOps application delivery.
Q: What is your opinion about the development and evolution of cloud-native in recent years?
Zhang Lei: Cloud-native technologies are becoming more popular. We can see that this new way of application delivery is combining with key technologies, such as the standard application model and Mesh-based progressive release. The combination has become the mainstream method of building application platforms in the industry.
The well-known cloud-native is a set of best practices and methodologies for using cloud computing technologies to reduce costs and improve user efficiency. Therefore, the cloud-native has innovated and evolved since its birth.
Whether it is the great success of container technologies represented by Docker in 2014, or the rapid rise of container orchestration technologies represented by Kubernetes in 2019, or the almost “all-encompassing” ubiquity of cloud native today, the concept of cloud native is continually evolving from theory to practice, and has successfully helped us develop new ideas and architectures. The continuous evolution of cloud-native is based on its core idea, which gradually affects all aspects of the cloud computing field. This is a main development trend of the cloud-native ecosystem in recent years.
Q: What do you think were the major changes in the cloud-native field last year? What influences will they bring?
Zhang Lei: In 2020, we can see that the rapid popularization of cloud-native is bringing changes based on the cloud to more fields. These fields can be integrated into the capability pool of cloud computing through the cloud-native system, which reduces costs and improves efficiency for end users. Let’s use the CNCF open-source community as an example. In 2020, OpenYurt and OpenKruise from Alibaba entered the CNCF Sandbox. Virtual Cluster from Alibaba became an official sub-project of Kubernetes, and many core projects, such as OAM/KubeVela were incubated. The emergence and popularization of these open-source technologies promote the continuous development and evolution of the cloud-native ecosystem. They are also making it a reality to provide the benefits of cloud computing.
In 2020, cloud-native remained the fastest-growing element in the entire cloud computing ecosystem. With this growth momentum, it is already time to think about the future developments of cloud-native. Many cloud vendors and teams are actively investing in and exploring the future cloud-native technologies in different fields.
Q: What do you think cloud-native will become in the future?
Zhang Lei: Today, cloud-native is getting closer to the idea that “software is naturally born and grows on the cloud.” However, the development of cloud-native has also revealed many problems. For example, the original cloud-native technology focused too much on infrastructure abstraction and management while neglecting the end-user experience. Fortunately, the changes in the cloud-native field in 2020 have shown that the cloud-native community is gradually moving closer to end users by submerging capabilities and producing value. This explains the influence brought by many cloud-native technologies after Kubernetes. For example, Service Mesh is rapidly changing middleware and microservice governance technologies. GitOps is having a critical influence on the continuous delivery field. OAM and Dapr are solving the problems of application abstraction models and service access models.
Over the next few years, we expect the inherent agility and user stickiness of cloud-native technologies will be applied further. Cloud-native will witness further application in more vertical fields, such as database, AI, and edge, together with the huge capability pool of cloud computing. Thus, cloud-native will widely change the underlying infrastructure of cloud computing and the deployment and distribution of cloud applications. It may directly reflect the “ubiquity of cloud computing” in the future.
Q: What areas will you focus on after being a TOC member?
Zhang Lei: In the future, I will work with CNCF TOC to continuously focus on upper-layer technology fields that can bring direct value to end users, such as application management and delivery, cloud-native programming models, and cloud developer experience. Together with the community, I will help better incubate and introduce potential open-source projects in these fields to CNCF. At the same time, TOC will make long-term plans, especially on key underlying technologies, such as WebAssembly and eBPF, which have recently risen rapidly. Don’t be surprised if Kubernetes is no longer the best-known project in CNCF in the near future.
Q: Do you have any ideas or suggestions on how to promote the development of the cloud-native ecosystem in China?
Zhang Lei: Many people think that the best and most robust cloud-native community in the world today is in China since Google and AWS do not use Kubernetes. Today, the cloud-native community in China has gathered many professionals, and the cloud-native scenarios and environments are growing rapidly. These are what cloud-native developers around the world dream about. They facilitate the rapid development of the cloud-native ecosystem in China and help release the benefits of cloud-native.
Therefore, as a member of the cloud-native ecosystem from China, we should not follow and be bolder in innovation. At the same time, we should be more confident to introduce our technologies to the world and actively work with some neutral organizations to speak our minds. These organizations, such as CNCF, must have global influence and consider our discourse power. We should actively incorporate users, participants, and contributors from North America and Europe into our community. The full collaboration of various Chinese cloud-native vendors, community members, and open-source project maintainers is required. I think that the nearly-fully-localized KubeCon summit and the cross-company and cross-region Cloud-Native Programming Hackathon will soon bear fruit in the Chinese cloud-native community.
What exactly is cloud-native? Since its emergence, many engineers, teams, and enterprises that have encountered cloud-native have asked this question.
Cloud-native is a set of best practices and methodologies that “uses cloud computing technologies to reduce costs and improve efficiency for users.” Cloud-native is in continuous self-evolution and innovation. As Zhang Lei said in an article, “This continuous vitality with “no exact definition” is the source of the attraction of “cloud-native” to the cloud computing ecosystem.”
Zhang Lei said, “Cloud-native requires continuous thinking, accumulation, and innovation from the entire cloud-native community to implement and update cloud-native technologies. By doing so, cloud-native can produce more value and a better experience for end users with its technologies. It realizes the goal of building a simple and easy-to-use cloud-native platform. This is what I have always invested in.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/why-its-easier-than-ever-go-cloud-native-with-alibaba-cloud-f0b241f94e83?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 20, 2019·9 min read
By Liu Sheng.
The cloud-native era is upon us and it is reshaping the entire cloud application lifecycle.
As one of the first to invest in cloud-native technologies, Alibaba Cloud’s Container Service team has been helping thousands of customers containerize their services and migrate to the cloud. Container Service’s customers are a diverse group of businesses that include Alibaba Cloud’s top 10 customers, overseas enterprises looking to expand operations in China, and customers who made the move from other cloud vendors. Several customers left their on-premises data centers behind, opting to migrate all services to Alibaba Cloud. And now, increasing numbers of new customers are looking to do the same. The business requirements of customers differ, and as a result, they require different customizations for their container platforms. However, out of all the use cases, some things are common among all of our customers. We consider these as best practices, tools, and solutions that can help all of our customers to migrate to the cloud, quickly and in a seamless manner. The main idea behind this article is to share these best practices, tools, and solutions.
Before each migration starts, there are three questions we must answer. The first question is how to ensure reliability, stability, security, and flexibility for our customers’ services by using Alibaba Cloud Container Service for Kubernetes (ACK). The second question is how to design the migration process to ensure a smooth, seamless migration. The third question is how to make further improvements to provide greater scalability with ACK.
ACK is built on the stable foundation of Alibaba Cloud’s Infrastructure-as-a-Service (IaaS) platform and provides such advantages as low costs, a maximum of scalability, and global integration with Alibaba Cloud’s other services. Moreover, ACK runs under the security umbrella of Alibaba Cloud, which covers everything from infrastructure to containers and from the bottom to the top service layers. Over the years, ACK has supported thousands of customers to run their services smoothly, accumulated massive data on user experience, and hosted Alibaba’s annual Double 11 Shopping Festival. ACK is built on the standard of Kubernetes and greatly improves the capabilities most requested by our customers and users alike. Therefore, our users do not have to worry about vendor lock-in, given all of advantages we have to offer.
Most of our customers host their services on self-hosted Kubernetes clusters before migrating to ACK. What convinced them to migrate to our services are the advantages of low costs, scalability, integration with the Alibaba Cloud’s complete infrastructure, great performance, good security and the overall customer experience that ACK holds over standard Kubernetes.
Besides all of this, ACK is available in all regions where Alibaba Cloud has rolled out cloud services, which means that, aside from multiple regions in China, ACK is also available in Southeast Asia, the Middle East, Europe, and the US, to meet the demands of our increasingly global customer base.
Complete service migration involves cluster planning, data migration, monitoring switchover, log switchover, and the final step, production traffic switchover.
Therefore, a user must have a clear understanding of the components, data, and services involved in migrating to ACK. Let’s start with planning the cluster and its many aspects. First up is the server model. Do you need CPU-heavy or GPU-heavy servers? Is Elastic Container Service or X-Dragon Bare Metal Server better for your services? Then, there are the network resources. Do you want VPC private networks or classic networks? Do the pods in your cluster use Flannel or Terway to communicate? Next is how much storage is needed. We recommend that most users start with the amount of storage needed to get your service up and running and to set up dynamic scaling.
Security is also very important. Infrastructure security requires that you have well-configured rules and container image security might require private registries and periodic security scans. Application security including network security policies that guard the communication among various services is essential. After security comes monitoring. ACK provides monitoring that is more comprehensive than self-hosted clusters, ranging from the complete infrastructure to individual containers. You can also set alerts using various threshold values. For this, Alibaba Cloud Log Service is our all-in-one, complete service suite for log data. Most of our customers opt to use Log Service when they migrate to ACK.
Data migration is arguably the most important part of the whole effort. This includes the migration of databases, storage data, and container images. Data migration is a very important part. The data includes database data, storage data, and container images. To ensure the process is smooth and safe, Alibaba Cloud offers a set of enterprise-grade products and migration tools. We also offer an application transformation service to improve our customers’ services so that they can get the most out of ACK. This includes but is not limited to updating container image addresses, optimizing the way services are exposed, and storage drive mounting methods.
Last but not least, we offer a CI or CD solution to our customers so that they can achieve rapid service iteration. After the preceding steps are implemented and tested, production traffic is gradually switched over. From cluster planning to traffic switchover, these are the steps involved to perform a migration to ACK.
The preceding table is a lifecycle model of an enterprise containerized application. This model is based on the aspects and roles of an application. For example, a business architect needs to focus on what value the move to the cloud can bring to the company, what benefits it brings to the total-cost-of-ownership (or TCO) and different business scenarios, whether the cloud platform meets the current business needs in terms of security, computing, storage, and network capabilities. And the network administrator needs to focus on planning the cluster capacity and scale required by the current business and network selection. The rest is up to the system administrator and application administrator. The main focus of this model is to make sure that the service is more stable, cost-effective, and efficient after it is migrated to the cloud.
There are two types of full-stack cloud migration: one-time migration and gradual migration. As you may have guessed, one-time migration means that the migration is done all at once. In this case, all components are migrated and tested, and production traffic is switched over to ACK. Then, after services remain stable for a set period of time, then the original production environment is taken offline. Gradual migration naturally means that the migration is done gradually. An ACK cluster is set up to take over part of the service. This is used in conjunction with the original production environment to provide service. And the rest of the components are gradually migrated to the cloud until the original production environment is no longer needed and taken offline. One-time migration is simpler to perform but the impact on business is big, whereas gradual migration is more complex but has a smaller impact on your business operations. Pick the method that suits your need best.
One-time migration can be further divided into two scenarios. The first one is that the customer is migrating from a self-hosted Kubernetes cluster to ACK. In this case, the customer has already completed a large part of the cloud-native transformation process, and therefore the migration is relatively simple after this step. In the second scenario, the customer uses traditional applications that run on virtual machines or bare metal servers to provide service. This scenario takes more work, which is why we offer a set of tools to help with the process. One example is derrick, which can automatically inspect source code and generate the Docker file and the YAML file used for application deployment. Another is that we are working with ECS SMC to use their software to convert virtual machines to container images that can then be run on ACK clusters.
We have also been developing and releasing open-source tools to help our users to make the migration to ACK easier. ack-image-builder generates a template for creating custom images for the ACK cluster and checks whether the custom image meets the requirements of the ACK cluster through the verification module. sync-repo helps users batch migrate container images to Alibaba Cloud Container Registry. Velero helps users quickly migrate all applications in self-built Kubernetes clusters or other cloud platforms to the ACK cluster.
Data integrity is crucial for data migration. For different data types, we have different enterprise-level tools to ensure data integrity, such as Data Online Migration Service (DOMS). As part of the data migration piece of the puzzle, reliable migration is crucial. Based on different user data types, we will use the matching enterprise-level migration tools, such as the online data migration service DOMS, Object Storage Service (OSS), and the PB-level, point-to-point, offline data migration service Data Transport.
After data and applications are migrated to the cloud, the rest of the components, such as monitoring and logging, need to be configured and tested. If all is set, use Alibaba Cloud Domain Name Service (DNS) to switch over production traffic.
Migrating from a self-hosted Kubernetes cluster to ACK also requires support for features such as auto storage scaling. Migrating traditional applications to ACK is more complicated. Therefore, we came up with some solutions to reduce the work needed. For example, we set up a remote active-active environment and integrate the traditional applications, usually hosted in virtual machines or on bare metal servers, with the Istio grids deployed on ACK, and gradually migrate all services to ACK clusters.
In the process of gradually improving an application, problems such as how to containerize an application and how to migrate the network environment data often come up. We usually use Server Migration Center (SMC) to convert virtual machines to container images. Network issues can be solved by using iptables, External, CoreDNS, and PrivateZone to deal with IP address and DNS changes while retaining the original internal IP addresses and domain names. Istio is used to implement virtual routing and visibility management.
Let’s look at some case studies on cloud migration given in the figure above, including a customer looking for high-performance networks, a customer in need for large-scale GPU resources for deep learning, and a customer ended up with bare metal servers.
Different users have different business needs, resulting in different designs and implementations of cloud-native migration solutions. The ACK team is able to meet these challenges with the experience they have gained over the years and the tools they developed. Looking for a quick and clean migration to the cloud? You cannot go wrong with us.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/yangqing-jia-launches-the-new-generation-cloud-native-data-warehouse-6dc0527f0c73?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 19, 2020·5 min read
Relive all product launches in the Alibaba Cloud Summit 2020 at https://www.alibabacloud.com/campaign/summit-live-2020/live-streaming/product-launches
At the Alibaba Cloud Summit on June 9, 2020, Jia Yangqing, Vice President of Alibaba and Senior Researcher of the Alibaba Cloud Computing Platform Division, announced the launch of the new generation of cloud-native data warehouse . Based on the innovative technical architecture, Hologres, the new-generation cloud-native data warehouse supports PB-level data correlation analysis and real-time query, integrating the offline data, real-time data, analytics, and serving.
The following article is the full text of Jia Yangqing’s speech.
Most of today’s economic activities, data analysis, and services are inseparable from the industrialization of digital technologies and digitalization of various industries. However, we must take these changes step by step. Most enterprises advance cautiously in the process of business and technology upgrading. When we encounter different data analysis and service requirements, we seek a single point system to solve our single point problems. In this process, data silos appear behind a seemingly complete system. The data connectivity and real-time data transmission among silos have become a major problem.
From the perspective of an enterprise, the business system faces difficulties in data insights, while the system sees the cost of data splitting. As a data warehouse is so important for enterprises, I think we need a top-level design to reconstruct the data warehouse. Today, we will introduce a concept of real-time as a service that integrates offline data, real-time data, analytics, and serving based on Hologres, MaxCompute, and Realtime Compute. If we go back to the most essential requirements of data warehouses, the core issue is not complex. A data warehouse needs to integrate data from multiple sources and incorporate them into a set of storage systems in real-time. Meanwhile, it performs offline, real-time, or interactive analysis, and displays results, and provides services. We used to hear about a concept called Hybrid Transaction and Analytical Process (HTAP.) Transactions consider more specific aspects of data. For example, a database has some indicators, such as read and write performance and security. Today, we see analytics and serving become more integrated. Analytics means that we need to gain insights into the laws of such massive data, thereby providing services. Both data dashboards and operation analysis are processes of displaying service data. To eliminate data silos, we must integrate analysis with serving more closely. We call this mode Hybrid Serving Analytical Processing (HSAP). With data warehouses based on Hologres and MaxCompute, we can connect Hologres with MaxCompute and implement high-performance and low-latency data analysis through Hologres. In addition, we can perform large-scale and low-cost offline computing through MaxCompute. On this basis, we can push the results of data analysis and the data accumulated in real-time to different services, such as data dashboards and operation dashboards.
Within Alibaba Group, the biggest demand for data was during the Double 11 Global Shopping Festival, when a large amount of data was transferred and business decisions were complex. In 2019, we upgraded the business support system through data warehouses based on Hologres and MaxCompute. On the day of Double 11, a set of systems supported 145 million online queries, which supported complex business analysis and decision-making. These analyses are also supported by 130 million real-time records. With the correct top-level design, performance is not a problem. An entire data warehouse system based on MaxCompute, Realtime Compute, and Hologres can deal with data silos. In the absence of data redundancy, we can simplify the system, reduce costs, and improve data analysis efficiency. We also think open source, community, and ecosystem are crucial in building a data warehouse. When we built Hologres, we adopted a fully compatible open-source ecosystem based on PostgreSQL. Data engineers and upper-layer BI tools can easily and seamlessly connect their existing systems to Hologres and MaxCompute, seamlessly migrating the analysis and serving.
With real-time as a service and HSAP, we can greatly simplify the design of the data warehouse and build a system integrating offline data, real-time data, analytics, and serving in the entire data lifecycle.
Today, we launched the new generation of cloud-native data warehouse. This solution adopts a system design featuring one set of storage systems to eliminate data silos, multiple computing concepts, and real-time as a service. We also provide DataWorks, Machine Learning Platform for Artificial Intelligence (PAI), and other platforms on Alibaba Cloud. With a complete set of data products, we can provide digital and intelligent applications. We believe that every enterprise will build a data warehouse solutions on the cloud to solve numerous and complicated data problems.
Catch the replay of all product launches in the Alibaba Cloud Summit 2020 at https://www.alibabacloud.com/campaign/summit-live-2020/live-streaming/product-launches
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/security-transformation-leadership/cloud-native-environments-a-challenge-for-traditional-cyber-security-practices-924e6bb78186?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
Clouds are those blurred masses of condensed watery vapor floating in the sky whose gloomy nature often leads to questionings around their true physical state. Are they really tangible? Could we touch what we look up to? And above all, is there a difference between what we imagine seeing and what they truly are?
In the computing industry, “the cloud” means something else but it is above all a marketing trick: Tech firms would like you to believe it is something soft and fluffy but it is in fact a huge network of remote services — held together by countless pages of legal terms — hosting and managing data. And it’s not fluffy at all: At the end of the day, there is no “cloud”.
“The cloud” is tens of thousands of racks in datacenters filled with servers.
From the early days of computing and through the first phase of the Internet explosion up to the early 2010s, companies were mostly protecting their information internally, and they usually had some form of direct control over it. Most security standards and accepted good practices were drafted in that era and are still heavily inspired by a world where you could know where your data and your servers were.
In recent years, however, the development of massive computing and storing capacities in the hand of a few internet juggernauts led to the rise of the cloud economy. For the last decade, companies of all sizes — from tech startups to Netflix serving in excess of one hundred million users globally — have been moving their mission-critical servers and operations to the data centers of Google, Amazon, or Microsoft.
On the face of it, the development of Infrastructure as a Service (IaaS) should be good news for the state of cybersecurity. Economies of scale and their vast pool of talents should allow tech giants to dedicate much more resources into properly securing data centers. Servers should be easier to patch in a timely manner, state-of-the-art firewalls should be used and the physical location of these data centers should be heavily guarded. In this context, it is easy to believe that moving to the cloud could mean solving many of your cybersecurity issues.
It is also easy to believe that moving to the cloud would make your cybersecurity someone else’s problem. Nothing could be further from the truth. Of course, each organization retains its own regulatory obligations irrespective of how operations are technically delivered
For example, going to the cloud will not make any business GDPR-compliant in and by itself. In fact, all of the GDPR most important prerogatives around cybersecurity — adequacy of the protective measures, appropriate data management processes around consent, retention and deletion, etc. — do remain firmly within the organization’s remit. Not only is the CISO still a cornerstone of your GDPR strategy, but it inherits a new key role: That of dealing and interacting with Cloud vendors in this new world where your physical technology stack is delegated to someone else while the regulatory obligations remains firmly in your hands.
Looking at Amazon Web Services’ Shared Responsibility Model makes this dichotomy very clear.
AWS is responsible for the security “of” the cloud while you remain responsible for the security “in” the cloud — atop of which sits your consumer’s data. While a car manufacturer is responsible for the security of your car, you are ultimately responsible for driving safely.
Similarly, AWS will never prevent you from driving into a tree. In their own words: “AWS trains AWS employees, but a customer must train their own employees.”
Platform as a Service (PaaS), Software as a Service (SaaS) and all hybrid models of course bring up the same challenges, often compounded by their inter-dependence (e.g. a SaaS solution built on IaaS or PaaS services), and a real supply chain which can become blurred very quickly.
The issue brought by the shift to the cloud paradigm in cybersecurity is not one of adaptability but of adaptation. As such, a key role for the CISO is increasingly to act as a bridge between internal structures and cloud suppliers in order to ensure that all stakeholders are aware of all security requirements (driven by internal policies or regulation) and that all appropriate measures are in place.
This evolution in the role of the CISO epitomizes a fundamental trend in cybersecurity which centers more and more activities around governance, people and culture rather than technology, data and networks.
It does challenge organizational models as well as the profile of the CISO, and brings to the forefront vendor risk management practices: In the cloud, you are never sure of what’s really going on, your relationship with vendors is framed by contracts which are often one-sided, and a small SaaS provider carrying out sensitive business operations could expose your organization considerably.
For regulated industries (which isn’t in the age of GDPR?), blind trust will never be enough and being able to demonstrate a sufficient degree of due-diligence on key vendors will always be essential to defend against any liability in case of a data breach.
Welcome back to the “Trust-But-Verify” era…
Click here to join our newsletter for more Cyber Security Leadership insights.
Contact Corix Partners to find out more about developing a successful Cyber Security Practice for your business.
Corix Partners is a Boutique Management Consultancy Firm, focused on assisting CIOs and other C-level executives in resolving Cyber Security Strategy, Organisation & Governance challenges.
The Security Transformation Research Foundation is a…
Delivering a challenge and an alternative view on common practices in the CyberSecurity space to help the Industry move forward Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Written by
Founder & MD @CorixPartners | Co-president #CyberSecurity Group @TelecomParisAl | Non Exec Director @Strata_Sec | Board Advisor | Author | Cyber Security Leader
The Security Transformation Research Foundation is a dedicated think-tank and research body aimed at approaching Security problems differently and producing innovative and challenging research ideas in the Security, Business Protection, Risk and Controls space
Written by
Founder & MD @CorixPartners | Co-president #CyberSecurity Group @TelecomParisAl | Non Exec Director @Strata_Sec | Board Advisor | Author | Cyber Security Leader
The Security Transformation Research Foundation is a dedicated think-tank and research body aimed at approaching Security problems differently and producing innovative and challenging research ideas in the Security, Business Protection, Risk and Controls space
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/log-platform-solution-in-the-cloud-native-architecture-956141cb108?source=search_post---------296,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 29, 2021·8 min read
By Ford, FDD architect, with 12 years of software development experience, mainly responsible for the design of cloud native architecture with focus on infrastructures, Service Mesh advocate and practitioner of continuous delivery and agility.
In recent years, many businesses have experienced unexpected growth, which has increased the pressure on traditional software architecture. To cope with this, many have started adopting microservices for their software architecture. Consequently, the number of online applications has multiplied after horizontal and vertical expansion. In traditional monolithic application scenarios, the methods of log query and analysis by using tail, grep, and awk commands cannot meet new requirements. In addition, these methods cannot cope with the huge increase of application logs and the complex operating environment of distributed projects in cloud native architecture.
During the transformation of the cloud native architecture, observability has become the enterprise-level issue for quick fault locating and diagnosis, under complex dynamic cluster environments. Logs are particularly important as one of the three major elements that can be monitored. The three elements are logs, metrics, and traces. The services of log system are no longer limited to application system diagnosis. Now business, operation, BI, audit, and security are also included. The ultimate goal of the log platform is to achieve digitization and intelligence of all aspects in the cloud native architecture.
The log solutions based on cloud native architectures are quite different from that based on physical machines and virtual machines. For example:
Log collection solutions in the cloud native architecture
Based on the above advantages and disadvantages, we choose Solution 3. Solution 3 is selected because it balances scalability, resource consumption, deployment, and maintenance.
The following figures show the architectures of each solution.
Solution Description
When a cluster starts, a Fluent-bit agent is started on each machine in DaemonSet mode to collect logs and send them to Elasticsearch. Each agent mounts the directory /var/log/containers/. Then, the agent uses the tail plug-in of Fluent-bit to scan log files of each container and directly send these logs to Elasticsearch.
The log of /var/log/containers/ is mapped from the container log of a Kubernetes node, as shown in the following figures:
Monitoring of Fluent-bit and Input configuration
The collection agent is deployed based on Kubernetes cluster. When nodes in the cluster are scaled out, Fluent-bit agents of new nodes are automatically deployed by kube-scheduler.
The current services of Elasticsearch and Kibana are provided by cloud vendors. The services provide the X-pack plug-in and support permission management feature which is only available in Business Edition.
Implementation
1. Configure Fluent-bit collectors, including collectors for server, input, filters, and output.
2. Create RBAC permission of Fluent-bit in the Kubernetes cluster.
3. Deploy Fluent-bit on cluster nodes of Kubernetes in DaemonSet mode.
In the solution, Fluent-bit collects event audit logs of Kubernetes clusters and generates corresponding logs for status changes caused by kube-apiserver operations. The following kubernetes-audit-policy.yaml defines which audit logs are collected. To do that, the reference of this configuration in kube-api startup file is needed by using --audit-policy-file.
As the distributed system in the cloud-native architecture grows complex, logs are becoming scattered. So, it is difficult to monitor application and troubleshoot, and the efficiency is low. The centralized log platform of Kubernetes cluster in this article aims to solve these problems. The collection, retrieval, and analysis of cluster logs, application logs, and security logs, and Web management are centrally controlled by the platform. It realizes quick troubleshooting, and become an important way to solve problems efficiently.
During production and deployment, the introduction of Kafka queue can be determined based on the business system capacity. In the offline environment, it doesn’t have to introduce Kafka queue. Simple deployment is enough, and Kafka queue can be introduced when it needs to scale out the business system.
The services of Elasticsearch and Kibana in this article are provided by cloud vendors. In the considering of cost-saving factors, Helm can be chosen to quickly build offline development environments. The example for reference is as follows:
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/future-direction-of-observability-in-cloud-native-a-case-study-of-autonomous-driving-851f044ac0d9?source=search_post---------297,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 22, 2021·12 min read
By DavidZhang
I was honored to attend the meetup of the cloud-native community in Beijing and had the opportunity to discuss cloud-native technologies and applications with experts in the industry. In this meetup, I talked about the topic of observability in cloud-native. This article is mainly a literal summary of my presentation, and I welcome all readers to leave a message for discussion.
Observability originated from the field of electrical engineering. The main reason is that with the development and complexity of the system, a mechanism must be set up to understand the internal operation status of the system in order to better monitor and repair problems. For this reason, engineers have designed a number of sensors and dashboards to demonstrate the internal status of the system.
If a system is said to be observable, for any possible evolution of state and control vectors the current state can be estimated using only the information from outputs.
Electrical engineering has been developed for hundreds of years, in which observability in various sub-fields is being improved and upgraded. For example, vehicles (cars/planes, etc.) are also the masters of observability. Regardless of the super project of the plane, there are hundreds of sensors inside a small car to detect various states inside and outside the car, so that the car can be stable, comfortable, and safe.
With the development of more than a hundred years, observability under the electrical engineering has not only been used to assist people to check and locate problems. In terms of automobile engineering, the entire observability development has gone through several processes:
As the peak of observability of electrical engineering, autonomous driving gives the best play to all kinds of internal and external data obtained by automobiles. To sum up, it mainly has several core elements:
With decades of development, the monitoring and troubleshooting methods in IT systems have also been gradually used for observable engineering. At that time, the most popular method was to use a combination of Metrics, Logging, and Tracing.
The preceding figure may already be very familiar to you. It is excerpted from a blog post published by Peter Bourgon after he attended the 2017 Distributed Tracing Summit. This figure briefly introduces the definitions and relationships of Metrics, Tracing and Logging. Each of the three types of data plays a role in observability, and each kind of data can not be completely replaced by other data.
The following shows a typical troubleshooting process described in Grafana Loki.
The preceding example illustrates how to use Metrics, Tracing, and Logging for joint troubleshooting. Different combination solutions can be used in different scenarios. For example, a simple system can directly trigger alerts based on error messages from Logging and locate problems. It can also trigger alerts based on Metrics (latency and error code) extracted from Tracing. But on the whole, a system with good observability must have the above three types of data.
What cloud native brings is not only the ability to deploy applications on the cloud, but also the upgrading of a new IT system architecture, including development models, system architecture, deployment models, and the evolution and iteration of the infrastructure suite.
Many readers will have a deep understanding of the preceding problems. The industry has also withdrawn various observability-related products, including many open-source and commercial projects. For example:
A combination of these projects can solve one or several specific problems, but when these projects are applied, you will find various problems:
In this context, the OpenTelemetry project was born under the Cloud Native Computing Foundation (CNCF). It aims to unify Logging, Tracing, and Metrics to achieve data interoperability.
Create and collect telemetry data from your services and software, then forward them to a variety of analysis tools.
The core function of OpenTelemetry is to generate and collect observability data, which can be transferred to various analysis softwares. The following figure shows the architecture. The Library is used to generate observability data in a unified format, and the Collector is used to receive the data and transmit the data to various backend systems.
The revolutionary progress that OpenTelemetry has brought to the cloud-native, includes:
From the above analysis, you can see that the orientation of OpenTelemetry is the infrastructure for observability and the solution for data specification and acquisition problems. Subsequent implementations rely on vendors. Of course, the best way is to have a unified engine to store all Metrics, Logging, and Tracing, and a unified platform to analyze, display, and correlate these data. Currently, no vendor can well support the unified backend of OpenTelemetry. However, we still need to use the products of each vendor to implement OpenTelemetry. Another problem brought by this is that the association of various data is more complex, and the data association between each vendor needs to be dealt with. This problem will definitely be solved in one to two years. Now, many vendors are trying to implement a unified solution for all types of data in OpenTelemetry.
Our team has been responsible for monitoring, logging, distributed tracing, and other observability-related tasks since we started the Apsara 5K project in 2009. We have experienced some architecture changes from minicomputers to distributed systems, and then to microservices and cloud services. The relevant observability solutions have also undergone a lot of evolution. We think that the development of the overall observability correlation is very consistent with the setting of the autonomous driving class.
Autonomous driving is divided into six levels, of which level 0–2 is mainly decided by people. Unconscious driving can be carried out above level 3, that is, hands and eyes can temporarily not pay attention to driving. At level 5, people can completely leave the boring job of driving and move freely on the car.
The observability of an IT system can also be divided into six levels:
Log Service (SLS) is currently working on cloud-native observability. Based on OpenTelemetry, the future cloud-native observability standard, collects all types of observability data, covers all kinds of data sources and data types, and achieves multi-language support, multi-device support and unified type. We will provide unified storage and computing capabilities to support all kinds of observability data, support PB-level storage, Extract, Transform, and Load (ETL), stream processing, and analysis of tens of billions of data records within seconds. We will provide strong computing power for upper-layer algorithms.
The problems of the IT system are very complex, especially when different scenarios and architectures are involved. Therefore, we combine the algorithm and experience to carry out abnormal analysis. The algorithm includes basic statistics and logical algorithms, as well as Algorithmic IT Operations (AIOp)-related algorithms. Experience includes manually input expert knowledge, problems, solutions, and events accumulated on the internet. The top layer provides functions to assist in decision-making, such as alert notifications, data visualization, and webhooks.
In addition, the top layer provides rich external integration capabilities, such as integration with third-party visualization, analysis or alerting systems. It also provides OpenAPI to facilitate integration among different applications.
As the most active project under CNCF with the exception of Kubernetes, OpenTelemetry has received attention from major cloud vendors and related solution companies. It is believed that OpenTelemetry will become the observability standard under the cloud-native in the future. Although it has not yet reached the level of production availability, the Software Development Kit (SDK) and Collector in various languages are basically stable, and the production available version can be released in 2021, which is worthy of everyone’s expectation.
OpenTelemetry only defines the first half of the observability, and there is still a lot of complicated work to be done, so there is a long way to go.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-evolution-of-cloud-native-success-stories-from-our-customers-49af0ca508b1?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 6, 2021·9 min read
By Heyi
“Do we need to scale out further?”
“Scale out by a factor of ten, and then we’ll talk.”
Li Nuo, Co-Founder and CTO of Onion Academy, has talked about scaling many times. In response to the COVID-19 pandemic, schools across Mainland China postponed the beginning of term this year and held classes online. Online education produced network traffic spikes that far exceeded expectations and required constant scale-out.
When Li Nuo discussed this work with Yang Linfeng, Co-Founder and CEO of Onion Academy, Yang Linfeng wrote an open letter to users. As the COVID-19 pandemic wreaked havoc across Mainland China, Onion Academy received a lot of attention from the public and had the opportunity to mine the value of traffic. In Yang’s opinion and based on his personal experience, homeschooling is a challenge for every student.
According to Li Nuo, it was a daunting task for the Onion Academy to maintain stable server operations and a good user experience as their traffic spiked.
In December 2013, Yang Linfeng, Zhu Ruochen, and Li Nuo co-founded Onion Academy (formerly known as Onion Mathematics.) It is an online education company that specializes in K-12 education and the development of an online education platform for human-machine interactive learning. Onion Academy initially provided mathematics courses for junior high school students and offered other courses later. Onion Academy built an online curriculum system based on the national curriculum standards and teaching materials. The system provides 5 to 8-minute videos explaining each knowledge point in textbooks based on in-depth research and design. These courses offer a personalized educational experience for students.
Human-machine interactive learning is now at the forefront of education. Soon after Onion Academy was established, the founding team made a farsighted decision to build a complete business system based on Alibaba Cloud.
Initially, Onion Academy did not grow as fast as other online education companies. According to Li Nuo, the team spent the majority of its effort on course development and learning experience optimization. For example, it took them four years to develop video courses for junior high school mathematics. Onion Academy has gained a strong foothold in the online education sector after a lot of persistence. The COVID-19 pandemic caused the demand for online education to increase sharply in a short time. In the past, online education companies had to make great efforts to promote themselves. After the beginning of the COVID-19 pandemic, everyone turned to these companies and expected them to provide high-quality online courses.
On January 28, 2020, Onion Academy announced a course donation plan to offer all 2,650 of its major courses developed over the past six years free of charge during the pandemic. However, Onion Academy had to deal with heavy traffic that far exceeded expectations. According to the statistics of Qianfan Analysis, Onion Academy served 7.9592 million active users in February 2020, a year-on-year increase of 151%.
Onion Academy used Alibaba Cloud Container Service to deal with highly concurrent access traffic spikes and ensure business stability based on the suggestions of Alibaba Cloud Technical Experts. The Container Service can allocate resources based on the configuration requirements of different modules and automatically scale containers based on defined rules. This avoids complex scheduling and maintenance.
The Container Service can scale out underlying resources in minutes, allowing a business to deploy thousands of application instances quickly. For its tenfold scale-out, Onion Academy optimized the configurations of its many low-specification Elastic Compute Service (ECS) instances by upgrading them to 30 to 50 cores. This made O&M more convenient. After Onion Academy adopted Container Service, its system resource utilization rate improved by about 60%. When problems occur, they can be detected and isolated quickly. When faced with a sharp increase in business volume, they can also expand their business support capacity quickly. Onion Academy uses the native monitoring products provided by Alibaba Cloud to detect faults quickly and respond promptly, which can monitor and trigger alerts for all kinds of problems. This significantly shortens the time needed to find problems.
As a typical Internet company, Onion Academy has adopted a forward-looking vision from the moment it adopted cloud migration in 2013 to its current cloud-native usage.
The development of cloud-native in recent years has not been a smooth process.
Migrating applications to the cloud has become an irreversible trend. In recent years, many enterprises have gone through digital transformations to digitalize their businesses by accelerating technology iteration in terms of business channels, competition patterns, and user experience.
Fast technology iteration relies on the powerful capabilities of the cloud. Enterprises must fully utilize the availability and scalability of cloud technology to improve the efficiency of release and O&M. Enterprises must redevelop their infrastructures, architectures, and platforms and redesign their development, deployment, and maintenance methods based on the features of the cloud to develop cloud-based or cloud-native applications.
As predicted by Gartner in 2019, we are now in the cloud-native era. Within the next three years, 75% of global enterprises will deploy containerized applications in their production environments. Cloud-native technology applies to cloud computing. Microservices, containers, and Kubernetes continue to provide effective applications and standards to implement edge computing, which is independent of and collaborative with cloud computing.
In the past, when an enterprise wanted to build a cloud-native platform through cloud-native technology or products, it had to extensively research open-source projects and conduct O&M and management manually while considering integration and stability assurance. Alibaba Cloud provides a complete cloud-native product family with powerful service level agreement (SLA) assurance to allow enterprises and developers to use cloud-native technology and products, better understand the cloud-native concept, and ensure business reliability, performance, and continuity.
Alibaba Cloud has helped a lot of Chinese enterprises understand and use cloud-native technology. Alibaba has applied and optimized cloud-native technology in a variety of large-scale internal scenarios. This has allowed Alibaba to contribute the mature cloud-native technology to the community to help raise its technological quality and development level.
As cloud computing and cloud-native are widely used, more businesses and decision-makers will realize that cloud-native is key to business technology innovation and the shortest path to digital transformation. Therefore, forward-thinking Internet companies will develop cloud applications from scratch. Enterprises and institutions in the new retail, government, finance, and medical fields are carefully migrating their business applications to the cloud to take full advantage of the value of cloud-native technology and architecture.
Chanjet is China’s leading cloud service and software provider for small and micro businesses, currently providing smart cloud services to more than four million clients. Boosted by rapid business growth, Chanjet ventured into the cloud-native field when its IT Team made many microservice transformations to its original IT system to adapt to the fast iteration and frequent release of large Internet applications.
Next, Chanjet proceeded to cloud service provision through the Software as a Service (SaaS) model. This was a daunting challenge during the microservice transformation process and arose from a series of factors, such as large user volumes, complex businesses, long traces, and deep integration with third-party application systems. Application upgrades may cause the entire system to stop responding if the new version release process does not go as expected.
To solve this pain point, the Chanjet IT Team reached out to Alibaba Cloud Technical Experts because Alibaba has experience developing and applying cloud-native technology to its complex business scenarios. Chanjet gradually migrated its microservices model to Alibaba Cloud Enterprise Distributed Application Service (EDAS). Microservice applications based on Spring Cloud can be directly deployed in EDAS without code modification. The migration process is smooth and transparent to Chanjet users. Now, Chanjet can keep up with frequent version iteration to meet complex business needs.
After Chanjet could leverage the advantages of cloud-native technology and products, it started to use related methodologies to implement end-to-end and phased microservice governance.
Unlike most computing models, serverless architecture separates deployment from O&M to free developers from tasks, such as building application runtime environments, installing operating systems, configuring networks, and calculating the required CPU resources. In architecture abstraction, the cloud starts a business process or schedules a running business process to handle inbound business traffic or events. Afterward, the cloud stops the business process or schedules it to run upon the next trigger. This means the cloud handles all runtimes by itself.
Serverless applies to event-driven data computing tasks, applications based on the request-response model that have short calculation times, and long-term tasks without complex mutual calling.
Baifu Travel is the world’s leading online tourism trade platform. It provides travel agencies worldwide with an all-in-one smart solution based on cloud computing and big data decision-making. To date, Baifu Travel has concluded business partnerships with more than 600 airlines, all Chinese train lines, 2,500 bus stations, and more than 60 cruise groups, with a presence in more than 100 countries and regions worldwide.
After rapid business growth, the Baifu Travel Technology Team built a comprehensive microservices model based on open-source frameworks, such as Spring Cloud. After microservice applications are deployed on the cloud, the Technology Team is freed from O&M tasks, such as hardware purchase and server setup, so they can devote more effort to business operations. However, a series of problems occur as system iterations become more frequent.
In a system architecture, there are no differences between a microservice deployed on a cloud virtual machine (VM) and a microservice deployed in a physical data center. The team still has to perform underlying O&M tasks for each application instance, such as operating system tuning, disk capacity planning, and installation of Java Development Kit (JDK) and other components. These tasks are necessary for running each cloud VM.
Each developer or development team must create an independent test environment to deal with frequent system iteration. Over time, the Technology Team has created multiple test environments, some of which contain all microservice applications. This significantly lowers resource utilization and wastes a lot of resources. The microservices model can scale out each application to adapt to the seasonal fluctuations in the tourism sector. However, many resources will remain idle if the VMs required for scale-out must be purchased in advance.
The problem of idle and wasted resources can be solved by serverless. Serverless architecture can avoid resource waste through auto scaling. Serverless applications can apply for resources as needed without having to purchase underlying server resources. This frees the Technology Team from complex O&M tasks, such as capacity planning and operating system tuning.
The Technology Team surveyed common serverless implementation methods and products to select a suitable serverless architecture.
Serverless architecture can be implemented through two methods. In the first method, each microservice application is containerized and then orchestrated by Kubernetes. Cloud service providers offer elastic container instances that can be called as needed at the container layer. This method is highly demanding. The serverless architecture must be managed by an O&M Team that specializes in Kubernetes, and application containerization is a project that demands great effort to complete. Therefore, this method does not apply to small and efficient technology teams like Baifu Travel.
The second method is to refactor the complete business logic into functions through a computing engine, such as AWS Lambda or Alibaba Cloud Function Compute. This method requires re-coding and cannot bring the benefits of serverless to minor areas with complex traces. Therefore, it was not suitable.
After several rounds of technical research and in-depth exchanges with Alibaba Cloud Technical Experts, the Baifu Travel Technology Team chose Alibaba Cloud Serverless App Engine (SAE). Unlike other serverless solutions, SAE directly supports Spring Cloud, Apache Dubbo, and other development frameworks. Developers can deploy serverless applications using WAR packages, JAR packages, or images without learning Kubernetes and container technology. The microservice applications deployed in SAE can apply for resources as needed, and resources are billed in pay-as-you-go mode every minute. This avoids unnecessary fees incurred during idle hours. SAE allows developers to start and stop test environments with a few clicks, avoiding idle resources. SAE helped Baifu Travel significantly reduce its investment in cloud resources and reduce its O&M workload by more than half. This laid the foundation for subsequent business innovation.
Companies that have confidence in and want to implement new technologies or new technical concepts are the drivers behind the wide acceptance and fast development of these technologies and concepts. These companies have benefited the most from exploring cloud computing. Innovative companies, such as Onion Academy, Chanjet, and Baifu Travel, tend to embrace changes and come up with new ideas. When we look back at the growth of these companies in 5 or 10 years, we will see how they were the creators of a vigorous era full of possibilities and cutting-edge innovators ahead of the curve.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@GiantSwarm/giant-swarms-top-cloud-native-posts-of-2018-e92bc6d7619?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Feb 25, 2019·2 min read
Our team is hard at work perfecting the ideal Kubernetes stack for you. Still, we find time to share knowledge about our experiences in the containerized world. With 2018 a near-distant memory, we’d like to take a moment to reflect on the cloud native topics that you all loved so much.
Skim through our top 10 list below and catch up on an article or two that you may have missed, or would like to revisit.
We are continuously updating our blog and Kubernetes Documentation. Follow us on Twitter or stop by our blog to get updates from our team.
Written by Tommy Hobin— Inbound Marketing @ Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
See all (60)
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@grigorkh/what-is-cloud-native-computing-and-how-cncf-contributes-to-industry-47856ce0fc78?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
Grigor Khachatryan
Dec 9, 2020·4 min read
Cloud computing has become the leading method for scaling up workloads and growing businesses at a steady rate. It allows companies to build and run scalable applications in dynamic environments known as clouds.
Cloud technologies allow integration of multiple systems, offering a new platform designed to enable easy management and detailed…
"
https://medium.com/@chrischinchilla/the-weekly-squeak-cloud-native-automation-with-garden-676056c038cc?source=search_post---------301,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Chinchilla
Jan 30, 2021·3 min read
This episode I speak with Jon Edvald of Garden about their cloud native automation platform.
I also cover Linux on the M1, Google Chrome, Web extensions on Safari, and more!
xx Chinch
"
https://medium.com/@GiantSwarm/who-is-keeping-your-cloud-native-stack-secure-cfcbd01c90a2?source=search_post---------302,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Mar 13, 2019·2 min read
Survey after survey is showing that 70% of organizations are using cloud infrastructure. The last bits of the late majority will be joining this year and the laggards will come within a few years. Regardless of the stage of a company’s cloud native journey, security still remains an issue. The same survey states that security ranks as first priority with enterprises.
At Giant Swarm we pay close attention to the need for security. Last year we shared a 5 part series about security on our blog (I, II, III, IV, V). It presented some of the basics and our philosophy when it comes to security.
So…we talk the talk, great! What is important to our customers is that we also walk the walk.
In December 2018, a severe vulnerability was discovered in the Kubernetes API server. It allowed an unauthenticated user to perform privilege escalation and gain full admin privileges on a cluster.
The details of the vulnerability were discussed at length in the Kubernetes community. The chain of events is well documented across GitHub and Google Groups. Other contributors to the Kubernetes ecosystem provided analyses of the problem. One could easily find information about the problem, its identification and suggested mitigation.
The recommendation was to upgrade Kubernetes and new releases that included the fix were created for all active versions (v1.10.11, v1.11.5, v1.12.3). Earlier versions, did not receive an upgrade, so their upgrade deficit grew to include a security vulnerability.
At Giant Swarm, we were ready to upgrade all our customers to the secure version the next day. Regardless of the Kubernetes version, or the cloud provider. Customers on AWS, Azure, and on-premises, were all proactively notified of the vulnerability and its solution. Most of our customers don’t have Kubernetes APIs exposed to the public internet. Still, all benefited from a quick and transparent upgrade that allowed them to keep running their businesses — threat free.
This incident highlights how important it is to have several layers of security. But also that only with an automated update system, as well as the ability to quickly test and release upgrades, you and your business can really be safe.
Want to find out how the Giant Swarm Infrastructure deploys and scales Kubernetes Clusters? Request your free trial here by taking our survey and find out if you’re eligible.
Written by Oshrat Nir — Product Marketing Manager @Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-native-helps-enterprises-achieve-digital-transformation-9e15d0de1358?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 11, 2020·7 min read
At this Alibaba Cloud Summit 2020, Alibaba Cloud Native Group shared two topics. The following content is taken from the speech.
By Guoqiang Li.
At this Alibaba Cloud Summit 2020, Guoqiang Li brought us the release of two popular open source software products on the cloud, Prometheus and EDAS3.0.
Alibaba Cloud native team have made active contributions to the open source community, directly contributing to well-known open source software such as Apache dubbo, RocketMQ, Spring Cloud Alibaba, Dragonfly, Dragonwell, and actively participating in the construction of open source communities such as Kubernetes. At the same time, Alibaba Cloud also provides many cloud products that support open source software, such as container service Kubernetes, message queue RocketMQ, Alibaba service mesh, etc.
In the field of basic monitoring, the open source software prometheus is developing very rapidly, and it can comprehensively monitor the customer’s infrastructure, from virtual machines to components to Kubernetes. It can be well supported. With the release of prometheus service, until now, Alibaba Cloud has covered the field of application monitoring comprehensively, including metrics, tracing, and logging. From the host to the container, to the full stack support of the application, the language is also fully covered by open tracing.
The ARMS Prometheus service provides a fully compatible open source experience, supports PromQL, native service discovery and alerting.At the same time, Alibaba Cloud solve the pain points of native prometheus. It provides unlimited expansion of cloud storage, high availability and high stability, and very low load of Indicator collection consumption.
Pay-as-you-go, which is the payment model is cheaper than open source self-build. In addition, ARMS Prometheus provides dashboard and defaul alarms for out of box, which is more convenient to use. At present, ARMS Promethus and ACK are deeply integrated and are available in major regions around the world.
Guoqiang Li also introduced the latest version of EDAS. In EDAS3.0, Alibaba Cloud focused on strengthening the support for Kubernetes and the open source microservice framework Spring Cloud and Dubbo. When users deploy Java applications, they do not need to care whether the underlying layer is VM or Kubernetes. If the bottom layer is Kubernetes, EDAS will help users build an image and deploy it to Kubernetes. After that, the application’s full life cycle management, monitoring, logging, and microservice management capabilities are fully configured. EDAS3.0 also provides a series of critical capabilities in production.
In terms of microservices, EDAS supports all versions of spring cloud and dubbo in the past 5 years through a non-intrusive probe model. Users can enjoy the microservice governance capabilities provided by EDAS without deploying upgrades or modifying programs. EDAS also includes Alibaba’s safe production of three axes, observable, grayscale, and rollback, and all are built into the product design to ensure the user’s business stability. Based on the above capabilities, EDAS can support the operation of microservice loads on the cloud in a more cloud-native way, helping users build stable and efficient microservice operating environments.
By Shuwei Yin.
Shuwei Yin (Lishan), the Senior Solution Architect for Alibaba Cloud-Native Services makes an Introduction to Alibaba Cloud-Native Solutions.
We all understand the importance of the digital transformation, especially in an era where a vast majority of business activity has moved online. To that end, businesses want to run and design their digital operations to deploy products faster, scale effortlessly, control costs, and sustain an innovative advantage. Cloud-Native is a major enabler of the digital transformation, and Alibaba Cloud-Native services can use their capabilities in elasticity, serverless, observation, and hybrid/multi cloud to fulfil these requirements.
Alibaba Cloud has a full and diverse portfolio of container services to support your cloud-native transformation. We have two primary container orchestration services, ACK, which includes dedicated and managed version of Kubernetes, and ASK, which is serverless (meaning you don’t manage any infrastructure and only pay for what you use). Built to support both public and private clouds, as well as edge computing, Alibaba’s container services are deeply integrated with high-performance compute, network, and storage products. On top, Alibaba Cloud provides end-to-end security, observability, and many more services to simplify, secure, and accelerate your cloud-native transformation.
Alibaba Cloud provides multiple different templates to choose from when creating your kubernetes cluster to meet your needs for a diverse pool of scenarios. For example, you can choose a heterogenous computing cluster template which uses NPU/GPU instances as worker nodes, to support your own machine learning and AI workloads.
There are two ways to run your kubernetes clusters on Alibaba Cloud. For long-running applications, you can use ACK to manage the pods running on basic ECS servers, but this will require capacity planning and manual maintenance. For temporary applications, like batch processing or CI/CD, you can use ASK to manage the pods running on ECI, where auto-scaling is done on demand and no node maintenance is required.
You can see that ACK supports a combination of different runtimes, RunC, sandboxed, as well as ECI. You can use a mixture of all three. Some pods can run in a normal RunC docker runtime, and some pods that require isolation and better security can run in a sandbox. You can use a combination of ECS nodes and serverless ECI nodes, whose pods will share the same attributes as a serverless Kubernetes cluster.
Heterogeneous computing is very important for AI and Machine Learning workloads. ACK is ideal for the job because AI algorithms must be able to scale to be optimally effective. Some deep learning algorithms and data sets require a large amount of compute. Kubernetes helps because it is all about scaling based on demand. It also provides a way to deploy AI-enabled workloads over multiple heterogenous resources across the software pipeline while abstracting away the management overhead.
Hybrid deployment has become a common choice for enterprises to migrate their workloads to the cloud. However, the adoption of hybrid cloud brings to mind a new challenge: There is a huge margin in terms of capabilities and security requirements between on-premises and cloud-based infrastructures. And so we arrive at the question: how can you manage both of them effectively at the same time? To address this issue, Alibaba Cloud’s Container Service for Kubernetes has provided the application-centric hybrid cloud 2.0 architecture.
You can use the “Register Cluster” feature for unified traffic and cluster management of on-premise and other cloud kubernetes clusters. This is such a great feature because you can easily manage your clusters from different cloud vendors, different runtime environments with unified governance, observability, scheduling, and deployment.
Alibaba Cloud container services are used by customers from more than seven thousand (7000) customers around the globe, from a variety of different industries. Many major brands like Siemens and Akulaku all run their most mission-critical applications on Alibaba Cloud container services.
This customer is Indonesia’s leading B2B marketplace lending platform for SMEs. This customer built the front-end and back-end platform through a monolithic architecture. As the modules and features of the application were increasing, the IT operation team faced a number of challenges, the biggest was the use of a monolithic architecture, which meant that any change request and upgrade had to involve editing the database, code, and front-end platform, which is quite a time-intensive process.
Since all individual components on the platform are highly coupled and dependent on each other, a single point of failure can bring down the entire system, leading to the outage of business operation.This customer deployed the application on ACK. Alibaba Cloud helped the customer redesign their monolith architecture to microservices, which involved breaking up the monolith into its own, single service components. Alibaba Cloud helped the customer re-architecture from monolith to microservices; shortened their time of App development and deployment; reduced IT and resources expenditures; and improved reliability and agility.
Adira Finance, established in 1990, is a leading Indonesia-based multi-finance company primarily engaged in provisioning consumer financing for either used or new motorcycles and cars. The company has close to 600 branches across Indonesia, over 21,000 employees, serving more than 3 million customers.
As their business grew, they needed to redesign their monolithic applications to microservice applications to increase business agility and cut IT costs. Alibaba Cloud stepped in and provided database, storage, log service, API gateway and other products to work in conjunction with ACK to support their new microservices architecture. Alibaba’s solution provided them with high availability with multi-zone redundancy to minimize business disruptions and allowed them to easily scale up or down, saving costs and boosting development and deployment efficiency.1_5_
Join us at the Alibaba Cloud Summit 2020 for more.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/seven-challenges-for-cloud-native-storage-in-practical-scenarios-9ecddb873bdf?source=search_post---------304,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 3, 2020·6 min read
By Eric Li (Zhuanghuai), Head of Alibaba Cloud-native Storage Business
Portability, extensibility, and dynamic features are inevitable for high performing cloud-native applications. However, cloud-native applications also impose requirements on density, speed, and hybrid performance of cloud-native storage. These requirements translate into requirements for efficiency, elasticity, autonomy, stability, low application coupling, GuestOS optimization, and security, in addition to the basic capabilities of cloud storage.
It’s critical to address challenges related to performance, elasticity, high availability, encryption, isolation, observability, lifecycle, and other aspects, which arise from the containerization, cloud migration, and storage of new enterprise load or intelligent workload. Just upgrading storage products is not enough, improving the cloud-native control and data planes to promote the evolution of cloud-native storage are also equally important.
The following section describes possible cloud-native storage problems, scenarios, and feasible solutions. And further, describes what cloud-native storage and cloud storage are capable of now and what else to expect in the future.
Scenario
Huge data is processed as a batch in a high-performance computing scenario, where thousands of pods start simultaneously through Kubernetes clusters, and autoscaler scales out hundreds of ECS instances to read data from and write data to shared file systems.
Problem
Latency time increases under heavy load, with an increased spike in high latency and unstable reads/writes.
Solution
Scenario
Read/write requests of 10 Gbit/s in size are distributed to the same storage cluster in a high-performance computing scenario for centralized processing.
Problem
The bandwidth of the storage cluster is squeezed and affects the cluster access experience.
Solution
Scenario
A large amount of biological data is processed and the number of files is small. However, the peak throughput is as high as 10 Gbit/s to 30 Gbit/s, with request density of 10,000 requests per second.
Problem
The occupied bandwidth almost reaches the bandwidth limit of the exclusive cluster.
Solution
Scenario
Multi-host multi-card GPU training is performed. This is a read-intensive scenario as data in OSS is read directly.
Problem
Higher latency leads to a high IO wait value and GPU waits.
Solution
The tenant or application-level I/O metric monitoring and alerting are implemented in a multitenancy ZooKeeper or etcd environment.
Shared file systems or cache systems are created and deleted in a declarative manner.
In response to the preceding challenges in storage performance, elasticity, high availability, encryption, isolation, observability, lifecycle, and other aspects of a new computing mode, it’s imperative to not just upgrade storage products, but also make improvements to the cloud-native control or data planes, to achieve stable, secure, autonomous, and efficient cloud-native storage v2 in the near future.
With effective improvements and enhancements on the cloud-native application layer, the storage cloud product layer, the underlying storage adaptation, and the storage core layer, it’s possible to provide more stable, secure, autonomous, and efficient application-oriented cloud-native storage.
Let’s take a look at a quick snapshot of the article:
The evolution of cloud-native storage v2 still requires a joint effort from the container team and the storage team, so that the storage capability in the cloud-native era is improved.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-savings-plans-cost-optimization-provided-by-cloud-native-c09f6495d1a5?source=search_post---------305,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 21, 2021·10 min read
At the Apsara Conference 2020, Alibaba Cloud launched new savings plans that help customers optimize their computing costs. Customers can purchase cloud resources in advance and enjoy pay-as-you-go services to improve flexibility and efficiency, as well as reducing costs. This article will introduce the newly launched savings plans in detail.
First, let’s take a look at a typical case.
A customer has purchased ten cloud hosts (instances) in zone F of Beijing region for a year using the subscription billing method. Due to changes in business requirements, five of them have to be migrated to zone G after three months. The customer has to ask for a refund of five hosts in zone F and then purchase five new hosts in zone G. This process is unnecessary repetitive and the customer needs to pay for any price differences. This problem can be evaluated from two perspectives:
The root cause of this problem is that the subscription billing method binds product preferences to specific instances. Once a customer makes a purchase, he/she has actually purchased a server on the cloud. By doing so, the customer is bound to the server and any changes due business adjustments will inevitably introduce problems. For example:
(1) When adjusting the region or zone, the customer needs to ask for a refund and repurchase a new host.
(2) When adjusting the computing power, the customer can only upgrade or downgrade the configurations of a server, and this may also introduce additional expenses. Sometimes, such operations cannot succeed due to the technical limitation.
(3) When adjusting the business operations, the customer has to clean up the current business environment and initialize a new business environment. This process involves a large number of invalid O&M procedures.
However, in the cloud native era, most of customers need to adapt to the rapidly changing market and are unable to ensure that they will not adjust their businesses within a billing cycle. Is there a solution to this issue? The answer is simple: pay-as-you-go service. This billing method works because it treats cloud consumption as a resource that can be purchased quickly, similar to a fast-food operation.
Create instances as needed and release when not needed. Any business adjustments should not be limited by the instance lifecycle, nor should it involve complex lifecycle management. This is the way to purchase resources in the cloud native era.
You may be wondering now, if a pay-as-you-go billing method is so beneficial, why would anyone use subscription billing? The answer: cost. Generally speaking, a pay-as-you-go billing method is not cost effective for large and frequent consumption of cloud resources. And that is exactly why Alibaba Cloud has launched the savings plans. This article discusses what the newly launched savings plans are about, and compares how much you can save using the savings plans when compared with a pay-as-you-go billing.
In the simplest terms, the savings plans can be viewed as a “VIP member card” by Alibaba Cloud”. This “card” has following features:
(1) Wide scope of application. It can be used for almost all families of ECS and ECI instances in all regions. One purchase provides cost optimization capabilities to all ECS consumption.
(2) Annual validity period. The validity period is either 1 or 3 years.
(3) Fee-based membership card. The membership card itself is charged, but the fee is far lower than the pay-as-you-go billing, which is approximately within the range of annual and monthly subscription billing.
(4) The service covers pay-as-you-go users. Within the coverage of the membership card, users are exempted from charges when using pay-as-you-go instances.
For the full definition of Alibaba Cloud savings plans, see Alibaba Cloud Documentation.
But what exactly is the extent of the coverage of the savings plans? To understand this, let’s discuss an important concept for the savings plans, which is known as “committed hourly consumption”.
As shown in the figure above, the “committed hourly consumption” is an option on the purchase page of the savings plans.
To put it simply, committed hourly consumption is the upper limit of exemption for pay-as-you-go bills. Within this limit, bills for the pay-as-you-go instance are discounted and then deducted. Let’s take a look at a specific example:
(1) The pay-as-you-go price of the ecs.c5.large instance in Shanghai is 0.62 yuan per ECS instance for an hour. (For more information about the instance price, see ECS Price).
(2) With savings plans, the ecs.c5 instance family in Shanghai can be purchased at a discount of 57.8%. (Note: This example uses the one-year general purpose instance with full pre-payment as reference, which will be discussed further. The discount price of the savings plans can be found in Discount Page.)
(3) Suppose the customer selects a committed hourly consumption of 10 yuan.
Note: The price for this example is provided by Alibaba Cloud on September 27, 2020.
The figure above shows the discount details of savings plans in the Discount Page. The number of ecs.c5.large instances that can be deducted per hour for this savings plans is: 10/(0.62*0.422) = 38.22 instances. It can be seen that the committed hourly consumption determines the deduction of each savings plans, which is the card “coverage” mentioned above.
You may notice that 38.22 is not a round number. In reality, only 38 or 39 instances can be used. How do we go about this?
It is very simple actually. If 39 instances are used, 22% of the bill for the 39th instance will be deducted, while the remaining 78% of the bill needs to be paid in regular pay-as-you-go mode. However, if only 38 instances are used, the hourly deduction of 0.22 instances will be unused.
Therefore, it is very important to choose the appropriate committed hourly consumption to maximize your savings. If a customer chooses too few instances, any additional instances still need to be paid in pay-as-you-go mode. If customers choose too many instances, any unused discounts will be wasted. So, how can we optimize this? We will share a detailed analysis in the later sections; you can skip directly to the “How to Purchase Savings plan?” for more details. In addition, the Overview Page of savings plans provides more detailed examples about discounts.
At present, Alibaba Cloud offers two types of savings plans: general purpose type and compute optimized type.
General purpose type can be simply interpreted as having no restrictions. It can be applied to all ECS and ECI instance families in all regions.
Compute optimized type requires a region and an instance family, which applies only to the same ECS instance family in the same region.
General purpose type provides better flexibility, while compute optimized type provides better discounts. As shown in the figure below, for 1-year ecs.s6 instance family in Shanghai with full pre-payment, general purpose type offers a 57.8% discount, while compute optimized type offers a 63.3% discount.
Note: The price for this example is provided by Alibaba Cloud on September 27, 2020.
Currently, Alibaba Cloud offers three payment types: full pre-payment, partial pre-payment, and no pre-payment.
After a customer selects his/her preferred committed hourly consumption, the total price of savings plans (also called total configuration fee in the purchase page) is basically determined. For example, if the committed hourly consumption is 1 yuan per hour, the total price for one year is about: 124365 = 8760. If customer selects full pre-payment, he needs to pay all the fees at the time of purchase, similar to subscription mode. If customer selects partial pre-payment, he needs to pay about 50% of the purchase fee. The remaining fee is included in the hourly bill. If customer selects no pre-payment, he does not need to pay any fees when purchasing. All the fees will be included in the bill of subsequent hours.
In this example, when selecting full pre-payment, the price is displayed as follows:
When selecting partial pre-payment, the price is displayed as follows:
When selecting no pre-payment, the price is displayed as follows:
Note: You may be wondering why the total configuration fee of no pre-payment and partial pre-payment is slightly higher than that of full pre-payment. Here is the explanation from the official documentation:
The discount differs by the type of payment. Full pre-payment receives the highest discount, and no pre-payment receives the lowest discount. Let’s compare the discounts offered for 1-year ecs.s6 general purpose instance family in Shanghai:
Note: The price for this example is provided by Alibaba Cloud on September 27, 2020.
Compared with subscription mode, savings plans supports pay-by-installment, which can help customers optimize their cash flow. Generally speaking, customers can choose their preferred payment methods based on the actual cash flow and financing costs.
The current savings plans offer two types of durations: 1-year and 3-year models. The discount for 3-year model is greater than that of 1-year model. Let’s compare the discounts offered for ecs.s6 general purpose instance family in Shanghai:
Note: The price for this example is provided by Alibaba Cloud on September 27, 2020.
The process to purchase savings plans are as follows:
As mentioned earlier, the choice of coverage should be optimized based on your actual needs. Let’s look at some general tips to help you make the most out of the savings plans.
For the selection of savings plans type:
(1) If the instance family and deployment region are limited, select compute optimized type for higher discount.
(2) If the instance family and deployment region are not limited, select general purpose type for better flexibility. In addition, this option maximizes deployment utilization. By purchasing savings plans for multiple resources, your O&M and billing management can be simplified.
(3) For ECI users, select general purpose type to experience cloud-native cost optimization.
For the selection of committed hourly consumption:
(1) It is recommended that customers purchase savings plans for stable service loads, and continue to use pay-as-you-go billing method for short-term elastic services. The daily use time cutoff points are roughly recommended as follows:
(2) Lower committed hourly consumption is recommended. If it is lower than the actual situation, other deductions can be accessed by purchasing more savings plans.
(3) If your business is mostly built based on pay-as-you-go services, you can select the committed hourly consumption in the recommendation page, which is based on your historical pay-as-you-go consumption.
(4) If your business is mostly built based on subscription billing, you can search for the corresponding pay-as-you-go prices and discounts with savings plans, according to your instance families and regions. After the subscription expires, you can choose pay-as-you-go service and purchase corresponding savings plans instead of renewing your subscription.
For example, say you want to purchase a 3-year general purpose type of savings plans with full pre-payment for 10 ecs.g6.large instances in Shanghai. The pay-as-you-go price for ecs.g6.large instance is 0.5 yuan per hour, and the total price for 10 instances is 5 yuan per hour. The discount for ecs.g6 instance family in Shanghai provided by a 3-year general purpose type of savings plans with full pre-payment is 54.5%. The committed hourly consumption should be: 5*0.455 = 2.275 yuan per hour.
Note: The price for this example is provided by Alibaba Cloud on September 27, 2020.
For selecting of payment type:
You can select the corresponding payment type based on your cash flow and financing cost.
For selecting of duration:
The 3-year model provides greater discounts when compared with that of the 1-year model. You can select your preferred duration based on the size of your business. Generally, with better flexibility of savings plans, longer duration can provide better discounts.
Ready to save more? Click here to purchase your savings plans today!
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mohamed-ahmed/securing-cloud-native-applications-is-the-new-foundation-to-digital-transformation-success-d115c59c2abb?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Ahmed
Feb 18, 2021·4 min read
This article was originally published at: https://www.magalix.com/blog/securing-cloud-native-applications-is-the-new-foundation-to-digital-transformation-success
It seemed like just recently simply transforming your monolith into a Cloud-Native application was the way to digitally transform your business and organization into an ever scaling, highly functioning machine. Leveraging DevOps, organizations have continuously delivered new…
"
https://medium.com/@alibaba-cloud/alibaba-cloud-showcases-cloud-native-polardb-database-at-icde-826015dec4c0?source=search_post---------307,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 22, 2018·4 min read
Alibaba Cloud demonstrated its self-developed PolarDB database with up to 1,000,000 QPS to European technical experts for the first time at the International Conference on Data Engineering (ICDE) in April 2018. The release of new database falls in line with Alibaba Cloud’s efforts towards establishing cloud-native databases as a standard in the industry.
According to Alibaba Cloud’s Database Architect Cai Songlu, with the hardware and software of the new 3DXpoint storage media, NVMe SSD, and RDMA, PolarDB can implement the shared storage among nodes and horizontal scalability in just a few seconds. The database can also support seamless scaling up to 100 TB, 2-minute read-only replica scaling, and 1-minute full backup. In standard scenarios, PolarDB has six times higher performance than MySQL, can provide one million QPS, and 100 TB in one single table. Perhaps the best part of all is that all these features come at a cost ten times lower than that of other commercial databases.
In the recent years, exponential data growth and data diversification have led to diversified database needs. In addition to database performance, users now focus more on how to bring offline service data online, how to make services available to customers in real-time, and how to reduce operations cost, and O&M workloads.
Cai Songlu pointed out that “only Cloud-Native Database can meet these diverse user needs.” Alibaba Cloud’s innovative cloud-native database concept can provide the HTAP, Server-less, and Intelligence features, and help users implement online real-time analytics, automatic SQL statement optimization, automatic parameter adjustment, and fault analysis. Additionally, users can directly utilize the computing power without having to focus on O&M.
At ICDE held this year, Alibaba Cloud innovatively applied the PAXOS algorithm. Therefore, the new-generation POLARDB database can ensure consistency as well as implement error tolerance of nodes. “Instead of the relationship between 0 and 1, the difference between 99% and 100% should be the core of the CAP theorem. Cloud databases will implement perfect CAP balance, just like the Chinese martial art Tai Chi,” Cai Songlu said.
At ICDE, technical experts expressed astonishment over China’s switch from cash payments to mobile payments.
According to Alibaba Cloud Product Manager He Yunfei, mobile payment, high-speed rail, bicycle sharing, and online shopping (collectively known as China’s four new great inventions) are all backed by China’s prowess in cloud computing.
Additionally, there are other scenarios in China where Alibaba Cloud has played a vital role; this includes the 12306 Online Ticketing System with 40 billion visits at peak time, the Double 11 Shopping Festival during which various systems process 320 thousand trades every second, and the ET City Brain program which makes urban transportation easier and smoother.
Being one of China’s representative breakthroughs, cloud computing is now changing the way of life for people in China.
Cloud computing technology supports the massive demand for numerous services which are together bringing a transformational change. “From 5,000 servers in 2013, and we have reached up to 10,000 servers in one cluster; all this in just four years,” said He Yunfei. This size expansion from 5,000 servers to 10,000 servers is not just about doubling the number of servers. It also involves the evolution of underlying technologies such as Alibaba Cloud’s independently developed large-scale and distributed operating system Apsara.
In 2016, Alibaba Cloud established its first European data center in Frankfurt, Germany. This data center enables Alibaba Cloud to gain increasing momentum in the European market. A growing number of enterprises and organizations have started utilizing Alibaba Cloud services to create their technology ecosystems, including Vodafone and Met Office in the UK. Currently, Alibaba Cloud has 47 zones in 18 countries worldwide, establishing its preliminary coverage of global infrastructure.
Read similar articles and learn more about Alibaba Cloud’s products and solutions at www.alibabacloud.com/blog.
Reference:
https://www.alibabacloud.com/blog/alibaba-cloud-showcases-cloud-native-polardb-database-at-icde_593907?spm=a2c41.11905323.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/addressing-security-challenges-in-the-container-and-cloud-native-age-4397ad4fff39?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 13, 2021·5 min read
By Muhuan, Dahu, and Zhuanghuai
The China Academy of Information and Communications Technology believes that Alibaba Cloud Container Service takes the lead in the Chinese market and outperforms other competitors in the international market in the following areas: minimizing attack surfaces, verifying the signatures of binary images, and encrypting data by using the bring-your-own-key (BYOK) model.
Back in 2018, we were already prepared to meet security challenges of the container and cloud-native age. Containers and cloud-native computing have brought security challenges that were new to traditional environments.
High Density and Dynamicity
Each traditional server runs a few applications. With the introduction of containerization, hundreds of applications now can run on a single server. In addition, the automatic recovery of containers may result in their migration between servers at a moment’s notice.
Fast and Agile Iteration
Containerized applications developed in the DevOps model are released several times more frequently than traditional applications.
More Risk Sources
In a time of open standards, and as the software industry makes increasing use of community-developed projects, the introduction of non-trusted, open-source third-party software leads to greater security risks. At the same time, containers by nature require a higher level of cloud-native security.
To deal with the unceasing difficulties brought by the preceding challenges, in 2018, the Alibaba Cloud Container Service team put forward the concept of end-to-end enterprise-grade security and developed a three-layer comprehensive, end-to-end cloud-native security architecture.
As shown in the preceding figure, the underlying layer of this architecture relies on the physical security, hardware security, virtualization security, and cloud product security capabilities provided by the Alibaba Cloud platform.
The middle layer of the architecture, container infrastructure security, is based on the idea of attack surface minimization. Many important security capabilities work at this layer, including access control, permission management, configuration hardening, and identity management. It also provides necessary elements for user access security, such as credential issuance, certificate and key management, and cluster audit. These elements together form an automated operations and maintenance (O&M) system.
At the top layer are runtime and supply-chain security capabilities that cover the lifecycle of containerized applications from building an application to running it. For example, image signing and security scanning are available in the building phase, and integrated security management capabilities are available in the deployment and runtime phases. These capabilities include runtime policy management, configuration inspection, and runtime security scanning. The architecture also supports sandboxed containers and a trusted execution environment (TEE). These features together provide enterprises with improved isolation and data security for containerized applications.
Container security now faces a host of new challenges. On one hand, critical vulnerabilities frequently occur in open-source projects such as Kubernetes, Helm, and etcd. On the other hand, containerized applications have increasingly short lifecycles and are more deployed on cluster nodes. These facts make it difficult for traditional security scanning on the supply-chain side to fully expose the risks associated with these applications.
To address these new challenges, we have used the three-layer cloud security architecture described in the preceding section with Alibaba Cloud Container Service for Kubernetes and Container Registry to conduct the following successful trials:
With these benefits, we have built the overall security architecture for Alibaba Cloud container services and can guarantee the security of containerized applications throughout their lifecycles.
The overall security architecture for Alibaba Cloud container services provides the following benefits for user applications in different phases of their lifecycles:
Development, Testing, and Building Phases
Pre-deployment
With the centralized policy management platform in Alibaba Cloud Container Service, a cluster security administrator can implement customized security governance for application systems in different clusters.
Post-deployment
The Container Service security management center allows users to protect the runtime security of container applications and provides in-depth protection for overall container security.
Since the beginning of containerization in 2011, Alibaba has taken the lead among Chinese enterprises in applying cloud native technology on a large scale in various industries, including e-commerce, finance, and manufacturing.
Over the past decade, Alibaba Cloud has provided security solutions for Container Service customers in the following scenarios:
Security Protection for Corporate IT Assets
An international new-retail giant uses the image signing and scanning features of the Container Registry security software supply chain, resource access management (RAM) roles for RBAC, end-to-end mTLS authentication in Alibaba Cloud Service Mesh, and certificate management and auditing.
Data Operation Security
To implement fine-grained control of east-west application traffic, an international financial bank uses end-to-end data encryption in Container Service for Kubernetes and runtime alert monitoring in Security Center in conjunction with Alibaba Cloud Service Mesh.
Permission Management and Control
To protect sensitive data on containers from disclosure, an international game company uses permission-based isolation of control-plane cloud resources at the pod level, synchronously imports and updates keys from external key management systems, and implements data-plane permission control and key management.
The preceding scenarios are typical to our customers. With our experience dealing with the requirements of these scenarios, Alibaba Cloud has launched the most extensive cloud-native product family, made the most comprehensive open-source contributions to cloud-native computing, developed the largest container clusters, established the largest customer base, and acquired rich practical experience in deploying cloud-native applications.
In May 2020, Gartner released its report “Solution Comparison for the Native Security Capabilities”, which was the first comprehensive assessment of the overall security capabilities of the world’s top cloud vendors. Alibaba Cloud, the only Asian company made in to the report, ranked second in overall security capabilities, with 11 specific capabilities evaluated as the highest level (High).
The Alibaba Cloud Container Service team believes that these strengths awarded Alibaba Cloud with the certification for advanced trusted cloud security from the China Academy of Information and Communications Technology.
Source: Lingyun Moments (WeChat official account: linuxpk)
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@glengilmore/telstra-and-ericsson-a-seamless-journey-to-5g-and-cloud-native-e296e3e5b930?source=search_post---------309,"Sign in
There are currently no responses for this story.
Be the first to respond.
Glen Gilmore
Sep 30, 2021·4 min read
As an adjunct faculty member at Rutgers University School of Business, Executive Programs, I have taught, among other topics, Emerging Technologies. Recognized as a top influencer in Digital Transformation, I regularly provide research and consultations with leading tech companies, including Ericsson. Having served two terms as the mayor of a city of about 90,000 people in the United States, I have an abiding interest in the role of “Tech for Good”. I currently reside in Italy.
Sponsored by Ericsson Digital.
In the video above, we learn about Telstra's “Why”. As Australia’s leading telecommunications company, Telstra is literally entrusted with safeguarding lives, accelerating business opportunities, and offering Australians the best telecommunications services possible. An industry leader, Telstra built Australia’s first and largest 5G network.
As we have seen with many leading communication services providers, there has been a flattening of the average revenue per user (ARPU) in the telco industry. To combat this — Telstra has been extremely innovative in enhancing its services to capture new growth opportunities from new technologies. One way Telstra did this, is that they introduced a more efficient solution to reduce network complexity while seizing the opportunities of 5G.
A seamless journey to 5G and “cloud native” was a must for Telstra with so many people and enterprises depending so much on Telstra’s critical, uninterrupted services. Greater automation and orchestration were needed to quickly roll out and optimize new technologies and services. A partnership with Ericsson made a swift and seamless journey to 5G and “cloud native” possible.
5G, the fifth generation of cellular networks, is up to 100 times faster than 4G. It brings businesses, consumers, and communities near-real-time data exchange, offering new services, efficiencies, security, and experiences that come with its enhancements. It accelerates collaborations and innovations. It gives us the ability to connect everything, everywhere without delays.
Cloud Native “unlocks the potential of 5G” by providing a business model and technological framework that take advantage of all “the possibilities of the cloud.” Cloud Native gives enterprises the operational efficiencies that come with leveraging the cloud’s elasticity to quickly scale new capabilities, automate upgrades, adapting instantly to changing environments and needs, and obtaining cost efficiencies in the process. It brings vastly improved interoperability.
“The cloud-native 5G Core is a must-have component of a 5G network to deliver its full benefits to industries and consumers,” explains Monica Zethzon, Vice President and Head of Solution Area Packet Core at Ericsson.
“It’s exciting to see how leading customers and 5G pioneers such as Telstra are now making it all a reality for their customers with networks empowered by our solutions. This is a recognition of our strategic investments to bring a leading 5G Core portfolio to market. It also shows how our vast experience in 5G supports our customers to achieve their visions and ambitions using our technology,” she added.
With Ericsson’s “Spectrum Sharing” (ESS), Telstra has been able to operate 4G and 5G services simultaneously, in the same spectrum, allowing Telstra to “move very quickly in offering a 5G coverage service across the whole 4G network layout.” In doing so, ESS enabled the introduction of 5G on existing 4G bands with minimal impact on live end-user service.
Ericsson’s “dual-mode” 5G Cloud Core solution empowered Telstra to launch new services based on 5G stand-alone technology while still caring for its existing 4G customer base in a common software platform.
Ericsson also ensured that the existing network infrastructure serving 4G was upgraded (software only) to add support to 5G non-standalone services.
Telstra successful transitioning to 5G and cloud native, which is progressing in steps, “with each step providing concrete benefits”, is empowering the company to “expand its role in the communication services value chain by seizing new opportunities to offer new use cases and applications to both consumers and enterprises.”
With Ericsson’s collaboration, by way of example, Telstra will be able to leverage Ericsson’s cloud-native core solution capabilities in combination with Artificial Intelligence/ automation so that the network can “adapt to traffic needs or even predict what the traffic needs are, as well as to self-heal, move around and re-deploy across the underlying cloud infrastructure.”
Summarizing Telstra’s successful transitioning to 5G and cloud native, David Aders, Telstra’s Group Owner for Mobile Development and Product Engineering, observed:
The sky is the limit with this highly programmable architecture. We need to disrupt ourselves as an industry to actually get out there and be able to realize new services and move into all of the promising areas available to us with 5G.
* Special thanks to Ericsson’s Monica Zethson and Telstra’s David Aders for sharing their important insights on this case study.
FORBES Top 20 Social Media Influencer | TIME “man of action” | Rutgers Digital Marketing & Emerging Tech Educator | Strategist | Lawyer | Futurist
1 
1 
1 
FORBES Top 20 Social Media Influencer | TIME “man of action” | Rutgers Digital Marketing & Emerging Tech Educator | Strategist | Lawyer | Futurist
"
https://medium.com/@alibaba-cloud/how-to-achieve-stable-and-efficient-deployment-of-cloud-native-applications-e2002d5f1463?source=search_post---------310,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 3, 2020·13 min read
By Jiuzhu, Alibaba Cloud Technical Expert, and Mofeng, Alibaba Cloud Development Engineer
On May 28, we livestreamed the third SIG Cloud-Provider-Alibaba webinar. This livestream introduced the core deployment problems encountered during the large-scale cloud migration of the Alibaba economy and their corresponding solutions. It also explained how these solutions helped improve the efficiency and stability of application deployment and release after they were transformed into universal capabilities and made open-source.
This article summarizes the video and data download of the conference livestream, and lists the questions and answers collected during the livestream. We hope it is helpful for you.
As Kubernetes has gradually become a de facto standard and a large number of applications have become cloud-native in recent years, we have found that the native workload of Kubernetes is not very “friendly” to support large-scale applications. We are now exploring ways to use Kubernetes to provide improved, more efficient, and more flexible capabilities for application deployment and release.
This article describes the application deployment optimization we have made during the process of fully connecting the Alibaba economy to cloud-native. We have implemented an enhanced version of the workload with more complete functions, and have opened its source to the community. This makes the deployment and release capabilities used by Alibaba’s internal cloud-native applications available to every Kubernetes developer and Alibaba Cloud user.
Alibaba’s containerization has taken a big leap all over the world in recent years. Although the technical concept of containers emerged early, it was not well known until Docker products appeared in 2013. Alibaba began to develop LXC-based container technology as early as 2011. After several generations of system evolution, Alibaba now has more than one million containers, with scale leading in the world.
With the development of cloud technologies and the rise of cloud-native applications, we have gradually migrated Alibaba’s containers to a Kubernetes-based cloud-native environment over the past two years. Throughout the migration process, we have encountered many problems in application deployment. First of all, application developers have the following expectations for the application migration to the cloud-native environment:
Alibaba has complex application scenarios. Many different Platform as a Service (PaaS) layers grow based on Kubernetes, for example, the operation and maintenance (O&M) mid-end, large-scale O&M, middleware, serverless, and function computing for the e-commerce business. Each platform has different requirements for deployment and release.
Let’s take a look at the capabilities of two commonly used workloads provided by the Kubernetes native:
To put it simply, Deployment and StatefulSet work well in small-scale scenarios. However, for large-scale applications and containers like Alibaba’s, it is infeasible to only use native workloads. Currently, the number of applications in Alibaba container clusters exceeds 100,000 and the number of containers reaches millions. For some core applications, a single core application even has tens of thousands of containers. Taking the problems shown in the preceding figure into consideration, we find that the release capability of a single application is insufficient. Moreover, when a large number of applications are being upgraded at the same time during the release peak time, ultra-large-scale pod recreation becomes a “disaster”.
Native workloads are far from meeting requirements in application scenarios, so we abstract common application deployment requirements from a variety of complex business scenarios and develop multiple extended workloads accordingly. While we have significantly enhanced these workloads, we ensure the generalization of the functions and avoid the coupling of business logic.
Here, we focus on CloneSet and Advanced StatefulSet. In Alibaba’s cloud-native environment, almost all e-commerce-related applications are deployed and released using CloneSet, while stateful applications, such as middleware are managed using Advanced StatefulSet.
As the name implies, Advanced StatefulSet is an enhanced version of the native StatefulSet. Its default behavior is the same as the native StatefulSet. In addition, it provides features, such as in-place upgrade, parallel release (maximum unavailable), and release pause. CloneSet, on the other hand, targets native Deployment. It serves stateless applications and provides the most comprehensive deployment and release strategies.
Both CloneSet and Advanced StatefulSet support specifying pod upgrade methods:
During an in-place upgrade, the workload does not delete or create the original pods when the template is upgraded. Instead, it directly updates the image and other data on the original pods.
As shown in the preceding figure, CloneSet only updates the image of the corresponding container in the pod spec during an in-place upgrade. When the kubelet detects the change in the definition of the container in the pod, it stops the container, pulls a new image, and uses the new image to create and start a container. In addition, we can see that the sandbox container of this pod and other containers that are not upgraded this time continue running normally during this process. Only the containers that need to be upgraded are affected. In-place upgrades have brought many benefits:
Later, we will provide a dedicated article explaining Alibaba’s in-place upgrade on Kubernetes, which is of great significance. Without in-place upgrades, it is impossible for Alibaba’s ultra-large-scale application scenarios to be implemented in the native Kubernetes environment. We recommend that every Kubernetes user “experience” the in-place upgrade, which brings a change that is different from the traditional release mode of Kubernetes.
As mentioned in the previous section, Deployment supports stream upgrade of maxUnavailable and maxSurge currently, while StatefulSet supports phased upgrade of partitions. However, Deployment does not support phased release, and StatefulSet can only release pods sequentially, so it’s not possible to perform stream upgrade in parallel.
Therefore, we introduced maxUnavailable to Advanced StatefulSet. Native StatefulSet uses a one-by-one release. In this case, we can consider that maxUnavailable is set to 1. If we configure a larger value for maxUnavailable in Advanced StatefulSet, more pods can be released in parallel.
Let us take a look at CloneSet. It supports all the release policies of native Deployment and native StatefulSet, including maxUnavailable, maxSurge, and partition. So, how does CloneSet integrate these policies? Here is an example:
The CloneSet has five replicas. Assume that we modify the image in the template, and set maxSurge to 20%, maxUnavailable to 0, and partition to 3.
When the release begins:
If we change the number of partitions to 0, CloneSet will create an extra pod from the later version first, then upgrade all of the pods to the later version, and finally delete one pod to reach the final state of full upgrades of the five replicas.
We cannot configure the release sequence for native Deployment and Statefulsets. The release sequence of pods under Deployment depends entirely on the scaling order after ReplicaSet modification, while StatefulSet upgrades pods one-by-one in a strict reverse order.
Therefore, in CloneSet and Advanced StatefulSet, the release sequence is configurable so users can customize their release sequences. Currently, the release sequence can be defined based on either of the following two release priority policies and one release scattering policy:
You may ask why it is necessary to configure the release sequence. For example, before you release applications, such as ZooKeeper, you need to upgrade all the non-primary nodes first and then upgrade the primary node. This ensures that the primary node switchover takes place only once in the entire release process. You can label the node responsibilities on ZooKeeper pods through the process or write an operator to do it automatically. Then, you can configure larger release weights for non-primary nodes to minimize the number of times of switching over to the primary node during the release.
Making containers lighter is also a major reform at Alibaba in the cloud-native stage. In the past, most Alibaba containers ran as “rich containers”. A rich container is a container that runs both businesses and various plug-ins and daemons. In the era of cloud-native, we have gradually separated bypass plug-ins from rich containers and incorporated them into independent sidecar containers so that main containers only serve businesses.
We do not further describe the benefits of function separation. Let’s look at another question — how are these sidecar containers managed? The most straightforward method is to explicitly define the sidecars required by pods in the workload of each application. However, this causes many problems:
Therefore, we have designed SidecarSet to decouple the sidecar container definition from the application workload. Application developers no longer need to care about which sidecar containers need to be written in their workloads. Instead, sidecar maintainers can independently manage and upgrade sidecar containers through in-place upgrades.
You should now have a better understanding of the basics of Alibaba’s application deployment mode. The above capabilities have been open-sourced to the community, with a project called OpenKruise. The project currently provides five extended workloads:
In addition, we have more extended capabilities on the way to open-source! Soon, we will make open-source the internally used Advanced DaemonSet to OpenKruise. Advanced DaemonSet provides release policies, such as phased release and selector release based on maxUnavailable of the native DaemonSet. The phased release function allows DaemonSet to upgrade some of the pods during a release. The selector release function allows preferential upgrading to pods on the nodes that meet certain labels during a release. This brings the phased release capability and stability guarantee for upgrading DaemonSet in large-scale clusters.
In the future, we also plan to make Alibaba’s generalization capabilities, such as internally extended HPA and scheduling plug-ins open-source for the community, to make the cloud-native enhancements developed and used by Alibaba available to every Kubernetes developer and Alibaba Cloud user.
Finally, we welcome every cloud-native enthusiast to participate in the construction of OpenKruise. Unlike other open-source projects, OpenKruise is not a copy of Alibaba’s internal code. On the contrary, the OpenKruise Github repository is the upstream of Alibaba’s internal code repository. Therefore, every line of code you contribute will run in all Kubernetes clusters within Alibaba, and will jointly support Alibaba’s world-leading application scenarios!
Q1: What is the number of pods for Alibaba’s current largest-scale business and how long does a release take?
A1: Currently, the largest number of pods for a single application is in the unit of ten thousand. The time a release takes depends on the duration of specific phased releases. If many phased releases are scheduled and the observation time is long, the release may last for a week or two.
Q2: How are the request and limit parameters configured for a pod? By what ratio are requests and limits configured? Too many requests cause waste, while too few requests may cause an overload on hotspot nodes.
A2: The ratio is determined based on application requirements. Currently, most online applications are configured with a ratio of 1:1. For some offline applications and job-type applications, the configuration of requests greater than the limit is required.
Q3: How do I upgrade the deployment of the current version when I upgrade the kruise apiVersion?
A3: Presently, the apiVersion of resources in kruise is unified. We plan to upgrade the versions of some mature workloads in the second half of this year. After you upgrade your Kubernetes clusters, the resources of earlier versions are automatically upgraded to later versions through conversion.
Q4: Does OpenKruise provide go-client?
A4: Currently, two methods are provided:
Q5: How is Alibaba’s Kubernetes version upgraded?
A5: Alibaba Group uses the Kube-On-Kube architecture for large-scale Kubernetes cluster management and uses one meta-Kubernetes cluster to manage hundreds of thousands of business Kubernetes clusters. The meta cluster version is relatively stable, while business clusters are frequently upgraded. The upgrade process of business clusters is to upgrade the versions or configurations of the workloads (native workloads and kruise workloads) in the meta cluster. This process is similar to the process of upgrading business workloads in normal conditions.
Q6: After the phased release begins, how do you control the inlet flow?
A6: Before an in-place upgrade, kruise first uses readinessGate to set pods to the not-ready state. At this point, controllers, such as endpoints, will perceive the setting and remove the pods from the endpoints. Then, kruise updates the pod images to trigger container recreation and sets the pods to the ready state.
Q7: Are phased releases of DaemonSet implemented in a way similar to the pause function of Deployment? DaemonSet counts the number of released pods, pauses, continues the release, and then pauses again.
A7: The overall process is similar to the pause function of Deployment. During an upgrade, earlier and later versions are counted to determine whether the desired state has been reached. However, compared with Deployment, DaemonSet needs to handle more complex boundary cases. For example, the specified pod is not in the cluster in the first release. For more information, pay close attention to the code that is open-sourced.
Q8: How does the release start on the multi-cluster release page?
A8: The livestream demonstrates an example where a demo release system is integrated with Kruise Workloads. In terms of interaction, the user selects the corresponding cluster and clicks to execute the release. In terms of implementation, the difference between the YAML in the new version and the YAML in the cluster is calculated and then patched into the cluster. After that, the control field (partition or paused) of DaemonSet is operated to control the phased release process.
Jiuzhu is an Alibaba Cloud Technical Expert, OpenKruise maintainer, and the Leader of cloud-native workload development at Alibaba Group. He has many years of experience in supporting the ultra-large-scale container clusters for the Alibaba Double 11 Global Shopping Festival.
Mofeng is an Alibaba Cloud Development Engineer. He is responsible for building the phased release system for the Alibaba Cloud-Native Container Platform and developing etcd. He also participated in the stability construction of Alibaba’s large-scale Kubernetes clusters.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/acna-explained-the-6-key-capabilities-behind-cloud-native-f40361e86db7?source=search_post---------311,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 13, 2021·4 min read
To learn more about designing and developing the right cloud-native architecture for your business, download The Cloud-Native Architecture White Paper today.
A cloud-native architecture is the next evolution of the cloud. It provides a plan or design for your cloud-based applications and services, focusing on how to optimize your system architectures for the capabilities of the cloud to realize seamless agility, elasticity, fault tolerance and portability.
To achieve this, your cloud-based architecture will consist of many different components, including containers, microservices and service meshes. With a cloud-native architecture, you can add, change or replace these components with ease — and without affecting the other components in your infrastructure. But how do you design a cloud-native architecture? Where should you start?
Appropriately, the design process for a cloud-native architecture also breaks the project down into manageable chunks. Our Alibaba Cloud Native Architecture (ACNA) is one example, using a “4 + 1” architecture design process. The number 4 indicates the key perspectives of the architecture design, including enterprise strategies, business development, organizational capabilities, and cloud-native technologies. The number 1 indicates the closed loop of continuous cloud-native architecture evolution.
ACNA is based on a DevOps with the cloud methodology, accelerating your software development and delivery. It is available at varying levels of maturity, which are ranked across six architectural dimensions.
A cloud-native architecture uses microservices, which are an architectural approach to building applications in a distributed and loosely coupled fashion. This is a form of service-oriented architecture (SOA), which separates the business into distinct modules, each of which has different iteration cycles and integrates and orchestrates modules by using standardized API operations.
Your services are also integrated, based on events, to further reduce dependencies between different services. By using different methods to assess your service capabilities, this provides a higher level of SLA-defined services. Alibaba Cloud excels in this space, being the first cloud providers to offer 99.975% SLA for a single instance and network.
Elasticity is a defining feature of the cloud, where a cloud-based infrastructure can dynamically scale your resources up and down. But just because an app exists on the cloud, does not guarantee its elasticity — this is where a cloud-native architecture can help, automatically scaling your applications based on business peaks and resource loads to achieve ultimate elasticity. Using a cloud-native architecture, the total transaction amount during the Double 11 Shopping Festival in 2020, for example, reached 498.2 billion RMB.
Serverless Computing is an event-driven cloud-native development model that allows developers to build and run applications without managing any servers. It is sometimes referred to as Function-as-a-Service (FaaS) or Backend-as-a-Service (BaaS). Serverless does not mean “without a server.” Instead, the details of your infrastructure orchestration are managed by a serverless platform provider and are hidden from your developers and end users.
As a result, developers can focus on the design and execute applications in a stateless manner and deploy stateful applications in cloud services. These stateless containers are event-driven, created when an event occurs and triggers a specific action. Your applications are bundled as one or more functions, uploaded to your platform and then executed, scaled, and billed in response to the exact demand needed at the moment.
Every IT infrastructure must be continuously managed, to fix any hardware and software errors before they adversely affect your business. This requires observability to create a robust and comprehensive quality of service (QoS) policy. Using a cloud-native architecture, there are many methods to keep your system under observation, including automation to free up your time and resources.
At Alibaba Cloud, these methods include our Log Analysis function for real-time log search and analysis, Tracing Analysis to identify performance bottlenecks and Application Real-time Management Service (ARMS) to comprehensively monitor your applications.
A cloud-native architecture supports high availability, disaster recovery, and asynchronization, using a vast range of technologies, including circuit breaking, throttling, degradation, automatic retry, and backpressure to achieve this. If you opt for an ACNA-4 maturity model, for example, your architecture uses serverless services and services meshes, allowing you to switchover in seconds.
A cloud-native architecture can expedite your development, testing, and O&M efforts, with several automation options available to streamline your efforts further. Using an Open Application Model (OAM), for example, you can standardize your software delivery, and an IaC or GitOps technology can also automate your Continuous Integration (CI) and Continuous Deployment (CD) pipeline and O&M.
While a cloud-native architecture is not scored based on security, it provides a robust set of security solutions for your business. In addition to using cloud services to enhance business security, it uses the software development life cycle (SDLC) to develop applications that follow the ISO 27001, Payment Card Industry Data Security Standard (PCI DSS), and Baseline for Classified Protection of Cybersecurity standards.
A cloud-native architecture is the next evolution in cloud computing, providing a vast range of value-added capabilities to your business. Alibaba Cloud can help you set-up, deploy and manage a cloud-native architecture to innovate and optimize your business. To find out more download The Cloud-Native Architecture White Paper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/a-deep-understanding-of-cloud-native-technology-and-application-bd545b5aba45?source=search_post---------312,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 22, 2020·9 min read
In this article, we will review the concepts and definition of cloud native in detail, and discuss its potential applications and impacts.
The cloud native concept emerges as cloud computing enjoys rapid development. Cloud-native has become extremely popular. You will be considered outdated if you do not understand Cloud-native as of this year.
Although many people are talking about cloud native, few have told you exactly what cloud native is. Even after finding and reading some cloud-native materials, most of you may still feel confused and lack a complete understanding of cloud native. At this point, you might start to doubt your own intelligence. In my case, I always tend to blame the authors’ stupidity for my incomprehensibility of certain articles, though this is not necessarily true. However, this way of thinking prevents me from being held back by my own self-doubt and I can try to remain positive.
The reason why cloud native cannot be explicitly described is the lack of a clear definition. Since cloud native is undergoing constant development and changes, no individuals or organizations have the absolute right to define cloud native.
Technical changes are always heralded by certain ideologies, just as the invincible Marxism leads to the prosperity of proletarian revolutions.
Cloud native is an approach to building and running applications. It is a set of systematized techniques and methodologies. Cloud native is a compound word made up of “cloud” and “native”. The word “cloud” represents applications residing in the cloud instead of in traditional data centers. The word “native” represents applications that are designed to run on the cloud and fully utilize the elasticity and the “distributed” advantage at the very beginning of application design.
The cloud native concept was first put forward by Matt Stine at Pivotal in 2013. In 2015 when cloud native was just becoming popular, Matt Stine defined several characteristics of cloud-native architectures in the book Migrating to Cloud Native Application Architectures: 12-factor applications, microservices, self-service agile infrastructure, API-based collaboration, and anti-fragility. At an InfoQ interview in 2017, Matt Stine made some changes and indicated six characteristics of cloud-native architectures: modularity, observability, deployability, testability, replaceability, and handleability. The latest description about cloud-native architectures on the official Pivotal website shows four key characteristics: DevOps, continuous delivery, microservices, and containers.
In 2015, the Cloud Native Computing Foundation (CNCF) was founded. CNCF originally defined four characteristics of cloud-native architectures: containerized encapsulation, automated management, and microservices. In 2018, CNCF updated the definition of cloud-native architectures with two new features: service meshes and declarative APIs.
As we can see, different individuals and organizations have different definitions for cloud-native architectures, and even the same individual or organization has different definitions for cloud-native architectures at different points in time. This complexity makes it hard for me to clearly understand cloud-native architectures. After a while, I came up with a simple solution: to choose only one definition that is easy to remember and understand (in my case, DevOps, continuous delivery, microservices, and containers).
In a word, cloud-native applications are required to meet the following: Implement containerization by using open-source stacks like K8s and Docker, improve flexibility and maintainability based on microservices architectures, adopt agile methods, allow DevOps to support continuous iteration and automated O&M, and implement elastic scaling, dynamic scheduling, and efficient resource usage optimization by using cloud platform facilities.
Cloud native supports simple and fast application building, easy deployment, and allows applications to be scaled as needed. Cloud-native architectures bring many advantages over traditional web frameworks and IT models, and have almost no disadvantages. Cloud-native architectures are definitely a powerful secret weapon in this industry.
The CNCF and Alibaba Cloud have jointly launched a new technology course to provide developers with insights into the future of cloud native.
As the cloud native technology stack represented by Kubernetes is becoming increasingly mature and the CNCF ecosystem is growing gradually, “Cloud Native” has become a keyword in the future cloud computing era. However, what exactly is “Cloud Native”? What is the relationship between cloud native, CNCF, and Kubernetes? Since the technology wave of cloud native has already arrived, how can we, as cloud computing developers and practitioners, implement cloud native technologies within our own organizations and gain a foothold in the cloud native technological revolution?
As witnesses and practitioners of the “cloud native” technical community for many years, we know that it is not easy to have every developer understand and adopt a new technology. For this reason, we have jointly developed the CNCF x Alibaba Cloud Native Technology Open Course with Cloud Native Computing Foundation (CNCF). We aim to answer every technical question about “cloud native” for each developer and reveal the thinking and essence behind the cloud computing revolution.
Over the years, CNCF has been dedicated to making cloud native computing universal and sustainable to global developers. The launch of the CNCF x Alibaba Cloud Native Technology Open Course is the first cloud native course tailored by CNCF exclusively for Chinese developers. You can learn from world-class technical experts to fully understand cloud native technologies. We firmly believe that the solution to making cloud native available to everyone relies only on a deep understanding of every design and thinking behind “cloud native”.
This article discusses how Alibaba Cloud is a leader in cloud native technology and applications and provides enterprises with powerful solutions.
In 2015, the first ever KubeCon + CloudNativeCon was held in San Francisco, with just over 200 attendees. But fast forward to the second KubeCon, this time held in China, over 3,000 attendees come from all over the globe — making KubeCon one of the biggest workshops of its kind. As predicted by Gartner, we are now entering the Cloud Native Era, where within the next three years, 75% of global enterprises will deploy containerized applications in production.
A forerunner in developing cloud native technologies and applications, Alibaba Cloud has shared 26 insightful presentations to the attendees of this year’s KubeCon + CloudNativeCon, of whom many are representatives of various global enterprises and others prominent developers. Ding Yu, the director of Alibaba Cloud Container Platform, pointed out:
“Cloud native is reshaping the entire software lifecycle. Containers, Kubernetes, and cloud native have become three key standards in the age of cloud. Alibaba Cloud will double down on its development of cloud native technologies and products, while maintaining its role in giving back to the open-source community. We are also collaborating with ecosystem partners to set cloud native standards and implement cloud native applications.”
This article is based on a speech given by Yi Li, Director of Alibaba Cloud Container Service, at the Cloud Native Industry Conference sponsored by CAICT.
Currently, most enterprises completely embrace cloud computing. The all-in-cloud era has witnessed three important changes: cloud-based infrastructure, Internet-based core technologies, and data-driven and intelligent services. In different fields and industries, many business applications were born in the cloud so that many enterprises are more and more similar to Internet companies. Therefore, technical capabilities are viewed as indispensable core competencies. At the 2019 Beijing Alibaba Cloud Summit, Zhang Jianfeng, President of Alibaba Cloud Intelligence, mentioned the significance of vigorous investments in the cloud native technology when talking about “Internet-based core technologies”.
Why should we embrace cloud native technology? On the one hand, cloud computing has rebuilt the entire software lifecycle, from architecture design to development, construction, delivery, and O&M. On the other hand, the IT architectures of enterprises have changed significantly and services deeply depend on IT capabilities. These two aspects contribute to complexity and challenges.
As the development of human society is followed by technology revolutions and changing divisions of labor, complexity can be reduced by using cloud native technology, which reflects IT progress.
First, Docker decouples applications from the runtime environments: The loads of many business applications can be containerized, and containerization makes applications agile, migratable, and standardized. Second, Kubernetes decouples resource orchestration and scheduling from underlying infrastructure: Application and resource control is simplified, and container orchestration improves the efficiency of resource orchestration and scheduling. Third, the service grid technology represented by Istio decouples service implementation from service governance capabilities. In addition, Alibaba Cloud provides diverse development tools (such as APIs and SDKs) for integrating third-party software. This opens up extensive possibilities for cloud ecosystem partners. Such layered technology architecture has advanced the division of labor and significantly accelerated technical and business innovation.
Alibaba Cloud believes that cloud native technology can support Internet-scale applications, accelerate innovation, allow low-cost trial and error, and avoid differences and complexity of underlying infrastructure. In addition, new computing approaches, such as service grids and serverless computing, make the entire IT architecture extremely flexible so that applications can better serve business purposes. You can build domain-oriented cloud native frameworks based on Alibaba Cloud Container Service, for example, Kubeflow for machine learning and Knative for Serverless Runtime.
Building off discussions during the SOFAStack Workshop at KubeCon China 2019, this article details how you can quickly build a cloud native e-commerce platform.
This article is based on the content of the SOFAStack Cloud Native Workshop event in KubeCon China 2019. This article will first go into the details of how cloud native emerged as a new and powerful alternative to a traditional centralized architecture, and then we will discuss how Ant Financial switched to cloud native for their architecture solution, Last, we will look at how you can quickly build a cloud native e-commerce platform like the one demoed at the SOFAStack Cloud Native Workshop at KubeCon.
On June 25, 2019, at KubeCon China 2019, Cloud Native Computing Foundation (CNCF), a world-renowned open-source organization, announced that Alibaba’s Ant Financial has now become an official CNCF gold member, being at the second top tier. On hearing the news, representatives from Ant Financial said that they would continue to increase support for open-source projects, including Kubernetes, Service Mesh, Serverless, and secure containers, and would exert their own strength.
At this conference, Ant Financial developers also worked to build a Cloud Native e-commerce platform with hundreds of Cloud Native enthusiasts in just five hours. So you may be wondering how did they do it so quickly anyway?
Websoft9 WAMPServer stack is a pre-configured, ready to run image for running PHP application on Alibaba Cloud.WampServer is a Windows web development environment. It allows you to create web applications with Apache2, PHP and a MySQL database.
Websoft9 WordPress is a pre-configured, ready to run image for running WordPress on Alibaba Cloud.WordPress is the best open-source content management system,there 28% of the web uses WordPress, from hobby blogs to the biggest news sites online in the world.
Disks are block-level data storage products provided by Alibaba Cloud for ECS. They feature low latency and high performance, durability, and reliability. Disks use a distributed triplicate mechanism to ensure 99.9999999% data reliability for ECS instances. If service disruptions occur (for example, due to hardware failure) within a zone, data in that zone is copied to an unaffected disk in another zone to help ensure data availability.
Container Registry allows you to manage images throughout the image lifecycle. It provides secure image management, stable image build creation across global regions, and easy image permission management.
A fully-managed service compatible with Kubernetes to help users focus on their applications rather than managing container infrastructure
Elastic Container Instance (ECI) is an agile and secure serverless container instance service. You can easily run containers without managing servers.
This course will introduce Alibaba Cloud’s ultra-intelligent AI Platform for solving complex business and social problems. Powered by new advanced technologies, Alibaba AI technology is powering global breakthroughs in artificial intelligence and machine learning.
This course introduces the basic concepts and features of docker container technology, then showes the steps of basic installation and some frequently used commands. At the same time introduces the the basic concepts and architecture of container service on alibaba cloud, and then shows the steps of container based application creation procedure from scratch.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-upgrades-its-cloud-native-partner-program-731849e66b86?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 22, 2020·4 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Ning Xiaomin from Alibaba Developer
In the past few years, the concept of “Cloud-Native” has become increasingly known to customers and partners. Cloud-native technologies, cloud-native products, and cloud-native architectures are being gradually defined. At the Apsara Conference 2020, Ning Xiaomin, a Senior Solution Architect at Alibaba Cloud, introduced the concept and practices of cloud-native and the release of Cloud-Native Industry Alliance (CNIA).
As for cloud-native architectures, we need to make the most of cloud capabilities. Cloud-native architectures are developed based on cloud-native services, such as containers, microservices, Service Mesh, and Serverless. Alibaba was one of the first enterprises exploring cloud-native technology. As early as 2007–2008, Alibaba’s e-commerce system transitioned from the original IBM Oracle EMC (IOE) architecture to the distributed architecture in a service-oriented manner, and open-source through Dubbo. In 2015, it was made commercially available through Alibaba’s middlewares.
In 2011, Alibaba started to explore and research containers and open-source through Pouch. In 2016, Alibaba provided container services for users. In 2017, Alibaba also began to commercialize Service Mesh and Serverless. Alibaba has been deeply engaged in the cloud-native community for decades. Through exploration and practice, Alibaba has accumulated considerable experience and excellent solutions. The mature technologies and products help enterprises seize opportunities in the wave of digital transformation.
At the same time, Alibaba has made remarkable achievements in the cloud-native community. In terms of technical standards, we and Microsoft jointly launched Open Application Mode (OAM), which defines cloud-native application standards and architecture modeling. In terms of commercialization, Alibaba Cloud is the only vendor in China that was listed in Gartner’s Competitive Landscape of Public Cloud Container Service Market in China for two consecutive years. With Alibaba Cloud’s exploration and practice in the cloud-native community and the development and maturity of cloud-native technologies, Alibaba Cloud has become a definer and leader in the cloud-native community.
For the future of cloud-native technology, we believe that it has become a new interface for cloud computing. Cloud-native technology can reshape the entire lifecycle of software development and promote the transformation and upgrading of the information industry. Therefore, we believe that cloud-native is the shortest path to realize the benefits of cloud computing technology. At this conference, Alibaba Cloud officially launched CNIA. The Middleware Partner Program was fully upgraded to the Cloud-Native Partner Program of Alibaba Cloud. In the new program, we have made great upgrades in the product range, cooperation mode, cooperation rights, and interests. We will focus on supporting and empowering 100 Independent Software Vendors (ISVs), 10,000 partners, and 500,000 developers. Alibaba Cloud helps millions of customers achieve digital transformation.
The Cloud-Native Partner Program of Alibaba Cloud is driven by Solution Partners and integrates Distribution Partners and Managed Service Partners (MSPs). It helps these partners improve their sales, product, solution, and service capabilities. Adhering to the positioning of cloud-native infrastructure, practicing skills, and being integrated, we help partners build core competitiveness and jointly serve enterprises for digital transformation.
Presently, the Cloud-Native Partner Program has expanded to 40 listed leading software companies in the industry, building more than 200 joint solutions and serving more than 20,000 customers. Leading partners include Yilian Tongda, Serveyou, Nanrui, SuperMap, Terminus, Sunyur, and Chanjet. Alibaba Cloud has been pursuing “openness, win-win cooperation, and being integrated”. This year, Jeff Zhang, President of Alibaba Cloud Intelligence, announced the company’s strategy of “ecosystem”. This year, two billion CNY special funds will be invested for the joint research and development of product technology, the construction of the ecosystem service system, and the cultivation of core ecosystem partners. We will help 50 partners achieve cloud-based revenue of over 100 million CNY. Here, we would like to invite all of our partners to join the Cloud-Native Partner Program to release more technical benefits through cloud-native and jointly empower all industries to enter the era of digital native!
alibaba-cloud.medium.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TheDigitalTP/becoming-a-cloud-native-business-the-digital-transformation-people-3bc7e884343d?source=search_post---------314,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Digital Transformation People
Jul 20, 2018·4 min read
The progress of technology has forced many businesses to improve just so they can keep up. Robots and automation are driving some of this pace, but there are far more sophisticated options only the Internet and similar technology can provide. Businesses are moving their entire structures to the cloud to become more high-tech and speed up their manufacturing. Becoming a cloud-native company is not an immediate or easy task, but the long-term rewards are worth the price.
Many consumers already do most of their business online anyway. Becoming cloud-native shouldn’t be a company’s first step into online business, but rather the goal.
What Is Cloud-Native?
A cloud-native business is one doing all their operations in the cloud, meaning resilience, instant communication and a much larger customer base. Cloud-native is more of a corporate strategy than a business model. Operating in the cloud means allowing infrastructure as a service and having a solid IT department.
Every company has different needs to meet. A business can change the structure of the cloud around as they see fit to maximize their goals. Obtaining the right structure from the cloud takes thought and care, which is where IT comes in.
DevOps
Development and operations are two separate groups in a company. They follow two different paths, which eventually converge to provide the delivery of software services to production. In a cloud-native business, everything is about speed. Because of the high demand to move faster, development and operations have to merge into one team to meet goals.
DevOps is the combination of development and operations necessary in a cloud-native business. Investment into DevOps is required for success, which companies can achieve through team collaboration and automation. Automation, in this case, means to deliver, operate, monitor and manage software automatically so the DevOps team can do their best job.
Time Is Money
Being cloud-native is all about the speed of the job. Agility is the most significant selling point for becoming cloud-native and puts the company ahead of its competition. Getting a service or product from the drawing board into a customer’s hands with no problems in record time is the dream.
Accelerating time to market means the company creates a wider profit margin and earns more revenue. Monetizing APIs on apps deployed and developed faster than ever before also means reduced operating costs. At the same time, customers will come to expect quick service and will react badly if the system ever becomes unexpectedly unavailable. Great power comes with great responsibility.
Better Customer Experience
The customer service a cloud-native business provides is just as much of a benefit as the agility. Employees and customers alike enjoy streamlined communication. And, because transparency is much easier to implement through the cloud, companies can keep track of how their business is going and where they are spending money.
Cloud-based solutions dramatically shorten response time to problems, complaints and questions. While a social media presence is almost necessary for the modern day, people most often want to go through official channels to speak to the business directly. Customer feedback is critical for a business to stay afloat.
Stay Ahead of the Game
Cloud-native business is a new way to promote healthy competition in the business world. Most businesses are looking to the cloud to get ahead of their peers, but a lot of businesses are switching or have already switched to the cloud. A competitive atmosphere increases technological advancement and better customer experiences.
Big companies like Netflix, Amazon and Google were some of the first businesses to become cloud-native. Strategic decisions by CEOs have higher expectations because of the stiff competition. Power might be hard to grab for even this early into cloud-native business’ conception, but the sky is the limit after they meet their goals.
The Setbacks
While cloud-native businesses seem like the best idea for the future of commerce, some companies fail to make the switch. When asked about barriers to the transition, 70 percent of CEOs cited a lack of skills, cultural issues, technology and cost for the biggest drawbacks.
Becoming cloud-native might benefit most businesses, but not every company has the resources to make it a reality. For instance, IT is an important cost center for making the transition, and CIOs need more power to get the job done. Not every company is in a good position to make those kinds of changes.
Advancement Means Opportunity
While not every company is in their best position to become cloud-native, switching to the cloud is good for business as a whole. Stronger competition combined with better customer experiences makes commerce better for all sides, not to mention the higher revenue a company receives. If the resources are there and the employees are up to the task, going cloud-native is the best option available. Not doing so may mean the end of some businesses very soon.
Originally published at www.thedigitaltransformationpeople.com on July 20, 2018.
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
"
https://medium.com/@alibaba-cloud/insight-into-the-future-of-cloud-native-cncf-x-alibaba-cloud-native-technology-course-b53c0d9832c2?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 2, 2019·4 min read
As the cloud native technology stack represented by Kubernetes is becoming increasingly mature and the CNCF ecosystem is growing gradually, “Cloud Native” has become a keyword in the future cloud computing era. However, what exactly is “Cloud Native”? What is the relationship between cloud native, CNCF, and Kubernetes? Since the technology wave of cloud native has already arrived, how can we, as cloud computing developers and practitioners, implement cloud native technologies within our own organizations and gain a foothold in the cloud native technological revolution?
As witnesses and practitioners of the “cloud native” technical community for many years, we know that it is not easy to have every developer understand and adopt a new technology. For this reason, we have jointly developed the CNCF x Alibaba Cloud Native Technology Open Course with Cloud Native Computing Foundation (CNCF). We aim to answer every technical question about “cloud native” for each developer and reveal the thinking and essence behind the cloud computing revolution.
Over the years, CNCF has been dedicated to making cloud native computing universal and sustainable to global developers. The launch of the CNCF x Alibaba Cloud Native Technology Open Course is the first cloud native course tailored by CNCF exclusively for Chinese developers. You can learn from world-class technical experts to fully understand cloud native technologies. We firmly believe that the solution to making cloud native available to everyone relies only on a deep understanding of every design and thinking behind “cloud native”.
CNCF x Alibaba Cloud Native Technology Open Course is a new course jointly launched by CNCF and Alibaba. It is a series of technical open courses that focus on the cloud native technology stack and feature both technical interpretation and practice implementation. We aim to help every cloud computing developer and engineer to grasp the nature and essence of the cloud native era.
This course is aimed at beginners and intermediate-level audiences, including:
Xiang Li, Senior Staff Engineer at Alibaba Cloud, is responsible for improving software infrastructure. He is one of the nine Technical Oversight Committee (TOC) representatives for the CNCF. Prior to Alibaba Cloud, he led the distributed project team at CoreOS, responsible for the development of distributed systems related projects such as Kubernetes and etcd in CoreOS. His main interests are consistency protocols in distributed systems, distributed storage, and distributed system scheduling, and he is the author of etcd. He also witnessed and experienced the complete development process of CoreOS, from being a Y Combinator alumnus to being acquired by RedHat.
Lei Zhang, Staff Engineer at Alibaba Cloud, is a senior member and co-maintainer of the Kubernetes project. Lei focuses primarily on CRI, scheduling, resource management, and hypervisor based container runtimes. Lei is co-leading the engineering efforts on Kubernetes upstream and large-scale cluster management systems of Alibaba Group. Lei was once a visiting researcher at Microsoft Research (MSR) and a member of the KataContainers team.
Hongliang Sun, Senior Engineer at Alibaba Cloud, is responsible for the construction of the Alibaba container technology PouchContainer. He has been engaged in the field of cloud computing for several years. He is among the first batch of engineers in China who conducted research and practices on container technology, and has played an important role in spreading the mainstream container technology in China. He has published a book titled Docker Source Code Analysis, and is enthusiastic about open source.
More experienced lecturers will be participating in the near future.
Principles and Essentials of Cloud Native Tech Stack
1. Container Fundamentals (25%)
Container core concepts
2.Kubernetes Fundamentals (20%)
3. Container Advanced Concepts (5%)
4. Kubernetes Advanced Concepts (45%)
We will add more lectures including Serverless and ServiceMesh in the near future. If you intersted in the course please click here
Reference:https://www.alibabacloud.com/blog/insight-into-the-future-of-cloud-native-cncf-x-alibaba-cloud-native-technology-course_594623?spm=a2c41.12714855.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/a-new-chapter-in-modernizing-security-with-cloud-native-technologies-identity-management-8a995f550c28?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 13, 2020·8 min read
By kirazhou
The cybersecurity event RSAC2020 has wrapped up in San Francisco. In Hangzhou, more than 10,000 kilometers away, Xiao Li spoke about the theme of this year’s conference, the Human Element. 2020’s conference kicked off with a focus on the Human Element of cybersecurity. What effect does this have on China’s cybersecurity market and what makes empowering the Human Element essential?
Gartner’s 2020 Planning Guide for Identity and Access Management states that IT must advance IAM (Identity and Access Management) initiatives, which necessitates the trends of identity governance and management, and gives hybrid- and multi-cloud environments more momentum.
This interview deciphers the relationship between the human element, identity, and the cloud, and sheds light on the infinite possibilities among them.
We often say that the essence of safety lies in the confrontation between people. The human factor makes the offense-defense confrontation a dynamic and enduring process. During this time, the means, tools, and strategies of attackers are changing while the protection capabilities of defenders are improving. The ongoing conflict between the two systems keeps the security level fluctuating.
Throughout the offense–defense confrontation, humans are both defenders and attackers. In many cases, confrontation occurs not only between an enterprise and the outside world but also within an enterprise.
Humans are the absolute core of security. This was the principal message of this year’s RSA conference. While focusing on building the cybersecurity capacity and enhancing skill for humans, it is also necessary to note that humans’ vulnerabilities make themselves a weak link in security. While enterprises deal with external attacks, learning how to prevent threats from insiders is equally critical.
According to a Kaspersky Lab report announced in 2017, 46% of IT security incidents are caused by employees. This proportion has now risen to 70% to 80%. The escalation could be caused by an increase in cases such as internal developers failing to abide by the security regulations or lacking security skills and therefore leaving security flaws in applications, or the security threats posed by non-standard operations or malicious behaviors of current and ex-employees.
“The security system is definitely not just about safeguarding against network worms. They are only the tip of the iceberg.”
Facing the security implications brought by the “human factor”, Xiao Li believes that the root cause of the problem lies in an inadequately disciplined security baseline. Many enterprises are emphasizing threat detection and response. Indeed, this is useful, but it is not enough. “What demands our attention should not be how to solve problems, but how to take precautions and prevent the problems from happening.” Therefore, defining security baseline policies proactively is more critical than subsequent detection and response. Security baseline policies for enterprises include:
Developing and implementing a security baseline discipline is only going halfway. To create an enhanced security infrastructure, enterprises must also improve their threat detection and response capabilities. Since “identity” is the intuitive persona on the Internet, identity management plays a vital role in reducing the risk of internal security breaches.
The year 2010 marks a turning point in the formulation of corporate security strategies, where identity gained significance to become the paramount security concern.
Xiao Li said, in the past, especially from 2000 to 2010, boundary isolation was the primary means of enterprise security protection, but after 2010, this situation changed dramatically.
The traditional boundary will gradually disappear as the IT environment keeps evolving. As a result, security can no longer be guaranteed by adopting network isolation alone. Zero trust addresses this modern security challenge and reestablishes trust boundaries around corporations with unified identity management controls.
The zero trust approach allows enterprises to build centralized authentication and authorization systems to manage accounts, authentication, and permissions. For example, zero trust protects corporations against the security vulnerabilities that could result from employees’ departure. Enterprises must implement security measures that ensure one-click permission updates for employees that are transferred or leave the organization so that the system permissions of former employees can be promptly deleted from internal systems.
Some security risks are completely avoidable if proper identity authentication and management measures are taken. An example of this is the recent Weimob incident where an employee deliberately sabotaged the company’s production environment and database. Xiao Li shared his thoughts on this issue:
Technical implementation and baseline security standards are important but the most important things are identity authentication and management. According to Xiao Li, the level of authority and influence of the security team in a company determines whether a baseline can be established and implemented effectively. This answer can be found by checking the organizational structure of the company to see whether the security team is independent and reports directly to the CTO or the CEO.
In the future, IAM will become a prominent pillar of organizations’ zero-trust strategies and will play out in several identity management scenarios. “Identity authentication” and cloud resources will be mutually reinforcing and be used together to build a cloud-based zero-trust system.
Mark McClain, CEO and co-founder of the identity management company SailPoint, once said, “The world of governance is about who has access to what, who should have access to what, and whether they are using it correctly. Most customers are so far away from the first two, they should not even worry about the third yet.” Fortunately, IAM tools and services are becoming easier to use and are designed to deploy seamlessly in the cloud.
Xiao Li said that “cloud-native services feature outstanding advantages when security is concerned.” The cloud has almost become an enterprise operating system that provides IaaS, PaaS, and SaaS services. Cloud service providers invest huge amounts of money, manpower, and material resources in developing cloud security products and technologies, and bring benefits of cloud-native security to enterprises.
Alibaba Cloud, like other cloud service providers, facilitates its customers’ journey to the cloud with maximum security, allowing them to not have to manage the underlying cloud infrastructure themselves.
Furthermore, cloud-native security delivers an integrated set of capabilities: comprehensive cybersecurity controls and network isolation measures, real-time and intelligence-driven monitoring with automatic incident response, cloud-based and centralized solutions for identity management and authentication, reliable hardware infrastructure for building a trusted environment, and the DevSecOps philosophy that injects security into the software development lifecycle. Cloud computing allows organizations to choose the right compute options for security management by adopting a “unified” security model instead of stitching together “fragmented” solutions.
Many organizations are embracing the cloud to run their applications, which means that cloud-based IT infrastructure and Internet-based core technologies will ultimately transform enterprise architectures. As enterprises migrate to the cloud, they start to explore how IAM can be strengthened in hybrid- and multi-cloud ecosystems.
To help simplify what could become a complex hybrid environment for businesses, managing identities across the hybrid cloud consolidates the multiple identities employees are creating for both on-premises and cloud networks. Permissions are assigned dynamically based on the analysis of cloud infrastructure, which enables employees to access internal resources anywhere and anytime. Multi-cloud allows the use of Active Directory to service the load for identity management.
Cloud-based IAM solutions offer a series of capabilities that would be much harder for on-premises solutions to accomplish. IAM solutions across the hybrid- and multi-cloud environments will become the new strategic pivot for enterprises.
Finally, the data security issues that come with the adoption of IAM solutions also deserve attention. Data security led to the hottest topic of discussion in 2019. People experienced frequent leaks that affected hundreds of millions of data entries and witnessed several data privacy laws and regulations being introduced. One thing is being addressed for sure, the importance of data security.
At the end of the interview, Xiao Li also talked about Securiti.ai, the winner of the RSA Conference 2020 Innovation Sandbox Contest. Interestingly, two of the past three winners of the Innovation Sandbox Contest were recognized for their prowess in data security. This seems to have raised market attention in the next direction of cybersecurity defense.
First, data security itself poses a major challenge. The mobility of data makes data security critical among all domains of cybersecurity and presents risks in all aspects of business. Second, the market demand for data security solutions is huge. Enterprises have an urgent need for ensuring the security of internal data and customers’ data. “Perhaps next year’s winner will also specialize in data security,” Xiao Li joked.
In the next 5 to 10 years, if security companies can develop core technologies or make breakthroughs to rid users of data security threats, such as providing visibility into the location and identification of sensitive data, they will win substantial market share from other players.
Xiao Li remarked that market demand is now stimulating technological activity. There is a pressing need for technological advances in the field of data security.
Xiao Li is a Vice President of Alibaba Group, the General Manager of Alibaba Cloud Intelligent Security Division, and the first security engineer of Alibaba Group. He has been deeply engaged in the construction of enterprise security architecture and cloud computing security and has nearly 20 years of technical and managerial experience.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/sofastack-building-a-financial-level-cloud-native-architecture-6c687ebb304d?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 11, 2021·16 min read
By Zhu Sansong
I recently came across an interesting quote that sounds counterintuitive at first but will start to make sense if you give more thought to it. It goes something like, “progress is driven by lazy people.”
Scientific technologies continuously evolve because “lazy people” always seek for easier ways to do things. To save our energy from traveling, we created the steam engine. To prevent manual calculation, we created the electronic computer. In order not to carry cash, we invented online transactions and contactless payment. Data and information technology have become the foundation of this era, and innovative services and applications are all around us.
Today, we acquire almost everything from the Internet, and massive volumes of data and code are flowing in the cyberspace. Developers who built codes are starting to think about packaging some common code modules to the upper layers for them to use at any time. By doing this, they do not need to code repeatedly and can turn typing code into a modular job.
This laziness has motivated a new concept more commonly known as “middleware”.
Most people are unfamiliar with “middleware”. Technically, middleware is a special software between the infrastructure and business system. Programmers have designed a variety of metaphors for the middleware. Some people said that it is a “prefabricated part” at a construction site, so that workers do not have to stir cement from the scratch. Meanwhile, some other people said that it is a “middleman” integrating the source of goods, so that sellers are free from repetitive price inquiry and comparison.
“There are lots of communication and integration work between the infrastructure and business system. Enterprises never want to do this with extra human labor for every business system,” said Ma Zhenxiong, a senior product expert at Ant Group. “This is a common requirement for all enterprises.”
To meet this requirement, Ant Group proposed Scalable Open Financial Architecture Stack (SOFAStack).
SOFAStack came out quietly, with the original intention of “rescuing” Alipay. At that time, Alipay did not provide Ant Forest, Ant Credit Pay, or Health Code. It was a simple application that ran on an application server, used a database, and served Taobao. SOFAStack is a collection of cloud native middleware components, which are designed to build distributed systems with high performance and reliability, and have been fully validated by mission-critical financial business scenarios.
Featuring simplicity, ease-of-use, and convenience, SOFAStack supported the development of Alipay from 2004 to 2006. However, with the increase of the transaction volume and the complexity of businesses, Alipay encountered pain points for growth.
“The technical team was expanded from dozens of people to hundreds of people and finally thousands of. In different scales, the R&D and organization methods were varied.” Huang Ting, a senior technical expert at Ant Group, said, “As the number of people increases, code written by different developers varies and conflicts increase.”
In a word, the R&D efficiency was affected.
If Alipay was used to be a bungalow, it now has to develop into a city. To build each building in the city, workers have to burn bricks and stir cement from the scratch without excavators or hydraulic hammers. This is unacceptable for the team who is responsible for “building the city”.
For example, it takes 5 or 6 minutes to start iWallet each time, an electronic wallet system of Alipay. If a bug is found, iWallet has to be relaunched after modification. As a result, developers are trapped in the “endless loop” of code compilation and relaunching every day.
This issue lies in that the iWallet system contains dozens of projects developed by over 10 teams concurrently. In fact, the original Alipay system cannot support this complex business logic or allow so many engineers to work concurrently. Therefore, developers call iWallet a monolithic system.
Alipay has the following requirements. First, hundreds of projects run concurrently, and engineers can work without interference. Second, when the complexity of business logic increases, the system complexity does not increase exponentially.
To meet these requirements, Alipay needs a set of “middleware”.
In 2006, the opportunity came. The technical team held a series of meetings with the only core subject of Alipay’s future technical architecture. Team members proposed two ideas. One was the centralized architecture, like banks. The other was the distributed architecture, which was not the small-scale architecture in client/server mode but the ultra-large-scale distributed architecture in the Internet era.
The second idea has never been explored before.
However, Alibaba employees have never been retreated or hidebound. After about a year of thinking and argumentation, the technical team decisively chose the second idea. Since 2007, Alipay started to reconstruct its transaction system, merchant system, membership system, and payment and settlement system to conceive a new architecture.
This distributed architecture is called “service-oriented fabric architecture (SOFA)”.
It is called SOFA because At that time, service-oriented architecture (SOA) was popular. By incorporating financial businesses into SOA, SOFA is generated.
Second, it is spelled the same as the word sofa. As its name implied, developers hope that engineers can work comfortably with SOFA.
What is SOA? Technically, an enterprise’s IT system is reorganized in the unit of “services”. Then, these services are connected through the “service bus” to form a pluggable enterprise IT architecture. This architecture is SOA.
Some of you may find that this definition is difficult to understand. Fret not. At that time, SOA was simply an idea for the traditional enterprise IT architecture, that is, a theoretical framework. There were no specific successful practices of SOA in the industry.
At the beginning, Ant Group pioneers were cautious. The first-generation SOFA only solved two problems. First, it functioned as the glue or connector to interconnect distributed systems as a whole. Second, it allowed each service to be component-based. In other words, engineers only needed to focus on their own components. Finally, the components were assembled to services, and the services were assembled to a complete system.
Huang Ting said, “SOFA can isolate different modules to be developed by different developers. So, everyone has a more detailed division of labor and will not have too many intersections with others.”
The first-generation SOFA clearly defined the boundaries between teams, including labor division and collaboration. To demonstrate this, Huang Ting gave an example. For a simple bank transfer, the system needs to call the transferer’s address book, while accounting-related subsystems may have to inquiry the bank whether the account balance is sufficient. The whole process involves complex system interactions. To ensure efficient interactions between these systems that are developed and maintained by different teams to complete a transaction, SOFA is required.
However, the new SOFA distributed middleware cannot deal with all problems. It needs to be continuously iterated.
On this unexplored road, there are no pioneers but growing technical issues.
With SOFA, Alipay splits the financial business system (business mid end) and underlying IT system (data mid end and computing mid end). In addition, it has to cope with massive volumes of data during Double 11 and various emerging technical issues. The two SOA transaction standards provided by the industry cannot support the transaction volumes of Alipay core systems. Therefore, the Alipay team planned to develop their own standard to ensure distributed service consistency.
To produce several lines of SOFA code, SOFA had encountered countless similar obstacles over these years and accumulated many ideas and technical practices.
It pioneered new technologies to light up new areas.
The first-generation SOFA was modular.
The second-generation SOFA was service-based.
The third-generation SOFA was unitized, which was called a cutting edge technology of Ant Group. The active geo-redundancy architecture made it easier to scale out server resources and ensured the stable and smooth processing of each user order. The SOFA team confessed that the unitized architecture was actually forced by the businesses for distributed reconstruction of ultra-large-scale Internet financial transactions. There was no precedent in the industry.
“We did review some papers and concepts. However, no one was sure that unitized architecture can work for the large-size Alipay,” said SOFA team members.
SOFA continuously iterates and grows with the optimization of the Alipay architecture. At the beginning, it was just a simple framework. Later, it strengthened communication performance, improved disaster recovery efficiency, built geo-disaster recovery architectures, performed unitized reconstruction, and introduced LDC logical data center projects. As a result, SOFA gradually became mature with more and more technical tools. It was no longer a middleware but a tool library.
So far, SOFA has completed its first evolution. Its full name is also changed to Scalable Open Financial Architecture, dedicated to building the architecture for financial-level systems. Some developers also append Stack, which means a suite, to SOFA.
From this new name, we can easily see the developers’ visions and expectations.
In “SOFAStack Financial Distributed Architecture White Paper” released in 2020, Ant Group defined SOFAStack as a technology stack for constructing financial-level cloud-native distributed applications.
Over the past years, SOFAStack has withstood the tests of many big promotion activities and supported the development of Ant Group’s all-round businesses. It has become a star product of Ant Group. As the distributed architecture gradually enters the public, the middleware market develops rapidly.
Some team members propose to launch SOFAStack to the market.
With the agreement of most team members, SOFAStack goes to the market.
The market competition is fierce.
Before SOFAStack was launched to the market, traditional enterprises still use the centralized architecture for their core systems, especially the well-known IBM, Oracle, EMC (IOE) architecture. Specifically, IBM who provided minicomputers with powerful computing capabilities, EMC who offered expensive high-end storage devices, and Oracle who provided databases are the three core components of the centralized architecture. However, the running of a large amount of service logic depends on J2EE containers or the CISC transaction middleware.
Under the prosperity, the cornerstone of the centralized architecture is unstable. Even though IBM standalone servers have good performance, standalone core applications based on the server system can no longer support high concurrency after a large number of financial institutions transform to digitization and actively develop online businesses.
To solve the problem, we need scale-out.
However, it is expensive to upgrade the server configuration under the IOE architecture. Not all enterprises can afford this. During Double 11 in 2013, Oracle asked Alibaba to pay expensive extra bills for surged traffic running on its databases.
Fortunately, Alibaba has developed their own database product, OceanBase.
The proprietary OceanBase database helped Alibaba reduce costs. Ant Group found that there was strong support for replacing the IOE architecture in the market and therefore launched SOFAStack.
“Many banks have seen previous achievements and financial innovations of Ant Group.” As the head of the SOFAStack commercialization team, Ma Zhenxiong thinks that the future of SOFAStack is prosperous. “Look, the customers have reached a consensus and recognized the trend. They also want to move to this way.”
At the beginning of 2017, Bank of Nanjing started “dual-mode operations”. It retained the traditional stable core system and built an open and flexible agile core system. In April 2017, multiple Ant Group teams, including Platform Architecture Department, Financial Core Platform Department, Technology Risk Department, and Micro-Loan Business Department comprehensively diagnosed Bank of Nanjing.
Each team had tried the best because this was the first customer. SOFAStack showed all its features. This was the first showcase in its life.
In July 2017, Ant Group assigned a technical team to Bank of Nanjing. They were responsible for the roadmap and top-layer architecture design for distributed architecture transformation to prevent the customer from taking detours at the beginning. In October 2017, Bank of Nanjing released its own Internet financial open platform named “Xinyun+” at the Yunqi Conference.
On November 18, “Xinyun+” was officially implemented.
After the first project, SOFAStack accumulated experience during commercialization and quickly and flexibly made adjustments to cope with customers’ feedback and requirements. In general cases, it takes a long time to respond to customers’ requirements. Customers’ requirements are first reported to the delivery department and after-sales O&M department. Then, the O&M department analyzes the requirements and submits the core requirements to the product team. Next, the product team arranges for production, asks the technical team to implement, and finally releases a new version and asks the after-sales team to maintain the version.
However, Ant Group assigned a joint team to Bank of Nanjing, involving product, technology, business, after-sales, delivery, and O&M. When any bugs or product requirements occurred, the project team could quickly solve them. The project team could even release six product versions in one day. This Internet-style quick iteration mode made the traditional financial industry stunned.
After being polished and experienced on the road of commercialization and productization, the fourth-generation SOFAStack emerged.
After the Bank of Nanjing project, the financial-level cloud-native architecture solution provided by SOFAStack was widely recognized by the industry. More and more financial institutions that wanted to get rid of the IOE architecture hoped to cooperate with Ant Group.
The market is violently stirred, and the “new specie” is undergoing metamorphosis.
Today, SOFAStack has obtained an increasing number of customers, including well-known large institutions and small-sized enterprises with unique visions. SOFAStack has undergone smooth transition but also encountered a lot of function adaptation issues. Ma Zhenxiong said that sometimes after the team deployed the platform and entered the development or testing phase, the customer might have dozens of questions for a product in a day.
When I asked him whether he was discouraged for this,
Ma Zhenxiong said with a smile that the team was both painful and happy most of the time.
On the one hand, it was painful because the Ant Group star product was overwhelmed by coming problems. On the other hand, it was happy because customers had high expectation on our team. If customers were not confident about our product at all, the team would feel embarrassed. Ma Zhenxiong added that such customers were valuable. “We are not afraid of noise but no noise.”
In the list of customers, Huarui Bank is a typical case.
Compared with joint-stock banks and urban commercial banks with more than 100 billion assets or Bank of Nanjing with trillions of assets, Huarui Bank with an asset of more than 30 billion is nothing but a minor customer.
However, a minor case can pose a major impact. The cooperation between SOFAStack and Huarui Bank was evaluated by Ma Zhenxiong as “the benchmark for private bank businesses”. Before cooperating with Alibaba and Ant Group, Huarui Bank spent almost one year on researching cloud platform building. Huarui Bank does not have offline outlets or counters. Instead, customer acquisition, account opening, and deposit and load businesses are all done online.
In other words, it is an Internet-based bank, same as Ant Group with Internet genes. At the end of 2019, Huarui Bank used the SOFAStack financial-level distributed architecture, mPaaS mobile development platform, and Alibaba Aspara cloud computing system to build its own financial cloud platform “Xiangyun” to support business systems, such as mobile banking, marketing, anti-fraud, and loan accounting.
Many technical tools can be used out-of-the-box, and innovations are continuously provided.
Ye Ning, the general manager of the science and technology department of Huarui Bank, mentioned in an interview that small- and medium-sized banks must know what they must do and what must not do. Since they do not have the technical strengths of large state-owned banks and joint-stock banks, they need to ask financial technology companies to provide assistance.
“Through the cooperation with Alibaba Cloud and Ant Group, we can free ourselves from inefficient work. We do not need to spend time on the construction of standardized software and hardware technologies.” Ye Ning compared this process to “cooking”. Some people like to grow vegetables, raise pigs, and squeeze oil themselves from scratch. This is something that complies with the green and health concepts. However, not every housewife has efforts to undertake these tasks.
“Huarui Bank does not want to be a farmer. We just want to bring over semi-finished products in the supermarket and make dishes that meet our taste.” Ye Ning said.
This analogy coincides with the birth of “middleware” at the beginning. With mixers on the construction site, semi-finished products in the household refrigerator, and modular components available, everyone does not need to waste energy on repetitive and inefficient work.
In the first quarter of 2020, Huarui Bank earned an increase of online customers by 468%, improved the system development speed by more than 30%, and significantly shortened the system environment preparation and resource expansion periods. During the epidemic, upgraded financial-level distributed core systems effectively supported the outbreak of online business volume.
In addition to the bank industry, SOFAStack also proven its capabilities in the insurance industry.
In 2018, Ant Group cooperated with PICC. The solution with technical products such as mPaaS and SOFAStack helped this well-known insurance company successfully eliminate technical bottlenecks and build an industry-leading new-generation core business system.
In just a few months, PICC has improved its insurance policy processing capability by thousands of times, can issue 1,000 orders per second, improved the access efficiency of external channel products by 6 times, shortened the launch time of new products by 80%, and ensured 99.99% availability of platform services. In the past, PICC needs to spend 4 hours processing tens of thousands of daily settlement files. Now, it only requires 6 minutes.
In the insurance industry, SOFAStack develops well.
Ma Zhenxiong said that SOFAStack’s previous mission was to support all businesses of Ant Group, including Yu’E Bao, Insurance, and Zhima Credit. Almost all business requirements in the financial industry are covered.
“SOFAStack natively supports all segmented sectors of the financial industry.” said Ma Zhenxiong. However, this depends on a lot of technologies accumulated in the background.
Ant Group launches SOFAStack from the initial attempt to rapid development and from a pioneer on unexplored roads to a leader of digital transformation. SOFAStack is also used together with the mPaaS mobile development platform and the OceanBase distributed database product.
Other customers, including Shunde Rural Commercial Bank, Shenzhen Rural Commercial Bank, Cathay Century Insurance, and Trust Mutual Life, start to cooperate with Ant Group for SOFAStack. The slogan “distributed architecture is the future” has attracted more and more customers to adopt the distributed architecture, which will bring a new revolution to the era.
Now, SOFAStack has evolved to its fifth generation. The previous simple middleware framework is now a magic box, including SOFABoot, SOFARegistry, MOSN, and SOFARPC. In the open-source community, tens of thousands of members have contributed to these projects and components. SOFAStack is trained and verified in more application scenarios.
When I asked Huang Ting what are the new changes of the five-generation SOFAStack, he said that the major change is “trusted native”. When SOFAStack provides services for a well-known application, users have high requirements for data privacy, security, and reliability. The SOFAStack team has made improvements on breaking technical boundaries and building stable frameworks to ensure a more secure and reliable architecture.
To explain the “trusted native”, we have to first know “cloud native”.
Cloud native is dedicated for cloud applications. With the cloud native method, cloud applications can be quickly and frequently built, released, and deployed with excellent performance in scalability, availability, and portability. Cloud-native technologies have become a development direction of modern cloud computing technologies. More and more enterprises accept and use cloud-native technologies.
Since 2018, Ant Group has completely turned to cloud-native technologies. As the carrier of core cloud-native technologies, SOFAStack is also undergoing dramatic changes.
In some technical fields, SOFAStack is already at the forefront of the industry. The most well-known field is Service Mesh. Based on open-source community projects, SOFAStack developed its own component SOFAMosn. This component was independently operated later and upgraded to the MOSN brand. In 2019, MOSN was used to detect the traffic of Alipay core links during Double 11 and was one of the largest service mesh clusters in the world.
The wave of innovation is surging up. However, the “trendsetter” is not showing up.
Cloud-native technologies have a huge impact on the original technical architecture. Most business personnel and customers hold a wait-and-see attitude because they distrust emerging technologies. In recent years, financial institutions have only used cloud-native technologies for new businesses, and few of them dare to implement cloud-native technologies in the core transaction system.
If customers’ distrust of emerging technologies remains, it is difficult for SOFAStack to develop further. After a long period of thinking and practice, Ant Group puts forward “trusted native”, whose essence is to make cloud native trustworthy.
However, to make cloud native trustworthy, a long technical link is involved. Both businesses and users have high requirements for security, stability, and trustworthiness. This cannot be done by strengthening some technical points. Instead, we must ensure that the whole system from hardware to application, the whole application lifecycle from development, deployment, upgrade, to deprecation, and users’ access from the mobile client to the core database are all trustworthy.
As a practitioner of trusted native, SOFAStack is seeking for a more magnificent transformation.
In reliability, SOFAStack has withstood practical inspection of Double 11 and active geo-redundancy over the past years. In secure production, data protection, and other aspects, key technologies in trusted native “security containers” and “confidential computing” have been added to SOFAStack. In the future, SOFAStack will cooperate with academic institutions and industry customers in and outside China to continue to strengthen the construction of trusted native.
New technologies bring both risks and opportunities.
“We can use new technologies to build a safer and more reliable system than before,” said Wang Xu, a senior technical expert at Ant Group. “More importantly, we need to deliver the intangible product of ‘trust’ to users through our technologies.”
Is progress really driven by lazy people? Not necessarily.
But I believe that those who rely on the invention of new technologies to look for easier ways to do things are the world’s most lazy people but also the most intelligent and diligent people.
This is also true for those who made the “middleware”. They turned heavy workloads in the code world to modules to free most programmers and make programming smooth, elegant, and easy. They are lazy because they are unwilling to accept boring and inefficient work. They are hardworking because their efforts are no less than others, and they benefit the industry with their developed tools.
Evolved to today, the distributed architecture is facing fierce competition. To reach the peak of the mountain, SOFAStack still has a lot of issues to tackle.
After all, new technologies continuously emerge in the tide of times.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/future-development-trends-of-cloud-native-cb7cf3c065ff?source=search_post---------318,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 14, 2021·5 min read
By Raghav K.
Monolithic applications and application delivery models cannot withstand the rapid build cycles that current businesses require. Organizations are repeatedly hitting a wall when it comes to delivering value with solutions. Innovation in such scenarios takes a back seat unless the CIO creates new strategies for the cloud. Cloud-native is the future of application development and will help generate more accelerated build and deploy cycles with delivered efficiency.
Serverless computing and containers together are beginning to form a new trend with cloud computing. This integration can add a great deal towards reduced operations and maintenance (O&M) tasks and reduce the complexities associated with Kubernetes. When we talk about reduced O&M, the integration of serverless and containers can reduce security-based maintenance operations, enable a higher degree of fault diagnosis, and provide better observability to the organizations. Serverless containers can be the answer to a secure, scalable, and highly-available computing solution.
The next decade will give birth to more scenarios where different aspects of computing will be applicable. Technologies, such as IoT and AI, are going through the evolutionary cycle and are expected to mature over the next decade. The Open Container Initiative (OCI) provides a standard set of requirements for security and execution.
The Alibaba Cloud Container Service supports the KataContainer-based Alibaba Cloud Kangaroo container engine that can run workloads to ensure multi-tenant data isolation. This is an example of secure containers that offer increased security isolation. Containers that integrate software and hardware might emerge to provide confidential containers. A next-gen lightweight virtual machine called WebAssembly (WASM) holds great promise for application in the IoT, edge computing, and blockchain fields. This portable VM shows that a cross-platform container service can be implemented.
Gartner predicts, “By 2023, the leading cloud service providers will have a distributed ATM-like presence to serve a subset of their services.”
Migrating to the cloud is an essential part of Digital Transformation. Enterprises are rapidly making a further shift towards hybrid cloud or multi-cloud based on which architecture suits the level of data control and security they wish to implement. Multi-cloud also adds easier global expansion due to the regional presence of different cloud providers. For example, Alibaba Cloud is your universal gateway if you wish to set up your business in Mainland China.
Gartner also predicted, “More than 75% of large- and medium-sized enterprises will adopt a multi-cloud or hybrid cloud architecture by 2021.”
This prediction has been proven right (to an extent) since both multi-cloud and hybrid cloud architectures have been rapidly adopted by enterprises. Alibaba Cloud released its hybrid cloud 2.0 model last year, and it showcases extensive capabilities to manage hybrid clouds with Kubernetes.
Alibaba Cloud Container Service for Kubernetes (ACK) helps centrally-manage a Kubernetes cluster. It accounts for security practices, monitoring, management, backup, and recovery scenarios spanning multiple clusters. Alibaba Cloud ACK offers comprehensive monitoring capabilities for on-premises and cloud-based clusters.
When it comes to the security of a solution and maintaining the health of an architecture, you can use the security center and Application High Availability Service (AHAS). On the other hand, Alibaba Cloud Service Mesh (ASM) helps centrally-manage the entirety of services with a combination of multi-region and hybrid cloud networks using Alibaba Cloud Enterprise Network (CEN) and Smart Access Gateway (SAG).
Although there are significant differences between multiple cloud providers and the infrastructure capabilities, Kubernetes is a pure cloud-native solution that can overcome any differences between the infrastructures and work with hybrid cloud or multi-cloud architectures.
Kubernetes has evolved as a multi-cloud management solution for enterprises. It is a cloud-native technology that can support application lifecycle management.
There will be more focus on edge computing. Edge computing will become an important part of the formulation of enterprise cloud strategies. Edge computing lowers the network latency considerably and provides better network bandwidth for the applications to leverage.
Cloud-native offers effective transformation with data services. This accelerated transformation of data services enables a much flexible and robust solution since the computing and storage services are isolated. This is a prime example of the amalgamation of serverless computing and containers that provide a highly elastic solution for enterprises.
Cloud-native operating system abstracts the infrastructure resources just like traditional operating systems. Both of these utilize computing power, storage, network, and other necessary resources to provide a stable platform for other applications to execute. The simplest task of an operating system is delivering services securely and according to the application requirements.
Kubernetes is surfacing as the operating system for cloud-native. Previously, Kubernetes worked with stateless applications based on web frameworks, but the tech stack is evolving, and more stateful applications are being deployed using Kubernetes. These applications include DataWorks (like big data applications) and Machine Learning for AI.
Alibaba Cloud offers a unified technology stack with better resource utilization. Different computing workloads can be managed and scheduled in a centralized manner using Kubernetes.
Gartner said, “By 2023, 70% of AI tasks will run on containers and serverless architectures.”
When working with AI and machine learning, Kubernetes can provide lower latency rates to schedule workloads. This helps organizations extract better compatibility and capability to centrally-schedule multiple workloads. Kubernetes can be deployed in multiple scenarios, including cloud and edge computing nodes. It also provides extreme value to DevOps practices and is an important addition as a tool in its toolchain.
Alibaba Cloud has made technological upgrades to introduce and help enterprises leverage the benefits of cloud-native architecture. The solutions are simple, flexible, and highly-scalable. These solutions represent the highly-technological suitability and availability for the masses.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.datadriveninvestor.com/a-brief-introduction-to-ai-cloud-native-tals-ai-platform-practice-ac8b7c2c9615?source=search_post---------319,"There are currently no responses for this story.
Be the first to respond.
By Liu Dongdong, Technical Director of TAL AI Platform
Tomorrow Advanced Life (TAL) implements flexible resource scheduling through Alibaba Cloud’s cloud-native architecture. Alibaba Cloud supports its AI platform with powerful technologies. This article is a summary of a speech delivered by Liu Dongdong, Technical Director of the TAL AI Platform, at the 2020 Apsara Conference.
In the AI era, enterprises are confronted with greater challenges in the abundance and agility of underlying IT resources. TAL leverages stable and elastic GPU cloud servers, advanced GPU containerized sharing and isolation technologies, and the Kubernetes cluster management platform of Alibaba Cloud. Together with cloud-native architecture, TAL realizes flexible resource scheduling, supporting its AI platform with powerful technologies.
At the 2020 Apsara Conference, Liu Dongdong shared his understanding of AI cloud-native and the AI platform practice of TAL. This article is a summary of his speech.
Hello everyone, I am Liu Dongdong, Technical Director of the TAL AI Platform. Today, I will deliver a speech entitled “A Brief Introduction to TAL AI Cloud-Native.”
My speech is divided into four parts:
In the cloud-native era, the most distinctive feature of AI services lies in better computing resources support and higher stability requirements.
Service has transformed from single services to cluster services. The requirements for performance stability are increasing, improving from 99.9% to 99.999%. These problems are no longer solvable by traditional technical architecture. So, a new technical architecture is expected. That architecture is cloud-native architecture. In terms of changes brought by cloud-native, four key points and two aspects will be introduced.
The four key points are DevOps, continuous delivery, microservices, and containers. The two aspects are service deployment and service governance. There are 12 elements for the overall and systematic summary.
Today’s focus is on service deployment and service governance, but how can we manage service deployment and service governance in the cloud-native era?
On the one hand, based on AI and cloud-native service deployment, which is Kubernetes, the increasing demands of AI services on various hardware resources can be met, coupled with resource virtualization and resource pooling technologies.
On the other hand, AI and cloud-native service governance is combined organically. Service governance technologies, such as service discovery, Horizontal Pod Autoscaler (HPA), and load balancing, are used to meet the requirements for AI service availability of 99.999% of SLA.
Firstly, there is a contradiction between increased hardware demand and cost growth. The demand for the hardware of AI services is increasing, but the hardware budget does not follow the same trend.
Secondly, AI services require diverse hardware types, such as high-performance GPU and CPU, large-capacity memory, and even partially mixed types.
Thirdly, AI services demand resource isolation. Each AI service can use these resources independently, free from any interaction.
Fourthly, AI services demand resource pooling. AI services do not need to perceive the specific configuration of the machine. Once all of the resources are pooled, resource fragmentation can be reduced, and utilization can be improved.
Finally, AI services may request unexpected resources. Since traffic cannot be predicted, enterprises are required to know how to expand the resource pool at any time.
First, Docker virtualization technology is used to isolate resources. GPU sharing technology is used to pool GPU, memory, CPU, and other resources and manage the entire resource in a unified manner.
Finally, Kubernetes resources, such as taints and tolerations, are used to deploy services flexibly.
It’s also recommended to purchase high-configuration machines, which could further reduce fragmentation.
Cluster hardware monitoring is also necessary to make full use of the complex scheduling features of ECS. By doing so, we can cope with traffic peaks. The Cron in the following figure is a time-based scheduling task:
Now, I’d like to explain how the TAL AI platform solves these AI deployment problems.
This page is the service management of a Node. Based on this, the information on each server can be seen, including resource usage and pods and nodes to be deployed.
The next page is the service deployment page in the AI platform. The memory, CPU, and GPU usage of each pod can be precisely controlled by compressing files. Additionally, through technologies, such as taints, the diversified deployment of servers is realized.
According to comparative experiments, cloud-native deployment provides 65% of the reduction in costs compared to deployment by users themselves. Moreover, as AI clusters grow, it will bring more benefits in terms of economic benefits and temporary traffic expansion.
What are microservices? Microservices, as an architectural style, develop single services as a complete set of small services. Each application program operates independently and communicates with each other through lightweight methods, such as HTTP and API.
These services are built around the business and can be centrally-managed through automated deployment. They can also be written in different programming languages and use different storage resources.
What are the characteristics of microservice?
AI services are naturally suitable for microservices. Each microservice is designed for one task. For example, OCR only provides OCR services, and ASR mainly provides ASR services.
In addition, each AI service request is independent. One OCR request is unrelated to another one. AI services naturally demand horizontal scaling because AI services demand vast resources, which makes scaling very necessary. AI services are slightly dependent on each other. OCR service may have few requirements for NLP service or other AI services.
All AI services can provide AI capabilities via declarative HTTP or API. However, not all AI services can be transformed into microservices. So, what are the solutions?
First, AI services need to be stateless. These stateless services are disposable and do not store requests on a disk or memory. Thus, services can be deployed on any node or anywhere.
However, not all services can be stateless. For stateful services, the status of each request will be stored in databases, including configuration center, log center, Redis, MQ, and SQL. At the same time, the high reliability of these components is ensured.
This is the overall architecture diagram of the TAL AI platform. The upper layer is the service interface layer, which provides AI capabilities for the external environment.
The most important part of the platform layer is the service gateway, which is mainly responsible for dynamic routing, traffic control, load balancing, and authentication. Other functions are also demonstrated, such as service discovery and registry, fault tolerance, configuration management, and elastic scaling.
Below is the business layer, which provides AI inference services. In the bottom layer, there are Kubernetes clusters provided by Alibaba Cloud. In short, Kubernetes is responsible for service deployment, and SpringCloud is responsible for service governance in the architecture.
Eureka is taken as the registry to realize service discovery and registration in the distributed system. The configuration center Apollo manages the configuration properties of the server and supports dynamic updates. Gateway isolates the inner and outer layers. Hystrix mainly supports time-sharing blow and number-based blow to prevent service blocking.
Load balancing and Fegin can implement the overall traffic load balancing and consume the Eureka-related registration information. The message bus, Kafka, is an asynchronous processing component. The authentication is carried out via Outh2 and RBAC, which realizes the authentication management of user login, including interfaces, and ensures security and reliability.
Tracing analysis uses Skywalking to track the status of each request for request locating and monitoring with APM architecture. The log system collects logs from the entire cluster in a distributed manner via Filebeat and ES.
Other services are also developed, such as deployment and control services. These services are mainly for communication with Kubernetes and the collection of service deployment information in Kubernetes clusters and hardware information related to Kubernetes.
Based on Prometheus and Monitor, the monitoring system collects hardware data and handles alarms related to resources and services. Data service and data reflow are mainly used for data downloads and data interception in our inference scenarios. Traffic limiting service restricts the requests and Queries Per Second (QPS) of each customer. As the most important part, HPA supports memory-level or CPU-level HPA, some P99, QPS, GPU, and other related rules. The statistics service is mainly used to count the relevant calls, such as requests.
Based on a unified console, all-in-one solutions are available for AI developers that enable them to solve all of the service governance problems on one platform. Thus, the O&M automation is improved. One person can handle more than a dozen AI services now.
This figure shows the configuration pages of service routing, load balancing, and traffic limiting:
This figure shows the interface-level service monitoring and deployment-level hardware monitoring:
This page shows log retrieval and real-time log:
This page shows manual scaling and auto-scaling operations. Auto-scaling includes CPU- and memory-level HPA and HPA based on the corresponding response time and timed HPA.
Now, let me introduce the organic combination of Kubernetes and SpringCloud.
As is shown above, the left part is the diagram from the SpringCloud data center to Route, while the right is from the Kubernetes Service to Pod.
The two diagrams are very similar in structure, but how did we achieve that? Application is bound to Kubernetes Service. In other words, the LB address registered in SpringCloud is converted to the address of the Kubernetes Service. By doing so, Kubernetes and SpringCloud can be combined, making them a route-level combination. Thus, the final effect can be achieved.
SprigCloud is a Java language station, while AI services languages are diversified, including C++, Java, and even PHP. Sidecar technology was introduced to communicate with AI services through RPC for multi-language support and avoid language features. Sidecar has the following functions, application service discovery and registration, route tracing, tracing analysis, and health check.
Thank you very much for listening to my speech!
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@alibaba-cloud/containers-and-cloud-native-technology-realizing-the-value-of-cloud-36bef8e7e697?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 22, 2020·11 min read
By Ding Yu (Shutong), Head of Alibaba Cloud Container Platform
New technologies such as cloud computing, big data, and artificial intelligence (AI) are rapidly changing our world. Their enormous influence has shifted from quantitative changes to qualitative changes. For any enterprise to survive in today’s world, they must adapt through digital transformation. According to IDC, among the world’s 1,000 largest enterprises, 67% have escalated digital transformation to the level of enterprise strategy, and many Chinese enterprises are incorporating digital transformations in their core strategies. As migration to the cloud becomes the trend in the business world, it is time to make full use of open-source technologies and cloud services when constructing software services. For enterprises, embracing cloud computing and cloud-native technology and using this technology to accelerate innovation will become the keys to successful digital transformations.
At the 2020 Alibaba Cloud Online Summit, Ding Yu, a cloud-native application platform researcher with Alibaba Cloud, pointed out that “cloud-native technology, such as containers, has become the shortest path to realize the value of the cloud and empower enterprises to fully embrace cloud computing.” In his view, many enterprises have invested substantial time and energy in digital transformations, but they have taken too many detours. This is because they lack an understanding of and practical experience with cloud-native technology and the support of sound technical solutions and products.
As we all know, traditional development models cannot meet the rapidly changing iteration speed, frequency, and O&M methods needs of the market. However, cloud-native technology is designed to maximize the use of technical models to maximize the productivity of cloud computing. It seeks to fuse the thinking patterns that run through application design, development, delivery, and management with best practices in the shortest time. This is what Ding Yu means by “the shortest path”. Take the container technology as an example. Containers are encapsulated on top of virtualized hardware. As one of the new interfaces for interaction between the cloud platform and customers, the construction, distribution, and delivery of applications have been standardized at the container level. For enterprises, containers can reduce the costs of IT implementation and O&M to improve the efficiency of business innovation.
According to Ding Yu, “One of the core advantages of Alibaba Cloud is that Alibaba’s core services run on the cloud, which is the best breeding ground of innovation. The most advanced technologies will first be tried in Alibaba’s own business systems, where they must prove their versatility and value, before being rolled out on a large scale and made available to customers.” Cloud-native technology has essentially carved out a shortcut for digital transformation, and enterprises should use this as an opportunity to overtake their competitors.
It has been ten years since Alibaba first adopted container technology in 2011 and set out down the path of cloud-native. This technology has endured the test of the Double 11 Shopping Festivals over the past decade. A case in point was in 2015, when the global containerization program achieved rapid and elastic scaling for the shopping festival. The ultra-large scale of the Double 11 event makes it highly complex and a great challenge to the implementation of container technology. For instance, tens of thousands of images must be released and distributed at one time through the container image distribution process. The sheer magnitude of traffic was a major challenge. To meet high demands for efficiency, Alibaba Cloud introduced P2P technology to quickly distribute images on a large scale and achieved cross-IDC image download and container startup within 10 seconds. The significant impact of container technology on Double 11 was also reflected in the implementation of hybrid deployment technologies, which helped the Alibaba Group reduce IT costs by 30%. During Double 11, we reduced the cost of every 10,000 transactions by more than 75%.
In this April, Gartner released its report on the competitive landscape in the public cloud container service market, and Alibaba cloud was once again the only Chinese vendor selected. According to Gartner’s report, Alibaba Cloud Container Service is a strong performer in the Chinese market and offers a wide variety of products under a sound technology development strategy in fields such as serverless containers, service mesh, sandboxed containers, hybrid cloud, and edge cloud. In March 2020, Gartner published its annual survey report Competitive Landscape: Public Cloud Container Services for the second time. The report compares the offerings from various vendors in ten functional dimensions, including serverless Kubernetes, service mesh, and container imaging. Covering nine of these ten features, Alibaba Cloud and AWS outperform Google, Microsoft, IBM, and Oracle in terms of product diversity.
In recent years, container services have been widely accepted by enterprises in various industries. With the most diversified family of container products and services, Alibaba Cloud Container Service has achieved rapid annual growth of more than 400% for several consecutive years. At the 2020 Alibaba Cloud Online Summit, Jiang Jiangwei, a senior researcher from the Alibaba Cloud Intelligent Infrastructure Products Division, announced the release of the cloud-native bare metal instance solution.
The new-generation container service Alibaba Cloud Container Service for Kubernetes (ACK) fully demonstrates the powerful performance of the latest Alibaba Cloud ECS Bare Metal Instances. It features outstanding performance, efficient scheduling, and comprehensive security.
Within Alibaba, the ultra-high performance of Container Service + ECS Bare Metal Instances gave DingTalk the support it needed to cope with its largest traffic peak ever. In the past, DingTalk was completely deployed on ordinary physical machines. Since the outbreak of the COVID-19 epidemic, the demand for online collaboration among government, business, and school users has soared. The elastic deployment of cloud-based ECS Bare Metal Instances and Container Service quickly met the needs of the DingTalk business application by scaling up its capacity to 100,000 cores.
In the market, particularly under the influence of the epidemic, many enterprises need to rapidly scale out their capabilities. For example, in the online education industry, the rapid increase in demand has created both opportunities and challenges for all online education organizations. According to Li Gangjiang, CEO of Baijia Cloud, a full-service video technology provider, the company’s business volume surged by dozens of times over in a short period of time at the height of the outbreak. The difficulty of rapidly resizing in a way imperceptible to its users is no less difficult than delivering a new system.
Fortunately, before it experienced this surge in traffic, Baijia Cloud had optimized its container cluster architecture and planning with the help of the Alibaba Cloud team. By using ACK and ECS Bare Metal Instances developed based on the X-Dragon architecture, the company easily coped with the traffic peak. In contrast, online education companies that had not deployed containers had no choice but to exponentially increase the number of servers when faced with an abrupt surge in users and traffic. This involved time-consuming deployment work, dramatically increased business costs, and undermined the user experience.
Why Choose the Elastic Deployment of ECS Bare Metal Instances + ACK In the case mentioned in the preceding section, Baijia Cloud needed to scale up its capacity dozens of times over, and its Kubernetes clusters had to meet rigorous performance requirements. The ECS Bare Metal + ACK solution was perfectly suited to such a high-traffic and high-concurrency scenario for the following reasons. First, Alibaba Cloud ECS Bare Metal Instance servers have high specifications and can help Baijia Cloud significantly increase the capacity of a single node.
Second, the container-based construction approach can meet the needs of fast and elastic service provisioning. ECS Bare Metal Instance servers eliminate virtualization loss and improve computing performance by 8%. Moreover, their quasi-physical-machine feature supports secondary virtualization. High-performance ECS Bare Metal instance servers and elastic containers work seamlessly together. Data shows that containers that run on ECS Bare Metal Instance servers provide 10% to 15% higher performance than those that run on physical machines. This is because the virtualization overhead is offloaded to the MOC card, and the CPU or memory of ECS Bare Metal Instance servers has zero virtualization overhead. Each container that runs on cloud-based ECS Bare Metal Instance servers has an exclusive Elastic Network Interface (ENI), which improves network throughput by 13%.
Third, ECS Bare Metal Instance servers separate the storage bandwidth from the computing bandwidth, meeting the need for massive reading and writing in Baijia Cloud’s business scenarios. The adoption of an X-Dragon server solution significantly increased computing power. By using high-performance Alibaba Cloud NAS and scaling out the storage to four clusters, Baijia Cloud solved the I/O performance bottleneck.
With the preceding solution and its own large-scale cluster management capabilities, Alibaba Cloud helped Baijia Cloud effectively upgrade the original architecture solution and scale their capacity out dozens of times over in just several days. As a result, Baijia Cloud enjoyed significantly higher cluster performance and stability, allowing it to cope with the traffic peak.
Alibaba Cloud has made extensive investments in cloud-native technologies, including a wide range of container, service mesh, and serverless services. In China, Alibaba Cloud provides the largest cloud-native service family, made the most comprehensive cloud-native contributions to the open-source community, offers the widest range of cloud-native application practices, and possesses the largest cloud-native customer base. Alibaba Cloud’s service family includes more than 20 services in eight categories, ranging from underlying infrastructure and data intelligence to distributed applications. This allows Alibaba Cloud to meet the needs of a variety of industry scenarios.
Alibaba Cloud is the technology company that has made the most comprehensive contribution to the open-source cloud-native community in China. It provides a wide range of services, including orchestration and scheduling, job management, and serverless frameworks, and is the lead maintainer of various prominent CNCF projects, such as etcd, containerd, and Dragonfly. To date, Alibaba Cloud has had ten projects elected into the CNCF Cloud-native Landscape. Last January, Li Xiang, a senior technical expert at Alibaba Cloud, became the first Chinese engineer to be selected as a CNCF TOC representative. He is dedicated to promoting the implementation of cloud-native technologies. This May, Alibaba made OpenYurt, the first cloud-native project for edge computing, open source in a bid to facilitate collaborative community development in the cloud-native and edge computing fields.
Recently, China Open Source Cloud Alliance (COSCL) announced the “Top 10 Excellent Cloud-native Application Cases” at the cloud-native event for OSCAR Open Source Day. STO Express’s generic cloud-native computing platform and China Minsheng Bank’s scenario-based data service mid-end platform, both of which were supported by Alibaba Cloud’s technological solutions, were named among the 10. The four criteria used in the ranking were: digital transformation solutions for traditional industries, cloud-native technologies implemented on a large scale, improvement of resource utilization and R&D efficiency in enterprises, and empowerment of business innovation. These two cases stood out due to their outstanding performance in technical practices involving cloud-native technologies and data service mid-end platforms.
The STO Express solution provides a good example. The original IDC system enabled the company to rapidly develop its business in the early stages, but it also exposed many problems. Under the traditional IOE architecture, non-standard and unreliable system architectures and low development efficiency suppressed business development. After many technical discussions with Alibaba Cloud, STO Express finally chose us as its sole partner to provide a stable computing and data processing platform.
The original architecture adopted by STO Express was developed on the basis of the VMware + Oracle database architecture. By migrating to Alibaba Cloud, the company completed the transformation to a complete cloud-native architecture based on Kubernetes. There are two highlights in this user case.
At present, STO Express’s core business system on the cloud has completely taken over the business traffic and, on a daily basis, processes tens of millions of orders and hundreds of millions of logistics tracking procedures, generates 1 TB of data, and utilizes more than 1,300 compute nodes to process business in real time. In the words of STO Express’s executive in charge of migration to the cloud, “STO Express has been fully transformed by Alibaba Cloud. The cloud-native architecture based on Kubernetes has achieved remarkable results in terms of cost-saving, stability, efficiency, and business enablement. The value created by these cloud-native technologies is the core driving force behind STO Express’s decision to use the public cloud as our main computing resources.”
The goal of container technology is to build or produce a new foundation, new computing power, and a new ecosystem. Alibaba Cloud is dedicated to helping enterprises better implement global application delivery and distributed architectures involving hybrid clouds and cloud-edge integration. According to Gartner, more than 80% of global enterprises will adopt a hybrid cloud architecture. Alibaba Cloud is currently focusing on developing hybrid cloud and integrated cloud-edge solutions. The cloud architectures of the future will be dynamic and hybrid architectures that integrate clouds, edges, and devices. Public cloud capabilities will be extended to edge devices, and computing capabilities and AI will need to reach the edge. In this context, containers provide a consistent way to deploy and deliver applications to clouds, edges, and devices. Based on the innovative software-hardware integration technology native to the cloud, Alibaba Cloud provides powerful computing capabilities to accelerate enterprises’ intelligence upgrades. Alibaba Cloud Container Service makes full use of the performance and elasticity of the X-Dragon architecture and supports the scheduling and sharing of the Hanguang 800 chips, optimizing the efficiency and reducing the costs of deep learning scenarios.
Containers, Kubernetes, and cloud-native technologies are becoming the new technical standards of the cloud era and reshaping the entire software life cycle. With cloud-native solutions, Alibaba Cloud is helping business customers and developers use the cloud to its full potential.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/analyticdb-alibaba-clouds-cloud-native-data-warehouse-broke-the-tpc-ds-world-record-70d32a695e0e?source=search_post---------321,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 17, 2020·17 min read
By Nanxian
On May 4, 2020, Transaction Processing Performance Council (TPC), the world’s most famous non-profit data management system evaluator, officially published the results for AnalyticDB on the TPC-DS on its official website. AnalyticDB passed the rigorous TPC-DS tests with a Query-per-Hour Performance (QphDS) of 14895566 and a unit price of CNY 0.08, as shown in the following figure. AnalyticDB performed 29% better than the world record set by an in-depth optimization of Spark, at a unit price of two-thirds less.
These results make AnalyticDB the world’s leading data warehouse in terms of performance and cost-effectiveness on the TPC-DS benchmark, breaking its own record set on April 26, 2019. For more information, see TPC-DS Results.
Enterprises’ data needs are constantly changing, evolving from traditional big data to fast data. This is mainly manifested in the following four areas (some data is sourced from Gartner and IDC):
With the explosive data growth and full migration of enterprise data to the cloud, the key technical metrics of cloud-native data warehouses will be the storage and processing performance and cost-effectiveness of massive data. The TPC-DS test is a rigorous assessment and audit of a data warehouse in terms of:
TPC-DS is a core benchmark for measuring the maturity and competitiveness of a data warehouse.
The TPC-DS test of AnalyticDB is an important process to improve the product development capabilities of Alibaba and verify core technical breakthroughs. The core technical breakthroughs in this process are helping our customers improve the performance of real-time processes and greatly reduce costs, getting them ready for a new era of database and big data integration and online businesses.
AnalyticDB is Alibaba Cloud’s only proprietary PB-level real-time data warehouse that has been verified by using it in massive core businesses. Since its launch in 2012, nearly a hundred versions have been released, providing support for many online analytics businesses, such as e-commerce, advertising, logistics, entertainment, tourism, and risk control. Alibaba Cloud started providing AnalyticDB for external enterprises in 2014, initially serving traditional large and medium-sized enterprises, government institutions, and numerous internet companies in more than ten industries.
AnalyticDB for MySQL 3.0 is a cloud-native data warehouse, which has evolved based on advances in database and big data integration and engineering over the past eight years. In the TPC-DS test, AnalyticDB for MySQL 3.0 fully demonstrated its outstanding cloud-native technology advantages, proving that it is nearly 10 times better than its closest competition.
TPC is the world’s most famous non-profit data management system evaluator. It formulates the standards, performance, and price metrics for business application benchmarks and publishes the test results. The benchmark results are a core metric for measuring the performance and cost-effectiveness of a data management system.
The TPC-DS test simulates a typical decision support system for retail data warehouses. It is the most challenging database benchmark and is an upgraded version of TPC-H. It uses the star and snowflake schemas, and the test set includes complex applications such as big data set statistics, report generation, online query, and data mining, making it similar to a real scenario.
The following factors make TPC-DS challenging:
The following figure displays the TPC-DS test procedure and data model:
AnalyticDB for MySQL 3.0 uses a cloud-native architecture that separates computing and storage and also separates cold and hot data. It supports high-throughput real-time writing and strong data consistency, ensuring high performance with hybrid loads of high-concurrency queries, and high-throughput batch processing.
The first layer is the access layer, which consists of Multi-Master scalable coordinators. The coordinators implement protocol access, SQL parsing and optimization, real-time data writing to shards, data scheduling, and query scheduling.
The second layer is the computing engine, which supports the integrated execution of distributed massively parallel processing (MPP) and directed acyclic graphs (DAGs). By using intelligent optimizer, it supports high-concurrency and complex SQL statements. Leveraging the cloud-native infrastructure, compute nodes implement elastic scheduling and can be scaled out in minutes or even seconds based on business needs, which ensures effective resource utilization.
The third layer is the Raft-based distributed, real-time, and high-availability storage engine. It implements parallelization based on data shards and multiple Raft groups, separates cold and hot data to reduce costs through tiered storage, and achieves high performance through row-column-mixed storage and intelligent indexing.
AnalyticDB for MySQL 3.0 was independently researched and developed by Alibaba Cloud. Based on the Raft protocol, it provides a distributed, highly consistent, and highly reliable lightweight storage architecture for high-throughput real-time data writing. Therefore, it is suitable for scenarios that require very high analytics performance.
Compared with HBase and Kudu, AnalyticDB for MySQL 3.0 provides higher performance in SQL analytics. It also provides high consistency and visibility in real-time writing and supports ACID, which is not supported by open-source Elasticsearch and ClickHouse.
The following figure shows the overall storage architecture of AnalyticDB.
AnalyticDB for MySQL 3.0 is a database-based parallel data model. It is implemented in a multi-layer parallel architecture based on storage models and MPP computing models.
In the TPC-DS test, the distributed parallel storage architecture works closely with the query optimization and execution engine that perceives storage distribution, resulting in excellent overall performance.
Data import is the basic capability of a cloud data warehouse. Accordingly, TPC-DS requires high import performance.
The first optimization we made was to adopt a lightweight build, which converts real-time data into full partition data. AnalyticDB for MySQL 3.0 implements a lightweight local build with full memory and a single replica. Compared with the earlier MR-like full build, this greatly reduces the overhead of the Distributed File System (DFS) read and write and writing data to disks. It allows the system to make full use of the CPU to improve performance by localizing vector commands.
Then, we turned to I/O and network optimization. We used technologies such as DirectIO, Binary, full stream, asynchronization, and zero-copy to significantly improve import performance.
Next, we reduced the data volume. We used the Raft 2+1 technology (2 data records + 1 log record) to reduce data volumes by a third while ensuring data reliability. We further compressed the data by using the high-performance LZ4 compression algorithm, which greatly reduces the data read and write I/O and network transmission overhead.
Finally, we were able to import 50 million records per second on 18 nodes in TPC-DS.
AnalyticDB for MySQL 3.0 provides high-throughput real-time data update capabilities based on Raft and achieves excellent performance by using the full asynchronization, zero-copy, and high-efficiency coding compression technologies during writing. In the TPC-DS Dynamic Mechanical Loading (DML) test, AnalyticDB can write tens of millions of transactions per second (TPS) in real-time while ensuring linear consistency (allowing query upon writing). In actual production environments, the writing performance is fully scalable, easily reaching 100 million TPS.
In TPC-DS, the data modification and ACID capabilities of the data warehouse need to be verified. AnalyticDB for MySQL 3.0 supports ETL transactions and ACID capabilities (supporting a complete TPC-C test). In the TPC-DS DML test, the Multiversion Concurrency Control (MVCC) capability of the storage engine assumes a major role. The storage engine is divided into real-time data (Delta), partition data (Main), and asynchronous data conversion (Build), creating an LSM-like write optimization architecture. AnalyticDB provides block-level MVCC and snapshot isolation to ensure the isolation (visibility) of data during ETL and data update and the atomicity of data update upon disk failures.
The proprietary row-column-mixed storage format enables AnalyticDB for MySQL 3.0 to ensure a high filtering rate and high throughput scanning. This format ensures better details checking than the column-only-storage format of open-source ORCFile, provides higher random read performance than Parquet, and has a lower cost than the data redundancy models of row- and column-oriented storage. In AnalyticDB for MySQL, each table has a row-oriented or column-oriented storage file. Data is divided into different row groups. In a row group, data is stored in blocks of columns. In a block, data with a fixed length or unfixed length (toast) is encoded and compressed and can be read randomly or sequentially.
In the TPC-DS test, we significantly improved storage scanning performance by properly configuring the storage block size (4 KB), data block prefetching, and source operator vector reading. Having accurate statistical information (such as min, max, sum, and cnt) for storage can accelerate Smart Scan and provide various statistics for the query optimizer, allowing it to formulate an optimal execution plan.
AnalyticDB for MySQL provides a proprietary intelligent indexing framework that supports five types of indexes: inverted indexes (string), bitmap indexes, KD-tree indexes (digit), JSON indexes, and vector indexes. Different types of indexes support combinations of multiple conditions (intersect, union, and except) at the column level. Unlike a traditional data framework, this framework does not require the creation of a compound index (which avoids space expansion) and supports Index Condition Pushdown (ICP) for more conditions such as OR and NOT. For greater ease of use, you can enable automatic full-column indexing when creating a table in AnalyticDB. ICP is dynamically selected through Cost-Based Optimization (CBO) during query, and the ICP index chains will be merged and output at the predicate calculation layer in a streaming and progressive manner.
The query engine of AnalyticDB for MySQL 3.0 consists of the proprietary query optimizer and query executor. This is an important component that enables AnalyticDB for MySQL to provide high-concurrency and high-throughput data warehouse analytics. Its core advantages over a single computing engine include its awareness of data characteristics, in-depth integration with the storage engine architecture, and support for data warehouse analytics scenarios such as reporting, ad hoc, and ETL.
The optimizer of AnalyticDB for MySQL is a distributed cloud-native real-time data warehouse product. It must be able to deal with the problems of a traditional optimizer such as NP-hard issues in complex Join Reorder and the uncertainty of cost estimates, as well as new problems arising from distributed parallel plans in a distributed environment. CBO, the latest achievement of AnalyticDB for MySQL 3.0, was first used in the TPC-DS test and has helped a lot in the overall plan optimization.
Based on the hybrid load management capability of unified memory pooling and queries, the query execution engine of AnalyticDB for MySQL uses technologies such as dynamic code generation, innovative hybrid execution models, vectorization algorithms of the single instruction, multiple data (SIMD) instruction set, and adaptive query execution for row-column-mixed storage. This ensures the superior query performance for AnalyticDB for MySQL in TPC-DS.
The cost-based optimizer is essentially a complex search problem. To solve this problem, you need to start with the following factors:
A common belief is that the traditional volcano model cannot meet the high-throughput performance requirements of analytics scenarios. With the continuous development of various systems, two types of post-evolution execution frameworks have been developed for computing engines in the industry:
In the preceding figure, the red column indicates JIT compilation, while the green column indicates vectorization. AnalyticDB for MySQL is the only proprietary analytics engine that supports these two query modes. The hybrid execution framework adaptively integrates multiple computing-intensive operators into a drive based on the vectorization mode. It empowers a query execution engine that features both compilation and vectorization.
Efficient memory management is the cornerstone of computing optimization. In a type-oriented memory model, different basic type storages are used for different data types. As a result, different types of data cannot be stored in contiguous memory addresses. Instead, the data can be stored only by column, reducing the additional cost of multiple memory objects. In addition, memory cannot be reused across different memory types, which results in additional memory management costs.
The query execution engine of AnalyticDB for MySQL solves the preceding problems through unified memory management.
Join operations between a fact table and a dimension table are typical in a data warehouse. The ratio of the data volumes of the two tables can reach into the tens of millions. The computing costs of Join operations arise mainly from data scanning. Therefore, we use the DynamicFilterPushDown method to greatly reduce the data volume of the left table. In addition, the data warehouse contains a large number of WITH statements and implicitly shared statements. In this case, common table expressions (CTEs) can be shared to avoid repeated computing.
After DynamicFilterPushDown (DFP) reads the Join operation with a high filtering rate (low hit rate) and the data on the probe from the storage, most of the data will be discarded. Therefore, if the evaluated build data is maintained below a small threshold, we can use the build result as the filter condition (dynamic filter) for the left table and push down the data to the storage, reducing the scanning amount. The optimizer’s primary task is to properly evaluate the number of distinct values (NDV) that hit the Join condition at the build end.
Different Join orders directly affect the range and granularity of dynamic filters. The cost of a Join operation that can be optimized is significantly different from that of a real Hash join, which affects the Join orders. Based on the complete and scalable CBO framework of AnalyticDB for MySQL, we selected the globally optimal dynamic filter solution based on cost considerations.
We implement an efficient DFP at the following three key points:
For a CTE, more than 30% of SQL statements in TPC-DS contain the “with as” subquery. The “with as” subquery is referenced multiple times in the main query, and each reference causes repeated computing, resulting in a waste of resources. For basic CTE optimization, the results of the “with” clause are reused for multiple references to reduce the cost of repeated computing. However, in some scenarios, derivation of the relationship with the main query can further reduce the amount of computing in the “with” subquery. In this case, directly sharing the complete “with” clause will lead to additional performance rollback. Then, the optimal plan generated after inline can be used to identify common subtrees. This further reduces the amount of repeated computing and ensures no bad cases. In implementing the executor, we introduced deadlock detection to solve the deadlock problem by analyzing the dependencies among multiple consumers of common subtrees.
Since its debut in the cloud in 2014, AnalyticDB has been available in major regions around the world and has entered the global analytics market. In 2018, AnalyticDB was selected as a contender in Forrester’s research report “The Forrester Wave™: CloudData Warehouse, Q4 2018” and Gartner’s “Magic Quadrant for Data Management Solutions for Analytics” report.
AnalyticDB is widely used in the Alibaba Group and by our customers to replace Teradata and Oracle RAC. It has been applied and verified on a large scale in core industries such as the internet, new retail, finance, taxation, transportation, meteorology, and logistics. For example, in the logistics industry, AnalyticDB has enabled China Post to perform large-scale centralized query and analysis on nationwide logistics data for the first time.
AnalyticDB has been verified theoretically by the Very Large Data Bases (VLDB) paper, “AnalyticDB: Real-time OLAP Database System at Alibaba Cloud”. This is an honor reserved for top large-scale commercial systems, such as Google F1 (VLDB’2013) and AWS Aurora (SIGMOD’2017). AnalyticDB has also proven itself through the TPC-DS benchmark (proving itself to be the world-leader in cost-effectiveness and performance), through customer feedback (including major government ministries and internet-scale customers), and through successful application in the Alibaba Group over many years. We have now upgraded AnalyticDB to a cloud-native data warehouse based on the highly efficient cloud computing and the integration of databases and big data.
In the future, big data and database integration and cloud-native will redefine the data warehouse for the cloud computing era. Breaking the TPC-DS world record is just a start. AnalyticDB will continue to strive to become an infrastructure for digital transformation and upgrade and a way for enterprises to realize the value of online data.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@knoldus/cloud-native-future-of-software-architecture-1ca6a238938e?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Knoldus Inc.
Jun 11, 2019·5 min read
With software increasingly, the key to how users engage with businesses and how businesses innovate to stay competitive, the speed of application development and delivery is the new digital business imperative. Technology leaders are rapidly being disrupted, and they’re being disrupted by businesses with software at their core. Companies like Square, Uber, Netflix, Deliveroo, Airbnb, and Tesla continue to generate enormous economic growth and business value and turn the heads of executives of their industries’ historical leaders.
What do these innovative companies have in common?
Cloud-native application architectures are at the center of how these companies are disrupting the Technology leaders. Their success is driven by their ability to fully harness the elasticity of the cloud by adopting a “cloud-native” approach to application development. Time has come to replace the traditional workloads with cloud-native architecture to reap the full benefits, capture business opportunities and avoid irrelevancy.
Gartner has to say the same about cloud-native — “Technology leaders must deliver cloud-native offerings now to capture business opportunities and avoid irrelevancy. ” — Craig Lowery, Gartner Principal Analyst
There are three driving forces behind moving to cloud-native application architectures are: Speed, Scale, and Safety
Cloud-native is an approach to building and deploying applications that maximize the advantages of the cloud computing delivery model. Despite its name, a cloud-native approach is not focused on where applications are deployed, but instead on how applications are built, deployed, and managed. It’s appropriate for both public and private clouds. Most important is the ability to offer nearly limitless computing power, on-demand, along with modern data and application services for developers. When companies build and operate applications in a cloud-native fashion, they bring new ideas to market faster and respond sooner to customer demands.
FOUR ELEMENTS OF CLOUD-NATIVE APPLICATION DEVELOPMENT AND DEPLOYMENT
Cloud-native application development is an approach to building and running applications that takes full advantage of the cloud computing model based upon four key elements: Microservices architecture, API-based communication, container-based infrastructure, and DevOps processes.
1. Service-based architecture (Microservices) — Service-based architecture, such as microservices, advocates building modular, loosely coupled services and making application easier to develop and test. It helps organizations increase application deployment speed without increasing complexity and scale their services independently. According to IDC research has found that 100 percent of businesses with “optimized” cloud adoption have adopted microservices compared to just 18 percent of those with an “ad hoc” approach — just lines of business or other groups experimenting with the cloud. Read more about microservices here.
2. API Based Communication — Services are exposed via lightweight, technology-agnostic APIs that reduce the complexity and overhead of deployment, scalability, and maintenance. Businesses can create new capabilities and opportunities internally and externally via the exposed APIs. API-based design only allows communication via service interface calls over the network, avoiding the risks of direct linking, shared memory models, or direct reads of another team’s datastore. This design extends the reach of applications and services to different devices and forms.
3. Container Based Infrastructure — Cloud-native applications rely on containers for a common operational model across technology environments and true application portability across different environments and infrastructure, including public, private, and hybrid. Container technology uses operating system virtualization capabilities to divide available compute resources among multiple applications while ensuring applications are secure and isolated from each other.
Cloud-native applications scale horizontally, adding more capacity by simply adding more application instances, often through automation within the container infrastructure. The low overhead and high density of containers, allowing many of them to be hosted inside the same virtual machine or physical server, makes them ideal for delivering cloud-native applications.
4. DevOps Process — Application development for cloud-native approaches follows agile methods with continuous delivery and DevOps principles that focus on building and delivering applications collaboratively by development, quality assurance, security, IT operations, and other teams involved in the delivery. Agile is all about coping with and driving change — making development processes fast and easy.
The benefits of implementing cloud-native features typically include the following:
Depending on your stage within and cloud-native journey and your priorities, Knoldus has the technologies and services to support you at every step of your cloud-native journey. Knoldus cloud-native expertise can help your team to move forward — whether you have to modernize existing application, build new cloud-native apps, accelerate app delivery or drive business innovation. Book a meeting or drop us a message here or at hello@knoldus.com to get you started on your cloud-native journey.
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
"
https://medium.com/@alibaba-cloud/cloud-native-technology-on-the-core-system-of-double-11-for-higher-efficiency-and-lower-costs-260b22671b48?source=search_post---------323,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 26, 2020·4 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Developer
The 2020 Double 11 Global Shopping Festival was a huge success, witnessing a world record-breaking GMV of US$74.1 billion! The first order peak appeared at 0:00:26 on November 11 and exceeded numbers from 2019’s shopping festival, with a record of 583,000 orders created per second at peak. Despite the high traffic, customers still enjoyed a stable and smooth experience. For the 12th year in a row, Alibaba completed another Double 11 Global Shopping Festival without any major issues.
It was reported that the cloud-native core system brought surging power and ultimate efficiency during the shopping festival. The IT cost of every 10,000 transactions has dropped 80% compared with four years ago, and the delivery efficiency of scale application has doubled.
Ding Yu, the Head of Cloud-Native Application Platform in Alibaba Cloud, revealed that the 2020 Double 11 Global Shopping Festival triggered many “firsts” in cloud-native. For example, 80% of the core services were deployed on Alibaba Cloud Container Service for Kubernetes (ACK), which could scale-out over one million containers within one hour. Serverless was applied on a large scale for the first time, and the auto scaling was improved more than ten times over. The peak call volumes of cloud-native middleware exceeded ten billion queries per second (QPS). Also, the cloud-native database PolarDB and cloud-native data warehouse AnalyticDB were applied on a large scale for the first time, improving read and write performance by 50% and transactions per second (TPS) by 100 times, respectively.
Computing records are constantly being broken. Flink’s processing peak value reached four billion records per second. This is similar to reading all five million entries in the Xinhua Dictionary 1,000 times in about one second. MaxCompute’s computing data volume reached 1.7EB per day. This is the equivalent of processing 230 high-definition photos for more than seven billion people. PolarDB set a new processing peak, with TPS reaching 140 million, 60% higher than last year. AnalyticDB processes 7.7 trillion rows of real-time data, corresponding to the total amount of data in more than five national libraries.
Cloud-native technologies have not only been extensively used in Alibaba, but they have also provided services for the Double 11 Global Shopping Festival through Alibaba Cloud. We can look at the logistics industry as an example. China Post introduced the cloud-native databases PolarDB-X and AnalyticDB to handle billions of business orders during the shopping festival. It provided about 100,000 users to query the real-time status of their parcels at the same time. STO Express moved its core system onto the cloud and used Alibaba Cloud Container Service to obtain a stable system for millions of parcels in transit with 30% less IT costs. Cainiao also used real-time computing Flink and interactive analysis MaxCompute to analyze parcels. Its synchronization time of the overall data process was reduced from one hour to only three minutes, significantly improving the efficiency of parcel flow.
Based on technical practice for over ten years, Alibaba Cloud has a comprehensive suite of cloud-native products and open-source ecosystem. More than 100 innovative products have been provided, including cloud-native bare metal instances, cloud-native databases, container services, and microservices.
Ding Yu said, “Cloud-native is the quickest way to access the benefits of cloud computing, and these core technologies are also being opened to the public through Alibaba Cloud.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/series/behind-double-11-cloud-native-practices-df3930ac1bbf?source=search_post---------324,
https://medium.com/altoros-blog/meet-altoros-at-the-springone-conference-2021-99cfb37cb8c2?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
Join our Live Office Hours during the SpringOne conference to discuss all things Cloud Native! Experiencing any bottlenecks when adopting Kubernetes? Bring them to the table!
Register for the Zoom meeting: https://bit.ly/3DrenFk
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
1 clap
1 
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/magalix/how-shifting-security-left-helps-mitigate-cloud-native-security-risks-7b79a50d3f33?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
The cloud-native ecosystem has steadily grown over the past decade with the promise of faster deployments, cost-efficient infrastructure, and auto-scalability spurring its growth. Businesses are now developing and deploying easily scalable, cost-efficient, and more resilient…
"
https://medium.com/@alibaba-cloud/three-reasons-why-businesses-should-consider-cloud-native-databases-9a5a188d0ad3?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 10, 2020·3 min read
To discover more about the benefits of adopting a cloud-native database, and how they’re impacting industries worldwide, download the Creating a Robust Cloud-Based Database for Fintech, E-Commerce and Gaming whitepaper today.
Databases play a vital role in our digitized economy. From e-commerce to banking, they are important in keeping the wheels of commerce turning and connecting communities the world over.
Research reveals that 75 percent of all databases will be on a cloud platform by 2023 [1].
The shift from on-premise to cloud databases is inevitable for businesses that need to scale up their operations to ensure real-time data delivery and continuity of service to their global customers. But what does cloud-native mean?
Cloud-native databases provide a cross-domain service that integrates traditional relational databases with cloud environments, serving applications better in the cloud environment. Traditional databases are much slower, less efficient and more costly with their underlying structure — leading to an array of problems.
Let’s look at three common reasons why businesses should consider the cloud-native option.
DDoS-attacks are a recurring problem across various industries and pose a serious threat for your organization. Even a small security loophole could be exposed, irreversibly damaging your brand and causing customers to disappear overnight.
For example, if you’re an online game provider, the reliability of both your hardware and software under high load conditions, protection against cyber-attacks and the security of personal and payment data are paramount for your business survival.
Fortunately, Alibaba Cloud provides a vast range of cross-industry security solutions to stop attacks of all sizes and defend your organization against online and offline threats.
From a database perspective, PolarDB is the perfect solution for a large-scale massively multiplayer online game (MMOG) thanks to its separation of storage from computation. This allows your organization to quickly expand its services and perform necessary operations and maintenance should you experience a cyber-attack.
Your database can be seen as your most valuable business asset. As such, it is essential to maintain regular backups. This isn’t always easy though.
Particularly for those working in industries such as financial services, it is not acceptable to simply switch off a database to complete a backup on a weekly, daily or hourly basis. Furthermore, if a fault does occur, it is equally unacceptable to lose periods of critical data.
As a FinTech company providing financial analysis of the world’s stock markets, you cannot afford to be offline in any capacity. You need to minimize backup times to realize the necessary cost-savings for your startup — and you need to provide real-time data analysis to your growing customer base in this dynamic industry.
The solution is a Database Backup Service, which not only minimizes the data loss if an error occurs, but also restores your database to a precise point in time with minimal Recovery Point Objective (RPO).
Finally, let’s look at the database needs of a digital platform. Let’s assume you’re an online vacation booking business, helping users search for a diverse range of breaks on your various websites and applications.
You want your users to access all these services under the umbrella of your parent business, with one login. You also want to provide a superior user experience, where information is retained if, for example, a user loses connectivity halfway through the online booking process.
Under Alibaba Cloud’s Data Transmission Service (DTS) Solution, you can couple different services asynchronously through a real-time message notification. As a result, your interrelated services can be concurrently started, providing a stable and reliable service and an excellent user experience for your customers.
To find out more about Alibaba Cloud’s fully managed, trouble-free and optimized database services, download the Creating a Robust Cloud-Based Database for Fintech, E-Commerce and Gaming whitepaper today.
blogs.gartner.com
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/altoros-blog/altoros-achieves-vmware-master-services-competency-in-cloud-native-65c1e41eb8d7?source=search_post---------328,"There are currently no responses for this story.
Be the first to respond.
Excited to announce that Altoros achieves VMware Master Services Competency in cloud native! We’re committed to growing our partnership around the VMware Tanzu portfolio, being passionate about Kubernetes-based platforms. VMware Master Services Competencies are designed to help partners demonstrate customer-centric solutions and technical proficiency, with proven success and expertise in a specialized area of business. Thanks, VMware, for this amazing collaboration!
For more information, read the press release.
www.prnewswire.com
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/digital-leaders-uk/webinar-recording-modern-cloud-native-development-using-live-streaming-data-5ea06161c7f8?source=search_post---------329,"There are currently no responses for this story.
Be the first to respond.
Tune in to hear Adam Fowler, Pivotal’s Platform Architect for Public Sector, UK & Ireland as he talks about the challenges of development in a Multi-Cloud World.
Developers have a number of demands on their time and Adam will showcase how using a modern container-based platform can streamline development and improve developer productivity, going from idea to having an application in production in days rather than years.
Join this webinar session to see a demonstration of an application that uses the same architectural principles as those used by the United States Air Force’s flagship Kessel Run Digital factory.
More thought leadership
Originally published at https://digileaders.com on January 29, 2020.
Thoughts on leadership, strategy and digital transformation…
Digital Leaders is a global initiative that has created a shared professional space for senior leadership from different sectors promoting effective, long-term digital transformation.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kentlangley/well-ive-been-calling-this-cloud-native-since-2011-but-like-everything-else-it-gets-abused-for-deb0acaae9a4?source=search_post---------330,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kent Langley
Dec 8, 2016·1 min read
P.S. For everyone’s sake, someone please come up with a better name than serverless. The original Zimki service was described as FaaS (Framework as a service) back in 2006. Unfortunat…
swardley
Well, I've been calling this ""cloud native"" since 2011 but like everything else it gets abused for marketing purposes. Now people are trying to make serverless and cloud native mean container based virtualization. You know, like Solaris Zones.
Ultimately, I'm not sure I care what it gets called. I do know I'm building it and have been for years in edge cases like what evolved into CDN.
This way of building, no matter how it is labeled, fundamentally changes the economics of large scale data center operations in potentially dramatic ways. For example, no one organization is paying for the bitcoin blockchain data center operations for example and that's a pretty impressive operation on a global scale. This is not single actor cost displacement like cap-ex to op-ex for one organization. This is, to agree with you, much bigger. It de-monetizes and de-centralizes the on-going operations costs for epic style data-center operations to the users and benefactors if the tools are properly weilded over time.
It's going to be very interesting as we bounce through the 2020's!
www.kentlangley.com
1 
1 
1 
www.kentlangley.com
"
https://medium.com/@alibaba-cloud/alibaba-cloud-launches-enterprise-level-cloud-native-data-lake-during-2020-double-11-bf8a367159a0?source=search_post---------331,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 30, 2020·4 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Cloud Storage
On October 23, the 2020 Data Lake Summit was held in Beijing. Alibaba Cloud announced the launch of an enterprise-level cloud-native data lake solution. This solution provides data storage and analysis capabilities at the exabyte level. By doing this, Alibaba Cloud can realize comprehensive lake storage, lake acceleration, lake management, and lake computing, helping enterprises perform in-depth data mining and analysis, and gain insight into the data. Therefore, this data lake solution is more suitable for emerging industries with massive data scenarios, such as artificial intelligence (AI), Internet of Things (IoT), and autonomous driving.
Chen Qikun, Senior Director of Alibaba Cloud’s intelligent storage products, said, “The cloud-native enterprise-level data lake solution will be applied on a large scale during the Double 11 Global Shopping Festival for the first time this year. The solution will support Alibaba’s economy and millions of customers to fully access the cloud, unleashing the value of data to the greatest extent.”
The cloud-native enterprise-level data lake solution of Alibaba Cloud adopts the storage and computing separation architecture. This solution is based on the Alibaba Cloud Object Storage Service (OSS) and created in combination with the Alibaba Cloud Data Lake Analytics (DLA), Data Lake Formation (DLF), and E-MapReduce (EMR). This solution is compatible with a wide range of open-source engine ecosystems, meeting the needs of large-scale unified data storage. As such, the cloud-native enterprise-level data lake solution is more reliable, flexible, and secure.
The concept of a data lake is not new. Ten years ago at the Hadoop Summit in New York, a data lake was proposed and defined, “to pour what you have on tape into a lake of data and then start to explore the data.” With the development of big data, cloud storage, and cloud computing, today, the concept of a data lake is mature and has been widely brought into practice in various enterprises.
Unlike traditional big data solutions, the cloud-native data lake solution is based on the next-generation data lake architecture. By adopting this solution, customers can directly access the business production center, including the raw data and log data in the business system. Also, data can be directly stored in the data lake through the Internet without intermediate processing, improving business efficiency by 100%, and driving the shift of enterprise IT systems from a cost center to an innovation center.
We can use a well-known multiplayer online gaming company in China as an example. Based on the Alibaba Cloud data lake solution, this company delivered its global data to OSS in real-time using Log Service (SLS). The company took advantage of the massive elastic capabilities of OSS to separate hot and cold data as well as the EMR and DLA to build a big data architecture for separating storage and computing. As such, the company can perform real-time channel statistics and real-time analysis of the best process for tens of millions of active gamers daily. These refined operations have helped the company increase user retention by 30%. Currently, thousands of enterprises have built their data lakes on Alibaba Cloud.
Li Feifei, Vice President of Alibaba Group and the Head of Alibaba Cloud’s Intelligent Database Product Division, believed that the integration of databases and big data is accelerating the large-scale implementation of data lakes. The cloud-native data lake allows enterprises to mine the value of data in a more flexible, agile, efficient, and easier way, without needing to manage computing resources. The data lake also empowers enterprises to regenerate and innovate quickly, making data insights a core competency of enterprises.
According to Jia Yangqing, Vice President of Alibaba Group and the Head of the Alibaba Cloud’s Intelligent Computing Platform Division, the solution that enterprises want is based on the Alibaba Cloud data lake OSS and data warehouse MaxCompute without data transmission. The data can be flowed intelligently and computed through multiple platforms. The solution will ensure the continuity and timeliness of the data services of enterprises due to its combination with the flexibility of the data lake and the growth of the data warehouse.
“In the digital economy era, if we think of big data like oil and computing power as an engine, the cloud-native enterprise-level data lake will be a solution that can integrate both of them. In the near future, a data lake will become a standard for enterprise innovations, helping enterprises achieve intelligent and digital transformation in an all-round way,” said Chen Qikun.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/knoldus/lets-mindmap-cloud-native-64f01641c8dc?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
Cloud-Native evokes different responses from every engineer on any team. As a part of this visual presentation, we would like to get a common understanding of what is cloud native, talk about its pillars and which applications are a good candidate for the same.
With this vlog the idea is to make a common denominator of understanding so that the team can be succesful together.
Happy viewing!
Want us to help with building a well architected cloud native product. We are waiting to say hello :)
Knols, Insights and Opinions from the curious minds at…
1 clap
1 
Knols, Insights and Opinions from the curious minds at Knoldus Inc.
Written by
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
Knols, Insights and Opinions from the curious minds at Knoldus Inc.
"
https://medium.com/@developernationworld/vs-code-extensions-the-search-for-a-cloud-native-database-useful-ios-developer-interview-4440d9acd409?source=search_post---------333,"Sign in
There are currently no responses for this story.
Be the first to respond.
Developer Nation
May 6, 2021·5 min read
Hey dev community, we’re happy to share the latest news and resources from our biweekly newsletter. Enjoy!
The search for a cloud-native database. As we move to the cloud, how do we evolve our data storage approach? Do we need a cloud-native database? What would it even mean for a database to be cloud-native? Let’s take a look at these questions. [DEVELOPERECONOMICS]
50 VS code extensions that will make you a better developer. Here is the curated list of the most useful extensions that help developers to increase their productivity and make them a better developer. [JAVASCRIPT.PLAINENGLISH]
Why programmers don’t write documentation. Kislay believes that there are two main reasons software engineers don’t write documentation. Tools play their part but they are a hugely distant third. [KISLAYVERMA]
iOS 14.5 brings the new Safari 14.1 to PWAs and the Web Platform. Maximiliano Firtman looks at what’s new, what’s missing, new challenges and new capabilities for iPhone and iPad. [FIRTDEV]
An elegant Angular architecture. Good architecture can save us from heavy refactoring when the applications become large and complicated, as we all know that product owners will never stop adding new features in the backlogs :). Therefore, Zhichuan JIN shares her elegant Angular (V2+) architecture to help new developers avoid detours. [MEDIUM]
Typetester. A free tool to test, compare, select and design with over 2800 web fonts from Adobe Edge, Adobe Typekit and Google Fonts. Export your designs as fully responsive HTML and CSS snippets. [TYPETESTER]
67 useful iOS developer interview questions. Are you the interviewer or the interviewee? It’s good to know answers for these answers either way. [MEDIUM]
A guide to undoing mistakes with Git. No matter how experienced you are, mistakes are an inevitable part of software development. But we can learn to repair them! And this is what we’ll be looking at in this two-part series: how to undo mistakes using Git. [SMASHING.MAGAZINE]
A gallery of App Tracking Transparency (ATT) prompts. A useful site showing how other developers are handling copy in ATT prompts. [ATTPROMPTS]
Best Practices for API Testing. In this whitepaper, RapidAPI examine some of the best practices for testing APIs. [RAPIDAPI]
Beautify your GitHub! If you have ever visited someones GitHub and noticed that they have some fancy pictures, cool emojis, and stats on their homepage you may have wondered how to make a page like that on your own. If this sounds like you, keep reading, because Philip Haines tells you exactly how to make your GitHub README sparkle! [DEVTO]
Women in tech with INCO. [EVENT] 14 May, 19.00 Athens time. A one hour discussion with INCO team to help women who are interested in joining tech career through a fully-funded scholarship by Google. . Whilst this event is for Greek participants, Devs Alliance welcome all participants to future events.
HPE Munch & Learn #5 — Data Science Unplugged Part 2. [EVENT] May 19, 2021 05:00 PM in Paris. Today, the pressure to extract value from data through the application of Data Science is intense. Yet many of us, even those who are technical specialists in this field, lack the depth of understanding required to successfully implement the art, processes, and business applications of Data Science. In this series of Data Science 101 primers, HPE hope to clear up much of the confusion and help you understand how data is transformed into information, knowledge and wisdom to create meaningful business value.
Vulnerable Dell driver puts hundreds of millions of systems at risk. A driver that’s been pushed for the past 12 years to Dell computer devices for consumers and enterprises contains multiple vulnerabilities that could lead to increased privileges on the system. It is estimated that hundreds of millions of Dell computers, from desktops and laptops to tablets, received the vulnerable driver through BIOS updates. [BLEEPINGCOMPUTER]
Rust programming language: We want to take it into the mainstream, says Facebook. “We are joining the Rust Foundation to help contribute to, improve and grow this language that has become so valuable to us and developers around the world. We look forward to participating with the other foundation members and the Rust community to make Rust a mainstream language of choice for systems programming and beyond.” [ZDNET]
Commodore 64 emulator brings retro gaming to Oculus Quest. Available now on the Oculus Quest via SideQuest, Real Commodore 64 — Virtually brings the original Commodore 64 experience to VR, allowing Quest and Quest 2 players the chance to explore a massive catalogue of retro PC games. [VRSCOUT]
Peloton’s leaky API let anyone grab riders’ private account data. Jan Masters, a security researcher at Pen Test Partners, found he could make unauthenticated requests to Peloton’s API for user account data without it checking to make sure the person was allowed to request it. But the exposed API let him — and anyone else on the internet — access a Peloton user’s age, gender, city, weight, workout statistics and, if it was the user’s birthday, details that are hidden when users’ profile pages are set to private. [TECHCRUNCH]
Nintendo announces Game Builder Garage, an easy way to make your own games. Somewhat like the PS4’s Dreams, Game Builder Garage lets you create games without having to write any code. Instead, it uses visual programming, allowing you to connect and interact with creatures calls ‘Nodons’ — a play on a ‘node’ in the programming sense, one assumes. [THENEXTWEB]
JetBrains previews Jetpack Compose for Web. JetBrains is offering its first technology preview of Jetpack Compose for the Web, which brings Google’s Kotlin toolkit for building reactive user interfaces to the web. Introduced on May 3, Jetpack Compose for Web works on top of Kotlin Multiplatform, enabling developers to build an application for Android, the desktop, or web using Jetpack Compose as the UI framework, all within the same project. [INFOWORLD]
Kotlin 1.5.0 — the first big release of 2021. This release delivers stable language features such as JVM records, sealed interfaces, inline classes, and includes the new default JVM IR compiler which more than 25,000 developers have already tried in IntelliJ IDEA. [JETBRAINS]
Python 3.8.10, 3.9.5, and 3.10.0b1 are now available. Python 3.10 is still in development. 3.10.0b1 is the first of four planned beta release previews. Beta release previews are intended to give the wider community the opportunity to test new features and bug fixes and to prepare their projects to support the new feature release. It’s strongly encouraged that maintainers of third-party Python projects to test with 3.10 during the beta phase and report issues found to the Python bug tracker as soon as possible. [PYTHON.INSIDER]
A global community of software creators who want to influence how software is built.
A global community of software creators who want to influence how software is built.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-ack-pro-and-ack-edge-cloud-native-evolution-for-enterprises-4c279ccf6297?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 23, 2020·7 min read
By Alibaba Container Service
The Alibaba Cloud Container Service for Kubernetes (ACK) team has been exploring how to better support the hybrid cloud and the distributed cloud architecture that integrates the cloud and edge applications and better support the global application delivery, to help enterprises reduce costs and improve efficiency. In this blog,
Kubernetes-based cloud-native computing has also become a new operating system. A growing number of industries and enterprises adopt and benefit from the prototype of the cloud-native operating system. Alibaba Cloud also continuously refines the cloud-native operating system provided for customers. What outstanding features does the cloud-native operating system have?
First of all, the infrastructure layer provides powerful infrastructure as a service (IaaS) resources. The computing resources based on the 3rd generation of X-Dragon architecture can be elastically scaled to provide higher performance at an optimized cost. The cloud-native distributed file system is built to persist data in containers. The cloud-native network accelerates the delivery of applications and provides application-type load balancing and container network infrastructure.
Second, the ACK service has accompanied thousands of enterprise customers to carry out a large number of production-grade scenarios at the container orchestration layer, in various industries since its launch in 2015. An increasing number of customers are arranging most or even all of their applications in a cloud-native architecture. As the business further develops, to meet the requirements for high reliability and security of large- and medium-sized enterprises, Alibaba Cloud launches ACK Pro, the Enterprise Edition of ACK. ACK Pro provides an SLA-warrantied availability and customers can be compensated for the failure to meet the availability.
Apply for Free Trial of ACK Pro >>
Learn More about ACK Pro Clusters >>
ACK Pro inherits all the strengths from ACK clusters of the Managed Edition, such as managed primary nodes and high availability. In addition, ACK Pro clusters provide higher reliability, security, and scheduling performance. ACK Pro is applicable to enterprise customers with massive services in production environments, where high stability and security are required.
ACK Pro is built for massive production environments in enterprises. A single ACK Pro cluster supports up to 5,000 nodes. Management resources can be automatically scaled. The etcd uses encrypted disks and adopts frequent cold and hot backup and geo-disaster recovery. Management components such as kube-apiserver and etcd have enhanced observability. You can view monitoring metrics of core management components on dashboards and set alert rules for them. The ACK team has also strengthened the autonomy mechanism for the managed primary components, striving to detect and fix errors in a timely manner to minimize adverse impact. In addition, the ACK team has made an official commitment that cluster management can provide an SLA-warrantied availability of 99.95% and customers can be compensated for the failure to meet the availability.
Container security has been valued by an increasing number of customers. ACK Pro further improves the security features during application deployment and running by introducing a new security management module. The module provides several features. First, the module allows you to configure container security policies. You can define a Kubernetes Pod Security Policy (PSP) to check whether a request to deploy and update a pod in the cluster is valid. Second, the module allows you to configure inspection jobs for the cluster. The module can scan and detect security risks in the workload configurations of the cluster and interprets the information in the inspection reports. Based on this, you can learn whether the runtime configurations of applications in the current state are secure in real time. Third, the module monitors runtime security and generates alerts. Specifically, it scans and terminates malicious image startup, viruses, and malware and detects attacks on the container side, such as intrusions into containers, container escapes, and high-risk operations on containers. ACK Pro also allows you to encrypt and decrypt secrets data by using the keys defined in Alibaba Cloud Key Management Service (KMS). This way, secrets can be encrypted when they are stored in disks. In addition, ACK Pro supports sandboxed containers and encrypted computing clusters, providing comprehensive security assurance.
ACK Pro has enhanced the scalability of the native kube-scheduler in Kubernetes. It supports batch scheduling. You can schedule a group of associated processes or jobs to a cluster at a time based on the All-or-Nothing principle. This prevents deadlocks caused by scheduling failures of some jobs. In addition, it provides topology-aware CPU scheduling for CPU-sensitive workloads. This ensures resource allocation to this type of business and avoids performance deterioration caused by context switching. In our best practices, intelligent CPU scheduling improves the performance of applications in X-Dragon-based Elastic Compute Service (ECS) Bare Metal instances by 60% to 150% and decreases the latency of applications by 100% under high loads.
ApsaraVideo has used ACK as the service basis to manage resources on tens of thousands of nodes in more than 10 regions around the world. ACK Pro ensures O&M efficiency and high stability for massive computing resources at the infrastructure layer, allowing the ApsaraVideo team to focus on the video field to provide more value to customers.
ACK Pro has been available for public preview. You are welcome to apply for a trial on the official website.
Apply for Free Trial of ACK Pro >>
Learn More about ACK Pro Clusters >>
In the era of the Internet of Everything (IoE), many enterprises are exploring the measures to extend the computing power of intelligent edges to edge nodes such as IoT devices. They are seeking to increase the connection speed, improve the real-time performance of services, and reduce the transmission restrictions caused by the central cloud and network.
Alibaba Cloud has deeply researched the requirements for implementing edge computing and cloud native and led the concept of “cloud-edge integration” in the industry. In June 2019, Alibaba Cloud officially released ACK@Edge. This edge container is designed to “extend cloud-native capabilities to the edge” and manage and control edge clouds, edge devices, and terminals in a centralized manner to achieve cloud-edge-terminal collaboration.
In the past year, ACK@Edge has been used in scenarios such as live audio and video streaming, cloud gaming, industrial Internet, transportation and logistics, and city brain. It has been providing services for Hema Fresh, Youku, ApsaraVideo, and many Internet and new retail enterprises.
After ACK@Edge is used in YY, YY can use APIs to manage and maintain edge container clusters and central container clusters in a centralized manner. This enables quick access to the edge computing power and autonomy of edge nodes. In addition, this allows YY to seamlessly access Prometheus to report monitoring data, significantly improving the overall O&M efficiency and resource utilization.
As its business grows, Youku is considering extending its centralized architecture from the Internet data center (IDC) to the edge architecture. Youku uses ACK@Edge to manage thousands of edge nodes of Alibaba Cloud in dozens of regions and release and elastically scale applications in a centralized manner. The dynamic scaling capability has reduced their machine costs by 50%. After the new architecture is used, the video playback pipeline is shifted from the Internet to the pipeline that starts from the Alibaba Cloud global network to edge nodes and then to terminals, reducing the network latency by 75%.
ACK@Edge allows Hema Fresh to build digital full-pipeline integration of people, goods, and sites and implement cloud-edge-terminal collaboration. Based on the excellent resource scheduling and application management capabilities of the cloud-native technology system and the advantages of nearby access to edge computing and real-time processing, Hema Fresh has achieved “cost reduction and efficiency improvement” in all aspects. The computing resource cost in stores has been reduced by 50% and the service provisioning efficiency at new stores has improved by 70%.
After ACK@Edge is commercially launched, ACK@Edge continues to meet the requirements for enterprise-grade edge containers for customers. ACK@Edge is applicable to scenarios including edge intelligence, smart buildings, smart factories, live audio and video streaming, online education, and content delivery networks (CDNs).
Cloud-native technology can maximize the scalability of the cloud, helping enterprises reduce costs and improve efficiency. It also provides more room for innovation. Cloud native will be combined with new technologies such as AI, edge computing, and confidential computing to build an intelligent, connected, and trusted innovative infrastructure for the digital economy.
“We are developing ACK products to be a new cornerstone, a new computing power, and a new ecosystem,” said Yi Li. “Cloud-native technologies are becoming the shortest path to realize the value of the cloud. The Alibaba Cloud team will help enterprises better support hybrid cloud, distributed cloud architectures with cloud-edge integration, and global application delivery. Alibaba Cloud will accelerate the upgrade to intelligent business by driving technical innovations on software and hardware integration based on cloud native, such as the X-Dragon architecture, Hanguang network processing units (NPUs), and shared scheduling of graphics processing units (GPUs). In addition, we will open up the technological ecosystem and global partner programs to allow more enterprises to enjoy the benefits of technologies in the era of cloud.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@GiantSwarm/one-stop-cloud-native-consulting-development-operations-c26f6806e656?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Oct 13, 2020·5 min read
The exciting growth potential unlocked by a cloud-native approach is what drives companies to envision their digital transformation powered by cloud-native innovation. However, setting up a cloud-native environment is a challenging and complex endeavor that can make companies feel stuck in the old world.
The three main challenges companies should prepare for:
In only a few years, release cycles, updates, new technologies, and consumer demands have evolved at a rapid pace, which has demanded even more specialization. Embracing the cloud-native approach is not enough to ensure a competitive edge. Partnering with industry leaders who have a track record of success ensures companies reap the benefits without the risk of failure.
That’s why viadee and Giant Swarm joined forces to help companies to actually benefit from a cloud-native stack by providing consulting, development, and operations at one stop.
viadee has a strong background in developing tailor-made applications for enterprise customers for more than 25 years and is happy to help with IT architects, developers, or coaches to either support internal teams and foster knowledge transfer or to take over the full responsibility of the application/platform itself. With over 150 experts and consultants based in Cologne and Münster (Germany), services can be provided on-premise or remotely, depending on the customer’s location.
At viadee,
Complementing internal teams with external consultants can significantly accelerate the journey towards tailor-made platforms and applications based on proven technologies and architectures. It’s even possible to outsource implementation work to unleash internal resources.
viadee’s competence center for cloud platforms and architectures is happy to staff different roles:
For more details, please visit viadee.
Having now covered the first two categories of challenges, ensuring the smooth operation of the platform and its applications is still needed. Otherwise, they cannot provide actual value for the business and its end users.
Giant Swarm builds, operates, monitors, and manages cloud-native platforms so customers can completely focus on building digital products that drive their competitive edge. This way they can accelerate their competitive advantage and avoid risks and delays. When it comes to operating your platform, don’t leave anything to coincidence and instead rely on trusted experts.
Giant Swarm not only develops and operates the open-source technology for the cloud-native infrastructure, but is also available to internal teams at any time for questions, knowledge transfer, and solution development. Dedicated Solution Engineers can be included in the planning via Slack and weekly sync meetings to discuss future changes, plan upgrades, and support with individual solutions. This means that development teams can fully focus on their digital products. It reduces the risk of jeopardizing smooth operations or temporarily devoting themselves completely to problems with the infrastructure.
Benefit from the close partnership between viadee and Giant Swarm for smooth operations of your apps:
If you’re interested in a one-stop cloud-native solution backed by years of experience and real-life support, get in touch.
Contact Giant Swarm
Contact viadee
FYI: We will host a joint webinar on the topic How to scale modern software development through a common platform on September 1st. Details to follow soon!
We would love for you to join. If you have any questions you’d like us to address, drop us a tweet.
Written by The Team @Giant Swarm
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
See all (60)
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/highlights-of-the-2019-cncf-china-cloud-native-survey-report-8fa810399e6a?source=search_post---------336,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 1, 2020·10 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Cloud Serverless
For a better understanding of the use of open source and cloud native technologies, the Cloud Native Computing Foundation (CNCF) conducts community surveys regularly. This is the third China Cloud Native Survey conducted in Mandarin Chinese, aiming at gaining a deeper understanding of the methods of cloud native technology adoption in China. This survey also aims at understanding how cloud native technologies empower developers and make changes in a large and growing community. This report is based on the previous two Cloud Native Survey Reports released in March 2018 and November 2018.
• 49% of respondents use containers in production, and other 32% are planning to do so. This is a significant increase compared to the survey results showed in November 2018, when only 20% of respondents used containers in production.
• 72% of respondents use Kubernetes in production, which is higher than 40% of that in November 2018.
• The usage of public clouds has dropped to 36% at present from 51% in November 2018, while 39% of respondents choose to use hybrid clouds.
• The number of CNCF projects has increased exponentially. CNCF now has four projects born and widely used in China. They are Dragonfly and KubeEdge in the incubation phase, and newly graduated Harbor and TiKV.
The 2019 China Cloud Native Survey included 300 respondents. 97% of them were from Asia, mainly from China.
It is well known that containers have changed the cloud-based basic infrastructure. However, in the past year, the use of containers in production has become the norm. According to the 2019 Global Cloud Native Survey Report released earlier this year, 84% of respondents use containers in production. Therefore, containers are used across the world.
The survey in China shows that although China’s container usage lags behind the world, its momentum is increasing. According to the survey, nearly half (49%) of respondents use containers in production, leaping from 32% in March 2018 and 20% in November 2018.
The number of respondents planning to use containers in production decreases to 32% now, while it is 57% in March and 40% in November 2018. This means that many organizations have put the container plan into practice instead of in the planning stage. There is still room for a continuous growth.
As container application proportion in production increases, there are fewer containers used in the test environment. About 28% of Chinese respondents currently use containers in the test environment. The result shows a slight increase from 24% in March 2018, but a decrease from 42% in the November 2018.
Although containers have brought amazing advantages, they also brought certain challenges. Changes have taken place over time, but the challenge of complexity has remained unchanged. In the survey, 53% of respondents list complexity as the most significant challenge. In contrast, in the survey in March 2018, 44% of respondents believed so, accounting for the highest proportion. In November 2018, that proportion was 28%, accounting for the third place of all challenges.
In terms of challenges, security ranks second with 39% of proportion. Security is given top priority for the first time. Challenges of lacking of training and network rank third, accounting for 36%. 35% of respondents regard reliability and monitoring as deployment challenges.
As a general platform for container orchestration, Kubernetes is emerging in the industry and has rapidly increased in adoption in the CNCF community in China. 72% of respondents say they use Kubernetes in production, showing a significant increase from 40% of that in November 2018.
As a result, the number of respondents evaluating Kubernetes decreases from 42% to 17%.
The survey report also shows that the growth of Kubernetes production clusters in two deployment number section. Most respondents in China use less than 10 clusters, but the number of respondents using over 50 clusters has increased. This may be a result of the increase in the number of new respondents using containers in production.
36% of respondents have 2 to 5 clusters, higher than 25% of that in November 2018. Besides, almost 50% of respondents use 1 to 5 clusters, and 70% use 1 to 10 clusters. Only nearly 13% of respondents have more than 50 clusters in production, compared with 5% of that in November 2018.
Helm is the most popular method for packaging Kubernetes applications, with 54% of respondents choosing the Helm.
NGINX (54%) is the most popular Kubernetes entry supplier, followed by HAProxy (18%), F5 (16%), and Envoy (15%).
Managing objects in a cluster is challenging, but namespaces could facilitate management through filtering and controlling by group. 71% of respondents use namespaces to detach Kubernetes applications. From respondents with several teams using Kubernetes, 68% of them use namespaces.
For those users who use solutions of monitoring, logging and tracing, it is more common for local operations to be hosted through remote servers. 46% of respondents use local monitoring tools, while 20% run local operations through remote server hosting. Fewer respondents adopt solutions of logging and tracking, but 26% of respondents run tracing locally and 20% through remote servers. In addition, 21% of enterprises run tracing tools internally, while other 21% through remote servers.
With the support of continuous integration (CI) and continuous delivery (CD), powerful functions of cloud and containers have jointly promoted the speed of development and deployment in China. The survey quantifies the development speed by the code checking-in frequency of developers. 35% of respondents check in code for multiple times a day, 43% check in for a few times a week, and 16% check in for a few times a month.
The release frequency of most respondents (43%) is once a week, while only 21% perform releasing once a month, and 18% once a day. 12% of respondents release on a specific schedule.
Many people think that the foundation of successful CI and CD is process automation. However, the survey in China shows that there are relatively few pure automation environments. Only 21% of respondents adopt automated release cycles, while 31% rely on manual release. The hybrid mode is most popular, accounting for 46%.
CI and CD is a technology for flexible delivery and lifecycle management of cloud native systems. Jenkins is the most popular CI and CD tool in Chinese community, accounting for 53% in community and 40% in GitLab.
The use of clouds is growing. However, the survey shows great changes of public, private and hybrid clouds. Public cloud usage appears to peak at 51% in the November 2018, dropping to 36% this year. Private cloud usage remains nearly unchanged and occupies 43% in November 2018. It’s worth mentioning that hybrid cloud is the new choice this year, accounting for 39%.
CNCF manages a large number of open source projects. These projects are crucial to the development, deployment and lifecycle management of cloud native. CNCF projects are growing exponentially in China. For example, 57% of respondents use Prometheus monitoring and alarm systems, greatly increased from 16% in March 2018. 35% of respondents now use CoreDNS, while only 10% in March 2018. The Containerd also achieves amazing growth, from 3% in March 2018 to 29% in early 2019.
CNCF also hosts four projects created in China, which are more widely used in China. Among them, Dragonfly, 17% of respondents used in production and KubeEdge, 11% of respondents used in production, are the two most commonly used Sandbox Level Projects. Now both are in incubation stage. Harbor and TiKV are graduated CNCF projects, with 27% and 5% of usage respectively in production.
Since CNCF’s last survey in China, the benefits of using cloud native projects in production have changed.
• Faster deployment benefits most for the first time, mentioned by 47% of respondents.
• Improved scalability remains in the second place as before, accounting for 35%.
• Cost savings still ranks third, accounting for 33%.
• Improving developer productivity, cloud portability and higher availability tied for fourth, with a proportion of 31%. Availability ranked first and portability ranked fourth in the survey of November 2018.
In the survey in China, 36% of respondents use hosted platforms and 22% use installable software.
For enterprises that use hosted platforms, the top three suppliers are Alibaba Cloud Function Compute (FC) (46%), AWS Lambda (34%), and Tencent Cloud Serverless Cloud Function and Huawei FunctionStage (both occupy 12% of respondents).
For those users who use installable software, Kubeless ranks first, accounting for 29%, and is followed by Knative for 22% and Apache OpenWhisk for 20%.
In the 2019 survey in China, new questions about cloud native storage and service network are added. Both of them are popular cloud native projects that offer following advantages in active production environments.
The most commonly used cloud native storage projects are Ceph, accounting for 24%, Amazon Elastic Block Storage (EBS) for 23%, and Container Storage Interface (CSI) for 18%.
CNCF now has nearly 50 members in China. Evaluating by the number of contributors and submitters, China is also the third largest contributor to the CNCF projects, only behind the United States and Germany.
Here are some case studies of Chinese companies as below.
• JD.Com, Inc. has adopted Harbor as the central memory of its private images, saving about 60% of maintenance time.
• China Minsheng Bank has improved the delivery efficiency by 3 to 4 times, and its resource utilization has doubled through Kubernetes.
• Ant Financial Services Group has increased ten-fold in operations by using cloud native technologies.
Kubernetes and Cloud Native courses have been offered in China and more than 20,000 people have participated. Recently, the first China Cloud Native and Open Source Virtual Summit has been hold successfully as well.
The community in China is learning about cloud native technologies in many different ways.
72% of Chinese respondents learn about cloud native technologies through documentation. There is a large number of documentation about each CNCF project on corresponding websites. For documentation of CNCF projects, see: https://www.cncf.io/projects/
CNCF invests thousands of dollars annually to improve all aspects of project documentation, including project documentation hosting, tutorials of adding documentation, and operation guides.
Participation in activities is a popular way for respondents to learn about cloud native technologies.
41% of respondents chose KubeCon + CloudNativeCon to learn new technologies. The next virtual KubeCon + CloudNativeCon is planned to be held from November 17 to November 20.
37% of respondents chose meetups and local events, such as Cloud Native Community Groups, as the way to learn about cloud native technologies.
22% of respondents learn about cloud native technologies through technical webinars, while 8% choose business-oriented webinars, and other 8% choose CNCF virtual webinars.
CNCF has improved its webinar projects and planned to arrange regular webinars for Chinese audiences. Audiences can view the schedule, videos and slides of upcoming webinars, and playback of previous webinars through the website below https://www.cncf.io/webinars/
Thanks to everyone who participated in this survey!
The survey was conducted in October 2019 in Mandarin. 97% of the 300 respondents were from Asia.
For the source article from CNCF, see: https://www.cncf.io/blog/2020/10/13/cncf-cloud-native-survey-china-2019/
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/aixuexi-education-group-racing-against-time-through-cloud-native-practices-e9b814cfa478?source=search_post---------337,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 25, 2020·11 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Shanlie
The data from iiMedia Research shows that the market size of online education keeps increasing each year. In 2019, it was valued at more than 400 billion yuan. Driven by the COVID-19 pandemic, online education is accelerating its development, and its market is expected to be further expanded. It is estimated that the market size of China’s online education will reach 453.8 billion yuan in 2020.
The Aixuexi Education Group (formerly known as Gaosi Education) was created in 2009 and started its business by providing extracurricular training for students from primary and secondary schools. Aixuexi Education Group was originally a kindergarten through twelfth grade (K-12) education institution. In 2014, the Aixuexi Education Group joined the business market and is currently widely recognized by schools and institutions in China in the business-to-business field. Thus, it has been upgraded from a K-12 institution to a K-12 education supply platform.
In April 2019, the Aixuexi Education Group received a 140 million US dollars investment led by Warburg Pincus in the D round financing. Before 2014, as an enterprise focusing on “researching and developing education products,” Aixuexi Education Group provided personalized education products and education-related services for children aged 3–18. It has developed various online education products, such as “Siquan Chinese,” “Gaosi Mathematics,” “one-to-one Gaosi VIP,” and “Middle School Science.” It has also created many well-known sub-brands, such as “Leleketang,” “Aixuexi,” “Aishanggushi,” “Aijianzi,” and “Aitifen.” Extracurricular training providers and public schools use these products and sub-brand to improve children’s learning experience.
By 2029, the Aixuexi Education Group plans to serve 100 million students and five million teachers around the world and assist 500,000 schools. As a leading company with excellent content and technology, the Aixuexi Education Group will bring a better learning experience to students.
In the highly competitive online education market, how does the Aixuexi Education Group stand out?
In recent years, the rapid development of the online education industry has provided unprecedented convenience for the knowledge dissemination of society. Through various online education platforms, teachers can teach students remotely without the limitation of time and space. Based on streaming media transmission technology, online classes provide the same kind of experience for teachers and students since they are still having a face-to-face class. Online classes are welcomed by a vast number of users.
Streaming media transmission technology is the core technology used in the construction of online class applications. To create an excellent online class application, it is not enough to solely rely on streaming media transmission technology. Therefore, Alibaba Cloud integrated various class interaction scenarios with the application to improve user experience and learning efficiency. These interactive scenarios include asking questions, class presentation, repeating, liking, textbook navigation, and real-time display of whiteboard content. Improving the operation fluency of these interactive scenarios as much as possible is the common pursuit of technical teams in many online education platforms. It also reflects the competitiveness of online class applications. The R&D Team of Aixuexi Education Group has ultimately improved the user experience in interaction scenarios of online classes through continuous iteration of technologies.
The rapid growth of online users is a huge challenge to the R&D Team of Aixuexi Education Group. With the elastic scaling capability of cloud computing, livestreams can directly use cloud services to meet the needs of high-concurrency. However, the challenge is how to support the stable operation of these interaction scenarios. By supporting the horizontal scaling of nodes, the mature distributed microservices architecture can easily cope with a sudden surge of HTTP requests. Unfortunately, the standard HTTP communication can only handle one-way requests from the client to the server, as shown in the following diagram:
Online class interaction scenarios involve communications between different users as well as an active notification sent from servers to clients. Therefore, the following communication model needs to be established:
Take the scenario of sending text messages in class as an example. When a student submits a text message, all other users in the same class can see this message. To ensure that the content of the message does not violate any rules, the server has to review the content first. The server filters out sensitive information before sending it to other users in the online class. It is similar to the following process:
Therefore, the essence of online class interaction is the two-way communication between the server and the client. To realize better interaction scenarios in online classes, a robust, scalable, high-performance, and high-cost two-way communication mechanism needs to be established. However, the standard HTTP protocol cannot support a service scenario where the server actively sends notifications to the client. Therefore, traditional web architecture cannot meet the requirements of interaction in online classes. There are many solutions to this problem in the industry. The simplest way is the polling solution based on HTTP. Developers only have to make a few modifications to HTTP, so the client can periodically send requests to the server for getting back its notifications, as shown in the following diagram:
This solution, to some degree, can enable the server to actively send notifications to the client. However, it cannot meet requirements for real-time performance, and it generates a large number of empty polling in the case of no notification. There is another solution available besides the polling solution. The HTTP-based long polling solution improved based on general polling. The Long polling solution can avoid empty polling with limited optimization, but it cannot meet the real-time requirements of interaction scenarios.
Among all solutions based on the HTTP protocol, the WebSocket-based solution can implement the native two-way communication between the server and the client. By establishing a WebSocket connection, the server can send notifications to the client in real-time. This is also a commonly adopted solution for lightweight Instant Messages (IM) based on web pages. The R&D Team of Aixuexi Education Group has also considered using the WebSocket-based solution to implement interaction scenarios in online classes. However, after some research, they found limitations with the solution and decided against it.
In online classes, interaction scenarios have two features:
1. A single message is often sent to all clients at the same time.
The sending message scenario mentioned above is a very typical example. Messages sent by one user will be visible to all other users in the same class.
2. The number of online users is large when tens of thousands of classes start simultaneously during peak periods.
The WebSocket-based solution requires the server to establish connections with each client. If a notification needs to be sent to multiple clients, the sending logic will be implemented on the server.
When the number of users rises sharply, this architecture will put great pressure on the server, making it difficult to support a large number of online users at the same time. Therefore, the R&D Team of Aixuexi Education Group decided to abandon the WebSocket-based solution and try other ways to support interaction scenarios in online classes.
The implementation of the application-layer communication protocol based on TCP or UDP is also a solution that has been studied in depth by the R&D Team of Aixuexi Education Group. This flexible solution can expand the message transmission through customized protocols. To achieve this, the R&D Team of Aixuexi Education Group has to design an application-layer communication protocol, which is an extremely complex task with various challenges, such as:
These are all factors the R&D Team should consider. The R&D Team of Aixuexi Education Group is capable of handling these details to design a communication protocol suitable for their business scenarios. However, time costs and risks are very high. With the rapid development of business, the R&D Team needs to race against time to find a new solution for interaction scenarios in online classes to support massive amounts of online users.
Through the experience accumulated by continuous technology research, the R&D Team of Aixuexi Education Group has reached a conclusion. To meet the needs of interaction in online classes, technical architecture must be able to meet the following requirements:
Among them, the best solution to meet the fourth and fifth requirements is implemented based on the Message Queuing Telemetry Transport (MQTT) protocol. The MQTT protocol is a client-server-based messaging and subscription transport protocol that provides reliable network services for devices in low bandwidth and unstable network environments. The MQTT protocol is open, simple, lightweight, and easy to use. It supports one-to-many messaging to decouple applications. It also ensures message reachability, which is very suitable for sending notifications to the mobile client.
However, MQTT is just a protocol. Before applying the MQTT protocol to large-scale commercial scenarios, it needs support from mature and stable products. None of the MQTT solutions from the open-source community have gone through rigorous testing and commercialization. When the number of clients reaches 1,000, their performance sharply declines. Therefore, they cannot support tens of thousands of online clients at the same time.
Focusing on interaction scenarios in online classes, the R&D Team of Aixuexi Education Group has made in-depth exchanges with technical experts from Alibaba Cloud. After several rounds of testing and evaluation, the R&D Team finally decided to use Alibaba Cloud’s micro-Message Queue (mMQ) for MQTT to build its interaction platform for online classes. Compared with open-source solutions, Alibaba Cloud’s mMQ for MQTT has been tested and evolved within Alibaba Group. It supports tens of millions of online connections, millions of concurrent messages, and notification sending at the millisecond level. It is designed with a distributed architecture. With no Single Point of Failure (SPOF) and infinite horizontal scalability between components, it ensures the elastic scalability of capacity and transparency to users.
In this architecture, clients on the Internet access Alibaba Cloud’s mMQ for MQTT through the standard MQTT protocol. The SDK of the MQTT protocol covers almost all mainstream development languages and can be adapted to the unstable network of the mobile client. Server cluster on the cloud accesses Message Queue (MQ) for RocketMQ through the RocketMQ protocol and realizes two-way interconnections between servers and clients through protocol conversion between RocketMQ and MQTT.
Why was a new component, MQ for RocketMQ, introduced? Why are server instances accessed to MQ for RocketMQ through RocketMQ protocol? There are three main reasons:
1. Compared with client instances, server instances are smaller in scale, while the message throughput of a single server instance is larger than that of the client.
In a typical online class scenario, tens of thousands (or hundreds of thousands) of clients are connected to the server at the same time. Each client should send and receive no more than ten messages per second. However, each server instance may process tens of thousands of messages per second in a cluster with 100 instances. The difference between the server and the client determines that they need to use different communication protocols for access to maximize their performance and efficiency.
2. When the processing capability of the server is insufficient, messages need to be temporarily stored in the queue.
The introduction of RocketMQ provides message storage for MQTT.
3. Multiple instances of the server cluster imply peer-to-peer and task-allocation relationships. The RocketMQ-based cluster consumption mode can provide the native load balancing mechanism.
Let’s come back to the message sending scenario in online classes. After the message is submitted, an instance in the server cluster receives the message through the load balancing mechanism. After reviewing, the message can be distributed to other users in the same class by delivering it to the MQTT-based cloud service. Thus, for server instances, they only need to establish connections with the MQTT-based cloud service for serving tens of thousands of users at the same time.
When the server instance encounters a performance bottleneck during peak hours, this issue can be solved by adding more server instances. For MQTT-based cloud services, the performance improvement of it can be achieved by upgrading configurations according to rules, which has little influence on the operation of the application.
Based on this architecture, the R&D Team of the Aixuexi Education Group built a complete class interaction system in half a month. The R&D Team does not need to focus on complex technical issues at the application layer, such as bad network environments, reconnection, exception handling, high concurrency, and high system availability. This architecture helps the team greatly reduce development costs and improve user experience.
To support the rapid increase of users brought by the “On-Cloud Learning” program, Aixuexi Education Group has expanded the capacity of this system several times. As a result, it has successfully resisted several traffic peaks and ensured the stable operation of the business.
Note: In response to China’s “classes have stopped, but learning will continue” proposition from the Ministry of Education of China, and to transform from offline teaching to online teaching in time, the Aixuexi Education Group has launched the “Learning on the Cloud” program. The Aixuexi Education Group has opened up high-quality content and provided livestream capabilities to help local K-12 institutions migrate the business to the cloud at the same time. More than 9,000 institutions teach online through the Aixuexi Education Group’s online platform. The Aixuexi Education Group is helping more institutions smoothly transform and promote the “On-Cloud Learning” program together.
The “On-Cloud Learning” program has been highly recognized in China. At the same time, the R&D Team of Aixuexi Education Group also continues to iterate the system architecture and apply MQTT technology to more two-way communication scenarios between devices and the cloud. By doing so, the Aixuexi Education Group can solve more challenges in the future. Li Chuan, Co-Founder and CEO of Aixuexi Education Group, said, “We would like to do something more valuable, and we have better expectations for the future.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/learn-how-cloud-native-securitys-enabling-online-learning-businesses-to-accomplish-protection-in-174bef7dd5c4?source=search_post---------338,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 29, 2020·8 min read
Bolster the growth and digital transformation of your business amid the outbreak through the Anti COVID-19 SME Enablement Program. Get a $300 coupon package for all new SME customers or a $500 coupon for paying customers.
By Alibaba Cloud Security.
With the current pandemic, distance teaching and e-learning has become the new normal. Teachers and students across China and all over the world have begun to go online. However, with this change, also followed several challenges, of which fierce competition and a greater degree of threats to security were two major problems.
Consider this situation. Company A is an education group that has a global business presence. On its first day responding to China’s education authority’s call for “suspension of classes but not of teaching and learning”, a malicious attack was launched through massive waves of “zombie bots” on the company’s teaching platform paralyzed its business and prevented a large number of users from accessing the platform.
As a result of a massive leak of core data from Company B, another online education brand, all the parents who signed up for Company B’s trial courses for their children received marketing SMS messages and phone calls from Company C, and many of Company B’s prospects were poached by its competitor who offered a lower price. In addition, due to the data leak, Company B received complaints from many parents.
With the epidemic in full swing, teachers and students across China went online. This quickly brought the online education industry to the attention of the whole of society. Meanwhile, keeping pace with the mounting public attention led to escalating security challenges.
Online education and e-learning businesses have become prime targets for hackers. Hackers are driven by profits and go wherever they can find an opportunity. When public attention began to shift to online education and huge amounts of high-quality data started flowing into online education companies, hackers naturally saw these companies as prime targets. Common attack methods included the following:
Besides these, malicious competitors may even place malicious ads, hidden links, malicious documents and the like on a company’s website by tampering the source code, for the purpose of damaging the victim company’s reputation.
This outbreak has brought the online education industry into the limelight and has put the security efforts of many companies and organizations to an unprecedented test. The major concerns are as follows.
First, core data is “exposed” on the Internet. Generally, online education companies provide services through websites or apps. However, most companies lack effective website vulnerability discovery mechanisms and intrusion prevention measures to protect their systems from being intruded by hackers. Moreover, many of these companies may have no idea that their loophole-ridden security systems may expose their core data assets on the Internet, leaving behind hundreds of web vulnerabilities that can be easily exploited, and let data be stolen unknowingly.
Second, the design defects of business systems are left unattended. Many online education companies are running business systems that are flawed in design, making it easy to obtain server permissions. However, most education service companies find it difficult to defend themselves effectively against hacker attacks, because they neither hire an in-house security team nor know how to thoroughly check for system vulnerabilities and security design defects.
Given these issues, therefore, attacks occur frequently but no protection measures are taken. Some companies have implemented no protection measures against frequent hacker attacks. In particular, they are quite helpless in the face of zero-day vulnerabilities.
Their security management and security awareness are falling short. Today, as online education is taking off, most companies and organizations in the business focus on how to quickly acquire customers, but neglect the need to develop security capabilities. Some companies have their O&M personnel undertake the concurrent responsibility of security operation. Some are even not in the least concerned about security.
The boundaries of cloud security responsibility are not clear. Some online education companies lack a clear understanding of their security responsibilities after migrating their business to the cloud. They live under the misconception that “buying cloud services means the cloud service provider is obliged to ensure security and will be held liable for all security matters.” However, the fact is, both cloud service providers and users have their respective security responsibilities. In other words, the fundamental security guarantee on the cloud platform alone is far from enough, so users need to be responsible for the security of their cloud systems and businesses as well.
Today, these concerns are still widespread in many businesses. What is more terrifying than a hacker attack is not knowing how to defend oneself and having no clue about the fact that one has been attacked.
Alibaba Cloud has developed a complete set of security solutions to address the security challenges in the online education industry.
Traditional security solutions mainly solve various security concerns by using plug-in devices. However, in the new cloud environment, plug-in devices pose many compatibility risks. It has been proven, more than ever, that more efficient, stabler, and more secure solutions are needed to replace the simple approach of transforming plug-in devices into software installed on virtual machines in the cloud. What businesses need is a set of native security solutions “grown” in the cloud.
Cloud-native security solutions differ from traditional plug-in solutions in the following aspects:
The following describes the types of cloud-native technologies and products included in the cloud-native security solutions provided by Alibaba Cloud and the issues that they can address.
Even for enterprises without security professionals, these capabilities can be easily built in minutes. This is also one of the greatest advantages of cloud-native security. Cloud-native is so convenient.
Take an online education company as an example. TAL Education Group has many education brands that vary in forms of education, customer base, and operation models. Centralized security control across all business platforms is one of the major challenges in building security capabilities. After being tested and verified, Alibaba Cloud’s host security product, Security Center, became TAL’s choice for centralized server security control. In the meantime, the solution also integrates the capabilities of Cloud Security Scanner to regularly verify vulnerabilities and monitor content security for web services on all of its business platforms, in a bid to build a sound security detection and protection system from the underlying servers to the upper-layer business platforms.
Currently, Alibaba Cloud’s online education security solution has more than 100 customers. During China’s battle against the coronavirus outbreak, our solution has ensured that hundreds of millions of teachers and students can safely and smoothly teach and learn online and puts the principle of “suspension of classes but non-stop learning” to practice.
While continuing to wage war against the worldwide outbreak, Alibaba Cloud will play its part and will do all it can to help others in their battles with the coronavirus. Learn how we can support your business continuity at https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-recognized-as-a-top-hosted-serverless-platform-in-cncf-cloud-native-survey-china-2019-3c9403978b4a?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 1, 2020·4 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Alibaba Cloud Serverless
Recently, the world’s leading open source organization Cloud Native Computing Foundation (CNCF) officially released CNCF Cloud Native Survey China 2019. According to the report, Alibaba Cloud’s Function Compute ranks first in China with a 46% share in enterprise service markets that use Serverless as a hosting platform. Alibaba Cloud has long been a leader in Serverless in China. The first 2020 User Survey on China Cloud Native released in the same month by China Academy of Information and Communications Technology (CAICT). The survey shows that Alibaba Cloud Serverless is considered as the first choice by 66% Chinese Serverless users.
With the arrival of digital transformation, cloud computing is seen as the key to realizing digital enhancement and innovative business. Although cloud computing is common in the information era, it is not really possible to be ready to use and billing flexible in real practice. Serverless is the third generation of general computing technology after virtual machines and containers. Compared with traditional back-end architectures, Serverless has many advantages, such as O&M-free for cost savings, fast deployment and delivery, and flexibility. Next decade will be the era for cloud computing to truly become infrastructure in information age. Although Serverless is the trend of the times, there are still many challenges in Serverless deployment.
According to 2020 User Survey on China Cloud Native by CAICT, serverless has been applied in the production environments with nearly 30% of users, but only 16% of users use serverless for the production of their core businesses. In a recent statistical report from O ‘Reilly, a leading market research firm, shows that 60% of organizations that have not adopted serverless cited “security concerns” as the chief reason they have avoided serverless.
In addition to that, most businesses face the following four challenges when adopting a serverless approach. First, existing applications cannot be migrated smoothly with its high entrance threshold. It is costly and risky to migrate to business system for complex enterprises. Second, it is lack of a unified open source standard due to vendor locking. Most Serverless applications still rely on the non-standard Backend as a Service (BaaS). Third, the tool chain is incomplete. Compared with traditional application development, Serverless applications do not provide proper debugging and monitoring tools. Fourth, learning costs are high. It is hard for developers to change their thinking mode from “O&M for fixed IP addresses” to “O&M for the final state of applications”.
To address the challenges above, Alibaba Cloud strives to perfect its product services and improve its developer ecosystems. First, the Serverless product matrix has been established, including container instance-oriented Elastic Container Instance (ECI), container orchestration-oriented Serverless Kubernetes (ASK), application-oriented Serverless App Engine (SAE), and function-oriented Function Compute (FC). This matrix reduces the threshold for enterprises to adapt Serverless, and quickly implements to migrate to inventory application smoothly in complex business system. Second, through open-source projects, Alibaba Cloud Serverless improves the developer ecosystem, supports mainstream frameworks, and provides experience-level specifications to reduce the impact of vendor locking. Third, a simpler and more convenient Serverless tool chain system has been provided for developers to improve experience and reduce learning costs. Fourth, Alibaba Cloud builds a Serverless platform for everyone to construct called Serverless Devs. The platform reduces the learning cost, making Serverless available to most people.
To promote the development of Serverless technology for all industries, Alibaba Cloud has been one of the earliest cloud service vendors in function computing since 2016. In 2017, Alibaba Cloud launched the first Serverless and FaaS product that supports millisecond-level elastic scaling in China. The Network Attached Storage (NAS) for function computing was released in 2018, which is the first Serverless product in the world that supports the sharing of read and write capabilities between functions in local file systems. In 2019, function computing 2.0 was launched, providing pioneering product capabilities including reserved instances, single instance with multi-concurrency, custom runtime, and subscription. Among these capabilities, the excellent multi-concurrency capability is still the only one in China until now. In 2020, function computing launched many exclusive products features in container images, performance instance, link tracking, and asynchronous configuration. In the same year, Alibaba Cloud obtained the certification of Trusted Cloud Service with full marks and developed the first open-source developer platform, Serverless Devs. Based on the powerful cloud ecosystem capabilities of Alibaba Cloud, function computing has integrated 15 Alibaba Cloud products. Moreover, by its unique event-driven features, Alibaba Cloud helps customer processes complex cloud businesses in a simple and fast way.
As a leader of Serverless technology and products in China, Alibaba Cloud possesses much experience in implementation practices, covering all industry scenarios in e-commerce, retail, Internet of Things (IoT), audio and video, artificial intelligence (AI). In addition, Serverless supports thousands of enterprises all over the world, such as Sina Weibo, Shimo.im, Century Mart, Mango TV, and GSX Techedu Inc., to achieve cost reduction and efficiency improvement. During Double 11, Alibaba Cloud Serverless stably supported internal businesses of Alibaba Group, including Taobao, Tmall, Xianyu, Ant Financial, DingTalk, and 1688 ,with tens of millions of queries per second (QPS) peak. Furthermore, the Serverless successfully achieved the first large-scale implementation in the core business in China in 2020 Double 11.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-launches-a-series-of-cloud-native-databases-at-the-apsara-conference-2020-c3e2eb9a1dc9?source=search_post---------341,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 21, 2020·5 min read
Catch the replay of the Apsara Conference 2020 at this link!
By ApsaraDB
At the Apsara Conference 2020 held on September 18, Li Feifei, Vice President of Alibaba Group, Chief Database Scientist of Alibaba DAMO Academy, and Head of Database Business Group of Alibaba Cloud Intelligence, officially announced the release and upgrading of PolarDB-X, AnalyticDB, Data Lake Analytics (DLA), Lindorm. Since then, Alibaba Cloud has officially upgraded its databases to the era of cloud-native databases.
In the field of data analysis, traditional user-created analysis systems face a series of challenges, such as poor scalability, high construction costs, and complex systems. Users cannot give full play to the value of data.
Alibaba Cloud continues to explore and upgrade AnalyticDB. AnalyticDB has the features of auto scaling, massive storage, and online and offline integration, and is fully compatible with the database ecosystem. AnalyticDB makes big data easy for developers, helping enterprises easily explore and use the value of data.
AnalyticDB for MySQL was upgraded and you can use the features of time-sharing elasticity of computing resources, tiered storage of cold and hot data, and resource group isolation. It meets users’ demands for resources for differentiated workloads, effectively meets data analysis performance, and greatly reduces enterprise data analysis costs.
The multi-master feature and self-developed laser engine of AnalyticDB for PostgreSQL were released. The multi-master feature supports linear scaling of concurrency to easily handle high-concurrency scenarios. The Laser engine developed by Alibaba Cloud doubles the performance of the native Greenplum engine, ensuring real-time analysis and computing.
In this year’s TPC-H and TPC-DS the most authoritative database performance benchmark tests, AnalyticDB set two new performance and cost-effectiveness records and won the top prize. At the Zhejiang Science and Technology Awards, AnalyticDB won the Scientific and Technological Progress Award due to its excellent performance and its important role in the transformation of traditional data warehouses.
To help enterprises quickly build a data lake, the end-to-end data lake analytics services of Data Lake Analytics) (DLA) were released. DLA is an end-to-end data lake management solution, and this launch ushers in a new era for serverless data lake management, analysis, and computing. DLA supports automatic metadata discovery, management, and updates. It provides the full incremental data lake creation and end-to-end serverless computing and analysis. The new Serverless Spark feature is more cost-effective than user-created systems, which now has a 300% percent increase in cost-effectiveness in data analysis.
To support users’ demands for high-concurrency transactions and complex online queries, PolarDB-X upgraded and released two new enterprise features: HTAP (Hybrid Transaction and Analytical Process) and global secondary indexes (GSIs.) This service is based on cloud-native technology and focuses on solving the bottlenecks of online databases
Based on the cloud-native architecture that decouples computing from storage, PolarDB-X can support tens of millions of concurrent requests and PB-level storage. The new HTAP feature covers highly concurrent real-time transactions and some online data analysis and processing scenarios at the same time, ensuring data consistency and real-time performance in complex online queries. Compared with traditional centralized databases, the efficiency of online transactions and complex online queries can be improved up to 10 times.
PolarDB-X provides another new feature, GSIs. This feature supports multidimensional sharding and transparent distributed sharding. It can meet the requirement of sharding queries of different dimensions. The overall processing efficiency is improved by a hundred times compared with the previous feature, which reduces the database usage costs.
In addition, PolarDB for MySQL is the only cloud-native relational database in the world that supports all MySQL “active” versions (5.6, 5.7, and 8.0.) With the global database network (GDN), you can use implement global system deployment with zero code modifications. With the “computing package” and “storage package” capabilities, ApsaraDB for PolarDB allows you to achieve a balanced economy and flexibility, you can enjoy the flexible resource usage experience like water and electricity billing.
To better meet the requirements of diversified users, Alibaba Cloud has launched a solution to migrate databases to the cloud that is optimized for enterprise customers’ ApsaraDB for MyBase. It supports database engines, such as MySQL, PostgreSQL, SQL Server, and Redis. Based on existing cloud database services, ApsaraDB for MyBase provides the advantages of low-cost and self-maintenance. ApsaraDB for MyBase provides services, such as cloud resource exclusive, resource overallocation, resource scheduling, opening up some databases, and Operating System (OS) permissions.
As an important part of digital transformation, databases have entered the cloud-native distributed era. The cloud-native distributed database matrix fully utilizes the elastic and distributed advantages of the cloud platform. Based on resource pooling, elastic expansion, intelligent O & M, and online and offline integration, it meets the core requirements of enterprises for burst traffic, cost reduction, and efficiency improvement, and helps users achieve digital transformation and upgrade in a more agile, intelligent, and cost-effective way.
It is understood that the ApsaraDB users cover many leading enterprises in the fields of finance, manufacturing, retail, aviation, logistics, government affairs, including China Post, China Southern Airlines, Haier, and Midea. ApsaraDB helps customers achieve digital transformation and upgrades. Currently, Alibaba Cloud ranks first in the ApsaraDB market in the Asia Pacific. More than 100,000 enterprise users choose ApsaraDB services, and more than 400,000 database instances have been migrated to Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-can-xianyu-achieve-trillion-level-gmv-transaction-with-cloud-native-e2d835a88d7d?source=search_post---------342,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 1, 2020·14 min read
Step up the digitalization of your business with Alibaba Cloud 2020 Double 11 Big Sale! Get new user coupons and explore over 16 free trials, 30+ bestselling products, and 6+ solutions for all your needs!
By Wang Shubin, Head of Alibaba Xianyu ArchitectureContributed by Alibaba Cloud Serverless
On June 28, 2014, a team of 28 people, after working day and night in a tea room in Hangzhou for three months, launched a secondhand trading platform called Xianyu, also known as Idle Fish. In May this year, Alibaba released the data of Xianyu in its annual report, including 200 billion yuan in gross merchandise volume (GMV), which is an increase of 100% year-on-year, with more than 30 million active sellers per day. In just 6 years, Xianyu has grown from an ordinary business product to a leading C2C platform in China.
According to iiMedia Data, the transaction volume of the second-hand goods trading market in 2020 will reach more than a trillion yuan. In order to maintain the growth of this industry, businesses need to constantly adjust and evolve the technical architecture to support the rapid development of their business. For Alibaba, Xianyu represents innovation, which is more than just bringing revenue to the company. Xianyu is innovative not only in its business model, but also in its exploration of technical architecture, that is, towards Flutter, cloud native, and Serverless.
In 2009, Wang Shubin, who worked for UTStarcom for three years after graduation from Zhejiang University, joined Alibaba. In 2017, he introduced Flutter to Xianyu. In 2018, Wang started to lead his Xianyu technical team into a bigger game in business: to explore Serverless architecture. Disruptive innovations often take place at margin places. For Xianyu, adopting cloud native technologies or Serverless is a brand new path. However, the success of Xianyu will be of a valuable example for many companies of online transactions.
Now, let’s get to the story between cloud native and Xianyu.
As a frontend business that relies on the Alibaba e-commerce system, Xianyu has unique business features and user requests. While relying on Alibaba system at the bottom layer, Xianyu needs a more suitable, rapid, and flexible R&D system at the presentation and business layers.
The IT system of Xianyu, if developed in its original way, will face many challenges, such as:
With the emergence of Serverless, the cloud-end integrated R&D becomes possible, greatly reducing the collaboration cost required by small business. In addition, compared microservices, Serverless provides a more reasonable way to split up huge applications in the business glue layer.
The following triangle shows the mutual restrictions of the cost (speed), stability, and quality of traditional huge applications.
The emergence of new technologies such as cloud native and Serverless can submerge O&M capabilities of applications. Thus, the problems of mutual restrictions of the cost (speed), stability, and quality of traditional huge applications can be broken. In the process of implementing new technologies, Xianyu first focused on developing the hybrid engineering system and the high-performance component library of Flutter. Then, it focused on the cloud-end integrated R&D system and the service-side architecture of business assembly layer based on Serverless.
The Xianyu client performs architecture evolution and innovation based on Flutter. After improving the R&D efficiency by unifying Android and iOS systems with Flutter, Xianyu hopes to solve many collaboration problems among different roles through the combination of Flutter and Serverless. It is those problems that lead to low overall R&D efficiency and cause mobile ends to get further away from business. As a result, developers have no time for underlying field improvement on the service side. By introducing Serverless, the overall R&D efficiency of Xianyu will be obviously improved.
In 2018, Xianyu technical team began to explore Serverless, which can be divided into four stages of self-building the Dart server, relying on the Functions as a Service (FaaS) platform, realizing cloud-end integration, and achieving Serverless-based traditional huge applications.
In May 2018, the Dart Server framework with 2-second cold boot was built based on Serverless, which was designed for lightweight development of the service-side business glue layer.
From the end of 2018 to the beginning of 2019, Xianyu launched a project in collaboration with a Gaia team to build Dart Runtime based on the Gaia platform. Some services have already been released online. Note: Gaia is a FaaS platform for Taobao business, which is packaged based on Alibaba Cloud and features of Taobao’s business.
In 2019, Xianyu, based on the Dart Runtime, explored the on-cloud integrated programming with the combination of Flutter and FaaS, as well as metadata-directed domain interfaces. At last, the architecture of glue layer, such as Nexus, came into being and was implemented in more than 20 business of Xianyu.
In 2020, Xianyu began to integrate engineering and tools in the cloud, with the goal of realizing multiple deployments of single engineering. At present, Wang is working with the technical team to solve governance issues of traditional huge applications in the glue layer. They are also integrating huge applications with Serverless. “We will make everything right in 3 to 6 months.” said Wang.
Specifically, the practical results of Xianyu by adopting Serverless over the past two years can be shown in following five aspects:
This framework aims to unify the programming models of Flutter and FaaS, and to integrate user interface (UI), interaction, data, and logic. Wang mentioned that when they decided to conduct the integration of Flutter and FaaS, they only had a vague understanding of the word “integration”. All they knew was that Dart can be used to write FaaS functions, which was still the language-level integration. In terms of capabilities of FaaS, they only knew few of FaaS capabilities in the backend for frontend (BFF) level, which was implemented for a long time in the frontend.
It took them quite a long time to realize that under the Dart ecosystem, the frontend FaaS was actually not efficient in R&D delivery. There are two major problems in the R&D phase:
The programming language is not unified. Although the programming language itself is not the biggest obstacle, it does set a lot of thresholds to frontend development. What’s more, the ecology, environment, and system behind the language are more difficult and complex for frontend developers.
The development mode and architecture are separated, and the environment is complex. The end-side project and the FaaS side project are independent. Both of them have their own tool chains for building, debugging, integrating, and publishing. In addition, FaaS also has its own supporting environment, runtime, and framework. Faced with such a complex FaaS R&D environment and dual R&D workflows, developers can hardly achieve efficient delivery.
After discussion and understanding, Wang and his team finally have a clear consensus on integration, that is, to achieve integrations of two key points:
• Language integration• Integration of development mode and architecture
The integration of programming languages can provide developers with a technology stack that they are familiar with. The integration of development modes and architectures can help developers solve the problems of separated projects and handle the complicated local running environment of FaaS. By doing so, developers can have the same experience as that of original R&D model.
The purpose of above two integrations is to minimize the gap between the development of Flutter page and FaaS. For example, the client Flutter of Xianyu used to be developed under the Redux framework. Now, with the Nexus API framework, Redux and FaaS calls can be seamlessly integrated.
In cloud-based integrated development, some details of FaaS development are shielded by using command line interface (CLI). Thus, the development experience of FaaS development is standardized, meeting the local development practices of the client developers.
Over the past two years, we have been simplifying the basic service capabilities, such as object storage, messaging, and search. At the same time, a metadata center for business domain layer services is built. These simplified basic service capabilities, together with the existing business domain layer services, allow the client developers to quickly assemble different services.
After the successful introduction of Flutter, Xianyu has formed a cross-end R&D system at the end side, with Flutter as the main part and H5 as the auxiliary. Thus, Xianyu integrates the traditional R&D on Android and iOS. When the productivity on the end is released, the end developers have the opportunity to move a little bit deeper to the lower layer. Thus, the simple data assembly-oriented logic on the service side can be completed by end developers in a closed loop. This model is especially suitable for some small business demands. Similar attempts have already been made in the industry, as you can see from the popularity of GraphQL frameworks and the formation of the frontend BFF layer. However, with Serverless, the development of lightweight code on the service side can be greatly simplified. That is why Xianyu chooses to promote cloud-based integration at this time.
Cloud-based integration concerns cloud-based programming frameworks, tool chains, engineering systems, BaaS-based basic services, and submerging of domain services. It also involves organizational support, labor remodeling, and production safety training.
Serverless is not a silver bullet, but it magically matches the features of the business glue layer. It is very suitable for splitting up traditional huge applications of the glue layer. As such, how to bring it into practice is the next challenge that Xianyu is tackling.
It is not easy for Xianyu to implement a Serverless transformation. Wang mentioned that during Serverless cloud-based integration, the team was confronted with some technical difficulties. They had to solve problems such as the heterogeneous language access of Java rich clients, the unifying of open environments, and a lack of familiarity with domain interfaces among client developers.
The Java system of Xianyu involves a large number of Java rich client applications. For the problem of heterogeneous language access of Java rich clients, Xianyu establishes Java proxy in Sidecar mode to solve this problem.
Then, Xianyu develops its own CLI tool (namely, GCLI) to unify the development environment. GCLI is a command-line tool that supports the FaaS R&D lifecycle. It defines the closed loop of Xianyu FaaS development, unifies the FaaS R&D environment, and serves as a powerful tool to improve the FaaS R&D efficiency. GCLI disassembles the R&D loop into development commands that are suitable for Serverless R&D. To allow users to apply their R&D habits and tools, Xianyu chooses the local-based development solution. Xianyu also adopts the Docker technology to unify the development environment and declares the running environment in Dcoker (software plus configuration) on which the Dart FaaS technology stack depends. With the container technology, the FaaS software environment can be transplanted to any operating system that supports Linux operation. Thus, the problem of how to unify environments is solved. Moreover, GCLI realizes the interoperability between local and functional platforms through FaaS OpenAPIs, forming a complete R&D loop.
In addition, to solve the problem that client developers are unfamiliar with domain interfaces, Xianyu develops the domain layer metadata center.
Cloud-end integration has reshaped the traditional boundaries between the cloud and ends, reduced collaboration, and made the labor division more flexible. In addition, it significantly improves the R&D efficiency and quality of technologies. These changes directly benefit the business in making the business iterate and adapt to the changes of market and user demands more quickly.
Cloud-end integration is currently applied in both interaction-centered and lightweight business scenarios of Xianyu. The improvement of technical R&D efficiency and quality can be easily presented in the form of quantitative data. For example, based on the sampling statistics of typical large and medium-sized business demands, the requirement on personnel and time of development decreases by 30%, and the bug rate per 1,000 lines of code reduces by 20%. The improvement will be more obvious if scattered demand statistics are used. In the past, it often took several weeks to solve a small business demand due to the high requirement on the developer number. However, thanks to the cloud-end integration, the flexibility of resources is significantly enhanced, greatly improving the demand response speed.
“However, there are still some problems to be solved.” said Wang. In terms of splitting up huge Serverless applications, Xianyu has encountered more serious problems, such as:
The solutions for these problems are still being verified. We will share them with you as soon as they are proven useful.
Which company, application, or scenario should choose Serverless architecture? At present, there is no specific explanation. The key is to think through the situation, which means to balance the benefit, cost, efficiency, and the ability to respond to the market. Among them, the cost is the factor that requires more attention from enterprises, which includes the cost of infrastructure construction, O&M, expansion, and security.
Netflix is a successful example of adopting Serverless. Netflix is always innovative in designing products. In addition to continuous A/B testing, many new features are released every week. In order to achieve such wonderful work results, an API service platform is needed to help client developers quickly and effectively deploy the modification requirements to the service layer. To do so, FaaS abstracts all platform components related to services into business logic. Serverless, on the contrary, provides a platform for Netflix, so that engineers even without server or operation experience can develop highly available services.
The adoption of FaaS is essentially the customization of transaction speed and possibility. The FaaS service performance of some applications is great, like Netflix API. Netflix runs relatively unified microservices and only needs to access and modify the data of downstream services. However, if services require to be customized, for example, changing the components of a service platform, such as Remote Procedure Call (RPC), data access, caching, and authentication, the FaaS mode may not be flexible enough for these services.
Self-built Serverless platform has higher requirements on IT personnel of enterprises, and the construction cost is also high. In addition, the mature service ecology is required to adopt Serverless. In most cases, enterprises that are already in the cloud should give priority to Serverless products of cloud service providers. Enterprises that are not in the cloud should consider whether the ecology of the existing system can be compatible with Serverless products of their potential cloud service providers.
For the selection of Serverless products, it is suggested to consider the following aspects: ecological maturity, supported development languages, feature richness, and charging standards. The key is to consider the needs of the enterprise’s own business development.
O’Reilly once conducted a survey on the application of Serverless. The survey showed that Serverless received concerns from and was adopted by developers in the software industry mostly, which was not a surprise. However, the financial and banking industries are also paying close attention to Serverless. One of the reasons is that more and more financial technology start-ups are founded. These start-ups with traditional architectures are accepting and embracing Serverless with more opened altitude.
As for the reason for refusing Serverless, 60% of the interviewees said they were worrying about security problem. Many industries have high security requirements for IT environments, and the adoption of any new technology may bring security risks.
In addition, developers also worried about being bound to providers. As a result, organizations in certain sizes start to build their own Serverless platforms based on open-source solutions such as Knative. Once an open-source solution becomes the mainstream, cloud providers will take the initiative to provide services that are compatible with open source standards and increase their investment in the open source community.
Serverless not only affects technologies and business services, but also imposes new requirements on the organizational structures of enterprises and technicians.
Firstly, Serverless has changed the communication structure. According to Conway’s law, the organizational structure should align with the new communication structure. In the past, personnel concerning the client and service side in Xianyu are different. Now, under the brand-new Flutter and Serverless architecture, the organizational structure needs to be adjusted accordingly. After discussion, Xianyu decided to regroup the developers of the client and service side according to the business line.
Secondly, Serverless offers the client developers more opportunities to learn about the business. Therefore, client developers must be more sensitive to the business than before. At the same time, Serverless helps client developers expand their technical boundaries, so developers also need to better understanding certain concepts of the service-side development.
Finally, Serverless requires the original service-side developers to have better data modeling and domain modeling capabilities, so that higher reusability of the underlying interface can be achieved.
In the beginning, Xianyu was not favored by the public and even ridiculed as “salted fish” (someone with no achievement or prospect). But now, Xianyu has tens of millions of daily active users (DAU) and has revitalized a market worth at trillions. The emergence of Xianyu has greatly influenced both the frontend e-commerce ecology and user lifestyle on the Internet.
To support the trillion-yuan-level transaction, Wang and his technical team are racing to transform traditional huge applications through Serverless. Wang says that “I will be satisfied if we can totally apply Serverless.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-cloud-nas-the-one-container-solution-for-cloud-native-technology-8c8b815128bf?source=search_post---------343,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 7, 2021·9 min read
By Meng Wei
Today, more applications are becoming cloud native, and storage solutions are following their lead. Containers are the infrastructure of the cloud-native era, but what is the infrastructure of the container technology?
“Cloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments, such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructures, and declarative APIs exemplify this approach. These techniques enable us to build loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, cloud-native technology allows engineers to make high-impact changes frequently, predictably, and effortlessly.” This is the definition of cloud native provided by the Cloud Native Computing Foundation (CNCF).
Kubernetes, a platform that orchestrates network, storage, and computing, has become the operating system for cloud-native technology. Featuring a novel interface, it simplifies operation and maintenance, improves the elasticity of resources, allows for use on demand, and lowers the costs for users. Cloud native has been embraced by enterprises and developers. Forrester predicts that the percentage of global organizations and companies that run containerized applications in production environments will increase significantly from less than 30% today to more than 75% by 2022. The trend of containerized applications in the business world is unstoppable.
The proportion of container applications in the production environment has been rising rapidly, from 23% in 2016 when the survey was first conducted, to 73% in 2018, and 84% in 2020.
Cloud-native applications are leading the transition to the cloud-native architecture in various application fields and have profoundly changed every aspect of application services. Essential to run any application, storage solutions are also faced with new requirements posed by cloud-native services. To suit the characteristics of cloud native, cloud-native storage has been substantially optimized in terms of its core capabilities, including availability, stability, scalability, and performance.
Alibaba Cloud, the cloud service provider in China, offers a wide variety of cloud-native services. Unlike network and computing, storage must be seamlessly connected to Kubernetes for orchestration and scheduling. To this end, Container Storage Interface (CSI) was released as the universal protocol to seamlessly integrate storage with Kubernetes. This article discusses the challenges cloud-native containers pose to storage. To keep pace with cloud-native technology and containers, Alibaba Cloud has been adapting and evolving its file storage solution Alibaba Cloud NAS. Now, it can effectively cope with the challenges of cloud-native storage and has become the natural choice for container storage.
To address the various performance, elasticity, high availability, security, and lifecycle challenges arising from containerization and the cloud migration of new workloads, we must not only improve storage services, but also improve cloud-native controls and data in a way that can promote the technological evolution of cloud-native storage. Now, let’s walk through these challenges.
Cloud-native applications are widely used in scenarios related to big data analysis and AI, which have demanding requirements for storage throughput and IOPS. In a scenario where the container clusters process massive data volume, launch thousands of pods at the same time, or add a large number of pods to read and write to the shared file system, the heavy workloads will increase latency, introduce high-latency glitches, and undermine read and write stability. In addition, the characteristics of cloud-native applications, such as rapid resizing and elastic scaling, will also test the ability of the storage service to cope with traffic peaks over a short span of time.
The elasticity of cloud-native technology poses new challenges to storage solutions. As a result of the diverse development of cloud-native services, databases and metadata management applications can be scaled out online, but local storage cannot be scaled out elastically.
In application and system O&M scenarios, a storage solution needs to meet stability and high availability requirements as it migrates along with containers.
In scenarios that require isolation among containers, a storage solution must cope with security challenges, such as multi-application sharing, capacity coordination of file systems in multi-tenant environments, permission control for shared access to cluster-level file systems, and end-to-end data encryption for user applications.
The storage of persistent data in massive container clusters needs to address the challenges arising from cold and hot data separation and storage costs.
In the preceding emerging computing scenarios, the challenges posed to storage in terms of performance, elasticity, high availability, security, and lifecycles must be addressed with improvements not only to storage services, but also to cloud-native applications, storage cloud services, and the adaption of underlying storage and the core storage layer. Then, an application-oriented cloud-native storage can be created with higher stability, greater security, and higher efficiency.
As cloud-native technology continues to develop, public cloud service providers compete to transform or adapt their cloud services to the cloud-native architecture and improve their service agility and efficiency to meet the needs of cloud-native applications. Alibaba Cloud Network Attached Storage (NAS) is also extensively optimized for adaptation to cloud-native applications. The solution supports the CSI protocol and the Flexvolume driver for seamless integration of data interfaces between cloud-native applications and storage services. Users can use the storage resources for service development, without having to worry about underlying storage services.
To meet the requirements of cloud native for elasticity, Alibaba Cloud NAS offers a fully elastic and shared file system that enables use on demand. Alibaba Cloud NAS optimizes and evolves its cloud-native storage to improve performance, elasticity, high availability, security, and lifecycle management.
To meet performance requirements in scenarios involving AI, big data analysis, and high-performance computing, Alibaba Cloud NAS can distribute I/O workloads among multiple file systems, storage clusters, and zones through container orchestration. The solution supports local read-only caching and distributed storage, which can reduce network latency, I/O latency, and GPU wait time. It can boost the computing power with rigid delivery of throughput in the dozens of GBs.
In terms of elasticity, Alibaba Cloud NAS, a fully managed file system, supports auto scaling and the pay-as-you-go billing method. Extreme NAS features a latency of hundreds of microseconds. To cope with cases where elasticity is urgently needed in industries such as finance and Internet, the solution can launch thousands of containers in a few minutes and rapidly load and unload data.
To meet high availability requirements, Alibaba Cloud NAS supports fast failover for containers and provides enterprise-level features such as storage snapshots and backup.
To ensure security, Alibaba Cloud NAS supports comprehensive AD/ACL permission management and quota management and provides a unified namespace with I/O isolation and management among large quantities of pods. It also supports features such as transmission encryption and disk encryption.
To address the challenges arising from massive data volumes, Alibaba Cloud NAS is capable of managing data lifecycles and automatically archiving cold data, which can reduce the costs to users by 90%.
Alibaba Cloud NAS provides storage services for unstructured data. As the result of rapidly evolving cloud-native technology, many companies choose containerized applications that use NAS to store data. Some NAS solutions are even used to store petabytes of data. Following its cloud-native strategy, Alibaba Cloud offers Container Service for Kubernetes (ACK) and Elastic Container Instance (ECI) with container instances that use the NAS file system for persistent storage. Shared file storage is indispensable for container storage.
Alibaba Cloud NAS is a fully managed cloud-native file system that is highly available and optimized for cost savings. Alibaba Cloud NAS offers three services, General-purpose NAS, Extreme NAS, and Cloud Paralleled File System (CPFS).
Alibaba Cloud NAS, as a fully managed service, is easy to configure, supports auto scaling when adding or deleting data, and provides the flexibility and convenience of container infrastructure, which makes it the natural choice for container storage.
Containers that share data with each other usually share file storage. Containers that must run for a long period of time can also use the shared file storage to cope with faults. Alibaba Cloud NAS can meet the requirements for auto scaling, flexible mounting, and high performance of persistent storage in container scenarios. In addition, the configuration files or initial loading data storage for container images can be shared in NAS and read in real time during batch container loading. Multiple pods share persistent data by using NAS and can switch over in the case of pod failure.
As new technologies continue to develop, applications including machine learning, AI, and gene processing make extensive use of shared file storage. Here are a few examples:
Rapidly adopted by corporate users as a container technology and a cloud-native computing solution, Kubernetes has gradually become an essential infrastructure in the era of cloud native, as have container services. New workloads drive the evolution of cloud-native storage and cloud storage. The cloud-native control plane ensures high efficiency, improves data storage stability, and reduces data security risks. To form a storage ecosystem in the cloud-native environment, it is imperative to consolidate the performance of cloud storage solutions, including fundamental capabilities such as capacity, elasticity, and density.
As the natural choice for container storage, Alibaba Cloud NAS can effectively cope with the challenges posed to cloud-native storage in terms of performance, elasticity, high availability, security, and lifecycles. The rapidly evolving cloud-native file storage technology from Alibaba Cloud will continue to empower the fast growth of cloud-native technology and container technology.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://stacksense.io/three-approaches-to-cloud-native-platforms-82bb6610643?source=search_post---------344,"Sign in
Krish
Jul 10, 2018·5 min read
Cloud-Native platforms, driven in large part by Kubernetes, are at the peak of the hype cycle and almost every enterprise we talk to has embraced or in the process of embracing such platforms. At Rishidot Research, we focus on finding patterns emerging from the enterprise users and share them with a larger audience. One of the patterns we expect to gain traction in the coming years is to empower developers to handle the underlying distributed systems through code. Though the concept of infrastructure as code is nothing new, we are seeing a new trend that empowers developers to use familiar programming languages to handle the underlying infrastructure than using config management tools or complex YAML files. We are bucketing the application platforms, broadly, into three buckets. In this post, we will differentiate between these three platform philosophies so that enterprise users can find the right platform for their needs.
The traditional approach to solving the complexity for developers is to build an abstraction and offer developers an API to deploy their applications. The abstraction took away most of the underlying infrastructure complexity and made it easy for developers to deploy their applications. Typical examples include Red Hat OpenShift, Pivotal, Mesosphere, Docker EE, Rancher Labs, etc. They range in focus from developer centricity to focus on IT operators. In the early cloud days, this bucket was categorized as PaaS.
Driven by public clouds and the self-service interface for consuming infrastructure services, a more DevOps-centric approach to operations gained traction in the last decade. With configuration management tools like Chef, Puppet, and Ansible gaining traction and modern approaches like GitOps promoted by Weaveworks, treating infrastructure as a code is widely used by startups and some of the cutting edge enterprises. Hashicorp’s Terraform is another example in this category. In the early cloud days, this was categorized as IaaS+.
Infrastructure as a code is nothing new. It has steadily gained adoption with the cloud, especially among startups and some enterprises. Platform abstraction was attractive for many large enterprises wanting to modernize their infrastructure. In both cases, the configuration files are fast becoming a bottleneck with the ever-increasing complexity. A third new approach to application infrastructure is gaining attention and it could change the way enterprises are thinking about platforms. Driven by startups like Pulumi Inc and the Metaparticle project started by Brenden Burns of Microsoft, this new approach aims to bring abstractions to infrastructure components straight to developers and empower them to handle the underlying infrastructure in a language (as in programming languages) familiar to them. Metaparticle is focused on Kubernetes for now and Pulumi takes an approach that is independent of the type of the cloud service or provider. The rationale behind this philosophy is that this abstraction will not only reduce the complexity of configuration files dramatically but also make it more palatable for developers who need not learn a brand new language specification for handling the underlying infrastructure.
These are three different approaches to application platforms that will compete to gain enterprise adoption. If the developer-centric approach to infrastructure gains momentum, expect to see vendors in the first two categories bring in another layer of abstraction in front of their platforms. Very early days but an interesting approach that requires the attention of modern enterprise decision-makers.
tl:dr version: Abstracted platforms keep the role of Devs and Ops separate; Infrastructure as code requires Devs to understand Operations; Infrastructure as developer-friendly code puts a developer-friendly wrapper around infrastructure
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
This blog helps enterprise decision-makers understand the emerging technologies and it is part of Rishidot Research publications
"
https://medium.com/@piotrzan/i-think-vclusters-can-be-the-next-big-thing-in-the-cloud-native-space-as-it-often-is-with-smart-a7ab266e96ce?source=search_post---------345,"Sign in
There are currently no responses for this story.
Be the first to respond.
Piotr
·Jul 6, 2021
Loft Labs
I think vclusters can be the ""next big thing"" in the cloud native space, as it often is with smart abstractions. From the perspective of multi-tenancy vcluster seems to be great fit, but I look at also it same way as at virtual machines. One can imagine having a regular cluster while being able to spawn virtual clusters for specific tasks with dedicated configuration and versioned addons etc. Similar like with vms, one could package together bundled resources almost like vm image. Once this technology is fully developed it will be very powerful! To add to use cases you mentioned, development clusters spawned on demand and externally accessible is good idea too.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/polyglot-cloud-native-debugging-beyond-apm-and-logging-8955ef25fc1b?source=search_post---------346,"There are currently no responses for this story.
Be the first to respond.
I’ve done quite a few conference talks since I became a developer advocate for Lightrun. One of my most popular talks has a title similar to the title of this post. In this post I’ll try to cover the gist of the post. You can see one of the longer versions of the talk that I gave at DDD at the end of 2021:
There’s practically an alphabet soup of terms covering public cloud, hybrid cloud, etc. There’s also a lot of overlap between the various terms. The nice thing about this post is that it applies, regardless. You will run into problems with microservices, architecture and monolithic applications or serverless.
Debugging is universal and nowadays so is polyglot. With the recent log4j shell bug, a lot of companies discovered they use Java. Python is everywhere and Node is also deeply entrenched by now. It’s rare to find a cloud platform that doesn’t have all three of them somewhere… A polyglot approach improves time to market by leveraging best of breed to deliver a product faster.
But I’m getting ahead of myself. Let’s take a step back and discuss the problem.
Without those two, we’re blind. Regardless of the architecture you choose. If you don’t have observability, you have a cat in a box (I’ll get back to it).
We need to provision cloud resources, need to know if things work… Actually DevOps teams need to know those things. R&D should know it too, but they aren’t immediately connected to that. Back in the days before cloud-native applications and the advent of DevOps, we would often deploy things ourselves.
This often ended badly.
DevOps practices and cloud native revolutionized this painful and problematic approach. They did it by removing the wildcard problematic variable: R&D. Thanks to DevOps teams, we enjoy a level of uptime we could only dream of 20 years ago. Private cloud made sure that these benefits of cloud are now available to everyone.
Container orchestration, microservice architecture, serverless, etc. made scaling easier. But it also made application deployment far more challenging. Cloud infrastructures are much easier to deploy thanks to IaC solutions, but the complexity shifted.
Troubleshooting cloud-native applications is harder. We have amazing observability tools but they’re dealing with complex underlying infrastructure.
In recent years, cloud-native apps rely more and more on modern logging infrastructure. These aren’t our old log files. Cloud provider logging solutions handle ingestion, search, meta-data, PII reduction, etc. at scale. I can’t imagine running a major deployment without one of those solutions.
Logs are wonderful, but they have two major drawbacks…
This is the situation with logging. We aren’t clairvoyant. We don’t know how our native applications will fail in the deployment target. If a log is missing, we need to go through CI/CD, which can take hours (or days for some deployment processes). Then you need to go through that again, for the fix or additional logging.
During that time, there’s a bug in production and the development teams end up wasting time. As a result, over-logging is pretty common for a cloud application. This leads us to a different problem.
Logging a cloud-native application at scale can quickly become your biggest expense. We can reduce IO costs by caching, but even a read only operation that has a perfect cache hit will produce a log. Modern cloud applications must log everything due to scale. The size of ingested logs can become prohibitive. Performance hit of over-logging can compound this problem. Ingesting more data can seriously impact performance, which would require more containers to handle the same load. Additional container images breed complexity, which means more bugs and a bigger need for logs.
When we observe, we affect the observer. It’s as true for Schrödinger’s cat as it is for scalable applications.
Modern observability tools are a marvel. I feel there should be a German word to describe the satisfaction one gets from a detailed APM dashboard. These tools enable the production of complex applications like nothing we’ve seen before.
These tools are wonderful. I don’t want to qualify that with a “but they are…”. They’re perfect. For the operations team, who are the target market of these tools.
They’re nice for R&D teams too, but we aren’t the target market.
I usually compare these tools to the check engine light or even the bat signal. They tell us there’s a problem with cloud-native services, but they aren’t exact. E.g. if your car shows the check engine light, you take it to the mechanic. She then connects his computer directly to the engine in order to debug the problem…
The job of these tools is to detect a problem and alert us. As a bonus, they often give us the initial direction of the problem, but if the solution isn’t immediately obvious, developers are left with limited options.
For most apps, the overhead of observability is acceptable and well worth it. But there are limits. Most observability tools can be tuned to observe more information and trade off performance.
This is usually a problematic notion that brings us back to the discussion of scaling infrastructure to support observability.
When we native applications we can debug them using existing native technologies such as platform debuggers. Those work great for local debugging but at scale (especially horizontal scaling) they have some problems:
If we work around those issues, debuggers are the perfect tool. R&D knows how to use them as they are a core tenant in software development.
This is where continuous observability steps in. Observability tools work by monitoring the entire system and exposing relevant information.
Continuous observability flips the switch, we ask the system about specific information we would like to know. It’s the natural progression of problem solving… An observability platform points to a problem in our native software, and we use continuous observability to investigate the problem.
There are many tools that we can classify as continuous observability tools. I’ll focus on Lightrun, which exposes its capabilities as a debugger for cloud-native computing environments. It differs from existing observability tools in the following ways:
Some software developers consider logging to be superior to debugging. I always considered them to be two approaches that excel at different things. Logging is debugging.
The static nature of logging is its biggest problem. If we could just inject a log dynamically, it would fit far better into the cloud-native architecture.
Notice, we still need existing logging to give us context and direction. This is simply for those cases where a developer is reviewing the logs and is missing a specific log. This solves the problem of over-logging, as a new log can be injected dynamically as needed.
Furthermore, injected logs are interlaced and ingested with the native logs. This gives them the context we need to understand the root cause of a problem.
Debugging is first and foremost the practice of verifying assumptions. But some assumptions are harder to verify with snapshots or logs, e.g. “this is the method that slows my docker containers”.
This is something for which we would normally use a profiler, but it’s problematic to do in a production setting. Alternatively, we use a timer where we take the current time on method entry and log it on exit… That’s great for a development machine, but deploying something like this to production is “noisy” and problematic. We usually want to keep narrowing the scope of measurements to find the true culprit.
That’s where metrics come in. We can add counters, timers, and more to measure the execution of a specific method. We can even pipe this information to Prometheus and other such tools. When an APM alerts us to a performance penalty in our finished application code, we can narrow it down with a metric.
I somewhat side-stepped one of the biggest features in continuous observability conditions. Let’s say I want to place a snapshot, log or metric on critical code. It will get hit instantly. That can be exactly what I want, but what if I want to debug a problem that happens only under specific conditions, e.g. to a specific user… Debuggers solved this problem before by coming up with conditional breakpoints. This applies to every action on a continuous observability platform. You can place a condition on any action within the platform, e.g. I can set a snapshot that would only be triggered by a user with the given ID.
We sometimes tread more lightly when building in the cloud-native world. We know how a minor change can have a cascading effect and we know how hard it is to track such issues. Continuous observability changes that dynamic, high-impact changes can be made more easily as we now have the tools to deal with them. In that sense, a debugger serves as a safety net for faster code to production cycles.
But even if you don’t subscribe to a more “ambitious” continuous deployment strategy. The additional safety net afforded by continuous observability provides many advantages for your entire application.
E.g. Have you ever asked yourself if a particular method is even used in production?
Is the dependent service at fault for the failure or is it my bug?
With continuous observability, you can find out right from the comfort of your IDE.
In that sense, it’s the realization of cloud-native computing by giving developers a direct channel to the applicable information.
Most observability tools are platform agnostic at the system level. Continuous observability tools are a bit more complex. A continuous observability tool works at the code level and needs deep access to the language runtime. As such, it’s deeply adopted in each programming language on every platform. It’s technically possible to support all languages, but there are challenges involved.
In the demo video above, I show debugging a snapshot in NodeJS which leads me to code in a Spring Boot Java backend. Using such independent services isn’t out of the ordinary for the cloud-native approach. I didn’t demo a lambda service because of a lack of time. With stateless services and serverless, the need is even greater. Debugging serverless locally is so difficult some developers advocate deploying directly to production (through CD and testing).
With a continuous observability tool, we can use tags creatively to monitor serverless services as they come online and bind the right actions to the function.
In my talk I also do a demo, you can check it out in the video above.
Continuous observability is already revolutionizing the cloud native industry in the same way observability, modern logging and docker containers did more than a decade ago. It fills an important blind spot in our current set of native technologies: a tool for developers that lets them deep dive production issues in the finished application code.
Production bugs are like spores, they survived QA, staging, etc. They are resilient; we need heavy guns to kill them at scale. Cloud-native technologies made the scale much bigger and, as a result, much harder to debug.
With serverless, microservices and worker frameworks (e.g. AirFlow, Spark, etc.) things are even more dire. The lifecycle is so difficult some developers accept a relatively high failure rate as “the cost of doing business”.
I hope this article will make your first steps into continuous observability. I believe this technology can facilitate a new generation of applications that will change the customer experience and yours.
Follow me on Twitter to learn more and get updates for future posts.
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
7 
7 claps
7 
A humble place to learn Java and Programming better.
Written by
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
A humble place to learn Java and Programming better.
"
https://medium.com/@tcv/empowering-the-cloud-first-enterprise-devos-advanced-cloud-native-security-analytics-provide-a-2029fc28c48e?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
TCV
Nov 3, 2021·6 min read
‘Cloud-first’ organizations now outnumber on-premise businesses by a ratio of three to one, according to a new report from Devo, a leader in next-generation, cloud-native security information and event management (SIEM), and an exciting addition to our security portfolio, focused on the components of building out a modern security operations center (SOC) that combines endpoint + network + security analytics.
As investors in Rapid7 and Splunk, we’ve gained significant experience in SIEM and we see Devo as the next wave of SIEM for the cloud-native enterprise. We’ve also seen how the threat landscape has evolved on the network side via Vectra, and we backed Webroot, which provides organizations with multi-vector endpoint protection.
Barriers to cloud adoption were already lower than ever before the pandemic struck-magnifying the benefits of remote/virtual IT infrastructures, software hosting, and data management. The scale of the ensuing enterprise shift to cloud infrastructure and remote hosting has been driving up security complexity and associated volumes of data, so that this now exceeds the capabilities of legacy security toolsets and the capacity of already-overstretched security teams.
In this context, Devo-with its cloud-native logging and security analytics platform-is having a huge and fast-growing impact. It is saving enterprise security operations teams untold hours of manual analysis as security data volumes soar, by empowering security analysts to cut through the noise and focus on the threats that matter most.
Next-generation SIEM is big business, and in the cloud-first enterprise market Devo is well ahead of this trend. The company, based in Cambridge, Massachusetts, is dedicated to producing the most innovative logging and security analytics platform on the market, and its cloud-native solutions strike right at the heart of current enterprise challenges.
“Digital Transformation, and the massive threat surface associated with it, have elevated security analytics to the centerpiece of frontline cyber defenses. Some of the world’s largest businesses choose Devo because we combine unmatched scale, powerful analytics, and the ability to get answers in real time.
The global market potential ahead of us is absolutely massive, and we’re excited at working with TCV’s sector experts to realize that. What really stands out for us is that the TCV team is as enthusiastic and driven as we are. The new funding validates the disruptive force we have become and sends signal to the industry that we will continue to set the pace for innovation and customer value.”
Marc van Zadelhoff, CEO of Devo
Devo has already made a name for itself with some of the most security-sensitive organizations in the world, including the U.S. Air Force, which relies on the company as its central security hub for protection, detection, and response for enterprise defense worldwide.
Speed and scalability are among its many differentiators: Devo supports thousands of concurrent real-time queries, allowing security teams to query petabyte-sized data sets to answer questions quickly, visualize trends, and perform advanced analytics-so they never miss a critical threat.
Providing unparalleled performance, scalability and clarity, the Devo Platform lets security teams ask more questions, visualize more data, and access valuable analytics to defend and monitor their organization. Its data ingestion capabilities are the industry’s most scalable and flexible. The Devo Platform can ingest data from any source, even if it’s unrecognized or the format changes.
Crucially, it is designed to integrate effortlessly with all of the main enterprise IT environments favored by large enterprises-from software giants like IBM, Microsoft, Oracle, and Salesforce, to the big cloud platforms (Google Cloud, AWS) and network providers (Cisco, Juniper) to some of the biggest names in IT security, including Sophos, Symantec, and Check Point. This enables a consolidated view of the security landscape and a single point of actionable analytics.
Suffice to say, Devo’s potential market is substantial-and global-and to date we believe the company has only skimmed the surface of the opportunity.
Devo has cemented its leadership in the next gen SIEM and IT Ops markets with accelerating growth driven by enterprise demand for modern, cloud-first, SaaS based, AI/ML driven tools. TCV is proud to have led Devo’s $250 million in Series E funding round, and we look forward to working with a world-class team to provide a better, faster, and more economic value proposition for enterprise customers around the world struggling to keep up with security and performance blind spots.
We anticipate that the new funding will fuel strong growth across new regions and market verticals, and significantly increase investment in channel expansion and product innovation. Plans include growth in new verticals and geographies including an expanded presence in the public sector, as well as internationally in the Asia Pacific region.
Building on the rapid adoption of Devo as the platform of choice for leading resellers and managed security services providers (MSSPs) across the globe, the company is also redoubling its commitment to the channel and integration partners. Devo also expects to invest heavily in technology alliances, content, and people to build out a global security community to usher in a new era for the industry.
Of course, it’s the people that make a remarkable and investable business, and again this is where Devo shines. The company is made up of some of the world’s brightest minds, all respected experts in their fields yet from diverse backgrounds-giving Devo breadth as well as depth in its market experience.
CEO Marc van Zadelhoff took the helm as CEO a year ago, marking the start of a momentous year for the company. He has brought to the company some 20 years’ experience working in cybersecurity for organizations globally, most recently as the COO for LogMeIn, and before that as General Manager for IBM Security-a IBM Business Unit he co-founded. Since assuming leadership of Devo, Marc has driven the company to new heights, evidenced by:
The team Marc is building around him is impressive, combining subject matter expertise and commercial depth.
Devo and its people are the ideal fit for TCV’s investment strategy. Our track record of supporting portfolio companies for the long term as a crossover investor is well established, and our well-documented successes with other high-growth tech businesses at the intersection of cloud, networking, and security (including Cradlepoint, Silver Peak, HashiCorp, Splunk, Vectra, Rapid7, and Venafi, and more recently Aviatrix) provide a strong hint at the ambitions we have for Devo.
We can’t wait to roll up our sleeves and help Devo capitalize on its global potential.
***
The views and opinions expressed are those of the author and do not necessarily reflect those of TCMI, Inc. or its affiliates (“TCV”). TCV has not verified the accuracy of any of the data or statements by the author and disclaims any responsibility therefor. This blog post is not an offer to sell or the solicitation of an offer to purchase an interest in any private fund managed or sponsored by TCV or any of the securities of any company discussed. The TCV portfolio companies identified above are not necessarily representative of all TCV investments, and no assumption should be made that the investments identified were or will be profitable. For a complete list of TCV investments, please visit www.tcv.com/all-companies/. For additional important disclaimers regarding this interview and blog post, please see “Informational Purposes Only” in the Terms of Use for TCV’s website, available at http://www.tcv.com/terms-of-use/.
Originally published at https://www.tcv.com on November 3, 2021.
TCV backs growth-stage private & public tech companies. For a complete list of TCV investments, visit: www.tcv.com/portfolio-list.
TCV backs growth-stage private & public tech companies. For a complete list of TCV investments, visit: www.tcv.com/portfolio-list.
"
https://lab.wallarm.com/wallarm-to-sponsor-kubecon-cloudnative-con-2aab48f2ab81?source=search_post---------348,"If you have not registered yet for the main Kubernetes event in North America which will start on December 10th in Seattle, you may be out of luck.
The event is sold out and is only taking the waitlist applications.
But if you are going, KubeCon + CloudNativeCon promises to be a treat with the content geared for developers, IT professionals, and C-level leaders. The topics will include emerging trends in microservices architectures, cloud deployments and container orchestration with technologies like Kubernetes, Prometheus, Envoy and many more.
With over 6,000 attendees, there should be plenty of discovery and networking opportunities. In addition to end users and technology providers, this vendor-neutral cloud event brings together the industry’s most respected experts and key maintainers behind the most popular projects in the cloud native ecosystem.
The first day of the conference, December 10th, will also include KubeSec Enterprise Summit, a co-located event focused on the challenges faced by larger organizations with demanding security and compliance requirements when deploying Kubernetes in production. The day will provide a unique opportunity to hear from organizations like JMPC, Starbucks and Tinder who have already deployed Kubernetes to support highly secure solutions, as well as a range industry & technology experts who will talk about trends and best practices in securing cloud-native applications.
In Wallarm booth S/A3 you will have the opportunity to view the demo of Wallarm-on-Kubernetes deployment with a secure ingress controller. First introduced at Google Next conference earlier this year, Wallarm solution includes secure NGINX-based ingress controller compatible with most cloud-native deployments and enriched with Prometheus monitoring. Also, on December 12th, we will also be announcing the winners of Wallarm Machine Learning Hackathon; a project stemming from a module for false-positive detection we have recently released into open source.
See you in Seattle!
You can learn more about the conference here and view last year’s keynotes and other videos here.
I agree to Wallarm Privacy Policy.
Webinars
More insights
Subscribe for the latest news
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/@rothgar/the-economics-of-writing-a-technical-book-689d0c12fe39?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Justin Garrison
May 7, 2018·12 min read
I am not an expert. I have co-authored a single book in 2017 called Cloud Native Infrastructure for O’Reilly Media. Many people have asked me what it was like so I will attempt to explain the process, time investment, and financial incentive here.
This was my experience. If you have written a book, or would like to, I promise your experience will be different. Nonetheless, I hope you can learn something from the things I learned.
The process was about what I expected. I was introduced to Brian, our first of three editors, from someone I knew who was already writing their third book. They thought I might be a good fit for what they were looking for so they made the introduction.
I thought about it for a couple weeks and then submitted a formal book proposal which entailed filling out a Word document template and emailing it to the editor. I didn’t hear back for about 3 weeks and then, after a follow-up email, heard the proposal was approved. After a kick-off call it was suggested that I find a co-author to help write the book. I had a week to find one and then needed to sign a contract with O’Reilly for dates and deliverables. I interviewed a few people and Kris Nova and I complemented each others skills perfectly for the content we wanted to cover in the book. She agreed it sounded like a good topic and she was excited to take on the challenge.
The contract seemed fairly standard and focused around content ownership and royalties split. The default split between authors is 50/50 which we stuck with. The contract stipulated that Kris and I own the copyright for the content, but O’Reilly has exclusive rights to use the content any way they see fit throughout the world now and in the future for the duration of the copyright.
Once the contract was signed there was a steady pace of work as we both figured out how to lay out content and what we should write about. O’Reilly provides a platform called Atlas for writing which is quite good. You write in plain text AsciiDoc and then O’Reilly’s Atlas platform can generate a PDF, or other formats, via the web interface or API. We both used atlas-cli to generate PDFs as we wrote. Generating the PDFs was a good feedback loop on the content. It helped make sure formatting was right and also allowed us to take a step back to read what we wrote.
Atlas works as a remote git repo but Kris and I chose to mostly work out of a private GitHub repo. We originally envisioned making pull requests to collaborate on each other's content but that didn’t happen as much as expected. Atlas has editing tools and some additional features available but we mostly just used it as a git remote URL.
On March 1st we were assigned our cover animal which Kris and I named Andy O’Connor the Andean Condor. We were pretty excited to see the cover for the first time even if the subtitle went through multiple revisions. We didn’t get to pick the animal or the picture. We were told up front we wouldn’t get to pick the animal so we knew what to expect. We were also told that Tyrannosaurus Rex and unicorns are not allowed.
We kept writing until the 1/2 draft was due in early June. We turned it in and got less feedback than we expected, but it was still good to have a fresh set of eyes looking at it. We didn’t like what we had created. We had written almost 6 chapters and threw away 3 of them. The first two were heavily edited and the remaining chapter was trimmed down significantly and turned into an appendix.
We had some more planning meetings and came up with a revised outline that we submitted to our editor for review. By this time we were on our 2nd editor who wasn’t very familiar with the project so we got very little feedback and went with what we had.
We kept writing with minimal interactions with our editor until we were really close to needing the full draft due. I had sent select chapters to friends to look at and tried to incorporate the changes they suggested.
The first Tuesday of September the full draft was due and then went into a review process. There were technical reviewers we were able to suggest but mostly O’Reilly pulled from a pool of their trusted reviewers. We got minimal feedback from most of them (a survey form) and one returned notes on the PDF. We had a week to make edits. During this time the draft was made available as a preview on Safari books. In retrospect I wish we had posted preview chapters sooner which was something our first editor suggested, but we were both too embarrassed to follow through.
It wasn’t enough feedback for me so I reached out to more people and sent them chapters looking for someone to tell me it sucks and why. Luckily, I found someone who would give me the harsh feedback I wanted and I had about 3 days to incorporate their changes into the book before it went off to post production.
The last push was very difficult and stressful. There were a lot of big changes on the last weekend which was a risk, but I think in the end made the book better. The final weekend we moved some chapters around and wrote a chapter from scratch for content we felt was missing.
Post production was handled by non-technical, professional editors for grammar, spelling, and general readability. I believe the first PDF came back with more than 1300 edits. Overall there were more than 2000 changes made during post. I later found out this amount of edits is fairly standard for our book length. We had about 3 weeks of emailing large, heavily notated PDFs back and forth which was no fun compared to the plain text git workflow of writing.
Post production took about a month to complete and then the book went off for printing. At the same time it was posted on Safari Books Online and immediately available. We each received 6 copies of the book in the mail shortly after it was available for sale.
All in all I worked from Feb — Oct for roughly 5 nights a week at 2–3 hours per night. I also worked about 3 weekends non-stop when a draft or final edits were due. Roughly I’d say I worked about 500 hours total. That was only my time and doesn’t include Kris’. I was lucky to have a co-author to share the load.
Some people have asked how I found motivation to keep writing. I’m a fairly driven individual and the deadlines in the contract were enough for me to put in time most nights. I wasn’t motivated enough to meet the original 250 page goal, but I was happy with the content we were able to cover and how much we accomplished.
Kris had a different creative process than I did. She was better at mulling over a topic in her head and putting it all down in one sitting; often the week before one of the deadlines. This made me nervous on multiple occasions and was probably the biggest thing I stressed about. Everything was submitted and completed on time, but I would suggest you have a sense for how you and your co-author work together at the beginning instead of the week before a project deadline.
Throughout the writing process I felt like I finished writing multiple times. Once when the final draft was due, once when technical reviewer’s feedback was incorporated, and once at the end of the post editing process. In each case it meant we got to take a break from writing while we waited for feedback.
At the end of final edits I was done (contractually and mentally). I had read through the entire book at least three times and much of the content was starting to lose meaning. After sending the final edited PDF I wanted to stress about missing an edit before going to bed, but I was too tired to care.
O’Reilly provides an affiliate program which was terrible to set up and in the end hardly worth the time. You get a cut from all sales that go through your link but I have never received any money from affiliate book sales. The only money I got was when someone used my link and then bought a ticket to an O’Reilly conference. To date only one person has done that and I received $200. If anyone is looking to buy tickets to Velocity or OSCON feel free to click the O’Reilly book links at www.cnibook.info and then buy a conference ticket. 😊
I attempted to set up an affiliate program for Amazon but my application was denied. Amazon offers an author central site to create a 1998 inspired author profile page and an out of date book sale statistics and rankings. I’m really not sure the point of creating the Amazon author information outside of claiming the book(s) you author and confirming that you have a terrible book rank.
While setting up these accounts is when I created the cnibook.info website and @cnibook twitter handle. Luckily the website is a static page hosted on GitHub so there is no recurring costs. The twitter account I still maintain but has minimal interactions.
The landing page was a valuable use of my time as it gave a URL to point people to for anyone searching for the book or wanting more information. I would suggest anyone writing a book spend a night to register a domain and set one up. I launched it on August 31, 2017 and it has over 4,600 visits which is terrible by most website standards but good as a place to funnel users for info.
During AWS Re:Invent I decided it would be fun to try a Twitter campaign to try and get some interest in the book. I set a limit of $50 which bought 7,648 impressions (people saw the tweet). This includes 161 engagements (clicked on the tweet, RT’d, or favorited), and 37 clicks (clicked the link). I promoted the tweet above and I don’t think it was worth the money. I don’t believe it generated any sales and cost $.31 per engagement. After that campaign I had another tweet that was retweeted by a friend (I didn’t ask them to do it) with a lot of followers and it had more engagements than the paid for, promoted tweet.
The book was originally supposed to be 250 pages and would have cost $59.99. Instead it was only 160 pages and cost $39.99. Because we co-authored the book we each got 5% of revenue for physical books and 12.5% for ebooks and digital access (10% and 25% for individual authors).
This breaks down to we each get $.99 for a physical book and $.46 for an ebook. Safari Books Online (SBO) pays based on how many pages are viewed and how many people have added the book to their personal bookshelf (I never knew before why that was a thing). So far I have received $539 over 5 months for SBO.
The payments come on the last day of the month after the month when O’Reilly receives payment for the sale. If you bought a book on February 1st, $.99 was added to the check I received on March 31st.
From December through March the book has sold 1337 copies. I have no idea how well other books in this category sell. This total also includes 2 book signings at conferences that were sponsored by the CNCF (Thank you!) which was roughly 150 physical books total. On average, the book has sold 222 copies per month which is greatly skewed by the first month which had 930 sales. The last month (March) had 34 physical book sales. I suspect that number will go down even more over the next few months.
Sponsorships was an unexpected source of income. We have been lucky enough to have 3 sponsors so far. The sponsor pays O’Reilly for exclusive rights to provide a PDF and optional print version of the book. The company gets to put a forward in the book that Kris, me, and an O’Reilly editor approve. Once the sponsor completes their contract with O’Reilly they can do whatever they want with the books. Usually, the PDF gets put behind a web form so you fill out your email address and the company uses it for marketing services and getting customer leads. Physical books are usually given away at conferences where they can scan badges.
I have no idea if this model is financially viable for the companies but I would assume so. I’ve asked for download numbers but was never provided any and the sponsors are not contractually obligated to give numbers. I’m not sure how many books get sponsors, but it helps a lot with raising awareness of the book because companies have more of a presence on social media and have budgets for advertising.
Each full book sponsorship for one month nets me $3,705 and partial sponsorships give an amount based on percentage of the book sponsored (e.g. 5 chapters in a 10 chapter book is 50% sponsored). That’s much better than I expected because a one month full sponsorship is more than all other sales combined. I’m not going to say exactly what percentage I get from the sponsorship simply because I don’t want to disclose how much any of these 3rd parties pay and their finances are not my business to disclose.
There are also some other sponsorships that I think count as ebook sales but I never got a clear answer how royalties work for those. Book licensing incurs a small payment but I’m unclear how that is used. From my statements, three people have licensed the book or excerpts from it which has netted me $2.37.
My April 2018 statement (sales from December — March) says I’ve made $11,554.15 which roughly breaks down to $23 per hour for the estimated 500 hours of work. Without the three sponsorships that would have been $5.29 per hour. Luckily that number only gets better with time. I’ve heard 2nd editions are a better rate per hour because they have less time investment with similar sales as the original, but I don’t have any experience.
Going into this project I had a rough estimate in my head to make about $2000–3000 so this is much better than I expected. Set your expectations accordingly.
I’m extremely happy with the how things have turned out. We’ve got a lot of great feedback and some average reviews on Amazon (please leave us a review!). I’m grateful that people are reading it and I hope it’s helping them in some way. We worked hard and wanted to help engineers get better at running infrastructure.
I learned that you can’t just write a book and expect it to sell. There needs to be a lot of effort (a.k.a. budget and time) in marketing and it takes more than just individuals with twitter accounts to get the word out. It has been humbling to have friends and organizations help us promote the book and even pay to put the book in people’s hands.
So far I know of only one grammatical mistake, which is a win in my book (pun intended).
The book has provided a few other opportunities that I probably wouldn’t have had. So far I’ve done a couple podcast interviews, spoken at a few events, did one webinar, and have had a few opportunities for more writing projects with O’Reilly (some of which I’ve taken).
Would I write another book? Not for the foreseeable future. I would like to update Cloud Native Infrastructure to keep it fresh with current industry trends, but another book from scratch is not a year long project I’d be looking forward to at this time.
I’m very proud of the work we accomplished and glad we wrote it. I can’t think of anything else I would have done with that time that would have been better spent.
I’d love it if you bought a copy of the book using the affiliate links at www.cnibook.info and if you’ve already read it please let us know what you think. We’re available on twitter rothgar and krisnova. We also love pictures of the book with your pets. It makes us smile. 😄
I hope this has given you some insights into what it takes to write a book and what you can expect if you do it.
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
Trying new things. Breaking stuff. Likes open source.
See all (227)
4.2K 
29
Thanks to Amy Chen. 
4.2K claps
4.2K 
29
Trying new things. Breaking stuff. Likes open source.
About
Write
Help
Legal
Get the Medium app
"
https://blog.graph.cool/introducing-the-graphcool-framework-d9edab2a7816?source=search_post---------350,
https://medium.com/spaceuptech/how-to-make-microservices-communicate-84b09d077c96?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
I’m sure you want to build scalable apps, right? Who doesn’t? If so, you must have come across the words “Cloud Native.” This approach is like an angel which can resolve most of your scaling challenges. So, what exactly is cloud native?
Cloud native is an approach used to build apps which can harness all the capabilities of the cloud.
Yup, you got that right. It’s an approach. Not a framework. Not a bunch of steps to follow. And due to this, there are a million different approaches to go cloud native and achieve cloud computing “Moksha.”
"
https://blog.getambassador.io/cloud-native-patterns-canary-release-1cb8f82d371a?source=search_post---------352,"In order to effectively build cloud native applications, your engineering organization must adopt a culture of decentralized decision-making to move faster. In this series, we’ll discuss key patterns in cloud native application development, why they’re effective, how to implement them in your organization, the consequences of doing so, and provide examples using popular cloud native tools. In the first part of this series, we’ll discuss canary releases and show an example of how to implement them with the Ambassador API Gateway.
Last updated: July 2020
A canary release is a software testing technique used to reduce the risk of introducing a new software version into production by gradually rolling out the change to a small subgroup of users, before rolling it out to the entire platform/infrastructure.
Canary releases are commonly confused with blue-green releases, feature flag releases, and dark launch releases. A canary release differs from a blue-green release by enabling an incremental rollout of a new service. With a blue-green rollout the new software version is “switched” in one action and made available to all users instantaneously.
A canary release is also different from a feature flag release, as feature flags are used to expose a specific feature to a small subgroup of users. A canary release exposes a specific version of the entire application or service.
A dark launch canary release differs from a regular canary by duplicating traffic from a small subgroup of users and routing this to a new version of the service that does not return data to the user. A “dark launch” is named this because the response is “dark”: although the new service is tested will real traffic, the end-users do not see the results — only the engineering team do.
This technique was inspired from the fact that canary birds were once used in coal mines to alert miners when toxic gases reached dangerous levels — the gases would kill the canary before killing the miners, which provides a warning to get out of the mine tunnels. As long as the canary kept singing, the miners knew that the air was free of dangerous gases. If a canary died, then this signaled the need for an immediate evacuation.
This technique is called “canary releasing” because– just like canaries that were once used in coal mining to alert miners when toxic gases reached dangerous levels– a small set of end users selected for testing act as the canaries and are used to provide an early warning. Unlike the poor canaries of the past, obviously no users are physically hurt during a software release, but negative results from a canary release can be inferred from telemetry and metrics in relation to key performance indicators (KPIs).
Canary tests can be automated, and are typically run after testing in a pre-production environment has been completed. The canary release is only visible to a fraction of actual users, and any bugs or negative changes can be reversed quickly by either routing traffic away from the canary or by rolling back the canary deployment.
You can use canary releases when:
You should not use canary releases when:
Typically canary releases are implemented via a proxy like Envoy or HAProxy, smart router, or configurable load balancer. The releases can be triggered and orchestrated by continuous integration/delivery pipeline tooling (such as Jenkins or Spinnaker) automated “DevOps” platform (like Electric Cloud), or automate or feature management SaaS platforms (like LaunchDarkly or Optimizely).
Here are some implementation issues you should consider:
Using canary releases has both benefits and liabilities:
Benefits include:
Liabilities include:
An example of how to implement a canary release with the Ambassador API gateway can be found in the article “Canary deployments, A/B testing, and microservices with Ambassador.”
The following list highlights organizations that are known to use the canary release pattern:
Developer-First Kubernetes.
654 
654 claps
654 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.getambassador.io/the-rise-of-cloud-native-engineering-organizations-1a244581bda5?source=search_post---------353,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Over the past decade, engineering and technology organizations have converged on a common set of best practices for building and deploying cloud-native applications. These best practices include continuous delivery, containerization, and building observable systems.
At the same time, cloud-native organizations have radically changed how they’re organized, moving from large departments (development, QA, operations, release) to smaller, independent development teams. These application development teams are supported by two new functions: site reliability engineering and platform engineering. SRE and platform engineering are spiritual successor of traditional operations teams, and bring the discipline of software engineering to different aspects of operations.
Platform engineering teams apply software engineering principles to accelerate software delivery. Platform engineers ensure application development teams are productive in all aspects of the software delivery lifecycle.
Site reliability engineering teams apply software engineering principles to improve reliability. Site reliability engineers minimize the frequency and impact of failures that can impact the overall reliability of a cloud application.
These two teams are frequently confused and the terms are sometimes used interchangeably. Indeed, some organizations consolidate SRE and platform engineering into the same function. This occurs because both roles apply a common set of principles:
Platform engineers constantly examine the entire software development lifecycle from source to production. From this introspective process, they build a workflow that enables application developers to rapidly code and ship software. A basic workflow typically includes a source control system connected with a continuous integration system, along with a way to deploy artifacts into production.
As the number of application developers using the workflow grows, the needs of the platform evolves. Different teams of application developers need similar but different workflows, so self-service infrastructure becomes important. Common platform engineering targets for self-service include CI/CD, alerting, and deployment workflows.
In addition to self-service, education and collaboration become challenges. Platform engineers find they increasingly spend time educating application developers on best practices and how to best use the platform. Application developers also find that they depend on other teams of application developers, and look to the platform engineering team to give them the tools to collaborate productively with different teams.
Site reliability engineers create and evolve systems to automatically run applications, reliably. The concept of site reliability engineering originated at Google, and is documented in detail in the Google SRE Book. Ben Treynor Sloss, the SVP at Google responsible for technical operations, described SRE as “what happens when you ask a software engineer to design an operations team.”
SREs define service level objectives and build systems to help services achieve these objectives. These systems evolve into a platform and workflow that encompass monitoring, incident management, eliminating single points of failure, failure mitigation, and more.
A key part of SRE culture is to treat every failure as a failure in the reliability system. Rigorous post-mortems are critical to identifying the root cause of the failure, and corrective actions are introduced into the automatic system to continue to improve reliability.
One of us (Bjorn Freeman-Benson) managed the engineering organization at New Relic until 2015 as it grew from a handful of customers to tens of thousands of customers, all sending millions of requests per second into the cloud. New Relic had independent SRE and platform engineering teams that followed the general principles outlined above.
One of the reasons these teams were built separately was that the people who thrived in these roles differed. While both SREs and platform engineers need strong systems engineering skills in addition to classic programming skills, the roles dictate very different personality types. SREs tend to enjoy crisis management and get an adrenaline rush out of troubleshooting an outage. SRE managers thrive under intense pressure and are good at recruiting and managing similarly minded folks. On the other hand, platform engineers are more typical software engineers, preferring to work without interruption on big, complex problems. Platform engineering managers prefer to operate on a consistent cadence.
Over the past decade, DevOps has become a popular term to describe many of these practices. More recently, GitOps has also emerged as a popular term. How do DevOps and GitOps relate to platform and SRE teams?
Both DevOps and GitOps are a loosely codified set of principles of how to manage different aspects of infrastructure. The core principles of both of these philosophies — automation, infrastructure as code, application of software engineering — are very similar.
DevOps is a broad movement that began with a focus on eliminating traditional silos between development and operation. Over time, strategies such as infrastructure automation and engineering applications with operations in mind have gained widespread acceptance as ways better build highly reliable applications.
GitOps is an approach for application delivery. In GitOps, declarative configuration is used to codify the desired state of the application at any moment in time. This configuration is managed in a versioned source control system as the single source of truth. This ensures auditability, reproducibility, and consistency of configuration.
In short: DevOps is a set of guiding principles for SRE, while GitOps is a set of guiding principles for platform engineering.
Tweet this.
Site reliability engineering and platform engineering are two functions that are critical to optimizing engineering organizations for building cloud-native applications. The SRE team works to deliver infrastructure for highly reliable applications, while the platform engineering team works to deliver infrastructure for rapid application development. Together, these two teams unlock the productivity of application development teams.
Related: The Developer Experience and the Role of the SRE Are Changing, Here’s How
Developer-First Kubernetes.
672 
672 claps
672 
Written by
Ship software faster with Ambassador. Learn more at www.getambassador.io
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Ship software faster with Ambassador. Learn more at www.getambassador.io
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/adobetech/three-principles-of-api-first-design-fa6666d9f694?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
In my conversations with developers who are looking to embrace cloud-native development and openness, one question frequently comes up: what exactly does “API First” mean? All too often, “API First” can come to mean “Yeah, APIs are important, but they’re not essential”.
There are three principles of API First Design:
1. Your API is the first user interface of your application
2. Your API comes first, then the implementation
3. Your API is described (and maybe even self-descriptive)
Let’s take a look what each of these principles mean.
API first thinking means that your API is the first interface of your applications. This means that the people developing against your API are your users, and your API needs to be designed with those users in mind.
Your API is how your product exposes its functionality, so if there is functionality in your product not covered by an API, it can’t be covered by a graphical user interface, command line interface, or voice interface either, effectively making the functionality invisible.
Finally, as your API is the first and most important way to access and interact with your product, the API needs to be managed and designed deliberately. Just as you spend time to design your graphical user interface, invest time to design your API: what it exposes, what it does, and how it grows.
Once you realize that your API is an interface that deserves attention of its own, you begin to realize that the API has — and should have — a life of its own.
Your implementation will change as your application evolves and you optimize, refactor and grow the functionality. Your API, however, should not change frequently, but instead grow slowly and deliberately.
Your implementation will change frequently, your API should not.
Here’s another way to think about this approach: If your API is the surface area of your product; the functionality is its volume. Doubling the functionality will only grow your API surface by 25%.
It’s important to think of API evolution in terms of growth and increasing flexibility. Graceful API evolution is additive in terms of functionality, and subtractive in terms of requirements. While change is inevitable, planning for a graceful API evolution is a good way to minimize changes that break things. For example: required input may become optional, but not the other way around.
Treating your API as independent from the implementation, even if that’s harder, allows you to decouple development of API and implementation. Your API becomes a contract and specification to the implementation, instead of just being a thin veneer on top of your implementation.
The third principle of API First Design is about descriptiveness. In order to be used, your API needs to be easily understood by people that have not been involved in its creation. That means documentation.
Usable API documentation is an essential prerequisite to making it consumable by humans. As robots and AI aren’t taking over programming anytime soon, this makes it essential.
When it comes to documentation for APIs, structured documentation beats unstructured documentation. Following a standard pattern for URLs, resource types, request methods, headers, request parameters, and response formats will make it easier to explore and understand functionality, and reduces surprises when your API grows.
Speaking of surprises: in your API design, try to minimize surprises and follow established standards and best practices wherever possible. When it’s impossible to do so, document the deviation so that your API consumers won’t waste hours chasing a bug introduced by an API that is working just a bit differently than expected.
There is one thing that makes even the best documented APIs stand out: APIs that are self-descriptive by using hypermedia constructs like links that allow the discovery of other API resources. This gives you a great way to close the loop from design, over implementation, to documentation.
Follow the Adobe Tech Blog for more developer stories and resources, and check out Adobe Developers on Twitter for the latest news and developer products.
News, updates, and thoughts related to Adobe, developers…
791 
3
Thanks to Rachel Luxemburg. 
791 claps
791 
3
Written by
Principal at Adobe
News, updates, and thoughts related to Adobe, developers, and technology.
Written by
Principal at Adobe
News, updates, and thoughts related to Adobe, developers, and technology.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/databases-that-play-nice-with-your-serverless-backend-13eb7dc1e1c?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
Update (12/3/19): AWS announced a new feature known as Amazon RDS Proxy that makes using RDS databases with a serverless backend significantly easier. This isn’t covered below but it’s a great option to consider in addition to the solutions discussed here. This blog post from the Serverless Framework blog does an excellent job of summarizing it.
"
https://medium.com/ibm-garage/devops-adoption-approach-build-and-deploy-edc2a52c075?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
The new era of DevOps is about the change that brings developers, IT operations, quality assurance, and business together with a common…
"
https://medium.com/@shijuvar/microservices-overview-misinterpretations-and-misuses-56a1979edafb?source=search_post---------357,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Shiju Varghese
Jul 4, 2017·12 min read
For the last three years, I have been working as a consultant and trainer on Golang and cloud-native distributed systems architectures including Microservices. In my consulting experience on Microservices, I have had lot of strange experiences from various clients due to the misinterpretations of Microservices. I work in India, at least here, Microservices is the most misinterpreted terminology in IT industry at this moment. Everyone is talking about Microservices. Lot of people are saying that their applications are based on Microservices architecture regardless of its merits to be called as Microservices or even called as distributed system.
Here’re the few misinterpretations, which I have heard about Microservices:
The list of misinterpretations of Microservices are not end here. This is an endless list. You may use some of the technologies from the preceded list, for building your Microservices, but tied-up with some tools and frameworks to be called as Microservices don’t make any sense.
Microservices is a distributed systems architecture, but development teams, those who never worked on distributed systems, are trying to learn this like learning a new framework, and building applications based on some misinterpretation, and claims that their applications are based on Microservices. You can see lot of articles titled something like Building Microservices with language X (Go, Node.js, Java, etc), Docker and Kubernetes, which may demonstrate building an HTTP service, running in a Docker container and clustering with Kubernetes, to be called as Microservices. Recently in one of my training on Microservices, an attendee who claims himself as an experienced architect, told me that he is a master on Microservices and know everything on it. During my discussion with him, I realized that he has never heard about things like DDD and Bounded Context, Event Sourcing, CQRS, gRPC, etc, and heard these things first time from me. A man who never heard about at least on domain modelling and DDD Bounded Context claims that he is a super expert on Microservices. Many of the startups to which I had discussions, claim that they’re following a Microservices architecture. During the discussions with them, I realized that they call it Microservices because of their products are running with 2–5 apps, and for them each of which app is a Microservice. Their customer facing app claim as one Microservice, an admin app claim as another Microservice, a background worker claim as another Microservice, and a RESTful API claim as another Microservice. People have been coming with their own version of Microservices based on their understanding and what they are comfortable with. The sad part is even people don’t realise that Microservices is a distributed systems architecture.
First and foremost, Microservices is an architectural style for building distributed systems. Microservices tackles the complexity of building distributed systems. In Microservices architecture, software systems are composed of a suite of independently deployable, small, modular services. Each of these Microservices are built around a small business capability, which will be managed by an independent team. A Microservice is an independently deployable component of a small business capability, and Microservice architecture is a style of engineering for building highly scalable and evolvable software systems, made up of fine-grained microservices. In a nutshell, Microservices are small, autonomous services that work together.
In most of the time, you may start an application as a monolith in which all business capabilities are put into a single application that runs in a single process. This kind of applications are easy to develop, but it would be difficult to scale effectively because every components in the applications are tightly coupled together. In microservices architecture, we build applications by composing a suite of autonomous services where each service runs in its own process. With this approach, we can scale easier — we can give different scalability power to different services, upgrade and replace each service in isolation, and gain lot of other architectural benefits.
Many people think that Microservices is a unified architecture framework and a silver bullet that provide solutions for all problems of software engineering. So people ask questions like what’s the security in Microservices, etc. Microservices is not a unified architecture framework that does not represent solutions for all problems in software engineering, but it’s a way to build distributed systems where the core idea is modularisation via services. In another way, Microservices is an idea to build distributed systems, where you need to put various architecture approaches to implement this idea. Microservices is an evolution of various architecture approaches which we have been using for long time. So in a real-world Microservices, you’re still using various architecture approaches which you have used in the past.
Containerisation is a great companion to build and run your Microservices. If you can package your Microservices in containers, it gives you immensive capabilties to scale and dynamically orchestrate your Microservices without depending on where it runs. Packaging tools like Docker, rkt, and orchestration tools like Kubernetes, are great technologies for containerising Microservices.
When you’re moving to Microservices architecture from a monolithic architecture, the big question is how to decompose the Microservices. How to broke up a larger software system into functional components?. What’s the size of a Microservice? The answer is very simple: domain modelling. Sam Newman, on his book titled “Building Microservices” says all roads to Microservices pass through domain modelling. The book titled “Domain-Driven Design: Tackling Complexity in the Heart of Software” by Eric Evans, published in 2003, is a classic book on domain modelling, which provides guideline for building complex software systems based on domain model. Ever since the release of this book, the term Domain-Driven Design (DDD), was widely accepted by the community and have been using DDD as the way to build oftware systems. DDD, introduced various building blocks such as Entity, Value Object, Services, Repositories, Aggregates and Bounded Context.
Aggregates and Bounded Contexts are the building block to build your Microservices, which will decide the size of a single Microservice. Typically business entities like Order, Customer, Account are Aggregates, which represent a graph consisting of a root entity and one or more other entities and value objects, and can be treated as a unit. A better way to perform a transaction in Microservices is to perform persistence against aggregates. Bounded Context is a central pattern in Domain-Driven Design, which provides logical separation of business problem into various sub domains by dividing a large model into different Bounded Contexts. A Bounded Context encapsulates the details of a single domain, a sub domain of large domain model, which can be partitioned into a single Microservices. A common strategy for building Microservices is to build it against each Bounded Context. Each Microservice uses its own database to persist data of domain model of a Bounded Context. A single Bounded Context can include many aggregate roots, or we can organise a single aggregate root into a single Bounded Context. An Order aggregate root, consists of entities such as OrderLine, Customer and Product, and value objects such as ShippingAdress, PaymentMethod etc. Within the graph of Order aggregate root, Customer entity can also be treated as a aggregate root because it can organise as a root entity for getting all information for a customer. Aggregates and Bounded Contexts are important concepts in Microservices architecture, which are the foundational block to build your Microservices, which will also decide the size of your Microservices. In short, Microservices are autonomous services around Bounded Context.
Microservices architecture is indeed a great approach for building distributed systems. But keep in mind that Microservices is neither a silver bullet nor an easy approach. It has never been an easy job to build distributed systems, even with Microservices although it tackles the complexity of building distributed systems.
When you move to Microservices architecture from monolithic applications, you need to solve many practical challenges. For an example, a business transaction may span into several Microservices because we broke up a monolithic system into several autonomous services based on Bounded Context. A transaction may need to perform persistence into many Microservices where you need to manage data consistency. And another challenge is querying data from multiple databases. With a monolithic, you can easily perform inner join queries from a single database. Because the monolithic database has been moved into several databases as part of the decomposition of functional components, you can’t simply execute inner joins, thus you must get data from multiple databases. Here you don’t have any centralized database.
DDD Aggregates and Bounded Contexts are the foundational building blocks of Microservices, but when you’re going to choose a concrete architecture for building your distributed system to solve practical challenges of Microservices, an event-driven reactive system on DDD aggregates would be a great approach. For this, I highly recommend to use Event Sourcing, which is an event-centric architecture to construct the state of an application by composing various events. Event Sourcing deals with an event store of immutable log of events, in which each log (a state change made to an object) represents an application state. Because every state change in the application, is treated as an immutable log, you can easily troubleshoot the application and can also going back to a particular version of application state at any time. An event store is like a version control system. In a Microservices architecture, we can persist aggregates as a sequence of event. Events are facts, which represent some actions happened in the system. These are immutable, which can’t be changed or be retracted. If you would like to make a change in the system, do persist logs into the event store to represent an another set of events. The example of events are OrderCreated, OrderApproved, OrderShipped, OrderDelivered, etc. In your Event Sourcing architecture, when you publish one event from one Microservice, other Microservices can be reactive to those events and publish another set of events. Sometimes, the sequence of events can be compared with Unix pipes. A single transaction in a Microservices system may span into multiple Microservices where we can perform a transaction as a sequence of events by building reactive Microservices. Each state change of an aggregate can be treated as an event, which is an immutable fact about your system. In order to publish events to let other Microservices know about something has happened in our system, we can use messaging systems like Apcera NATS, Kafka, RabbitMQ, etc. My personal choices are Apcera NATS and Google Cloud Pub/Sub. An event-driven, reactive architecture is a great choice of architecture approach for building massively scalable Microservices.
When you make persistence as a sequence of events by using an Event Sourcing, you may need an architecture approach to make queries for your Microservices. An architecture pattern, Command Query Responsibility Segregation (CQRS) is an ideal pattern for implementing queries for your Microservices. As the name implies, CQRS segregates an application into two parts: Commands to perform an action to change the state of aggregates, and Queries to provide a query model for the views of aggregates. We may also use different databases for both write operations and query operations. This will also allows you to make highly performant query model by loading denormalised data sets into data stores of read models. NoSQL/NewSQL databases are great options to store the data of read models.
Although Event Sourcing and CQRS, are great patterns for implementing a distributed system using Microservices architecture, it’s not a silver bullet and it has its own limitations. You may need different architecture style for building some kind of Microservices systems. But generally, I feel that Event Sourcing, paired with CQRS, is a great approach for implementing Microservices systems.
In Microservices architecture, you may need to make lot of inter-process communication between Microservices. Here’re the two options you may use to make inter-process communication between Microservices:
By using an Event Sourcing architecture with a messaging system, you can implement an asynchronous event-driven architecture to manage the state of aggregates. You may also need to make APIs to communicate between Microservices. When you make APIs to perform inter-process communication between Microservices, performance and scalability are very important things. You should not feel that there are lot of communications are happening over networks by building high performance APIs. When you build massively scalable systems, JSON based RESTful APIs are not a good option because of the performance challenges and the lack of capability to expose domain-specific operations as APIs as RESTful systems are working over the concept of resources. Here, gRPC, a high performance, open-source remote procedure call (RPC) framework, can be used for building massively scalable APIs for the inter-process communication between Microservices. By default, gRPC uses Protocol Buffers as the Interface Definition Language (IDL) and as its underlying message interchange format. gRPC is an open source version of Google’s internal framework, Stubby, which is used to scale 10 billion API calls per second. gRPC can be called as a protocol for inter-process communication in Microservices architecture over APIs.
As I mentioned in the beginning of this post, there’re lot of misinterpretations on Microservices, are coming from various developer communities. I have been getting lot of requests for conducting a workshop on Microservices from various clients, and most of the time, clients are coming with a course agenda, which is really shocking as it’s not about anything on Microservices. Lot of developer communities are tied-up Microservices with a some frameworks and tools. Lot of Java people believe that building RESTful APIs with Spring Boot and leveraging some tools of Netflix OSS is Microservices while some people from .Net community say that running applications with Azure Service Fabric is Microservices. Even I have been asked questions like what’s the difference between a RESTful service and a Microservice, as many people think that building some light-weight RESTful APIs are Microservices. For some other people, building those APIs using Spring Boot is Microservices. I don’t know from where the Spring Boot came to the picture as a misinterpretation to be called as Microservices.
I feel that people just heard the hype and associating it with some tools and frameworks without understanding the soul and purpose of this evolutionary architecture. The situation is very similar like architecture patterns like SOA and some engineering practices like Agile. I know lot of companies are claiming that they’re following agile engineering because they’re just following Scrum process even though they’re building applications with a highly conventional manner. For the sake of TDD, lot of organisations are writing unit tests after writing the production code, and then just trying for getting a target test coverage, to be called as engineering applications with TDD. Even though it is a minor community, but lot of people are adopting technologies, patterns and practices just for the sake of technologies and to be called as something.
Microservices is indeed great architectural style for building massively scalable applications. Microservices is also well suited for building internet-scale applications like Netflix, Uber, Amazon, eBay, etc. But this is not for everyone and it’s not a silver bullet. Although Microservices is a right architectural approach for building massively scalable applications including public face internet-scale applications, it might be a wrong choice for most of other kind of applications, especially for building enterprise applications, which has a complex domain model. Personally, I don’t recommend Microservices for a complex business applications, which has very complex domain model. While Microservices might be an ideal for a massively scalable application in which domain model is not much complex, and scalability is the most challenging factor to be called as a successful application. For some kind of applications, a Microservices architectural style of implementation may create lot of performance problems and add complexities to the sytem and finally there is a chance to failure. Adopting Microservices for the sake of its hype may make lot of side effects to your system.
In my consulting experience, I realised that Microservices architecture is not required for most of the use cases, and a middle-grounded solution is better approach to engineering most of the applications. You can adopt a hybrid approach to solve your problems. Your company may not have the muscle power of engineering capabilities as organisations like Google, Amazon, Netflix, Uber, eBay, Square, etc. Your problem is unique within your organisation. An architecture approach should solve your problem and use it for solving your own unique problems instead adopting something for the sake of that.
You can improve your existing monolithic applications in different ways: splitting your monolithic application into multiple systems without blindly following the Microservices guidelines, but with a hybrid architecture approach and with a DevOps culture and a modern CI/CD pipeline. The great benefit of Microservices is modularisation. When you build new applications, you can architect systems by applying modular system design principles without a Microservices architecture in front of you. For example, the package ecosystem of Go programming language lets you design your applications with better modularity. This approach will also enable you easier migration to Microservices if you would like to move to Microservices later on for scaling modules and teams independently. You don’t need to start a new application with Microservices. It is better to start as a monolith application by applying modular system design principles, and when your system and teams are evolving, you can move to Microservices if it is really essential.
Keep in mind that your company and products are unique. Instead of becoming a poster child of technologies, patterns and practices, solve your own unique problems by using middle-ground solutions. Adopting technologies for the sake of technologies don’t make any sense.
You can follow me on twitter at @shijucv. I do provide training and consulting on Go programming language (Golang) and distributed systems architectures, in India.
Consulting Solutions Architect and Trainer on Go and Distributed Systems, with focus on Microservices and Event-Driven Architectures. Author of two books on Go.
See all (310)
568 
10
568 claps
568 
10
Consulting Solutions Architect and Trainer on Go and Distributed Systems, with focus on Microservices and Event-Driven Architectures. Author of two books on Go.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jgwest/microclimate-a-new-container-based-multi-language-cloud-friendly-development-tool-98a2d03326a9?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan West
Oct 1, 2018·10 min read
Microclimate is a new free-of-charge development tool from IBM that makes it easy to create Java/Node/Spring/Swift cloud-native microservices. But, unlike traditional IDEs, with Microclimate your full development environment (including the IDE itself) is entirely contained within lightweight Docker containers that are running on your local machine (or optionally, in Kubernetes). Now, you may ask, why have we at IBM created yet another development tool? Excellent question… read on to find out!
Microclimate is our team’s vision for the future of microservice/cloud-native application development tools, targeting a full range of cloud environments. This include Docker-based containers on IaaS VMs, hybrid cloud environments through IBM Cloud Private, public cloud Kubernetes hosted on the IBM Cloud Kubernetes service, as well as general support for similar environments in the clouds of other major providers.
As of this writing, the Microclimate project is still new and growing, with it’s first public release dating back to only February 2018 (and it’s first internal release not far behind that).You can learn more by visiting our product page at https://microclimate-dev2ops.github.io/.
There are many features that set Microclimate apart from other development environments. Here are just some of the highlights:
Microclimate is built from the ground up to support the polyglot development model, where small independent applications (ie microservices) written in different languages/runtimes combine to form a single large application. This allows you to choose the best language/runtime combination for the job at hand: for example, a Node.js-based application may prove most adept at implementing a Web frontend that interacts with the browser, while Java may be better suited to backend business logic, or Swift better suited to a mobile applications frontend.
This multi-language multi-runtime world requires a development tool that treats all environments as equal first-class citizens: these different environments should run side-by-side in the UI, while still providing the features that developers expect from an IDE for that language (for example, content assist/intellisense). Microclimate lives in this brave new polyglot world, allowing you to create Java/Node/Spring/Swift applications that work together and exist side-by-side within a single consistent UI. Within this environment we include full real time code intelligence powered by the Language Server Protocol (LSP) technology. Microclimate is one tool to rule them all!
When you download the Microclimate installer, you will see it is a tiny file: only about 750KB. This is because when you extract and run it, the full contents of Microclimate will download as a self-contained set of Docker images. Microclimate exists entirely within these pre-built Docker images, and the footprint on your machine that is outside these containers is limited to the source code of your Java/Spring/Swift/Node projects.
Microclimate’s container-based architecture enables the polyglot development model described above by automatically downloading the required language/runtime development tools when you first begin developing in a new language.
For example, when you create or import a Java application, Microclimate will automatically download the Java JDK (the compiler and Java APIs), Maven (build system), and WebSphere Liberty Docker images (web container). Unlike traditional IDEs that require you to download/install these language-specific packages on your own, Microclimate grabs them on your behalf both automatically and seamlessly. And it’s not just Java: the Node, Spring, and Swift development/ecosystem tools are fully supported as well.
Microclimate gives you the choice of which editor or IDE you wish to use to develop. Included as part of the Microclimate browser-based UI is a code editor based on Eclipse Theia, which is a open source project designed from the ground up to provide a rich development environment delivered through web technologies.
In addition to developing using Microclimate’s integrated Theia editor, you can also develop using the IDE of your choice, whether it be Eclipse, VSCode, IntelliJ Idea, Sublime/Atom, etc. Code that is generated or imported into Microclimate is available outside the container as well, using a Docker bind mount, which means that code can easily be imported into your own IDE as easily as importing code from another local directory.
All your of Microclimate-based project files are accessible as a folder on your local machine under the ‘microclimate-workspace’ directory. Any changes you make here will be visible both inside and outside the Microclimate UI, and from here you may access the other Microclimate functionality. More information about using an IDE is here.
Even if you are not using the Microclimate code editor you can still take advantage of Microclimate’s many other features, including self-contained application containers, internal build system, build and application logs, integrated performance metrics, and Jenkins-based delivery pipeline.
As developers, our general day-to-day development workflow looks like this: we write some code, we wait for that code to build, we deploy that build, and then we run our application on that deployment. Hopefully our new code made our application better rather than worse, but using feedback from our changes we then do some more coding. Next we wait for that code to build, deploy it, and run it. That loop, what we refer to as the development inner loop — code, build, deploy, run — must be as fast possible, in order to keep your developers both happy and productive.
Personally as a developer I like to be as happy and productive as possible 😄, and as such in Microclimate this fast turnaround time has been a core focus. We want to avoid the scenario you may see in some other container-based development tools where the build and deploy steps take much longer than they should, leaving the programmer to get distracted by nearby shiny objects.
In Microclimate, using various clever Docker/Kubernetes tactics, we have achieved that fast turnaround time, reducing our waiting time to as slim a slice as possible.
Microservice projects that are generated by Microclimate are container native. That is, they are hosted in, built by, and deployed to, Docker containers, with the container images themselves being as close as possible to the Java/Node/Spring/Swift production containers you’ll find on Docker Hub. All of this combines to ensure that not only is it easy to build and run your project, but also that it will be seamless to transition that container into production.
Behind the scenes, when you create a new project or import an existing one, we automatically create a Dockerfile for that project as well as configuring an automated build (for example, a Maven build for Java). Microclimate then manages the lifecyle of that application, building and hosting it as a containerized application.
At any time you can see your application happily running along if you run the docker ps command (to list all running containers), or from within the deployment resource list in the Web UI of IBM Cloud Private (where containers are run at ‘enterprise scale’).
And to double-down on our ease of deployment, with just a few clicks Microclimate will create a Jenkins-based delivery pipeline to automatically deploy your containers to IBM Cloud Private or the IBM Cloud Kubernetes Service.
So you’ve developed an application in Microclimate, and now you need somewhere to host it: Microclimate makes it easy to create a deployment pipeline between your microservice project and the target IBM Cloud Kubernetes server.
With just a few clicks inside the Microclimate UI, a Jenkins-based delivery/deployment pipeline is configured in your target cloud environment. From that point on, any commits made to your projects will trigger a build that will then be packaged into a Docker image and uploaded to the target Docker image registry. The applications deployed by the Jenkins delivery pipeline are managed as Helm charts (linked to the Jenkins-built image in your Docker registry), and thus are visible with the other Helm releases from within the IBM Cloud Private or IBM Kubernetes Service user interfaces.
In less than a minute and with only a small handful of steps, you can go from starting the Microclimate environment for the first time, to creating and running your first Java/Node/Spring/Swift microservice.
When designing our user interface we exercised careful, considered focus to ensure that it is intuitive to new users, thus minimizing the barrier-of-entry to getting a real application up and running in the language and runtime of your choice.
You can throw out the excessively long step-by-step guides required by other IDEs in order to create and deploy a new project, because with Microclimate each next step is obvious and intuitive. Skeptical, or, curious to know more? Browse through the screenshots in this article and then try it for yourself!
If you’ve ever hit a Java OutOfMemoryException, or been baffled by why your supposedly lightweight code is eagerly consuming a full CPU, then you have been “lucky” enough to work with code that strains the limits of its virtual environment. Finding and fixing performance issues such as these can be tricky, and having the right performance tool to point you in the correct direction is key.
That’s why every application running in MC is automatically configured to run with performance monitoring by default, which includes CPU usage, heap usage, % of time spent garbage collecting, HTTP Incoming Requests, HTTP Throughput, and maximum heap used after GC.
Also included is response time monitoring for individual HTTP endpoints, which includes total endpoint hits, average response time (ms), and longest response time (ms).
In addition we include the ability to further drive application load to test performance using JMeter, which you can kick off by clicking Run load in the App monitor tab.
In the past, the user interface of an IDE would traditionally have been coded using a native widget toolkit specific to a particular operating system, in tools like XCode (Cocoa), Visual Studio (Windows Forms/WPF/MFC/etc), or Eclipse (SWT, which interfaces with GTK/Cocoa/Windows).
In recent years, however, Web technologies like HTML, CSS, JavaScript, the HTML Canvas, and WebGL have been used to create richer user interfaces, as seen in tools like Visual Studio Code, Slack, and Atom. Applications based on Web technologies mostly maintain the overall feel of a native application, though not necessarily the specific look of one.
Microclimate is based on those same web technologies, both through our browser-based code editor Eclipse Theia, but also through the suite of tools that are integrated into Microclimate at the browser level. This ensures that Microclimate provides the same rich UI of other competing tools, while enabling fluid animation, vibrant colours, sophisticated layouts, and custom widgets.
Our UI is based on IBM’s Carbon Design System, which is built to provide beautiful and intuitive designs that are logically oriented around consistent guidelines. Not only is the Microclimate UI attractive and modern, but it also shares a unified design experience with our sister products in IBM Cloud and IBM Cloud Private.
Microclimate can be hosted either locally or on IBM Cloud Private (based on Kubernetes). This allow you to host multiple self-contained developer environments in a private cloud environment. Microclimate containers hosted on IBM Cloud Private can be installed through Helm, either through the catalog UI or the Helm CLI tool. Ultimately hosting on IBM Cloud Private means that the resources of the cluster are distributed across developers, and are fully managed inside the cloud, with developers requiring only a thin client (the web browser) to connect.
Hosting your development environment in the cloud has the advantage of persisting your workflow across individual machines and across physical contexts: if you start developing your code, but turn off your machine and head out of the office, when you later return to your cloud-based environment it will be exactly as you left it. Cloud-based development environments are likewise not tied to a specific machine, but are accessible from anywhere.
The best part of Microclimate is that you don’t have to take my word on all of its great features… you can try it out for yourself right now! Microclimate is available for download from our website.
The installer is less than a single megabyte, and running the installer will automatically download Microclimate into self-contained Docker images hosted on your local machine.
So try it out, and let us know what you think!
315 
2
315 
315 
2
"
https://medium.com/@odedia/spring-session-redis-part-i-overview-a5f6c7446c8b?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Oded Shopen
Nov 22, 2017·9 min read
In the beginning, there was the servlet container. And it was good.
With it, we got the infrastructure for session management , a.k.a. the HttpSession. The idea behind it was simple: The server would bind the session to a unique token and send that token to the browser. The browser would send the token with each request, usually in the form of a cookie, and the server would identify the browser session for that token on the server side. This simple solution allowed us to build stateful web applications over the stateless HTTP protocol.
For better or worse, you could use a load balancer that supported sticky sessions to route the same client to the same instance. That allowed us to have multiple server instances in a production environment.
And then there was the cloud… and with them, microservices.
Suddenly, you no longer had a couple of instances that needed to know the session details. You had hundreds, if not thousands of instances, in a typical production environment.
Additionally, the chances of having a particular instance die in production grew exponentially. Virtual machines and containers may die and resurrect based on the cloud infrastructure you were using, and your application had to accept the fact that a single instance WILL fail at some point, thus kicking out all the users routed to that instance.
Lastly, HttpSession clearly became an anti-pattern. It does not conform to the cloud-native 12 factors app guidelines, specifically factor #6:
“Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service, typically a database.”
Clearly, the HttpSession stores stateful data about its users, and as such it makes the entire process stateful. You bind the process with the data, which creates a tight coupling and an expectation that the process will never fail ungracefully.
We needed a better solution.
For many scenarios, the industry moved on to fully stateless authentication mechanisms such as Java Web Tokens. And those are great! Mostly… For mobile applications, JWT seems like the way to go. For classic web applications, including single-page applications such as those based on Angular, there are still major benefits for server-side session management.
That’s where Spring Session comes in. I love the idea behind this framework. In a very Spring Boot fashion, the idea is as follows: replace an existing implementation with an abstraction layer that can be one of many implementations at runtime, based on your runtime dependencies.
For Spring Session specifically, this meant replacing the HttpSession with an implementation of your choice. There are quite a few of them available out of the box, and if you’re bored you can implement your own:
From the list above, Spring Session Redis was one of the first implementations and is still one of the most popular.
Redis is an in-memory database that is perfect to managing resources such as sessions, which require very fast access times and the ability to self-expire. The database is single-threaded, And as such can make optimisations that promise performance of up to millions of operations per second (in theory…).
RedisLabs Enterprise is a commercial solution that allows you to easily manage and monitor a clustered redis environment. It also allows you to access an entire Redis cluster via a simple proxy endpoint.
According to Redislabs’ latest survey, redis is mostly used for caching and user session management.
Since the session no longer resides as part of the application itself and is instead stored in a database, it conforms to factor #6 above.
Redis is written in C, and can be installed on any linux or unix based system. If you’re using a Mac, I highly recommend using homebrew to install the database. Although not officially supported, you can run redis on windows as well.
As an alternative to the open source version, you can download a trial version of Redislabs Enterprise here.
Execute brew install redis to get started:
In this demo, we will build a simple API that returns some dummy order information.
The source code for this demo is available here. The configuration repository for the application can be found here.
As a baseline for any decent Spring Cloud application, we will require at least a configuration server to host our property files and a service discovery solution. We will use Spring Cloud Config Server and Spring Cloud Netflix Eureka, respectively.
Head over to start.spring.io and create a config server:
Repeat the process for Eureka Discovery Server:
Make sure your redis database runs in the background by running redis-server in the terminal:
Our service discovery application is very simple:
This simple, one-class application annotated with @EnableEurekaServer boots a eureka server for us.
Let’s have a look at its bootstrap.yml:
We set the port to the default 8761 eureka port, and make sure that the server would not register with itself by setting register-with-eureka and fetch-registry to false.
The config server is also quite simple:
Now that our infrastructure is in place, let’s move on to the API Gateway.
Our gateway requires several dependencies:
Let’s review the GatewayApplication class:
The application.yml for the gateway defines several important parameters:
The LoginManagementResource class defines a login and logout rest APIs. Note that these are not mandatory. Any REST API call that would receive a valid username and password with HTTP Basic Authentication would generate a new session that would be returned as a token back to the client.
Let’s see what happens when we call the /login API:
If we’ll connect to redis using redis-cli, we can see the key is in the database:
Our very simple “microservice” simulates a response from a backend system.
Let’s review the configuration for the microservice:
Let’s test our call. Using the same session ID as provided in the previous step, we can make a call through the gateway to our microsevice:
In this short introduction, I showed you how to use Spring Session Redis to externalise session management from your application executable to a separate DB.
In the next article, we’ll focus on some production considerations when using Spring Session Redis.
Stay tuned and happy coding!
Oded Shopen
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
333 
8
333 
333 
8
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
"
https://itnext.io/securing-the-configuration-of-kubernetes-cluster-components-c9004a1a32b3?source=search_post---------360,"In the previous article of this series Securing Kubernetes for Cloud Native Applications, we discussed what needs to be considered when securing the infrastructure on which a Kubernetes cluster is deployed. This time around, we’re turning our attention to the cluster itself.
Kubernetes is a complex system, and the diagram above shows the many different constituent parts that make up a cluster. Each of these components needs to be carefully secured in order to maintain the overall integrity of the cluster.
We won’t be able to cover every aspect of cluster-level security in this article, but we’ll aim to address the more important topics. As we’ll see later, help is available from the wider community, in terms of best-practice security for Kubernetes clusters, and the tooling for measuring adherence to that best-practice.
We should start with a brief observation about the many different tools that can be used to install the cluster components.
Some of the default configuration parameters for the components of a Kubernetes cluster, are sub-optimal from a security perspective, and need to be set correctly to ensure a secure cluster. Unless you opt for a managed Kubernetes cluster (such as that provided by Giant Swarm), where the entire cluster is managed on your behalf, this problem is exacerbated by the many different cluster installation tools available, each of which will apply a subtly different configuration. While most installers come with sane defaults, we should never consider that they have our backs covered when it comes to security, and we should make it our objective to ensure that whichever installer mechanism we elect to use, it’s configured to secure the cluster according to our requirements.
Let’s take a look at some of the important aspects of security for the control plane.
The API server is the hub of all communication within the cluster, and it’s on the API server where the majority of the cluster’s security configuration is applied. The API server is the only component of the cluster’s control plane, that is able to interact directly with the cluster’s state store. Users operating the cluster, other control plane components, and sometimes cluster workloads, all interact with the cluster using the server’s HTTP-based REST API.
Because of its pivotal role in the control of the cluster, carefully managing access to the API server is crucial as far as security is concerned. If somebody or something gains unsolicited access to the API, it may be possible for them to acquire all kinds of sensitive information, as well as gain control of the cluster itself. For this reason, client access to the Kubernetes API should be encrypted, authenticated, and authorized.
To prevent man-in-the-middle attacks, the communication between each and every client and the API server should be encrypted using TLS. To achieve this, the API server needs to be configured with a private key and X.509 certificate.
The X.509 certificate for the root certificate authority (CA) that issued the API server’s certificate, must be available to any clients needing to authenticate to the API server during a TLS handshake, which leads us to the question of certificate authorities for the cluster in general. As we’ll see in a moment, there are numerous ways for clients to authenticate to the API server, and one of these is by way of X.509 certificates. If this method of client authentication is employed, which is probably true in the majority of cases (at least for cluster components), each cluster component should get its own certificate, and it makes a lot of sense to establish a cluster-wide PKI capability.
There are numerous ways that a PKI capability can be realised for a cluster, and no one way is better than another. It could be configured by hand, it may be configured courtesy of your chosen installer, or by some other means. In fact, the cluster can be configured to have its own in-built CA, that can issue certificates in response to certificate signing requests submitted via the API server. Here, at Giant Swarm, we use an operator called cert-operator, in conjunction with Hashicorp’s Vault.
Whilst we’re on the topic of secure communication with the API server, be sure to disable its insecure port (prior to Kubernetes 1.13), which serves the API over plain HTTP (--insecure-port=0)!
Now let’s turn our attention to controlling which clients can perform which operations on which resources in the cluster. We won’t go into much detail here, as by and large, this is a topic for the next article. What’s important, is to make sure that the components of the control plane are configured to provide the underlying access controls.
When an API request lands at the API server, it performs a series of checks to determine whether to serve the request or not, and if it does serve the request, whether to validate or mutate the the resource object according to defined policy. The chain of execution is depicted in the diagram above.
Kubernetes supports many different authentication schemes, which are almost always implemented externally to the cluster, including X.509 certificates, basic auth, bearer tokens, OpenID Connect (OIDC) for authenticating with a trusted identity provider, and so on. The various schemes are enabled using relevant config options on the API server, so be sure to provide these for the authentication scheme(s) you plan to use. X.509 client certificate authentication requires the path to a file containing one or more certificates for CAs (--client-ca-file), for example. One important point to remember, is that by default, any API requests that are not authenticated by one of the authentication schemes, are treated as anonymous requests. Whilst the access that anonymous requests gain can be limited by authorization, if they’re not required, they should be turned off altogether (--anonymous-auth=false).
Once a request is authenticated, the API server then considers the request against authorization policy. Again, the authorization modes are a configuration option (--authorization-mode), which should at the very least be altered from the default value of AlwaysAllow. The list of authorization modes ideally should include RBAC and Node, the former for enabling the RBAC API for fine-grained access control, and the latter to authorize kubelet API requests (see below).
Once an API request has been authenticated and authorized, the resource object can be subject to validation or mutation before it’s persisted to the cluster’s state database, using admission controllers. A minimum set of admission controllers are recommended for use, and shouldn’t be removed from the list, unless there is very good reason to do so. Additional security related admission controllers that are worthy of consideration are:
Dynamic admission control, which is a relatively new feature in Kubernetes, aims to provide much greater flexibility over the static plugin admission control mechanism. It’s implemented with admission webhooks and controller-based initializers, and promises much for cluster security, just as soon as community solutions reach a level of sufficient maturity.
The kubelet is an agent that runs on each node in the cluster, and is responsible for all pod-related activities on the node that it runs on, including starting/stopping and restarting pod containers, reporting on the health of pod containers, amongst other things. After the API server, the kubelet is the next most important cluster component to consider when it comes to security.
The kubelet serves a small REST API on ports 10250 and 10255. Port 10250 is a read/write port, whilst 10255 is a read-only port with a subset of the API endpoints.
Providing unfettered access to port 10250 is dangerous, as it’s possible to execute arbitrary commands inside a pod’s containers, as well as start arbitrary pods. Similarly, both ports provide read access to potentially sensitive information concerning pods and their containers, which might render workloads vulnerable to compromise.
To safeguard against potential compromise, the read-only port should be disabled, by setting the kubelet’s configuration, --read-only-port=0. Port 10250, however, needs to be available for metrics collecting and other important functions. Access to this port should be carefully controlled, so let’s discuss the key security configurations.
Unless its specifically configured, the kubelet API is open to unauthenticated requests from clients. It’s important, therefore, to configure one of the available authentication methods; X.509 client certificates, or requests with Authorization headers containing bearer tokens.
In the case of X.509 client certificates, the contents of a CA bundle needs to be made available to the kubelet, so that it can authenticate the certificates presented by clients during a TLS handshake. This is provided as part of the kubelet configuration (--client-ca-file).
In an ideal world, the only client that needs access to a kubelet’s API, is the Kubernetes API server. It needs to access the kubelet’s API endpoints for various functions, such as collecting logs and metrics, executing a command in a container (think kubectl exec), forwarding a port to a container, and so on. In order for it to be authenticated by the kubelet, the API server needs to be configured with client TLS credentials (--kubelet-client-certificate and --kubelet-client-key).
If you’ve taken the care to configure the API server’s access to the kubelet’s API, you might be forgiven for thinking ‘job done’. But this isn’t the case, as any requests hitting the kubelet’s API that don’t attempt to authenticate with the kubelet, are deemed to be anonymous requests. By default, the kubelet passes anonymous requests on for authorization, rather than rejecting them as unauthenticated.
If it’s essential in your environment to allow for anonymous kubelet API requests, then there is the authorization gate, which gives some flexibility in determining what can and can’t get served by the API. It’s much safer, however, to disallow anonymous API requests altogether, by setting the kubelet’s --anonymous-auth configuration to false. With such a configuration, the API returns a 401 Unauthorized response to unauthorized clients.
With authorizing requests to the kubelet API, once again it’s possible to fall foul of a default Kubernetes setting. Authorization to the kubelet API operates in one of two modes; AlwaysAllow (default) or Webhook. The AlwaysAllow mode does exactly what you’d expect - it will allow all requests that have passed through the authentication gate, to succeed. This includes anonymous requests.
Instead of leaving this wide open, the best approach is to offload the authorization decision to the Kubernetes API server, using the kubelet’s --authorization-mode config option, with the webhook value. With this configuration, the kubelet calls the SubjectAccessReview API (which is part of the API server) to determine whether the subject is allowed to make the request, or not.
In older versions of Kubernetes (prior to 1.7), the kubelet had read-write access to all Node and Pod API objects, even if the Node and Pod objects were under the control of another kubelet running on a different node. They also had read access to all objects that were contained within pod specs; the Secret, ConfigMap, PersistentVolume and PersistentVolumeClaim objects. In other words, a kubelet had access to, and control of, numerous resources it had no responsibility for. This is very powerful, and in the event of a cluster node compromise, the damage could quickly escalate beyond the node in question.
For this reason, a Node Authorization mode was introduced specifically for the kubelet, with the goal of controlling its access to the Kubernetes API. The Node authorizer limits the kubelet to read operations on those objects that are relevant to the kubelet (e.g. pods, nodes, services), and applies further read-only limits to Secrets, Configmap, PersistentVolume and PersistentVolumeClaim objects, that are related specifically to the pods bound to the node on which the kubelet runs.
Limiting a kubelet to read-only access for those objects that are relevant to it, is a big step in preventing a compromised cluster or workload. The kubelet, however, needs write access to its Node and Pod objects as a means of its normal function. To allow for this, once a kubelet’s API request has passed through Node Authorization, it’s then subject to the NodeRestriction admission controller, which limits the Node and Pod objects the kubelet can modify — its own. For this to work, the kubelet user must be system:node:<nodeName>, which must belong in the system:nodes group. It’s the nodeName component of the kubelet user, of course, which the NodeRestriction admission controller uses to allow or disallow kubelet API requests that modify Node and Pod objects. It follows, that each kubelet should have a unique X.509 certificate for authenticating to the API server, with the Common Name of the subject distinguished name reflecting the user, and the Organization reflecting the group.
Again, these important configurations don’t happen automagically, and the API server needs to be started with Node as one of the comma-delimited list of plugins for the --authorization-mode config option, whilst NodeRestriction needs to be in the list of admission controllers specified by the --enable-admission-plugins option.
It’s important to emphasize that we’ve only covered a sub-set of of the security considerations for the cluster layer (albeit important ones), and if you’re thinking that this all sounds very daunting, then fear not, because help is at hand.
In the same way that benchmark security recommendations have been created for elements of the infrastructure layer, such as Docker, they have also been created for a Kubernetes cluster. The Center for Internet Security (CIS) have compiled a thorough set of configuration settings and filesystem checks for each component of the cluster, published as the CIS Kubernetes Benchmark.
You might also be interested to know that the Kubernetes community has produced an open source tool for auditing a Kubernetes cluster against the benchmark, the Kubernetes Bench for Security. It’s a Golang application, and supports a number of different Kubernetes versions (1.6 onwards), as well as different versions of the benchmark.
If you’re serious about properly securing your cluster, then using the benchmark as a measure of compliance, is a must.
Evidently, taking precautionary steps to secure your cluster with appropriate configuration, is crucial to protecting the workloads that run in the cluster. Whilst the Kubernetes community has worked very hard to provide all of the necessary security controls to implement that security, for historical reasons some of the default configuration overlooks what’s considered best-practice. We ignore these shortcomings at our peril, and must take the responsibility for closing the gaps whenever we establish a cluster, or when we upgrade to newer versions that provide new functionality.
Some of what we’ve discussed here, paves the way for the next layer in the stack, where we make use of the security mechanisms we’ve configured, to define and apply security controls to protect the workloads that run on the cluster. The next article is called Applying Best Practice Security Controls to a Kubernetes Cluster.
Written by Puja Abbassi — Developer Advocate @ Giant Swarm
twitter.com
ITNEXT is a platform for IT developers & software engineers…
249 
249 claps
249 
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/hackernoon/why-dataproc-googles-managed-hadoop-and-spark-offering-is-a-game-changer-9f0ed183fda3?source=search_post---------361,"There are currently no responses for this story.
Be the first to respond.
So far I’ve written articles on Google BigQuery (1,2,3,4,5) , on cloud-native economics(1,2), and even on ephemeral VMs (1). One product that really excites me is Google Cloud Dataproc — Google’s managed Hadoop, Spark, and Flink offering. In what seems to be a fully commoditized market at first glance, Dataproc manages to create significant differentiated value that bodes to transform how folks think about their Hadoop workloads.
Jobs-first Hadoop+Spark, not Clusters-first
Typical mode of operation of Hadoop — on premise or in cloud — require you deploy a cluster, and then you proceed to fill up said cluster with jobs, be it MapReduce jobs, Hive queries, SparkSQL, etc. Pretty straightforward stuff.
Services like Amazon EMR go a step further and let you run ephemeral clusters, enabled by separation of storage and compute through EMRFS and S3. This means that you can discard your cluster while keeping state on S3 after the workload is completed.
Google Cloud Platform has two critical differentiating characteristics:
When your clusters start in well under 90 seconds (under 60 seconds is not unusual), and when you do not have to worry about wasting that hard-earned cash on your cloud provider’s pricing inefficiencies, you can flip this cluster->jobs equation on its head. You start with a job, and you acquire a cluster as a step in job execution.
If you have a MapReduce job, as long as you’re okay with paying the 60 second initial boot-up tax, rather than submitting the job to an already-deployed cluster, you submit the job to Dataproc, which creates a cluster on your behalf on-demand. A cluster is now a means to an end for job execution.
Again, this is only possible with Google Dataproc, only because of:
Operational and economic benefits are obvious and easily realized:
I’m sure I’m forgetting others. Feel free to leave a comment here to add color. Best response gets a collectors’ edition Google Cloud Android figurine!
Dataproc is as close as you can get to serverless and cloud-native pay-per-job with VM-based architectures — across the entire cloud space. There’s nothing even close to it in that regard.
Dataproc does have a 10-minute minimum for pricing. Add the sub-90 second cluster creation timer, and you rule out many relatively lightweight ad-hoc workloads. In other words, this works for big serious batch jobs, not ad-hoc SQL queries that you want to run in under 10 seconds. I write on this topic here.(do let us know if you have a compelling use case that leaves you asking for less than a 10-minute minimum).
The rest of the Dataproc goodies
Google Cloud doesn’t stop there. There’s a few other benefits of Dataproc that truly make your life easier and your pockets fuller:
Dataproc for stateful clusters
Now if you are running a stateful cluster with, say Impala and Hbase on HDFS, Dataproc is a nice offering here too, if for some reason you don’t want to run Bigtable + BigQuery.
If you are after the biggest baddest disk performance on the market, why not go with something that resembles RAM more than SSD in terms of performance — Google’s Local SSD? Mr. Dinesh does a great job comparing Amazon’s and Google’s offerings here. Cliff notes — Local SSD is really, really, really good — really.
Finally, Google’s Sustained Use Discounts automatically rewards folks who run their VMs for longer periods of time, up to 30% off. No contracts and no commitments. And, thank goodness, no managing your Reserved Instance bills.
You win if you use Google’s VMs for short bursts, and you win when you use Google for longer periods.
Economics of Dataproc
We discussed how Google’s VMs are typically much cheaper through Preemptible VMs, Custom VMs, Sustained Use Discounts, and even lower list pricing. Some folks find the difference to be 50% cheaper!
Two things that studying Economics taught me (put down your pitchforks, I also did Math) — the difference between soft and hard sciences, and the ability to tell a story with two-dimensional charts.
Let’s assume a worst-case scenario, in which EMR and Dataproc VM prices are equal. We get this chart, which hopefully requires no explanation:
If you believe our good friend thehftguy’s claims that Google is 50% cheaper (after things like Preemptible VMs, Custom VMs, Sustained Use Discounts, etc), you get this compelling chart:
When you’re dishing out your precious shekels to your cloud provider, think of all this extra blue area that you’re volunteering to pay that’s entirely spurious. This is why many of Dataproc’s customers don’t mind paying egress from their non-Google cloud vendors to GCS!
Summary
Google Cloud has the advantage of a second-comer. Things are simpler, cheaper, and faster. Lower-level services like instances (GCE) and storage (GCS) are more powerful and easier to use. This, in turn, lets higher-level services like Dataproc be more effective:
Fundamentally, Dataproc lets you think in terms of jobs, not clusters. You start with a job, and you get a cluster as just another step in job execution. This is a very different mode of thinking, and we feel that you’ll find it compelling.
You don’t have to take my word for it — good folks at O’Reilly had this to say about Dataproc and EMR.
Find me on twitter at @thetinot . Happy to chat further!
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
250 
4
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
250 claps
250 
4
Written by
VP of PM at Firebolt. All views are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
VP of PM at Firebolt. All views are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://chatbotsmagazine.com/what-is-a-conversational-ui-and-why-it-matters-de358507b9a2?source=search_post---------362,"Many researchers believe conversational AI will soon be developers’ first priority, surpassing cloud-native and mobile-first projects in importance. Chatbots — built on a foundation of containerized microservices and connecting to back-end systems are not only an easy way to break into conversational UIs, they’re an onramp to building and training artificial intelligence. Well, what is a Conversational UI?
A Conversational UI gives the privilege of interacting with the computer on human terms. It is a paradigm shift from the earlier communications achieved either by entering syntax-specific commands or clicking icons. Conversational interface allows a user to tell the computer what to do. Conversational UI is more social in the way the user “contacts”, “invites” and “messages” than the traditional apps that are technological in nature where the user downloads and installs.
Rewinding to the BC days, before chatbots arrived, customers were assisted by shop assistants during their visit to a shop. The shop assistant used pre-defined scripts to respond to customer queries. Fast forward to the AC, time after the chatbots hit the market; chatbots on a website are creating conversational websites and interacting with the customer in the same way a shop assistant would do in the past. Conversational UI takes two forms — voice assistant that allows you to talk and chatbots that allow you to type.
Tech giants Amazon, Google, Microsoft and Google have not only introduced voice assistants but are also making the voice assistants smarter by the day. Hey Cortana from Microsoft, Ok Google from Google, Hey Siri from Apple and Echo from Amazon are classic cases of voice assistants responding to the user by voice. Users can ask these voice assistants to show the trailer of a movie, book tables at a restaurant, schedule an appointment among other things.
On the Chatbot front, Facebook M is a classic example that allows real time communication. The human-assisted chatbot allows customers to do several things from transferring money to buying a car. Slack’s slackbot is another shining example of a chatbot. This human-assisted chatbot allows the user to do many things. If there is a slackbot for scheduling meetings, there is a slackbot for tracking coworkers’ happiness and taking lunch orders.
There’s more to conversational interface than the way they recognize a voice. Conversational interfaces have kindled companies’ interest by presenting an intelligent interface. The intelligence does not result merely from words being recognized as text transcription, but from getting a natural-language understanding of intentions behind those words. The intelligence also combines voice technologies, artificial intelligence reasoning and contextual awareness.
The interface is platform-agnostic working well across desktop, smartphone and smartwatch. Conversational UI also work well in devices that do not have screens, as that of Amazon Echo. The most alluring feature of conversational interfaces is the way they facilitate frictionless experiences for a user working with a computer.
Apple, Facebook and Mattel have one thing in common. They have all set up conversation-based interfaces powered by the AI chatbots that have come good to serve several business purposes. Yesterday, customer responses were a phone call or a web-search away. Today, it is a chatbot away. Chatbot takes its place in chat products and also serve as stand-alone interfaces to handle requests.
Take 1–800-Flowers for instance. They encourage customers to talk to a chatbot and order flowers. The company is now leveraging the natural-language ordering mechanism through Facebook Messenger to make this possible. That’s not all. 1–800-Flowers came up with a startling revelation that 70% of its Messenger orders came from new customers once it introduced the Facebook chatbot.
KLM, an international airline, allows customers to receive their boarding pass, booking confirmation, check-in details and flight status updates through Facebook Messenger. Customers can book flights on their website and opt to receive personalized messages on Messenger.
Conversational UI is evolving into the interface of the future. The conversation assistant capability made available through Nuance’s Dragon Mobile Assistant, Samsung’s S-Voice and Apple’s Siri is just the beginning. Looking into the future, language and reasoning frameworks are going to blend with big data and machine learning to give way for conversational user interfaces that better understand customer needs and wants, better understand the customer and his surroundings.
More and more business models will benefit from chatbots. Retail, media companies distributing content, research and consulting are some of the industries that will drive business value from chatbots.
In conclusion, we’ll be entering a new era of computing, where advances in machine learning and artificial intelligence are creating a resurgence of interest in conversational interfaces and natural language processing, creating the potential for conversation as the new mode of interaction with technology.
For the most part, the problem of recognizing spoken input has been largely solved, now opening up a new challenge: how to build a user experience that’s modeled after natural human conversation.
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and…
545 
2
545 claps
545 
2
Written by
We are a digital product development company and your guide on the digital transformation journey.
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.
Written by
We are a digital product development company and your guide on the digital transformation journey.
Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.
"
https://blog.getambassador.io/why-it-ticketing-systems-dont-work-with-microservices-18e2be509bf6?source=search_post---------363,"In order to effectively build cloud native microservices applications, your engineering organization has to adopt a culture of decentralized decision-making to move faster. You will also need the ability to use self-service platforms and tooling in order to implement operational concerns, such as provisioning infrastructure, declaring resource requirements for services, and implementing (and alerting based upon) monitoring. If you want your teams to move independently and at speed, and avoid potential bottlenecks, then it is simply not practical to be raising tickets in a support system to accomplish these tasks.
In this article series we are discussing key patterns in cloud native application development, and exploring why they’re effective and how to implement them in your organization. We are also examining the consequences of implementation (both good and bad), and providing examples using popular cloud native tools.
In the second part of this series, we are discussing “soup to nuts self-service”, which is a pattern focused on offering self-service functionality throughout the entire software development and deployment cycle.
“Soup to nuts self-service” is a practice that enables fast feedback for engineers, from the generation of a hypothesis or an idea for new functionality, to a running customer-facing experiment or implementation. Offering self-service provisioning of infrastructure, development environments, and observability tooling also reduces potential bottlenecks and reliance on a central operations team when engineers are working within the inner development loop, and also increases ownership of “software products” across teams.
“Soup to nuts” is an American English idiom that conveys the meaning of “from beginning to end”, and is derived from the description of a (perhaps old-fashioned) full course dinner, in which courses progress from soup to a dessert of nuts.
The phrase “soup to nuts self-service” was chosen to convey the end-to-end nature of self-service required within the typical software development lifecycle. With the rise in popularity of DevOps and other related methodologies, many organisations appreciate the need to minimise hand-offs and prevent software artifacts being “thrown over the wall” from development to operation teams.
However, fewer teams have recognised the need to be fully self-sufficient, with the goal of being capable of taking a business hypothesis, prototyping code, experimenting by exposing a fraction of real user traffic to this code, and evaluating success against observable metrics and key performance indicators (KPIs). In a competitive market, the engineering teams that can address user needs and iterate the fastest will typically gain the biggest market share.
The genesis of this pattern can be seen within Deming’s Plan-Do-Check-Act (PDCA) cycle or Boyd’s observe-orient-decide-act (OODA) loop.
In modern software development, we believe that the OODA loop (per product) is realised as a four phase cycle: develop, test, release, and observe. The diagram below of the cycle purposely does not line up our phases with the OODA loop diagram phases, as they aren’t directly analogous:
Use soup to nuts self-service when:
Do not use this pattern when:
The soup to nuts self service pattern is typically implemented across a range of technologies with differing utility. For example, at one end of the spectrum, the development team is simply given access to the bare metal servers, VMs or cloud APIs/SDKs and can do whatever they require. This implementation style requires that the development teams are very operationally skilled, and can also be trusted to be enabled to deploy and change anything they require.
At the opposite end of the spectrum, the development teams are given tightly-scoped access to a PaaS or FaaS platform, where they request and configure infrastructure by declarative configuration files, and push and verify proposed updates via version control (e.g. ‘cf push’ in Cloud Foundry). This implementation style only requires that development teams understand the workflow and properties of the platform, and safeguards or guide rails can be added at an administration level and enforced within build pipelines.
Here are some implementation issues to consider:
Using the soup to nuts self-service pattern has the following benefits:
And liabilities:
Developer-First Kubernetes.
211 
3
211 claps
211 
3
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
"
https://itnext.io/tutorial-auto-scale-your-kubernetes-apps-with-prometheus-and-keda-c6ea460e4642?source=search_post---------364,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Scalability is a key requirement for cloud native applications. With Kubernetes, scaling your application is as simple as increasing the number of replicas for the corresponding Deployment or ReplicaSet - but, this is a manual process. Kubernetes makes it possible to automatically scale your applications (i.e. Pods in a Deployment or ReplicaSet) in a declarative manner using the Horizontal Pod Autoscaler specification. By default, there is support for using CPU utilization (Resource metrics) as criteria for auto-scaling, but it is also possible to integrate custom…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/securing-the-base-infrastructure-of-a-kubernetes-cluster-64e79646b7bf?source=search_post---------365,"The first article in this series Securing Kubernetes for Cloud Native Applications, provided a discussion on why it’s difficult to secure Kubernetes, along with an overview of the various layers that require our attention, when we set about the task of securing that platform.
The very first layer in the stack, is the base infrastructure layer. We could define this in many different ways, but for the purposes of our discussion, it’s the sum of the infrastructure components on top of which Kubernetes is deployed. It’s the physical or abstracted hardware layer for compute, storage, and networking purposes, and the environment in which these resources exist. It also includes the operating system, most probably Linux, and a container runtime environment, such as Docker.
Much of what we’ll discuss, applies equally well to infrastructure components that underpin systems other than Kubernetes, but we’ll pay special attention to those factors that will enhance the security of Kubernetes.
The adoption of the cloud as the vehicle for workload deployment, whether it’s public, private, or a hybrid mix, continues apace. And whilst the need for specialist bare-metal server provisioning hasn’t entirely gone away, the infrastructure that underpins the majority of today’s compute resource, is the virtual machine. It doesn’t really matter, however, if the machines we deploy are virtual (cloud-based or otherwise), or physical, the entity is going to reside in a data center, hosted by our own organisation, or a chosen third-party, such as a public cloud provider.
Data centers are complex, and there is a huge amount to think about when it comes to the consideration of security. It’s a general resource for hosting the data processing requirements of an entire organisation, or even, co-tenanted workloads from a multitude of independent organisations from different industries and geographies. For this reason, applying security to the many different facets of infrastructure at this level, tends to be a full-blown corporate or supplier responsibility. It will be governed according to factors such as, national or international regulation (HIPAA, GDPR), industry compliance requirements (PCI DSS), and often results in the pursuit of certified standards accreditation (ISO 27001, FIPS).
In the case of a public cloud environment, a supplier can and will provide the necessary adherence to regulatory and compliance standards at the infrastructure layer, but at some point, it comes down to the service consumer (you and me), to further build on this secure foundation. It’s a shared responsibility. As a public cloud service consumer, this begs the question, “what should I secure, and how should I go about it?” There are a lot of people with a lot of views on the topic, but one credible entity is the Center for Internet Security (CIS), a non-profit organisation dedicated to safeguarding public and private entities from the threat of malign cyber activity.
The CIS provides a range of tools, techniques, and information for combating the potential threat to the systems and data we rely on. CIS Benchmarks, for example, are per-platform best practice configuration guidelines for security, consensually compiled by security professionals and subject matter experts. In recognition of the ever increasing number of organisations embarking on transformation programmes, which involve migration to public and/or hybrid cloud infrastructure, the CIS have made it their business to provide benchmarks for the major public cloud providers. The CIS Amazon Web Services Foundations Benchmark is an example, and there are similar benchmarks for the other major public cloud providers.
These benchmarks provide foundational security configuration advice, covering identity and access management (IAM), ingress and egress, and logging and monitoring best practice, amongst other things. Implementing these benchmark recommendations is a great start, but it shouldn’t be the end of the journey. Each public cloud provider will have their own set of detailed recommended best practices1,2,3, and a lot of benefit can be taken from other expert voices in the domain, such as the Cloud Security Alliance.
Let’s take a moment to look at a typical cloud-based scenario that requires some careful planning from a security perspective.
How can we balance the need to keep a Kubernetes cluster secure by limiting access, whilst enabling the required access for external clients via the Internet, and also from within our own organisation?
This scenario demonstrates the need to carefully consider how to configure the infrastructure to be secure, whilst providing the capabilities required for delivering services to their intended audience. It’s not a unique scenario, and there will be other situations that will require similar treatment.
Assuming we’ve investigated and applied the necessary security configuration to make the machine-level infrastructure and its environment secure, the next task is to lock down the host operating system (OS) of each machine, and the container runtime that’s responsible for managing the lifecycle of containers.
Whilst it’s possible to run Microsoft Windows Server as the OS for Kubernetes worker nodes, more often than not, the control plane and worker nodes will run a variant of the Linux operating system. There might be many factors that govern the choice of Linux distribution to use (commercials, in-house skills, OS maturity), but if its possible, use a minimal distribution that has been designed just for the purpose of running containers. Examples include CoreOS Container Linux, Ubuntu Core, and the Atomic Host variants. These operating systems have been stripped down to the bare minimum to facilitate running containers at scale, and as a consequence, have a significantly reduced attack surface.
Again, the CIS have a number of different benchmarks for different flavours of Linux, providing best practice recommendations for securing the OS. These benchmarks cover what might be considered the mainstream distributions of Linux, such as RHEL, Ubuntu, SLES, Oracle Linux and Debian. If your preferred distribution isn’t covered, there is a distribution independent CIS benchmark, and there are often distribution-specific guidelines, such as the CoreOS Container Linux Hardening Guide.
The final component in the infrastructure layer is the container runtime. In the early days of Kubernetes, there was no choice available; the container runtime was necessarily the Docker engine. With the advent of the Kubernetes Container Runtime Interface, however, it’s possible to remove the Docker engine dependency in favour of a runtime such as CRI-O, containerd or Frakti.4 In fact, as of Kubernetes version 1.12, an alpha feature (Runtime Class) allows for running multiple container runtimes, side-by-side in a cluster. Whichever container runtimes are deployed, they need securing.
Despite the varied choice, the Docker engine remains the default container runtime for Kubernetes (although this may change to containerd in the near future), and we’ll consider its security implications here. It’s built with a large number of configurable security settings, some of which are turned on by default, but which can be bypassed on a per-container basis. One such example is the whitelist of Linux kernel capabilities applied to each container on creation, which helps to diminish the privileges available inside a running container.
Once again, the CIS maintain a benchmark for the Docker platform, the CIS Docker Benchmark. It provides best practice recommendations for configuring the Docker daemon for optimal security. There’s even a handy open source tool (script) called Docker Bench for Security, that can be run against a Docker engine, which evaluates the system for conformance to the CIS Docker Benchmark. The tool can be run periodically to expose any drift from the desired configuration.
Some care needs to be taken when considering and measuring the security configuration of the Docker engine when it’s used as the container runtime for Kubernetes. Kubernetes ignores much of the available functions of the Docker daemon, in preference of its own security controls. For example, the Docker daemon is configured to apply a default whitelist of available Linux kernel system calls to every created container, using a seccomp profile. Unless specified, Kubernetes will instruct Docker to create pod containers ‘unconfined’ from a seccomp perspective, giving containers access to each and every syscall available. In other words, what may get configured at the lower ‘Docker layer’, may get undone at a higher level in the platform stack. We’ll cover how to mitigate these discrepancies with security contexts, in a future article.
It might be tempting to focus all our attention on the secure configuration of the Kubernetes components of a platform. But as we’ve seen in this article, the lower layer infrastructure components are equally important, and are ignored at our peril. In fact, providing a secure infrastructure layer can even mitigate problems we might introduce in the cluster layer itself. Keeping our nodes private, for example, will prevent an inadequately secured kubelet from being exploited for nefarious purposes. Infrastructure components deserve the same level of attention, as the Kubernetes components, themselves.
In the next article, we’ll move on to discuss the implications of securing the next layer in the stack, the Kubernetes cluster components.
Written by Puja Abbassi — Developer Advocate @ Giant Swarm
twitter.com
ITNEXT is a platform for IT developers & software engineers…
323 
1
323 claps
323 
1
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://eng.lyft.com/envoy-joins-the-cncf-dc18baefbc22?source=search_post---------366,"Today I am thrilled to share that Envoy is joining the Cloud Native Computing Foundation (CNCF), home of Kubernetes, as its 11th hosted project. Lyft originally announced Envoy almost exactly one year ago on September 14, 2016. What an incredible year it has been! In this post I will briefly recount the history of the project and how we arrived at this momentous day.
I joined Lyft in May 2015. At that time Lyft had already begun its journey from a monolith to microservice-based architecture, having deployed more than 30 services. Although Lyft was already experiencing many of the organizational benefits of parallel and decoupled development, the new architecture also brought challenges. Primarily, Lyft developers were facing the reality that the network is inherently unreliable; when problems occurred it was nearly impossible to determine the source of the issue. Physical network? Virtual network? Hardware? App? Who knew? During this era, Lyft developers were unwilling to move mission critical functionality out of the monolith because of the perceived instability of our burgeoning microservices-based infrastructure. If Lyft was to realize the complete benefits of a fully distributed architecture it was imperative that we directly tackle microservice networking and observability.
Before Lyft, I had the opportunity to work on large scale distributed networking for many years both in Amazon’s EC2 as well as Twitter. I had observed how different organizations attempted to solve the general problem of distributed networking. At Twitter specifically, I saw the pros and cons of using Finagle for all service to service communication. At the same time, I led development of a new C++ edge proxy that continues to serve all of Twitter’s traffic today.
The power of libraries like Finagle and the Netflix OSS suite cannot be understated. They provide a rich set of distributed systems networking functionality such as service discovery, load balancing, retries, timeouts, circuit breakers, protocol translation, stats, logging, tracing, etc. All of which aim to make the network transparent to application developers. However, the library based deployment approach was problematic at Twitter even though nearly 100% of services ran on the JVM; an update to Finagle might take months to fully rollout as service owners slowly upgraded and deployed.
At Lyft we wanted to replicate and expand on the feature set of Finagle, but move the functionality into a self-contained process that was easier to iterate and deploy. Lyft — like many modern companies — also has services written in many languages, making the out of process proxy approach even more critical so as to avoid replicating every feature in many different libraries. Additionally, from my work on edge serving systems, it was clear that performance, and in particular minimizing tail latency variance, is critical for distributed networking components.
And thus, after investigating existing open source options and determining there was no good fit, Envoy was born. For performance reasons, modern C++ was chosen as the implementation language and development began in May 2015. The MVP was completed and deployed in early September 2015. Initially, Envoy was used as Lyft’s edge proxy. Iteratively, our small but growing team added features and also began to deploy Envoy as our sidecar service-to-service proxy. By the early summer of 2016, Envoy was fully deployed at Lyft and being used for all edge and service to service networking, forming a mesh between over a hundred services and transiting millions of requests per second. Perhaps most importantly, Lyft engineering had moved into a phase where developers trusted the networking abstraction that Envoy provides. Critical functionality was being moved out of the monolith at an increasing pace with little concern given to overall network stability and observability.
Lyft’s business is almost entirely based on open source technology. Without it, it’s unlikely that the ridesharing service we know and love would exist today. Given the large development effort that had gone into Envoy, and understanding that many other organizations face identical challenges when moving from a monolithic to microservice architecture, we wanted to give back to the larger community that had nurtured our own company growth. Therefore, we decided to proceed with open sourcing Envoy and working to build a community around it.
I won’t fully recap what happened in the months after open sourcing as I already wrote about it. I also won’t spend a lot of time discussing in detail why I think Envoy has seen such tremendous growth in the past year (the previously linked post discusses that as does my recent post on the universal data plane API).
I will, however, mention that all of us at Lyft have been both amazed and humbled at the industry-wide reaction over the past year. The number of large organizations that are Envoy users continues to grow, along with the number of products being built on top of Envoy as a fundamental technology. It would have seemed incomprehensible a year ago to think that Envoy might become a foundational networking technology of the modern Internet. Yet, a year later, it seems conceivable that the project is on that trajectory.
The incredible growth of the Envoy community has not been without challenges. The trials and tribulations of maintaining a successful OSS project have been getting more attention lately as it is a tough and often thankless job. Although Google has been an incredible partner in the past 9 months (we now have several Google maintainers!), I still have at times felt that the project was being held back by my lack of time to fully focus on things like marketing, community organization, documentation, developer outreach, etc. Because of this, we started investigating contributing Envoy to a foundation that might help share the maintenance load across the entire breadth of things that need doing along with helping us grow. Additionally, the right foundation would also help advance Envoy’s technical goals with close synergies to existing hosted projects.
After researching the options, many discussions later, and a formal application process and vote, we have found our home in the CNCF. Lyft is extremely excited to contribute a technology that has been critical to the stability of our infrastructure during our massive growth. We hope that Envoy will help many other organizations as much as it helped us. The CNCF offers a staff of experts on all things open source who can help with project best practices, as well as an existing stable of technologies such as Kubernetes, Prometheus, OpenTracing, gRPC, and Fluentd that are complementary to Envoy and the overall cloud native development community.
As of this writing, Envoy has 78 contributors from at least 10 different organizations with primary maintainers working at Lyft and Google. Overall the project has been going incredibly well with massive growth and uptake. Lyft’s contribution of Envoy to the CNCF is both a way to give back to the larger open source community as well as a nod to the fact that the targeted resources of the foundation will help unlock the next stage of project growth. As a technology, Envoy has the opportunity to become a primary building block of modern service architectures. All of us at Lyft — and all of us who work on Envoy across many different organizations — are extremely excited for what the future holds in partnership with the CNCF.
Stories from Lyft Engineering.
417 
417 claps
417 
Written by
Engineer @lyft (https://mattklein123.dev/)
Stories from Lyft Engineering.
Written by
Engineer @lyft (https://mattklein123.dev/)
Stories from Lyft Engineering.
"
https://medium.com/nerd-for-tech/software-architecture-for-the-cloud-c9226150c1f3?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
From the first programming languages, to advanced programming paradigms, to the development of virtualization and cloud infrastructure, the history of computer science is the history of the evolution of abstractions that hide complexity and empower the creation of ever more sophisticated applications.
The development of reliable, scalable applications for the cloud seems to be far harder than it ought to be.
In recent years, containers and container orchestrators like Kubernetes have proved to be an important abstraction that dramatically simplifies the development of reliable, scalable distributed systems. Though containers and container orchestration are just now entering the mainstream, they are already enabling developers to build and deploy applications with speed, agility, and reliability that would have been unimaginable only a few years ago — but these technologies are still new and software architects and engineers are still working to understand how to best apply them.
How do we make the real power of cloud computing available, manageable, and affordable for companies wishing to succeed at digital transformation? How do we facilitate that transformation while both managing and mitigating risk? Cloud-native applications can exploit the automated deployment, scaling, reliability, and fail-over capabilities available with the cloud. However, the old patterns of application architecture, development, and deployment — and the old methods of data organization and access — do not deliver the levels of reliability, performance, scalability, and fault tolerance we really want.
The great hockey player,Wayne Gretzky, once said, “I skate to where the puck is going to be, not where it has been.” Exploiting the real potential of the cloud requires us to skate to where the puck is going to be.
This article focuses on the distributed data management challenges of cloud-deployed applications, and not the UI/UX aspects — because the technologies and techniques for delivering UI/UX functionality to browsers and mobile devices are well understood, have an experienced developer base, and differ only little between traditional client-server and cloud-deployed applications.
To start, let’s clearly define what we mean by cloud-native applications and the requirements we expect them to satisfy. In this context, a cloud-native application is a collection of small, independent, and loosely coupled services and resources implemented to execute or enable some business function and be deployable in a cloud environment. For this article, services are executable components of an application and resources are things that reside in non-volatile storage like files, key-value stores, and databases.
If an application is “cloud-native”, it is specifically implemented to provide a consistent development, deployment, automated management, and communications model across private, public, and hybrid clouds. It is designed to exploit the automated deployment, scaling, reliability, and failover capabilities available through containers and container orchestration. For maximum deployment flexibility, it avoids dependence upon technologies that are proprietary to specific commercial cloud service vendors — except when such technologies are crucial to meeting the application’s functional objectives. The primary requirements that need to be met for most successful cloud-native application implementations and deployments are detailed in Section A, below.
Section B describes the five most common architectural patterns in use today when designing software applications. They each embody real-world experiences in the implementation of software — and, as with all software architectures, they each represent a mix of compromises. Each has its own strengths and weaknesses when applied to the design and implementation cloud-native systems. Our challenge is to select the best attributes of each with which to synthesize an effective cloud-native architectural pattern for using and managing distributed data resources — and to interact with the modern UI/UX technologies necessary for complete applications.
The elements and patterns presented in this article are tried and proven and are the result of experience in the design and implementation of commercial software systems — for distributing data and processing across networks — beginning in the 1980s and continuing through the present time. Much of the technical inspiration for these solutions is owed to Alan Kay, Michael Stonebraker, and Werner Vogels, whose contributions have literally changed the way we implement and use software today — Alan Kay for understanding the power of messaging in software organization, Michael Stonebraker for his comprehensive vision of data management, and Werner Vogels for identifying and describing the underlying principles of practical cloud computing.There are a number of concepts that are extremely useful when creating an architectural pattern for cloud-native applications.
The actor model in computer science is a mathematical model of concurrent computation that treats actors as the universal primitive of concurrent computation. Actors communicate by passing messages. In response to a message, an actor does its job. Actors may modify their own private state, but can only affect other actors indirectly through messaging. Actors are reentrant and thread-safe. Actors are an appropriate model for microservices design.
An actor has a simple job:
Actors can be many things, including microservices, microservice clients, event publishers, event handlers, message brokers, message loggers, event loggers, and error handlers.
Using actors, communicating through message passing, as the basic building blocks of cloud-native applications simplifies the execution of networked concurrent processing and:
Patterns like the Actor Model communicate through message passing. Message passing is a technique for invoking behavior by another actor. The invoking program sends a message to a process and relies on that process and its supporting infrastructure to then select and execute the appropriate logic. Both asynchronous (event) messaging and synchronous (request-response) messaging can be implemented — giving application developers leverage to optimize communications for specific use cases and performance objectives — all within a common unifying framework. As a basic rule, the far more efficient asynchronous messaging should be the default messaging choice and synchronous messaging used only when a sender must wait for a response before proceeding. Message passing also makes it easier to exploit modern stream handling technologies.
Message passing implements loose coupling, but also can implement dynamic coupling. Dynamic coupling, using brokers, provides a very powerful mechanism for implementing load balancing, failover, and dynamic scaling. Brokers can also be an important mechanism for implementing self-organizing systems.
Complexity is the primary limiting factor in the successful implementation of large distributed systems. It is the Achilles heel of large microservices and API management implementations. As the number of things (APIs, services, resources) and connections between them grows, complexity increases exponentially, i.e., c = n(n-1)/2. Top-down hierarchical controls, as implemented in most systems, are ill-suited to cope with this complexity. A better solution is needed.
The cloud gives us the power to create increasingly large and complex applications, integrating and operating on data spread across countries and even continents — if we can only manage them. Today, most of the working machines of that complexity occur in the natural world. We need to look at self-organizing systems, the way nature copes with complexity. Self-organizing systems emerge from bottom-up interactions, unlike top-down hierarchical systems, which are not self-organizing. Ant colonies are a useful example of emergence, where the whole is greater than the sum of its parts.
Ants, governed by very simple rules and only local interactions, can through their own activities, implement colonies that exhibit complex structures and behaviors that far exceed the intelligence or capabilities of individual ants. Ant colonies also illustrate the decentralized nature of self-organizing systems. The queen does not tell individual ants what to do — rather each ant reacts to stimuli from chemical messages (pheromones) exchanged with other ants.
In this way control is distributed over the whole system and all parts contribute to the resulting functionality — as opposed to centralized structures that are often dependent upon a single coordinating entity — and this decentralized structure, inherent to self-organizing systems, gives them resiliency and robustness. When any element fails it can easily be replaced by a like element. A successful cloud-native architecture mimics the decentralized structure of organic, living systems where complex capabilities can emerge from the interaction of relatively simple parts — while at the same time minimizing the complexities of configuration and deployment.
Much of the work of data management is cleaning, validating, filtering, combining, and transforming data. The passing of a message or stream of messages provides a perfect opportunity to execute declarative rules for validating and manipulating the data payloads of those messages through the use of Intelligent Transformers. Intelligent Transformers can be chained together to enforce rules and even implement branching into one or more additional streams. Implementing this kind of repetitive rule-based processing within individual actors is wasteful and difficult to modify and manage — especially, when it can easily be handled by attaching intelligent transformer processing to the input and output message streams connecting actors.
Replication services manage sets of identical physical datastores distributed across multi-cloud clusters in order to facilitate horizontal scalability and failover. These services should be transparent to application actors and include:
In summary, these underlying concepts can help software implementers meet the requirements identified in Section A and enable them to both mitigate the weaknesses in the Microservices and Space-Based Patterns and incorporate many of the desirable features in the other common architectural patterns. Multi-Cloud Apps: Part 1 digs a little deeper into designing and building successful cloud applications.
This article considers the requirements common for most cloud-native data management applications to be:
Architecture refers to the fundamental structures of a system and the discipline of creating such structures and systems. Each structure is made of elements, relations among elements, and the properties of both elements and relations.
The following five architectural patterns are the most common in today’s application design, but for the most part predate cloud computing. They each have strengths and weaknesses — they each have some aspects of applicability to cloud-native design — and they all have flaws when applied to cloud-native applications. Our challenge is to select the positive attributes and mitigate the negative when defining a cloud-native architectural pattern.
The images and links, below, for the full description and analysis of each pattern are from:
Software Architecture Patterns by Mark Richards
Copyright © 2015–2021 O’Reilly Media, Inc. All rights reserved.
O’Reilly provides an introductory free view.
The layered architecture pattern is the most commonly used architecture pattern, otherwise known as the n-tier architecture pattern. In discussions of cloud architecture and microservices, layered is often mistermed monolithic architecture. This pattern is the de facto standard for most Java EE applications and therefore is widely known by most architects, designers, and developers. It is a clear confirmation of Conway’s Law: “Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.” It is the pattern that most of us were trained to try first.
Characteristics as a cloud-native solution:
The event-driven architecture pattern is a popular distributed asynchronous architecture pattern used to produce highly scalable applications. It is also highly adaptable and can be used for small applications and as well as large, complex ones. The event-driven architecture is made up of highly decoupled, single-purpose event processing components that asynchronously receive and process events.
Characteristics as a cloud-native solution:
The microkernel architecture pattern (sometimes referred to as the plug-in architecture pattern) is a natural pattern for implementing product-based applications. A product-based application is one that is packaged and made available for download in versions as a typical third-party product.
Characteristics as a cloud-native solution:
The microservices architecture pattern is quickly gaining ground in the industry as a viable alternative to monolithic applications and service-oriented architectures. Because this architecture pattern is still evolving, there’s a lot of confusion in the industry about what this pattern is all about and how it is implemented.
Characteristics as a cloud-native solution:
In any high-volume application with an extremely large concurrent user load, the database will usually be the final limiting factor in how many transactions you can process concurrently. While various caching technologies and database scaling products help to address these issues, the fact remains that scaling out a normal application for extreme loads is a very difficult proposition.
The space-based architecture pattern, often called the cloud architecture pattern, is specifically designed to address and solve scalability and concurrency issues. It is also a useful architecture pattern for applications that have variable and unpredictable concurrent user volumes. Solving the extreme and variable scalability issue architecturally is often a better approach than trying to scale out a database or retrofit caching technologies into a non-scalable architecture.
Characteristics as a cloud-native solution:
From Confusion to Clarification
263 
5
263 claps
263 
5
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Written by
A US Army Vietnam War veteran with a wonderful wife and family, I’m a software architect and engineer who also loves building and messing about in boats.
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
"
https://medium.com/@betz.mark/kubernetes-five-steps-to-well-behaved-apps-a7cbeb99471a?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mark Betz
Jan 22, 2018·6 min read
There’s a reason why the kubernetes project is the current crown jewel of the cloud native community, with attendance at Kubecon 2017 in Austin nearly four times that of last year’s conference in Seattle and seemingly every major enterprise vendor perched behind a booth in the exhibit hall eager to help attendees take advantage of the platform. The reason is that the advantages are significant, especially in those areas that matter most to developers and system engineers: application reliability, observe-ability, control-ability and life-cycle management. If Docker built the engine of the container revolution then it was kubernetes that supplied the chassis and got it up to highway speed.
But driving at highway speed means keeping your hands on the wheel and obeying the rules of the road. Kubernetes has its own rules, and applications that adhere to best practices with respect to certain key touch points are much less likely to wipe out and take a few neighboring lanes of traffic with them. In this post I am going to briefly discuss five important design features that will affect how well your application behaves when running on kubernetes: configuration, logging, signal handling, health checks, and resource limits. The treatment of each topic will be necessarily high level, but I will provide links to more detailed information where it will be useful.
Configuration is data required by a process at startup that varies between deployments. Config should be loaded from environment variables with sane defaults. There are a few reasons to follow this advice. Reading env vars with defaults is easy and a basic feature of every general-purpose language. It’s also portable and there are standard ways to initialize a container’s environment whether you’re running on kubernetes, Docker Engine, Docker Compose, etc. On kubernetes you can interpolate env var values into templates using a tool like helm, or load them from ConfigMaps or Secrets previously installed into the cluster. Lastly environment variables are observable, in the dashboard or from the command line, and kubectl describe pods mypod is less work to type than kubectl exec mypod -- cat /etc/myapp/config.py.
The kubernetes documentation has a good section on setting environment variables for a container. For background see the config topic in the Twelve-Factor App manifesto, and don’t miss Kelsey Hightower’s excellent hands-on explanation.
Every server app produces a stream of log events which describe what it is doing, and sometimes what went wrong. A well-behaved kubernetes app writes these events to stdout and stderr, and does not concern itself with routing or managing log stream delivery. Logs written to the standard streams are observable on the console locally during development, and through the dashboard or command line when the container is running in the cluster. On Google Kubernetes Engine you can easily configure fluentd to capture log streams and deliver them to stackdriver for long-term storage, searching and analysis. A well-behaved kubernetes app should specifically not write to local log files. Doing so creates state in the container filesystem that has to be managed, either by mounting the log directory to a host volume so the data is persistent, or by running a shipper like rsyslog inside the container.
For more background see the logging topic in the Twelve-Factor App manifesto, and this overview of the logging architecture in kubernetes.
Pods in kubernetes get restarted for any number of reasons, including shuffling resources around or just running kubectl delete pods mypod. When the system wants to kill a pod it first sends SIGTERM and then waits a set number of seconds before sending SIGKILL. This period is known as the termination grace period and is a property of the podSpec that can be overridden from the kubectl command line. If your process doesn’t implement a handler for SIGTERM then it will be SIGKILLed. Processes that are killed are immediately removed from etcd and the API, without waiting for the process to actually terminate on the node. Bottom line: if you have anything you need to do to ensure graceful shutdown then you need to implement a handler for SIGTERM.
There are two other things that are very important to keep in mind: the first may seem obvious and it is that if you have more than one container in a pod they will all be signaled on shutdown and you need to have the right strategy for each depending on what it is doing. The second is less obvious, and actually is a bit of a trap: only the process running as PID 1 in a container is going to be signaled. If you use the non-exec form of CMD in your Dockerfile, for example, then your thing is running as /bin/sh -c thing and it isn’t PID 1 and won’t get signaled. Instead use the exec form CMD [""/usr/local/bin/thing""] or use the exec form of ENTRYPOINT. For additional info see this good overview of pod termination in the kubernetes documentation.
Kubernetes works best when it knows the health status of every container in the system. The kubelet needs this information in order to restart containers that have failed, and to keep service endpoints up to date. To determine the health of a container kubernetes relies on two signals: liveness and readiness. Liveness measures whether a container is running or not, and readiness measures whether a running container is able to accept traffic. Specific tests for liveness and readiness can be configured for a container object. There are several options from executing external commands to making tcp and http requests.
If you don’t define liveness and readiness probes for your containers then the system will rely on the default signal, which is whether the container’s PID 1 still exists. If that process terminates then the kubelet finds out about it from the container runtime (i.e. Docker) and schedules a restart. For almost all long-running applications of any complexity this default behavior is insufficient. Your container’s PID 1 could be up but stuck in a loop, or blocked on some other broken bit and unable to do its job. Note that you should always define both a liveness and readiness probe, though it is possible these probes may use the same test for some apps. See the kubernetes documentation for a good explanation of creating liveness and readiness probes.
A big part of kubernetes’ job is scheduling pods to run on nodes. A node in a cluster is an environment with constrained resources: memory, cpu, and ephemeral storage all represent things that have to be shared by containers running on the node (host network ports too, but we won’t talk about them here). For each of these resource types kubernetes defines the concept of a request and a limit. A request tells the system how much of a particular resource a container uses normally, and is an input to scheduling. A limit specifies the maximum amount of the resource a container is allowed to use, and may cause the hosting pod to be evicted from a node under constrained conditions.
The benefit of setting requests for every container is to allow kubernetes to do a better job of fitting pods onto nodes. The benefit of setting limits is as an additional health check: containers will not have the ability to go off the rails and consume all of memory, cpu or storage, thus forcing the scheduler to start moving things around to find available resources. If you don’t define requests and limits your containers are subject to the defaults, which are under the control of your cluster administrator. Defaults can be set per namespace, making it possible to divide the total resources in a cluster between different teams or different applications. The kubernetes documentation contains an overview of configuring requests and limits, and the same for managing defaults starts here.
I’ll close by mentioning the Twelve-Factor App manifesto, which I linked to twice above. This document represents a set of design guidelines and patterns for implementing applications that are deployable, scalable, observable, and work well within cloud-native environments. If you’re intending to write applications to run on kubernetes you can’t go far wrong following the recommendations outlined in it.
Senior Devops Engineer at Olark, husband, father of three smart kids, two unruly dogs, and a resentful cat.
266 
266 claps
266 
Senior Devops Engineer at Olark, husband, father of three smart kids, two unruly dogs, and a resentful cat.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kevinhoffman/golang-are-we-webassembly-yet-e0a2e180fc98?source=search_post---------369,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Hoffman
Sep 5, 2019·3 min read
For the past several months, I’ve been working on an open source project called Waxosuit — a secure, cloud-native host runtime for WebAssembly modules. This project is written in Rust and takes advantage of a number of incredibly powerful Rust libraries and tools for manipulating and interpreting WebAssembly modules.
Since before I wrote the first line of code, I’ve always anticipated creating a Go SDK for building guest modules (currently Waxosuit only has a Rust guest SDK). I started looking into the Wasm ecosystem for Go and was surprised to discover a number of gaps.
When I went searching for examples on how to create Wasm modules in Go, most of what I found involved interaction with the browser. Released in Go 1.11, there’s the syscall/js package that provides a means of interaction with the host runtime environment — if the host environment is a browser.
Right at the top of that documentation page, there’s a disclaimer indicating that the package is experimental and should not be used in production and is only there to make tests pass.
Today there are two main ways you can compile Go into Wasm modules — through the regular Go compiler and through TinyGo. Phil Kedy, a colleague of mine who is far more immersed in the Go community than myself was also looking into this and did some digging on Go’s ability to produce clean, functioning Wasm modules.
The first thing we noticed is that building using the standard Go compiler has no support for imports from arbitrary namespaces. The only imports Go will support are the ones that come from the default env namespace. This is not compliant with the Wasm 1.0 specification and is one of the key reasons why I can’t (yet) produce a Go SDK for Waxosuit — Go can’t produce a Wasm module that complies with the Wascap spec (which requires clean Wasm 1.0 compliance). Go also produces enormous Wasm modules. There are tools to trim them, but it’s nothing like what you get with C++ or Rust.
Phil had a chance to talk to the folks building TinyGo recently at GopherCon. This project looks to be more in line with what I’ve been looking for. Their compiled output is far smaller and a more pure WebAssembly target. It also will support proper namespaces (Phil submitted a PR to add that feature). However, TinyGo doesn’t currently support reflection, which means it can’t compile a whole family of dependencies, including Google’s protobuf (the binary encoding format I use for marshaling data across the guest/host boundary).
In short, Go’s WebAssembly support seems pretty early days (and self-labeled as experimental) and not ready for production use. That immaturity is even more visible when you look at Go’s support for non-browser host runtimes. This doesn’t mean I’m stuck twiddling my thumbs waiting for this situation to improve — I’ve got more work than I have time working on Waxosuit.
I am hoping that the Go community continues to investigate and embrace WebAssembly, not because I have any desire to build browser-based apps in Go, but because I believe that WebAssembly’s true home is in the cloud, and Go is often considered the “de facto cloud language.”
As Go’s Wasm capabilities mature, I’ll continue to re-evaluate it and once it feels like something that’s stable enough on which to build a foundation, I’ll get to work on providing a Go guest SDK for Waxosuit so people can build secure, boilerplate-free, cloud native services and functions in Go.
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
381 
3
381 
381 
3
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
"
https://blog.bernd-ruecker.com/zeebe-io-a-horizontally-scalable-distributed-workflow-engine-45788a90d549?source=search_post---------370,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
There are many use cases for workflow automation out there. Many people think that workflow automation is only used for slow and low frequency use cases like human task management. Despite the fact that this is not true (see e.g. 24 Hour Fitness or Zalando) I do see limitations of current workflow technology in terms of scalability, but on a very different order of magnitude. As traditional engines are based on relational databases they are naturally limited in scale to what that database can handle. Even if this is sufficient for most companies, I know there are definitely interesting use cases requiring more performance and scalability, e.g. to process financial trades which need soft real-time guarantees under a very high load.
Over the last few years a lot of smart folks at Camunda dived deep into the question of how to scale a workflow engine beyond smaller iterative improvements. The result of this thinking lead us to the source-available project Zeebe.io. And we’ve just released the first production-ready version of it!
Zeebe will push the frontiers of what workflow automation can do as it provides true horizontal scalability. That means that adding nodes to the system will result in being able to process more load — and this increase is linear.
The key ingredients to achieve this are:
As a result, Zeebe is in the same class of systems like Apache Kafka. In early attempts we could process roughly the number of events per second as Kafka, which was a few hundred times faster than Camunda 7.8 (which is an example for a traditional workflow engine, and actually even the fastest open source one according to a study by the university of Lugano in May 2016):
So how could we achieve this? One important idea is to build an event sourced system.
Traditional workflow engines capture the current state of a workflow instance in a database table. If the state changes the database table is updated. Simplified, it looks like this:
Using this approach the workflow engine can leverage a lot of guarantees from the relational database (RDMS), e.g. ACID transactions.
Zeebe works very differently and leverages event sourcing. That means that all changes to the workflow state are captured as events and these events are stored in an event log alongside commands. Both are considered to be records in the log. Quick hint for DDD enthusiasts: These events are Zeebe internal and related to the workflow state. If you run your own event sourced system within your domain you typically run your own event store for your domain events.
Records are immutable and therefore the log is append-only. Nothing will ever be changed once it is written, it is like a journal in accounting. Append-only logs can be handled and scaled very efficiently, something we will dive deeper into in part two of this article.
The current state of a workflow can always be derived from these events. This is known as projection. A projection in Zeebe is saved internally as snapshot leveraging RocksDB, a very fast key-value store. RocksDB allows Zeebe internally to find certain objects by key, as a pure log would not even allow for simple queries like “give me the current state for workflow instance 2”.
Zeebe stores the log on disk. Currently this is the only supported storage option (other options like e.g. Apache Cassandra are regularly discussed, but not on the roadmap so far). RocksDB also flushes the snapshot state to disk, which not only creates much faster start-up times, but also allows Zeebe to delete processed records from the log, keeping it quite compact (something we will dive deeper into in part two of this article).
In order to achieve performance, resilience and scalability we applied the following distributed computing concepts:
I cover this in-depth in part two of this post: how we built a highly scalable distributed state machine.
Zeebe runs as an own program on a Java Virtual Machine (JVM). Relating to architecture options to run a workflow engine this is the remote engine approach, as the application using Zeebe talks remotely with it. But as we leverage streaming into the client and use binary communication protocol this is very efficient and performant. Its huge advantage is that the broker has a defined setup and environment and cannot be influenced by your application code. So this design decision provides proper isolation, we learned the importance of that in years of experience supporting a workflow engine.
Zeebe uses visual workflow definitions in the ISO standard BPMN, which can be modeled graphically with the free Zeebe Modeler.
If you prefer you can also use a YAML to describe workflows, e.g.:
Please note, that not all language constructs are currently supported in YAML.
A workflow can include so called service tasks. When an instance reaches these tasks some of your code needs to be executed. This is done by creating Jobs which are fetched by JobWorkers in your application. Zeebe provides native language clients, e.g. in Java:
or in NodeJs:
or in C#:
or in Go:
Or in Rust or Ruby. More languages will follow. And thanks to gRPC it is easy to use almost any programming language, as described in this post of how to use Python.
As you might have spotted in the code, you can use a reactive programming model in your application.
You can connect as many clients to Zeebe as you want to and the Jobs will be distributed (currently in a round-robin fashion) allowing for flexible scalability of the workers (up and down). Zeebe will soon support back-pressure, so making sure that jobs are provided only in a rate a client can process them. No clients can be overwhelmed with work. If in doubt the jobs are saved in Zeebe until new clients connect.
Clients are competing consumers which means that one job will only be executed by exactly one of the clients. This is implemented using a lock-event which needs to be written to Zeebe before a job can be executed. Only one client can write that lock-event, other clients trying to do so get an error message. A lock is held for a configurable amount of time before being removed automatically, as Zeebe assumes that the client has died unexpectedly in this case.
It is important to note that Zebee Clients do not implement any form of ACID transaction protocols. This means that in case of failures no transaction will be rolled back. With this setup you have two design alternatives:
Most of the time you will decide to go for “at least once”, as it makes the most sense in the majority of use cases.
As your code might be called multiple times you have to make your application logic idempotent. This might be natural in your domain or you might think of other strategies and create an Idempotent Receiver (see e.g. Spring Integration). I tackled idempotency briefly in 3 common pitfalls of microservices integration — and how to avoid them and plan an extended article on it.
The Zeebe broker is responsible for executing running workflows. It is optimized to apply new commands to the current state in the way to reach the performance and scalability goals mentioned in the beginning. But the broker cannot serve any queries like “what workflow instances were started this morning between 8 and 9 but haven’t finished yet?”. As we are not using a relational database anymore we cannot do simple SELECT statements. We do need a different way to handle the so-called query model in this case.
This way of separating command and query model is known as Command Query Responsibility Segregation (CQRS) with big advantages:
CQRS allows you to separate the load from reads and writes allowing you to scale each independently. […] you can apply different optimization strategies to the two sides. An example of this is using different database access techniques for read and update.
This is exactly what we do with Zeebe. The Broker leverages event streaming and optimizes for high throughput and low latency. But it does not provide query capabilities. That’s why Zeebe provides so called Exporters which can access the whole event stream. One out-of-the-box exporter is for Elasticsearch. By using it all events are written to Elastic and stored there, ready to be queried.
Zeebe now comes with an operation tool you can use to look into the workflow engine: Operate. You can see what’s going on, recognize problems (so called incidents) as well as root-causing and fixing incidents.
Operate is also built to scale and uses its own optimized indices on Elasticsearch:
One interesting side note goes on open source. You might follow the latest development around source-available licenses (e.g. from Cockroach Labs, Confluent, MongoDB, Redis, Timescale). The background is that cloud vendors can simply take existing open source projects and provide a managed service offering, without paying anything back to the community. And big cloud vendors can typically leverage their market position to compete easily with managed service offerings of the open source companies themselves. This could drain the communities, but also turn into an existential threat for companies with the core developers on the payroll. In the long run this could kill a lot of innovation. Source-available licenses protect open source companies from that threat, even if the Open Source Initiative (OSI) doesn’t acknowledge these licenses as open source, hence the clumsy name.
Zeebe is distributed under The Zeebe Community License, a comparable source-available license. It
This license allows for all intended use cases of existing users and customers. It actually “feels” like MIT. You can download, modify, and redistribute Zeebe code. You can include Zeebe in commercial products and services. As long as you don’t offer a generic workflow service.
Zeebe is designed as a truly scalable and resilient system without a central database. It is very performant. It can be used together with almost any programming language. It uses visual workflows in BPMN, that allow for true BizDevOps. This combination sets it apart from any orchestration or workflow engine I know of.
It is Open Source (or source-available to be precise) and the usage is pretty simple. So there are no barriers to get started.
Got an appetite to learn more about the distributed computing concepts we used to build Zeebe? Move on to my deep dive article how we built a highly scalable distributed state machine.
Bernd Ruecker is co-founder and chief technologist of Camunda. I am passionate about developer friendly workflow automation technology. Follow me on Twitter. As always, I love getting your feedback. Comment below or send me an email.
My personal blog.
385 
4
Thanks to Charley Mann. 
385 claps
385 
4
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@burshteyn/secrets-management-guide-approaches-open-source-tools-commercial-products-challenges-db560fd0584d?source=search_post---------371,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mike Burshteyn
Oct 7, 2018·21 min read
Secrets management is a hard problem. Many different approaches and tools are out there as well as new innovations in the space. Because we are deeply focused on this emerging space at CryptoMove, we put this post together as a resource to anybody who is thinking about or trying to learn more about secrets management. If this is an area of interest for you, we are running a private beta for CryptoMove’s Tholos Key Vault now—and we’re hiring!
I. Default secrets management approaches without toolingII. Why secrets management mattersIII. Options, tools, and commercial solutions for secrets managementIV. Key objectives for secrets management (pun intended)V. Challenges around secrets management
Usually secrets management defaults to whatever is easiest and fastest. When devs are working together on an image in AWS, GCP, Azure or some other cloud native environment, it’s not uncommon to see .PEM files, SSH keys, and other configuration secrets passed around in cleartext and across a variety of communications channels. Some of the most popular “anti-patterns”:
There are plenty of examples of situations where the default “fast & easy” approach to secrets management and sharing have led to issues with devops & cloud transformations.
www.scmagazine.com
www.zdnet.com
qz.com
Due to changes in infrastructure and software development processes, secrets are proliferating widely. Here are a few ways enterprise transformations are affecting secrets management at scale:
Many mainstream Global 2000 and Fortune 500 enterprises are behind on this, stuck with legacy HSMs focused on encryption keys, and struggling to manage secrets in multi-cloud and services based architectures.
But why don’t HSMs and traditional encryption key management products work for modern secrets management?
There are legacy key management products for encryption, databases, even cloud services in some cases. The difficulty many teams encounter is when these legacy tools are used for devops and cloud infrastructure related keys like API keys, pem files, micro-services secrets, tokens, and others. We are seeing a gap in capability where they are not able to address these needs for a variety of reasons. Here are a few challenges of legacy key management solutions we’ve collected speaking with teams working on a modern cloud-based devops workflow:
One example of a legacy company trying to plug a hole in its offerings here is CyberArk. They actually purchased an open source secrets management tool called Conjur to add that capability. Here’s how CyberArk announced the Conjur acquisition:
CyberArk aims to revolutionise DevOps security by using the fruits of the Conjur acquisition to deliver an “enterprise-class, automated privileged account security and secrets management” platform to secure the DevOps lifecycle and cloud-native environments. Which is nice. Privileged account credentials — SSH keys, API keys and more — are proliferating throughout IT infrastructure because of DevOps practices. CyberArk has acquired Conjur in an attempt to train and saddle this wild horse of software development.
Other traditional password and key management and encryption vendors haven’t addressed this capability gap — so they’re somewhat behind the 8-ball.
This gap in capability around secrets management for cloud infrastructure & devops is why a lot of companies have built their own secrets management tools in-house and helps explain why there are so many open source approaches — per below. This phenomena of many varied open source solutions to the secrets management problem shows it is a pretty large pain point for modern development organizations.
No real industry standard exists yet, as everything is fairly early in this space. At CryptoMove, we’re working on an enterprise-scalable solution for secrets management leveraging moving target defense technology under the hood. More on that later.
Due to the absence of too many off the shelf solutions, many companies have tried to build their own secrets management tools. Here are a few:
Keywhiz is a tool built by Square and operated in production for some time. It has a lot of features for storing and managing application and infrastructure secrets. Square describes Keywhiz as follows:
“Keywhiz makes managing secrets easier and more secure. Keywhiz servers in a cluster centrally store secrets encrypted in a database. Clients use mutually authenticated TLS (mTLS) to retrieve secrets they have access to. Authenticated users administer Keywhiz via CLI. To enable workflows, Keywhiz has automation APIs over mTLS.”
Pinterest also built its own secrets management tool and open sourced it, calling it Knox (as in Fort Knox). Here were the problems Pinterest was facing which led to the creation of Knox:
“Pinterest has a plethora of keys or secrets doing things like signing cookies, encrypting data, protecting our network via TLS, accessing our AWS machines, communicating with our third parties, and many more. If these keys become compromised, rotating (or changing our keys) used to be a difficult process generally involving a deploy and likely a code change. Keys/secrets within Pinterest were stored in git repositories. This means they were copied all over our company’s infrastructure and present on many of our employees laptops. There was no way to audit who accessed or who has access to the keys. Knox was built to solve these problems.”
Per Pinterest, the goals of Knox are:
“Ease of use for developers to access/use confidential secrets, keys, and credentialsConfidentiality for secrets, keys, and credentialsProvide mechanisms for key rotation in case of compromiseCreate audit log to keep track of what systems and users access confidential data”
Lyft’s approach to secrets management resulted in a tool called Confidant. Lyft keeps it short and sweet on the description: “Your secret keeper. Stores secrets in DynamoDB, encrypted at rest.”
In describing the problem Lyft was trying to solve with Confidant, Lyft explains that with all the new internal and external services, keeping track of keys and secrets became a very laborious process:
“As Lyft has grown, we’ve added numerous services to our infrastructure. These services have credentials to internal services and external services, SSL keys, and other types of secrets. Each of these services has multiple environments, and to insulate these environments from each other, they have a version of each of these secrets for each environment. In many cases some of these secrets may be shared across a few services. Given a large number of services, this leads to a very large number of credentials.The rotation of these secrets can be a laborious process, especially credentials for external services, since a large number of external services don’t have rotation methods that can be done without some amount of downtime and coordination. Coordination of the rotation of secrets became a difficult and time-consuming process for us pretty early on, and we knew the problem would only get worse as we added more internal services and more external dependencies.”
Here is a discussion of Confidant on Hacker News, with a lot of interesting feedback and compliments.
Many other tech companies have tried to are or are trying to create their own secrets management tool. Name a major tech company and they are probably working on this. More mainstream Fortune 500 or Global 2000 companies often are not aware of the problem yet, but as they increasingly transform their infrastructure and software delivery model, it’s likely they will run into the same issues that led to the creation of Keywhiz, Confidant, Knox, and others.
There are other open-source secrets management solutions outside of the secrets management tools built in-house at Lyft, Pinterest, Square (and many others we are aware of at other major tech companies that are not available to the public). These include a secrets management product from Docker, Hashicorp, and tools like Torus, Credstash, Sneaker, as well as generic tools that can be used for secrets management — with varying levels of sophistication — from AWS, Azure, and Google.
Docker secrets management is one of the newer secrets management tools out there. Coincidentally, it was designed by some of the overlapping folks who worked on Keywhiz at Square. Here’s how Docker describes the problem and why traditional static secrets management tools or encryption key management products can’t deal with these issues:
“A critical element of building safer apps is having a secure way of communicating with other apps and systems, something that often requires credentials, tokens, passwords and other types of confidential information — usually referred to as application secrets. We are excited to introduce Docker Secrets, a container native solution that strengthens the Trusted Delivery component of container security by integrating secret distribution directly into the container platform.With containers, applications are now dynamic and portable across multiple environments. This made existing secrets distribution solutions inadequate because they were largely designed for static environments. Unfortunately, this led to an increase in mismanagement of application secrets, making it common to find insecure, home-grown solutions, such as embedding secrets into version control systems like GitHub, or other equally bad — bolted on point solutions as an afterthought.”
Here’s an architecture diagram of how Docker secrets works:
Here’s an example of how Docker secrets can be used to manage environmental variables as well as AWS credentials. More documentation on Docker secrets is available here.
Hashicorp is an open source software company with many products, probably best known for its products like Vagrant and Terraform, and also has a secrets management product called Hashicorp Vault. The New Stack publishes a good overview of the pros and cons of Hashicorp Vault here, which discusses some of the benefits of managing secrets in an external tool vs. environmental variables, working nicely in the Hashicorp ecosystem, and the benefits of API integrations.
There is also an interesting comparison between Hashicorp Vault and Keywhiz from The New Stack:
While all the above is great — as is typically the case with software — certain tradeoffs were made in Vault’s design, and there are some limitations. While there are many benefits to being able to run Vault as a service, this does lead to increased infrastructure costs, and the associated pains of managing that infrastructure. Additionally, not all of Vault’s benefits are available for all use cases. For example, dynamically generated secrets can only integrate with a limited number of other services. Also, Keywhiz by Square is another big player in this area that is worthy of attention. The biggest fundamental difference between Vault and Keywhiz, is that while Vault exposes secrets over an API, Keywhiz uses a FUSE filesystem. Vault wrote a nice and fairly objective writeup on the comparison. Lastly, if you are working with Chef or a related tool, it’s probably easier initially to use their integrated solution than it is to wire it up to Vault.
Due to the complexity of standing up and maintaining a Hashicorp Vault infrastructure, there are quite a few guides and tutorials. Many companies end up hiring consultants or engaging in professional services contracts as part of their Hashicorp Vault rollouts. Here’s a handy guide and tutorial for best practices:
Have you ever installed Hashicorp Vault and wondered to yourself: “Am I actually protecting my organization?” You’re not alone. While it’s easy to install Vault, making sure that it is configured correctly for productivity and security can be a challenging task. I’ve built my fair share of guides and webinars and worked with Vault a-lot recently. This has led me to create my own list of Vault’s best practices.
Other tools out there include one created by T-Mobile called “T-Vault”, which uses some integrations to Hashicorp Vault under the hood and adds a variety of enterprise functionality. One CISO recently commented about the comparison of Hashicorp Vault to CryptoMove’s Tholos Vault in a post:
If you are using, or thinking of using, Hashicorp Vault you should look at Cryptomove. For being an early stage startup the product seemed quiet mature. The concept of constantly moving keys and data was very interesting and which I cannot do justice in a short LinkedIn post.
Torus is another tool for secrets management. It takes a hosted approach, while open sourcing its clients. There is an interesting HN discussion of Torus here.
Credstash is a pretty interesting distributed credential management system that is built on top of AWS KMS. Here’s how it describes the problem it tries to solve as well as its lighter-weight approach to the secrets management toolbox:
Software systems often need access to some shared credential. For example, your web application needs access to a database password, or an API key for some third party service.Some organizations build complete credential-management systems, but for most of us, managing these credentials is usually an afterthought. In the best case, people use systems like ansible-vault, which does a pretty good job, but leads to other management issues (like where/how to store the master key). A lot of credential management schemes amount to just SCP’ing a secrets file out to the fleet, or in the worst case, burning secrets into the SCM (do a github search on password).CredStash is a very simple, easy to use credential management and distribution system that uses AWS Key Management Service (KMS) for key wrapping and master-key storage, and DynamoDB for credential storage and sharing.
Sneaker is a tool that uses AWS KMS and S3 to create a lightweight secrets management capability:
sneaker is a utility for storing sensitive information on AWS using S3 and the Key Management Service (KMS) to provide durability, confidentiality, and integrity. Secrets are stored on S3, encrypted with AES-256-GCM and single-use, KMS-generated data keys.
There’s a pretty comprehensive and helpful comparison of various open source secrets management solutions and tools here. In addition to its comparison of tools, the author — who provides consulting on microservices and cloud architecture — lays out a great way to think about secrets best practices:
No secret should be written to disk in cleartext — everNo secret should be transmitted over a network in cleartext — everAll secret lifecycle and access events should be recorded in an incorruptible audit logSecret distribution should be coordinated by a authoritative delegator such as a container/service scheduler or working in a close trust relationship with the schedulerOperator access to secret cleartext should be limited — if not impossible without subversive effortsSecret versioning or rolling should be easier to accomplish than revealing cleartextAll infrastructure components related to secret management and distribution should be mutually authenticatedSecure system configuration should be easier than advanced, and likely insecure configurationThe attachment of a secret to a service or container should be protected by rich (pluggable) access control mechanisms — role based access control is a plusAnything that can be done to minimize the value of a secret should be done
Any time so many open source projects pop up in a space, it is a pretty good indication that there is a common problem lots of developers, security professionals, and software organizations are running into. Open source solutions and tools are often the first to sprout up.
Cloud infrastructure vendors have noticed that their users and customers are struggling with secrets management, and have started developing their own generic solutions, like AWS Secrets Manager, Azure Key Vault, and others described below. In some ways, these products are reminiscent of Amazon’s Whole Foods 365 “Lacroix-killer” or Safeway’s famous “Select” brand of sodas and other private label products. In the cloud world we live in, it seems reasonable to expect cloud-generic versions of such products and others to pop up.
AWS Secrets Manager is a tool that enables AWS users to manage secrets and credentials rather than saving them on disk or using one of the KMS-backed credential management open source solutions, like Sneaker. Here’s a video from the AWS product manager on how Secrets Manager is supposed to work:
There’s a handy discussion of comparisons between AWS Secrets Manager and one of the open source solutions, Hashicorp Vault, on reddit. Some of the limitations of AWS Secrets Manager (according to a person from Hashicorp on Reddit) include: lack of zero knowledge for admins, dynamic secret generation, one-click/API call certificate generation, SSH CA authority, encryption as a service, cross-region/cloud/datacenter replication, control groups for multi-person approvals, pluggable architecture, and some others.
AWS Parameter Store. AWS parameter store can be another way to manage secrets, without leveraging Secrets Manager. Here is a guide for how to set up AWS Parameter Store for secrets management.
AWS KMS. AWS KMS is not really a secrets management tool, although it is possible to wrap secrets with encryption using KMS, which is what some of the open source solutions referenced above do with KMS.
Azure key vault is a service that enables customers to manage all secrets (keys, certificates, connection strings, passwords etc) for their cloud application in a single place. It is integrated out of the box with sources and destinations of secrets in Azure, but can also be used by applications outside Azure.
It is sort of a combination of AWS KMS and AWS Secrets Manager, both supporting encryption services within Azure as well as managing non-encryption secrets and credentials. Here’s how Microsoft describes the problems Azure Key Vault is meant to solve:
Azure Key Vault helps solve the following problemsSecrets Management — Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secretsKey Management — Azure Key Vault can also be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.Certificate Management — Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with Azure and your internal connected resources.Store secrets backed by Hardware Security Modules — The secrets and keys can be protected either by software or FIPS 140–2 Level 2 validates HSMs
Here is a video of Microsoft’s product manager on Azure Key Vault describing how it’s supposed to work:
Google has an encryption key management service, but lacks a secrets manager product. The closest to come in Google cloud is to use KMS to wrap secrets with encryption and then store them using other GCP services that store secrets elsewhere. Here is Google’s documentation on how to do this. Also, here’s an interesting tutorial on how to use a combination of Google Cloud Platform services to store secrets in serverless application architecture. Google also has a general guide on ways to store secrets, ranging from in storage to in code itself (is this a great idea?).
Apart from the open source tools and cloud-vendor products, there are a few startups focused on building products in the space of key and secrets management.
At CryptoMove, we started out with a technology for moving target data protection for any type of data. As we’ve productized the underlying technology, we’ve seen prototypes of a variety of use cases ranging from protection of keys, secrets, files, databases, even live-streaming video or data across drone swarms, embedded devices such as cameras or sensors, and robotics.
However, after working with many early design and R&D partners across the Fortune 100, as well as federal agencies like the Department of Homeland Security, NIST, we realized that by far the most common first use case for CryptoMove was trying to apply our technology to key and secrets management. Armed with this learning, we spent over a year talking with hundreds of security teams, devops teams, developers, CISOs, and others to learn about the pain points around existing key and secrets management tools and processes especially as applied to multi-cloud, microservices, and IoT infrastructures.
Based on what we’ve learned, we have been working on a cloud-native SaaS secrets management product we call CryptoMove Tholos (Tholos being the greek word for Vault), with moving target data protection technology under the hood. More information is available here on our website and we’ll continue to blog about it.
Whether a commercial solution, open source project, or homegrown tool, there are a few common objectives for key and secrets managers and key vaults.
Secrets management is a problem many organizations encounter, as the examples from Square, Lyft, Pinterest, T-Mobile, and others show. Outside of the general issues around managing secrets for cloud and services based systems, what are the specific challenges organizations face around secrets?
Like with any technology transformation wave, there is a maturity model of organizations with various stages in the secrets management journey. Wherever an organization is on the maturity model and cloud-native/devops transformation, here are some helpful questions to think about when designing, implementing, upgrading, or optimizing a secrets management workflow and infrastructure.
Secrets management is a complex and emerging problem presented with the transformation towards cloud-native, services, and IoT infrastructure. As can be seen with the most advanced technology companies like Square, Pinterest, Lyft, and many others, every modern software organization at some points hits the wall when it comes to secrets management.
The exciting thing about secrets management is it sits at the intersection of data security and application security, opening up new ways to think about risk management and threat modeling an attack surface. And when done correctly, secrets management at scale can accelerate business process change around cloud and services transformations.
One thing is clear — we are in the early days of the curve when it comes to secrets management with a lot of room for innovation ahead.
CEO/Founder @CryptoMove
See all (835)
341 
5
341 claps
341 
5
CEO/Founder @CryptoMove
About
Write
Help
Legal
Get the Medium app
"
https://blog.bitsrc.io/building-a-serverless-webapp-why-you-should-consider-the-monolith-4f0105935589?source=search_post---------372,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
With the technology advancements in the cloud, it is possible to build full-stack web apps using Cloud native technologies, which are often called Serverless. Though it is a natural choice to go ahead with Serverless Microservices, for many instances, it becomes overkill. The fastest way to develop the application is by using the Serverless Monolith approach. If we take AWS, for example, you can build a Serverless Monolith by using a single AWS Lambda function for the backend.
Besides, it is likely that you write more code in the frontend where you actually need to look at breaking it down into smaller pieces.
Since Microservices is the buzz word these days, it is essential to remove the misconception that Monoliths are bad. Monoliths might be the best choice unless your application and development team is large enough to experience Monolith’s limitations.
Deploying the Frontend in AWS is quite straight forward. You can use AWS CloudFront to route requests and Amazon S3 to host the frontend artifacts. For more information about the technology stack, refer to my article on Full Stack Serverless Web Apps with AWS.
As mentioned earlier, it's better to build your frontend code as independent components right from the get-go. Your code will eventually get compiled into a single monolith but that does not take away all the benefits of a highly modular codebase. For that, you might want to use cloud component hubs like Bit.dev.
Bit.dev lets you publish components from any codebase and even document and organize them in your own component collection (on bit.dev). It will maximize code reuse, encourage a more modular design and will help you ship faster.
When we consider DevOps frameworks like Serverless Framework, it expects to point each route to a separate Lambda function. That means, let’s say you have the following routes.
It requires implementing each operation in a separate Lambda function, which in my opinion, is an overkill in terms of separation for most of the practical use cases with Web Apps.
You can find various Serverless Coding Patterns and their benefits in the Serverless Architecture Code Patterns article.
However, when moving to the monolith approach, AWS API Gateway forwards all the traffic to the same Lambda function. Here the API Gateway only acts as the request trigger for the Lambda function. Then for each routing path and HTTP method, we have to write conditional handling to execute the appropriate logic based on event path and event method (GET, POST or DELETE) inside the Lambda function code itself.
Writing if-else statements or switch statements isn’t as clean as you think. Even if we separate this logic to a handler function or a separate file, still the challenges remain the same, where you have to make sure no route and method combination overlaps with each other. Besides, you need to edit this file when you introduce or modify a new route that could potentially add conflicts.
But isn’t the above problem already solved in ExpressJS like frameworks, where you can register a listener inside code, instead of centralizing to a single place?
But how can we do this with AWS Lambda? Do we need to use ExpressJS itself? The good news is, you don’t need to use ExpressJS inside Lambda. You can use a handy library called Lambda-API that will handle the routing problem. The above code example is from the Lambda-API NPM library.
When following the Serverless Monolith pattern, its essential to understand that your Lambda function needs to structure its code in a maintainable way. As easy as it gets, we can follow the well-proven MVC pattern, separating the request handling, business logic, and response messages.
You can follow the below structure to organize your code effectively. I will use an example for NodeJS considering Lambda-API compatibility. I would recommend considering using Typescript instead of JavaScript, but it is entirely up to you to decide.
Well, if your application grew over time and decided to move towards Microservices, you can break your Backend API into smaller chunks. If you maintain a folder structure as mentioned above, you can choose to create a new backend and move some of the routes and business logic over there. For shared dependencies across Microservices, you can consider using Lambda Layers. Still, you need to find ways to handle the data separation depending on your bounded contexts.
You might wonder, is it worth to try the monolith approach in the beginning and move to Microservices someday? Is it worth the efforts?
Just imagine if you had to think of shared dependencies, data separation, transactions across Microservices at the beginning of a project in contrast to following the Monolith approach. It could have slowed down your development with a significant margin for a small team. You might speed up your development without dealing with distributed systems even for a couple of years, building a competitive advantage, where you have sufficient knowledge, capacity, time, and money to consider Microservices.
Besides, the monolith approach doesn’t limit you from building multiple APIs if you have a clear separation between them. For instance, if you plan to write an API for B2B integration, you can develop it separately from the backend of your web app as a separate Monolith. Also, there aren’t any hard and fast rules to only consider Monolith. For instance, if you have a specialized operation that requires a significant amount of resources, you can separate it from the Monolith web app backend and put it into a separate Lambda for apparent reasons.
Overall, the Serverless Monolith pattern provides a straight forward approach that is faster to develop. If you are new to Serverless and Microservices, I would say this is the way to go. Otherwise, do consider the overheads, before trying to break things into smaller pieces and do it smartly.
At last, smaller pieces of code with separate runtimes don’t mean that it’s always easy to maintain. Remember, whenever you consider coupling as a problem, there is the cohesion that needs to be balanced.
blog.bitsrc.io
blog.bitsrc.io
blog.bitsrc.io
The blog for advanced web development. Brought to you by the Bit community.
476 
3
476 claps
476 
3
Written by
Solutions Architect and a Content Specialist. For more details find me in Linkedin https://www.linkedin.com/in/ashanfer/
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Written by
Solutions Architect and a Content Specialist. For more details find me in Linkedin https://www.linkedin.com/in/ashanfer/
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/planet-stories/analysis-ready-data-defined-5694f6f48815?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
The next topic in my Cloud Native Geoprocessing series is “Analysis Ready Data” (ARD) — a growing trend among satellite imagery providers to handle more of the data preparation work typically done by end users in their desktop environments. Much of the use of modern geoprocessing systems goes toward making data ready for analysis, instead of actual analysis. It is worth understanding data preparation operations in depth through the lens of ARD.
This post will examine the core of ARD today and present a vision of how the earth observation industry can advance cross-provider ARD to make our data even more user-friendly. Tomorrow’s post will expand on the ARD vision, looking at how cloud native geoprocessing systems can be utilized to create ‘On-Demand Analysis Ready Data’ that can deliver data that matches the unique needs of different users, while handling the processing pipeline more efficiently.
Analysis Ready Data are time-series stacks of overhead imagery that are prepared for a user to analyze without having to pre-process the imagery themselves. Those who don’t work with satellite imagery every day likely underestimate the amount of labor involved in preparing imagery for analysis.
The typical satellite imagery expert looking to do any sort of time-series analysis has a number of preparatory steps they must take before beginning. First they must search and download all the data that overlaps their area and time range(s) of interest. Since they want as much data on their area as possible, this usually means going to several different providers’ portals, as there is no central index of all imagery. Next they must ‘clip’ the data, selecting only the area of analysis since most all the data they just obtained will include more pixels than needed. The resulting images never quite line up spatially pixel for pixel, and often come in different resolutions, so the next step is to geospatially ‘co-register’ and resample the data.
Then the user must perform atmospheric correction, first running specific conversions to translate the image into a physical measurement of light on the ground (for eg, converting from Digital Numbers (DNs) to At Aperture Radiance; transforming radiance to TOA reflectance before using an atmospheric model to get to BOA reflectance; and finally doing a BRDF correction). These conversion steps are essential for many analyses as they produce an accurate reading of the surface of the Earth and subtract out the effects of tens of miles of atmosphere. Next, the analyst usually masks out clouds and image issues to remove readings that may not actually be the ground. Finally, a user will radiometrically ‘co-register’ the data so that readings from different sensors can be treated as the same.
Once these steps are completed, you are ready to start analysis!
The idea behind Analysis Ready Data is that providers of satellite imagery are in a better position to undertake these routine steps than the average user. The concept of ARD has existed for quite awhile, but a few people in the Landsat community have really pushed it forward in the last couple years, making use of new computation capabilities. The core of the Australian Geoscience Data Cube initiative (which evolved to become opendatacube.org) was processing Landsat data into regular grids of surface reflectance data for time-series exploitation. USGS has recently released their Landsat Analysis Ready Data (ARD) product for the United States, also clipping the data into regular grids and making the surface reflectance data available. These groups have defined the core of modern ARD.
Building on the Landsat work, I’d like to put forward an accessible definition and explanation of Analysis Ready Data, starting with a minimum standard that many meet today. I’ll follow that with a vision for how cross-provider ARD could be a real step forward for the satellite industry by making data far more accessible and useful to a wider audience. As mentioned above, ARD is time-series stacks of overhead imagery that are prepared for a user to make use of without having to do their own pre-processing before analysis. But what exactly are the pre-processing steps that enable better analysis?
There are a few data preparation operations that should be done at a minimum to be considered Analysis Ready Data.
Image Clipping. A majority of satellite imagery is distributed in a satellite-centric way — you get the full “picture” of wherever the satellite happened to be looking. This makes for nice large pictures, but the problem is most users only care about a particular area. The first step is to ‘clip’ the image to a regular grid. This lets a user know that they care about the grid around their area of interest and only need to download data in that grid. Planet’s imagery spec has a good illustration of this:
Good Analysis Ready Data defines a fixed grid and clips all scenes to fit into those tiles, so users can treat data as a coherent stack instead of a bunch of randomly placed overlapping images. This is an example of Landsat’s Continental United States Tiles.
Unusable Data Masks. Although satellite imagery contains a lot of great information, the downloaded data usually contains at least some pixels that aren’t that useful. One of the hardest to deal with is clouds, which account for roughly 60 percent of the Earth’s surface at any given time. Clouds and the shadows that they cast, haze, and snow throw off the results of most analytic algorithms, so it is useful to have a map that tells which pixels are clouds, which are hazy, which are shadowed, and which contain snow.
More advanced ‘Unusable Data Masks’ (UDM) report other things that can throw off algorithms, like issues with the sensor. UDMs also make it easy to filter out pixels that were not captured, which often happens with regular grids (seen in the Planet image clipping example above). It’s pretty standard for most satellite imagery to come with at least a basic cloud and valid pixel mask, but good ARD should make sure the mask is really accurate, as often those that come by default don’t identify all clouds or get thrown off by snow and ice.
Atmospheric Correction. One of the most challenging aspects of working with satellite imagery is that the atmosphere is constantly changing. You can see this in the images below.
For many types of analysis, these differences can throw off the algorithm. Most satellites are calibrated to be able to translate what was captured to an actual ‘reflectance’ value — an approximation of the percentage of the incident light returned instead of just a ‘Digital Number’ of relative pixel values. But without atmospheric correction, the reflectance values are just what’s at the top of atmosphere.
Atmospheric correction takes into account what is in the atmosphere when the picture was taken — how much water vapor, ozone, aerosols, etc. — and compensates for the losses and apparent gains injected by the atmosphere, resulting in accurate reflectance values. Ideally, there are further corrections for terrain effects and target Bidirectional Reflectance Distribution Function (BRDF). The result is called the ‘surface reflectance,’ a reading of what the ground was actually reflecting. The leading satellite imagery providers now offer surface reflectance data, which is the result of all that processing. It’s what any analyst wants to work with, as it makes the day-to-day measurements as close to reality as possible. Analysis Ready Data should be processed all the way to surface reflectance, so people do not have to provide their own atmospheric correction.
Pixel Alignment. The way imagery works is that every image is referenced to its position on Earth, so that many disparate images can line up exactly with each other. Unfortunately, this is much harder to do in practice. The gif to the left shows an example of this. If the alignment is off more than a pixel, then it can disrupt the analysis, since the algorithm won’t be comparing the same spot on Earth. Usually providers optimize for ‘absolute positioning’ — making sure that every image is as close to its position in real life as possible. But if one wants to guarantee that pixels in a multi-temporal ARD stack of data are aligned to one another, then one can optimize for ‘relative positioning’ using a process called ‘co-registration.’ This picks a single base image and lines up every subsequent image against that. This choice privileges the relative position, even if the absolute is a bit off. Since Analysis Ready Data is built for time-series analysis, it is essential to perform a co-registration step.
Sensor Alignment. The final component of Analysis Ready Data is confidence that the relative spectral response of any given image band is aligned against other imagery in the stack, particularly in the case of diverse sensors. The most common bands are red, green, blue, and near infrared. Sensor alignment means that the red captured in one image represents the same captured values as red in another image. This is important for algorithmic comparison. Practically speaking, this is easy for most current Analysis Ready Data, as they tend to be acquired from a single sensor, or against a constellation that has invested lots of effort in making sure that each sensor aligns with one another. This gets more difficult when one wants to make ARD from multiple sensors, but those harder problems are not relevant to most existing providers.
Most Analysis Ready Data today meets the standard listed above. But I believe the real potential of ARD lies beyond what individual providers do to prepare data for analysis. Instead, the industry should work toward standardization of cross-sensor and cross-provider Analysis Ready Data. Users should not have to figure out a specific stack of tools to prepare data from each provider, and they should be abstracted from questions around if the width of the red band captured by DigitalGlobe’s WorldView 2 is different than Planet’s SkySat constellation. There is room to improve on baseline ARD in most every direction, especially when viewed in light of data from a variety of sensors.
In light of this, we can reevaluate the categories above with a view toward what evolution in each looks like.
Image Clipping. My view on image clipping and gridding is heavily influenced by my strong belief that computer vision and deep learning is going to fundamentally transform remote sensing. A computer vision expert who knows nothing about geospatial ought to be able to consume the right image chips and labeled training data without having to think about projections or GIS formats. Therefore, I believe the main improvement that can be made to ARD is to create image grids that are easily consumed by machine learning. The typical deep learning models for images utilize much smaller ‘chips’ of pictures than the geospatial industry typically uses. There are a number of nuances in how those algorithms work, and in the geospatial industry we can likely provide some guidance on how earth observation imagery can work better (for eg, image chips that include a ‘collar’ of additional pixels to provide context). There’s a need to get to hierarchical grids for imagery that decompose down elegantly to the shapes and sizes that work well with the leading computer vision algorithms.
Unusable Data Masks. At Planet we’ve been talking about the idea of moving to Usable Data Masks, which means not just communicating which pixels are bad. Some analysis applications are thrown off by cloud shadows, and some are happy to use slightly hazy data. A great ‘Usable Data Mask’ should include classes not only for clouds, but also for heavy and light haze, clouds, and shadows. Including snow and ice can also be good, as it is seasonal variation that can also throw off analysis, and often gets confused for clouds. A great UDM should be as accurate as possible, but more importantly it should also include a per-pixel assessment of its confidence.
Atmospheric Correction. Many atmospheric correction models are custom to certain datasets, as they may have particular bands that are useful to assess the state of the atmosphere. Many take into account third-party data, like from MODIS. As we move to cross-sensor Analysis Ready Data it is important that images use compatible atmospheric correction models. Great ARD should supply additional correction maps to enable an advanced user to correctly interpret the data, including sun elevation maps and BRDF maps. Having atmospheric correction done well would mean that a single-color curve could be applied to all imagery in a stack and not return different colors like above. The ‘visual’ product of an ARD dataset should look highly consistent as a result of successfully modeling out all the effects of the atmosphere, sun angles, observation geometry, and terrain.
Pixel Alignment. While co-registration can align pixels against one another, ideally the original ground-locking of images is done against high-quality ground control points (GCPs), and with a great Digital Elevation Model (DEM) to do orthorectification. Cross-sensor ARD should correct all images in the stack against the same DEM and GCPs, instead of just stretching the pixels to align. Open data sets that can form an open baseline would help with this, and those who have access to the higher quality GCP and DEM data could also process the complete stack against their own data.
Sensor Alignment. Perhaps the trickiest problem for cross-sensor and cross-provider ARD is to align the bands properly. Ideally, there is major investment in cross-sensor calibrations, with every pair of sensors comparing images taken of the same place at the same time. This would allow validated translation equations based on real-world observations. Short of that, it is important for every image first to do surface reflectance and then some sort of radiometric normalization that enables a user to treat two red bands as the same red band, for example.
…
We’ll draw to a close here, as I broke this into two posts to make sure I didn’t overwhelm people with text. I do want to thank Keith Beckett for his contributions to this post, helping me get all the details correct. Tomorrow we’ll pick up with exploring ARD generated on demand using cloud native geoprocessing systems and wrap up.
Read Part Two: On-Demand Analysis Ready Data.
Using space imagery to tell stories about our changing…
255 
3
255 claps
255 
3
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
"
https://medium.com/@lizrice/non-privileged-containers-based-on-the-scratch-image-a80105d6d341?source=search_post---------374,"Sign in
There are currently no responses for this story.
Be the first to respond.
Liz Rice
Sep 6, 2017·3 min read
I’m on my way home from an exceptionally interesting Cloud Native London meetup, where one of the talks was by Michael Hausenblas and Nic Jackson on the benefits of running containers as non-root users. You can find a ton of info about the topic at the snappily-named canihaznonprivilegedcontainers.info.
Michael and Nic showed how you can add commands to your Dockerfile to add a user, and then switch to that user identity before running the executable with the USER command.
An intriguing question came up: if you build a container based on the scratch image, can you run as a non-privileged user? I’m a big fan of using scratch as the basis for running binary executables (such as you might compile from a Go program), so I wanted to find out. That’s what 30-minute train journeys home are for, right?
Here’s a Dockerfile that gives us what we’re looking for.
Building from this Dockerfile starts FROM an Ubuntu base image, and creates a new user called scratchuser. See that second FROM command? That’s the start of the next stage in the multi-stage build, this time working from the scratch base image (which contains nothing).
Into that empty file system we copy a binary called dosomething — it’s a trivial Go app that simply sleeps for a while.
We also copy over the /etc/passwd file from the first stage of the build into the new image. This has scratchuser in it.
This is all we need to be able to transition to scratchuser before starting the dosomething executable.
After building this image we can run it, and then look for the running dosomething from the host machine.
The first line of the results is the grep command itself, and the second is the executable running inside the container. As you can see, it is running under the user ID 10001. That ID is listed as a number rather than a name because it isn’t defined in the host’s /etc/passwd file.
I’m pretty sure you could simply create an appropriately formatted, minimal /etc/passwd file (it’s just a text file, after all). and copy it into the scratch image. You would probably want to also create a group, but hey, it was only a short train ride home!
If you want to find containers that are running as root, Nic Jackson has created cnitch. It is also a feature available in the ultra-powerful Aqua Security console.
EDITS: 6 Sep 2017 updated to include detector tool information
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
377 
5
377 
377 
5
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
"
https://betterprogramming.pub/k8s-vanilla-all-the-way-9ba8c746e4f0?source=search_post---------375,"Sign in
Luc Juggery
Jan 8, 2021·10 min read
A couple of weeks ago, I stumbled upon the Vanilla Stack, a technology stack based on Kubernetes and embedding many great open source components. In this article, which is mainly a presentation of the stack, we will quickly go through the installation…
"
https://medium.com/appgambit/what-all-has-changed-in-the-aws-lambda-in-the-past-one-year-c5ff69f2df60?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
Serverless computing, in general, is now considered a rapid way to build modern cloud-native applications. Like every major design systems in the past decades, the Serverless has also gone through a number of changes and improvements over time.
It has been only 6 years since AWS launched the Lambda service and this year, by far, has seen some major improvements. Lambda alone can do so little, but the thriving eco-system of services (integrations and event-emitters) is helping the overall Serverless Computing on AWS to cater to a vast variety of use cases.
This was one of the quotes from Andy Jassy at the recently concluded re:Invent 2020 keynote.
Forrest Brazeal wrote a great post about recent Lambda additions and the trend-shift in Serverless computing for better Serverless computing.
acloudguru.com
Let’s look at some of the most important features and their impact going forward.
Yes, because this is designed to be a serverless, pay-per-use, auto-scaling, and low-operations system. That means heavy-lifting is done by AWS. To work on the other features, Lambda service had to compromise on this one.
A year back, Lambda service announced the availability of the Provisioned Concurrency. This means we can define how many Lambda functions will be kept in memory to avoid spending extra time on serving new requests.
This does not break any promise of serverless computing. We still don’t need to worry about servers and we can configure the concurrency when required.
aws.amazon.com
Lambda service had default ephemeral storage of only 512Mb and need to rely on S3 for all the persistence.
In mid-2020, AWS announced the integration of EFS with Lambda to allow low-latency, high throughput file processing. This feature helps Lambda function to process large files seamlessly.
The biggest difference in using EFS vs S3. EFS is a file system, so the code can use the file system APIs instead of using the S3 APIs.
aws.amazon.com
Lambda service calculates the billing on a 100ms time block. So for example, if a function takes 55ms to complete the processing, the Lambda service will still charge for a 100ms time block. That’s like paying nearly double the cost, and at a large-scale, this is a big amount.
With a recent announcement, Lambda service now charges in a 1ms block. That means for a 55ms function execution, we will be paying around 55ms only.
aws.amazon.com
Yes, and this is a good thing. Lambda was originally designed to build micro-functions that can run independently to execute code based on the attached events.
With the latest announcement, now Lambda service can run Functions (as in code in zip file) or Containers (as in containers).
Depending on your understanding of Serverless computing, this is a very interesting addition. This allows builders to leverage Serverless computing without making major changes in their existing software packages, provided they are containerized.
If your goal is to design better cloud-native software, you will find this feature a very good option for future design.
aws.amazon.com
Lambda functions can only configure up to 3GB of RAM. There is no way to configure the CPU, and it is allocated proportionally based on the amount of RAM configured.
With the latest announcement, Lambda functions can now allocate up to 10 GB of RAM.
With CPU proportional to the amount of RAM, now you can perform more complex tasks in a function and may still be fine with the 15 minutes maximum time duration.
aws.amazon.com
Yes, it does, because the promise is to get away from the low-level details and instead focus on building features.
But that is changing, with steep adoptions, builders need more insights and control so that they can design better serverless functions without adding more code.
With the latest announcement, Lambda Extensions can help extract and augment the internal details. Extensions help integrating external tools like a “sidecar” without making any changes in the function code.
aws.amazon.com
With these many changes, we hope to see newer and better use cases in the future with Lambda and Serverless services.
https://virtual.awsevents.com/media/t/1_pskgtbiq/188376503
https://virtual.awsevents.com/media/t/1_b9a1su9s/188376503
https://virtual.awsevents.com/media/1_d0h6ry5o
This is not about Serverless is better and other systems need to match up. This is about building and running better and efficient software for tomorrow.
AWS Consulting Partner | Web and Mobile Development Co Based in India
149 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
149 claps
149 
1
Written by
AWS Serverless Hero, 12x AWS, 2x GCP, CKAD, Docker Certified, Founder @ AppGambit
AWS Consulting Partner | Full-stack Web/Mobile, Serverless, Cloud Native Development and Consulting Agency.
Written by
AWS Serverless Hero, 12x AWS, 2x GCP, CKAD, Docker Certified, Founder @ AppGambit
AWS Consulting Partner | Full-stack Web/Mobile, Serverless, Cloud Native Development and Consulting Agency.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GiantSwarm/securing-the-configuration-of-kubernetes-cluster-components-18fa5c248ec2?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Dec 3, 2018·11 min read
In the previous article of this series Securing Kubernetes for Cloud Native Applications, we discussed what needs to be considered when securing the infrastructure on which a Kubernetes cluster is deployed. This time around, we’re turning our attention to the cluster itself.
Kubernetes is a complex system, and the diagram above shows the many different constituent parts that make up a cluster. Each of these components needs to be carefully secured in order to maintain the overall integrity of the cluster.
We won’t be able to cover every aspect of cluster-level security in this article, but we’ll aim to address the more important topics. As we’ll see later, help is available from the wider community, in terms of best-practice security for Kubernetes clusters, and the tooling for measuring adherence to that best-practice.
We should start with a brief observation about the many different tools that can be used to install the cluster components.
Some of the default configuration parameters for the components of a Kubernetes cluster, are sub-optimal from a security perspective, and need to be set correctly to ensure a secure cluster. Unless you opt for a managed Kubernetes cluster (such as that provided by Giant Swarm), where the entire cluster is managed on your behalf, this problem is exacerbated by the many different cluster installation tools available, each of which will apply a subtly different configuration. While most installers come with sane defaults, we should never consider that they have our backs covered when it comes to security, and we should make it our objective to ensure that whichever installer mechanism we elect to use, it’s configured to secure the cluster according to our requirements.
Let’s take a look at some of the important aspects of security for the control plane.
The API server is the hub of all communication within the cluster, and it’s on the API server where the majority of the cluster’s security configuration is applied. The API server is the only component of the cluster’s control plane, that is able to interact directly with the cluster’s state store. Users operating the cluster, other control plane components, and sometimes cluster workloads, all interact with the cluster using the server’s HTTP-based REST API.
Because of its pivotal role in the control of the cluster, carefully managing access to the API server is crucial as far as security is concerned. If somebody or something gains unsolicited access to the API, it may be possible for them to acquire all kinds of sensitive information, as well as gain control of the cluster itself. For this reason, client access to the Kubernetes API should be encrypted, authenticated, and authorized.
To prevent man-in-the-middle attacks, the communication between each and every client and the API server should be encrypted using TLS. To achieve this, the API server needs to be configured with a private key and X.509 certificate.
The X.509 certificate for the root certificate authority (CA) that issued the API server’s certificate, must be available to any clients needing to authenticate to the API server during a TLS handshake, which leads us to the question of certificate authorities for the cluster in general. As we’ll see in a moment, there are numerous ways for clients to authenticate to the API server, and one of these is by way of X.509 certificates. If this method of client authentication is employed, which is probably true in the majority of cases (at least for cluster components), each cluster component should get its own certificate, and it makes a lot of sense to establish a cluster-wide PKI capability.
There are numerous ways that a PKI capability can be realised for a cluster, and no one way is better than another. It could be configured by hand, it may be configured courtesy of your chosen installer, or by some other means. In fact, the cluster can be configured to have its own in-built CA, that can issue certificates in response to certificate signing requests submitted via the API server. Here, at Giant Swarm, we use an operator called cert-operator, in conjunction with Hashicorp’s Vault.
Whilst we’re on the topic of secure communication with the API server, be sure to disable its insecure port (prior to Kubernetes 1.13), which serves the API over plain HTTP (--insecure-port=0)!
Now let’s turn our attention to controlling which clients can perform which operations on which resources in the cluster. We won’t go into much detail here, as by and large, this is a topic for the next article. What’s important, is to make sure that the components of the control plane are configured to provide the underlying access controls.
When an API request lands at the API server, it performs a series of checks to determine whether to serve the request or not, and if it does serve the request, whether to validate or mutate the the resource object according to defined policy. The chain of execution is depicted in the diagram above.
Kubernetes supports many different authentication schemes, which are almost always implemented externally to the cluster, including X.509 certificates, basic auth, bearer tokens, OpenID Connect (OIDC) for authenticating with a trusted identity provider, and so on. The various schemes are enabled using relevant config options on the API server, so be sure to provide these for the authentication scheme(s) you plan to use. X.509 client certificate authentication requires the path to a file containing one or more certificates for CAs (--client-ca-file), for example. One important point to remember, is that by default, any API requests that are not authenticated by one of the authentication schemes, are treated as anonymous requests. Whilst the access that anonymous requests gain can be limited by authorization, if they’re not required, they should be turned off altogether (--anonymous-auth=false).
Once a request is authenticated, the API server then considers the request against authorization policy. Again, the authorization modes are a configuration option (--authorization-mode), which should at the very least be altered from the default value of AlwaysAllow. The list of authorization modes ideally should include RBAC and Node, the former for enabling the RBAC API for fine-grained access control, and the latter to authorize kubelet API requests (see below).
Once an API request has been authenticated and authorized, the resource object can be subject to validation or mutation before it’s persisted to the cluster’s state database, using admission controllers. A minimum set of admission controllers are recommended for use, and shouldn’t be removed from the list, unless there is very good reason to do so. Additional security related admission controllers that are worthy of consideration are:
Dynamic admission control, which is a relatively new feature in Kubernetes, aims to provide much greater flexibility over the static plugin admission control mechanism. It’s implemented with admission webhooks and controller-based initializers, and promises much for cluster security, just as soon as community solutions reach a level of sufficient maturity.
The kubelet is an agent that runs on each node in the cluster, and is responsible for all pod-related activities on the node that it runs on, including starting/stopping and restarting pod containers, reporting on the health of pod containers, amongst other things. After the API server, the kubelet is the next most important cluster component to consider when it comes to security.
The kubelet serves a small REST API on ports 10250 and 10255. Port 10250 is a read/write port, whilst 10255 is a read-only port with a subset of the API endpoints.
Providing unfettered access to port 10250 is dangerous, as it’s possible to execute arbitrary commands inside a pod’s containers, as well as start arbitrary pods. Similarly, both ports provide read access to potentially sensitive information concerning pods and their containers, which might render workloads vulnerable to compromise.
To safeguard against potential compromise, the read-only port should be disabled, by setting the kubelet’s configuration, --read-only-port=0. Port 10250, however, needs to be available for metrics collecting and other important functions. Access to this port should be carefully controlled, so let’s discuss the key security configurations.
Unless its specifically configured, the kubelet API is open to unauthenticated requests from clients. It’s important, therefore, to configure one of the available authentication methods; X.509 client certificates, or requests with Authorization headers containing bearer tokens.
In the case of X.509 client certificates, the contents of a CA bundle needs to be made available to the kubelet, so that it can authenticate the certificates presented by clients during a TLS handshake. This is provided as part of the kubelet configuration (--client-ca-file).
In an ideal world, the only client that needs access to a kubelet’s API, is the Kubernetes API server. It needs to access the kubelet’s API endpoints for various functions, such as collecting logs and metrics, executing a command in a container (think kubectl exec), forwarding a port to a container, and so on. In order for it to be authenticated by the kubelet, the API server needs to be configured with client TLS credentials (--kubelet-client-certificate and --kubelet-client-key).
If you’ve taken the care to configure the API server’s access to the kubelet’s API, you might be forgiven for thinking ‘job done’. But this isn’t the case, as any requests hitting the kubelet’s API that don’t attempt to authenticate with the kubelet, are deemed to be anonymous requests. By default, the kubelet passes anonymous requests on for authorization, rather than rejecting them as unauthenticated.
If it’s essential in your environment to allow for anonymous kubelet API requests, then there is the authorization gate, which gives some flexibility in determining what can and can’t get served by the API. It’s much safer, however, to disallow anonymous API requests altogether, by setting the kubelet’s --anonymous-auth configuration to false. With such a configuration, the API returns a 401 Unauthorized response to unauthorized clients.
With authorizing requests to the kubelet API, once again it’s possible to fall foul of a default Kubernetes setting. Authorization to the kubelet API operates in one of two modes; AlwaysAllow (default) or Webhook. The AlwaysAllow mode does exactly what you’d expect - it will allow all requests that have passed through the authentication gate, to succeed. This includes anonymous requests.
Instead of leaving this wide open, the best approach is to offload the authorization decision to the Kubernetes API server, using the kubelet’s --authorization-mode config option, with the webhook value. With this configuration, the kubelet calls the SubjectAccessReview API (which is part of the API server) to determine whether the subject is allowed to make the request, or not.
In older versions of Kubernetes (prior to 1.7), the kubelet had read-write access to all Node and Pod API objects, even if the Node and Pod objects were under the control of another kubelet running on a different node. They also had read access to all objects that were contained within pod specs; the Secret, ConfigMap, PersistentVolume and PersistentVolumeClaim objects. In other words, a kubelet had access to, and control of, numerous resources it had no responsibility for. This is very powerful, and in the event of a cluster node compromise, the damage could quickly escalate beyond the node in question.
For this reason, a Node Authorization mode was introduced specifically for the kubelet, with the goal of controlling its access to the Kubernetes API. The Node authorizer limits the kubelet to read operations on those objects that are relevant to the kubelet (e.g. pods, nodes, services), and applies further read-only limits to Secrets, Configmap, PersistentVolume and PersistentVolumeClaim objects, that are related specifically to the pods bound to the node on which the kubelet runs.
Limiting a kubelet to read-only access for those objects that are relevant to it, is a big step in preventing a compromised cluster or workload. The kubelet, however, needs write access to its Node and Pod objects as a means of its normal function. To allow for this, once a kubelet’s API request has passed through Node Authorization, it’s then subject to the NodeRestriction admission controller, which limits the Node and Pod objects the kubelet can modify — its own. For this to work, the kubelet user must be system:node:<nodeName>, which must belong in the system:nodes group. It’s the nodeName component of the kubelet user, of course, which the NodeRestriction admission controller uses to allow or disallow kubelet API requests that modify Node and Pod objects. It follows, that each kubelet should have a unique X.509 certificate for authenticating to the API server, with the Common Name of the subject distinguished name reflecting the user, and the Organization reflecting the group.
Again, these important configurations don’t happen automagically, and the API server needs to be started with Node as one of the comma-delimited list of plugins for the --authorization-mode config option, whilst NodeRestriction needs to be in the list of admission controllers specified by the --enable-admission-plugins option.
It’s important to emphasize that we’ve only covered a sub-set of of the security considerations for the cluster layer (albeit important ones), and if you’re thinking that this all sounds very daunting, then fear not, because help is at hand.
In the same way that benchmark security recommendations have been created for elements of the infrastructure layer, such as Docker, they have also been created for a Kubernetes cluster. The Center for Internet Security (CIS) have compiled a thorough set of configuration settings and filesystem checks for each component of the cluster, published as the CIS Kubernetes Benchmark.
You might also be interested to know that the Kubernetes community has produced an open source tool for auditing a Kubernetes cluster against the benchmark, the Kubernetes Bench for Security. It’s a Golang application, and supports a number of different Kubernetes versions (1.6 onwards), as well as different versions of the benchmark.
If you’re serious about properly securing your cluster, then using the benchmark as a measure of compliance, is a must.
Evidently, taking precautionary steps to secure your cluster with appropriate configuration, is crucial to protecting the workloads that run in the cluster. Whilst the Kubernetes community has worked very hard to provide all of the necessary security controls to implement that security, for historical reasons some of the default configuration overlooks what’s considered best-practice. We ignore these shortcomings at our peril, and must take the responsibility for closing the gaps whenever we establish a cluster, or when we upgrade to newer versions that provide new functionality.
Some of what we’ve discussed here, paves the way for the next layer in the stack, where we make use of the security mechanisms we’ve configured, to define and apply security controls to protect the workloads that run on the cluster. The next article is called Applying Best Practice Security Controls to a Kubernetes Cluster.
Written by Puja Abbassi — Developer Advocate @ Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
196 
196 
196 
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
"
https://itnext.io/grafana-logging-using-loki-45665916aec9?source=search_post---------378,"Loki is a Prometheus-inspired logging service for cloud native infrastructure.
Open sourced by Grafana Labs during KubeCon Seattle 2018, Loki is a logging backend optimized for users running Prometheus and Kubernetes with great logs search and visualization in Grafana 6.0.
Loki was built for efficiency alongside the following goals:
As said, Loki is designed for efficiency to work well in the Kubernetes context in combination with Prometheus metrics.
The idea is to switch easily between metrics and logs based on Kubernetes labels you already use with Prometheus.
Unlike most logging solutions, Loki does not parse incoming logs or do full-text indexing.
Instead, it indexes and groups log streams using the same labels you’re already using with Prometheus. This makes it significantly more efficient to scale and operate.
Loki is a TSDB (Time-series database), it stores logs as split and gzipped chunks of data.
The logs are ingested via the API and an agent, called Promtail (Tailing logs in Prometheus format), will scrape Kubernetes logs and add label metadata before sending it to Loki.
This metadata addition is exactly the same as Prometheus, so you will end up with the exact same labels for your resources.
The easiest way to deploy Loki on your Kubernetes cluster is by using the Helm chart available in the official repository.
You can follow the setup guide from the official repo.
This will deploy Loki and Promtail.
Promtail is the metadata appender and log sending agent
The Promtail configuration you get from the Helm chart is already configured to get all the logs from your Kubernetes cluster and append labels on it as Prometheus does for metrics.
However, you can tune the configuration for your needs.
Here are two examples:
You can use the action: keep for your namespace and add a new relabel_configs for each scrape_config in promtail/configmap.yaml
For example, if you want to get logs only for the kube-system namespace:
For example, if you want to exclude logs from kube-system namespace:
You can use the action: drop for your namespace and add a new relabel_configs for each scrape_config in promtail/configmap.yaml
For more info on the configuration, you can refer to the official Prometheus configuration documentation.
Fluentd is a well-known and good log forwarder that is also a [CNCF project] (https://www.cncf.io/projects/). It has a lot of input plugins and good filtering built-in. So, if you want to for example, forward journald logs to Loki, it’s not possible via Promtail so you can use the fluentd syslog input plugin with the fluentd Loki output plugin to get those logs into Loki.
You can refer to the installation guide on how to use the fluentd Loki plugin.
There’s also an example, of how to forward API server audit logs to Loki with fluentd.
Here is the fluentd configuration:
By default, Promtail is configured to automatically scrape logs from containers and send them to Loki. Those logs come from stdout.
But sometimes, you may like to be able to send logs from an external file to Loki.
In this case, you can set up Promtail as a sidecar, i.e. a second container in your pod, share the log file with it through a shared volume, and scrape the data to send it to Loki
Assuming you have an application simple-logger. The application logs into /home/slog/creator.log
Your kubernetes deployment will look like this :
2. Use a shared data volume containing the log file
3. Configure Promtail to read your log file
And you’re done.
A running example can be found here
So Loki looks very promising. The footprint is very low. It integrates nicely with Grafana and Prometheus. Having the same labels as in Prometheus is very helpful to map incidents together and quickly find logs related to metrics. Another big point is the simple scalability, Loki is horizontally scalable by design.
As Loki is currently alpha software, install it and play with it. Then, join us on grafana.slack.com and add your feedback to make it better.
Interested in finding out how Giant Swarm handles the entire cloud native stack including Loki? Request your free trial of the Giant Swarm Infrastructure here.
Written by Julien Garcia Gonzalez — Solution engineer @ Giant Swarm
twitter.com
ITNEXT is a platform for IT developers & software engineers…
128 
4
128 claps
128 
4
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://blog.bernd-ruecker.com/publishing-practical-process-automation-234c8f4001a4?source=search_post---------379,"IMPORTANT UPDATE!
The book is out, please visit the final release blog post instead this one: https://blog.bernd-ruecker.com/publishing-practical-process-automation-with-oreilly-db993db68411
In today’s IT architectures, microservices and serverless functions play an increasingly important role. But how can you create meaningful, comprehensive, and connected business solutions if the individual components are decoupled and independent by design? How does this all affect business processes and process automation?
I’ve been thinking about this question for a long time now, and I discussed it with many customers in real-life scenarios. This resulted in many blog posts, conference talks and articles. This again led to countless discussions, that showed one thing clearly: We need guidance.
Today I am thrilled to announce that I’ve condensed my experience (and of course the whole Camunda team’s to some extent) in a book called “Practical Process Automation” published by O’Reilly.
The book provides a framework through examples and practical advice, and reveals how you can design complex processes in modern architectures to deliver true business value.
I will demonstrates how to use process automation technology to apply typical long-running patterns around resiliency, messaging, orchestration, or consistency, without forcing your service implementation to become stateful itself. This is especially important, as modern systems become more distributed, asynchronous, and reactive and thus require state handling to deal with long-running interactions.
I further describe how process automation compares to business process management, service-oriented architecture, batch processing, event streaming, and data pipeline solutions.
From the cover:
I am excited that this book is now available in early release. The first four chapters are already live, and more will follow soon.
https://www.oreilly.com/library/view/practical-process-automation/9781492061441/
I plan to finish content end of Q1/2021. As I already have a first draft of the whole book, so I am confident :-) By the way: If you fancy doing a review of the content, contact me!
There will be
I am happy that I can share a 30 days free trial code for the O’Reilly platform. So you can check out the early release of the book and event attend my training if you like. Send me an email if you are interested.
Bernd Ruecker is co-founder and chief technologist of Camunda. He likes speaking about himself in the third-person. He is passionate about developer-friendly process automation technology. Connect via LinkedIn or follow him on Twitter. As always, he loves getting your feedback. Comment below or send him an email.
My personal blog.
138 
1
Thanks to Charley Mann. 
138 claps
138 
1
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
"
https://blog.getambassador.io/part-2-api-access-control-and-authentication-with-kubernetes-ambassador-and-ory-oathkeeper-q-a-127fa57f6332?source=search_post---------380,"The Datawire and ORY teams have recently been discussing the challenges of API access control in a cloud native environment, the highlights of which I capture below in a Q&A. There are many possible solutions (with associated benefits and tradeoffs), and our friends at ORY have put together a tutorial on how to use their Oathkeeper with Ambassador running on Kubernetes, which I have also included below.
The web application and web service landscape is changing radically as large software companies are making their internal infrastructure and software development and operation practices open to the public. Initiatives such as the Cloud Native Computing Foundation, and open source standards and software like Istio and Kubernetes, are making a big impact on how software is developed and operated. Go, the programming language written and maintained by Google, shines with it’s toolchain. However, some of the tools behave differently than expected and it may cost you several hours of debugging and experimenting to find the arguments and execution orders.
This affects also access control — which many developer’s have a love-hate relationship with — too. In the past, we have relied on language-level APIs provided by libraries such as OmniAuth, Spring Security, and PassportJS. These libraries will always have their place in the developer’s toolbox. As applications grow and companies move away from monoliths to the Service Mesh, using these libraries isn’t quite so easy any more.
As you move to a distributed service architecture, you move away from integrating with local libraries and SDKs, and towards calling services that operate on the network. This happens naturally as you adopt more languages (e.g. you are using the best language for each use case) and start more services. This, combined with differing zones of trust across your network, obviously impact how you perform access control as well.
In order to set the scene, I recently sat down with Aeneas Rekkas, founder and CEO of ORY, and explored the concepts mentioned in more details:
Datawire: Can you explain what you mean by “Zero Trust API Access Control” please?
Aeneas Rekkas: Companies usually differentiate between an internal network (intranet) and an external network (internet). The intranet is typically within the physical premises of the company (e.g. office) and most, if not all, traffic from within that network is trusted without question. This is, for apparent reasons, a bad security practice. Advanced persistent threats (APTs) reek havoc in such environments as, once they are in the network, they comprimes everything else. Another issue is the rise of remote work and bring your own device. Both do not play well with the idea of giving internal and external traffic different privileges.
Zero Trust API Access Control defines that each API is protected, regardless of where the traffic comes from. Instead of having an file server API that can be accessed by anoyne from within the intranet, the file server should instead require valid credentials. These credentials should typically come from one source (Authorization Server) and could be API Keys, Bearer Tokens, TLS Client Certificates, or HTTP Basic Authorization. Because this system will be deployed with every service, it is a good idea to use an open source service specifically designed for this use case, such as ORY Oathkeeper.
DW: What are the advantages of using ORY over other solutions?
AR: We started ORY because developer tools are clunky and so is application security. ORY’s vision is to improve the way developers approach application security with an open source ecosystem. All services are written cloud-first and adhere to principles like 12-factor design. Besides extremely low resource consumption, every service deploys in seconds due to a ~12mb docker image size. The ORY ecosystem is being used in production by SME and F200 alike and has been acclaimed for its straight-forward design.
DW: Why did you choose to integrate with the Ambassador API Gateway? What has your experience working with Ambassador been like?
AR: Ambassador is the perfect vehicle to implement Zero Trust API Access Control on top of. With its Kubernetes-first approach it perfectly fits in the Istio Service Mesh. Because we don’t want to reinvent the wheel and write another API Reverse Proxy, integrating with Ambassador was a no-brainer. The community and maintainers from DataWire are exceptionally responsive and helpful. You should give it a try!
DW: What are the future plans for ORY?
AR: We’re just getting started. Besides working on a new open source Identity Management product called ORY Hive, we are planning a cloud environment service for running the ORY Ecosystem with zero friction and managed security. Our vision is to make cutting-edge application security as available as AWS EC2 or GCP Compute Engine!
Let’s try and make some of the concepts discussed a little more concrete, by setting up an example with Ambassador and ORY Oathkeeper on Kubernetes. Before you go ahead, you’ll need to:
Ambassador is a Kubernetes-native API Gateway built on the Envoy Proxy. Ambassador supports a wide variety of features needed in an edge proxy, e.g., rate limiting, distributed tracing, dynamic routing, metrics, and more. Ambassador also includes an authentication API where you can plug in an external authentication service. This is the API that we will be using in this post.
The first step is confirming that kubectl is set up properly:
I recommend first completing the Ambassador Getting Started tutorial on thegetambassador.io website, but I have included the core steps to set Ambassador up for this tutorial here (these steps were correct as of publication in August 2018).
To deploy Ambassador in your default namespace, first you need to check if Kubernetes has RBAC enabled:
If you see something like --authorization-mode=Node,RBAC in the output, then RBAC is enabled.
Note: If you’re using Google Kubernetes Engine with RBAC (which is the default for all new clusters), you will need to grant permissions to the account that will be setting up Ambassador. To do this, get your official GKE username, and then grant cluster-admin role privileges to that username:
If RBAC is enabled:
Without RBAC, you can use:
Ambassador is deployed as a Kubernetes service. Create the following YAML and put it in a file called ambassador-service.yaml.
Deploy this service with kubectl:
The YAML above creates a Kubernetes service for Ambassador of type LoadBalancer. All HTTP traffic will be evaluated against the routing rules you create. Note that if you're not deploying in an environment where LoadBalancer is a supported type, you'll need to change this to a different type of service, e.g., NodePort.
Create the following YAML and put it in a file called httpbin.yaml:
Then, apply it to the Kubernetes with kubectl:
When the service is deployed, Ambassador will notice the getambassador.io/configannotation on the service, and use the Mapping contained in it to configure the route. (There's no restriction on what kinds of Ambassador configuration can go into the annotation, but it's important to note that Ambassador only looks at annotations on Kubernetes services.)
In this case, the mapping creates a route that will route traffic from the /httpbin/endpoint to the public httpbin.org service. Note that you are using the host_rewrite attribute for the httpbin_mapping — this forces the HTTP Host header, and is often a good idea when mapping to external services.
To test things out, you'll need the external IP for Ambassador (it might take some time for this to be available):
Eventually, this should give you something like:
You should now be able to use curl to httpbin:
or on minikube:
When you have found your Ambassador IP, I would recommend placing this into an appropriate variable e.g.
ORY Oathkeeper is a cloud native Identity & Access Service. As such, it evaluates incoming HTTP request based on a set of rules, decides whether the request should be allowed or not, and converts the session data to a consumable format. Decisions are made by consulting two deciders: Authenticators and Authorizers.
Authenticators look for access credentials in the HTTP header — for example abearer token, and implement business logic which validate those credentials. ORY Oathkeeper currently ships with different authenticators:
For a complete list of implemented authenticators, head over to the ORY Oathkeeper developer guide..
Authorizers use the session state returned by the authenticator to authorize the request. This could be by consulting an Access Control List (ACL), Role Based Access Control (RBAC), or more advanced Access Control Policy Definitions like the one provided by ORY Keto.
Credential Issuers convert the session state returned by authenticators to an easily consumable format. The session state can be converted to a JSON Web Token signed with a private/public keypair, to HTTP Headers, and to HTTP Cookies.
ORY Oathkeeper has two operational modes. One is a reverse proxy which can be deployed as a sidecar or in close proximity to the API Gateway. The second is as anAPI which is connected to the API Gateway of your choice. For this tutorial, you will exclusively look at the API operation mode.
First you need to create a secret which will be used to sign the ID Token. The secret must be 32 characters long:
Next, deploy the ORY Oathkeeper Service and Deployment in “API mode”.
This configuration sets up the ORY Oathkeeper API with an in-memory database (please note, that restarting the service will remove all existing data!). ORY Oathkeeper can connect to other database backends such as MySQL or PostgreSQL for persistence.
This configuration additionally creates a ClusterIP service which makes it available from the Kubernetes-internal network.
But you want the service to be accessible from the outside world as well! To do that we’ll fetch the yaml definition
and open it in a text editor. The first section reads the service definition of ORY Oathkeeper:
This configuration does not include metadata for Ambassador. Let’s change that and make ORY Oathkeeper’s API available to the outside world. In a production deployment, you wouldn’t do this under normal circumstances, and instead you would expose this API only internally, or with some type of access control in place — for example Ambassador + ORY Oathkeeper!
Ok, let’s define a mapping that makes ORY Oathkeeper available through ambassador. To do so, the metadata of the service needs to be updated:
The complete file should now look like this:
Let’s re-apply the configuration:
Now you can check if the ORY Oathkeeper is alive via the Ambassador route you have created, and you can also list all access rules via the Oathkeeper CLI you downloaded earlier (for now just an empty array):
Next, you will define an access rule for accessing ORY Oathkeeper’s API. To keep things simple, you will require no authentication or authorization to access the API. Let’s echo to a new file access-rule-oathkeeper.json:
You need to make sure that the value of match.url (here http://${AMBASSADOR_IP}/ory-oathkeeper/<.*>) has the host and port where ambassador is available to you. If you set the environment variable previously, this is the case. ${AMBASSADOR_IP}would be, for example, the IP:Port you can find with minikube service list. The rule itself is very simple, it matches all requests with prefix http://${AMBASSADOR_IP}/oathkeeper-api/ and does not enforce any authentication (“anonymous” allows access by unauthorized clients), allows all requests, and does not transform the authorization header. You will set up a more sophisticated rule in the next sections.
Let’s import this rule into ORY Oathkeeper:
Now you are ready to activate the external auth service in Ambassador. To do so, you add another section to the annotations you downloaded earlier as file oathkeeper-api.yaml:
The complete file should now look like this:
And re-apply the configuration:
If you retry the command from earlier
You will notice that the request passes and you will also see the access rule you just created! Now, if you try to call the httpbin service, the request will fail with a 404 because no access rule has been configured for this service:
Let’s change that by creating a simple access rule in file access-rule-httpbin.json for the httpbin service (Don’t forget to replace the URL with your Ambassador IP and port number):
The access rule is very similar to the one you created for ORY Oathkeeper. This time however, you are using a simple authorizer that denies all requests. Let’s import the rule and see what happens when you request the httpbin service.
Ok, so authorization was not granted. Let’s update the rule and allow all requests:
Import the file again and execute curl:
It worked! There are obviously many more authentication and authorization strategies. However, you have barely touched the surface. For example, you can authenticate OAuth 2.0 Access Tokens using the OAuth 2.0 Token Introspection Authenticator. A list of all the possible handlers can be found in the ORY Oathkeeper documentation.
If you’re looking for an OAuth 2.0 Server that just works, you should check out ORY Hydra immediately. All ORY products integrate very well with one another but can also work completely standalone. The ORY team are also working on an ORY Oathkeeper Authorizer that works with the Open Policy Agent (OPA). If you find this interesting, check out the GitHub issuefor this.
In this tutorial you succesfully deployed Ambassador and ORY Oathkeeper to Kubernetes and set up different access rules that grant or deny access to the upstream httpbin service!
Keep an eye out for a follow up blogpost that will introduce ORY Hydra and ORY Keto. This will explain how to set up all four services in Kubernetes for a full-stack, cloud native access control system. Sign up to the ORY newsletter to be notified when the blogpost is released.
You can learn more about Ambassador at https://www.getambassador.io, and about the Ambassador Authentication options in the docs. If you have any questions, please join our Slack, drop us a line in the comments below, or @getambassadorio on Twitter.
Developer-First Kubernetes.
181 
2
181 claps
181 
2
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
Code, ship, and run apps for Kubernetes faster and easier than ever — powered by Ambassador’s industry-leading developer experience.
"
https://medium.com/avitotech/introducing-netramesh-lightweight-service-mesh-13f60162feb6?source=search_post---------381,"There are currently no responses for this story.
Be the first to respond.
When migrating from a monolithic app to microservices, we face new problems. Today we will discuss one of the most painful problems that microservices bring — observability — and issues arising when trying to solve it.
When dealing with a single monolithic app, usually it’s easy to understand where the problem lies — in your app or in the database. But when you have dozens, hundreds or even thousands microservices, each with a dedicated database, locating a particular service causing the problem is a daunting task.
Distributed tracing is a common solution to this problem. But what if your apps don’t send tracing spans? Or worse, sometimes they do and sometimes they don’t. To identify the root cause, you still need a full picture of the system and an understanding of which service paths your business-critical requests follow.
This is where the service mesh approach comes into play. It helps intercept all network traffic, analyze, and transform it on the fly.
Let’s briefly review this approach. The main idea is in injecting a special sidecar container into each microservice in your system and routing all traffic first to the sidecar instead of the microservice. Usually, it is a transparent proxy that proxies all traffic and does some traffic analysis. Also this is the place where we can do client load balancing and apply security policies, rate limiting.
There are several service mesh implementations available. These are Istio and linkerd2. They offer multiple features. Simply check out their websites. But this powerful set of features brings a major overhead to the infrastructure. The larger your clusters are, the larger is the overhead you get when using such systems. At Avito, we have thousands of instances of our microservices, and in the current Istio architecture (which we used as the main service mesh solution) it requires a lot of RAM in each sidecar instance (approximately 350Mb), even after all recommended optimizations. And what was worse, it’s not the only problem for us. It also brings a major latency overhead (up to 10ms on each request).
Eventually, we reviewed the key features we expect from a service mesh solution and found that the main thing we need is transparent distributed tracing of our microservices network interaction.
And this was where we applied a new solution — Netramesh.
Netramesh is a lightweight service mesh for unlimited scalability.
The main goal of the new solution is a small footprint and high performance. Also, we wanted to be able to collect distributed tracing spans to our Jaeger system.
Nowadays, the majority of cloud native technologies is implemented in Golang. And of course, there is a reason for this. It offers a convenient interface for coding asynchronous multithreaded applications. And what’s also very important, its performance is high enough for this problem. That’s why we chose Golang.
This is where we focus our efforts. We wanted a small RAM and CPU footprint for each sidecar. And of course, latency overhead should be also relatively small. This is what we have so far:
Netramesh consumes ~10Mb without traffic and 50Mb at most with a load of up to 10000 RPS to one instance.
Istio envoy proxy always consumes ~350Mb in our clusters with thousands of instances. It’s too much for us and we can’t scale it up for our cluster sizes.
With Netramesh we get ~10x decreasing of RAM usage.
CPU usage is relatively equal under load. It depends on the load and number of requests per second to the sidecar. Performance at a 3000 RPS peak value:
But there is another problem with Istio. Envoy proxy CPU utilization without load is not zero due to control plane interaction:
And sometimes, it went up to cores:
We use HTTP/1 for interaction between microservices and latency overhead with the Istio sidecar injected was 5–10ms. With Netramesh, we have an overhead of 0.5–2ms.
Small footprint gives us a possibility to inject it to each instance of each microservice. But most common service mesh solutions have an additional component — special control plane component providing service discovery, common settings, timeouts to sidecars. Usually, it provides all discovery information to each sidecar in our system (it can be one or more clusters). Eventually, we get fat sidecars and are unable to scale up to big clusters.
In the first version of Netramesh, we decided not to create the separate control plane component to support unlimited scalability. Our solution has no control plane and can be used with any orchestrator — Kubernetes or any other. But in further versions, it can appear as an option to support new features, such as security policies.
Netramesh doesn’t add any additional mechanism for service discovery. It transparently proxies all traffic through itself.
For now, Netramesh supports HTTP/1 application level protocols. And as I said, it has no control plane, so we can’t collect any additional information from our orchestrator or other components. This was the point when designing an architecture for Netramesh. Currently, it defines the application level protocol using port mapping. Usually, you have single port numbers for app level protocols. For example, we have ports 80, 8890, 8080 for HTTP protocol (microservices interaction). You can parametrize it in Netramesh using the environment variable NETRA_HTTP_PORTS.
If you use Kubernetes as an orchestrator and its service discovery mechanism to make requests from one service to another, the mechanism remains the same. First, the microservice resolves the virtual service IP address using kube-dns and then makes a request to it. But all TCP packets go first to the netra sidecar, and only then to the original destination. Pod IP translation also remains the same, NAT to pod IP takes place in the host node.
It’s not that easy to understand what is happening in your system. And we can solve this problem by using distributed tracing. Netramesh provides the functionality required to send tracing spans from HTTP interactions. It parses HTTP protocol, measures latencies, extracts information from HTTP headers. Ultimately, you can have all system traces in a single Jaeger tracing system. You can configure it using the simple environment variables that the jaeger go library provides.
But there is a problem. Unless your microservices generate and propagate special uber context tracing header, you won’t see connected tracing spans in your system. And Netramesh comes in handy again. It extracts headers from HTTP requests and generates a uber trace id header if it is missing. It also stores the context information to match inbound and outbound microservice requests. All you need to do in your microservices is propagate any request ID header customizable using the environment variable NETRA_HTTP_REQUEST_ID_HEADER_NAME(defaults to X-Request-Id). To manage the storage, you can use the following environment variables: NETRA_TRACING_CONTEXT_EXPIRATION_MILLISECONDS (tracing context mapping cache expiration) and NETRA_TRACING_CONTEXT_CLEANUP_INTERVAL (tracing context cleanup interval).
Also, it is possible to combine several tracing routes by tagging them with a session marker. Netra allows setting HTTP_HEADER_TAG_MAP to convert HTTP headers into corresponding tracing span tags. It can be useful in testing, as you can send several requests with the same session ID and then query Jaeger to show all the requests generated in the session.
Netramesh consists of two main components. The first one is netra-init, which sets up network routing rules. It uses iptables redirect rules to intercept all or some of the traffic to netra sidecar, which is the second main component of Netramesh. You can configure which ports you want to intercept for inbound and outbound connections using these variables: INBOUND_INTERCEPT_PORTS, OUTBOUND_INTERCEPT_PORTS.
Also, it has an interesting feature — probabilistic routing. If you don’t need to trace all connections (for example, in a production environment), you can use the environment variables NETRA_INBOUND_PROBABILITY and NETRA_OUTBOUND_PROBABILITY (from 0 to 1). The default value is 1 (intercepting all traffic).
After successful interception, netra sidecar accepts the new connection and uses SO_ORIGINAL_DST socket option to retrieve the original destination. Then it opens a new connection to the original destination and sets up bidirectional TCP streaming between two sides. If the port has been identified as an HTTP port, then it tries to parse it and trace. If it fails, it gracefully falls back to TCP streaming.
After collecting lots of tracing information in the Jaeger system, you want to retrieve the entire system graph. But if you have billions tracing spans daily, it’s not that easy to aggregate them promptly. The standard way to do this is spark-dependencies. But it can take hours and consumes a lot of computing and network resources.
If you use Elasticsearch to store tracing spans, then you can use a simple tool implemented in Golang that can build the entire graph within minutes: jaeger-dependencies.
You can easily inject it into any of your services. Check out an example here.
For now, we don’t have an automatic injector for Kubernetes, but we are planning to implement this.
The focus of Netramesh is achieving a small footprint and high performance while offering features supported by the service mesh approach. It will support application level protocols other than HTTP1 and support L7 balancing (based on HTTP headers).
Probably it will support Kubernetes API to be able to easily collect additional system information (mostly for security policies and balancing). But it will always remain a lightweight and easy to use solution.
Stories from Avito.ru engineering team
352 
352 claps
352 
Written by
Software Engineer at Architecture, Avito
Stories from Avito.ru engineering team
Written by
Software Engineer at Architecture, Avito
Stories from Avito.ru engineering team
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.bernd-ruecker.com/publishing-practical-process-automation-with-oreilly-db993db68411?source=search_post---------382,"It is done and I am happy to share that my new book called “Practical Process Automation” is officially published by O’Reilly.
In this book, I distilled my practical experience implementing process automation solutions from the last two decades.
This book targets software developers, software engineers, and software or system architects that want to learn about process automation.
I am happy to get super positive feedback so far, some of which I can even share with you here:
Weaving together ideas from microservices, event-driven systems, and Domain-Driven Design, this book describes when, why, and how to effectively leverage workflows within a modern software architecture. Taking a pragmatic practitioner’s approach, it uses real-world examples and explores the pros and cons of multiple alternative patterns in detail. It should be on every architect’s bookshelf.
Randy Shoup, VP Engineering and Chief Architect at eBay
Bernd has championed and delivered developer-friendly process automation tools for the best part of the last decade. Now, in this pragmatic new book, Bernd brings that wealth of experience to show how process automation models, methods, and tools can be applied to tame the complexity of microservices and cloud-native architectures which are becoming ever more prevalent in today’s technology landscape. I highly recommend this book for architects and developers who want to add process automation thinking to their design toolkit.
Richard Tarling, Digitization & Workflow Engineering Co-lead at Goldman Sachs
Process automation has often been viewed as the antithesis of modern, agile software development: snazzy “doodleware” demos that don’t scale to real use cases, poor testability, and version control only for the fortunate ones. No one better than Bernd to show us that this perception has little to do with workflow and process automation but merely with past implementations. Seeing process automation as an extension of well-established software development methods and architectures breathes a much welcome breeeze of fresh air into the field.
Gregor Hohpe, author of Enterprise Integration Patterns and The Software Architect Elevator
At this moment the book is available electronically only, the print version should be available within 2 or 3 weeks.
The book should be available on Amazon and other book stores soon.
You can find the books website here: https://ProcessAutomationBook.com
A lot of blood, sweat, and tears went into this book. It was a project that took around two years to complete, from shaping the thoughts to the discussions with O’Reilly before actually writing anything. I thought you might be interested in some really useless facts around the book.
First of all let me describe the tech stack I used for writing, based on the O’Reilly Atlas stack with Git, Asciidoc, and some magic to produce the final artifacts. I used Visual Studio Code to write. Furthermore, I used Drawboard PDF on my Microsoft Surface to re-read and edit. That setting worked like a charm for me.
Some dates:
In Git history, you can see how the content evolved over time.
The bigger tech reviews happened around August 2020, so most things afterward were refactorings and re-arrangements. As you can see in the following table, this did not mean less work — probably the contrary.
But the statistics also reveal that I was really good not to commit on the weekends!
By the way, in this graph, you can even see at what time we typically have our family dinner :-)
Some final numbers (please bear with me, some numbers might be not 100% accurate as I did relative straight forward techniques to get them):
That’s all I have for now — I hope you will enjoy reading the book — and don’t hesitate to send feedback!
Bernd Ruecker is co-founder and chief technologist of Camunda as well as author or Practical Process Automation with O’Reilly. He likes speaking about himself in the third-person. He is passionate about developer-friendly process automation technology. Connect via LinkedIn or follow him on Twitter. As always, he loves getting your feedback. Comment below or send him an email.
My personal blog.
207 
2
207 claps
207 
2
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
"
https://engineering.salesforce.com/salesforce-goes-big-with-containers-service-meshes-joins-the-cncf-b5af2376ee33?source=search_post---------383,"Salesforce is the fastest growing top five enterprise software company and the #1 CRM provider globally. And open source projects are one of the ways Salesforce has been able to build and scale our cloud infrastructure quickly and efficiently. For example, when we built a SQL interface to HBase called Phoenix, it was clear there was more value in what the community could add to it, than if we kept it in-house. So we open sourced it with Apache and the project has since grown and matured immensely. We’re seeing the same dynamic with containerization and service mesh technologies like Kubernetes, Prometheus, Linkerd, gRPC, and Istio. To help with these efforts we are joining the Cloud Native Computing Foundation as a Gold member!
It’s a good sign when developers are quick to adopt technologies, because it proves that they see how those technologies can help to create better products, faster and easier. Many of the CNCF technologies are being adopted across our teams at Salesforce. We’ve seen how containerization simplifies the orchestration of software across a large fleet of servers. Kubernetes makes a great foundation for CI/CD which then improves our software delivery. Our service mesh team latched on to gRPC and has already contributed features back into that community. This kind of collaboration, with Salesforce as an active participant in open technology ecosystems, is key to helping us move forward.
We’ve received great value from our membership in the Apache and Linux foundations. Joining the CNCF will provide similar benefits to us in the field of large-scale cloud technologies. These communities provide our engineers with support and opportunities to collaborate on projects critical to our infrastructure. Joining the CNCF better connects us to these next generation technologies that we now rely on.
I asked Randy Kern, our EVP of Infrastructure, for his thoughts:
We’re thrilled to be supporting the foundation that underlies much of our next generation infrastructure. The CNCF technologies are an exciting addition to our distributed systems architecture. We look forward to contributing to these technologies and participating in the community.
My team and I are excited to expand our use of the CNCF technologies and to contribute to their growth. We’ll share our progress with you here on our blog and on Twitter.
Salesforce Engineering Blog: Go behind the cloud with…
94 
94 claps
94 
Written by
Cloud technology exec @Salesforce — alum @hpe @rackspace @yahoo
Salesforce Engineering Blog: Go behind the cloud with Salesforce Engineers
Written by
Cloud technology exec @Salesforce — alum @hpe @rackspace @yahoo
Salesforce Engineering Blog: Go behind the cloud with Salesforce Engineers
"
https://javascript.plainenglish.io/how-to-implement-a-graphql-crud-bff-efcb0117ec71?source=search_post---------384,"The BFF pattern accelerates innovation because the team that implements the frontend also owns and implements the backend service that supports the frontend. This enables teams to be self-sufficient and unencumbered by competing demands for a shared backend service. In this article, you will create a CRUD BFF service that supports data at the beginning of its life cycle.
The single responsibility of this service is authoring data for a specific bounded context. It leverages database-first Event Sourcing to publish domain events to downstream services. The service exposes a GraphQL-based API.
Before starting this recipe, you will need an AWS Kinesis Stream.
2. Navigate to the cncb-bff-graphql-crud directory with cd cncb-bff-graphql-crud.
3. Then, review the file named serverless.yml with the following content:
4. Review the file named ./schema/thing/typedefs.js with the following content:
5. Review the file named ./schema/thing/resolvers.js with the following content:
6. Review the file named handler.js with the following content:
7. Install the dependencies with npm install.
8. Run the tests with npm test — -s $MY_STAGE.
9. Review the contents generated in the .serverless directory.
10. Deploy the stack:
11. Review the stack in the AWS Console.
12. Invoke the function with the following curl commands:
13. Perform the same mutations and queries using GraphQL by using the endpoint output during deployment:
14. Take a look at the trigger function logs:
15. Review the events collected in the data lake bucket.
16. Remove the stack once you are finished with npm run rm:lcl — -s $MY_STAGE.
GraphQL is becoming increasingly popular because of the flexibility of the resulting API and the power of client libraries, such as the Apollo Client. We implement a single graphql function to support our API and then add the necessary functionality through the schema, resolvers, models, and connectors.
The GraphQL schema is where we define our types, queries, and mutations. In this tutorial, we can query thing types by ID and by name, and save and delete. The resolvers map the GraphQL requests to the model objects that encapsulate the business logic.
The models, in turn, talk to the connectors that encapsulate the details of the database API. The models and connectors are registered with the schema in the handler function with a very simple but effective form of constructor-based dependency injection.
We don’t use dependency injection very often in cloud-native because the functions are so small and focused that it is overkill and can impede performance. With GraphQL, this simple form is very effective for facilitating testing. The Graphiql tool is very useful for exposing the self-documenting nature of APIs.
The single responsibility of this service is authoring data and publishing the events, using database-first Event Sourcing, for a specific bounded context. The code within the service follows a very repeatable coding convention of types, resolvers, models, connectors, and triggers. As such, it is very easy to reason about the correctness of the code, even as the number of business domains in the service increases.
That’s why it is reasonable to have a larger number of domains in a single authoring BFF services, so long as the domains are cohesive, part of the same bounded context, and authored by a consistent group of users.
We hope you found this tutorial helpful. If you want to learn more about Cloud Native, you can read JavaScript Cloud Native Development Cookbook. This book helps you learn the major concepts of cloud-native development faster by taking a recipe-based approach, where you can try out different solutions to understand the concepts.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
115 
115 claps
115 
Written by
Stay Relevant!
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
Written by
Stay Relevant!
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pritianka/why-im-joining-gitlab-ee05b8b49fc?source=search_post---------385,"Sign in
There are currently no responses for this story.
Be the first to respond.
Priyanka Sharma
Jul 12, 2018·5 min read
I am joining GitLab as a Director of Alliances. I’ll be focused on the cloud native ecosystem and this is a dream job for me. In this blog post I share the amalgamation of personal, cultural, and market factors that led to my decision and so it may be a bit long. Feel free to Pocket it for that train ride back home. Or, if you are a GitLabber, you can read it in the time you’ve saved by killing the commute in an all-remote workforce :-).
Please note that these are my personal opinions: I do not speak for my employers, old or new. Also, perspectives develop over time and this is my understanding of our landscape in the here and now. I hope to keep updating this blog as my thought process evolves.
Since software began eating the world, more and more companies have turned “digital”, relying on their online offerings to generate and augment revenue. Workflows are in a constant state of flux and as companies face the onslaught of increasing web traffic every year, they need to build and operate differently. This has led to the rise of DevOps where developers are responsible for maintaining the code they write. The Kubernetes ecosystem provides a comprehensive platform to board the DevOps train. But implementing the moonshot ideas and best practices preached in the proverbial Kubernetes hallways is challenging for enterprises who have only recently adjusted to a software-first reality.
I have spent the last few years diving deep into observability for distributed systems. Working on the OpenTracing project in the Cloud Native Computing Foundation and at LightStep with Ben Sigelman, I learned about the challenges companies face when they start building and shipping software with microservices. I also learned that these challenges are often brought on by the decision to utilize cloud computing over owned-and-managed data centers. Many of the conversations I had with end users began something like:
“We’ve started our migration to the cloud. As we lift and shift, we are incorporating services in our workflows and developers are becoming more independent. We’re able to ship faster and build for our customers but are struggling to manage the operational load brought on by services. We need our developers to take ownership of what they ship, and therefore need to become practitioners of DevOps.”
Even with these intentions in place, I’ve seen companies struggle to choose between the various point solutions available for every little subsection of DevOps. GitLab is solving for the misery of fragmentation in this space and that vision resonates with me. But it was not always the case. I’ve known the GitLab founding team for many years now and I have always been impressed by their commitment to open source, community-driven development, and building boring but pragmatic businesses. When Sid and I started talking about an opportunity for me at GitLab, I wondered about the feasibility of a “One Solution to Rule Them All” product, as GitLab seemed to want to be. To address my concern, Sid shared stories of customers who onboard onto GitLab for version control and very quickly want to utilize various aspects of the single application solution — from CI, to integrations like JIRA or our own portfolio management, to the integrated Kubernetes deployment feature built directly into GitLab. I learned how fast the financial services industry is adopting GitLab and trusting it to deliver a best-in-class experience for its thousands of developers. Hearing these stories, talking to end users and buyers, and also discussing with my mentors in the industry, I realized how CIOs’ needs go way beyond what a best-of-breed point solution can provide.
As I work with our customers and partners, I will form a more in-depth and nuanced perspective on this overarching trend in DevOps and plan to expound on my learnings on this blog.
GitLab as a company is having its moment and many people congratulated me for picking so fortuitously. While positive market forces are always welcome, what really excites me about this company is its people and culture. I’ve worked with many founding teams and what’s special about Sid is his resolve (and execution-to-date) to build what will be a publicly traded, enduring company, his deep understanding of his customers, and the unique culture he’s doggedly built. It’s week one and I’ve already internalized the tenets of transparency, the importance of the public handbook, and the consistency with which I can rely on my colleagues to strategize and execute with me. I have rarely met such a motivated and effective people.
GitLab is a company on a mission and I am excited to be on board. As an ENFP Myers-Briggs type, my inherent nature is that of the campaigner. I want to champion something I believe in. I’ve been fortunate in that I’ve found causes that speak to me in the past and I’ve been able to deliver value to them — LGBTQ rights at Stanford, the open source ethos at the dev tools startup I founded, observability with OpenTracing at LightStep, and now GitLab. As Director of Alliances with a focus on the cloud native ecosystem, I will build upon the great work already accomplished by the company to offer a comprehensive DevOps layer on top of Kubernetes. I believe our workflows will win for our customers and we will help usher in the true cloud native era.
If you are considering joining a company that will challenge you to the fullest, push you to achieve goals you only deemed “wild-stretch” in the past, and will still respect your priorities with home and family, GitLab is one to consider. Our mission is “everyone can contribute” and that includes women, men, parents, caregivers from all over the world. GitLabbers are a team and the company supports families. As the company handbook states,
It is great that our team feels like a close-knit group and we should encourage that, this builds a stronger team. But families and teams are different. Families come together for the relationship and do what is critical to retain it. Teams are assembled for the task and do what is required to complete it. Don’t put the relationship above the task. The best companies are supporters of families.
With that shameless plug, I will end my missive. Please do stay in touch, and reach out if I can ever be of assistance — either in my capacity as a GitLab employee, as a member of the open source, cloud native ecosystem, or as a borderline fun person to talk DevOps with — and I will be delighted to be of service.
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
GM at CNCF @cloudnativefdn | Advisor at @Heavybit | Big fan — #observability, #opensource, good writing, & dog/ dogged people | Opinions my own
See all (229)
223 
223 
223 
GM at CNCF @cloudnativefdn | Advisor at @Heavybit | Big fan — #observability, #opensource, good writing, & dog/ dogged people | Opinions my own
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ibm-cloud/kubernetes-exploring-istio-for-event-driven-architectures-e042474071d6?source=search_post---------386,"There are currently no responses for this story.
Be the first to respond.
As 2017 wanes and we start to embark on the next set of challenges around cloud native architectures, we are starting to see major cloud providers all come to the same conclusion around containerization and the role that Kubernetes plays in this ecosystem.
www.eweek.com
The community has declared Kubernetes as the orchestration technology that will allow developers to deploy seamlessly across the various cloud providers. I think we can all agree that Kubernetes is great, and pretty much any tech question can be answered by responding “ Kubernetes.”
However, if we dig a bit deeper, we will find that our next challenge will be around how to leverage Kubernetes to orchestrate the deployment of cloud native capabilities, at enterprise scale, in this container world following a “ microservices” based architecture. This article discusses how you can modify existing Kubernetes-based microservices applications to use Istio to manage communication.
First, let’s review where “microservices” came from and what it means.
The term “microservice” was discussed at a workshop of software architects near Venice in May, 2011 to describe what the participants saw as a common architectural style that many of them had been recently exploring. In May 2012, the same group decided on “microservices” as the most appropriate name. James presented some of these ideas as a case study in March 2012 at 33rd Degree in Krakow in Microservices — Java, the Unix Way as did Fred Georgeabout the same time. Adrian Cockcroft at Netflix, describing this approach as “fine grained SOA” was pioneering the style at web scale as were many of the others mentioned in this article — Joe Walnes, Dan North, Evan Botcher and Graham Tackley.
I consider myself a self-proclaimed early adopter of microservices based architectures. Around late 2014, I led a team of technologists to build out a highly scalable cloud architecture that adhered to the principles of The Twelve-Factor App. Since the cloud was still in its infancy and Docker was just starting to emerge for production usage, we based our architecture on Cloud Foundry, which has its own concept of containerization and allowed us to move to a DevOps culture that promoted a release early and release often mindset.
We documented this project in the Transitioning to a cloud-centric architecture blog post to educate teams on how we approached this endeavor. Later, the team moved the project from Cloud Foundry to containers. This process is documented in the Migrate your Cloud Foundry applications to containers on Bluemix recipe.
In the spirit of these projects, I will outline how I could use Istio to further the evolution of this application. What I outline below is definitely achievable.
Before we get deeper into the article, let’s review this application. In this article, we learned what a Twelve-Factor App is and what are the main tenets of this class of application. The application we are going to dive into was a collection of Node.js microservices consisting of a combination of HTTP and Messaging endpoints. Each one of these applications served one purpose and were all loosely coupled to enable our developers to deliver new capabilities quickly without impacting the remaining architecture. The use case for this application is IoT (Internet of Things), where the vast majority of the APIs calls being executed are the results of events from a device where the events represented the detection of mobile devices and their location (indoor and outdoor device movements). This event-driven architecture supported a fire-and-forget execution model where the client was not blocked by waiting for the response from the server. To achieve cloud scale, we leveraged a messaging backbone to distribute messages across the system to various services listening on the queue. The image below depicts the deployment architecture and the data flows through the system.
While the general architecture and approach of this application is quite solid, there is an opportunity to inject a Service Mesh technology into the topology that will really bring this application to 2018 levels. A Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the network that microservices rely upon to be a cloud native application. By leveraging a Service Mesh, developers will not be responsible for implementing complex concepts such as request routing and failover that most Service Meshes handle natively. For my scenario, I’ve selected Istio as the Service Mesh platform of choice. Here’s now Istio describes itself in its About page:
Istio is an open platform that provides a uniform way to connect, manage, and secure microservices. Istio supports managing traffic flows between microservices, enforcing access policies, and aggregating telemetry data, all without requiring changes to the microservice code.
For deep detail about what Istio is, I suggest that you read the Istio Concepts section of its documentation. To me, Service Meshes such as Istio allow me to reuse common components and routing technologies seamlessly in my application without having to explicitly change my application code and logic to enable or introduce addition components such as http proxies to my topology just to route API calls. This seamless injection will be one of the key characteristics that I will outline in the section below.
For a relatively new project, the documentation for Istio is quite good. In the following sections, I will touch upon a selection of key features and share links to additional sections of the documentation I find useful.
The initial area where I felt Istio could be leveraged in the improved architecture was around our synchronous API calls. While the majority of the app’s calls were initiated as part of IoT devices, there were instances where we supported API calls for things such as management and configuration APIs. These calls retrieved data from its persistent storage systems, such as Cloudant, Elasticsearch, and Redis, but we did not want to support blocking HTTP calls in our messaging flows. Without Istio, I would have had to integrate a solution such as Netflix OSS or write my own libraries to handle request routing, load balancing, error handling, and a variety of other functions. Instead, I can use Istio for all service-to-service communication that support the HTTP transport protocols via Pilot component. Istio’s Pilot exposes APIs for service discovery, dynamic updates to load balancing pools which will simplify my request flows to the various management and configuration APIs. By starting with Pilot, my application can incrementally add additional features of Istio that I will describe in the following sections.
Istio depends on Envoy (from Lyft) as sidecar container for request routing and metrics collection. For applications wanting to leverage Istio, the application deployment YAML needs to be updated to include the envoy sidecar before deployment which will deploy the Istio components as sidecars to your deployment co-located within the same pod. Istio supports transparent injection, which means that you don’t need to call `istioctl kube-inject` to inject the Envoy sidecar container but need to only deploy a sidecar initializer that can help inject the Envoy sidecar container automatically. This makes the microservice management with Istio simpler as there will be no change to the application code. As a result, developers can deploy applications as usual but with the added benefit of being managed by Istio Service Mesh.
Another pain point around APIs is versioning of APIs and management of said APIs. Since the microservice application is multi-tenant by design, I wanted to be able to release new function while maintaining backwards compatibility for previous APIs. By implementing version aware routing, I can roll out new APIs to some of our services while maintaining the original API for consumers, such as client side SDKs that might not be able to update their code base as iteratively as our internal services. By applying the appropriate request policies by leveraging request headers, I can route the traffic to the appropriate version supported by our client.
Istio provides a great overview of request routing rules in one of their traffic management documentation.
As the team deployed new versions of the code, we typically had a high level of confidence in what we were delivering was well tested from both a performance and regression perspective. However, I want a simple way to test out new function in limited roll outs in a manner to not impact our larger customer base. Using Canary Deployments, I can incrementally roll out new services and using tools, such as New Relic, monitor the current performance against historical benchmarks ensuring we maintain the appropriate SLAs for our services.
Istio provides a great overview of Canary Deployments in one of their featured blogs.
When developing our microservices, we spent a good amount of time building out libraries that enabled us to get visibility across our entire system. We instrumented our code using a combination of Node’s Express Library, New Relic agents, and generating unique request IDs for each inbound request to correlate data across the system. Istio provides support for both Jaeger and Zipkin, which allows me to correlate our unique metrics with the logging data generated by Istio and Kubernetes to provide a holistic view of the requests to debug problems. With distributed tracing, I could also easily locate the performance issues in the call chain.
Istio provides a great overview of Distributed Tracing using their BookInfo application which highlights some of the tracing capabilities built into Istio.
In the preceding sections, I talked about version aware routing, canary deployments, and tracing of the overall system. For the overall operation of a healthy deployment, DevOps practitioners will be adamant about gaining insights into the live running system. Istio provides integrations with both Grafana and Prometheus with predefined views of key metrics mostly commonly used by operations teams. In the image below, the metrics range from response time to error counts to drill downs per service. These views are even customizable to add in your own metrics. If I combine these capabilities with what we already have with New Relic, I can quickly gain insights into the health and performance of the entire ecosystem.
Istio provides a great overview of Visualizing Metrics using their BookInfo application which highlights some of the Grafana views built using metrics captured by Istio.
Besides distributed tracing, I can also use Istio Service Graph to generate a web-based interface for viewing service graph of the service mesh, which can enable the end user to view the topology of the service mesh. It is a very important feature if you have large amounts of micro services in your cluster.
The service graph depends upon Metrics and Logs and building on top of Prometheus queries.
Enterprise Security requirements block many deployments and one of the most common security escapes is the lack of secured transports for data in motion. Istio solves this concern by providing support for mutual TLS out of the box between Istio endpoints by deploying sidecars along with their applications. When mutual TLS is enabled and the Istio sidecar is deployed along with our application, all communication between the two microservices endpoints is secured across pods as the sidecar intercepts all inbound and outbound traffic for Kubernetes pods that are leveraging Istio. This is an absolute life saver for developers not wanting to mess with SSL certificates and management the lifecycle of those certificates.
You can not only use Transport Layer Security for app to app communication, but you can also extend it to services such as Etcd, as described in the Medium article Istio is not just for microservices.
While Istio provides excellent support in other areas, there are components of the architecture I am not be able to move to Istio: messaging and the usage of AMQP for publishing and subscribing to topics. We leverage many core capabilities of the AMQP protocol such as wildcard pattern matching, exchange types such as fanout, as well as concepts such as store and forward in case endpoints are not available. Istio has plans to support additional protocols such as AMQP in the future but for now, the idea of moving from a message based approach to an HTTP centric approach with Istio seems a bit premature.
2017 is considered the year of Kubernetes by many. There is a lot of momentum around Service Mesh as the technology for 2018 that will continue to drive Kubernetes adoption with enterprise developers. The community is actively engaged around many Service Mesh projects such as Istio, Envoy, Linkerd, and the newest entry into the space, Conduit. There will be some time before the market determines how these fit in the overall ecosystem, but based on the convergence of the various cloud providers around Kubernetes, I expect to see the community drive one or two as the ultimate winners. Based on my examination of how Istio can improve an existing 12-factor app, I think it’s a strong contender in this race. I’m looking forward to seeing how 2018 evolves and what challenges lie ahead.
I wanted to take a moment and thank Guangya Liu and Kathryn Alexander for their contributions to this article. Guanya provided excellent content especially around concepts such as Transparent Injection and Service Graph. Guangya is one of my team members and has been driving a lot of the Istio initiatives inside of IBM. Kathryn leads our Content Development team and works tirelessly to cobble our technical jargon into human readable prose. I could not have delivered this article without their guidance.
Originally published at https://www.linkedin.com on January 3, 2018.
Understand how to bring elastic runtimes to the Enterprise…
52 
1
52 claps
52 
1
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
Written by
NCR Executive Director and Chief Architect for Retail Solutions. The opinions expressed here are my own. Follow me on Twitter @todkap
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
"
https://medium.com/google-cloud/kubernetes-routing-internal-services-through-fqdn-d98db92b79d3?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
I remember when I was first getting into Kubernetes. Everything was new and shiny and about scale. As I continued making Cloud Native Applications running on Kubernetes I found some small paragraph that stated that Kubernetes has a built in DNS server.
Of course it does, that makes so much sense.
But now with a built in DNS server this opens up so many opportunities. Routing and masking routes in new and complex ways while still within our cluster.
In this article we are going to look at how you can make custom routes within your cluster to simplify your inter cluster communication.
If you haven’t gone through or even read the first part of this series you might be lost, have questions where the code is, or what was done previously. Remember this assumes you’re using GCP and GKE. I will always provide the code and how to test the code is working as intended.
medium.com
When calling external services you may be used to writing fully qualified domain names (FQDN), like the following.
However when you are making requests within your cluster, how would you expect it to work? The pods are ephemeral, so the URL will change as often as the pod is created and destroyed. Not a solution.
You could use the external URL that is available from the load balancer exposed by the service. But then you’d be making an extra hop and wasting time and processing.
If we were wanting to communicate with our other services without making an unnecessary hop then you just need to use the internal communication scheme built into Kubernetes. Looking at part of the service yaml file we can pull out the values for the FQDN.
For this service, due to the service name, the FQDN would be:
The other parts?
This is the namespace of the pods that we are targeting. Since I didn’t set a namespace the namespace is default.
You can also shorten the FQDN by removing the svc.cluster.local. Leaving you with:
As you will see from the example provided below, it is really simple to insert parameterized routing into your application. This is extremely helpful with Kubernetes as you might want to have slightly different routing based on environments or other rules.
I’ve created an example project to highlight this feature. For this example I used pod environment variables and a single application to inject the necessary variables into the application so we can see how one service can call another.
And in the application code the values are injected to customize the code.
In this code I am using service-1 to call service-2 through the /foreign endpoint. I also set up the reverse so that service-2 can call service-1. You can run the code by running the following command in Cloud Shell.
This will produce an IP Address for service-1 when it is ready. If you hit the /foreign end point you will see the following result.
http://[service-1 IP Address]/foreign
You’ll see that service-1 calls service-2 directly just as easy as hitting any other endpoint. This wonderful magic makes it just a little easier when building your microservices.
Before you leave make sure to cleanup your project so you aren’t charged for the VMs that you’re using to run your cluster. Return to the Cloud Shell and run the teardown script to cleanup your project. This will delete your cluster and the containers that we’ve built.
medium.com
medium.com
itnext.io
medium.com
Jonathan Campos is an avid developer and fan of learning new things. I believe that we should always keep learning and growing and failing. I am always a supporter of the development community and always willing to help. So if you have questions or comments on this story please ad them below. Connect with me on LinkedIn or Twitter and mention this story.
Google Cloud community articles and blogs
90 
3
90 claps
90 
3
Written by
Excited developer and lover of pizza. CTO at Alto. Google Developer Expert.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Excited developer and lover of pizza. CTO at Alto. Google Developer Expert.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/built-to-adapt/a-unifying-foundation-for-the-customer-journey-at-mercedes-benz-fc8f8877816b?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
Providing an enjoyable customer experience is important for any company. But it’s especially important for luxury car makers. Customers expect the customer journey to be as smooth and pleasant as a drive in the car itself.
That’s why Mercedes-Benz decided to act. The German luxury auto-maker’s online presence was disjointed. Customers navigated through three disconnected web properties: a product focused website, an online customer portal, and a brand and lifestyle-focused site. All three sites were supported by different legacy platforms and couldn’t share user or customer data. So if a customer visited the product site, then jumped to the customer portal, there was no continuity of experience.
“The behavior of the customer was not clear to each of those sites because they were separate,” said Thomas Seibert, lead architect at Mercedes-Benz.io, speaking at SpringOne Platform. “It was a clearly broken customer journey and on top of that we didn’t gain any coherent knowledge of the customer or their intentions.”
But that was then. Today, Mercedes-Benz deploys new software several times a week. Its customer journey is supported by a single, cloud-native platform. And the company has plans to roll out dozens of new services and applications to customers around the world. This is how it got there.
Think about the last time you made a major purchase. Was your first and only step walking into a storefront or logging on to a website to make the purchase? Almost definitely not. Today, multiple steps are usually needed to satisfy a customer need according to Harald Fanderl and Nicolas Maechler, both partners at McKinsey & Company.
Maechler, on a recent McKinsey podcast, described the typical customer experience in the auto industry:
“If you think about buying a car, in most countries today, you have car configurators online. So you go there, you have an ID, you look at one brand, and you configure your car. Then maybe a few weeks later, you pay a visit to the dealer, and you test drive that car. Maybe you hesitate again, and then you have a financing discussion later on.”
Eventually, if the customer is satisfied with each step in the process, he or she may buy a car. “But if you don’t connect these elements,” Maechler said, “you don’t have the view of one single journey, which is, ‘I want a new car.’”
That’s a problem because if one step in the journey doesn’t inform the next one and the one after that, frustration can quickly set in. For example, if a customer uses an online configuration tool to build the car of their dreams, and later has an online chat with a sales associate who knows nothing about it, the customer may feel like they have wasted their time. The more experiences like this that occur over the course of the customer journey, the less satisfied the customer will be. And the less likely they will make a purchase.
“It’s all about putting customer needs at the center of what a company needs to do, and then ensuring along all the touchpoints and even more so along all the relevant customer journeys that the customers have a flawless experience,” said Fanderl.
The challenge for large enterprises is that most are organized in silos, both organizationally and technologically. That was the case at Mercedes-Benz. The team responsible for its customer portal, mercedes.me, was distinct from the group that managed the company’s product-focused website, which itself was disconnected from the team that handled the lifestyle and brand digital property. And each of the three ran on different foundations — JavaEE and two different content management systems, respectively — that were disconnected from one another and made it difficult to change the applications.
But an effective customer journey requires cross-functional collaboration. CMOs understand this, and must implement practices that encourage collaboration across organizational and technology silos that provide a cohesive foundation for the customer journey. From an organizational perspective, this often means taking a balanced team approach to development, in which members of the business join their IT and developer colleagues on small teams dedicated to specific functional requirements along the customer journey. In this case, teams take a product, versus project mindset and are responsible for that functional requirement from beginning to end — developing it, deploying it, running it in production, and improving it over time.
From a technology perspective, the balanced team approach coupled with a product mindset demands an application platform that breaks down the traditional development silos in order to be effective. A balanced team, in other words, can’t support the full software life cycle if the platform on which the software is developed doesn’t also support running it at scale and iterating in small, high-velocity batches.
Working with colleagues in the business, Seibert and his colleagues helped Mercedes-Benz adopted the balanced team approach. Rather than the business only being involved at the start of a software project to spell out requirements, line-of-business people joined Seibert’s developers, designers, and project managers on small, balanced teams. Each team would focus on a single functional requirement, or microservice, that together formed the larger customer journey.
Seibert and team also knew they needed a single, cloud-native platform to support and connect its various digital channels and applications. But the platform had to meet a number of rigorous requirements:
It had to integrate with legacy infrastructure and tools. This included content management systems, analytics tools, and applications and data running in Daimler’s data center (Mercedes-Benz is part of Daimler AG.)It had to easily scale out and back again to adjust to vacillating traffic and workload requirements.
The platform needed to support a microservices architecture. This would enable Mercedes-Benz to reorganize into small, independent development teams, each working on a distinct piece of functionality. This way, each team can develop and release software at its own pace.
Any platform had to abstract away as much of the underlying infrastructure and operations tasks as possible, freeing up development teams to be more productive and focus on what they do best — build great software.Relatedly, it needed to provide developers self-service tools and automate delivery pipelines to reduce the time from feature inception to production deployment, so teams could quickly deploy new features, analyze how they are used, and iterate to improve them.
Ultimately, Seibert and team chose Pivotal Cloud Foundry to support their customer journey initiative. The cloud-native platform allows IT teams to quickly build loosely-coupled, resilient applications connected via a high-performance routing tier. Through software design approaches like the strangler application pattern, developers can also break down existing monolithic applications into microservices, while still connecting to legacy data sources and related systems.
From a self-service perspective, the Pivotal Services Marketplace offers developers an easy way to provision supporting services — including data management services, messaging and integration services, and networking services — and with just a few clicks bind them to their application instances. There are also monitoring, metrics, and logging tools in the marketplace, which developers use to better understand how their applications are being used and iterate to improve them based on the data.
Since deploying Pivotal Cloud Foundry, development teams at Mercedes-Benz have been working diligently to simplify and improve its customer journey. Its previously disjointed web properties now all run on the platform, providing a common foundation for development and data sharing. And it’s development teams are now organized around distinct domains so they can focus on customer outcomes.
As a result of the platform and organizational changes Seibert and his colleagues implemented, Mercedes-Benz development teams today are building and releasing microservices applications faster than ever. Because development teams don’t have to provision hardware or virtual machines with Pivotal Cloud Foundry, they can quickly build and experiment with new functionality. And with small, independent teams working on distinct domains with few interdependencies, each team can deploy software at their own pace and without having to wait for infrequent, coordinated release windows.
“We … learned that a PaaS solution can tremendously help you in increasing the efficiency of your team because it automates specific tasks like the creation of PCF routes when you deploy applications. You don’t have to worry about this,” said Gregor Zurowski, an independent consultant that worked with Mercedes-Benz on the implementation. “You have service bindings which implement the concept of attached resources like described in the 12-Factor Apps, where you don’t have to worry about managing credentials by the development team. And it’s also extremely easy to scale in and out.”
Importantly, usage data and customer feedback informs feature development, which, as mentioned, are built and rolled out in small, frequent batches. This, as much as anything, puts the customer journey at the center of attention.
As part of the initiative, for example, Mercedes-Benz determined customers would benefit from local versions of the company’s web properties and applications. In the past, it would have taken months just to get an initiative like this off the ground, let alone deliver to production. Not so today.
“Provisioning of a centrally managed platform drastically improves the velocity of the teams and also improves the stability of the components.”
“We have rolled out to 20 countries in less than five months,” Seibert said. “This means on average we have rolled out four countries a month. Sometimes we even managed to roll out three countries in one week.”
Seibert credits the pace of delivery on not just an improved developer productivity, but also on the platform itself.
“Provisioning of a centrally managed platform drastically improves the velocity of the teams and also improves the stability of the components,” Seibert said. “This has all been done without any critical production incidents. And we had zero management escalations. This is why I’m confident that in the next year, 2018, we will be able to achieve more than 40 countries worldwide with our platform.”
While there’s still plenty of work to be done, Mercedes-Benz and its customer journey has come a long way in a relatively short period of time. And with Pivotal Cloud Foundry and a modern approach to software development, the road ahead for both the company and its customers is an exciting one.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
115 
115 claps
115 
How software is changing the way society and businesses are built.
Written by
Senior Director of Product Marketing at Privacera.
How software is changing the way society and businesses are built.
"
https://medium.com/@kevinhoffman/introducing-waxosuit-6ad754b48ed9?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Hoffman
Jun 27, 2019·7 min read
I’ve been developing back-end systems for a long time. Distributed systems; complicated, resilient back-end applications; and microservices has been my specialty and passion for as long as I can remember. I wrote a book on Cloud Native Go, a book on building Microservices with ASP.NET Core, and dozens of other books on various technologies. But more important than the books is that I’ve been building this stuff and shipping to production for decades, so I’ve made countless mistakes and learned just as many lessons from them.
Frankly, I’m sick and tired of typing. I’m not sick of coding — just typing. This isn’t a complaint about my carpal tunnel (though that is uncomfortable), it’s a complaint about the notion that no matter what I build, when I build it, or how I build it, the vast majority of what I type is redundant, superfluous, or an elaborate copy/paste dance designed to dazzle and distract me from the reality that I am simply lifting, re-purposing, and dragging boilerplate with me wherever I go. That baggage weighs me down, and it weighs down development teams everywhere.
Starting well before I began work on Programming WebAssembly with Rust, I knew WebAssembly had a lot of potential. I was branded a pariah, however, because I felt from the beginning that WebAssembly could provide the most benefit in the cloud — even more so than making browser apps faster and more powerful.
One of the ideas that has been stuck in the back of my head these past two years is that not only can we run WebAssembly in the cloud, but that we can take advantage of its sandbox and so-called limitations to reduce the excessive boilerplate developers drag with them in service of a never-ending list of NFRs (Non-Functional Requirements). Since the first time I really understood how WebAssembly worked, I’ve been convinced that we could harness it to flip the service development effort ratio of logic to ceremony so that developers could return to spending 90% of their time writing pure business logic and only spend 10% of their time on the typical ceremonies required by every new project.
As I continued writing my WebAssembly book and building tons of microservices and functions in Go (and other languages), I was inspired by the de-centralized, JWT-based security model used by NATS 2.0. I knew then that I could take the power of WebAssembly, the security of ed25519 signatures and JWT tokens, the speed and power of a Rust-based Wasm interpreter, and build something that would solve my problems and hopefully the problems of many enterprise developers working in the cloud.
All of these ideas ultimately became the OSS project Waxosuit. I wanted to build an exoskeleton — or exosuit — that would provide secure capabilities to WebAssembly modules. I didn’t want my development teams to have to re-write, copy/paste, or cargo cult stale implementations of NFRs for things like logging, tracing, contextual tracing (e.g. OpenTracing/Jaeger), Application Performance Monitoring (APM), health checking (e.g. live/readiness probes), message broker client wrappers, key-value store client wrappers, HTTP server endpoint wrappers, and so on.
If I had a dime for every time I started a new microservice project that involved copy/pasting (and subsequently updating) a bunch of old wrapper code to deal with the items listed above (especially things like HTTP endpoints and RESTful routing!), I could afford not to care about these problems.
I want developers to be able to rapidly create a new, empty project and then immediately write code that states, “when this happens, execute this business logic, utilizing these loosely coupled, abstract capabilities.” I don’t want to care about whether this service will properly scale to zero (or how it will do so), I don’t want to write code that tightly couples me to a specific cloud provider’s serverless framework, and I’m literally exhausted from spending so much time on the NFRs and barely getting enough time to write and easily test my business logic. Finally, I want to be able to run and test that code anywhere, deterministically.
The radical proposition I put forward is that for the majority of people writing code in the enterprise for the cloud, they might embrace the constraints of operating within a WebAssembly module where capabilities are provided by the host in exchange for better (by default!) security, faster execution time, better container density (and thus higher cost savings), easier deployment, easier testing, and a simpler overall developer experience.
So what does it look like to build a WebAssembly module that runs inside Waxosuit? You can create a new Rust project (a Go guest SDK will be coming soon) with a cargo generate template, and inside your lib.rs file, you’ll define a call handler. This pattern of a single call handler/entry point should be familiar to serverless and FaaS developers.
You’re free from the burden of figuring out whether you’re standing up your own HTTP server endpoint, managing ports, managing client connections, or the myriad other details that usually add 6 months of development time after your “hello world” prototype worked. Just declare what types of messages your module handles, provide a handler, and you’re done.
Take a look at a sample that declares a handler for a service that exposes the aggregate details for an Internet of Things sensor:
The most important piece of this code is the CapabilitiesContext (I haven’t yet written the Rust docs for this struct). This is a struct that provides the means through which your code interacts with the host runtime. If you want to make a key-value store request, you grab the KeyValueStore trait by accessing ctx.kv(). If you want to publish a message, just access the MessageBroker trait with the ctx.msg() function. If you want to access a private, enterprise-internal capability that your company has written, you can either provide your own strongly-typed wrapper around the guest SDK, or just access the Raw capability via ctx.raw() and send your own protocol buffer messages.
Each capability (Key-Value, HTTP server, HTTP client, Message broker, and so on) is dynamically loaded as a plug-in by the Waxosuit host runtime. This means that the host runtime process is only ever as big as you need it, and only ever loads the capabilities that have been granted to the WebAssembly module via the embedded, signed JWT.
Put another way, Waxosuit is a secure intermediary between your WebAssembly modules and capability plugins, which can be any combination of public OSS or private. An added benefit is developers don’t have to worry about threading — since Waxosuit handles all the dispatching and multiplexing in the host process, everything executed inside the Wasm module is single-threaded.
Lets say your company has some standards around accessing an internal capability. Rather than maintaining a client library in 30 different languages so all your enterprise developers will have access to that proprietary capability (a practice I have seen countless times in my career), you can create a single capability plugin, and expose that securely to WebAssembly modules, regardless of what language the developers used to build them.
With that one Waxosuit feature, you can now entitle individual workloads to use a specific enterprise capability with cryptographically secure verification. You don’t have to worry about which language the developers are using, since everything they build produces an immutable, signed WebAssembly file. This also means your capability team no longer needs to maintain 30 language-specific SDKs (which could have subtle, yet bug-inducing differences), they can just maintain a single Waxosuit plugin.
Waxosuit integrates with Open Policy Agent so that, after verifying the internal integrity of the JWT, it can optionally defer a “can this workload execute?” decision to OPA by passing it the module’s signed JWT.
Waxosuit isn’t going to be for everyone. It isn’t a panacea, nor does it solve every problem of enterprise cloud development. However, my hope is that it will resonate with developers who have run into the same problems I have throughout my career and they will start using it.
I only just recently published everything to GitHub, and the project is still in its infancy. The code is sparsely documented and tested and there are some missing features. I have a ton of work left to do on this, but in the spirit of open source, I wanted to get as many eyes on this as soon as I could. I would love feedback and for people to start exploring potential use cases for it and submitting PRs for where it falls short of its ambitions.
I will be rapidly filling in those gaps over the coming weeks and months, building out the features, docs, and samples. I will also be working on the enterprise registry server for Waxosuit-compliant modules (I’m calling it Gantry, and it will be built entirely out of Waxosuit modules) and would love to work with as many of you on this as possible!
Until then, stay tuned to the Waxosuit twitter for updates.
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
214 
2
214 
214 
2
In relentless pursuit of elegant simplicity. Tinkerer, writer of tech, fantasy, and sci-fi. Converting napkin drawings into code for @CapitalOne
"
https://blog.searce.com/design-a-highly-available-postgresql-cluster-with-patroni-in-gcp-part-1-dfb6ed277806?source=search_post---------390,"Sign in
Bhuvanesh
Sep 16, 2019·5 min read
Patroni is the new age HA solution for PostgreSQL with cloud-native features and advanced options for failover and failback. PostgreSQL is one of the world top open-source databases but the worst part is, it doesn’t have inbuilt automatic failover. Still, I remember my old days, fighting with repmgr for HA and its very hard for me to add the failed master back to the cluster. But later pg_rewind made that process more simple. The world is moving very fast to adopt the cloud. So some vintage HA solutions like DRBD, corosync + pacemaker, repmgr, and a few other technologies are out of date.
I learned this concept from a conference by Zalando. PostgreSQL will take care of its own process, but we need something to monitor the PostgreSQL service and its replication status in distributed systems like etcd, zookeeper or Consul. But PostgreSQL can’t to DCS directly right? So if PostgreSQL went down, then the bot will start electing a new master. Also if the old master came up, then the bot will add them back to the cluster. Here BOT refers to the Patroni. Patroni is the successor of compose governor.
ETCD is a distributed control system. We’ll use ETCD to keep the PostgreSQL cluster’s health, node state, and other information about the cluster. The other important thing is, etcd also will be in a High availability mode. So either you can use it on GKE clusters or create a cluster on Compute engine and put a load balancer on top of it. But unfortunately, the GCP’s HTTP load balancer will support port 80 as a Front end port. But no worries, it won’t affect anything.
To use Patroni, we need at least 3 nodes. In case of failover, we need some Virtual(or floating) IP address to make the application continue to access the database. To reduce this dependency, the HA proxy will always talk to the master node. Here is how HA proxy knows who is the master. In HA proxy, we need to give health check port. 8008 is the default Patroni rest API port. HA proxy will send a GET Request to all the nodes in the cluster in 8008 port. Only the Master node will give 200 OK status. Other nodes will return 503 Server Unavailable.
But as I mentioned above, its a Cloud Era. All the cloud providers have their own load balancers which work better than HA proxy(in terms of scalability, customization, availability and etc). Patroni provides feasibility via Rest API to determine the role of a node. On each node, we can trigger a Get Request to /master and /replica URLs. If the node is master, then the /master will return 200 OK. On the replica’s it’ll return 503. Similarly /replica will return 503 on replicas and 200 OK on slave nodes.
This feature will help us to deploy the GCP TCP load balancer to talk to master and slaves. We can create 2 load balancers.
Lets say if you have 3 node cluster, then the Writer LB shows 1/3 nodes are healthy meantime Reader LB shows 2/3 nodes are healthy.
Patroni is good for HA, but if you have some DR servers on a different Region or Reporting Replica with minimal hardware configuration, Automatic failover should not promote these nodes as a Master. Or if you have any delayed replica, that node also won’t become a master. In this case, we can define which nodes should be eligible for master.
In GCP, I have designed the below architecture for Patroni.
Part 2: Implementing this Solution
This is just an overview of the architecture and some fundamentals about the Patroni. We have written a step by step guide for implementing this solution on GCP on your part blog. Sometimes it's better to have a self-managed cluster over the CloudSQL one. Hope you found this helpful, please give some claps for our solution.
Less Talk, More Data | https://thedataguy.in
See all (503)
35 
35 claps
35 
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/cloud-native-development-a-diagrammatic-approach-to-the-twelve-factor-methodology-b13f9b4a129b?source=search_post---------391,"There are currently no responses for this story.
Be the first to respond.
We have been building cloud-native apps for a while and there are some common set of principles for the cloud-native development which is developed and introduced by one of the early adaptors Heroku. Understanding of these 12-factor apps is very important In fact, these are the starting point for your cloud-native…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/planet-stories/towards-on-demand-analysis-ready-data-f94d6eb226fc?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
Yesterday we explored the basics of “Analysis Ready Data,” (ARD) which consists of time-series stacks of earth observation imagery that enable a user to jump into the analysis without having to do their own pre-processing. Satellite imagery providers have started to produce ARD for particular sensors, and are figuring out how to work toward ARD consisting of data from any sensor.
The combination of Analysis Ready Data, a coherent, cross-calibrated time stack of data from any overhead source, with the capabilities of modern Cloud Native Geoprocessing, leads to what I call “On-Demand Analysis Ready Data.” This is the final step to enable truly user-centric satellite imagery workflows. In this framework, a user can define the exact area they care about, and use cloud-scale geoprocessing engines to generate data optimized for their analysis.
In practice, this means that if the user cares about Contra Costa County in California or Yosemite National Park, they receive images clipped to desired boundaries — not a set of tiles that overlap it. It also means exposing a palette of operations they can choose from. While it is essential that there is a baseline ARD definition that creates actionable data across use cases, there are cases when a custom ARD definition makes more sense.
With On-Demand ARD through Cloud Native Geoprocessing APIs, an analyst could use the baseline definition and tweak the parameters to be exactly what they want. Indeed, one could see domain-specific ARDs: 3D building modeling and vegetation health have different requirements for “best possible” parameters and inputs for data preparation. With On-Demand ARD, these can be sets of operations performed when the user requests.
It’s worth another look at how the various aspects of ARD may look different with On-Demand ARDs.
Image Clipping. The biggest immediate impact to users is that the ARD they request on demand can be shaped exactly to their area of interest. They would supply a vector file (or map drawing) of the place they want to analyze. This would kick off a series of processes to produce the ARD, with no wasted compute, storage, or network egress resources. Right now we process and send a lot of pixels that users already know they don’t want. On-Demand ARD could eliminate most of that, saving storage costs for users and ensuring every pixel delivered to them is actually relevant. Regular grids would likely be used by many users, but they could choose to have clips to their area within the grid.
The grids would also be created on the fly, for example, making image chips that are optimized for computer vision. A user accessing an On-Demand ARD API could clip to the area they want, then choose to return full images, use a default grid, or specify their own grid. Or, they could potentially receive composite images for full-pixel coverage in each image (see the section below on compositing).
Usable Data Masks. One annoyance many users have with statistics like ‘cloud cover’ is that they are made against a whole scene or grid cell, which rarely corresponds to the area they actually care about. Usable data is often discarded by users because cloud cover in an another area will increase the cloud estimate, even when the places they care about are actually totally clear. Generating the UDM on demand for the area requested means that the summary statistics are finally relevant to the analysis at hand. Users could discard the images without having to inspect and do their own analysis of whether their AOI has usable pixels.
Going further, users could specify which UDM classes they consider usable or not. One user may have an algorithm that works with low haze, while another may need completely clear images. They could be returned a simpler mask that tells them what pixels are or are not usable, derived from their preferences. The geoprocessing engine could even perform the mask operation, returning the stack of pixels they consider usable. Some people could even choose to perform an additional correction, for example, to turn a cloud shadow pixel into one they can use.
Image Compositing. A new category that becomes possible and indeed desirable if you have a geoprocessing engine is compositing images together for the user. Ideally, time series analyses are run on complete sets of pixels at regular intervals. Most ARD gives the user the capture, but they may have to combine and pick the best pixels from each one on their own.
Instead, On-Demand ARD could return a weekly or monthly image, selecting the pixels according to user criteria, leveraging the UDM and other quality analysis to return wall-to-wall pixels. There is even a trend towards ‘virtual composites,’ where one can use a lower resolution pixel (such as from MODIS) to model and estimate a prediction of what a higher resolution pixel would look like. The geoprocessing system could produce weekly Analytic Ready Composites at a given resolution, with metadata on whether they are new captures or if they were composited from lower-resolution assets.
Atmospheric Correction. While most people will be happy just having their data fully atmospherically corrected, there are always some who won’t be happy with the way the correction was done. There are lots of different steps, and not every analysis needs all of them. Some work is totally fine without the final steps, and some users may have their own atmospheric corrections that are optimized for their application. Even these advanced users, however, can benefit from On-Demand ARD if they can put their algorithms in a Cloud Native Geoprocessing engine of choice. A great API would bundle up a set of operations to output ‘surface reflectance data’ from the selected time and area of interest. It would also let users select and parameterize each component and specify the exact output they want.
Pixel Alignment. Creating ARD on-demand opens up new possibilities for better aligning pixels. For one, users could specify what they want to use for a base image to co-register all the additional images too. Maybe they have particular knowledge of why one image works better than others, and so an API could let them choose. Beyond that, a great geoprocessing system could do a sort of ‘bundle adjustment’ of the stack of images selected, to really optimize the whole stack alignment, instead of just having each attempt to match the base. The most flexible geoprocessing systems would let users specify their own datasets for rectification. They might have high-quality LiDAR data, or some super accurate ground control points that will perform better. Ideally, they could supply their own input data to use for the whole ARD stack.
Sensor Alignment. Similar to Pixel Alignment, users of ARD on demand should be able to align sensors to the base they prefer. If they’ve got an algorithm working well with Landsat data, then it should be possible to have an ARD with DigitalGlobe and Airbus and have it act like the Landsat data. It should be possible to go the other way as well: align a Landsat scene to act more like DigitalGlobe data. More advanced users should also have a number of parameters to tweak exactly how the sensor alignment is done.
Astute readers will notice that I didn’t talk about data formats or metadata standards at all. It is important to emphasize that Analysis Ready Data is independent of data formats. It is a coherent set of data preparation. I’m sure many who read my work have made the leap to the potential of Cloud Optimized GeoTIFFs (COG) and SpatioTemporal Asset Catalogs (STAC) to help, but I’m going to keep most of that discussion in its own post. The main thing to remember is that ARD is format independent. And indeed the question of format also drops away some with On-Demand ARD, as the format is just an additional parameter the user sets when ordering their data.
However, it is important that we start to think about the cloud-based storage of ARD, so that it can be close to the computation and live natively on the cloud. With On-Demand ARD, it is actually less important to have big complete datasets of ARD, but it is very important that the information needed for a cloud native geoprocessing engine to create ARD on demand is in place. As the ecosystem matures, it is quite likely that ARD will be taken for granted, because of course users want well prepared data for any analysis task. On-demand systems can help fill that gap by generating the data as needed by users, instead of duplicating the core data that runs massive processing on every pixel to get it ready.
It is important to remember that no single sensor will ever have all the information and answers to aid a decision. As a result, it is essential that the overall ecosystem architecture accepts that all data will not always live in one location, or that it will always go through one geoprocessing engine. We must continue to push forward a baseline of STAC catalogs and accessible COG data that can be streamed between different locations and systems.
I hope this post and the previous one has been helpful to form a clearer picture of what Analysis Ready Data is today, and where it could evolve. Cloud Native Geoprocessing systems can play a huge role in making overhead imagery far more useful to a broader audience. While advanced users should be able to tweak the parameters to their liking, it is even more important that the industry provide guidelines that tap into our expertise so that others aren’t required to become experts.
If you work in the earth observation industry and are interested in cross-provider collaboration on these topics, please get in touch (@opencholmes). At Planet, we want to help move the industry forward towards truly cross-platform and on demand Analysis Ready Data, but we can’t do it alone.
Using space imagery to tell stories about our changing…
188 
188 claps
188 
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/istio-as-an-example-of-when-not-to-do-microservices-50f2619a9e55?source=search_post---------393,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I’ve been pretty invested in helping organizations with their cloud-native journeys for the last five years. Modernizing and improving a team (and eventually an organization’s) velocity to deliver software-based technology is heavily influenced by it’s people, process and eventual technology decisions. A microservices approach may be appropriate when the culmination of an application’s architecture has become a bottleneck (as a result of the various people/process/tech factors) for making changes and “going faster”, but it’s not the only approach.
Microservices is not THE “utopian application architecture”.
I’ve written in the past how I didn’t think many teams would be able to pull it off, how there are “hard parts” to getting it working, and even heads up about some technology that might be beneficial to your efforts in the long run. FWIW I even wrote a book on the topic.
Staying away from microservices is likely the best place to start, though many organizations are way passed that today.
If you do go down the path of microservices, be honest with yourself and the organization when it’s not working. Correcting course may be the right move for the success of your product.
Despite best intentions, it may be the right choice to go back to a monolith once you’ve started with microservices, even if it was for the right reasons. “It’s okay” to go back to a monolith if your assumptions or the context around your decisions have changed.
In the Istio community, which builds a service mesh for microservices communication, the implementation of the control plane will be gradually changing from a microservices approach to a more monolithic approach. Louis Ryan, who’s the Principal Engineer and architect on Google’s API infrastructure, gave a talk back at the Istio meetup at KubeConNA 2019 detailing the motivations as well as outlining the case in a design doc. Starting in Istio 1.5 (expected mid-February 2020), we should begin to see the effect of the istiod approach where functionality previously assigned to various microservices deployments will be coalesced into a single daemon.
Istio is used to help solve difficult application-networking challenges introduced by a microservices/cloud architecture, so why would Istio itself move away from a microservices architecture? The most straight-forward answer is:
The complexity of the microservices approach proved to not deliver the value or goals it intended. On the contrary, it worked against those goals.
For the Istio project, it looks like a monolithic approach would better contribute to those goals. Let’s take a closer look.
Istio is a open-source service mesh, which is architected similar to other service-mesh implementations with a control plane and a data plane. The data plane consists of the proxies that live with each application instance and is in the request path. The control plane lives outside of the request path and is used to administer and control the behavior of the data plane.
Historically, Istio’s control plane was implemented as separately deployable services that did the following:
These services would be driven by a set of operator-defined configuration and coordinate to eventually serve and direct the data plane.
Microservices can enable an organization to go faster by reducing friction to make changes to the system. With a microservices architecture, each service would likely be operated independently (each with their own team) and have it’s own release cadence/lifecycle independent from others. This would give the developers and operators parallel tracks to move faster without the locking/synchronization/coordination of making changes which can serve to slow down deployments and feature changes.
Another reason why a service may be further broken down is it’s usage patterns and scaling properties. For a simple example, a service that has heavy reads and writes may benefit from separating out reads from writes because reads may be more memory intensive (maybe needing more cache space to make the reads super fast) while writes may be more storage or network intensive. You could optimize the read part of a service on machines/quotas that allow it to scale independently (more memory) then have the write part of the service on other machines that have SSD or optimized EBS/SAN, etc.
Some other reasons why you might split an app into services:
The #1 tradeoff when going to a microservices architecture is complexity. When you go from one thing (monolith) to a bunch of little things communicating with each other (to optimize for a particular concern), you significantly increase complexity both in the architecture as well as the infrastructure necessary to run things.
This may be a necessary tradeoff insofar you realize the benefits. If not, it’s best you evaluate your assumptions and correct course. That’s what’s happening with Istio right now.
The first thing to understand is who is developing and operating your services architecture. In the Istio community, there are different components in the project as can be seen with the different community working groups. On the other hand, the persona downloading and operating an Istio installation is not as deconstructed. In fact, the observation so far is that a single group (or even single person) operates the Istio control plane. In some ways, the Istio control plane as a set of microservices would work well if run as a larger SaaS, but in its current adoption that doesn’t seem to be the case.
The second thing to understand is how is a release done? Can the services be released independently? The answer for Istio was “theoretically yes” in practice this doesn’t seem to be the case. When a new version of Istio is released, you need to upgrade/deploy all of the control-plane components.
Lastly, in the Istio case, you could ask “aren’t there other scaling variables and security concerns that are different for the various components?” And an honest answer would realize that there isn’t really. Taken directly from the Istio design doc for istiod:
This is not the case in Istio today for the majority of components however — control plane costs are dominated by a single feature (serving XDS). Every other control plane feature has a marginal cost by comparison, therefore there is little value to separation.
For security, all of the control-plane services had the same level of privilege:
This is not the case today as the powers exercised by the Mutating Webhook, Envoy Bootstrap & Pilot are similar to those of Citadel in many respects and therefore exploits of them have near equivalent damage
As the subtext in the Istiod design doc states “Complexity is the root of all evil or: How I Learned to Stop Worrying and Love the Monolith”.
istiod is an incarnation of a monolith which supports all of the functionality of the previous releases with significantly reduced complexity. Note that the services that made up the previous control planes are still implemented as sub-modules within the project (complete with boundary and contracts, etc) but the operational experience is improved. An operator now needs to worry about running and upgrading a single binary vs a collection of them.
For Istio going to a monolithic control plane, a significant amount of complexity can be reduced which never fully paid off:
See the Istiod design doc for more detail.
Also as a side note: you can check out a demo I did of thisistiod approach which should appear in Istio 1.5. Just note, it’s demo’d on a super alpha build of Istio, so not all polished like it will be :)
I’m happy to see the Istio community continue to improve its usability and operability characteristics. Going to a monolithic deployment of the Istio control plane makes a lot of sense for the project. Is that something that makes sense for your project? Is that something you would consider if it did? Are you measuring the value-to-complexity ratio for your microservices architecture (and associated infrastructure) in such a way that you’d be able to determine the time to change approaches?
Reach out to me @christianposta on twitter if you have thoughts you’d like to share. A good follow up post would be something along the lines of a decision table or key indicators of when you should be tempted to change course once you’ve decided to go down the path of microservices et. al. Hit me up if you’d like to contribute to that.
ITNEXT is a platform for IT developers & software engineers…
77 
1
77 claps
77 
1
Written by
Field CTO, solo.io — all things serverless, cloud, devops, microservices, integration, messaging. Author Istio in Action.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Field CTO, solo.io — all things serverless, cloud, devops, microservices, integration, messaging. Author Istio in Action.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/how-to-create-a-serverless-payment-system-using-stripe-and-aws-lambda-3e532a5a3817?source=search_post---------394,"There are currently no responses for this story.
Be the first to respond.
We are in the online shopping epoch and the implementation of the online payment methods into cloud-native apps is becoming an increasing need for the market.
As we can guess, managing payments into our business flow requires a secure and reliable infrastructure, that can guarantee the privacy and the consistency of data and transactions.
The integration to more and more numerous payment circuits involves a considerable effort in development and maintainability.
Today we are introducing to you a fully serverless solution based on the famous service Stripe, a payment middleware that provides to its users a back office dashboard and a REST interface.
A fully-managed service — like Stripe is — can help, but each payment flow has its own features depending on different business requirements. It is strongly recommended to write server-side code in order to keep this information secret so that it is possible to avoid any sensitive data spread.
As a scalable and fully-managed service, Stripe allows us to build high performing applications. Anyway, to get the most out of this service it is important to build an equally scalable and agile back-end able to adapt the best way possible. To do so, Serverless technologies come to help.
In particular, in this article, we are focusing on the use of AWS Lambda, a serverless computing service provided by Amazon web services.
For the beginners, Lambdas are stateless serverless functions. The developer is able to work in an environment where he can write code without worry about host hardware and you pay only what you use.
Let’s deep dive into Stripe. How to use it?
First of all, sign up to Stripe.
Signing up is free and you’ll pay only for what you use. For pricing details check this page.
In this article, we are not going to explain every single feature of Stripe (form more details, see the official documentation). Instead, we are going to integrate an AWS Lambda serverless application with the API of our just-created Stripe account.
One of the main components of Stripe is the dashboard: it offers users the possibility to create and manage resources like subscriptions and products.
As you can see from the dashboard, we can choose between two different kinds of APIs:
keypair test APIs through which you will be able to create test data (note: they will be visible only if the “Viewing test data” is checked) and live API key, used to create real transactions (usually for production environment).
Both of the keys must be stored in a secure place. The most reliable tool to store those kinds of information is AWS Secret Manager, a key-value database used only to store credentials, access keys or other kinds of data that can be considered sensible.
To save new information, click on “Store a new secret” and select the secret’s type
In this case, we need to create only raw key-value data without any kind of integration.
Finally, it’s time to create the Lambda function which will manage the payment.
In this example, we will use Python 3.6
NOTE: Be sure to have attached to the Lambda a LambdaLayer containing pip, stripe, and boto3 packages.
Through the algorithm implemented in the example below, we are creating a new subscription instance attached to a user, starting from a subscription model created previously from the dashboard. To use this code you will need the secret manager ARN and plan_id.
As you can see, this Lambda gets the values from the event payload; it will change based on the service integrated to the Lambda.
There are several ways to use your Lambda: it can be used as a trigger in an SQS queue, as a resource in an API Gateway or it can invoke directly from your client.
Congratulation!
By following these steps, you have successfully created your serverless payment system. Now you are ready to handle millions of users containing and optimizing infrastructural costs. It’s time to try it in a production environment!
Still curious about Stripe or AWS Lambda? Contact us to have a chat with our Cloud Expert.
See you in the next article!
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬.
To join our community Slack team chat 🗣️ read our weekly Faun topics 🗞️, and connect with the community 📣 click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
152 
152 claps
152 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
We are a team of IT enthusiasts and Cloud pioneers proudly using the AWS Cloud since 2007 and loving to share all the cool things we do with it every day.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/sequoia-capital/rockset-and-the-future-of-data-driven-apps-34acf3e2a517?source=search_post---------395,"There are currently no responses for this story.
Be the first to respond.
One of the most important technology themes of the past 10 years has simply been data.
At the beginning of the decade, new data techniques helped drive an increased focus on analytics and A/B testing. In the past few years, the focus has shifted to machine learning. Both technologies are rooted in the same trend — the exponential increase in data.
The CEOs we meet know that their businesses need to become data-driven to compete and survive. But not everyone knows how.
Getting started is easy — you instrument your product or business and start collecting data. But turning that data into valuable, actionable insights is much more challenging.
One of the biggest pain points is just physically moving the data around. The data pipelines and ETL jobs that move data from its initial raw form to the final product are often slow, fragile, and hard to maintain.
There have been many efforts to simplify these pipelines, but the state of the art is still Rube-Goldberg-esque. It can take weeks or months for new types of data to make their way into production applications — if they ever make it at all.
Rockset
At Sequoia, we love partnering with founders that challenge conventional thinking.
When we first met Venkat and Dhruba, they wondered — why do we need data pipelines at all? In this new era of cloud-native infrastructure, why can’t we use web-scale search techniques to build data-driven applications directly on top of the raw data? Why make pipelines 10% better when we can eliminate them entirely?
Their answer is Rockset. A cloud-native service that helps developers run production-ready SQL directly on top of raw data. It’s super simple to get started:
Once you’ve configured your data source, Rockset immediately starts indexing your data, enabling you to explore your data and write application-ready SQL within seconds. At Sequoia, we’ve used Rockset to move many cumbersome nightly jobs to real-time dashboards. Not only are our internal tools faster, but our data science team now spends less time babysitting pipelines and more time helping our companies grow.
Rockset is the result of a wonderful, interdisciplinary data team coming out of Facebook. CEO Venkat Venkataramani was the founder of TAO (Facebook’s online graph database) and a database engineer at Oracle. CTO Dhruba Borthakur was one of the key architects of Facebook’s data warehouse and the co-creator of both RocksDB and HDFS. Tudor Bosman was a co-creator of Unicorn (Facebook’s internal search backend) and the Gmail backend. Shruti Bhat held senior product roles at both VMware and Oracle.
We first partnered with Rockset at the seed stage in 2016, along with our friends at Greylock. At the time, Rockset was a rough idea inspired by the idea of converging a traditional SQL database with a cloud-native search engine. Today we’re proud to deepen that partnership with Rockset’s Series A financing.
It’s been a fantastic journey with the team so far — and it’s just getting started. We’re incredibly excited to see what you build with Rockset.
From idea to IPO and beyond, Sequoia helps the daring build…
157 
1
157 claps
157 
1
From idea to IPO and beyond, Sequoia helps the daring build legendary companies.
Written by
@Sequoia. Work w/ Rippling, Notion, Statsig, Blues Wireless, Aquarium, Census, Rockset, PicsArt, Threads, Verkada, Starkware, Canvas, Citizen, WhisperAI & more.
From idea to IPO and beyond, Sequoia helps the daring build legendary companies.
"
https://blog.mobyproject.org/serverless-frameworks-and-containers-df835581893b?source=search_post---------396,NA
https://blog.heptio.com/certified-kubernetes-a-key-step-forward-for-the-open-source-ecosystem-1f845df65898?source=search_post---------397,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Today the Cloud Native Computing Foundation launched the Certified Kubernetes program. We are pleased to announce that Heptio is participating, and that the AWS Quick Start for Kubernetes is our first certified release. We are also thrilled the community chose Heptio’s Sonobuoy as the underlying technology to support the certification program.
A key goal for Kubernetes is portability. This is done by creating a framework that could isolate developers from the specifics of a given infrastructure environment. Developers can build an application in one environment and then deploy it into another with minimal changes. Businesses can pick the hosting platform that best meets their needs based on application requirements. Cost of infrastructure, connectivity and quality of service can be evaluated impartially and then the best environment for a specific application selected. If the requirements change or new offerings emerge the application isn’t tied to a single Kubernetes environment.
The best way to provision and run a Kubernetes cluster may vary from environment to environment. Managed offerings like Google Container Engine or Azure Container Service take a lot of the pain out of deploying and managing Kubernetes. Heptio’s AWS Quick Start for Kubernetes is another example that makes it easy way to get a basic Kubernetes running on AWS. These don’t have direct analogies on-premises. The certification program gives users a high degree of confidence the Kubernetes environment they chose meets the standards of the community. It is an important mechanism to avoid vendor lock-in and maintain flexibility.
Heptio built the Sonobuoy project to help support this program, but also as a way that users can independently verify that the environment meets certification requirements. This makes it not only the default certification tool for the Kubernetes community, but a powerful way for users to qualify their own environments. They can ensure that there will be no surprises when they deploy and run their applications. It is particularly useful for users that chose to build and run their own Kubernetes clusters and want to ensure what they have built works as it should.
In addition to the core open source project, we released a simple utility serivce called Heptio Sonobuoy Scanner to help users deploy and process the results of Sonobuoy. You can check it out here.
We look forward to continuing to work with CNCF to ensure that Heptio’s offerings remain true to the upstream Kubernetes project, and will continue to invest in and drive the capabilities of Sonobuoy as a flexible and easy to use conformance tool.
Heptio
53 
Thanks to Jennifer Rondeau. 
53 claps
53 
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Written by
Kubernetes, Containers, Cloud Native. VP Product VMware.
Heptio
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@peacecwz/net-core-consul-configuration-kullan%C4%B1m%C4%B1-16e3e068b18c?source=search_post---------398,"Sign in
There are currently no responses for this story.
Be the first to respond.
Baris Ceviz
Sep 17, 2020·3 min read
Bu yazımda .NET Core uygulamalarında Consul kullanarak configuration management üzerine bir çalışma yapacağız. Bu yazıda Consul nedir ve configuration yapısı temel olarak nasıl çalışır, sonrasında .NET Core uygulamalarında temel olarak Configuration nasıldır ve Consul nasıl bağlanır konusu üzerinden devam edeceğiz.
Consul, HashiCorp tarafından yapılan ve temel olarak Networking çözümü sağlayan bir üründür. DNS tabanlı Service Discovery yapmayı hedefleyen ve Distributed Key-Value storage sağlayan bir ürün. Bu ürün sonrasında Real-Time healthcheck, Dynamic Load Balacing gibi birçok özelliği sahiplendi. Temel amacı Service Networking üzerine güzel bir ürüne dönüştü.
On-Premise veya Cloud Provider üzerinde PaaS veya IaaS olarak rahatça kullanılabiliyor. Azure üzerinde Consul’u Managed Apps olarak PaaS bulabilirsiniz. VM based Apps veya AKS üzerinde çalışan app ile rahatça bağlayabilirsiniz
Projemizde Consul configuration’ı .NET Core’daki Configuration Chain içerisine eklemek için Winton.Extensions.Configuration.Consul paketini kullanabilirsiniz. Nuget üzerinde birçok paket mevcut fakat en iyi implementasyon bu pakette olduğunu gözlemledim.
Öncelikle erişebileceğiniz bir Consul ortamı yok ise Docker üzerinde bir consul ayağa kaldıralım
“http://localhost:8500/” adresine girelim ve KeyValue bölümüne girelim
Key or folder bölümüne ApplicationName/configfile şeklinde config dosyasını oluşturabilirsiniz. Buradaki ApplicationName, .NET Core uygulamanızda “applicationName” key i ile gelen name i kullanabilirsiniz. configfile olarak appsettings.json veya ortam bazlı config için appsettings.Development.json gibi kullanabilirsiniz. En temelde yapılan işlem, Application için Consul’de folder oluşturmanız ve o folder içerisinde de configfile oluşturmanız gerekir.
Bu işlemleri tamamladıktan sonra artık uygulamamıza Consul Configuration ekleyebiliriz. Bahsettiğimiz paketi projemize ekleyelim.
Ardından Program.cs içerisinde AppConfig’i Configure edelim. Aşağıdaki kod parçacığı ile rahatça yapabilirsiniz.
ConfigureHostConfiguration içerisinde appsettings.json ı ekliyoruz. Bunun temel sebebi ConfigureAppConfiguration içerisinde appsettings.json configuration chain e daha dahil edilmemiş olduğu için ConfigureAppConfiguration fonksiyondan önce eklenmesi gerekmektedir. Bu yüzden böyle bir configuration yaptık
Burada iki kez Consul configuration ekleniyor. Bir tanesi ortam bazlı config diğeri de appsettings.json ekleniyor.
Buradaki application name aslında Project name ile aynı oluyor (default olarak) Burada Consul config i customize edebilirsiniz. Örneğin reloadOnChange feature ını kullanabilirsiniz
Veya Consul non require bir configuration ise Optional true yaparak require durumdan çıkarabilirsiniz.
Burada ConsulClient veya authorization süreçlerini de configure edebilirsiniz. En temelde Host URL’i verirken aşağıdaki bir function yazdık ve ConsulConfigurationOptions property sine setledik. Bunu function yapmamızın sebebi iki kez Consul config eklediğimiz içindir.
ConsulHost config için aşağıdaki gibi projenizdeki appsettings.json a ekliyoruz
Bu işlemleri tamamladıktan sonra artık test için basit bir controller yazabiliriz. Ben MessagesController yazarak test işlemlerimi gerçekleştireceğim
Consul içerisinde eklediğim appsettings.json içerisinde aşağıdaki gibi config ekledim
appsettings.Development.json için ise aşağıdaki config i ekledim
Böylelikle ortam bazlı configlerin oluşturulduğunu görmem gerekiyor. ASPNETCORE_ENVIRONMENT variable ını değiştirerek testlerimi yapıyorum.
Development ve Production için ayrı ayrı çalıştıralım ve yukarıdaki gibi istek atalım. Böylelikle Consul Configuration’ı ortam bazlı eklenebildiğini de görmüş olacağız.
Uygulamanızın configuration larını dynamic olarak artık Consul üzerinden yönetilebilir hale getirebilirsiniz. Böylelikle .NET Core ile Cloud Native application lar geliştirmenizde Configuration tarafını çözebilirsiniz
.NET Core Configuration nasıl çalıştığını merak edenler için aşağıdaki yazımı okuyabilirsiniz
medium.com
Twitter ve Github üzerinden takip edebilirsiniz :)
github.com
@PeaceCwz
İyi okumalar :)
Jedi Software Engineer @Trendyol, Microsoft Student Partner, Yildiz Technical University, My kid @Scodeapp
100 
100 
100 
Jedi Software Engineer @Trendyol, Microsoft Student Partner, Yildiz Technical University, My kid @Scodeapp
"
https://medium.com/google-cloud/how-to-reach-on-premise-resources-with-serverless-products-747ebd37dc8e?source=search_post---------399,"There are currently no responses for this story.
Be the first to respond.
Serverless is awesome. You pay only when they run and they scale automatically. Serverless solves a lot of problem. Do have a micro-batch to run daily ? Use the resource only when required ! Do you have an event to handle ? Plug it to your code and process it.
When you begin to play with serverless, like AppEngine, Cloud Functions or Cloud Run, you want to use it everywhere and anytime, even for your on-premise workload. However, there is a major issue: your workload has to use on-premise data and data can’t be moved to the cloud because other on-premise workloads use them.
For using securely the on-premise data in the cloud, you have to secure the connexion, mainly without exposing public IP and by encrypting data in transit.
The cheaper solution is the VPN. The bandwidth is limited but it offers a good starting point with low investment for a secure channel to your on-premise environment
The ultimate level is the interconnect solution, which offers a direct connection between your on-premise environment and Google fiber network. Extremely fast (from 10 to 100Gbits/s), with high SLA and thus… expensive.
Finally, the intermediate level is the Partner interconnect. A tier company has a direct connect subscription with Google, and resell smaller bandwidths. The SLA are lower than with direct interconnect, but the price is more accessible for small and medium company.
In production environment, don’t forget to, at least, backup each link for avoiding SPOF (single point of failure)
I’m not expert in network and I can’t discuss about the best hybrid connectivity solution. There is a lot of great articles and examples here.
So, to connect your on-premise environment is time consuming, more or less expensive, and you have to double all for reliability. And, thereby, you don’t want to reproduce this work in all your project.
The solution is to connect all other projects to this “on-premise entry-point” project and thereby to re-routed other projects flows to the on-premise environment. Here is the purpose the VPC peering solution.
Now you have the right product for connecting your on-premise environment to all your GCP projects, in a secure way and with mutualized costs and efforts!
Serverless means without infrastructure to manage. Thereby, a design issue seems occur
How to connect not managed servers and networks to a specific and managed VPC ?
Hopefully, a beta feature helps us in this task: serverless VPC connector. Today available only for Cloud Functions and AppEngine; the team works hard to offer this capability to other products like Cloud Run.
The principle is simple:
Then attach this connector to your serverless product and its network flow will enter in the VPC network with one of IP defined in the range. The 2 worlds are unified!
Now, we have all the pieces of the puzzle, let’s have a look how to proceed step by step.
I will name “project-onpremise” the project with the VPC connected to on-premise environment, and “project-function”, the project where the Cloud Functions is deployed and has to reach Oracle DB on the on-premise environment.
In this part, I suppose that the connexion with your on-premise environment exists (VPN or interconnect). If you want to create a dummy VPN connection, this Qwiklab is well documented
For creating a VPC peering, you have to create a peer connexion in each project. Let’s start with the project-onpremise:
If you prefer command lines
Perform the exact same mirrored thing in the project-function.
If you prefer command lines
Great, now your peering should be green in the 2 projects.
Be careful, if you use the default VPC networks created automatically in the projects, it won’t work because IP range are overlapped. Indeed, the peering import all the subnet of the VPC from the target project. This builds a logical super VPC with the subnets of all the peered projects.
Thereby, I recommend you to create a custom VPC, or to turn to custom the default VPC and change/replace the current ranges in both projects for avoiding any overlapping.
Now, have a look on network routes in the projects.
As you can see, the custom VPC route for the VPN has been exported to the peer project and correctly imported in the project-function.
The routes between the projects are up. Let’s continue with the VPC Connector.
The serverless VPC Connector builds a bridge between the serverless “wild” world (not or few customizable) and your “in-house” VPC (fully customizable). Let’s set up one in the project-function:
If you prefer command lines (here an example with europe-west1 region)
Great, that works !
Don’t forget to update your routing rules on your on-premise equipments for sending back the response to the defined serverless VPC Connector range
Now, the final piece. But before creating the Cloud Functions, roles has to be granted to the “Cloud Function Service Agent” for allowing the Cloud Function to use the serverless VPC connector
You can add them manually in the IAM console:
Or use the command lines
Now, let’s deploy a simple test connexion Cloud Function in Go for testing the opened port on-premise. (In my case, I would like to reach a Oracle database, port 1521)
Change the function to execute to Ping
If you prefer to deploy with command lines (in the directory where you have created the files function.go)
Now, it’s time to test your function and the whole connectivity.
In the console, simply click on Test the function
Or with command line
Boom, that works !
In few clicks, or few command lines, we simply:
This capability is very useful for starting a migration process with no critical workload and with few data to process, especially with VPN. Indeed, the network latency and the connection availability avoid all mission critical workloads.
Serverless VPC Connector is also useful for allowing your serverless components to reach services only deployed on VPC, like Memorystore or VM only available in a VPC (without external IP).
Happy testing!
Google Cloud community articles and blogs
67 
1
67 claps
67 
1
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
