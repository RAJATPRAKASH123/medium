story_url,bodyText
https://medium.com/@kkempin/kierunki-rozwoju-us%C5%82ug-w-modelu-iaas-df7fd71283f4?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krzysztof Kempiński
Nov 23, 2021·7 min read
W ramach podcastu “Porozmawiajmy o IT” miałem okazję porozmawiać z Marcinem Szyszkowskim z Atman o kierunkach rozwoju usług w model IaaS.
Posłuchaj naszej rozmowy w wersji audio 🎧 👇
porozmawiajmyoit.pl
Cześć! Mój dzisiejszy gość od 4 lat zajmuje się wdrażaniem produktów zarówno w modelu B2B, jak i B2C w szeroko rozumianej branży telekomunikacyjnej. Prywatnie fotograf, crossfitter i największy fan swojej trzymiesięcznej córki. Moim i Waszym gościem jest Marcin Szyszkowski!
Cześć, Marcin! Bardzo miło mi gościć Cię w podcaście!
Cieszę się, że mogłem się z Tobą zdalnie zdzwonić! Witam wszystkich!
Zawsze jest dobrze porozmawiać, a tematem naszej dzisiejszej rozmowy będą kierunki rozwoju usług w modelu IaaS. Zanim do tego przejdziemy, to chciałbym Cię zapytać, jak każdego mojego gościa o to, czy słuchasz podcastów, a jeśli tak, to może masz jakieś swoje ulubione audycje, które możesz przedstawić i o nich opowiedzieć?
Dotychczas sporadycznie, ale dzięki mojej trzymiesięcznej córce coraz więcej, z racji tego, że wychodzi z nią często na spacery. Ona nie chce ze mną rozmawiać, więc słucham podcastów!
Należą do nich przede wszystkim Raport o stanie świata. Ostatnio zacząłem słuchać podcastów związanych z ogrodnictwem — Naturalnie o ogrodach oraz oczywiście Porozmawiajmy o IT.
Świetnie. Dziękuję bardzo! Fajne jest to, że możemy znaleźć już na tej scenie podcastowej takie podcasty, które pasują do naszych zainteresowań, naszego hobby. Do tego, co nas interesuje. To świetna sprawa!
Tyle o podcastach, przejdźmy zatem do tematu naszej rozmowy. Żeby rozjaśnić naszym słuchaczom czym jest ten skrót IaaS, to chciałbym Cię na początku właśnie o niego zapytać.
Jak taki model usług działa w praktyce?
Polega on na udostępnieniu infrastruktury jako usługi. Czyli klient, firma, która decyduje się na skorzystanie z modelu IaaS, czyli Infrastructure as a Service, korzysta z usługi firmy trzeciej, która zajmuje się całą tą otoczką związaną z zapewnieniem ciągłości pracy, osieciowieniem, aktualizacjami Firmware’ów itd.
Polega on na udostępnieniu infrastruktury jako usługi. Czyli klient, firma, która decyduje się na skorzystanie z modelu IaaS, czyli Infrastructure as a Service, korzysta z usługi firmy trzeciej, która zajmuje się całą tą otoczką związaną z zapewnieniem ciągłości pracy, osieciowieniem, aktualizacjami Firmware’ów itd.
W zależności od potrzeb możemy wyróżnić tutaj też bardziej skomplikowane modele, czyli PaaS oraz SaaS, czyli Platform as a Service oraz Software as a Service. Większość ludzi, z którymi się spotykam nie ma świadomości tego, że korzysta z usług w chmurze, korzystając z jakichś aplikacji czy oglądając telewizję streamingową.
Jesteś w stanie powiedzieć, w którym momencie taki model usług w ogóle się pojawił? Kiedy przeszliśmy od klasycznej kolokacji albo klasycznego modelu korzystania ze współdzielonych serwerów na coś takiego, co się nazywa modelem IaaS?
Przede wszystkim wzrost zapotrzebowania na moc obliczeniową, rozwój usług w Chmurze i potrzeby dynamicznego rozwoju. Ograniczenia związane z możliwościami firm, gdzie w pewnym momencie firma mająca własne serwerownie staje pod ścianą, bo kończy się przestrzeń i musi podjąć kolejne kroki.
Albo rozbudowywać się, tworzyć kolejno infrastrukturę albo skorzystać właśnie z modelu Infrastructure as a Service. Wynieść swój sprzęt do kolokacji, w jakiś sposób łączyć takie tego typu usługi. Popularyzacja tego typu usług rozpoczęła się w Polsce myślę, że jakieś 8–9 lat temu i w tym samym okresie firma, w której pracuję, też stworzyła ofertę tego typu usług.
Rozumiem. Z takim pojęciem tego modelu IaaS kojarzy mi się być może oddelegowanie pewnych odpowiedzialności albo zrzucenie pewnego balastu, pewnych problemów, ciężarów związanych z utrzymaniem swojej własnej serwerowni, co myślę — tutaj też nie było małym driverem, jeśli chodzi o rozwój tej sceny IaaS w Polsce i nie tylko. Natomiast możemy iść za daleko z tym zrzucaniem odpowiedzialności i podejmować taką próbę zaprzestania dbania o pewne rzeczy, ponieważ to właśnie, ten dostawca usług powinien się nimi zająć.
Dążę do tego, żeby zapytać Cię o to, jak rozkładają się odpowiedzialności klienta i usługodawcy w tym modelu?
Jest bardzo wygodny, bo dostajemy serwer, który ma działać — i on działa. Nas nie interesuje, czy popsuje się tam dysk twardy czy uszkodzeniu ulegnie wielkość pamięci. Po prostu jako klient dostajemy usługę i z niej korzystamy. Natomiast — i to jest taka wygoda — pewne ograniczenie tego typu usługi to jest przede wszystkim to, że nie mamy kontaktu fizycznego z takim serwerem. Jeśli to sobie poprzekładamy w głowie, to potem już mamy same korzyści.
Nasza odpowiedzialność jako klienta spoczywa, w zasadzie w ramach tej miesięcznej opłaty na dostawcy usługi. Ten musi martwić się o odpowiednią dostępność tej usługi, o jakoś działania, o upgrade’y BIOS-ów, o aktualizacje sterowników. Wszystko w zależności od modelu, w którym świadczymy taką usługę, bo możemy sobie wyobrazić taki model, że klientowi udostępniamy serwer i klient się o wszystko martwi, natomiast często spotykamy takie rozwiązanie jak zarządzanie tą infrastrukturą, tym serwerem, gdzie w zasadzie klient dostaje platformę jako usługę. To w zależności od oczekiwań, od możliwości klienta, od jego zaawansowania w obszarze IT, mocy, możliwości zasobów IT w jego firmie. Tutaj aspektów jest dużo — tak samo, jak dużo jest klientów, tak samo, jak wariantów świadczenia takiej usługi jest dużo.
Rozumiem. Czy można to streścić albo tak podsumować, że odpowiedzialność klienta jest bardziej w domenie aplikacyjnej, a infrastruktura to jest odpowiedzialność usługodawcy? Czy jest to może zbyt proste rozróżnienie?
Myślę, że rozróżnienie jest zbyt proste — mamy takie współprace, gdzie klient również w dużym stopniu konfiguruje otoczenie sieciowe w ramach takiego modelu. Tak jak powiedziałem — to wszystko zależy od projektu. Większość firm świadczących tego typu usługi jest bardzo elastyczna. Mogę mówić o firmie, w której pracuję. Jesteśmy bardzo elastyczni i dajemy takie możliwości, natomiast w typowym modelu klient martwi się o tę wersję aplikacyjną, tak jak powiedziałeś. Natomiast za cały hardware odpowiada dostawca usługi.
Dobrze. Przyjrzyjmy się może korzyściom, jakie klient może czerpać z przestawienia się albo z korzystania z usług w takim modelu. Można powiedzieć — dobrze, jeśli mam całą infrastrukturę u siebie w firmie, w budynku, w piwnicy przysłowiowej, to mam nad tym pełną kontrolę. Mogę dowolnie też konfigurować ten sprzęt, tę strukturę, te usługi itd. Jednak jest niewątpliwie wiele też korzyści płynących z korzystania z usług w modelu IaaS.
O te korzyści dla szeroko rozumianego biznesu chciałbym Cię teraz zapytać.
Przede wszystkim możliwość skupienia się na własnym, podcastowym biznesie. Brak konieczności troszczenia się o bezawaryjne działanie sprzętu, o zasilanie, o chłodzenie tego sprzętu. O przestrzeń do rozwoju.
Powiedziałem na początku, że czasami klienci stają przed takim dylematem: budować własną serwerownię czy przenieść się do kolokacji, skorzystać z modelu IaaS czy może modelu aplikacyjnego? To wszystko zależy od potrzeb, natomiast korzyść jest znacząca. Za jakaś miesięczną opłatę mamy z głowy wszystkie problemy związane z awaryjnością działania sprzętu, z możliwością rozwoju nie mamy w zasadzie żadnych ograniczeń związanych z infrastrukturą.
Powiedziałem na początku, że czasami klienci stają przed takim dylematem: budować własną serwerownię czy przenieść się do kolokacji, skorzystać z modelu IaaS czy może modelu aplikacyjnego? To wszystko zależy od potrzeb, natomiast korzyść jest znacząca. Za jakaś miesięczną opłatę mamy z głowy wszystkie problemy związane z awaryjnością działania sprzętu, z możliwością rozwoju nie mamy w zasadzie żadnych ograniczeń związanych z infrastrukturą.
Czy można powiedzieć, że też redundancja jest tą wartością? Możliwość szybkiego naprawienia pewnego problemu, szybkiego zastąpienia jakichś problemów, chociażby sprzętowych po stronie usługodawcy, co niekiedy byłoby dużym problemem, gdyby ta infrastruktura była u nas w budynku?
Przede wszystkim wiązałoby się to z dużym kosztem, bo zapewnienie zastępstwa za uszkodzony serwer, już tak mówiąc całościowo — że serwer uległ uszkodzeniu, zazwyczaj ulega jakiś pojedynczy element, ale trzymanie wszystkich podzespołów i być przygotowanym na awarię — to jest bardzo duży koszt.
Stworzenia środowiska HA, czyli tego wysoko dostępnego, to też jest bardzo dużo koszt. W modelu IaaS-owym też koszt jest znacząco niższy. Nie musimy budować drugiej serwerowni, żeby umieścić tam jeden serwer.
Zgadza się. Gdy myślę o tych korzyściach, to również bezpieczeństwo jest jednym z takich aspektów, aczkolwiek to jest trochę taki nieoczywisty temat, bo może się wydawać, że oddelegowanie czy korzystanie z tych usług zewnętrznych może być jakimś — może nie naruszeniem, ale jakimś takim tematem, który ociera się o decyzję, czy chcemy to robić czy nie, właśnie w kontekście bezpieczeństwa.
Ale z drugiej strony — jeśli tak nie robimy, to my jesteśmy odpowiedzialni za to bezpieczeństwo i my musimy zapewnić własnymi środkami u nas w firmie tak, że to bezpieczeństwo będzie na wysokim poziomie. Jestem ciekaw co Ty myślisz o bezpieczeństwie jako korzyści płynące z korzystania z tego modelu?
Sytuacja wygląda następująco. Nam się wydaje, że jak mamy coś obok siebie, to jest to bezpieczne. Wcale tak nie jest! Paradoksalnie, to jest zupełnie odwrotnie, bo najwięcej problemów, najwięcej awarii wywoływanych jest przez czynnik ludzki.
To, że dostępu do serwera tak naprawdę nie mamy, to jest z korzyścią dla ciągłości działania tej usługi. O bezpieczeństwie możemy mówić w wielu aspektach. Na przykład o bezpieczeństwie fizycznym. Mówimy tu o monitoringu wejść, monitoringu osób, które mają dostęp do serwerowni. O bezpieczeństwie związanym z zasilaniem, z awariami wodociągów, cieków wolnych, zalaniem itd.
Tych aspektów związanych z bezpieczeństwem jest mnóstwo. Kolejna sprawa to bezpieczeństwo sieciowe, bezpieczeństwo IT. Ataki hakerskie, gdzie tutaj w firmach świadczących tego typu usługi pracuje rzesza osób z odpowiednimi uprawnieniami, przeszkolonych, które są w stanie zapewnić bezpieczeństwo na najwyższym poziomie. Oczywiście nie wyłącza to odpowiedzialności klienta za to, kogo wpuszcza na ten serwer.
Tych aspektów związanych z bezpieczeństwem jest mnóstwo. Kolejna sprawa to bezpieczeństwo sieciowe, bezpieczeństwo IT. Ataki hakerskie, gdzie tutaj w firmach świadczących tego typu usługi pracuje rzesza osób z odpowiednimi uprawnieniami, przeszkolonych, które są w stanie zapewnić bezpieczeństwo na najwyższym poziomie. Oczywiście nie wyłącza to odpowiedzialności klienta za to, kogo wpuszcza na ten serwer.
Czy dba o aktualizację systemów operacyjnych? Natomiast w jakiś sposób jest chroniony z punktu widzenia sieciowego przed atakami typu anty DDoS.
👉 Czytaj dalej na: https://porozmawiajmyoit.pl/poit-139-kierunki-rozwoju-uslug-w-modelu-iaas/
IT expert. Ruby on Rails/iOS/Elixir programmer. Blogger. Podcaster.
See all (275)
IT expert. Ruby on Rails/iOS/Elixir programmer. Blogger. Podcaster.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/clouddon/modern-ai-stack-ai-service-consumption-models-f9957dce7b25?source=search_post---------101,"There are currently no responses for this story.
Be the first to respond.
If you are a developer developing new AI capabilities or LOB applications/ services leverage AI capabilities, you have various tools at your disposal. A necessary and sufficient collection of such tools can be visualized as the Modern AI Stack.
Artificial Intelligence (AI) refers to the tools and techniques that enable machines/ software to exhibit intelligence. Machine Learning (ML) refers to a subset of AI that enables a computing system to learn with data, without the need to be explicitly programmed. Artificial Neural Networks (ANN) are computing units inspired by neural networks of animal brains, that form the core of machine learning systems.
It is interesting to note that techniques/ algorithms used for AI haven’t changed in the past 20+ years. However, the tools have. If you were an AI researcher just ten years ago, you had to write code to build and train your neural networks. Not anymore. Gone are the days when you have to build such stack from scratch — now you have ML platforms, libraries, computing, and data platforms readily available as software platforms. Some of these capabilities are also available as a service that you can consume directly. This post provides an overview of such a stack and different consumption models available to consume AI capabilities as a Service.
Modern AI Stack consists of two components — infrastructure and developer environment.
Infrastructure refers to the tools, platforms, and techniques used to run store data, build and train AI/ ML algorithms, and the algorithms themselves.
Developer Environment refers to the tools that assist in developing code to bring out AI capabilities.
LOB applications and services are technically not part of the AI Stack. They derive value from the AI Stack.
Compute
Compute refers to the raw computational power required to run AI/ ML algorithms. One has a wide choice of physical servers, virtual machines, containers, specialized hardware such as GPUs, cloud-based computational resources including VMs, containers, and Serverless computing.
Data
Data makes an important component of machine learning system. Just like how one is made of what one eats, machine learning is only as good as the data it is trained with. One has a wide choice of data platforms — structured and non-structured databases, big data platforms, managed databases, and cloud-based databases.
Machine Learning Algorithms
Machine learning algorithms are of three categories — supervised, unsupervised and reinforced, with more choices of algorithms under each category.
Supervised Learning refers to learning to find the best fit function that maps input data to output data, based on training data of input-output pairs. Learning continues until a desirable accuracy on the training model is obtained.
Unsupervised Learning refers to learning to find the best match from an unknown category of data that was not encountered before (‘unlabeled’ data).
Reinforced Learning refers to learning based on trial-and-error.
Machine Learning Platform
Machine Learning Platforms/ frameworks provide necessary capabilities to enable one to develop ML capabilities. Such platforms usually accept different types of sources of training data, provides a choice of training algorithms, and support multiple programming languages. Commonly used ML platforms include Apache MXNet, TensorFlow, Caffe2, CNTK, SciKit-Learn¹, and Keras².
Libraries
You have a variety of libraries at your disposal — whether to leverage advanced mathematical operations (NumPy), or to add specific cognitive capability, such as computer vision (OpenCV), language translation (OpenNMT), etc. Particularly, if you are building cognitive services, say smart video surveillance services, you can use these libraries along with ML platforms.
IDE
Whether you are developing ML models or applications/ cognitive services that leverage underlying ML platform’s capabilities through APIs, you will be developing a good amount of code. An IDE would make your job easier.
There is a variety of integrated development environments (IDE) available for you, such as PyCharm, Microsoft VS Code, Jupyter, MATLAB etc. It is to be noted that IDEs for AI/ ML may not have the advanced debugging capabilities one is used with procedural or object-oriented programming languages.
Visualization
As we noted above, data makes an important component of machine learning. Naturally, data visualization plays an important role. One could argue that it is not an essential component of AI Stack, but given the importance of the datasets, we consider it to be an important part of AI Stack. Visualization choices include MATLAB, Seaborn, Facets, or data analytics platforms such as Tableau.
Workflow
We include workflow tools in the AI Stack as they make sharing, collaboration, and automation much easier. As more developers start leveraging AI/ ML capabilities, developer collaboration becomes more important. A variety of workflow automation tools are available, such as Jupyter, Anaconda, GitHub, VSTS etc.
Public cloud service providers are making more AI/ ML capabilities available as a service. This removes the need for having the entire stack deployed/ implemented from scratch. Such capabilities are also available at different levels of abstractions, enabling one to consume at the level that one prefers. As of now, AI/ ML services can be consumed through following ways.
AI Stack
This is the reference consumption model where every infrastructure component (ML platform, algorithms, compute, and data) is deployed and managed by the user. The user builds, trains, and deploys ML models. The user is also responsible for installing and managing all components of the developer environment.
This model is analogous to the consumption of on-prem/ private cloud services.
AI-aaS
AI-aaS refers to AI infrastructure services being offered by the services providers that one can consume directly. In this model, one continues to use their models, algorithms, types of data stores, compute resources as they would do with the AI Stack model. But they don’t install or manage the infrastructure components. They can leverage ML capabilities that are available as a service (Google Cloud ML Engine, Amazon ML), along with IaaS for compute and data requirements.
This model is analogous to the consumption of IaaS capabilities. Naturally, you will see a lot of Lift & Shift :).
Managed AI-aaS
It turns out that it is not trivial to build and train ML models. When ML models get complex, managing the supporting compute/ datastore is also not trivial.
Wouldn’t it be better if there is an easy way to train ML models? Wouldn’t it be better if compute resources are automatically allocated/ managed as the model requires? In short, wouldn’t it be more efficient if the developer can just focus on getting value out of ML without having to worry about the underlying infrastructure?
Managed AI-aaS services such as Google Cloud AutoML, Amazon SageMaker, Azure ML Studio belong to this category. They make it easier to consume ML by removing these pain points.
This mode is analogous to consuming Managed IaaS capabilities.
Cognition-aaS
Cognition-aaS refers to the consumption model where advanced cognitive capabilities themselves are available as a service. For example, if one has to build a video surveillance application, one can consume video recognition capabilities that are offered as service (Amazon Rekognition Video, Google Vision, Azure Compute Vision, etc). There is no need to build these capabilities using computer vision libraries and ML.
With such cognitive capabilities being readily available, an application developer can focus on business logic without having to worry about the underlying AI infrastructure components at all.
Last year, I had postulated that
Cognitive computing capabilities available as a service will double approximately every year
With more Cognition-aaS capabilities getting enabled by service providers and niche players adding more cognitive capabilities to the mix (Xoom.ai, Grammarly), this trend will continue.
Cognition-aaS is analogous to consuming PaaS capabilities. I am not seeing this as SaaS category as some might do, because, these capabilities are not complete solutions like a SaaS offering would be³.
Recommendations
Choose the right consumption model based on your application needs and in-house ML expertise.
If you want total control and everything in-house, choose AI Stack/ On-Prem.
If you want to build/ train ML models, but don’t want the overhead of managing ML platform/ underlying infrastructure components, choose AI-aaS.
If you want to leverage ML capabilities, but don’t want to manage infrastructure components, choose Managed AI-aaS.
If you would like to just focus on the business value, choose Cognition-aaS.
CloudDon - catalyzing modern enterprise IT transformations
243 
1
243 claps
243 
1
CloudDon - catalyzing modern enterprise IT transformations
Written by
Research Director, IDC. Irreverential Yogi; Single Dad; Son; Brother.
CloudDon - catalyzing modern enterprise IT transformations
"
https://medium.com/@manningbooks/improving-mad-libs-with-expressions-4d306683ab58?source=search_post---------133,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Feb 27, 2020·16 min read
From Terraform in Action by Scott Winkler
This article discusses how we can improve a Mad Libs generator using expressions.
__________________________________________________________________
Take 37% off Terraform in Action by entering fccwinkler into the discount code box at checkout at manning.com.__________________________________________________________________
We can generate a Mad Libs by templating files with a randomized pool of words and outputting the result to a file. Our solution is reasonably extendable, and it’s easy to envision introducing new template files or words to the word pool. This pattern isn’t only useful for Mad Libs, you could also apply it to templating server init scripts and JSON files for external consumption.
Let’s now suppose our Mad Libs engine becomes enormously popular around the office and suddenly everyone is asking for more Mad Libs. Maybe we now want to create not one, but a hundred such Mad Libs, using a variety of template files, and then zip them all together to can send them more easily to our coworkers to enjoy at their leisure.
Our initial solution is decent but has a number of deficiencies that we’d like to address, namely:
With these things in mind, let’s engineer a more advanced solution which solves all of these problems. Our revised architecture’s pictured in figure 1.
As indicated in the architecture diagram, instead of creating a single Mad Libs file, we’ll create one hundred Mad Libs files, and instead of being able to read from only one template, we’ll read from three different templates in a round robin fashion. We’ll also add a step for uppercasing all the words in each list before shuffling them to make them easier to pick out from the surrounding text. Finally, we’ll zip up all the Mad Libs into a single archive file which we can share more easily with our coworkers. When we’re done, the Mad Libs template engine will be much more robust and extendable. Along the way, we’ll see many of the expressive language features of HCL which you’ll want to incorporate in your own Terraform projects.
In particular, we’ll perform the following:
We’ll start from the left side of the architecture diagram and work our way rightwards. The first step requires us to uppercase all the words in the list before shuffling them. Although we could perform the uppercase operation at any time before the templating function, it makes sense to do it earlier in the process rather than later because we can cache the result in a local value to improve performance. Local values are conceptually like variables declared within a method, useful for temporarily storing the results of a calculation before using it elsewhere.
Before we set a local value, I should mention for expressions and how they can be used to uppercase each word in the list. A for expression outputs a complex type by transforming some other complex type. They work a lot like streams and lambda expressions in other programming languages (e.g. by performing arbitrary operations on each element of a map or list and outputting a transformed result). Figure 2 shows the syntax of a for expression which uppercases each element in an array of strings, and outputs the result as a new list.
The visualization of what the for expression from figure 2 is doing is shown in figure 3.
The brackets which go around the for expression determine the type of the output. The previous code used [], which means the output forms a list. If instead we used {}, then the result is an object. For example, if we wanted to loop through the words map and output a new map with the key being that of the original map, and the value being the length of the original value, we could do that easily with the following expression:
The visualization of what this for expression in figure 4 is doing is shown in figure 5.
For expressions are useful not only because they can convert one type to another, but because they can be nested and chained together. To make our for expression which uppercases each word in var.words, we’re going to chain the previous two for expressions we built into one mega for expression. Although making your code more complicated is generally a violation of clean code principles, which aims to mitigate and manage software complexity, sometimes the tradeoff is worth it.
TIP: Making your code cleaner and easier to understand is a principle of clean code. Using nested/chained expressions hurts readability and increases cyclomatic complexity; use them sparingly.
Our general logic is as follows:
Looping through each key-value pair in the words map, and outputting a new map (1) can be done with the following expressions:
Likewise, an expression which uppercases each word in a list and outputs a new list is (2):
By combining the above two expressions we get:
This can still be improved upon. It doesn’t make sense to uppercase numbers, and we could filter out any key which matches “numbers”, and doesn’t include this key-value pair in the output map. We can do this by making use of the optional “if” clause, which acts as syntactic sugar when conditionally filtering results in a for expression (3):
Finally, we can output the result in a local value to reference it later (4):
Local values are declared in a code block having label “locals”. You can declare as many local values as you want in the locals’ code block, and you can have more than one locals’ block in the same module scope. Any type (primitive or complex) that can be stored in an input or interpolation variable can also be stored in a local value.
Add in the new local value to madlibs.tf and update the reference of all the random_shuffle resources to point to the uppercase_words local value instead of var.words. Listing 1 shows what your code should now look like.
Listing 1. Contents of madlibs.tf
#A expression to uppercase each word in the var.words and save to a local value
#B input for random_shuffle now comes from locals.uppercase_words
#C numbers don’t need to be uppercased
Implicit Dependencies
It’s important to point out that because we’re using an interpolated variable to set the input attribute of the random_shuffle resources to the local value “uppercase_words”, we are in fact specifying an implicit dependency between the two elements. An implicit dependency on random_shuffle means that it won’t be created until after local value has been computed. If we were to examine the dependency graph for what we have right now, it’d look like figure 6.
Nodes nearer the bottom of the dependency graph have fewer dependencies, but nodes nearer the top have more dependencies. Cyclical dependencies aren’t allowed. In figure 6, var.words is a node with no dependencies; in contrast, the root node at the top depends on everything. Execution order for an apply starts at the bottom and works its way up, and nodes with fewer dependencies are created first but nodes having more dependencies are created last. In a destroy operation the order is reversed: nodes at the top of the graph are destroyed first but nodes at the bottom are destroyed last. Finally, Terraform by default processes multiple actions in parallel (although this behavior can be overridden by setting the parallelism flag), and you can’t guarantee any ordering between nodes at the same level in the dependency graph.
NOTE: These dependency graphs quickly become confusing when developing non-trivial projects. I don’t find them to be useful except in the academic sense.
If we’re going to make one hundred Mad Libs files, the brute force, ham-handed approach’s to copy our existing code one hundred times and call it a day. I don’t recommend doing this because it’s messy, doesn’t scale well, and doesn’t work at all when you don’t know the quantity of resources needed ahead of time. Fortunately, we have better options. For our particular use case, we’ll take advantage of the optional count meta argument on resources.
Count is a meta argument, which means all resources intrinsically support it by virtue of being a Terraform resource. The address of a managed resource uses the format: <RESOURCE TYPE>.<NAME>, and if the count argument is set then the value of this expression becomes a list of objects representing all possible instances. If we wanted to read the Nth instance in the list, we could do it with square bracket notation, e.g. <RESOURCE TYPE>.<NAME>[N]. This concept is illustrated in figure 7.
NOTE: You can combine count with a conditional expression to toggle whether you want a resource to be created (e.g. count = var.shuffle_enabled ? 1 : 0)
Let’s update our code to support producing an arbitrary number of mad libs. First add a new variable named “num_files”, having type number and a default value of one hundred. Next, reference this variable to dynamically set the count meta argument on each of the shuffle_resources. Your code will look like Listing 2. Note that some older code has been omitted, but the context remains the same.
Listing 2. Contents of madlibs.tf
#A declaring an input variable for setting count on the random_shuffle resources
#B referencing the num_files variable to dynamically set the count meta argument
A Better Way to Template
We’ve already seen how to template files using the template_file data source. A more versatile and powerful way to template files uses the built-in templatefile function. This function works similar to the template_file data source, except it allows you to pass in complex types as variable values, not only primitives. The syntax of templatefile() is showcased in figure 8.
Update madlibs.tf to include a local_file resource with the templatefile function to set the content. Your code will look like Listing 3.
Listing 3. Contents of madlibs.tf
#A this is how we create 100 Mad Libs files
#B the path of templatefile() is hardcoded to alice.txt for now
#C constructing an anonymous map to use as input
#D give the file a unique name with count parameter
A couple of interesting things are going on here. First, we adjusted the local_file resource to create multiple resources using the count parameter. This allows us to create one hundred Mad Libs files, or as many as we indicate with var.num_files. Next, we set the content attribute to the result of templatefile(). The templatefile function is hardcoded to use the template at relative path: ./alice.txt, and a variables map containing the result of the shuffled lists.
The expression “count.index” is how to reference the current index of a resource when counts is set. We use the current index to select the corresponding element from the random_shuffle resources (e.g. random_shuffle.random_verbs[count.index].result). This is done to ensure that each Mad Libs file gets its own distinct pool of randomized words.
Our template file needs to be updated to reflect the changes that have occurred in our configuration code. Edit alice.txt to look like Listing 4.
Listing 4. Contents of alice.txt
At this point you could try planning and applying the code; it will generate one hundred Mad Libs files in the /madlibs directory using the Alice in Wonderland template. Why stop here though? Let’s go crazy by adding two more Mad Libs templates! After all, who doesn’t like more Mad Libs? First create a template file called observatory.txt and set the contents to Listing 5:
Listing 5. Contents of observatory.txt
Next, create another template file called photographer.txt and set the contents to Listing 6:
Listing 6. Contents of photographer.txt
Move the three template files into a new folder called “templates”, to get them out of the way. At this point we’ll need to toggle between the three templates to create equal numbers of each. Add a variable named “templates”, with the default value being a list containing the relative paths of the three template files. Next, modify the path argument of templatefile() to an expression which selects one of the values from the templates variable. Your code will look like Listing 7.
Listing 7. Contents of madlibs.tf
#A A list of template file paths to use
#B Select the element from list var.templates at the current index
The element function allows us to retrieve a single element from a list at a given index. The function wraps around, and you’ll never get an out of bounds exception because it’s the “safe” way to retrieve an element from a list. In our case, the function always evaluates to one of: templates[0], templates[1], or templates[2], in a round robin sort of fashion.
Why would I suggest hardcoding a list of file paths? This seems dumb. Unfortunately, I must crush your spirits by telling you that, with 100% certainty at the time of writing, there’s no better way to do this. By that, I mean there’s no function, expression, resource or data source which allows you to read the files in a directory and output the results as a list.This isn’t to say that it’s impossible to do this through some other means. Numerous backdoors to the Terraform runtime can be exploited; it’s all about how clever and devious you want to get. For now, I don’t think it’s worth it to go into the gritty details.
We now have the ability to create arbitrary numbers of Mad Libs files and output them in a “madlibs” folder. Our final step is to zip the files together to share them more easily. As it happens, there’s an archive_file data source that does this. It’s part of the archive provider and works by taking all the files in a source directory and outputting a compressed file. This is exactly what we want. Add the following code from Listing 8 to main.tf.
Listing 8. Contents of main.tf
#A An explicit dependency on the local_file resource
#B The name of the zip file
#C The directory to compress
This is the first time we’ve seen an explicit dependency between resources. Explicit dependencies are declared using the “depends_on” meta argument and are reserved for situations where there’s a hidden dependency between resources. The reason I’ve included it here’s because the archive_file data source must be evaluated after all the mad libs files have been created; otherwise it’d be zipping up an empty directory. Normally we express this dependency through an interpolated input argument to archive_file, but this particular data source doesn’t accept any input arguments which makes sense to set from an output attribute of local_file. Explicit dependencies behave exactly like implicit dependencies but are confusing and should be used cautiously — only when absolutely necessary.
The complete code for madlibs.tf is presented in Listing 9. Besides this code, you should also have three template (.txt) files in the templates folder.
That’s all for this article. If you want to learn more about the book, you can check it out on our browser-based liveBook reader here and in this slide deck.
Follow Manning Publications on Medium for free content and exclusive discounts.
See all (32)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Follow Manning Publications on Medium for free content and exclusive discounts.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-bitcoin-times/immutability-as-a-service-c97a7966afce?source=search_post---------71,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aleksandar Svetski
Nov 26, 2019·5 min read
Bitcoin network = “IaaS” (not SaaS)
“Bitcoin provides immutability as a service”
There are not many applications in the world that need immutability, and perhaps only a couple that need to build immutability as part of their core stack. It’s just too expensive!
Now…If we view immutability as a service — one that any application in the world can “anchor” or connect to, then we begin to reframe how we view Bitcoin, i.e.; as a broader network that settles transactions or states with value associated to them.
An example here will help.
There is NO reason (or very little reason) that any company (tech or otherwise) today needs to buy, host and maintain its own server infrastructure. It’s costly and it makes up only a fraction of what matters in their actual business. So they use a cloud-based service such as AWS.
You’ll also note that because of the economies of scale; there are only three real options:
Why?
They got in early and they poured billions upon billions into it.
Immutability is similar (but also different).
Similar because the infrastructure required to make something truly digitally immutable is extraordinary (perhaps even more than all of the combined infrastructure that AMZN, MSFT and GOOG operate), and it only makes sense that people will anchor to it as and when they need to.
Different because it’s not something that can be run by one or a few parties. A concept like immutability (and things that inherently need it, i.e.; money) are only so if broadly owned. In other words; the more distributed and decentralized the architecture and higher the number the owners, validators and nodes, the more robust, costly and therefore immutable it is. Should one (or a few) entities manage all of it; it then undermines the value proposition and defeats the entire purpose.
Immutability as a service is what will bring more economic activity to the Bitcoin network in the long run, again; similar to the internet. The internet started off as a way to connect computers at a distance, and over time (as more people used and trusted it) it evolved into this new communication network that provides data / packet routing as a service. We built everything on top — and the innovation has been extraordinary.
The next step is baking monetary value into a protocol owned by the collective, whose core tenet is absolute digital immutability. A network where you can’t turn back time (like in the real world).
All of the economic value from applications that require this feature, along with any broader monetary / banking / capital or financial applications that require an absolute guarantee of the following key functions:
Will accrue on it.
And as I’ve stated ad-nauseum, the more economic activity that occurs on and on top of the Bitcoin network, the more immutable and secure it will become. It’s compounding, it is self reinforcing, it has already hit a critical mass, and it’s now a runaway train.
There are, and there will continue to be lots of other consensus mechanisms created. Some that might work; most that definitely won’t.
They may be used on their own networks, for applications that are either private, proprietary; or for applications that don’t require an absolute guarantee of immutability and security.
I personally don’t believe any money- related or high value applications will run on their own networks (except in vain over the next as this space evolves) because networks, especially those where the broad population participate, generally converge to unity.
It’s why we largely have one internet; one set of protocols for email; why we all use AC power; why, within a particular jurisdiction; the network of language converges to one, and similarly so with money (there is one USD in USA, likewise one AUD in AUS).
In fact — we see this as the world’s become more “global”.
English hit it’s critical mass, attained the primary network effect and it’s now more functional to speak English in most places around the world.
Aside from converging to unity due to efficiency and practicality, the world can probably only sustain ONE absolute, immutable, uncensorable, secure proof of work chain — because it’s expensive!
This chain is likely (at this stage at least) to be Bitcoin.
If we had to run proof of work for everything; we’d destroy the planet (plus it assumes nobody trusts each other for anything, which is a bigger problem anyway), and;
a) If someone wants to use it as a service; they’re going to go tothe one that’s got the highest guarantee. That in itself will increase that network’s guarantee; leading to that self-reinforcing recursive effect I described earlier.
b) Furthermore; if you do have a novel, “light” consensus mechanism, that’s fast — you could in future anchor it to something like Bitcoin as and when you need to substantiate any claim or make a final judgement.
It’s this line of logic that leads me to believe most of the economic value will be swallowed up by the Bitcoin Network over the long term, not to mention the new concepts and innovations that will emerge using the ingredients of immutability and verification — like how facebook and instagram emerged from the internet.
In the next chapter, we’re going to explore the idea of Bitcoin as a new “Monetary Operating System”. Think of it like a computer operating system, eg; MacOS.
We can call it the BoS (very fitting).
https://bitcointimes.news
twitter.com
getamber.io
twitter.com
twitter.com
medium.com
twitter.com
Host: Anchor.fm/WakeUpPod, Founder @ amber.app. Editor @ bitcointimes.news. Bitcoin, Money, Psychology, Philosophy, History, Entrepreneurship & Thinking
113 
113 
113 
Facts over Fiction. Timeless, Limited edition content on Bitcoin & its broader impact on Philosophy, Liberty, Science, Society, Economics, Morality, Ethics and Life. Support us at https://tippin.me/@timelessbitcoin
"
https://medium.com/odcuriocity/oracle-iaas-crash-course-210eb4c43c2b?source=search_post---------180,"There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
A complete walkthrough for Oracle’s IaaS services.
Oracles Infrastructure as a service (IaaS) is apart of Oracle Cloud Generation 2 services that are forms of cloud computing that provide virtualized computing resources over the internet. In an IaaS model, a cloud provider hosts the infrastructure components traditionally present in an on-premises data center, including servers, storage, and networking hardware, as well as the virtualization or hypervisor layer.
Oracle Cloud is a Generation 2 enterprise cloud that delivers powerful compute and networking performance and a comprehensive portfolio of infrastructure and platform cloud services. Built from the ground-up to meet the needs of mission-critical applications, Oracle Cloud supports all legacy workloads while delivering modern cloud development tools, so enterprises can bring forward their past as they build their future.
While first-generation infrastructure-as-a-service (IaaS) and platform-as-a-service (PaaS) clouds are built on decade-old technology, Oracle’s modern, second-generation cloud is built and optimized specifically to help enterprises run their most demanding workloads securely. With unique architecture and capabilities, Oracle Cloud delivers unmatched security, performance, and cost savings. Our Generation 2 Cloud is the only one built to run Oracle Autonomous Database, the industry’s first and only self-driving database. Oracle Cloud offers a comprehensive cloud computing portfolio, from application development and business analytics to data management, integration, security, artificial intelligence (AI), and blockchain.
“aaS is an acronym for As-A-Service and refers to something being made available to a customer as a service, always in the context of cloud computing.”
Over the last few years, the trend in IT spend has been moving in the direction of As-A-Service offerings. With As-A-Service models the need for costly, protracted implementations is shifted to subscription-based technology designed to reduce upfront IT spend, enhanced client flexibility, as well as giving access to the latest technologies. All this is accomplished while the As-A-Service provider maintains its infrastructure entirely.
IaaS is only one of several cloud computing models, and it is complemented by alternative models that include PaaS and SaaS.
PaaS builds on the IaaS model because, in addition to the underlying infrastructure components, providers host, manage and offer operating systems, middleware and other runtimes for cloud users. While PaaS simplifies workload deployment, it also restricts a business’s flexibility to create the environment that they want.
With SaaS, providers host, manage and offer the entire infrastructure, as well as applications, for users. A SaaS user does not need to install anything; he or she simply logs in and uses the provider’s application, which runs on the provider’s infrastructure. Users have some ability to configure the way that the application works and which users are authorized to use it, but the SaaS provider is responsible for everything else.
with the best service-level agreement (SLA) in the business. Oracle Cloud Infrastructure is the only major cloud provider with an SLA that covers performance and manageability in addition to uptime requirements.
with the next generation of cloud infrastructure that was conceived and architected with the principle that security comes first. Oracle security is about a well-thought-out and integrated layered approach, with controls built within the application and database ecosystem to leverage the hardened cloud infrastructure designed to identify and respond to threats.
So now let us have a look at the features Oracle provides within their IaaS services.
Oracle Cloud is a Generation 2 enterprise cloud that delivers powerful compute and networking performance and a comprehensive portfolio of infrastructure and platform cloud services. Built from the ground-up to meet the needs of mission-critical applications, Oracle Cloud supports all legacy workloads while delivering modern cloud development tools, so enterprises can bring forward their past as they build their future. Oracle’s most common IaaS services being:
With other services listed below:
Worried about how much it will cost to run all these virtual networks and machines? Oracle provides an online cost estimator that estimates monthly amounts and is based on your selected Cloud Services, configurations, and dependent services. Pay as you go is billed on actual usage, prepayment is not required. Monthly flex is a fixed commitment, has a minimum monthly charge, and requires a minimum 1-year agreement — additional discounts may be applied based on commitment amount and term. Your actual price at checkout may differ from this estimate.
IaaS is not just another stroll in the park, it is most likely that you will need more training to understand and implement this into your digital environment. But don’t worry, Oracle has you covered. Follow the links below to a series of free webinar courses that will teach you how to use IaaS effectively:
Maybe you are already an avid user of cloud technologies with another provider. If you want to migrate all that data over to Oracle, you can get in contact with one of Oracle Sales Reps at +44 207 5626 827 or at the link below:
www.oracle.com
If you want to migrate from AWS to Oracle Cloud services you can go here.
For more information follow this hour-long video presented by the Vice President of Product management Vinay Kumar:
searchcloudcomputing.techtarget.com
docs.cloud.oracle.com
www.oracle.com
Still worried about implementing applications, API’s or backends? Oracle is here to help, with industry-standard cloud applications, their team of experts will make implementation more than enjoyable.
☁️ Follow to get a free 30-day trial with Oracle Cloud services. ☁️
Thank you for taking the time to read my article, if you’re looking for more posts like this, you can find me on Linkedin, Twitter, or Medium.
*All views are my own, and not that of Oracle*
OD EMEA’s quest for insightful knowledge and…
100 
Thanks to Hugh Gallagher. 
100 claps
100 
Written by
Endorsing technology with a hint of excellence
OD EMEA’s quest for insightful knowledge and thought-provoking conversations. Each month we follow curated content around tech trends that spark digital disruption, all running on Oracle Solutions. Remember, always stay curious.
Written by
Endorsing technology with a hint of excellence
OD EMEA’s quest for insightful knowledge and thought-provoking conversations. Each month we follow curated content around tech trends that spark digital disruption, all running on Oracle Solutions. Remember, always stay curious.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mess-up/saas-paas-iaas%E6%AF%94%E8%BC%83-7d98cf52b194?source=search_post---------193,"There are currently no responses for this story.
Be the first to respond.
SaaS(Software as Service):軟體即服務，像是蘋果的icloud，或是微軟的onedrive，不必擔心數據的遺失，軟體不需要經過安裝就可以使用，可以精簡到只需要一個瀏覽器就可以使用，我想office online也應該算是SaaS的一種
PaaS(platform as service):平台即服務，提供平台、解決問題的方案，可以控制上層的程式碼，而不需要管理下層的網路、伺服器或是作業系統，PaaS是SaaS的一種形態，也是加速SaaS成長的原因。
IaaS(Infrastructure as a Service):基礎設施作為服務，提供基礎的設施，我第一個想的案例是AWS提供自己的server作為基礎設施給需要使用的人作為服務。所以不用架設硬體，但是卻可以操作上面的OS、儲存資源，等等的資源。
這項服務就很適合動態調控資源，像是確定周末會有大量的客戶湧入，那就可以在週末的時候增加server的量。也可能，當需要測試新類型的產品，但是手邊沒有相關的硬體資源，就可以利用IaaS去租用配備，先進行測試，完成後再行購入相關硬體。
整理自:https://dotblogs.com.tw/007_lawrence/2017/08/21/155203
We are nobody and we always mess up.
6 
6 claps
6 
We are nobody and we always mess up.
Written by
https://www.linkedin.com/in/yu-an-ko-630aa0173/?locale=en_US
We are nobody and we always mess up.
"
https://medium.com/sonm/were-live-what-next-d7c95850285c?source=search_post---------113,"There are currently no responses for this story.
Be the first to respond.
On June 30, 2018, after long months of hard work, the SONM decentralized fog computing platform was finally launched and is ready for use!
Those of you who are in need of extra resources for solving computing tasks can now use our platform instead of renting computing power from cloud service providers. And suppliers of computing resources can now rent out the capacity of their graphic and central processors, free space on their hard disk or RAM for the processing and storage of data, network capacity.
Read more
SONM is a global fog computing platform for general purpose…
55 
55 claps
55 
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
"
https://medium.com/@deborah.martin/five-elements-of-successful-self-service-infrastructure-with-iaas-cloud-4b40309a26f5?source=search_post---------216,"Sign in
There are currently no responses for this story.
Be the first to respond.
deborah.martin
May 6, 2016·4 min read
Moving from manual infrastructure provisioning processes to self-service IaaS enables IT to deliver infrastructure to the business in minutes for faster response to needs at a lower operating cost. Enabling self-service aspects for private cloud can remove bottlenecks and errors that not only weigh down your staff and budget but also can push internal teams to look to shadow IT cloud providers that may be riskier for them to use without your guidance and expertise.
As you look to transition to self-service IaaS for your organization, consider enabling these 5 key elements. They will help ensure success and prevent gaps in meeting different needs that can prevent you from achieving the boost desired for the business:
Customers such as Northern Ireland Civil Service have made the move to self-service provisioning using private cloud solutions like HPE Helion CloudSystem. They have seen the benefits of reduced costs, IT resources freed for higher-value projects, and the ability to scale up on demand and cope with seasonal peaks that moving to automated infrastructure delivery can bring. Solutions like Helion CloudSystem can help you quickly transition to self-service IaaS by providing all the needed capabilities in a pre-configured software and hardware solution or appliance-based software to leverage your existing hardware resources. Helion CloudSystem is built on the enterprise-grade OpenStack technology foundation of HPE Helion OpenStack®. Take advantage of the 90-day evaluation software to try out the self-service provisioning capabilities for your needs. The recently announced HPE Helion OpenStack 3.0 has several enhancements that will help customers bring OpenStack technology into production in their environment with a powerful self-service provisioning experience.
The implementation of automated self-service processes enables IaaS cloud technologies to do a lot of the heavy lifting for your team — at a much faster pace. It can boost key business drivers such as accelerating testing and development cycles so you can innovate faster including enabling developers to do more real-world production environment testing themselves. Deployment of new innovative services with a high quality experience to your customers can also be accelerated to as you cut days, weeks and months of wait time out of the operations production process with automated and standardized configurations for each service. There are also the benefits of reduced IT management burden and development costs as the automation frees staff from low value manual configuration and deployment tasks, such as VM vending and scaling, to focus on higher value service innovation and performance projects.
For more information and resources on enabling IaaS for your organization, explore the rapid infrastructure provisioning use case site and check out the short how to video for other capabilities to consider.
The OpenStack™ Word Mark and OpenStack Logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation’s permission.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
2 
2 
2 
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
"
https://blog.usdedicated.com/us-dedicated-becomes-dedicated-com-cee8f7432604?source=search_post---------341,"Please stand by, while we are checking your browser...
Redirecting...
Please enable Cookies and reload the page.
Completing the CAPTCHA proves you are a human and gives you temporary access to the web property.
If you are on a personal connection, like at home, you can run an anti-virus scan on your device to make sure it is not infected with malware.
If you are at an office or shared network, you can ask the network administrator to run a scan across the network looking for misconfigured or infected devices.

Cloudflare Ray ID: 6cf76d7ce9683390
•
Your IP: 103.25.231.104
•
Performance & security by Cloudflare

"
https://medium.com/digital-leaders-uk/infrastructure-as-a-service-considerations-for-the-public-sector-eeefbb994126?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Written by Paul Timms, Managing Director at MCSA
Like most industries, public sector organisations are under increasing pressure to reduce costs and streamline operations. In a heavily scrutinised environment, reliability and value for the money are two of many key drivers to improving return on investment for IT assets. As a result, Infrastructure as a Service (IaaS) is now becoming an increasingly relevant and viable option for any serious business IT user.
With ‘as a service’ offerings expanding rapidly, IaaS provides familiar computing resources — such as vm’s, servers, storage, networks, applications and services — which can be provisioned and managed to suit changing business requirements over time.
In the past, organisations raced to explore on-demand cloud computing platforms like Amazon Web Services (AWS) and Microsoft AZURE platforms. However, we are now seeing public sector organisations that need a technically and financially stable IT platform building their strategy around pay per use models. The new generation of IaaS provision can deploy a physical infrastructure hosted either on-site or at their trusted IT provider’s data centre. This development is encouraging more public-sector IT managers to take another look at this once overlooked option.
Overcoming obstacles
Although the early ‘as a service’ offerings have helped show that a pay-per-use model has great financial benefits, it is fair to say there have been some issues around education and management of these kinds of services in the public cloud. Failure to grasp the direct correlation of management versus cost has led to many organisations incurring big public cloud bills. Indeed, with data security high on the agenda for all businesses, use of the public cloud can be an issue in itself — for many public sector organisations there is a requirement to know precisely where and how data is held.
There can also be resistance to moving from legacy IT systems to a new IaaS model. For individual organisations, certain applications will have been designed to work on certain platform in a certain way and there can be understandable reluctance to moving to an ‘as a service’ platform over which they have little control.
‘As a service’ platforms can also create staff and resourcing questions. Organisations will have people in-house who have managed the IT and underpinned the business for years and are loyal toward them. There are clear business benefits to keeping such skills in-house or at least re-focusing on the new wave of applications and services which could add value to the core business function. Having an infrastructure foreign to those people is not ideal.
However, introducing IaaS based on a physical configuration that is known to the customer can overcome these obstacles. It can be a familiar technology platform, known to be stable and flexible so that applications will continue to work for the period of the service. A public sector organisation can choose to have its services on site or in any number of data centres across the country, depending on preferences.
Benefits of IaaS
Public sector organisations seeking to futureproof their IT infrastructure in an ever-changing marketplace where expenditure is closely scrutinised are turning to IaaS in increasing numbers. The drive to ‘do more with less’ means that budgets to replace ageing hardware may not always be available, while the pressure to adopt new technologies in order to improve public services and operational efficiencies continues to rise.
Cost saving
One of the main reasons why IaaS is so appealing to the public sector is its cost saving potential. It is a consumption-based model where organisations pay only for what they use, thereby avoiding large fixed monthly or annual fees for services they may not use. It is the natural progression for an organisation who sees a benefit of not investing all their budget on new IT infrastructure on day one, without a chance of seeing the full benefit realised until day 1000.
Budgeting
IaaS is a good fit for organisations that recognise the benefit of spreading the cost of IT over time. By shifting IT infrastructure spend from capital expenditure to monthly operational costs, organisations get greater stability and visibility on their IT costs.
Scalability
Another key benefit of IaaS is the ability to scale up and down quickly in response to a public sector organisation’s requirements. This on-demand scalability provides added flexibility and greater agility to respond to changing opportunities and requirements.
Business continuity
With cyber attacks targeting critical public services never far from the headlines, business continuity is high on the agenda across the public sector. Organisations will often have several disparate locations with different technologies, disaster recovery and business continuity plans, making management very difficult. IaaS facilitates a consolidated disaster recovery approach, reducing costs, improving manageability and maximising system up-time for employees from wherever they happen to be.
Operational improvement
IaaS sees public sector organisations engage in a closer relationship with a trusted IT provider to underpin their IT in a world where it is critical to operations. Ultimately, free from the day to day burden of managing and maintaining IT infrastructure in the first place, the in-house team can focus on looking at how the current world of IT can help and improve their organisation.
The IaaS model also means that organisations can refresh systems more quickly and upgrade to new technologies more easily. IaaS providers generally have the latest, most powerful storage, servers and networking technology to accommodate the needs of their customers.
The new IT infrastructure purchasing model reflects the fact that the subscription economy is evolving. As consumers we are becoming used to paying for things on a monthly basis — mobile phones, cars and streaming services, to name but a few. Now IT service providers are reacting to this change in buyer behaviour with an increasing number of ‘as a service’ offerings, and forward-thinking public sector organisations are beginning to recognise and exploit the benefits when it comes to futureproofing their IT infrastructure. A ‘cloud first’ offering, IaaS has now matured into a viable option for public sector IT teams to consider when developing a sustainable, stable, secure and scalable future IT strategy.
Originally published at digileaders.com on March 9, 2018.
Thoughts on leadership, strategy and digital transformation…
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
"
https://precipitation.io/acronyms-bad-8b63c8d3bffb?source=search_post---------66,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
It’s time to drop IaaS, PaaS and SaaS as broad definitions for cloud. They create false equivalence, confuse the market and get hijacked by marketeers*.
Whilst the NIST Definition of Cloud Computing attempts to describe three delineated service models for cloud with clear boundaries, it unfortunately describes three delineated service models for cloud.
The problem of course in reality the actual service offerings from cloud providers span the boundaries of IaaS, PaaS, and SaaS.
The current confusion of definitions is often evident in the Container as a Service (CaaS) space where there doesn’t appear to be a clear definition of what CaaS is, never mind whether it sits in PaaS or IaaS, and the continual comparison to Function as a Service (FaaS) platforms.
We see vendors offering different services for the same core technology, with the differentiation being on the amount of management provided.
In the CaaS world is the difference between an IaaS CaaS and a PaaS CaaS just the amount of management? Is a PaaS CaaS just a more managed IaaS CaaS?
The definition of PaaS, “The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment”, is misleading because it does not describe the degree of control the consumer has, and nature of the application being deployed.
In the case of Azure Container Service (ACS), the IaaS CaaS, it doesn’t quite meet the PaaS definition as you some level of control of the network and storage, and doesn’t meet the IaaS definition, as you cannot run arbitrary code on it, but are limited to specific container cluster managers.
How does all of this delineate from a Functions as a Service (FaaS) platform that uses containers under the hood but doesn’t use it in it’s marketing material?
By the NIST definition, FaaS services could easily be described as a SaaS platform, though no-one tries to define it as such. You could call FaaS a SaaS CaaS as it’s just a more managed form of CaaS?
The CaaS IaaS PaaS SaaS identity crises is just a glimpse of why trying to create delineated definitions for overlapping service models just confuses the market.
We need to stop using IaaS, PaaS, and SaaS to define the services and platforms we use or deliver, but rather focus on more specific definitions that clearly highlight service boundaries rather than lazy acronyms.
Ant
*marketeer: Like a buccaneer, but with less swashbuckling, and more exaggeration of loosely grasped topics. Dresses like the cover of a Manning book.
When playing with clouds, expect to get wet…
13 
1
13 claps
13 
1
Written by
Figuring it out…
When playing with clouds, expect to get wet…
Written by
Figuring it out…
When playing with clouds, expect to get wet…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://becominghuman.ai/how-mlaas-impacts-businesses-a5fe8eccfec9?source=search_post---------364,"There are currently no responses for this story.
Be the first to respond.
Over the past few years, multiple services have emerged, including Platform as a Service (PaaS), Infrastructure as a Service (IaaS) and Software as a Service (SaaS). There is intense competition in the cloud space market with Machine Learning as a Service (MLaaS) that also offers a competitive alternative. The growing trend of shifting data storage to the cloud, maintaining it and deriving the best insights has found an ally in MLaaS that provides these solutions at a lower cost.
MLaaS is a set of services that incorporate machine learning tools into cloud computing services. By using machine learning, organisations benefit without incurring the costs, time, and risk of establishing in-house machine learning teams. In addition, MLaaS can mitigate infrastructure concerns such as data pre-processing, model training, and model evaluation, resulting in accurate predictions.
In a nutshell, MLaaS provides ready-made, generic machine-learning tools that any organisation can adapt to meet their particular working needs. Among them are data visualisation, an array of APIs, facial recognition, natural language processing (NLP), predictive analytics, and deep learning. MLaaS algorithms are used to find patterns in data. By using these patterns, mathematical models can be built and used to predict using new data.
As a result, organisations do not need to perform the computation themselves. It is managed remotely by the providers’ data centres. MLaaS is the full-stack AI platform that consolidates mobile applications, enterprise information, industrial automation and control, and advanced sensors like LiDAR (Light Detection and Ranging). It combines pattern recognition with probabilistic reasoning, which provides a complete and sound ML solution that can be uniquely customised to meet the company’s needs using various methods.
1. Why Corporate AI projects fail?
2. How AI Will Power the Next Wave of Healthcare Innovation?
3. Machine Learning by Using Regression Model
4. Top Data Science Platforms in 2021 Other than Kaggle
MLaaS can be supported by algorithms like convolutional neural networks (CNN), deep neural networks (DNN), Bayesian networks, probabilistic graphical models, Restricted Boltzmann Machines (RBMs) and pattern recognition. Cloud providers like Microsoft, Amazon and IBM, among others, offer MLaaS tools.
Also Read: How AI is Improving Predictive Analytics
Many industries have already embraced MLaaS. The technology is used in many processes, including risk analysis, fraud detection, manufacturing, supply chain optimisation, network analytics, marketing, advertising, predictive maintenance, and inventory management optimisation. The application can be used across various industries, such as healthcare, banking, financial services and insurance (BFSI), transportation, retail, manufacturing, and telecommunications.
Also Read: Using AI effectively
Since SMBs lack the infrastructure to store large volumes of data and resources to manage it, as it is expensive to invest in storage facilities, MLaaS provide ascendable and customised technology and offer the ability to choose specific services suitable to an organisation’s needs and relieves them from building their infrastructure.
Companies can gain a competitive edge by utilising ML technology and the computing capacity provided by MLaaS. They can take advantage of similar services offered by more established and larger competitors without worrying about sophisticated and large scale ML and data needs. In addition, MLaaS provides the company with faster insights, enabling better and quicker decision-making.
Machine learning is merely an old concept with renewed interest in the area over the last decade, and the progressive transition of all services to the cloud makes MLaaS a relevant tool of the future. Amazon’s Amazon ML, Microsoft’s Azure ML, IBM’s Watson and Google Cloud ML are leading MLaaS service providers.
With data and its engagement moving to the cloud, MLaaS is set to revolutionise machine learning and create a synergistic result. According to a study, the MLaaS market will experience 49 per cent growth during the forecast period 2017–2023.
MLaaS will also drive innovation in IoT. By 2025, the IoT will consist of over 30.9 billion pieces of equipment (excluding PCs, tablets, and smartphones). Due to its ability to integrate with various sensors, MLaaS could also play a vital role in that area.
Latest News, Info and Tutorials on Artificial Intelligence…
51 
Watch AI & Bot Conference for Free Take a look.
51 claps
51 
Written by
Writer | Marketer | Storyteller | Coffee Lover
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
Written by
Writer | Marketer | Storyteller | Coffee Lover
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.edwardkuo.dev/%E7%82%BAazure-expressroute-%E8%A8%AD%E5%AE%9Arouter-filter-76da82e9305e?source=search_post---------391,"在Azure ExpressRoute可以分成兩種，一種是Azure Private Peering，另一種是Microsoft Peering，前者主要是讓地端與雲端IaaS相連接，例如透過地端VM連線到雲端的VM，這種方式基本上只要設定好ExpressRoute (需要和電信商搭配)，就可以直接連線。而後者則是地端與雲端的PaaS或SaaS直接做連線，當然前提還是一定要先設定好ExpressRoute。
什麼是ExpressRoute:
ExpressRoute 線路代表您的內部部署基礎結構與 Microsoft 雲端服務之間，透過連線提供者的邏輯連線。 您可以訂購多條 ExpressRoute 線路。 每一條線路可以位於相同或不同區域，且可透過不同的連線提供者連線到您的內部環境。ExpressRoute 線路不對應至任何實體裝置。 線路由一個稱為服務金鑰 (s 金鑰) 的標準 GUID 唯一識別。 服務金鑰是 Microsoft、連線提供者與您之間唯一會交換的資訊。 S 金鑰不是安全性用途的密碼。 ExpressRoute 線路與 s 金鑰之間存在 1:1 對應。
在Microsoft Peering部分，設定好ExpressRoute，其實還不夠，這時候還必須設定Route Filter。
什麼是Route Filter:
當 Microsoft 對等互連在 ExpressRoute 線路上設定時，Microsoft 邊緣路由器會建立一組 BGP 工作階段與邊緣路由器 (您或您的連線提供者)。 沒有路由會公告至您的網路。 若要讓路由公告至您的網路，您必須建立與路由篩選的關聯。路由篩選可讓您識別想要透過 ExpressRoute 線路的 Microsoft 對等互連使用的服務。 這基本上是您想要允許的所有 BGP 社區值的清單。 一旦定義路由篩選資源，並且連結至 ExpressRoute 線路，對應到 BGP 社群值的所有前置詞都會公告至您的網路
如果沒有設定好這部分，再測試時候可能就不會通
設定Route Filter
可以在Marketplace找到Route Filter的服務
在這邊的區域，最好是選擇電信商的節點所在位置，以CHT來說，其節點位置位於香港，所以區域就要選擇東亞
當Filter建立好時候，必須新增一個ExpressRoute的線路，在這邊可以設定多個線路，要設定多個線路，對應的電信商會好是不同一家，這樣可以做BGP備援
而新增線路完畢後，再來就是設定管理規則 ，管理規則部分細分很多Azure 服務區域的資料中心以及和儲存資料相關的服務，主要目的在於，可以限制地端連線到其他區域資料中心內或是避免碰觸到一些服務
如果是剛開始設定，會建議先把目前有在Azure用到的服務區域的資料中心都設定進去。之後再做慢慢刪減，這樣會比較安全一點，畢竟，在測試階段很多很難預料的事情都會發生的。此外，像是同樣設定Azure East Asia 後，我還會多設定一組Azure Storage East Asia ，雖然我們認知East Asia內就因該會包含Storage East Asia，但實際上並沒有，從下圖的BGP值部分，就可以看出有些差異
以上設定完成後，就可以開始測試地端與雲端PaaS的通訊了
Design,Thinking,Coding & have fun every thing
Written by
Enterprise IT Manager / Microsoft Regional Director / Microsoft MVP / DevOps Expert / Speaker, About me: https://profile.edwardkuo.dev/about/
Design,Thinking,Coding & have fun every thing
Written by
Enterprise IT Manager / Microsoft Regional Director / Microsoft MVP / DevOps Expert / Speaker, About me: https://profile.edwardkuo.dev/about/
Design,Thinking,Coding & have fun every thing
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mikethebbop/was-oracles-most-recent-quarterly-earnings-report-a-typical-scorecard-on-enterprise-business-52d92210c44f?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ira Michael Blonder
Nov 22, 2018·4 min read
On September 17, 2018 Oracle Corporation reported quarterly earnings for Q1 fy 2019. Despite a beat on profit estimates Oracle’s stock was hammered after hours and for a few trading sessions thereafter. The reason? on paper a $117M miss on revenue estimates.
Cloud revenues were effectively flat, up a mere 4% y/y. Safra Catz, CEO tried to “paint some lipstick on a pig” by pointing out the central role cloud offers play for Oracle’s current business: “ [cloud] accounted for 72% of total Company revenue and the bulk of it is recurring revenues.” (quote excerpted from a transcript of Oracle Corp Q1 fy 2019 earnings conference call. The transcript has been published on the Seeking Alpha website).
Given the trading action on Oracle’s stock post earnings call and after, big investors weren’t impressed by Ms. Catz’s effort to repackage cloud numbers.
Is Oracle alone? Maybe not.
Should investors compare Oracle’s paltry cloud revenue performance against industry leaders Amazon AWS and Microsoft’s “one two cloud punch”-Productivity and Business Processes & Intelligent Cloud”? Can’t hurt.
Perhaps a lot of other “SaaS as a platform” offers aren’t doing so well either. Let’s take a look: the clear cloud revenue leader is Amazon AWS. The hottest AWS product offer looks like S3. Amazon promotes S3 as a “simple storage service”. Let’s call S3 an IaaS offer.
Switching over to Microsoft’s cloud offers, what are the hottest products? Short answer? It’s hard to say. There is little published opinion from independent analysts on what drives enterprise business to subscribe to Office 365. For discussion purposes, let’s call it a love of Office Pro Plus. Why? Because Office Pro Plus is an annual subscription version of the bellweather, ever-most-popular desktop productivity suite for enterprise class organizations-Microsoft Office.
No other package of office automation tools comes close to the extent to which Microsoft has penetrated the office market for automation. Remember: Lotus 1–2–3 is gone, Wordperfect is gone, OpenOffice never established much of a beach head. Office automation is all about Microsoft Office and has been like this for years.
Google G Suite may change the story, but we need to stay tuned a bit longer to see if they can really pull it off. A story titled “Airbus ditches Microsoft, flies off to Google” written by Paul Kunert & published on The Register website is worth a read. Mr. Kunert reports “[Alphabet] confirmed it had four million paying punters on G Suite.” But compared to the “…29.2 [paying Microsoft Office subscribers] … according to [Microsoft] CFO Amy Hood.” there is still a very long way for G Suite to go if Alphabet is to win this race. (quotes excerpted from Mr. Kunert’s story as published on The Register web site. A link has been provided in this paragraph)
The other apps in the Office 365 SaaS platform haven’t magnetized the same widespread usage. Even OWA (the latest name for a service FKA “Outlook Web Access”) is not likely to be getting the same attention as the Outlook program included with Office Pro Plus. We have no real data on the extent to which users are engaging with the other apps in the Office 365 productivity suite.
Sure Office Pro Plus is a SaaS offer, but since it is built to hasten the pace at which enterprise class organizations transition from purchasing perpetual licenses to purchasing annual subscriptions, can we really say the 30% + y/y growth in Office 365 annual subscriptions for Microsoft’s most recent quarter represents growth (in other words real expansion) in the number of global Office users? Or are the same folks just switching hats? The answer isn’t clear.
Getting an answer to what’s driving big organizations to Microsoft Azure isn’t any easier. But for our discussion purposes let’s assume it’s Azure Storage-a direct competitor to Amazon AWS S3. Why make this assumption? Because a lot of the talk about other revenue streams-AI, IoT, Developer Tools, etc. are hard to get one’s hands around. The IaaS story, on the other hand, is a lot easier to swallow from a credibility perspective.
Why isn’t Oracle able to magnetize the same numbers of annual subscriptions for its SaaS offers as, for example, Microsoft looks like it’s doing? Because Oracle doesn’t own a globally popular, ubiquitous, “must have” desktop application suite comparable to Microsoft Office. Big relational databases are certainly “mission critical” but they carry a much higher price tag. There simply aren’t enough enterprise class customers out there to drive comparable growth in numbers of units sold.
Oracle also has 3 big competitors: Microsoft, IBM, and SAP. All three offer direct competitors to Oracle’s SaaS offers. But they still are the leader in the enterprise database market. “Oracle confronts a growing challenge from AWS in the cloud” is worth a read.
What’s the impact if Oracle’s quarterly report is really a tip of a bigger and more diverse iceberg? SaaS platforms are a tough sell and don’t look to become easier anytime soon. If the last two months of trading activity are indicator, big investors seem to have gotten the message. Even Microsoft has lost a bit over 10% of its market cap since October 1, 2018.
Note: I have not discussed in this story the public controversy Oracle provoked back in June of this year by announcing changes in how it would report cloud product sales, going forward. For my purposes, the change in reporting had no impact on my opinion. So I chose not to speak to it.
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/management-tech-bites/paas-vs-iaas-vs-running-your-own-server-network-a-management-perspective-9e5b84250fa0?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jens Kuerschner
Apr 24, 2020·6 min read
Cloud: The final frontier. These are the voyages of the starship [put your company’s name here] …
No matter if you see the “cloud” as some mysterious concept, a technological masterpiece, or just someone else’s computer: It enables new opportunities for basically every business.
However, this is not about the cloud, but about the basics of how you can host your application, tooling, website, and basically every code, which needs to run somewhere.In the end, it is always a server. But thanks to modern cloud offerings, you have exciting choices from an operations perspective.
This article is going to introduce you to the three most important hosting concepts at this time. It also provides you with a scheme to guide your decision.
Comparing differences, mind those three examples:
“Wild wild west.”
This is the traditional way. Running one or multiple servers right under your desk or at your own data center.
It requires at least one person, which is able to maintain not only the servers itself, but also the whole infrastructure around them. This includes carefully connecting each machine to each other and the overall network, making sure there is no backdoor open for any attacker, while also taking care of physical security (from fire extinguishers to physical backup protocols).
“Driving down the road.”
Think about the above, but outsourcing the whole physical part to an external service provider.
This can be one of the leading cloud hosters (AWS, Google Cloud Platform, Microsoft Azure) or one out of many smaller service providers (e.g. hetzner in Germany, or DigitalOcean).Basically, having your server somewhere else, paying another company to take care of its connection to the internet and the physical security — that’s IaaS.
You save on the “server janitor”, who takes care of the hardware part. But you still need experienced system administrators to manage the server on the operating system level.
“The rental car.”
This is some new development, mainly driven by Google (GCP) and Microsoft (Azure). You no longer need to take care of any infrastructure or platform issues (only if you want to). Hundreds of experts make sure your application can run on a stable, reliable, and secure system. You can automate any scaling and even do not need to think about complicated backup and distribution setups.
The only thing, you need to do: Push your application to the system, and see it just working. In some cases (e.g. AWS Beanstalk), you still need to think about the webserver, but that’s it.
Complexity of your service: In my opinion, this usually does not make a difference. You find PaaS offerings, which even cover satellite communication.
Security: There are cases, where you should not put your data and operations on an infrastructure, which is operated by a company from another country. Still, from a tech (and even hacker or intelligence) perspective, this is only critical for intelligence services themselves or governments. For basically every other company, it might be even riskier to have an own network with maybe more critical loopholes in place.
This is a high-level overview. It does not consider more detailed differences of various offerings. Two things might immediately pop up in the heads of more experienced people.
Web hosters often provide “shared hosting” packages. This basically means, that there is one server, which is used by many of their customers. The server, therefore, is virtually divided. Those offers usually limit you in what software you are allowed to run and might provide poor performance at times of high traffic. Still, those plans could also be seen as “pseudo PaaS”, since the hoster takes care of the operating system. It can be a good alternative for small websites, if this is your only web application.
Bear in mind, that cloud hosting is also usually “shared hosting”. However, there, it is not about simply dividing one server into many parts, but more about virtualizing the server itself and distributing compute resources to those virtual servers.
There are ways, you can minimize effort of managing the operating system part or web server as well as scaling and more. For example, working with containers (Docker, Kubernetes) became quite popular in the last years. I do not want to go into detail here. Of course, you can automate many parts up to basically building your own PaaS. However, this requires highly skilled and always available engineers. If you already employ them, if you can be sure to always have backup resources, and if this is cheaper than respective offerings, awesome.
This was a short intro to the different types of hosting. For a deeper dive and discussion, consult your engineers or respective tech consultants.
Tech Founder, Leader, End-to-End Product/Program Manager, Full-Stack Developer, Marketing and Digitalization expert. 🚀 https://jenskuerschner.de
1 
1 
1 
Management Tech Bites brings you smart & handy insights for technology decisions from a business perspective. You are a manager or consultant? Have a look.
"
https://medium.com/workday-engineering/openstack-at-workday-72087c75e73?source=search_post---------102,"There are currently no responses for this story.
Be the first to respond.
By Edgar Magana, Senior Principal Software Development Engineer, Workday
Workday was founded on a disruptive idea: to put people at the center of enterprise software. Now, over a decade later, we still deliver on that idea every day. To enable this promise, engineering leverages disruptive technologies and design patterns across our entire technology stack.
This post will go into the details of our innovation journey in building and managing infrastructure-as-a-service (IaaS) to cover both public and private cloud deployments.
The software industry has long started down the path to provide on-demand access to services and resources with minimal effort. Earlier, there was no open source solution available to manage compute clusters with over 100K cores. And there was a clear need for a cloud management system that could scale and provide open interfaces to all of these different businesses. In response to these needs, developers from NASA and Rackspace jointly launched an open-source cloud-software initiative known as OpenStack.
OpenStack is an open source set of software tools for building and managing cloud computing platforms for public and private clouds. It is most commonly deployed as infrastructure-as-a-service, where resources are made available to the end-user via APIs.
In addition to the gap described above, Workday had to maximize our consumption of all data center resources to avoid vendor lock-in and provide a complete level of automation via open application programming interfaces (APIs) to prevent configuration errors. We set out to revamp our original infrastructure stack to provide IaaS. We built an abstraction layer to allow application or microservice teams to deploy their work without having to worry about technology shifts in the underlying infrastructure.
Workday recently published a case study in collaboration with the OpenStack Foundation to share lessons learned and best practices for deploying, operating, and scaling OpenStack. From an engineering perspective, it has been a fast and thoroughly enjoyable ride. Our engineering team faced a steep learning curve around all our OpenStack projects. However, once we gained some momentum, the fun never stopped. Our efforts were validated when Workday was selected as one of the top five super user finalists during the OpenStack Summit in 2016.
The greatest gain from contributing to the OpenStack developer community was our collaborative experience working with developers from all over the world. It means a lot to our developers to have the support from Workday to contribute to open source projects. Our team has contributed in many areas of OpenStack, including networking (Neutron), identity (Keystone), compute (Nova), testing (Tempest), performance (Rally), and configuration management (Chef).
Workday has deployed OpenStack in five data centers for both development and production environments, providing over 50,000 cores ready to use. We have also scaled up tools around this platform to support these cores. We have more than 1,000 different monitoring metrics and alarms in place to make sure we have all the visibility needed to keep the system running properly. Another key factor in our success with OpenStack is our Continuous Integration and Continuous Deployment (CI/CD) pipeline. Our Jenkins and Gerrit integrations trigger a new OpenStack deployment each time one of our developers makes a change in our configuration or architecture. Our pipeline prevents unintentional changes from going to production for all OpenStack and SDN components. In the graph in Figure 1, we show the number of builds performed by our CI/CD pipeline and the elapsed time for each build over a period of 30 days. During this 30-day period, the system ran over 800 builds preventing three major failures. It has given us a very reliable pipeline for our OpenStack deployments.
We are currently working on a new version of this platform, uptaking newer versions of both OpenStack and OpenContrail. The team plans to scale up these clusters to four times more capacity, expanding to 200,000 cores. This is not an easy task, but we’re well on the way to achieving this goal by next year.
OpenStack has demonstrated its ability to be the go-to solution for open source IaaS software. However, we did not stop there. Service and application teams’ code is now deployed not just within VMs but also as containers. Developers love the idea of delivering their code in containers because they can specifically focus on features and services rather than the implications of running that code on different platforms. We have put the automation in place to deliver services and applications as software packages, or directly as a container, and all options work seamlessly in OpenStack.
Our engineering team is committed to engaging with the OpenStack community. We recently attended the OpenStack Summit in Sydney. Our team moderated a few forum sessions, and presented our work on running control plane operations as containers and performance results for OpenStack in production. But we won’t stop there. We are interested in new projects and continued iteration with open source solutions such as Kubernetes and Calico. Be on the lookout for more information in future posts.
The Workday Technology Blog is a collaboration of…
160 
160 claps
160 
The Workday Technology Blog is a collaboration of engineers, product managers, and designers at Workday to share and discuss best practices, lessons learned, and innovative solutions for the next wave of technology products.
Written by
Please visit our career site at https://www.workday.com/en-us/company/careers.html
The Workday Technology Blog is a collaboration of engineers, product managers, and designers at Workday to share and discuss best practices, lessons learned, and innovative solutions for the next wave of technology products.
"
https://medium.com/@ariya114/mengenal-iaas-vs-paas-vs-saas-menghindari-salah-kaprah-teknologi-cloud-5358dd843312?source=search_post---------4,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ariya Hidayat
Jan 13, 2020·7 min read
Karena dangkalnya pemahaman sesungguhnya soal teknologi komputasi di awan, atau yang lazim dikenal sebagai Cloud Computing, banyak pengamat dan pengguna teknologi yang malah terjerumus ke beragam kesalahpahaman tentang pemakaian Cloud. Mana yang lebih murah? Apa yang lebih aman? Harus pakai cloud yang mana? Dan rentetan kepeningan lainnya.
Hampir 99% kejadian, kalau ada tim/organisasi/perusahaan yang mengklaim telah “hijrah ke cloud”, yang dimaksud adalah sudah menggunakan Infrastructure-as-a-Service, disingkat IaaS. Namun demikian, masih banyak dan luas lagi ranah pemanfaatan cloud, termasuk juga Platform-as-a-Service (PaaS) dan Software-as-a-Service (SaaS). Bagaimana kesamaan dan perbedaan ketiganya?
Yang jelas, karena terhimpun dalam keluarga “cloud”, ketiganya punya karakteristik yang sama: yakni menjadi cara untuk menghantarkan sebuah layanan (service) melalui Internet publik (yang dikonotasikan dengan “cloud”). Contohnya, dulu tatkala kita mesti menulis laporan, meramu lembar kerja, atau menyusun slide untuk presentasi, andalannya adalah Microsoft Word, Excel, and PowerPoint (dipaket sebagai Microsoft Office) yang diinstal di komputer masing-masing. Akan tetapi, di zaman now, sebagian besar dari kita lebih memilih untuk menuangkan kreatifitas kita melalui Google Docs/Sheets/Slides yang semuanya notabene diakses secara online melalui layanan Google. Bisa dikatakan Google Docs ini adalah manifestasi dari paket Office ala Cloud. Tidak perlu lagi memburu dan memasang aplikasi secara lokal.
Dalam konteks IaaS/PaaS/SaaS, kesamaannya adalah tidak adanya infrastruktur fisik yang mesti dikelola oleh kita sendiri. Lawannya adalah “on-premises”, ditandai dengan aplikasi yang jalan di server fisik, biasanya diletakkan di ruang khusus (server room) atau pusat data (data center), dan disambungkan ke Internet. Di jaman sebelum ada teknologi cloud, inilah yang harus kita kerjakan: beli CPU dan disk, pasang sistem operasi, koneksikan ke Internet, dan konfigurasi aplikasinya.
Pun dalam keseharian, ada pekerjaan untuk memantau server tersebut, baik untuk memastikan bahwa perangkat keras sehat wal afiat (tidak kepanasan, kabel masih tertancap baik, dll) sampai sistem operasinya juga terus lancar (terus dimutakhirkan, tidak kekurangan swap, dll). Bila terjadi masalah, dari RAM yang jahil atau hard disk yang jebol, mesti diluangkan juga waktu dan tenaga untuk membenahinya.
Apakah bedanya IaaS dengan PaaS ataupun SaaS? Paling mudah kita lihat ilustrasi analoginya, Pizza as a Service, buah karya Albert Barron yang bekerja untuk IBM.
Dalam proses perjalanan sepotong pizza, dari awal hingga masuk ke lahapan kita, maka “on-premises” dapat digambarkan sebagai kegiatan yang dikerjakan sendiri, dan benar-benar dari awal. Kita mesti membuat adonan dan menambahkan saos tomat, keju, serta topping lain yang dikehendaki. Terus kita harus panggang sendiri juga (perlu oven dan api). Baru akhirnya menghidangkannya (lagi-lagi sendiri), barangkali juga bersama air limun segar sebagai pelengkapnya.
Sementara itu, IaaS, bisa dilukiskan sebagai modifikasi besar-besaran dari mekanisme on-premises, yaitu dengan pizza mentah yang sudah disiapkan oleh sang vendor (atau yang bisa dibeli di swalayan, seringnya masih beku). Memang potongan pizza tersebut belum matang. Adalah masih menjadi kewajiban kita untuk memanggangnya (masih perlu oven dan api/gas) serta menghidangkannya (meja, minuman pelengkap). Akan tetapi, bisa dilihat di sini bahwa kerja kita sendiri sudah sangat berkurang karena keribetan mengolah adonan pizza sudah bukan tanggung jawab kita.
Bagaimana bila pizza lezat ini dihantarkan dengan model PaaS? Dalam kasus ini, artinya pizzanya sudah jadi, alias tinggal dimakan. Entah caranya bagaimana, bisa nelpon pizzeria lalu minta diantar, atau ala PhD (Pizza Hut Delivery), ataupun dengan GO-FOOD, Grab Food, dan sebangsanya. Kerjaan kita minim sekali, hanya tinggal menghidangkan pizza tersebut sesuai selera dan langsung melahapnya (dan bersih-bersih, begitu sudah kelar). Tidak ada kerepotan bikin adonan dari nol. Tidak perlu punya oven, microwave, kompor, ataupun alat masak lainnya.
Akan halnya dengan SaaS, ini diibaratkan dengan makan di luar. Tidak perlu ada pengantaran. Kita cukup pilih kedai makan favorit, pesan dari menunya (misalnya pizza tersebut, bersama minuman pelengkap), lalu nikmati sajiannya. Sebagai bonus, kadang juga ada live music, baik dalam bentuk pengamen jalanan ataupun musisi serius, sebagai penghidup suasana. Selesai makan? Bayar tunai atau gesek kartu. Beres dah!
Di sisi lain, ada juga kombinasi lain yang juga ajib, seperti misalnya hybrid cloud atau malah multicloud (kita akan ulas di lain kesempatan).
Jawabannya: tergantung situasi. Tidak bisa dipukul rata.
Contohnya begini. Kalau Anda ingin makan nasi goreng (kearifan lokal: beralih dari contoh pizza di atas), murah mana antara bikin sendiri di dapur atau beli dari tukang nasgor. Dilihat dari harga bahan-bahannya, jelas lebih menghemat kala masak sendiri dong. Tapi terkadang, masak sendiri tidak memungkinkan. Barangkali Anda tinggal di kamar kost tanpa dapur. Atau juga, belum punya keahlian memasak nasi, apalagi meracik bumbu maupun menggorengnya hingga sedap. Atau malah, sedang buru-buru sehingga tidak bakal sempat untuk masak sendiri. Karena berbagai alasannya ini mungkin Anda lebih memilih pesan nasgor dari tukang nasgor di depan jalan atau lewat pengantaran GO-FOOD/Grab Food.
Apalagi untuk Anda yang sibuk sekali dan senantiasa dikejar waktu. Waktu Anda mungkin jauh lebih berharga untuk difokuskan ke pekerjaan, bercengkrama dengan keluarga, ataupun kegiatan yang lain. Walaupun perlu merogoh saku untuk membeli nasgor, dari perhitungan TCO (Total Cost of Ownership), masih lebih menguntungkan.
Makanya, kalau memikirkan soal harga murah atau nggak, tidak melulu hanya masalah harga beli. Ada juga ongkos waktu, pemeliharaan, dan komponen lainnya. Perhatikan TCO itu tadi.
Di sisi lain, walaupun masak sendiri itu menghabiskan ongkos lebih murah, belum kita dengar ada pasangan pengantin yang menyiapkan sendiri seribu porsi makan untuk tamu kondangannya. Selain sibuk dengan persiapan tetek bengek, tentu ada faktor scaling yang menyebabkan kemustahilan pasangan pengantin tersebut memasak untuk ribuan tamu kehormatan (gimana, kebayang capeknya nggak?). Di sini, lagi-lagi ada peran catering, sejalan dengan konsep SaaS, sehingga si mempelai (atau panitia yang didelegasikan) cukup membayar vendor untuk meramu selusin rupa-rupa hidangan, bahkan lengkap dengan alat makan, pelayanan, urusan kebersihan, dll. Tidak perlu pusing. Lebih mahal daripada masak sendiri? Sudah pasti. Mau mengerjakan sendiri? Jangan harap.
Orang yang benci on-premises tapi punya kendaraan sendiri itu berarti nggak konsisten (lho kenapa nggak selalu pakai ojol dan taksi, transportasi-as-a-Service?). Sama juga, mereka yang 100% bermusuhan dengan SaaS tapi masih suka beli kopi/Indomie/nasgor juga nggak taat doktrinnya sendiri (harusnya senantiasa masak sendiri dong!).
Jadi kalau ada pihak yang secara absolut sangat benci on-premises atau ada kubu lain yang 100% bermusuhan dengan cloud, jangan didengar. Pendapat seperti itu amat berbahaya dan menjerumuskan.
Jawabannya pun serupa: tergantung situasi. Tidak bisa dipukul rata.
Contohnya begini. Anda bernasib mujur dan memperoleh uang selembar 20 ribu. Bakal disimpan di dompet saja atau langsung disetor ke bank? Sebagian besar dari kita akan menyimpan lembaran tersebut di dompet saja. Ribet amat kalau hanya uang segitu terus harus repot ke bank segala. Nggak worth it, cetus anak millennial.
Lain halnya, kalau tiba-tiba ada yang menyetorkan uang kepada Anda, 100 juta rupiah, tunai. Wuih, mana berani disembunyikan di ransel lama-lama (apalagi di saku atau di dompet atau di bawah bantal). Pastinya lekas kita bergerak mencari cara untuk menjebloskan rejeki nomplok ini ke rekening bank. Gercep dong. Itupun sambil was was, siapa tahu ada penyamun di siang bolong manakala kita lagi beranjak ke kantor cabang bank tersebut.
Menyimpan data di sistem on-premises atau IaaS memberikan beberapa kelebihan, dari kontrol akses fisiknya hingga kesempatan kita menggunakan berbagai macam teknologi keamanan (kriptografi, penyandian data dari hulu ke hilir, dan lain sebagainya). Tetapi tidak selalu mutlak seperti itu, karena semua pekerjaan ini perlu ditangani ahlinya. Salah menggunakan algoritma kriptografi bisa berakibat fatal. Tidak melakukan analisis pemodelan ancaman (threat modeling) dapat menghasilkan ilusi keamanan. Ada mekanisme key rotation? Lantas, adakah review secara berkala untuk memastikan kalau semua personil yang terlibat tidak mudah ditipu oleh kampanye phishing atau social engineering lainnya? Dan selusin detil-detil lain yang mesti diperhatikan.
Sementara itu, kalau kita delegasikan urusan keamanan tersebut ke tim Google/Amazon/Microsoft karena kita menggunakan IaaS/Paas/SaaS yang mereka tawarkan, ada sejumlah hal yang berkaitan dengan kriptografi yang tidak lagi bisa kita kerjakan sendiri. Walaupun semua provider di atas menawarkan fitur untuk enkripsi (misalnya via KMS, Key Management System), tidak selalu semuanya bisa dimutlakkan. Di sisi lain, kita hendak mendapatkan manfaat dari keseriusan tim mereka untuk mengurusi keamanan fisik (triple biometrics), backup via redundancy, threat modeling, review berkala, dan beragam pekerjaan SecOps lainnya.
Ini namanya trade-off. Menimbun 100 juta rupiah di bank itu berarti kita nggak akan selalu bisa pakai kapan saja. Tetapi gila dong kalau kita pikir mengamankan 100 juta rupiah itu gampang. Bank dijaga beberapa satpam. Ada brankas lapis baja juga di dalamnya. Di rumah kita sendiri? Kadang ikan asin aja raib digondol kucing tetangga tanpa ketahuan!
Makanya, kalau berkaitan dengan isu keamanan dan ada pihak yang secara absolut sangat benci on-premises atau ada kubu lain yang 100% bermusuhan dengan cloud, juga jangan didengar. Sama-sama berbahaya dan menjerumuskan.
Mau naik sepeda atau naik mobil? Nah, tergantung ke mana dong. Tidak masuk akal kalau kita pilih moda transportasinya dulu sebelum jelas arah tujuannya. Menggenjot sepeda untuk pulkam dari Jakarta ke Semarang akan menghasilkan kepenatan tiada tara. Menyetir mobil untuk ke Indomaret depan gang juga buang-buang bensin/emosi/waktu.
Mau pakai on-premises, IaaS, PaaS, atau Saas? Cermati dari apa yang mau dibangun. Jabarkan kebutuhan dan calon-calon solusinya, lalu hitung TCO (jangan cuman harga beli), dan tinggal tentukan mana yang cocok. Tidak ada satu pilihan yang cocok untuk semuanya. Jauhi fanatisme dan jangan main pukul rata. Solusi yang manjur untuk startup kecil yang baru punya ribuan user akan sangat beda dengan apa yang bakal dipilih oleh Unicorn dengan jutaan pemakai.
Yang jelas, ketika membahas tajuk cloud, selalu gunakan kesempatan tersebut untuk menguraikan dengan rinci. Jangan hanya sebut cloud, tapi arahkan ke IaaS, atau PaaS, atau SaaS. Dengan lebih detil seperti itu, segala macam kesalahpahaman dan kerancuan dapat dihindari dan diskusinya bisa mengalir lebih cerdas.
Nah, siapa yang mau tinggal di Negeri di Awan?
Nggak bisa move on dari open-source.
283 
283 
283 
Nggak bisa move on dari open-source.
"
https://medium.com/cuelogic-technologies/saas-paas-iaas-decoding-the-3-cloud-computing-service-models-25407ee1a568?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
Over the years, this technology paradigm has evolved through multiple phases. The earlier forms of computing that preceded modern cloud computing included grid, utility, and on-demand computing. The earliest forms of modern cloud computing that include Software (SaaS), Platform (PaaS) and Infrastructure (IaaS) emerged as a technological outcome that attended the dipping costs of computer and server hardware. Users could purchase individual servers to power their computing requirements.
The cloud paradigm emerged when software makers and hardware vendors combined multiple servers in a concerted bid to harness the immense computing power generated by a grid (or network) of connected servers. Concurrently, the evolution of digital connectivity technologies that underlie the World Wide Web in recent years formally brought about the modern concept of “cloud computing.” In recent times, the purveyors of technology have parlayed cloud-computing systems into multiple tiers of service, variously labeled as SaaS, PaaS, and IaaS.
(SaaS) is a software licensing and delivery model that has gained a significant presence in a wide range of modern corporate, business, scientific, and commercial applications.
SaaS technologies allow users to license proprietary software on a subscription basis — monthly or annual. As providers of an ‘on-demand’ service, SaaS service providers host the software on the cloud to which users connect through a browser and an Internet connection. As a hugely cost-effective alternative to on-premise software installations and packages, the SaaS model seamlessly delivers a variety of applications that pertain to enterprise resource planning programs, office, and communications software, payroll and accounting packages, human resources management, mobile applications, etc.
This computing paradigm remains vulnerable to unauthorized access and malevolent hacking expeditions in online domains. Digital miscreants have targeted businesses that operate on the cloud by blocking customer access to critical online systems. This poses real risks that can translate into an erosion of market value for SaaS service providers. In response, service providers must continuously invest in improving security and authentication processes on the cloud, thereby delivering incremental assurances to their clients and customers.
Additional challenges that figure in the development of SaaS-powered products and services include custom third-party payment integration, safe and well-defined database access compliant with GDPR norms, guaranteeing zero-downtime deployment,managing the subscription lifecycle, and building a fully customizable SaaS system.
PaaS is “a category of cloud services that provide a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.”
PaaS providers host the hardware and software on their infrastructure, thereby releasing customers from any obligation to install in-house hardware and software to develop or run a new application.
PaaS technologies pose particular problems and risks for service providers. These challenges include balancing control, cost, and capacity of a PaaS-based service, providing full multi-tenancy support, designing role-based access controls, creating audit trails, and integrating third-party services into modern PaaS platforms. Additional challenges may emerge in the form of virtualization management, fine-tuning the PaaS compute architecture, designing inter-operability with other cloud services, and creating technically sound fault tolerance parameters.
IaaS operates by traditional cloud architecture. Per the IaaS cloud-computing paradigm, service providers host the infrastructure such as servers, storage units, networking hardware, virtualization or hypervisor layer, etc. This model negates the legacy business case for investing in on-premise data center infrastructure and equipment. Modern IaaS service providers also offer policy-driven services to clients and customers. These services include monitor service performance, detailed billing of customer services, log access, digital security, load balancing, backup, replication, and recovery, etc.
IaaS represents “the virtual delivery of computing resources in the form of hardware, networking, and storage services. It may also include the delivery of operating systems and virtualization technology to manage the resources. Rather than buying and installing the required resources in their own data center, companies rent these resources as needed,” according to popular definitions of IaaS.
A raft of business challenges have emerged to face service providers that offer IaaS services to clients and customers. These may include subscriber expectation management, defining support systems that handle different forms of payments from customers, accommodating the need for custom analytics that measures customer profitability and usage, managing the service value chain, and controlling business with multiple partners. Also, service operators must remain agile in terms of experimenting with product prices, product features, the configuration of service packages, and navigating the intricacies of customer licenses. They must also work to actively manage customer expectations and refine the concept of datacentre ‘in-the-sky.’
The business case for cloud computing technologies and frameworks will continue to burnish its relevance and utility years and decades into the future. The large and incrementally enormous volumes of data generated by modern scientific, commercial, and technological enterprises will require larger data centers powered by innovative technologies. These may form the central planks of local, regional, and national economies in the future. That said, the central role of the cloud may morph into differentiated expressions, marshaled by real-time processing technologies and a deeper engagement with refined versions of civilizational requirements.
Source: Cuelogic Blog
Tech for leaders & developers
69 
69 claps
69 
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tdi/lost-in-the-cloud-vendors-services-lingo-e0369ad82e57?source=search_post---------380,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dariusz Dwornikowski
Nov 5, 2016·2 min read
The biggest cloud players according to the famous Gartner IaaS magic quadrant are: Amazon Web Services, Microsoft Azure and Google Cloud Platform.
It is often hard to move from one vendor to another. If you know AWS and suddenly you need to do something in Azure, you realize that services are named differently, like Application Elastic Load Balancer in AWS is Azure Application Gateway. Moreover, some services in AWS can be mirrored by two or more services in Azure or GCP. This creates a confusion.
That is why, I have created http://letcompare.cloud so one can know how the same functionality can be achieved in the main three cloud vendors.
Of course, the site is probably not complete, it does not contain every possible service mapping, although tried to do my best. You can, however, help me by making the list better. You do it by contributing to the Github repo of the site (yes, the “engine” it is open source).
github.com
There are two main files to consider: services.json and mappings.json. Services.json includes three hashes: azure, aws and gcp. Keys in the hashes are services with the following fields, example:
The pricing key should point to the pricing URI, about to a general description and name is just a name. If the keys are not given, the site will try to guess the URIs based on the main key, in the example above this would be s3.
Head Of Engineering at Nordcloud, CTO at Nordcloud Poland, Computer Scientist at PUT
1 
1 
1 
Head Of Engineering at Nordcloud, CTO at Nordcloud Poland, Computer Scientist at PUT
"
https://medium.com/@bimap98/saas-vs-paas-vs-iaas-ebe39ab6d182?source=search_post---------291,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bima Putra
·Feb 23, 2021
www.ringcentral.co.uk
I am Data Enthusiast and interest especially in Data Science, Data Engineering also Big Data. GitHub: https://github.com/bimap98
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@anthonyspiteri/ninefold-head-to-head-with-aws-and-going-opensource-is-risky-in-cloud-land-virtualization-is-e0ec70042a1f?source=search_post---------382,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anthony Spiteri
Nov 3, 2015·4 min read
Today Ninefold (an Australian based IaaS and PaaS) provider announced that they where closing their doors an would be migrating their clients to their parent companies (Macquarie Telecom) cloud services. And while this hasn’t come as a surprise to me…having closely watched Ninefold from it’s beta days through to it’s shutdown it does highlight a couple of key points about the current state of play in the public cloud in Australia and also around the world.
As a disclaimer…this post and the view points given are totally my own and I don’t pretend to understand the specific business decisions as to why Ninefold decided to shut up doors apart from what was written in the press today around operational expenditure challenges of upgrading the existing platform.
“After an evaluation of the underlying technical platform, much consideration and deep reflection, we have decided not to embark on this journey,” the company said on Monday.
However, rather than have people simply assume that the IaaS/Cloud game is too hard given the dominance of AWS, Azure and Google I thought i’d write down some thoughts on why choosing the right underlying platform is key to any Clouds success…especially when looking to compete with the big players.
Platform Reliability:
Ninefold had some significant outages in their early days…and when I say significant, I mean significant…we are talking days to weeks where customers couldn’t interact or power on VM instances to go along with other outages all of which I was led to believe due to their adoption of CloudStack and Xen Server as their hypervisor. At the time I was investigating a number of Cloud Management Platforms and CloudStack (at the time) had some horror bugs which ruled out any plans to go with that platform at the time…I remember thinking how much prettier the interface was compared to the just released vCloud Director but the list of show stopping bugs at the time was enough to put me off proceeding.
Platform Choice:
CloudStack was eventually bought by Citrix and then given to the Apache Foundation where is currently resides but for Ninefold the damage to their initial reputation as a IaaS provider for mine did not survive these initial outages and throughout it’s history attempted to transform firstly into a Ruby On Rails platform and more recently looked to jump on the containers bandwagon as well as trying to specialize in Storage as a Service.
This to me highlights a fairly well known belief in the industry that going Opensource may be cheap in the short term but is going to come back and bite you in some form later down the track. The fact that the statement on their closure was mainly focused around the apparent cost of upgrading their platform (assuming a move to Openstack or some other *stack based CMP) highlights the fact that going with more supported stacks such as VMware ESXi with vCloud Director or even Microsoft Hyper-V with Azure is a safer bet long term as their are more direct upgrade paths version to version and there is also official support when upgrading.
Competing against AWS Head On:
http://www.itnews.com.au/news/sydney-based-cloud-provides-price-challenge-247576
Macquarie Telecom subsidiary Ninefold launches next week, promising a Sydney-based public cloud computing service with an interface as seamless as those of Amazon’s EC2 or Microsoft’s Azure.
Ninefold from the early days touted themselves as the Public Cloud alternative and their initial play was to attract Linux based workloads to their platform and offer very very cheap pricing when compared to the other IaaS providers at the time…they where also local in Australia before the likes of AWS and Azure set up shop locally.
I’ve talked previously about what Cloud Service Providers should be offering when it comes to competing against the big public cloud players…and offering a similar but smaller slice of the services offered targeting their bread and butter will not work long term. Cloud Providers need to add value to attract a different kind of client base to that of AWS and Azure…there is a large pie out there to be had and I don’t believe we will be in a total duopoly situation for Cloud services short to medium term but Cloud Providers need to stop focusing on price, so much as quality of their products and services.
Final Thoughts:
Ninefold obviously believed that they couldn’t compete on the value of their existing product set and due to their initial choice of platform felt that upgrading to one that did allow some differentiation in the marketplace compared to the big public cloud players was not a viable option moving forward…hence why their existing clients will be absorbed into a platform that does run a best of breed stack and one that doesn’t try to complete head to head with AWS…at least from the outside.
http://www.zdnet.com/article/ninefold-to-shut-operations/
http://www.itnews.com.au/news/ninefold-to-shut-down-411312?utm_source=twitter&utm_medium=social&utm_campaign=itnews_autopost
http://www.crn.com.au/News/411313,macquarie-telecoms-ninefold-closing-down.aspx?utm_source=twitter&utm_medium=social&utm_campaign=crn_autopost
Originally published at anthonyspiteri.net on November 2, 2015.
Technical Evangelist @Veeam- | Tech Addict | vExpert 12-16 | Cricket|Golf - Essendon Bombers ManU, Heat
2 
2 
2 
Technical Evangelist @Veeam- | Tech Addict | vExpert 12-16 | Cricket|Golf - Essendon Bombers ManU, Heat
"
https://medium.com/@aalphaindia/the-difference-between-paas-iaas-and-saas-aalpha-b3f2937e225c?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aalpha Information Systems India Pvt. Ltd.
Mar 27, 2020·7 min read
Anyone in a business, whether big or small, usually tend to cut on unnecessary costs as much as they look forward to making more and more profits. However, the organization can only achieve this through cloud application development. The service enables business setups to change to virtual management from the physical control of resources with ease. Globally, the public cloud service market is set to have a significant growth in 2020 from 2019 statistics.
However, there are many cloud-based services out there, which include Paas, Iaas, and Saas. Many business persons tend to have a hard time trying to choose the best for their businesses. There are significant differences between the three cloud services meant for different functionalities. For these reasons, this article is intended to reduce your doubts and select the perfect cloud service to achieve the goals for your business.
But before we discuss the differences between the three cloud services, we must understand what cloud computing is.
Defining Cloud Computing
The fact about cloud services is that it minimizes costs related to IT infrastructure. The service is on the rise because businesses can request and access the hardware and software of the system with ease. Services that can be accessed include data storage and computing power over the internet. All these advantages can be related to the phrase,” Anywhere you go, the cloud follows.”
What cloud computing does is that it helps businesses minimize the control of computing resources to give them time to concentrate on business development activities.
Note: Cloud can be made for access by several organizations, meaning public cloud. Cloud can also be made for a specific organization, meaning the enterprise cloud. The cloud-based services, which include Paas, Iaas, and Saas, offer beneficial services; hence they fall under public cloud service.The difference between Paas, Iaas, and Saas Cloud Computing Models
The main cloud-based computing services, PaaS, IaaS, and Saas, are improving business operations worldwide. First of all, they are known to reduce the cost of IT infrastructure while transforming your digital experience at the same time. Outlined below are the differences between the PaaS, IaaS, and Saas cloud-computing services.
PaaS — (Platform as a Service)
Platform as a service is perfect for software developers. The benefit of PaaS is that it is compatible with different languages for programming, and it has full control to create custom software. However, the disadvantage of PaaS is that it is not quite flexible when compared to IaaS
The main function of the platform as a service (PaaS) is to give a useful framework for developers to manage new product apps, to build the app, and also testing of the applications. Developers find it easy to use PaaS because it serves the database, application tools, and operating system that are required for the app development at the same time. This implies that there is no need to have the resources differently, thus, saving more time and resources.
Many developers love PaaS because it gives them a platform to build apps that can be provided as a Saas solution. The best example of PaaS is the giant Google App Engine. With Google App Engine, applications can be easily created and hosted without any difficulty.
Note: PaaS is accessible on a pay-as-you-go rule, meaning that you pay for the resources you have subscribed to. You only work on the built application since the vendor deals with the rest.Features of PaaS
IaaS — Infrastructure as a Service
The primary function of Infrastructure as a Service is to provide visual data centers to businesses. This cloud service is suitable for IT administrators. IaaS offers a full infrastructure, the server, and the storage space where new technologies and experiments are conducted over the cloud. With IaaS, you can perform data mining analysis, host a website and software solution, and creating virtual data centers for large scale enterprises.
In IaaS, the vendor works on networking resources, storage space management, and the dedicated data center. On the other hand, the business is working on specified tools for development, managing apps that are hosted, and deployment of the operating system.
Amazon Web Services is an excellent example of using IaaS, which is the top public cloud space. Apparently, Netflix and Salesforce brands are moving towards Amazon Web Services to support the customer base that keeps growing every other day.
Advantages of IaaS
Disadvantages of IaaS
Saas- (Software as a Service)
Software as a Service cloud service is hosted by the service provider, which is available to the consumers based on the pay-as-you-go. SaaS cloud service is suitable for consumers across different localities. The software license is open on either yearly or monthly subscriptions, and it can be accessed via a browser with stable connectivity of the internet.
Some of the business apps that use SaaS include the following:
The primary function of SaaS product development is to provide cloud-based apps to consumers. Dropbox, which is known for sharing and downloading files over the network and Google Docs, which is known for creating and sharing documents over the web, are perfect examples of SaaS cloud services.
If you are in digital businesses, you should consider SaaS in cloud computing to enjoy zero management since the vendor handles all activities. Another reason to consider Saas cloud services is how easily it creates solutions whenever technical issues arise.
Advantages of SaaS
Cost-effective: Since Saas works on a subscription basis, users don’t have to pay for an up-front fee for licensing; hence, no initial costs. Also, it is the Saas provider that manages IT infrastructure meant to run the software; therefore, no software and hardware maintenance fees.
Upgrades are easily deployed — As a user, you are relieved the work of implementing upgrades, hardware, and software updates. SaaS service providers do the job.
Scalability — There is a wide range of SaaS options for subscription, and you can always change any moment. For example, when your business shows growth, you can change the subscription plan because it will mean more users need access to the business.
It is easily accessible — You only require internet connectivity and a browser to access the Saas app. The requirements are available on multiple devices across the world, meaning that Saas is easily accessible.
Faster set up and deployment — The process of setting up and deploying SaaS is quick 0 because the service is already installed and configured in the cloud. This reduces delays experienced in the traditional software installation process.
Disadvantages of SaaS
Data and security issues — There is no privacy of sensitive information in hosted and cloud services.
Restricted range of apps — As much as SaaS has gained popularity, there exist several apps that do not offer a hosted platform.
Performance — At times, Saas may experience slow speed than on serve apps because the software is not hosted on a local machine.
Internet requirement — SaaS relies heavily on a stable internet to deliver web services. In case the internet connection fails, you can’t access the software data or, even worse, lose the data.
Having highlighted IaaS, PaaS, and SaaS, you will realize that all the cloud-based services are meant for different functions. It all depends on the needs and requirements of your business. Although it becomes a bit tricky to distinguish the threes, all you need to do is read and understand the basics of each cloud-based service. Know the feature, benefits, and the downfall of each. From there, you can determine the best service for your business.
Conclusion
You will require the IaaS service when starting a website to help you host the applications. However, you will need PaaS service if the purpose of your business is to build a custom software product. When the product is done, it will be termed as SaaS product, ready to be used.
You will notice that IaaS, PaaS, and SaaS are independent of one another to run the operations of the business. When choosing the service, know well the needs and requirements of your business and select appropriately. To succeed in your business, it will depend on how you use the services.
Good luck as you plan to uplift your business with amazing cloud-based services; Iaas, Paas, and SaaS.
Contact Aalpha today if you need further information.
Originally published at https://www.aalpha.net on March 27, 2020.
Aalpha is specialist India based Software Solutions company providing solutions for Web and Mobile development, https://www.aalpha.net
Aalpha is specialist India based Software Solutions company providing solutions for Web and Mobile development, https://www.aalpha.net
"
https://blog.containership.io/iaas-vs-paas-vs-caas-which-cloud-architecture-is-right-for-you-part-1-c7bf3c48c70c?source=search_post---------191,NA
https://medium.com/@aytanvahidova/saas-paas-and-iaas-in-oracle-cloud-computing-7e5592266e69?source=search_post---------183,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aytan Vahidova
Jan 25, 2021·4 min read
Learn the difference between 3 delivery modules in Oracle Cloud Computing
Let us talk about 2 different software existing in the market today. Oracle E-Business Suite and Oracle Fusion Cloud. What are the major differences between this 2 software?
The major difference is E-Business Suite is an on-premise server. If you will use Oracle E-Business Suite then you need to setup Server Infrastructure within your company. But for Fusion Cloud the company does not need any Server Infrastructure setup. Oracle Fusion gives you access based on your subscription.
The other point about Fusion is that you do not need a location to set up Fusion Server as for Oracle E-Business Suite you require a location for Server. You will need to create a Server Network and you need to setup Server Infrastructure for EBS. But for Fusion, you do not require any Server Network and can log in from anywhere and from any laptop, PC or any system. There is no restriction.
Also, Oracle E-Business Suite is more costly than Oracle Fusion as you are maintaining a Server Infrastructure. The major cost for IT Infrastructure is for Server only. Applications do not take a lot of costs to maintain. That is the reason E-Business Suite is more costly than Fusion.
So E-Business Unite you can own, but Oracle Fusion Application you can buy as a rent. Say Company goes to Cloud Services and say we want to buy Oracle subscription, how do you think Oracle sell them the subscriptions? There are 3 subscriptions Oracle Fusion can provide its customers with and we will see the details for each one:
When the company subscribes to SAAS, Oracle Fusion provides the customer with Software as a service. It means they give access only for Application. You can log in and you can process your business data and that is it. You will not be able to see the data in the tables if you have SAAS subscription. This subscription is mainly owned by small organizations as this subscription does not require a huge amount of investment.
The downside of this subscription is that you can not customize as you do not get access to the backend. You will continue to use the standard application the way how Oracle Fusion provided and developed the application.
However, you get an advantage of Oracle upgrades. Upgrade happens automatically.
In this subscription, the company receives platform access along with application access. A platform is a backend where software is developed. The main advantage of this subscription is that you can customize. This service can be advantageous for major big companies as such companies mostly do not use standard functions. They do not use functions as Oracle provides. Some functions are getting customized. Sometimes they customize the function by 10,20 or 30% according to their business needs.
However, you cannot develop a new application or function when you own this subscription. You are only entitled to customize an existing application or function.
With this subscription you receive infrastructure. We said earlier that with Oracle Fusion you do not have to set up infrastructure within the company. But with this subscription, we are talking about virtual infrastructure which works with the internet. Oracle Fusion creates a virtual infrastructure and then gives you access to it.
Say, for example, a company would like to use SAP as a software application and want to install it in this Oracle Fusion Server. The company wants to maintain server from Oracle Cloud but the application they will use will be SAP. So with this subscription company can install any application in Oracle Server.
As Oracle provides you with infrastructure access with this subscription it can be very costly as they are allowing you to use some third-party applications.
That is it, guys:) Hope you are clear with the topic. If not, please do not hesitate to reach me.
If you are not sure what to read next, here is the next article for you:
aytanvahidova.medium.com
aytanvahidova.medium.com
I am an Oracle Functional Specialist with more than 6 years of experience. I help clients to understand software capabilities and to analyze business needs. My main goal is to recommend solutions and ultimately guide through all phases of Oracle Cloud Applications. I am also inspired to write about life, improvement and personal growth.
2 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2 
2 
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/using-terraform-to-deploy-a-docker-swarm-stack-on-oci-8b7caa6876a1?source=search_post---------325,"Oracle Cloud Infrastructure (OCI) is an IaaS that delivers on-premises, high-performance computing power to run cloud native and enterprise company’s IT workloads. OCI provides real-time elasticity for enterprise applications by combining Oracle’s autonomous services, integrated security, and serverless compute. Available for public cloud.
This article is a continuation of the following series of related post:
But instead of describing how to deploy following step by step instructions this post is about using Infrastructure as Code using Terraform and Oracle Resource Manager, at the end of the post you will have a two node Swarm Cluster running at Oracle Always Free instances just in a few clicks.
Let’s start first introduction our test environment, see deployment diagram:
We see above that there is shared storage, this shared storage is implemented using VM Block storage and replicated using GlusterFS, the storage is designed for block level IO such MongoDB or MySQL and there is new one shared storage for our Docker containers implemented using Object Storage designed for storing HTML pages, images, configuration files or Docker Registry blob objects.
Note: By adding a Docker plugin for Oracle Object Storage We can access from the Oracle Cloud Shell as a regular file system way too.
Sample scripts and configuration files are available at my GitHub repository oci-swarm-cluster, just clone and execute:
And that’s all, oci-swarm-stack.zip is the file used to create a Swarm Stack.
Following a four click guide using Oracle Cloud Console, Resource Manager->Stacks menu
Choose Create Stack, ZIP File configuration and Browse for your generated oci-swarm-stack.zip file
click Next and unchecked auto generated public SSH keys, upload a public key from your machine, this public key will allow you to access the compute instances created for the cluster.
Click next and review public key and stack information
Click Create and your Terraform stack will be created, from Terraform Actions menu choose Apply
Apply confirm operation is shown, click on button Apply.
Your stack is being deployed…
At the end of the log you will see something like:
This project is like a template for deploying more complicated stacks, for example there is modified version of MuShop App developed by Oracle at: https://github.com/marcelo-ochoa/oci-cloudnative
To deploy this sample stack just do the same as above:
and deploy mushop-basic-stack.zip using Oracle Cloud Resource Manager.
Another simple tweak for testing is to add service with PortainerIO App, edit your terraform/scripts/docker-compose.yml file with:
once your stack is deployed you could access to Portainer console and start administering your Docker Swarm Stack graphically as is shown in this screenshot
Have fun with Docker Swarm at OCI, the next article could be a Terraform deploy for a large cluster with more nodes but it will require a cloud paid account to work.
ITNEXT is a platform for IT developers & software engineers…
68 
68 claps
68 
Written by
https://apex.oracle.com/pls/apex/f?p=ACES:DIRECTORY:::::SEARCH:Marcelo+Ochoa
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
https://apex.oracle.com/pls/apex/f?p=ACES:DIRECTORY:::::SEARCH:Marcelo+Ochoa
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@paulalision/infrastructure-as-a-service-iaas-market-by-solution-and-deployment-type-worth-56-05-d6954223df43?source=search_post---------281,"Sign in
There are currently no responses for this story.
Be the first to respond.
Paul Alision
Dec 7, 2015·4 min read
Infrastructure as a Service (IaaS) Market by Solution and Deployment Type worth 56.05 Billion USD by 2020
The infrastructure as a service (IaaS) market is expected to grow from USD 15.79 Billion in 2015 to USD 56.05 Billion by 2020, at a CAGR of 29.0%. Several factors such as faster implementation, scalability, flexibility, and agility provided by the service; increasing market competition; and increasing demand for reducing investment on IT infrastructures, hardware, and hiring skilled resources are expected to bolster the growth of this market.
Browse 66 market data tables and 43 figures spread through 137 pages and in-depth TOC on “Infrastructure as a Service (IaaS) Market — Global Forecast to 2020” http://www.marketsandmarkets.com/Market-Reports/infrastructure-as-service-market-262058075.html Early buyers will receive 10% customization on this report.
IaaS is a form of cloud computing relying on an underlying pool of physical computing resources, such as servers, network connections, load balancers, and bandwidth, which are provided in a virtualized environment “as a service” by the Cloud Service Providers (CSPs). Organizations are adopting IaaS solutions owing to the growing volume of business and financial data and other critical information among the businesses in various verticals, the need for reducing the burden of cost and IT administration, and need to focus more on their core operations. Moreover, the technological advancements have led to increased mobility, BYOD, and other digitalization trends in numerous sectors. Ease in deployment, scalability, and flexibility of services as well as low cost benefits are some of the factors bolstering the growth in IaaS market.
Increasing penetration of hybrid cloud will drive the IaaS market
Implementation of hybrid cloud architecture helps organizations avoid placing all their data at one place. Having all the data at one place makes organizations vulnerable to unexpected disasters. Hybrid cloud reduces the chance of data loss and threat in case of any adverse situation as compared to public cloud. Private cloud deployment is the safest mode of cloud deployment, but it is very expensive. Thus, hybrid cloud offers distinct advantages when it comes to security, confidentiality, and recovery of critical data or information and business continuity planning. It also helps the organizations reduce the expenses by providing the option of storing data either on-premise or any of the deployment modes, simultaneously. The data preserved on-premise in hybrid cloud helps in faster recovery and protects physical and virtual operating systems and applications.
IT and telecom sector is projected to showcase robust growth in the IaaS market
The IT and telecom vertical is one of the major verticals that generates huge amounts of data. Customer-focused organizations such as BFSI generating huge volume of financial and business data look at storage and security convenience for smooth business continuity. Thus, managed hosting, DRaaS, STaaS solutions provide businesses with cost effective computing and storage capabilities.
Inquiry Before Buying: http://www.marketsandmarkets.com/Enquiry_Before_Buying.asp?id=262058075
The key IaaS vendors and service providers profiled in the report include — AWS, IBM, Cisco Systems, Google Inc., VMware, Microsoft, Rackspace, Fujitsu, CSC, and Profitbricks among many others. Several key innovators have also been showcased in the study including Mindtree and Savvis.
Growing demand from increasing number of SMBs and the governments’ efforts to digitalize the economy creates remarkable potential for the IaaS market in the APAC region
The APAC region is expected to experience extensive growth opportunities in the next few years. The existence of a large working population and developing technology hubs, have expanded the competitive environment in the region. The countries such as China, Japan, and India, putting in efforts to deploy various data centers in country to increase the internet penetration in the rural areas of their economies, but also to digitalize the government sites and other processes are likely to increase the adoption of IaaS by 2020. These developments clubbed with a growing demand of enterprise level computing capabilities from massive and growing number of SMBs, signifies a tremendous growth potential for the IaaS market in the region.
MarketsandMarkets broadly segments the IaaS market by solution; by deployment type; and by user type. The study covers more than 9 industry verticals including IT & telecom, BFSI, government, healthcare, retail and e-commerce, media and entertainment, manufacturing, energy and utilities and others including education and research, hospitality, transportation and logistics.
About MarketsandMarkets
MarketsandMarkets is world’s No. 2 firm in terms of annually published premium market research reports. Serving 1700 global fortune enterprises with more than 1200 premium studies in a year, M&M is catering to multitude of clients across 8 different industrial verticals. We specialize in consulting assignments and business research across high growth markets, cutting edge technologies and newer applications. Our 850 fulltime analyst and SMEs at MarketsandMarkets are tracking global high growth markets following the “Growth Engagement Model — GEM”. The GEM aims at proactive collaboration with the clients to identify new opportunities, identify most important customers, write “Attack, avoid and defend” strategies, identify sources of incremental revenues for both the company and its competitors.
M&M’s flagship competitive intelligence and market research platform, “RT” connects over 200,000 markets and entire value chains for deeper understanding of the unmet insights along with market sizing and forecasts of niche markets. The new included chapters on Methodology and Benchmarking presented with high quality analytical infographics in our reports gives complete visibility of how the numbers have been arrived and defend the accuracy of the numbers.
We at MarketsandMarkets are inspired to help our clients grow by providing apt business insight with our huge market intelligence repository.
Contact: Mr. Rohan Markets and Markets UNIT no 802, Tower no. 7, SEZ Magarpatta city, Hadapsar Pune, Maharashtra 411013, India 1–888–600–6441 Email: sales@marketsandmarkets.com
"
https://medium.com/conseillers-num%C3%A9riques-suisses-romands/rfi-cloud-iaas-ou-paas-339578220a7a?source=search_post---------247,"There are currently no responses for this story.
Be the first to respond.
ICT-a.ch en association avec CloudReady.ch avec notre mandataire Rezolution.ch ont émis un RFI public, pour recenser les offres ‘IaaS’ et ‘PaaS’ disponibles sur le marché Suisse Romand.
Si vous êtes prestataire, intégrateur ou broker en ‘Cloud computing’ http://RFI-Cloud.ICT-a.ch
rfi-cloud.ict-a.ch
Le questionnaire pose la difficulté de disposer d’un serveur ‘backend’ sous Oracle, même sous RedHat, ce n’est pas aussi simple qu’avec du PostGreSQL…
Le ‘Frontend’ Tomcat/Apache/RedHat sera toutefois plus simple à proposer.
Les RFI d’ICT-a.ch sont des demandes d’informations préliminaires à des appels d’offres, généralement commanditées et financées par un client, parfois financé par nous-même. Le principe est de créer de l’intelligence collective, et de réaliser un comparatif en mode “Creative Commons”, pour réfléchir ensemble aux bonnes questions à poser, sur ce sujet.
La Table ronde “Infrastructure serveurs et stockage” est destinée a évaluer les meilleures options pour permettre à une entreprise d’héberger ses applications et ses données, que ce soit dans un Cloud, ou pas…
http://join.ICT-a.ch pour la newsletter ICT-a, http://join.CloudReady.ch pour de la veille technique collaborative, http://addme.ICT-a.ch pour se faire enregistrer comme fournisseur informatique pour la Suisse romande, et enfin, http://partner.ICT-a.ch pour collaborer ensemble, en particulier rejoindre notre réseau d’apporteurs d’affaires (10% sur 12 mois, 5% les 12 suivants: 2 années de revenus pour toutes nouvelles mises en relation). Bien entendu, nous pourrions aussi intervenir en simple sous-traitance.
partner.ict-a.ch
Le réseau des experts indépendants du digital pour des…
Some rights reserved

Le réseau des experts indépendants du digital pour des tranformations informatiques durables et responsables.
Written by
Réducteur de fractures numériques, éthicien digital, Suisse romande.
Le réseau des experts indépendants du digital pour des tranformations informatiques durables et responsables.
"
https://architecht.io/the-future-of-iaas-is-more-complicated-than-serverless-versus-containers-64c7ed9c06c6?source=search_post---------10,"This is a reprint (more or less) of the ARCHITECHT newsletter from May 6, 2018. Sign up here to get new issues delivered to you
There’s an increasingly common storyline told by industry pundits right now that serverless computing is going to rip the carpet out from under containers before they’ve even reached critical mass. The corollary effect of this is that Amazon Web Services will continue its market dominance because it was so early with its Lambda service, has seen Lambda adoption grow fast, and is pushing serverless very hard.
(I know, I just linked to two posts from the same author, Matt Asay, but I promise this is a broader conversation. Also, it’s somewhat ironic that people are starting to come back around on AWS, after a period where it was in vogue to question its continued dominance (a la Apple in smartphones) in part because AWS did not have any real open source and/or Kubernetes strategy.)
The argument about developers wanting the easiest and fastest unit of compute — a lambda or a function — makes perfect sense, but I think it ignores (at least) a couple of key things. One is that open source really does matter, especially at the infrastructure or foundational level, and it’s not clear a proprietary service on a proprietary platform will be what wins in the long run. Even if serverless does come to dominate, there’s no guarantee Lambda will be the industry standard (Amazon EC2 is the industry standard for pure IaaS, but really it’s Linux that dominates at layer most people care about).
The other thing is that while developers get most of the love, devops is a portmanteau comprised of two words. And the second one, operations, is still very important. I think that what Kubernetes, containers and the whole microservices / cloud-native movement have demonstrated is that at companies of any meaningful size, operational concerns around security, reliability, visibility, manageability, portability, etc, etc, will continue to drive IT decision-making. (You can get a sense of this in my August podcast interview with GitHub SRE Jesse Newland and also in his blog post explaining the company’s move to replatform certain piece on Kubernetes.)
Serverless can still win out as the preferred method and architecture for developers, but ops teams and engineering execs are going to decide how serverless is consumed. For folks who prefer to just use a ready-made service, all signs point to AWS Lambda as the market leader now and going forward. But for folks who want to maintain some level of control at the lower layers, I think some sort of serverless platform built atop Kubernetes is going to be the preferred option. I suspect this is where serverless efforts within the Cloud Native Computing Foundation will ultimately be focused, and indeed is the whole idea behind the Kubelessproject.
Among the openness and control set, Google seems to be in a better position to own future workloads. Just look at what it announced this week, with the open source Asylo frameworkfor developing containerized applications that run in hardware-based trusted execution environments, and with a point release of Kubeflow, a an open source project dedicated to running TensorFlow deep learning jobs on top of Kubernetes.
On their own, neither of these releases constitutes an industry-changing effort, but look at the big picture: Google is building for a world where Kubernetes is the OS, as it were, and it’s going to make sure Kubernetes checks all the boxes that users care about. Open source, controllable, composable, secure, artificial intelligence and, yes, even serverless. Whatever the love triangle situation is among Netflix, Amazon and Google, I think it’s openness and devops/AI prowess that brought Netflix and Google together in the first place.
Of course, who ultimately reigns as the cloud market leader is subject to any number of other factors, including the evolving strategies of companies like AWS, Google and even Microsoft. (And, internationally, Alibaba.) But right now, if I had to bet on a handful of AWS services against the community currently rallying and growing behind Kubernetes, I’d go with community.
On somewhat related note, Google scored Twitter as a cloud customer this week, and AWS scored Oath (nee Yahoo). Scroll down to the Cloud and Infrastructure section for my thoughts on that news and links.
MongoDB Atlas is a fully-managed, database as a service that runs on AWS, Azure and GCP. Trusted by thousands of customers — from the most bleeding edge startups to some of the world’s largest enterprises — MongoDB Atlas is globally distributed, self-healing, and fully elastic. Atlas leaves the database management to the experts and lets users make changes on the fly, painlessly upgrade their clusters, easily back up and restore data, and access the latest MongoDB features.
Visit MongoDB to get started with a free cluster running on MongoDB Atlas today.
mongodb.com
The argument is that tech companies poach professors with the lure of more money and more data, leaving universities struggling to retain talent. Or, at least, struggling to retain talent that doesn’t have a connection to a large company that might bias their agendas. Of course, it’s not just Facebook that’s doing this.
nytimes.com
When you think about the “AI war” between the U.S. and China, ask yourself if you’d rather see China using the same chips everyone else is using, or operating in a vacuum using chips not made by Nvidia or Intel. As I noted last week, I don’t think AI-based trade restrictions are necessary or would necessarily be effective, in part because China has its own means of producing hardware, and to the extent its government agencies and companies would chooseto use chips from U.S. manufactures, that at least keeps everyone on a known and level playing field.
chinamoneynetwork.com
This is one of those applications of AI techniques that’s not at all sexy but could actually produce very meaningful impacts. Simple things like making it easier to search for information or interact with enterprise software could save a lot of time and result in a much higher efficiency/productivity.
venturebeat.com
The hire, Manuela Veloso, was previously head of the machine learning department at Carnegie Mellon University. There’s some talk about chatbots in this article, but I’d bet her biggest focus will be on automating business processes and security.
cnn.com
I’m hopeful this company’s tech is as useful and utilitarian as it sounds.
techcrunch.com
architecht.io
I touched on this last week, as well, specifically re: the Canadian government’s investment in becoming the world’s AI leader. That can’t happen if all the jobs and capital are still in the United States, which means Canada and other companies will need some ground-level tech infrastructure in place if they want to compete.
theglobeandmail.com
No, not with robots, but by predicting which patients might be at risk of falling or having other age-related conditions. Predicting risk of rehospitalization and other things really could be a game-changer in the field of preventative medicine.
cnbc.com
This seems like an interesting approach to securing AI systems, and also making sure we keep humans in the loop. Because, after all, we want AI to serve us rather than us to bend to a model’s decisions.
openai.com
Google says it’s getting closer to quantum supremacy, and also posits materials science as a clear application for quantum computing.
googleblog.com
This is from the DAWN project at Stanford, which is ran by Spark creator Matei Zaharia, among others. If I recall correctly, this is not the institution’s first work on video analysis and search.
arxiv.org
Replicated gives SaaS and software vendors a cloud-native platform for easily and securely deploying their applications inside customers’ data centers or VPC environments. Replicated provides tooling for automatic updates, license management, support, audit logs, LDAP integration and more. Sign up for a free trial here.
replicated.com
This is Twitter announcing its moving some of its Hadoop workloads to Google Cloud. It’s unclear exactly how much data this will be, but it seems like it could be up to 300 petabytes. This is a big win for a number of reasons, including that Twitter is often considered an engineering-savvy company and has managed its own data centers for years now.
This also is the latest in a series of customer wins for Google where popular tech companies, including Spotify, move there for data-focused workloads. What will be really interesting to see is whether Twitter, which is famously open-source-centric, follows the lead of some of the others and starts transitioning off of Hadoop and onto some of Google’s alternative services, and how much it begins utilizing BigQuery, etc.
FWIW, The Register did the math and calculated a sticker price of nearly $10 million a month to host 300 petabytes of data.
twitter.com
This is a less-cool customer win, but potentially more lucrative (also, I didn’t realize Oath was such a big cloud user, but that’s because I always think of it as just being Yahoo, when it’s actually like 50 companies). AWS is also in line to win that big Pentagon cloud deal. So despite this whole Kubernetes vs. serverless debate, there’s still a ton of money to be made migrating big customers and their workloads to the cloud, and AWS is very well positioned for those deals.
datacenterknowledge.com
That’s roughly one-tenth of AWS’s current run rate, but Alibaba Cloud revenue is still growing at more than 100 percent year-over-year.
zdnet.com
Perhaps this will help with that whole brain drain situation discussed above. Gotta get those smart investors active and, well, investing in the Great White North.
bloomberg.com
architecht.io
This is pretty cool, and I think Datadog’s success speaks to the continued importance of ops. If the tooling is there to effectively and (relatively) easily monitor them, containers aren’t going anywhere.
datadoghq.com
Speaking of containers, an FYI if you use Docker for Windows Server.
theregister.co.uk
This is Apple’s open source database project, which I wrote about a couple weeks ago. I don’t know if I’d say its selling like hotcakes, but VMware, Snowflake and the JanusGraph project are working on it.
foundationdb.org
The short answer appears to be “yes, but marginally,” according to Backblaze’s analysis.
backblaze.com
This is about Google’s analysis of some of its applications, including Gmail. It also gets back to my previous comments about Google’s openness paying off for it. Everyone knows Google knows webscale computing inside and out, but I think publicly discussing its performance evaluations and techniques helps instill more confidence in companies thinking about Google as a cloud provider.
acolyer.org
architecht.io
It’s kind of weird to think that search is still such a big deal, but let’s remember that Google is only for Google. Companies wanting to utilize search in their applications still need a provider, and that’s still a pretty open space. Plus, keeping up with computer vision, knowledge graphs, etc, is going to be critical for anyone trying to sell search products.
venturebeat.com
The convergence of the data world and the cloud-native world is going to be a big deal. It also will help cement Kubernetes’ place inside companies that start building their data pipelines on top of it.
confluent.io
The rich get richer. I’m not certain Facebook needs anymore data scientists, but this is an interesting hire nonetheless. Even if he’s not working on the dating service, it can’t be good news for eHarmony.
techcrunch.com
architecht.io
LinkedIn does not have a great reputation when it comes to not annoying users. And the world is presently asking some very serious questions about data privacy. You have to wonder how this will go over.
geekwire.com
Pretty sure IBM also just launched something around blockchain and food safety, which is where Walmart’s efforts are focused. Maybe this will be the first killer business application.
thenewstack.io
This seems like a good start for companies thinking about encryption, decentralization and other techniques for keeping data private, even as it has to be shared across more users and more systems.
oreilly.com
This is not the first company to fold and cite GDPR as the reason. On the one hand, you’d hate to think that the regulation is so onerous it’s killing small companies. On the other hand, I don’t think that’s actually the case. I’m fairly certain that if a company (say, Facebook) thought its users would keep using once its data practices were revealed, it would do the work and keep on operating.
techcrunch.com
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
18 
18 claps
18 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@jonathan18186/azure-kubernetes-service-aks-on-azure-arc-a0d8502115bc?source=search_post---------350,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan
Mar 29, 2021·3 min read
Azure Arc is one of the latest Azure services that allow administrators to have have capability of managing cloud IaaS, Kubernetes (K8s) clusters and data services with an unified experience, meaning to be able to have an overview from Azure portal. Essentially, every resource needs to install Azure-Arc-related extensions to get connected to the platform. If you would like to learn more about how Azure Arc works, please take a look here.
For this article, we would focus on getting K8s cluster onboarded to Azure Arc. Since I do not have the physical servers to “actually” simulate the K8s-onboarding process, I went through the Azure Arc Jumpstart tutorial for getting the same onboarding experience.
This is meant to be easy as the tutorial already provided the script you would need to execute. This script would not only deploy a whole new AKS cluster in the target resource group but also install all the required extensions within the AKS cluster. So, it is really a one-liner to make life easier.
However, if any of you is like me that always fail to deploy resource through ARM template, worry not. Please follow the steps below to achieve the same goal.
** Please ensure all the variables in the shell script has values (pink part). As you are manually executing the script, it would not read the values from the environment, meaning it might require you to manually set the values.
** Please ensure SP has enough permissions to perform actions it needs to perform. To save time, you could temporarily provide contributor role to the entity (blue part).
** The script could also be found in the downloaded Git folder under
Unfortunately, I could not get Linux and Windows servers onboard with ARM template, either and I might create a GitHub issue to address that shortly. Hopefully, by the time you try this, the onboarding experience would feel like a breeze! Happy learning!
Learning new things about Kubernetes every day. Hopefully, the learning notes could help people on the same journey!
2 
2 claps
2 
Learning new things about Kubernetes every day. Hopefully, the learning notes could help people on the same journey!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mochadwi/which-iaas-provide-free-tier-you-should-choose-db7f9dcd13cd?source=search_post---------226,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mochamad Iqbal Dwi Cahyo
Aug 20, 2016·1 min read
What is IaaS exactly?
And PaaS also SaaS?
There’s DbaaS as well :D
Find out here articles from amazon web service, one of Cloud Computing Company that provide IaaS, PaaS and SaaS: https://aws.amazon.com/types-of-cloud-computing/
AWS PaaS product: https://aws.amazon.com/elasticbeanstalk/faqs/
Comparison: http://stackshare.io/stackups/heroku-vs-google-app-engine-vs-aws-elastic-beanstalk
Recently, I’ve a personal project with my friend. That she wanted me to build a WebApps — Ecommerce for her.
A geek enthusiast. Sometimes photographer, culinary seeker or traveler. He ready to craft ideas!
A geek enthusiast. Sometimes photographer, culinary seeker or traveler. He ready to craft ideas!
"
https://architecht.io/a-rumored-foray-into-learning-management-suggests-the-true-scope-of-aws-s-vision-a0ecdd0ce1c?source=search_post---------83,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is a reprint (more or less) of the ARCHITECHT newsletter from March 18, 2018. Sign up here to get new issues delivered to your inbox.
architecht.io
Once again, there was a remarkably high number of interesting news items and other content late this week, so make sure to read the whole newsletter. But here are my personal highlights:
Every time Amazon Web Services expands into an entirely new line of business (i.e., not just rolling out another IaaS product), we get a better view into how that company sees its place in the world. And with rumors this week that AWS is set to enter the learning-management space, I think it’s safe to say — assuming the story is accurate — that AWS wants to be much, much more than a “cloud computing provider.”
Not that this should come as any surprise. AWS has steadily expanded its suite of products since launching with S3 and EC2 more than a decade ago. From email to virtual desktops, and from vertically focused gaming products to a broadly focused call-center automation service. From a business perspective, the latter service, Amazon Connect, seems like the most direct relation to the rumored learning-management service.
Connect is a service that brings the Amazon approach to customer service to the world, and any sort of learning-management (or, possibly more accurately, employee-training) service could very well be Amazon’s way of bringing some of its HR and corporate culture to the world. It seems safe to assume that “learning” doesn’t mean challenging Coursera or other MOOCs, but rather training employees on necessary skills and tracking their progress. Whether that’s good or bad for employees is debatable, but it’s definitely a growing market (as the CNBC story linked to above points out) and one AWS might see itself as particularly well positioned to own.
(I will also be curious to see how much of this learning/training has to do with AWS technologies, and what effect that will have on AWS-training specialists such as A Cloud Guru. I spoke the founders of that company, which also runs the popular Serverless conference, on the podcast back in October.)
At this point, I tend to view AWS, more so than Google Cloud or Microsoft Azure, as a whole-hog enterprise IT vendor, on par perhaps with IBM during its heyday or, really, what a company like Oracle has grown into. IaaS is the core of the AWS business, which the company is using as a foundation for entrenching itself ever deeper and broader into users’ business processes. It doesn’t hurt that Amazon itself has internal processes and expertise to commercialize across such a broad range of areas, from consumer retail to online marketplaces to shipping and logistics.
Compared with Google or Microsoft, one big difference is that AWS was very distinct from Amazon.com from the start, and it entered into areas like collaboration (email, video conferencing, etc.) as extensions from IaaS. There was no large existing email or productivity business that then had to be integrated with a new IaaS business. AWS got started on IaaS early, established its dominance, and now is using that dominance to become a one-stop IT shop not so different from Amazon.com in the consumer world.
And in related news, Microsoft is launching a cloud-based gaming division to bring game developers onto the Azure platform and, it sounds like, to deliver a subscription-based game-streaming service. Gaming is another area where AWS got an early headstart with services like GameLift, Lumberyard and its Twitch acquisition, but Microsoft does have its own advantage here with Xbox. It knows the gaming industry and the people in it; it understands engineering for gaming systems; and it could build itself a nice data-engineering-business flywheel with a collection of developers on Azure and its own popular streaming service.
The Cloud Native Computing Foundation has accepted NATS as in incubation-level project, giving its growing ecosystem (centered around Kubernetes) its own messaging platform. I’ll be honest and cop to now knowing a lot about NATS, but my first question upon seeing this news was to ask how NATS compares with Apache Kafka — the very popular messaging platform originally built at LinkedIn. (Recently, I wrote about another competitive project, called Apache Pulsar).
Apparently, I’m not alone in asking this. At the risk of vastly over-simplifying things, the consensus seems to be that Kafka is more mature and expansive in terms of capabilities (and probably has a much larger community), but NATS is more “cloud-native.”
But the bigger-picture question that I always come back to is the influence of open source foundations and projects going forward. Despite being on the upswing in terms of adoption, Kafka is very much a product of the “big data” era where Apache projects (thanks to Hadoop) reigned supreme. The CNCF, on the other hand, started around Kubernetes and has been adding projects around it to fill out a more complete platform for building cloud-native applications.
While there have been some project-level integrations between these two worlds (especially around Kubernetes), they’re still quite distinct worlds. Between the various open source options for certain capabilities and the options developed by cloud providers, one has to wonder what will emerge as the de facto set of tools for building next-generations applications. It seems like some sort of convergence of the big data world and the CNCF world might need to be on the horizon, especially if developers are looking for the strongest choices at each layer of their application architectures.
For more on the evolution of Kafka, open source and its connection with the cloud-native world, check out my separate podcast interviews with Kafka creators, and Confluent co-founders, Jay Kreps and Neha Narkhede. (I’m sure we’ve spoken about it on other podcasts, too. Just check them all out;-) )
GV led a $56 million series A round in a company called SambaNova Systems, which came out of stealth mode this week. What’s most notable to me about SambaNova is the pedigree of its founders, which include Christopher Re and Kunle Olukotun of Stanford. They’re both helping lead the DAWN Project there which, like the RISELab at UC-Berkeley, is trying to advance AI at the systems and data level, as well as at the model level.
It seems unlikely that AI efforts in most companies will be able to live entirely separate from other data systems and data stores, so the integrated approach of a company like SambaNova, which is building a hardware+software platform, could go a long way toward bridging those worlds. Or at least helping make them all perform a lot better.
For more on how research projects like DAWN and RISELab are thinking about AI, machine learning and data systems, check out my podcast interviews with DAWN’s other two leaders, Matei Zaharia and Peter Bailis, and with RISELab director Ion Stoica.
ARCHITECHT
You’re reading this because, like the rest of ARCHITECHT’s audience, you’re intelligent and interested in the future of enterprise IT. Reach your fellow readers and help ARCHITECHT grow (including the return of the ARCHITECHT Show podcast) by becoming a sponsor. Email advertise@architecht.io for more info.
I can buy into this argument, but don’t really have a good idea for a solution. The argument, essentially, is that large tech companies buying up AI startups and hiring all the good researchers is leading to a homogenization of work and a dearth of products addressing other areas that might be valuable.
techcrunch.com
After lots of bad press, here’s a success story for Watson in health care.
healthcareitnews.com
Wants its help and needs its help. One could argue that if AI really is going to be world-changing, governments and private companies need to establish a framework on how they can help each other. There really are bigger problems to solve than making sure I can turn off my lights without getting up.
nytimes.com
Speaking of which, here’s an AI startup focused on national security applications.
percipient.ai
Hmm … does anybody buy this type of highly engineered appliance outside of the supercomputer world?
zdnet.com
It’s good to see more discussions like this happening, because once the novelty wears off, the actually important work of deploying these systems in production environments needs to happen.
oreilly.com
This is a cool use case for drones and computer vision. I love seeing people thinking about very practical, but not completely obvious ways of using new technologies and new data sources.
technologyreview.com
This is a summary of some of these research behind Google’s AutoML tool for automatically generating neural networks.
googleblog.com
architecht.io
You can probably guess what they’re seeing: edge computing, AI hardware, voice interfaces, and vertical applications.
venturebeat.com
This is a pretty level-headed Q&A on the dangers of AI, both real and imagined.
nautil.us
The Allen Institute for AI continues on its mission to create “smarter” AI systems that can reason and answer questions on par with people. It’s not work that produces many headline-grabbing lab results, but it could have a huge impact in the long run.
arxiv.org
I’m not familiar with this publication, but it seems legit. And the idea of Baidu trying to undercut Google’s advantage in driverless cars by democratizing access to data could be a pretty sound strategy.
medium.com
You know, for all your companies neuroevolution models ;-)
uber.com
architecht.io
Some insightful research from Cloudability about what AWS are most popular, and which are growing the fastest. I’ll admit I didn’t download the full report, but this stat from the press release also gives you something to think about: “A quarterly breakdown of growth rates over 2017 indicated that serverless grew 100% in Q1, 138% in Q2, 321% in Q3 and 667% in Q4.”
itprotoday.com
Speaking of serverless computing and AWS, here’s some more info on how companies are using it and what they’re learning from it.
acloud.guru
Not sure “AIOps” is really a market unto itself, but Moogsoft seems to be doing pretty well instilling some intelligence into operations workflows.
moogsoft.com
Not that Cloudflare is playing favorites. It also has similar deals in place with Microsoft and Google, according to this story,
fortune.com
I don’t doubt it was a record year, but I’d be interested to see the actual numbers behind the claim that spending more than doubled last year. What I wonder is if we’ll ever reach a point where gains in efficiency or some other factor lead to a flattening in capex.
recode.net
This would be more interesting if it looked like Service Fabric was a meaningful product for Azure, but there’s still the possibility it could help customers of other clouds spend less on managed services, which is indirectly good for Microsoft. The bigger picture is probably the continued rise of Kubernetes within Microsoft and elsewhere, to the point where, as the article notes, Microsoft is discontinuing its original Azure Container Service (that included DC/OS and Docker Swarm) and refocusing it on Kubernetes.
zdnet.com
Good interview / case study №1.
voucherify.io
Good interview / case study №2.
googleblog.com
Good interview / case study №3.
thenewstack.io
architecht.io
Good interview / case study №4.
hackernoon.com
Good interview / case study №5.
thenewstack.io
A couple of things are definitely true about Kubernetes: (1) There is so much investment happening in developing it that you can’t legitimately argue it doesn’t have a very big future (even if it’s not the most lucrative part of the stack); and (2) there are going to be a lot of competing projects spearheaded by various members of the community, including Google and Microsoft.
googleblog.com
The role of systems reliability engineering has taken on more prominence over the past few years, but really only for companies of a certain size. Is there a corollary function for cloud-based operations?
linkedin.com
I hadn’t heard of Zscaler, but it’s an “enterprise cloud security company.” It did well on IPO day, and brought in $126 million last year.
techcrunch.com
Speaking of cloud security …
zdnet.com
FYI, if you’re curious what a company like Facebook values in networking technology.
fb.com
architecht.io
This is exactly what it sounds like, and I think a very good idea by IBM. DIY is still not the name of the game for most companies when it comes to data science. For more, check out this patented super-deep dive by The Next Platform and listen to IBM’s Bala Rajaraman talk Kubernetes-powered Cloud Private from my podcast back in November.
zdnet.com
Another good case study from MongoDB, which seems to have struck a nerve with its cloud-based Atlas product.
mongodb.com
Just what it sounds like. And don’t forget that Microsoft has made big investments across both of these spaces in the past several years.
microsoft.com
Or maybe researchers can just get it surreptitiously like a certain political-analytics company … But seriously, this is a double-edged sword because on the one hand there’s privacy and what users did or didn’t sign up for. And on the other hand there’s the reality that if Facebook really does want to change the world or help its users, acknowledging that actual researchers could do a better job than the company alone can would be a good start.
wired.com
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@romaincaisse/iaas-pourquoi-choisir-un-cloud-public-plut%C3%B4t-quun-cloud-priv%C3%A9-pour-d%C3%A9ployer-une-application-42995880bd5c?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Romain Caisse
Jun 13, 2016·5 min read
Pour profiter des technologies de cloud computing, l’entreprise peut soit recourir à un prestataire proposant la mutualisation de ressources externalisées en cloud public, soit acheter des équipements sur lesquels déployer tout l’arsenal en cloud privé. Dans ce dernier cas de figure, l’entreprise gère directement l’infrastructure qui lui est exclusivement dédiée (ou la délègue à un prestataire). Certaines en profitent même pour mettre en place un nouveau datacenter automatisé de type SDDC (Software defined datacenter).
Parmi les principales différences, il convient de souligner que le cloud public ne génère que des frais de fonctionnement mensuels (Opex pour operational expenditure), sous la forme d’un contrat souvent pluriannuel de 3 à 5 ans, et parfois basé sur un usage horaire ou mensuel. L’infrastructure est prise en charge par le fournisseur cloud et s’adapte automatiquement selon les besoins, faisant varier le montant de la souscription.
En cloud privé, l’entreprise investit de façon importante sur le matériel et les logiciels nécessaires au déploiement de l’infrastructure, et planifie les coûts de maintenance annuels. En cas de besoin supplémentaire, elle devra acheter le matériel nécessaire (ainsi que les logiciels). Bien entendu, elle devra disposer des compétences nécessaires pour gérer cette infrastructure.
Attention toutefois : le coût du cloud public reste difficile à maîtriser, car les éléments qui composent la facturation ne facilitent pas la maîtrise des coûts. En effet, même lorsqu’il s’agit uniquement d’un tarif horaire en fonction de la puissance d’un serveur virtuel par exemple, comment déterminer à l’avance le nombre de réelles d’utilisation, ou encore de serveurs utiles pour assumer la charge applicative ?
On met souvent en avant le fait qu’un cloud public est une offre standardisée, contrairement au cloud privé où entreprise adapte finement les installations matérielles et logicielles en fonction des besoins applicatifs. Or depuis plusieurs années, les offres IaaS se dotent d’outils de plus en plus évolués permettant non seulement d’ajuster de plus en plus finement les ressources matérielles et logicielles, mais aussi d’y ajouter des options évoluées d’équilibrage de charge, de sécurité, de gestion des identités, d’accès aux applications… Avantage pour le cloud privé : l’entreprise sélectionne librement le type de solutions à déployer, sans être limitée à celles (ou celle) proposées dans le catalogue de services du prestataire. Cependant, les fournisseurs cloud enrichissent sans cesse leur catalogue, travaillant de concert avec de plus en plus d’éditeurs et de constructeurs.
Lorsqu’il s’agit de déployer un progiciel sur une infrastructure de cloud public, cela équivaut une installation sur un serveur distant, totalement maintenu et supervisé par un tiers. Lors d’une évolution ou d’une mise à jour, il est même possible de tester l’environnement sur une infrastructure temporaire, avant de mettre le tout en production. Pour réaliser ces deux opérations sur un cloud privé, l’entreprise devra mettre en place l’infrastructure matérielle et logicielle nécessaire. De plus, un environnement temporaire sur le système d’information interne nécessitera un investissement très important pour une utilisation limitée. Que deviendront les serveurs et autres ressources utilisées après le test ?
Dans le cas d’une application développée par l’entreprise (ou l’un de ses prestataires), un cloud public autorise la mise en place d’un environnement contrôlé et sécurisé pour le temps du projet, et accessible aussi bien par le prestataire que par les équipes internes. Alors, le coût sera limité à l’utilisation de ses ressources uniquement pour le temps du projet.
Sur un cloud privé, l’entreprise devra assurer elle-même la mise en place de cet environnement, acheter toutes les licences logicielles nécessaires, et gérer tous les aspects sécuritaires. Elle devra donc disposer des compétences nécessaires, ou faire appel à un prestataire, avec un coût supplémentaire non négligeable. Cependant, la DSI pourra choisir parmi toutes les solutions du marché, et pas seulement parmi les solutions proposées par un fournisseur.
Tous les arguments plaidant pour le cloud public restent valables pour le développement d’une application de nouvelle génération de type SaaS. Dans cette situation, l’entreprise se tournera vers un fournisseur d’IaaS proposant également du PaaS (platform as a Service) permettant de concevoir l’application et de la déployer en mode SaaS. Un fonctionnement possible en cloud privé, mais beaucoup plus complexe à mettre en œuvre.
Quel que soit le type d’applications développées, l’accès en mobilité est généralement un service intégré proposé par les fournisseurs d’IaaS. Ce qui simplifie non seulement l’ouverture à la mobilité, mais aussi l’intégration à la politique de sécurité de l’entreprise. Ce type de déploiement en cloud privé nécessite des connaissances appropriées et de fortes compétences en sécurité.
Certes, les deux types de clouds offrent l’élasticité des ressources en fonction des besoins applicatifs. Cependant, cette élasticité totalement automatisée dans le cloud public peut nécessiter l’installation physique de ressources supplémentaires pour un cloud privé. Et cela vaut aussi pour ajuster les performances de l’application, non seulement en fonction du nombre d’utilisateurs, mais aussi en fonction du trafic ou du nombre de transactions par seconde. Lorsque les besoins diminuent, la tarification en cloud public suit le même chemin, tandis qu’en cloud privé, les ressources achetées pour y répondre sont toujours là, inutilisées.
On met souvent en avant le fait que la liaison réseau entre le système d’information de l’entreprise et le fournisseur cloud peut créer une latence préjudiciable aux performances transactionnelles. Toutefois, la plupart des fournisseurs d’IaaS offrent la possibilité d’installer une liaison à très haut débit entre le système d’information de l’entreprise et leur propre infrastructure.
La sécurité du cloud public incarne souvent un frein psychologique majeur. Pourtant, les prestataires sont prêts à investir autant ou plus qu’une entreprise pour recruter les meilleurs spécialistes. Une opération d’autant plus bénéfique, que ses compétences sont alors mutualisées pour tous les clients. Par ailleurs, les fournisseurs d’IaaS sont généralement certifiés pour un très grand nombre de normes de sécurité. Enfin, la réputation de ce type de prestataires repose en grande partie sur cet aspect sécuritaire. Et très souvent, un examen rapide système d’information de l’entreprise, met rapidement en exergue les faiblesses des politiques de sécurité. Si l’informatique est devenue un service essentiel de l’entreprise, elle reste la raison d’être du fournisseur de cloud public.
Côté disponibilité de l’infrastructure, la plupart des clouds publics proposent une double sauvegarde (voire diverses aux offres de mirroring), parfois intégrée aux offres de base. Pour disposer d’une telle disponibilité, le cloud privé nécessitera des investissements supplémentaires en ressources matérielles, logicielles, et en main-d’œuvre. Sans oublier l’espace physique, dans des locaux géographiquement distincts.
Au-delà de la sécurité, la conformité réglementaire nécessite une attention particulière. L’entreprise devra s’assurer qu’elle peut juridiquement stocker ou faire transiter des données hors de l’entreprise, voire dans d’autres pays. Pour répondre à ce type de besoins, les fournisseurs cloud proposent des clauses contractuelles assurant que les serveurs et le trafic restent cantonnés à des datacenters installés dans une zone géographique prédéfinie (France, Europe, etc.). Néanmoins, pour répondre aux obligations réglementaires de certains secteurs d’activité, le cloud privé reste la seule possibilité.
Originally published at www.zdnet.fr.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@marketing_40464/whats-the-best-option-for-my-startup-aws-or-azure-iaas-a14122e5e857?source=search_post---------279,"Sign in
There are currently no responses for this story.
Be the first to respond.
Laitkor marketing
Jun 14, 2018·2 min read
When there arises a problem to choose between AWS & Azure IAAS, the statutory situation is relatively called the problem of trade-off ?
The criteria to face such a situation takes place in general manner where you have to opt the best alternative step which could be relevant or profitable for your purpose and could directly focus towards your motive.
You need to look after the platform you select for developing, according to which the case may depend on either cases.
First of all you should reveal that whether these two are interrelated or could be substitute with other following from some or more cons.
Here you should focus more on scalability and as we know Windows Azure could be well used for its PaaS benefits. It seriously works nice and you may get satisfied using it, especially if you are working to develop on Microsoft .Net stack.
It contains the new VM roles which add more utility to your performance as you can easily do those thing which are not fit in their normal working and web roles. It is highly recommended because they scale much easier with its simplicity in changing three to four servers. That was all about Azure Application Development Services.
Now let us consider, how much is AWS significant for startup and to what extent?
It’s also recommended sometimes due to having lot of options to look for any server configurations in AWS, whether it’s Java, PHP, Windows, Linux.
Here you are provided with some offers which can be easily availed, like a free micro instance totally free for the first year and less problem occurs using with it.
If you are developing in PHP, AWS is seemed to be better option to stick with. Many options are provided in AWS for scalability such as cloud front etc.
These were the some similarities or you could say a comparison between AWS and Azure.
You may select either of the platform according to suitability of your performance. For more details to works with us, let us know the Laitkor Team.
Source — https://www.laitkor.com/whats-the-best-option-for-my-startup-aws-or-azure-iaas/
"
https://medium.com/@aheadcrm/clash-of-titans-the-war-cry-oracle-and-salesforce-d48dfbc1dfac?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Sep 10, 2018·6 min read
In the last article Clash of Titans — Microsoft and SAP weigh in of this little series, I discussed the strategy of two of the big four and how they are positioned in the platform play of the business software titans — and others. This article covers the other two: Oracle and Salesforce. These business software vendors are competing in a market that is changing — commoditizing — at a fast rate towards an experience market, and probably beyond, if I follow the argumentation and thoughs of CRM godfather and friend Paul Greenberg. Business application vendors can stay really successful only if they morph into platform players. And this platform is more than just a technology platform, but encompasses four dimensions.
The four dimensions that are paramount to be able to deliver great engagements that result in lasting positive experiences are
In this article I look at Oracle and Salesforce and how they position themselves in this game of thrones.
But now, without further ado, let’s dig into the topic.
Since the launch of what originally was project Fusion and now is Oracle CX, the company has done a remarkable pivot from being an on premise company to becoming a cloud company. The company has its strength in being a full stack provider with a full range of business applications. However, its main strenght is owning the gold standard database engine that runs the majority of business workloads worldwide.
From its overall technological profile one could position Oracle somewhere between SAP and Microsoft as it with Open Office also owns a full fledged office suite that helps on the productivity side.
However, Oracle is not a public cloud IaaS player although, at its core, it is rather a technology company than a business software company. The main purpose of the Oracle cloud is running Oracle applications, and running them efficiently. And to further reduce the resulting risk of customers defecting to other vendors Oracle introduced its Cloud at Customer back in 2016, which was actually a pretty good idea. This is also evidenced by Microsoft copying it with the Azure Stack.
Ecosystem-wise, Oracle appears to be trailing Microsoft, Salesforce, and SAP. While the company owns significant assets that have their roots in the open source community, like Java, or Open Office, Oracle does not have the reputation of being much of an ecosystem player, but more of a kind of bully.
Still, Oracle has a good number of implementation partners.
Insight is one thing where Oracle has strengths. For one there is a full stack of business applications that are capable of delivering a lot of important data into Oracle’s machine learning algorithms. But there is more.
The database.
Especially since the introduction of the Autonomous Oracle Database Service. This service promises to bring down cost, while increasing performance and delivering unprecedented scalability and security. And it delivers data that can be used for improving one of the most worrisome challenges: It delivers the insight on security problems and is capable of acting on it. This is a value proposition that currently none of the other vendors can claim to have.
Although it is only a temporary advantage, as so frequently in this business.
Oracle owns a good number of productivity tools, namely the above mentioned Open Office. It has also intelligence built into the business applications — and the administration layers — that increase staff productivity. So, they are playing fairly well in this dimension, too.
In my eyes Oracle should work on becoming more approachable, and getting rid of the image of being a bully. Improving on the ecosystem frontier while using the great database asset that the company has could make it an even more formidable player than it already is.
Oh, yeah, and do not set your sights too much on AWS. Microsoft is your real opponent.
Salesforce surely is the current synonym for CRM software, and this not only because the company wisely chose this acronym as its stock ticker symbol.
Evolving from its origins the company has evolved its portfolio into a wide variety of customer facing applications. The company also has realized long ago that business applications will get commoditized and that there consequently is a need for a platform, which can get used by its vast ecosystem of ISV’s and implementation partners.
The platform that helped Salesforce starting their ecosystem is Force.com and, in combination with being an ecosystem player at its heart it was able to create one that is probably rivalled only by Microsoft’s. Salesforce’s Trailhead education platform is even better than what Microsoft offers.
While the technical platform is (mostly) limited to customer facing applications, which also has a challenge on the data side, discussed below, the thriving ecosystem is Salesforce’s biggest asset. The combination of ecosystem and relentless focus on customer facing applications resulted in Salesforce taking an undisputed leadership position in the wider CRM market.
Well, a strong sales strategy and execution helped to get there, too; still helps.
However, as mentioned above, the company’s focus on CRM-like applications and then e-commerce results in limited access to data.
And data is the raw material for insight — which is actionable information.
From an insight angle, Salesforce has created its Einstein layer, which is embedded into the applications.
Embedded intelligence is what can create immediate value for business users. It provides predictive analytics and recommendations, sometimes even prescriptions to users and/or takes away tedious tasks.
And this is exactly where Salesforce’s data challenge lies. Salesforce has a lot of customer and order data, but none of supply chains or relations outside a company realm. This is where Oracle, SAP, and especially Microsoft have an edge. This was also an important reason for Salesforce attempting to acquire LinkedIn and looking at acquiring Twitter.
The company finally acquired Mulesoft, which can alleviate the data weakness to some extent, while also serving as an improved glue between Salesforce owned applications and Salesforce to non Salesforce integrations.
Mulesoft connects processes and therefore gives access to business data which it can also feed into Einstein’s machine learning capabilities.
The main productivity tools that Salesforce offers, are Chatter, the embedded analytics applications including the Salesforce Inbox and the Lightning UI. With these the company covers business productivity but still trails the ability that e.g. Microsoft can offer with the office suite of products.
In summary, Salesforce is a formidable player. Right now, no competitor can afford ignoring them in the wider CRM area. It, however, is not all hunkydory in downtown San Francisco. The company is in need of getting more access to data and better access to the supply side of businesses instead of focusing on the demand side, where it is undoubtedly very strong.
Another facet is the need to staying perceived as the innovator of the industry. This one is particularly important as it helps Salesforce command premium prices, which keep it profitable. While, from a customer experience point of view one cannot go around Salesforce, the company’s low profitabilty is an achilles heel that the competition does attack and will continue to do to.
In order to not run into the risk of getting sidelined, Salesforce needs to continue playing its strength in innovation while improving on its profitability — without increasing prices.
Salesforce is sitting on the throne that the other three companies are after. However, it is not a stable position. Salesforce owns the definition of CX, but it is dangerously limited in its scope. Looking at the big four, Salesforce for SAP and Microsoft is the enemy’s enemy, that keeps them in a carefully balanced alliance … which probably gets instable if Salesforce shows signs of being dethroned — by either Microsoft or SAP.
Having said this, I do not see Oracle as being one of the top three vendors, rather a number four. This is in spite of its tremendous database force, which already is attacked, too.
Oracle is lashing out at Amazon, both Microsoft and SAP have Salesforce in their sights, for time being.
The company that should get into the sights of everyone else is Microsoft. Microsoft has all it takes to become the number one, including the most compelling strategy for small and emerging companies.
Then we see some smaller players like e.g. Freshworks or Zoho that have the chance of disrupting the big players from below.
But is this a fixed outcome? Not by far. All these companies are playing their strengths.
And then we have the big infrastructure players, too. Amazon, Google, and Alibaba.
And then there is Apple.
These companies, plus the likes of Facebook and Netflix, sit on the one commodity that becomes more valuable by the minute: Data.
Alea iacta est! Non tacitus!
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
"
https://medium.com/@iceburn/microsoft-azure-iaas-virtual-machine-security-improvement-d4dedc19b5f?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maciej
Jun 17, 2021·5 min read
In this post , I’d like to summarize what you need to do to make your Azure virtual machine more secure. Below there are some factors to consider to make your Azure virtual machines more secure:
Enable MFA on your portal The Azure portal is very versatile and allows you to configure and perform a variety of operations. It’s good at the time of verification, but I would like to improve security when performing production operation. So, in production production cases, enable MFA to improve security when accessing the Azure portal.
docs.microsoft.com
Properly control permissions using RBAC (Role Based Access Control). Properly deploy virtual machines by leveraging resource groups (RGs) and subscriptions. In Azure, where only the virtual machine administrator is given permission to access virtual machines in the first place, there are units called subscriptions and resource groups as units for managing resources. Access control can be applied to each unit. This is what we call RBAC. Give proper access to the right resources.
Leverage the Update Management feature Azure provides a feature called Update Management that manages security patches. There is no way not to use this. Updates that have not been applied are displayed on the dashboard, and you can set what rules and when to apply updates. In addition, you can manage security patches for virtual machines that run outside of Azure.
docs.microsoft.com
It is necessary to properly control access between the external network, the internal network, and multiple networks. The Azure services that realize these are as follows.
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
There are many possible ways to access a virtual machine securely. For example, you can use Express Route or VPN to connect with a private network. That said, line costs are high, but opening a port accessible from the Internet increases the risk of being attacked, which is usually not possible. In response to such problems, the actual solution so far has been to use a bastion server . However, if you use a bastion server, you will need to manage the operation of that server. Therefore, Azure Bastion provides the bastion server in a managed manner. You can access virtual machines that do not have a Bastion public IP address via Bastion. The route at this time is to access the virtual machine from the portal screen via a browser. Since the Bastion server is managed by Azure, there is no need for the user to manage it, and there is no need to set up NSG.
docs.microsoft.com
Just in Time this is service that opens a port for access to a virtual machine that has a public IP address for a limited time.
docs.microsoft.com
One of the features of Azure Security Center is adaptive application control. This feature allows you to use machine learning to analyze applications running in virtual machines and create application authorization lists.
docs.microsoft.com
Encrypt the Azure virtual machine disk. By encrypting the virtual disk, even if the virtual disk is leaked, the data inside is prevented from being accessed.
docs.microsoft.com
docs.microsoft.com
You can track changes made to the virtual machine’s OS and software. Whether it’s intentional or not, it’s important to check for changes.
docs.microsoft.com
Let’s evaluate continuously by using the function of Security Center, not limited to IaaS. You can also evaluate the on-premise environment.
Make a backup in case of unforeseen circumstances. Azure Backup is recommended because it is easy and has many functions such as online backup acquisition and restore on a file-by-file or VM-by-VM basis, and it is easy to implement.
Since the latest image is often updated and vulnerable, use the latest image as much as possible.
Would you like to open the NSG port in a slightly wider range just in case? If the security center function is different from the actual traffic used and the NSG rule, the machine learning algorithm will suggest a narrower rule.
Azure has a lot of security features to protect virtual machines, so please make good use of these features to ensure security.
DevOps Consultant. I’m strongly focused on automation, security, and reliability.
See all (50)
No rights reserved
 by the author.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
DevOps Consultant. I’m strongly focused on automation, security, and reliability.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloudwarebg/the-differences-between-saas-paas-and-iaas-34d11a3f6345?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloudware.bg
Aug 6, 2019·2 min read
IaaS, SaaS, PaaS, they all sound like unreal words from a comic book.
Actually, they are all different implications of what we called “cloud”. These are 3 pure versions of the options that you have when you want to use cloud technology.
This is the most well-known variant of a cloud. Many applications that you use are based on this model: Google Apps, Dropbox, OneDrive and many more. They deliver their service over the internet. Users can use it through their browsers, thanks to the companies’ APIs. Advantages of this model are that the users can manage their data from different devices and a failure of a device doesn’t result in data loss.
A typical example of this model is Microsoft Azure. It is a platform where the developer can create their applications. It has all that is needed to develop and the hardware is managed by the provider.
Advantages: Provides a secure framework for software development; It helps companies, by taking care of the hardware infrastructure; It provides a platform, that has tools for testing, developing and hosting applications.
In this model, companies, like our Cloudware provide the hardware infrastructure (servers, pre-configured or not). Users use it to create virtual machines on it, use it for networking purposes or storage solutions.
Advantages: Instead of purchasing hardware and get dependent from the ecosystem of choice, users can just rent it; Scalability, it is easy to expand; It is safer than having your own servers at your company.
There is no simple answer to this question, better consult with a specialist and explain all your needs. You can get a professional opinion from our technical team at info@cloudware.bg
Get proper information and try renting first, until you know your needs perfectly.
Reliable Dedicated Servers & High-performance Cloud Servers
Reliable Dedicated Servers & High-performance Cloud Servers
"
https://medium.com/futurists-views/innovation-in-modern-it-f70e3e93d7a?source=search_post---------395,"There are currently no responses for this story.
Be the first to respond.
“We are now seeing an emerging tension point between developers and CIOs, which is shifting the CIOs’ ability to be the driving force for innovation and change within their business. This is a risk for CIOs who don’t adapt, especially given that developers are fast becoming the bedrock of IT innovation today and their numbers are growing: the total developer population worldwide is expected to grow from 18.2m today to 26.4 million in 2019, according to Evans Data Worldwide.Developers that work in large enterprises should be considered the internal engine of innovation to the companies they work for. However, it is regretfully the case that IT budgets remain relatively flat and more often than not the developers are being asked to quarterback new projects that deliver competitive advantage. ” CIOs Must Adapt to Embrace Developer-Led Innovation
“In traditional IT, a wide gap exists between developers and operations. The priorities and goals of these teams are often misaligned, causing a wall of confusion. Developers want to move quickly from idea, to code, to production. Operations wants predictable production deployments to remain stable. Developers need rapid change and operations needs stability (and hence a desire for minimal change).The great wall of confusion between development and operations leads to typical enterprise slowdowns. Next comes the typical finger pointing between various stakeholders, each protecting the success they are responsible for. This wall between development and operations is cultural and a byproduct of their misaligned goals. It’s also aggravated by the lack of good available tools. Another source of misalignment is the lack of standardized tooling for development, testing, and production. To aggravate things further, for many teams, the development and test environments are different from production.Moving to cloud doesn’t automatically avoid these consequences. Unless the existing culture and processes change, you’ll only experience iterative improvement over traditional IT as the wall of confusion continues to slow down the application delivery process.”DevOps, PaaS and Modern IT
“Users are demanding new applications and services to engage with the new consumer-centric world. To satisfy this demand, I have seen a pattern of integrated approaches to cloud-enabled application delivery emerging across development and operations teams. This builds on accepted DevOps practices and couples them with public or private cloud infrastructure provisioned to greatly reduce the time taken to deliver new applications into production. ”How Software Defined Environments and DevOps accelerate social and mobile delivery
“A survey of enterprises by IBM has showed that those who adopt and leverage cloud computing for competitive advantage on average grow twice as fast and double their profit. The company’s Walter Falk, Christopher Ferris, Michael Fork and Rob Sauerwalt outline several areas where businesses can make gains.Continued technology advancements in cloud computing are creating exceptional opportunities for enterprises to transform business models, supply chains, and their interactions with customers and partners. New technologies and tools are transforming traditional software development lifecycles.This has a profound impact on how enterprises should think about their business”Why you should use cloud computing
“You must ensure that you provide great service to your hosted customers on day-to-day basis. This will ensure the subscription renewals, position you as a trusted partner for your customers and show them real cost savings. At the same time, you have to be able to provide your customers with data security in compliance with the law, data back-up and disaster recovery—all while meeting the promised SLA (Service Level Agreement). As you do this, try to minimize the maintenance downtimes. Provide enough notice to customers for essential maintenance downtimes to minimize any disruption. Invest in building efficient support teams. Remember, it is easier to switch SaaS vendors, so exceptional support and service is the key customer retention. ” 7 considerations when moving on-premise software to cloud
“Platform as a Service (PaaS) is the abstraction of the underlying application stack. With PaaS, developers don’t have to worry about managing operating systems, databases, application servers or programming stacks and can focus on business requirements.What’s the greatest hindrance to PaaS adoption? Not every workload is a smart candidate for PaaS.For workloads with extreme transaction processing requirements, developers need more control over the application stack in order to meet the desired performance requirements and SLAs. Today, only IaaS provides developers with this level of control. ”Blurring The Line Between PaaS And IaaS
Follow: @francescoeamMy blog (it-IT) : www.francescopollice.it
Predictive Thinking. Reading trends 
Written by
«I may dress like a nerd, but I can read trends». Nerd, or Geek. I cannot decide. #futurism, #tech, #innovation — www.francescopollice.it — @francescoeam
Predictive Thinking. Reading trends 
Written by
«I may dress like a nerd, but I can read trends». Nerd, or Geek. I cannot decide. #futurism, #tech, #innovation — www.francescopollice.it — @francescoeam
Predictive Thinking. Reading trends 
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/saas-vs-iaas-vs-paas-cloud-computing-models-explained-with-its-benefits-276298aea66d?source=search_post---------21,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Cloud Computing has become a hot topic for business and IT industries because of the advantages it provides to the digitally connected world to drive their business forward.
Cloud Computing has grown exponentially over the years and expected to enhance at an even faster pace. If you as an entrepreneur thinking to switch your business to the cloud, whether it is infrastructure deployment or application, make sure that you have got a clear idea about the differences and benefits of various models of cloud services.
www.datadriveninvestor.com
Cloud computing integrates mainly three models that are:
1. Software as a Service (SaaS)
2. Infrastructure as a Service (IaaS)
3. Platform as a Service (PaaS)
Each of these models delivers varied services and benefits. Thus, before picking any model for your business, ensure to understand the difference between SaaS, PaaS, and IaaS. Here below you can find the intro of each model and what services and benefits they offer.
1.Software as a Service (SaaS):
Software as a Service (SaaS) is a crucial element of the cloud-computing ecosystem. It helps the third party to rent or subscribe to a software application and execute it online, rather than purchasing, installing, and managing it. Today, most of the IT sector and businesses are adopting SaaS applications for fundamental business technologies such as email, CRM, HRM, Sales management, financial management billing, and collaboration. The leading SaaS providers incorporate Salesforce, Oracle, SAP, Intuit, and Microsoft.
Benefits of SaaS:
It reduces the time spent in installing and configuring of applications as well as issues that can be faced in the way of the software deployment.
It is cost-effective as you have to pay only for what you are using and not pay heavily on-used licensing.
The SaaS feature has flexible options as it is easy to change the usage plans according to the needs. It also allows subscribers from any location to access the software easily.
It is compatible with the new upgrades, as the subscribers have to simply log-on to already upgraded services.
2.Infrastructure as a Service (IaaS):
Infrastructure as a Service (IaaS) is an instant computing infrastructure, provisioned, and managed over the internet. Many organizations are investing in IaaS for accessing, monitoring, and managing remote datacenter infrastructures. Instead of purchasing the hardware outright, users can purchase IaaS as per the resource on-demand.
Benefits of IaaS:
It includes the ability to scale the resources up and down quickly as per the need of the customers.
It is affordable and cost-effective as you need to pay only for what you use.
It decreases the other expense and complexity of purchasing and handling your physical servers.
It also offers a consolidated disaster recovery infrastructure, by lowering down the costs and increasing manageability.
3. Platform as a Service (PaaS):
Platform as a Service (PaaS) is a cloud computing model in which the third-party provides users with hardware and software tools. Many small businesses rely on PaaS providers for development kits, database tools, and application management requirements to create and deploy applications. The PaaS providers also handle the infrastructure, which includes network, servers, operating systems, and storage.
Benefits of PaaS:
It assists the app makers to develop and deploy the apps in a simple and affordable way.
It allows the developers to create customized apps without any stress of maintaining the software.
It lowers the burden of developers by reducing the amount of coding.
It helps in streamlining the complete application management by reducing the unnecessary human configuration tasks.
Conclusion:
Every cloud models have various features and functionalities that fit for various industry needs. At GoodFirms, you can reach the cloud computing service providers that are evaluated and listed based on various qualitative and quantitative factors.
You can associate with the top cloud computing companies whether you are searching for cloud-based software to create inventive customized applications, looking for varied storage options or need to handle your entire infrastructure. Choose the best-suited model and migrate to the cloud for better prospects of the business.
empowerment through data, knowledge, and expertise.
69 
69 claps
69 
Written by
Research Analyst, Content writer at GoodFirms.co
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Research Analyst, Content writer at GoodFirms.co
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/iaas-edge-kubernetes-why-cloud-adoption-is-still-early-and-still-evolving-34be60080303?source=search_post---------25,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this episode of the ARCHITECHT Show, Capital One vice president of cloud strategy Bernard Golden and AVOA strategic adviser Tim Crawford discuss the current state of cloud computing and the trends driving companies to — and away from — the public cloud.
Golden suggests cloud adoption today is, to use a baseball analogy, in the second inning, with many changes still to come. Among the topics they cover as part of this debate are edge computing, private cloud, Kubernetes and the opportunities for large cloud providers to expand their presence.
Scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
This episode is sponsored by:
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
2 
1
2 claps
2 
1
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@umashankarn/automating-k8s-mongo-aws-prometheus-929f458bb8f6?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
Umashankar N
Aug 8, 2019·2 min read
Creating a configurable Kubernetes cluster on AWS EC2 instance with MongoDB and Prometheus.
There is EKS in order to jump start Kubernetes(K8s) in AWS, though it is still evolving in few aspects. If the intent is to leverage many of bleeding edge flexibilities and features of K8s then the choice is to have it on AWS EC2.
This article provides high level steps to deploy the following along with link to Github repo with all scripts and configurations and readme with detailed instructions:
a) K8s on AWS EC2 with your configuration choices of instance types and VPC subnets leveraging ‘kops’
b) mongo cluster with sharding, including config and router with configurable choice of number of replicasets and shards
c) prometheus with the necessary exporters of node and mongo
The scripts and configurations are available through ‘1CloudHub GitHub’ public repository here.
Who can use this?
This will be helpful for developer, Ops or DevOps person to create kubernetes cluster in AWS IaaS and install mongoDB with minimal manual work.
Key benefits:
Walk through of high level steps
Prerequisites: The git repository instructions covers the Pre-requisites including IAM access role for the client instance from where the script to be executed, s3 buckets and installation of AWS cli, kops, kubectl and anscible.
K8s Cluster & Mongo Cluster: First it helps to deploy K8s on AWS EC2 leveraging kops. Once the cluster is up and running mongo db cluster will be deployed on top of the kops.
Kindly note, mongodb clusters are launched using Statefulsets and hence will have its own EBS volumes created and which provides additional reliability for the mongodb cluster on top of K8s.
Prometheus & exporters: Finally this deploys prometheus along leveraging prometheus operator along with grafana. The required exporters including the mongodb exporter and node exporter are added along and starts to monitor mongodb cluster, k8s and nodes on which they run on.
Happy deployment folks.
Leave your feedback feel free to extend and make the scripts suit your needs.
Repository details:
github.com
Thanks to Geetha Pandiyan, Cloud Solutions Architect, 1CloudHub, in creating this reusable automated scripts and instructions on github.
CTO, 1CloudHub
12 
Some rights reserved

12 claps
12 
CTO, 1CloudHub
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cuelogic-technologies/saas-paas-iaas-decoding-the-3-cloud-computing-service-models-8f03b6c2be41?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
Over the years, this technology paradigm has evolved through multiple phases. The earlier forms of computing that preceded modern cloud computing included grid, utility, and on-demand computing. The earliest forms of modern cloud computing that include Software (SaaS), Platform (PaaS) and Infrastructure (IaaS) emerged as a technological outcome that attended the dipping costs of computer and server hardware. Users could purchase individual servers to power their computing requirements.
The cloud paradigm emerged when software makers and hardware vendors combined multiple servers in a concerted bid to harness the immense computing power generated by a grid (or network) of connected servers. Concurrently, the evolution of digital connectivity technologies that underlie the World Wide Web in recent years formally brought about the modern concept of “cloud computing.” In recent times, the purveyors of technology have parlayed cloud-computing systems into multiple tiers of service, variously labeled as SaaS, PaaS, and IaaS.
(SaaS) is a software licensing and delivery model that has gained a significant presence in a wide range of modern corporate, business, scientific, and commercial applications.
SaaS technologies allow users to license proprietary software on a subscription basis — monthly or annual. As providers of an ‘on-demand’ service, SaaS service providers host the software on the cloud to which users connect through a browser and an Internet connection. As a hugely cost-effective alternative to on-premise software installations and packages, the SaaS model seamlessly delivers a variety of applications that pertain to enterprise resource planning programs, office, and communications software, payroll and accounting packages, human resources management, mobile applications, etc.
This computing paradigm remains vulnerable to unauthorized access and malevolent hacking expeditions in online domains. Digital miscreants have targeted businesses that operate on the cloud by blocking customer access to critical online systems. This poses real risks that can translate into an erosion of market value for SaaS service providers. In response, service providers must continuously invest in improving security and authentication processes on the cloud, thereby delivering incremental assurances to their clients and customers.
Additional challenges that figure in the development of SaaS-powered products and services include custom third-party payment integration, safe and well-defined database access compliant with GDPR norms, guaranteeing zero-downtime deployment,managing the subscription lifecycle, and building a fully customizable SaaS system.
PaaS is “a category of cloud services that provide a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.”
PaaS providers host the hardware and software on their infrastructure, thereby releasing customers from any obligation to install in-house hardware and software to develop or run a new application.
PaaS technologies pose particular problems and risks for service providers. These challenges include balancing control, cost, and capacity of a PaaS-based service, providing full multi-tenancy support, designing role-based access controls, creating audit trails, and integrating third-party services into modern PaaS platforms. Additional challenges may emerge in the form of virtualization management, fine-tuning the PaaS compute architecture, designing inter-operability with other cloud services, and creating technically sound fault tolerance parameters.
IaaS operates by traditional cloud architecture. Per the IaaS cloud-computing paradigm, service providers host the infrastructure such as servers, storage units, networking hardware, virtualization or hypervisor layer, etc. This model negates the legacy business case for investing in on-premise data center infrastructure and equipment. Modern IaaS service providers also offer policy-driven services to clients and customers. These services include monitor service performance, detailed billing of customer services, log access, digital security, load balancing, backup, replication, and recovery, etc.
IaaS represents “the virtual delivery of computing resources in the form of hardware, networking, and storage services. It may also include the delivery of operating systems and virtualization technology to manage the resources. Rather than buying and installing the required resources in their own data center, companies rent these resources as needed,” according to popular definitions of IaaS.
A raft of business challenges have emerged to face service providers that offer IaaS services to clients and customers. These may include subscriber expectation management, defining support systems that handle different forms of payments from customers, accommodating the need for custom analytics that measures customer profitability and usage, managing the service value chain, and controlling business with multiple partners. Also, service operators must remain agile in terms of experimenting with product prices, product features, the configuration of service packages, and navigating the intricacies of customer licenses. They must also work to actively manage customer expectations and refine the concept of datacentre ‘in-the-sky.’
The business case for cloud computing technologies and frameworks will continue to burnish its relevance and utility years and decades into the future. The large and incrementally enormous volumes of data generated by modern scientific, commercial, and technological enterprises will require larger data centers powered by innovative technologies. These may form the central planks of local, regional, and national economies in the future. That said, the central role of the cloud may morph into differentiated expressions, marshaled by real-time processing technologies and a deeper engagement with refined versions of civilizational requirements.
Source: Cuelogic Tech Blog
Tech for leaders & developers
5 
Thanks to Kiarash Irandoust. 
5 claps
5 
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-capital/legal-risks-in-cloud-computing-sla-saas-iaas-and-paas-agreements-a3fa450a5a2d?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
Nо dоubt you’ve hеаrd thе tеrm сlоud computing or сlоud ѕеrvісеѕ еvеrуwhеrе, еѕресіаllу in thе IT world and wоndеrеd whаt іt wаѕ, аnd whаt іt mеаnѕ. Now уоu can rеlаx because after rеаdіng this аrtісlе уоu wіll get tо understand thе tеrm CLOUD COMPUTING.
WHAT IS CLOUD COMPUTING?
Clоud соmрutіng is used tо соvеr a multіtudе of trеndѕ rеlаtеd to the іntеrnеt аnd wоrk processes. It іѕ mоѕt соmmоnlу uѕеd tо dеѕсrіbе thе асt of ѕhаrіng dаtа, fіlеѕ, аnd software оvеr thе іntеrnеt to users in rеmоtе lосаtіоnѕ.
Cloud соmрutіng rеfеrѕ tо the delivery of соmрutіng in thе fоrm оf a ѕеrvісе and nоt аѕ a рrоduсt. This is a service, where ѕоftwаrе, ѕhаrеd resources аnd information are dеlіvеrеd tо dеvісеѕ like utility оvеr networks. Thіѕ means that as an еnd user, уоu аrе do not knоw the configuration аnd рhуѕісаl lосаtіоn оf thе ѕуѕtеm used tо dеlіvеr thе services. The cloud is usually shared with other users/subscribers. This technology іnсrеаѕеѕ сараbіlіtіеѕ wіthоut thе nееd for nеw іnfrаѕtruсturе, реrѕоnnеl аnd software licensing. Thе ѕеrvісеѕ саn bе рау-реr-uѕе or ѕubѕсrірtіоn bаѕеd.
The сlоud is еffесtіvеlу thе Internet аnd cloud соmрutіng іѕ thе term applied to the uѕеr рuttіng data into thе Internet as орроѕеd tо on a PC or lосаl server.
There аrе various advantages thаt uѕеrѕ gеt frоm сlоud соmрutіng.
LEGAL CHALLENGES
In аѕ muсh аѕ wе аgrее thаt сlоud computing is bringing quite аn amazing benefit tо everyone, there are legal aspects that need to be satisfied, and certain rules to be complied with. And hеrе аrе ѕоmе оf thеm.
It is vital that the cloud provider determines, usually, in the Terms of service, which law governs the contractual relationship and what under which jurisdiction the disputes shall be ruled. Normally, the cloud provider will determine its own national jurisdiction and applicable law, however, if it fails to do so, the competent court will have a great deal of trouble determining both.
Another legal aspect is the storage of data, specifically the geolocation of the storage of the data. Most of the cloud providers determine in the Terms of service that they are allowed to place data anywhere they wish and across multiple geolocations, regardless of the origin of the data. But, this can raise problems. Firstly, it raises the question of export control. If the data contain personal data, the issue of processing the data within the EU or outside of the EU could also arise.
As already said above, cloud providers do provide a certain level of security, however, they have to do determine their commitment in the Terms of service, to respect the industry standards of the security in order to ensure a greater level of security. Moreover, such a pledge to its users shall be audited periodically. Users shall be notified immediately if and when the data breach happens.
All data, that are collected for a specific purpose, shall only be used for that purpose. This is, however, often not the case, especially, when users of a cloud service give permission (without even reading the Terms) to the use of their data for other purposes as well. GDPR enacted a strict double opt-in for using personal data only in cases specified in the boxes, which every user must tick before letting a company use its personal data.
Lіkе we hіghlіghtеd рrеvіоuѕlу thаt all сlоud ѕеrvісе рrоvіdеrѕ is a third-party соmраnу рrоvіdіng ассеѕѕ to a server together wіth thе appropriate software аllоwіng the uѕеr tо ѕtоrе dаtа оn thіѕ ѕеrvеr. All thіrd раrtіеѕ using a multi-tenant shared cloud аrе uѕіng the ѕаmе аdmіnіѕtrаtіоn іntеrfасе, ѕо mаkе sure multі-fасtоr аuthеntісаtіоn and еnhаnсеd ѕесurіtу are present.
IT is important that a cloud provider understands that the data he is keeping in his cloud is not his but his users’ and that the cloud provider does not acquire any ownership, license, monetary or other similar rights upon using the cloud service.
Many such services face the risk of misuse of the service by placing inappropriate or illegal data in the cloud. Normally, service providers switch liability onto users for such misuse of the service and place a requirement onto customers to report all he suspicious, illegal or inappropriate use of the service.
Cоруrіght laws dіffеr frоm оnе соuntrу tо аnоthеr, ѕо it is nоt vеrу clear whаt copyright lаwѕ shall аррlу in the сlоud environment. Cloud providers should define intellectual property rights in the Terms of service. As already said, the cloud provider рrоtесt your dаtа and intellectual property and have safeguards in place for protecting intellectual property and avoiding роtеntіаl infringement ріtfаllѕ.
Another provision in the Terms of service is a provision about the Warranties. Typically, service providers disclaim all liabilities to the highest extent permitted by the law, which they also apply to guarantees that in case they might be in breach of intellectual property of a third party. It is important that the service provider guarantees that all the features, security standards, and functionalities are the same as advertised, offered and accepted by the customer. Additionally, the cloud provider must guarantee not to be in breach of any intellectual property of the third parties. Without such a warranty, customers will not have any protection in case of a breach or failure and no recourse against the cloud provider.
Make sure to always carefully read the Terms of service of the cloud provider and check for the provision of automatic renewal of your contract, or one-time purchase. You do not want to be surprised in a few weeks or months that your Visa or MasterCard account was charged. A cloud provider is obliged to provide the notice to a customer about an automatic renewal before the renewal happens and before the amount os charged.
It happens that cloud providers change their Terms of service, and it can happen quite often. It is important that the cloud provider notifies its customers of the change and offer them to cancel the agreement in case if a customer does not agree with the changes.
SAAS, PAAS and IAAS AGREEMENT
All of the three models are unique and demand a great caution when negotiating the agreement, not only from a legal but also from the technical point. The agreements must carefully define:
Another word for such service agreements is a so-called Sеrvісе Level Agrееmеnt (SLA). All of the agreements set out rules for the minimum level of service availability, quality, accessibility, rights and responsibilities, customer support, etc.
Together with the Tеrmѕ of Sеrvісе, the SLA shall contain appropriate disclaimers, prіvасу policy, cookies policy, along with аnу оthеr dосumеntаtіоn or соndіtіоnѕ thе соmраnу wіѕhеѕ tо іnсludе.
If you need a tech lawyer, contact us here.
Originally published at iuricorn.com.
Follow us on Twitter, InvestFeed, Facebook, Instagram, LinkedIn, and join our Discord and Telegram.
Read about our upcoming Altcoin Magazine Mastermind Event here.
A publishing platform for professionals now available on https://thecapital.io
Head over to https://thecapital.io, sign up and publish your first article today! Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Written by
Top technology lawyer, advising on technology-driven legal solutions and strategies in financial services, blockchains, AI, cloud computing. www.iuricorn.com
A publishing platform for professionals in business, finance, and tech
Written by
Top technology lawyer, advising on technology-driven legal solutions and strategies in financial services, blockchains, AI, cloud computing. www.iuricorn.com
A publishing platform for professionals in business, finance, and tech
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/from-single-tenant-iaas-to-multi-tenant-paas-multi-tenant-isolation-with-maxcompute-47280c741117?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 18, 2018·8 min read
In the big data session of the 2017 Computing Conference Beijing Summit, Li Xuefeng, a senior technical expert from Alibaba Cloud, talked about multi-tenant isolation on a financial big data platform. He started his speech with the problems of tenant isolation in a traditional single-tenant IaaS architecture and then talked about the multi-tenant PaaS architecture of Alibaba Cloud MaxCompute and how MaxCompute implemented secure isolation. We will discuss these architecture details in this article.
As shown in this figure, the bottom layer of the single-tenant big data product architecture is HDFS2, on which resource control platforms such as Hadoop Yarn and MESOS reside. We can implement specific computing models, such as MR, Hive, HBASE, and Spark over the resource control platforms. In this ecosystem, the IaaS platform is generally available to the same tenant. When new business requirements arise, the tenant can apply for a batch of VM clusters on the IaaS platform, and then deploy open-source products on the clusters. This ecosystem encounters following problems from the perspective of isolation:
Firstly, the IaaS single-tenant big data product architecture has logic issues in actual use. Users must know the specific logic of each product for data analysis. For example, users must understand the Hive logic when using SQL and possess knowledge of Spark when using Spark. It is possible to control the cost of learning at a low level with fewer products, but this increases exponentially when we have to use multiple products collaboratively. Additionally, different open-source products usually cannot identify the logical model of each other, which worsens the logic issue in authentication scenarios.
Secondly, each open-source product has its priority definition for system operation. When we use only one open-source product, jobs get executed based on the priority system of this product. Jobs of higher priorities obtain more resources than those of lower priorities, and we get a better estimate of their running durations. However, when we have to use multiple open-source products together, the IaaS single-tenant big data architecture cannot optimize the job execution priorities globally.
Lastly, open-source products often offer user-defined logic, for example, UDF of MR or Hive. Running user-defined code in big data products brings security risks. For example, Hadoop Yarn simply isolates user-defined code with Linux Containers. In this isolation mechanism, the user-defined code logic runs in the same kernel as the Hadoop process. If the attacking program in the code logic affects the kernel, the big data product processes running in the same kernel are also affected. Generally, a job of a big data product runs on most of or even all machines in a cluster, depending on the size of the data shard. In this case, the entire cluster becomes vulnerable to security risks. In an extreme condition, the entire computing cluster may break down when a hacker exploits a kernel vulnerability to attack a machine successfully, and the job shard is large enough.
To address these problems, MaxCompute offers PaaS multi-tenancy capability using a proprietary system architecture.
In the above PaaS multi-tenant architecture diagram, MaxCompute runs over the Apsara operating system. It depends on the Apsara Fuxi module to provide unified resource control, the Apsara Pangu module to provide unified storage, and the Apsara Nvwa module to provide consistency service. MaxCompute uses the same computing engine to offer multiple computing models, including SQL, MR, graph computing, PAI, and near real-time.
Now, this computing engine offers qualified computing capabilities for financial users on the public cloud.
MaxCompute uses the following methods to implement multi-tenancy:
We will now discuss these three isolation mechanisms offered by MaxCompute in detail.
Currently, a MaxCompute instance provides a unified tenant system, no matter how many physical clusters the instance is running on. In this tenant system, the data resource view and privilege management model for the same tenant is unique and bound to the tenant model. In real-world applications, a tenant on MaxCompute maps to a project, which contains all resources, properties, and privileges of the tenant.
As shown in the preceding figure, a project consists of three parts: properties, subject, and object. Properties of a project include information such as quota, owner, payment account, and region. All authorized accesses in a project must use the user IDs as the subject, based on which MaxCompute offers a role model for authority aggregation. The resources we have used in the above-mentioned computing models (such as MR and Hive) are all mapped to a specific object in a project. For example, resources in the SQL model are table objects, and resources in the UDF model are function objects.
Based on the preceding logical model, MaxCompute offers a set of authentication and authorization mechanisms for privilege control. A project owner has all the privileges over the project. Any user who wants to use this project for computing must get authorization from the project owner first (using the GRANT statement). When accessing the project, a user submits its user ID as the user identity to perform operations such as reading and writing tables, creating functions, or adding and deleting resources. MaxCompute uses a unified ACL logic to determine whether the current user ID has the required privilege before it allows execution of such operations.
The computing engine of MaxCompute depends on the Apsara operating system to offer resource operation and isolation capabilities.
As shown in the preceding figure, when we submit Job-0 to Job-n to the Apsara Fuxi module, the Fuxi scheduling system assigns operation levels to these jobs based on operation levels of users. The operation levels correspond to properties in a project. The Fuxi module converts Job-0 to Job-n into Fuxi jobs and then dispatches them to nodes of a computing cluster. Finally, a server in the computing cluster runs jobs of multiple tenants simultaneously. All these jobs run as Fuxi workers.
When the Fuxi engine on a machine in the cluster receives a worker plan, it sets the Cgroup parameter on this machine based on the quota of the user to which the worker belongs. In this way, jobs submitted by different users run with different Cgroup parameter settings on the physical machine. Currently, MaxCompute leverages the Cgroup capability of the Linux kernel to allocate CPU, memory, and other resources to a certain process on a physical machine.
Finally, let’s have a look at the operation isolation mechanism provided by MaxCompute to ensure secure operation of user-defined logic. When the Fuxi module runs user-defined code logic, it pulls an isolated environment and runs the code in an isolated process. For the Fuxi module, this process is the same as other processes but runs in an isolated system. That is, this is a common process for the Fuxi module but is isolated from untrusted code processes.
We can classify operation isolation further into process isolation, device isolation, and network isolation.
A single process running untrusted code (which may contain malicious code) may damage the computing platform. MaxCompute offers an embedding isolation solution to prevent this potential security risk. The innermost layer of this solution provides Java sandbox and Python sandbox. The language-specific sandboxes implement innermost isolation. For example, Java UDF can restrict the classes that can be loaded, and Python UDF can restrict specific functions. At the intermediate layer, MaxCompute isolates processes based on Linux kernel mechanisms, including namespace, Cgroup, and secomp-bpf. At the outermost layer, MaxCompute offers lightweight VMs (created in several hundreds of milliseconds) using in-depth custom Linux kernel and a minimized hypervisor. Finally, the untrusted code runs on a hypervisor over the physical machine. The Fuxi module treats the untrusted code as a hypervisor process, but the untrusted code is running in an isolated environment.
MaxCompute also supports hardware acceleration for a user-defined code. For example, PAI supports direct GPU access. MaxCompute supports GPU pass-through into a VM in PCIe pass-through mode, which allows guest processes to access the GPU over the PCIe bus and GPU driver in the guest kernel.GPU access from a VM over the PCIe bus has a similar speed to GPU access from a physical machine. Also, this GPU access mode removes the need to install the GPU driver on a physical machine, thereby eliminating the impact of GPU driver on platform stability and reliability.
MaxCompute offers the network isolation capability for users’ code logic on some products. It creates a virtual network between the VMs provisioned by the Fuxi module. These VMs can communicate directly through the virtual network, ensuring compatibility between open-source code running in the VMs. You can also see from the preceding figure that the user-defined code logic does not connect to the physical network directly, whereas the trusted code running on Fuxi, including the code in the MaxCompute framework, uses the physical network for communication. This guarantees a low communication latency in the MaxCompute framework.
We have discussed how Alibaba Cloud MaxCompute uses logical isolation, resource isolation, and network isolation methods to provide secure isolation for big data processing. You can learn more about MaxCompute and other Alibaba Cloud products and solutions at www.alibabacloud.com.
Reference:
https://www.alibabacloud.com/blog/from-single-tenant-iaas-to-multi-tenant-paas---multi-tenant-isolation-with-maxcompute_593817?spm=a2c41.11774969.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloudready-ch/cest-quoi-iaas-paas-et-saas-le-cloud-c169451d73bc?source=search_post---------167,"There are currently no responses for this story.
Be the first to respond.
Agendez une séance chez vous: http://callme.kotte.net
Dans le Cloud computing, ces trois abréviations ne sont même plus expliquées car elles entrent dans la culture numérique générale. Mais en 2016, plus de 80% de mes concitoyens en Suisse ne savent probablement pas ce qu’est seulement le ‘Cloud computing’…
Et pourtant, tout le monde utilise du Cloud computing
Gmail, Bluewin, sont des solutions ‘SaaS’… Ou ‘PaaS’ si nous les utilisons uniquement avec un client de messagerie (Eudora, Thunderbird, Outlook…).
Un autre ‘SaaS’ bien connu: Facebook, il fût parmi les premiers à reconnaître que les données saisies dans nos profils et fils de discussions nous appartenaient, avant même que le juridique soit à même de définir le statut de ces extensions numériques, dans notre humanité.
Mais c’est pour immédiatement léguer le droit à Facebook d’en user et abuser librement, dans leurs conditions générales… Le prix du gratuit, est-il donc léger?
L’intégrité humaine dans sa dimension digitale, n’est toujours pas reconnue ni inscrite dans les constitutions, ou pire, elles sont reconnues et refusées pour partie (Comme la constitution américaine qui reconnait le droit à la vie privée, mais aux seuls ressortissants américains, incompatible avec celle de l’UE).
Alors, il est important de mieux comprendre le Cloud!
Le ‘Cloud’ n’existe pas! C’est juste sur les ordinateurs de quelqu’un d’autre…
Ainsi, nous déléguons le traitement, le stockage de nos données, sur des systèmes qui ne nous appartiennent plus.
Mais si je vous prête une clef USB de stockage numérique, que vous posez un fichier d’adresses dessus. A qui appartiendra désormais la copie de ce fichier, dans la clef dont je possède la facture et preuve d’acquisition?
Sur le sol Suisse, ce sera moi le propriétaire de cette copie de votre fichier. Si ce fichier était un dessin signé, permettant d’en démontrer la paternité, pas de soucis, le droit et la protection à la propriété intellectuelle peuvent être mise en oeuvre, et en interdisent l’usage sans autorisation du propriétaire. Mais un simple fichier de contacts? Vous devrez y inclure des “signatures” non équivoques pour en prouver la paternité, et ensuite expliquer et prouver pourquoi vous l’aviez déposé sur cette clef USB (devant témoin?), si ce n’est pas pour le céder sans conditions? Et in fine, ce sont les personnes incluses dans ce fichier qui pourront s’en offusquer et selon les contenus, vous poursuivre en justice pour divulgation d’informations privées, ou pour lacunes sécuritaires...
Bienvenue à l’ère digitale!
Le retour de la nécessaire confiance!
Alors, il est temps de s’intéresser aux conditions et termes, des “ToS” et “CGU” que nous acceptons sans les lire, comme ceux d’Apple (et son iCloud), qui font l’équivalent de presque deux fois Macbeth de Shakespeare… Car elles n’ont plus rien à voir avec nos “EULA” (End User License Agreement) d’il y a moins de 20 ans, lesquelles se contentaient essentiellement de dégager la responsabilité du fournisseur, au lieu de l’autoriser à abuser de nos propres données…
D’autant que ce n’est plus simplement des applications en ligne que nous déléguons dans le nuage de l’Internet, mais l’ensemble de nos équipements informatiques, que nous allons louer et consommer à l’usage, comme le téléphone, l’eau ou l’électricité, au lieu de les acquérir…
Le Cloud, une fusée à 3 étages!
Que ce soit avec Amazon, le précurseur; avec Microsoft Azur, le suiveur: Ou encore OVH ou Infomaniak, ou bien parmi les milliers de fournisseurs désormais disponibles sur le Net: Il est désormais possible de créer des applications informatiques, ou d’intégrer des applications existantes, sur des serveurs virtuels, en location, dont nous ne verrons jamais le moindre élément physique. Ainsi, je peux activer un serveur Web en utilisant Tomcat d’Apache, sur un serveur Linux, en quelques minutes. Plus de 90% des services Cloud tournent sur des serveurs virtuels, sous un Linux ou autre Unix libre. Les solutions libres ne sont pas simplement “gratuites” en termes de licences, mais aussi moins gourmandes en termes de ressources sur les serveurs, que les solutions propriétaires comme Windows, ou MacOS (bien que ce dernier soit aussi basé sur un noyaux Unix). Comme les solutions ‘IaaS’ font payer la location en fonction des ressources consommées (CPU, RAM, Disque), alors il faut optimiser. L’offre ‘IaaS’ s’adresse principalement aux administrateurs de systèmes qui ne souhaitent pas gérer leurs propres équipements matériels serveurs (même dans un Datacentre tiers). En ‘IaaS’ je loue le serveur, généralement virtuel, et paye l’usage effectif, mois par mois (ou minute par minute).
Essentiellement et majoritairement pour les développeurs: Ces derniers n’ont pas besoin (ou ne savent pas bien) gérer la partie matérielle, ni la partie ‘Système d’exploitation’ (OS), qu’il faut apprendre à sécuriser, redonder, mesurer, optimiser… Alors louer un serveur en ‘IaaS’, c’est bien, mais louer un serveur géré et mis à jour en ‘PaaS’, c’est mieux… Encore faudrait-il que ce soit un vrai ‘PaaS’ ! Car c’est loin d’être évident.
La solution ‘Cloud’ ultime (en fait non, il y a encore la couche NOOPS), va permettre de se passer d’un ingénieur système, ni d’un développeur, pour directement souscrire un service ‘en ligne’. Ainsi, avec WIX par exemple, je n’ai plus besoin d’un ingénieur système pour installer et gérer un serveur web, ni d’un développeur pour en créer le contenu. Je peux faire tout cela moi-même. Mais je ne pourrai plus transporter mon site web chez un autre fournisseur que Wix… Bien entendu, chez WIX, il y a encore des développeurs informatiques. Toutefois, si eux-même utilisent les services ‘IaaS’ et ‘PaaS’ d’Amazon, alors ils n’ont peut-être plus d’ingénieurs systèmes, car aucun serveur. Ceux-ci sont cantonnés dans les Datacentres d’Amazon, de Google, d’OVH ou de Microsoft, entre autres…
Skype, Dropbox, WhatsApp, Gmail, Mailchimp, Zendesk, SalesForce: sont toutes des applications ‘SaaS’, et elles sont de plus en plus nombreuses…
Les espaces en couleur indéterminée, ni rouge, ni bleu, sont justement des zones de variations, en termes de responsabilité et de gouvernance.
Le choix des couleurs n’est évidemment pas innocent, quelle gouvernance avez-vous sur ‘Gmail’? Ils changent les fonctionnalités quand bon leur semble, et vous ne pourrez rien y faire. A la défense de Google, les nouvelles versions majeurs sont proposées en parallèle des précédentes, et la magie du Cloud permet de le faire sur les mêmes données: aviez-vous essayé http://inbox.Google.com? Les évolutions fonctionnelles sont censées suivre les desiderata des utilisateurs et Google sait que sa fortune ne repose que sur son adoption par les utilisateurs. (finalement abandonné, Mars 2019)
Oui mais, entre faire plaisir à ses utilisateurs, et générer quelques milliards de bénéfices supplémentaires, quel choix fera la multinationale?
Le choix de la prestidigitation bien entendu, j’attire ton attention sur des points de “plaisir” pour l’usager, et je soutire de nouveaux milliards, sans même le faire réellement à ton insu, en t’invitant même à y contribuer. Ainsi, mon téléphone m’a demandé :
“Il était comment le restaurant où tu étais? Peux-tu prendre une photo?”…
Ensuite, l’accès à ces données se fera librement, mais individuellement! Car collectivement: Avez-vous essayé de récupérer tous les pharmaciens de votre région pour leur faire une offre de promotion, ou une invitation à un événement gratuit? Il faudra passer par le détenteur de toute cette richesse, l’Information, et nous devrons accepter ses termes, sans conditions possibles!
L’illusion de contrôler ses données, encore: J’ai tenté de télécharger toutes mes données chez Google grâce à leur outil “fait pour” https://takeout.google.com/settings/takeout
“Une archive contenant 28 produits est actuellement en cours de préparation.
Sachez que la création d’archives est longue et peut prendre des heures, voire des jours entiers. Vous recevrez un e-mail lorsque votre archive sera prête.”
La dernière fois, je n’ai jamais pu la récupérer, et ensuite, j’en fais quoi? De gros fichiers de données, sous quels formats, pour quelles applications?
Oui… Mais à conditions de valider une réversibilité anticipée (réplique localement exploitable et automatiquement mise à jour), et de contrôler un non détournement d’usage de ses propres données (par la NSA y compris…).
Oui aussi… Mais à conditions d’utiliser des solutions libres (ou au minimum Open source), bien sécurisées et surtout réversibles, avec des hébergeurs non assujettis à des surveillances massives inacceptables (UK, USA, France, et bientôt Suisse?). Ce n’est pas gagné! Voir avec un CHATON?
fairsocialnet.ch
Un autre bon exemple, les outils libres de Framasoft: FramaCloud
framacloud.org
La question ne se pose plus! Nous avons décidé massivement de les adopter, par ignorance peut-être? Par facilité très certainement! La mobilité oblige… Machine arrière n’est plus possible, sauf avec un grand et massif “black-out” Internet? Mais tous ont tellement à gagner en le remettant en marche, y compris les Cybercriminels!
Que ce soit pour capturer des informations publiques pour en revendre les accès à prix d’or, comme avec un local.ch qui facture l’activation d’un simple lien ou d’un ‘TAG’, une fortune annuellement (30F/an). Au moins Google nous les capture-t-il gratuitement, et son local.google.com écrasera Swisscom à terme, même si ces derniers ont racheté Search.ch… Sauf s’ils changent pour quitter ce modèle hérité d’un annuaire papier.
Je suis convaincu qu’ils sont prêts! Ils ne sont juste pas pressés, notre ignorance leur rapporte tellement! Et si on se réappropriait nos propres données? http://OpenLocal.ch
Quand la création de valeur, devient une illusion. Uber apporte un bol d’air pour les voyageurs parisiens et new-yorkais, car enfin, ils peuvent trouver un taxi. Des abus d’hégémonies les rendaient volontairement “rares”, et donc plus “chers”, pour mieux rapporter… Alors UBER, c’est bien, pour ces voyageurs lassés de courir après un taxi. Par contre, c’est au détriment d’un métier “régulé” (parfois abusivement, c’est certains), et en précarisant les chauffeurs UBER eux-mêmes, comparativement aux Taxis. Nous assistons donc à une récupération et détournement de valeurs, avec destruction d’une partie au passage. C’est la raison pour laquelle UBER, c’est mal ! Mais ce n’est pas le seul exemple d’Ubérisation: Le premier s’appelle Google, qui a fait fondre les budgets publicitaires des mondes radio/télévision/presse, pour son propre profit (66 Milliards de $…)
Mais les vrais dangers,
ne sont probablement pas les plus visibles!
medium.com
Une croissance exponentielle des puissances, des mémoires, des données…
Le Digital est un Pharmakon (un poison et un remède), il permet déjà l’essor de la prévention sanitaire, l’éducation inclusive, la distribution des savoirs…
Mais aussi, la surveillance généralisée, la manipulation massive…
Les IA (Intelligences Artificielles) sont actuellement des pures usurpations, une arnaque littérale car elles sont profondément stupides, totalement ineptes. Par contre, une des briques de leur future genèse, est en train de monter en flèche: Le ‘Deep learning’. Ces machines (algorithmes) sont capables d’apprendre avec une redoutable efficacité. Un simple ensemble de rack autonomes de machines AlphaGo de Google a battu le genre Humain au jeu de Go en mars 2016, ceci était prédit impossible avant de très nombreuses décennies, quelques années avant. Ceci moins de 20 ans après avoir battu Kasparov aux échecs (ce qui était aussi annoncé impossible quelques années avant), avec 2 racks de machines qui de nos jours tiennent dans un seul de nos smartphones (J’ai calculé pouvoir battre 20 à 50 Kasparov aux échecs avec mon Fairphone)…
La face cachée de la Loi de Moore
Ces mêmes algorithmes, combinés avec les capteurs des objets connectés (IoT), vont permettre de prévenir et réduire des accidents de santé avant leurs arrivées, entre autres…
L’IA de Facebook pourra bientôt, si ce n’est pas déjà, calculer avec plus de 90% de justesse, la probabilité d’un nouveau couple s’étant déclaré au monde via “Facebook” (et non plus un Maire, ni un Prêtre), s’ils seront encore ensembles dans 6 mois, ou pas... Le résultat ne leur sera pas communiqué à ce nouveau ménage, car ce n’est pas vendeur. Mais il sera certainement monétisé, d’une façon ou d’une autre. Le “gratuit” risque de nous revenir très cher, à la fin…
Microsoft a racheté LinkedIn, pour exploiter, comme Google et Facebook, la manne d’informations personnelles que nous leur confions, le plus souvent à l’insu de notre propre conscience, mais avec notre accord et plein gré:
“I accept” — sans avoir de vrai choix…
www.theguardian.com
Je tremble à l’idée que ces multinationales, vont imposer leur éthique au monde, à l’instar de leurs habitudes à nous imposer leurs standards de complexifications techniques, et d’accroissement de nos dépendances…
Sans parler des développements dans les domaines de la surveillance massive, voir la production des robots tueurs, actuellement en élaboration, en Suède comme aux USA…
Oui ! Notre projet Responsibility.digital, et bien d’autres comme TOSDR.org, propose de clarifier les conditions générales d’utilisation (Terms of Services, TOS, CGU) que nous ne lirons jamais, sous forme d’un petit écran ‘radar’, ou avec un feu (vert, jaune, orange, rouge):
responsibility.digital
Afin de faciliter une prise de décision “éclairée” par le seul qui finalement décide de la direction que prendra notre monde: L’acheteur.
La loi de l’offre et de la demande
Alors il est alors important de pouvoir disposer de solutions “éthiques” alternatives, comme celles proposées par Framasoft:
framacloud.org
Quand toutes les multinationales et grands propriétaires (2% détenant plus de 90% des actifs du monde?), auront compris qu’il leur était nécessaire d’acquérir de l’éthique et du cœur, alors, l’humanité pourra acquérir une chance de vrais progrès possibles, voir de simple survie…
Observatoire francophone (basé en Suisse romande) sur le…
23 
4
Some rights reserved

Restez informé des opportunités, idées ou histoires de révolutions numériques durables et responsables. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
23 claps
23 
4
Written by
Réducteur de fractures numériques, éthicien digital, Suisse romande.
Observatoire francophone (basé en Suisse romande) sur le ""Cloud computing"", pépinière pour des transformations sociétales positives avec le numérique, vers une humanité digitale durable: http://Tech4good.ch
Written by
Réducteur de fractures numériques, éthicien digital, Suisse romande.
Observatoire francophone (basé en Suisse romande) sur le ""Cloud computing"", pépinière pour des transformations sociétales positives avec le numérique, vers une humanité digitale durable: http://Tech4good.ch
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://bartlorang.com/the-cloud-is-taking-over-and-why-you-should-apply-to-techstars-cloud-f6562f131d3b?source=search_post---------94,"The cloud is taking over everything. Whether it’s PaaS, IaaS, APIs, virtualization, security, infrastructure or hosting — everything is being built for (and in) the cloud.
The power of the cloud is real, and it’s here. Currently there is an enormous market opportunity for cloud infrastructure businesses. For proof, just look at Rackspace and Amazon AWS — both enormously successful cloud infrastructure businesses and growing like weeds. There’s also SendGrid (for email delivery), Twilio (cloud-based communications) and UrbanAirship (powering mobile apps).
We’re obviously big believers in the cloud. At FullContact, we’re building a comprehensive suite of cloud-based APIs to manage contact information and ultimately solve the world’s contact information problem. (Fun fact: the original company name of FullContact was even ‘CloudCenter.’)
As a result of this powerful shift towards the cloud, the folks at TechStars have created TechStars Cloud.
If you’re a cloud-based startup, you should apply to TechStars Cloud. Why? As I’ve written about previously, TechStars empowers entrepreneurs through incredible mentorship. And TechStars recently sweetened the pot by adding $100K of additional financing from an awesome group of investors.
So don’t delay, you’ve got until November 7th to get your application in. And if you need any advice or help on your application, hit me up at bart@fullcontact.com — I’m more than happy to help!
Originally published at https://bartsblogsite.wpengine.com on October 31, 2011.
Musings on life, business and the universe from a lifelong
Written by
Dad, Husband, and Co-Founder of @FullContact and @v1vc — Lifelong Entrepreneur and Investor.
Musings on life, business and the universe from a lifelong investor and entrepreneur
Written by
Dad, Husband, and Co-Founder of @FullContact and @v1vc — Lifelong Entrepreneur and Investor.
Musings on life, business and the universe from a lifelong investor and entrepreneur
"
https://medium.com/@mrrishisingh/iaas-paas-and-saas-57bef8fdf19a?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rishi Raj Singh
May 21, 2019·1 min read
IAAS is infrastructure service, think of it as renting an apartment/house then adding furniture, setting up the kitchen, TV , refrigerator, washing machine and other stuff to make it livable/useful to you/. In IAAS you define what type of servers/networks you want and then set it up from scratch.
PAAS is platform as service, think of renting a furnished apartment which has all the things for you use, like furniture, kitchen, TV etc but you can select what channels you will watch on TV what kind of food you will cook or may the layout of furniture can be altered. PAAS gives a platform the size of which is decided by you and then you can make changes, updates to it, an example would be asking for oracle 10g server, with 100gb storage, 10gb ram and 5 CPU. In this you can create databases, define schema and work with data but you can not control configurations after a certain extent.
SAAS is software as a service, kind of like a hotel room which is pre configured, you cannot make any changes and have to use it as it is. Like asking for an email account, you can manage only your account and dont get any control on how the underlying email servers work.
Monk who wants to buy back his Ferrari
2 
2 claps
2 
Monk who wants to buy back his Ferrari
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@eFileCabinet_57957/the-future-of-xaas-from-infrastructures-to-platforms-1b9d07f3a2b?source=search_post---------80,"Sign in
There are currently no responses for this story.
Be the first to respond.
eFileCabinet
Mar 30, 2017·2 min read
The death of commodity IT services (like email) begins with IaaS, PaaS, SaaS, MaaS, CaaS, and culminates in XaaS. The following strengths and pitfalls to each portion of the SPI model can be ascertained through this lens, and in the following ways:
The IaaS strength resides in giving leeway to users to build as they please, offer shared control over processes, shared on premise and cloud functionality, the prevention of “cloud bursting,” and also the offering of scalability. The weaknesses in the infrastructure model are that buyers feel they must rely too much on their IaaS provider to intervene when issues arise.
The strengths of the platform as a service model are speedy deployment, versatile configuration capacities, and virtually unparalleled performance assessment. Despite these benefits, the weaknesses still exist with the difficulty in integrating the technology with existing applications, software, and systems.
The strengths of this platform are the fact no tech expertise is required, very low-cost, great for small businesses, and reduced hardware dependence. It’s weaknesses, though minimal, can include input delays and exclusive dependence on the internet.
Monitoring as a service enables companies to move their monitoring tools to the cloud, which can eliminate the risks associated with downtime for output intensive companies or business cycles.
Communication as a Service lets workers and potential customers utilize business grade VoIP and VPNs (virtual private networks, as are typically found in healthcare clinics). As organizations grow in inter-connectivity and shared services, this component of XaaS will become especially prevalent.
In consideration of these technology models’ respective strengths and weaknesses, Xaas will combine all of these elements, and only the future can tell how the strengths and weaknesses will eventually coincide with each other.
Although the SPI models are important, the impact of XaaS will tip the scale toward a massive paradigm shift in which technology is no longer considered a mere resource, but also a strategic imperative for ensuring business profitability in mass.
Hosting has become a cloudy topic with all these different acronyms, and is even further complicated by the addition of “anything” or “everything” as a service to the sphere of interpretation.
Although these different delivery models culminate in XaaS, they are yet to reach the level of advent expected. One could argue, however, that these various cloud deployment models have always existed, and until recently were merely not compartmentalized as expected.
To begin your organization’s journey toward an XaaS world, begin with the right document management tools that solve common office problems today.
Originally published at www.efilecabinet.com on March 30, 2017.
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
1 
1 
1 
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
"
https://blog.fra.me/storage-on-frame-tools-for-teams-9bb6aa2a7060?source=search_post---------372,"Frame is IaaS (Infrastructure as a Service) provider-agnostic for a reason. We understand that certain tools and services may make one provider more fitting for your team than another. Supporting multiple cloud providers gives us the unique opportunity to integrate our own solutions with some of their great services. We seem to have a knack for making awesome things even more awesome.
Take our Personal Drive, for example. Frame’s Personal Drive is a storage drive that is mounted to each individual user’s session as the “P:” drive letter. The size of the drive is predetermined by the admin. Your users will simply see an additional drive when they open up “My Computer” and can securely save their information before closing applications or logging off of a Windows desktop. With Frame, the Personal Drive looks the same to the end user regardless of the IaaS provider.
If you’re using AWS, your Personal Drive solution relies on our own managed file server service. We provision our own fully-managed Linux server contained within your VPC to dynamically create a secured network share when a new user connects to a session. As soon as the session is ended, the P drive is disconnected.
Internally, Personal Drives work a little differently with Microsoft Azure. We leverage Microsoft’s fully-managed, built-in service — Azure Files. Azure Files allows us to easily mount drives to your users’ sessions and provides a strong, high-speed connection between the session and the drive. Azure files are also backed by Locally Redundant Storage (LRS) that provides 11 9’s of durability. Personal drives have a default storage size of 10 GB, but can be setup larger if needed.
As you may have guessed, we also have a few ways to set up shared drives for team collaboration — also known as “Group Drive Mapping.” Network file shares are commonly used in on-prem scenarios and they’re just as useful in a cloud environment. In one of our earlier storage articles, we talked about using Frame’s Utility Server as a network file share which is just one of many options.
After your Frame Utility Server has been set up, just follow the step-by-step instructions to map your network drive. We’ll give you a hint: It’s the same as setting up a network file share in your local office. And then… Voila! Your users can now save their data to the shared drive before ending their session.
Your users can now fire up a session, complete their work, and then save their data to a network mapped drive — from anywhere. Just like with Personal Drive, our Utility Server is available to both Azure and AWS-based accounts.
Remember Azure Files? Azure-based accounts can also use this Microsoft-managed service to mount drives for sharing. Customers using Azure have the option to use Frame’s Utility Server solution or Azure Files.
Whichever IaaS provider and service you use, the shared drive for your team will appear as the drive letter “T” and can be accessed by all of your users, simultaneously.
If you’d prefer something other than the options we reviewed above, there are a number of other ways to integrate storage solutions with Frame. If you want to read more, check out our storage guide for more information. We’ll continue to cover storage alternatives in our blog, so stay tuned!
The Official Frame Blog
19 
Thanks to Carsten Puls and Ruben Spruijt. 
19 claps
19 
Written by

The Official Frame Blog
Written by

The Official Frame Blog
"
https://medium.com/radixweb/choosing-the-right-cloud-computing-service-model-for-your-business-saas-iaas-paas-or-daas-15d72d7ec98c?source=search_post---------196,"There are currently no responses for this story.
Be the first to respond.
Organizations across all industries face a tremendous burden as they struggle to match their IT infrastructure with the fast-growing customer demand for secure, fast, and reliable services. In the effort to increase the storage capabilities and processing power of their IT systems, the companies realize that developing and maintaining a secure, scalable, and robust IT infrastructure is significantly expensive.
Thankfully, cloud computing allows organizations to leap beyond their on-premise IT infrastructure and embrace internet-based solutions. They were no more owning and maintaining costly IT infrastructure and data centers! From storage to processing power and applications, you can now avail on-demand computing services on a pay-as-you-go basis through cloud computing technology.
The biggest positives of cloud computing services are that organizations can eliminate the direct cost and complexities of owning and maintaining an IT infrastructure.
Cloud services are available on-demand, which means you pay only for what you want. The ongoing maintenance and other costs related to the cloud infrastructure lies with the service provider. It gives organizations the flexibility to use cloud services as the business scales.
Let’s face it: “cloud” is the future of computing. Cloud services are witnessing exponential growth in recent times, with a projected growth of 331.2 billion U.S. dollars in revenue by 2022 — as per Gartner’s report.
According to Cisco’s study, cloud data centers are estimated to manage 94% of organizational workloads in 2021. Keeping pace with unprecedented demand for cloud resources and data center in the future, it is predicted that there will be over 628 hyperscale data centers worldwide by 2021.
With uptake statistics like the above, it is evident that cloud computing is fast becoming the new standard, and many organizations are completely phasing out their on-premise software. With that, choosing the right cloud computing service model becomes more critical.
So, let’s understand what the different types of cloud computing service models are. This article will help you choose the appropriate one for your business.
In the SaaS model, the software is delivered via the Internet. Rather than owning, installing, and maintaining applications on their own, users can access them through the web. The advantage is — it provides quick, secure, and seamless access to cloud-based web applications. Typically, these run on the cloud, and users can access them for free with limited access or get complete access through a paid subscription.
Key Features:
· SaaS users can access the applications or software through a subscription model
· No need to install, manage or upgrade software; the SaaS vendor governs everything
· Data security is high; no chance of equipment failure that can result in loss of data
· Applications and software are accessible from anywhere and anytime; all you need is a device with internet connectivity
· Use of applications via SaaS is scalable based on your needs
Applications:
Applications of the SaaS service model include system monitoring, Field Service solutions, etc. Microsoft Office 365, Google G Suite, and Dropbox are some examples of SaaS cloud computing technology.
According to Gartner, today, SaaS is the most widely used cloud computing service model. It is predicted that SaaS will grow at 10.5% in 2020. Making it the fastest growing segment in IT.
radixweb.com
Infrastructure-as-a-Service is a model in which users access computing infrastructures such as IP addresses, storage, servers, bandwidth, networking, and load balancers. The onus lies upon the enterprise to manage data, middleware, operating systems, applications, and runtime.
On the other hand, the vendor is responsible for the availability and reliability of the computing infrastructure.
Key Features:
· A cheap, secure, and quick infrastructure solution for enterprises that do not have an owned data center
· Improved scalability based on the user’s storage and processing needs
· Saves the cost of owning and maintaining the hardware outright; pay-on-demand basis
· Virtualization of essential administrative chores, helping save time for more critical business operations
Applications:
The IaaS model is most suitable for traditional enterprises that lack capital expenditure but need robust and reliable computing power to manage workloads. Companies that do not have their own data center can adopt this type of cloud computing services. Google Compute Engine, Microsoft Azure, and Amazon Web Services are some of the best IaaS cloud service providers today.
A Gartner report indicates that the IaaS service model is estimated to reach 61.9 billion U.S. dollars in revenues by 2021.
Platform-as-a-Service is one of the most widely used cloud computing service models that provide users access to a cloud “environment” to build, test, and deliver applications. Adopting the PaaS model streamlines the enterprise software development process. Business applications can be made and tested in a virtual runtime environment using a full suite of built-in tools.
Key Features:
· Access to a platform with prebuilt tools to develop, test, and host a wide range of applications within the same environment
· Provides integration and important infrastructure components such as databases, web services, and LDAP (Lightweight Directory Access Protocol)
· Auto-provisioning and scalability of the primary infrastructure
· PaaS is a multi-tenancy service that can be concurrently used by many users
· Aids in improved collaboration even when the team is working remotely
· PaaS vendors are responsible for managing the operating systems, backups, security, and server software
Applications:
The service model is primarily used by developers to build a unique application in a time-efficient and cost-effective manner. It enables the developers to focus on improving the application’s design and functionality, thus, saving their time in testing, deploying, and managing security patches or software updates. AWS Elastic Beanstalk and Google App Engine are examples of PaaS service model.
Gartner is quite optimistic about the growth rate of PaaS in the coming years, estimating the total revenue-earning in 2021 to be 27.5 billion U.S. dollars.
SaaS, IaaS, and PaaS are the primary cloud computing services provided by leading vendors like Google, Microsoft, or Amazon. However, there is another new service model named Desktop-as-a-Service (DaaS).
DaaS is the newest layer in cloud computing technology, wherein the vendor hosts and provides access to the back-end of a Virtual Desktop Infrastructure (VDI). In this model, the provider will copy the user’s data to and from a Virtual Desktop to log-on and log-off.
Typically, the vendor is responsible for managing all the back-end infrastructure maintenance and cost. At the same time, the company needs to operate desktop applications, security, and images, unless it is covered in DaaS subscription.
It has a multi-tenant architecture, and services are available on a subscription basis.
medium.com
Cloud computing technology is radically transforming the way how computing is performed today. The growing significance of SaaS, IaaS, and PaaS is eliminating the need for costly, redundant, and less secure on-premise hosting.
Each cloud computing service model discussed herewith provides users with the cutting-edge features, choice, and flexibility that on-premise hosting cannot. Let’s quickly recapitulate the critical differences between these service offerings:
· SaaS provides innovative and ready-to-use applications that help meet a specific business need. Typically, SaaS is built on PaaS or IaaS platforms.
· IaaS, on the other hand, provides maximum flexibility and a comprehensive data center for improved data storage when it comes to building custom-built applications.
· PaaS is typically built on an IaaS platform that helps minimize the requirement for system administration. Designed for developer, it allows focusing more on app development than infrastructure management and maintenance.
· DaaS could be the game changer of all with its revolutionary way of operating and delivering solutions to end-users. We think it might become the forerunner of cloud computing in the future.
It all depends on your business’s core requirements, complexity, and size to determine the right cloud computing service model you need. You might want to consult with an experienced partner to help you identify which one of them is ideal for your business.
And who knows? Maybe you can go ahead with all of them!
Leading Custom Software & Web Development Company
45 
45 claps
45 
Explore our experts insights on innovative tech and biz solutions
Written by
Technologist and Vice-President of Sales at www.radixweb.com with a track record of growing revenues and enabling value-based partnerships to customers.
Explore our experts insights on innovative tech and biz solutions
"
https://medium.com/odcuriocity/the-evolution-of-iaas-7ec43c3a7955?source=search_post---------174,"There are currently no responses for this story.
Be the first to respond.
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
IaaS, what is it? how did it get here and where is it going?
Ok, so you’re probably considering moving to the cloud or work for a tech giant, you’ve probably heard the word “cloud” been thrown around over the past few years, so what is it?
You see, the cloud refers to how and where data is stored — and perhaps more importantly, where it isn’t. The cloud allows software and services to run on the internet, instead of only locally on one device, because the data is stored remotely across a variety of different servers. These software and services can be accessed on any internet browser, or via online apps that can be accessed on different devices.
Cloud computing is the practice of using a network of different servers that host, store, manage, and process data online.
Companies use different ways in monetizing cloud computing to offer customers different types of online services with the most common being the likes of IaaS, PaaS and SaaS.
Ok… But where did these come from? How did the cloud structure evolve into the state that it is today?
In the 1960s, computers were enormously big and expensive. Few small and medium-sized businesses could afford to invest in a computer. That’s why the software as a service industry was born. In the 1960s, the model we know today as “cloud computing” was simply called a “time-sharing system”.
This system involved multiple “dumb” terminals (keyboards and monitors without CPUs) that were networked to a mainframe or mini-computer (typically using a hub-and-spoke system). All applications and data remained on the mainframe.
To use the system, you entered input through the terminal keyboard. Then, output was sent from the mainframe/mini-computer to the appropriate terminal monitor. It was an early form of connecting computers together — a system we know today as “the internet”. Through this system, small and medium-sized businesses, educational organizations, and government entities could access modern computer systems in a cost-effective way. These organizations wouldn’t be able to afford the costs of computer hardware, software, support, and training, so they relied on SaaS as a way to stay competitive.
This type of SaaS system continued throughout the 1970s and 1980s. The costs (and physical size) of computers were declining every year, but many businesses still found it more cost-effective to rely on SaaS. During this period, things like CRM, payroll, and accounting services were all key products delivered over a SaaS system. The system relied on a dedicated phone line and modem to send data to/from clients. Meanwhile, the applications were simple, text-based interfaces. Since all of the data being transmitted was text or other small data, there was rarely a need to transfer larger files.
Eventually, the shrinking cost of computers was bound to shift the SaaS world. Throughout the late 1980s and early 1990s, we saw that shift take place. Computers were cheaper than they had ever been at any point in history. Employees were now able to afford to have a computer at their desk. Companies no longer had to rely on time-sharing systems, where multiple employees shared a single computer.
Nevertheless, the SaaS industry didn’t die out. Instead, it just adapted. The hub-and-spoke system we saw with early SaaS systems just changed places: now, it was found in the form of Local Area Networks (LANs). In these in-house systems, applications were hosted on local machines and critical business data was kept on a central server. Employees connected to the LAN to access these applications and data. This can be seen as an early form of cloud computing.
In the early 1990s, office workers were beginning to have their own computers, which means companies didn’t need to rely on remotely-hosted applications. By the mid-1990s, software developers had learned how to take advantage of this: they bundled “bloatware” into their software. When you installed a new program or operating system, it would come with extra programs that you didn’t specifically need.
Some people call Concur the world’s first SaaS company. Instead, the company was founded as a packaged software service. Concur sold floppy disks and CD-ROMs of travel and expense software. In 1998, the company went public with this sales model. But after the crash of 2001, the startup’s market cap totalled only $8 million.
In response, the company evolved into a pure SaaS business. Instead of restricting their sales to computer hardware stores, the company sold its services over the internet, expanding its market to anyone with a browser. 13 years later, in 2014, the company was generating more than $600 million in annual revenue. SAP bought them for $8.3 billion later that year, making it the largest SaaS acquisition to date. And many other companies began moving their resources to the cloud shortly after:
During all those years of development, IaaS evolved to what it is today:
Ultimately, even though it has been around since the 60s, today it’s bigger than it has ever been before. With the growth in many modern industries like media and entertainment, social/collaboration, mobile/location, and big data analytics service providers. These are the areas of cloud computing is expected to see the majority of growth over the coming years.
If you would like to learn more about IaaS you can register for our free webinars:
bebusinessed.com
www.tenfold.com
Still worried about implementing applications, API’s or backends? Oracle is here to help, with industry-standard cloud applications, their team of experts will make implementation more than enjoyable.
☁️ Follow to get a free 30-day trial with Oracle Cloud services. ☁️
Thank you for taking the time to read my article, if you’re looking for more posts like this, you can find me on Linkedin, Twitter, or Medium.
*All views are my own, and not that of Oracle*
OD EMEA’s quest for insightful knowledge and…
149 
149 claps
149 
Written by
Endorsing technology with a hint of excellence
OD EMEA’s quest for insightful knowledge and thought-provoking conversations. Each month we follow curated content around tech trends that spark digital disruption, all running on Oracle Solutions. Remember, always stay curious.
Written by
Endorsing technology with a hint of excellence
OD EMEA’s quest for insightful knowledge and thought-provoking conversations. Each month we follow curated content around tech trends that spark digital disruption, all running on Oracle Solutions. Remember, always stay curious.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renebuest/the-big-misunderstanding-shared-responsibility-in-the-public-cloud-40eb5743c3f1?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 2, 2016·6 min read
Responsibility in the public cloud is a story of several misunderstandings. Advisory sessions and conversations with different companies interested in public cloud unveil the certainty that the classical outsourcing concept is still widely spread among IT decision makers. Public cloud providers are being seen as full service providers. That complicates negotiations at eye level and blocks the quick adoption of public cloud services. „Shared Responsibility“ is the keyword that needs to be internalized. This research note clarifies the wrong assumptions and describes the concept.
In the past 10 years, for the sake of convenience cloud computing was often defined as “Outsourcing 2.0”. What should have led to a better understanding on the user side, however, did public cloud providers a disservice. With the understanding in mind — an external service provider takes over responsibility for (partly all) IT operations — IT decision makers developed the expectations that public cloud providers are full service providers. The IT department just coordinates and controls the external service provider.
What is true for a software-as-a-service (SaaS) provider as a vendor of low-hanging fruits is completely different at platform-as-a-service (PaaS) and in particular at infrastructure-as-a-services (IaaS) level. SaaS providers are delivering ready developed and ready-to-use applications. The complexity, for example with solutions from Salesforce and SAP, comes with the configuration, customization and, if necessary, the integration with other SaaS providers. So, the SaaS provider is responsible for the deployment and the entire operations of the software, and the necessary infrastructure/ platform. The customer is consuming the application. PaaS providers are deploying environments for the development and operations of applications. Via APIs, the customer gets access to the platform and can develop and operate its own applications and provide those to his own customers. Thus, the provider is responsible for the deployment and the operations of the infrastructure and the platform. The customer is 100 percent responsible for his application but doesn’t have any influence on the platform or the infrastructure. IaaS providers only take responsibility at infrastructure level. Everything that is happening at higher levels is in the customer’s area of responsibility.
Thus, it is wrong to see public cloud providers such as Amazon Web Services, Microsoft Azure or VMware (vCloud Air) as full service providers who take whole responsibility for the entire stack — from infrastructure up to application level. Self responsibility is required instead!
A decisive public cloud detail that contrasts this deployment model clearly from outsourcing is the self -service. Depending on their DNA, the providers are only taking responsibility for specific areas. The customer is responsible for the rest.
In the public cloud, furthermore, it is about sharing responsibilities — referred to as Shared Responsibility. The provider and its customer divide the field of duties among themselves. In doing so, the customer’s self-responsibility plays a major role. In the context of IaaS utilization, the provider is responsible for the operations and security of the physical environment. He is taking care of:
The customer is responsible for the operations and security of the logical environment. This includes:
A very important part is security. The customer is 100 percent responsible for securing his own environment. This includes:
Thus, the customer is responsible for the operations and security of his own infrastructure environment and the systems, applications, services, as well as stored data on top of it. However, providers like Amazon Web Services, Microsoft Azure or VMware vCloud Air provide comprehensive tools and services customers can use e.g. to encrypt their data as well as ensure identity and access controls. In addition, enablement services (microservices) exist that customers can adopt to develop own applications more quickly and easily.
By doing this, the customer is all alone in its area of responsibility and thus has to take self-responsibility. However, constantly growing partner networks are helping customers to set up virtual infrastructures in a secure way and run applications and workloads on top of public clouds.
In addition to requiring an understanding of the shared responsibility concept, using public cloud infrastructure also makes imperative the rethinking of the infrastructure design as well as the architecture of the corresponding applications and services.
During the way to public cloud infrastructure, the self-service initially looks simple. However, the devil is in the detail and hides in the complexity that is not obvious at first. That is why CIOs should focus on the following topics from the start:
The growing number of cloud migration projects at big medium-size companies and enterprises indicate that public cloud infrastructure platforms are becoming the new norm, while old architecture, design and security concepts are being replaced. After public clouds have been ignored over several years, this deployment model now also makes its way on the digital infrastructure agenda of IT decision makers. However, only CIOs with a changing mindset taking the shared responsibility concept for granted will successfully make use of the public cloud.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
9 
9 
9 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@casspolzin/whats-the-difference-between-saas-paas-and-iaas-ed45d3c19dce?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
This is your last free member-only story this month. Sign up for Medium and get an extra one
Cass Polzin
Mar 5, 2020·7 min read
Technology has evolved rapidly in recent decades. Dipping computer and server hardware prices led to the introduction and adoption of modern cloud computing. Models such as software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS) have emerged largely as a result of the desire to harness the computing power of a grid (a network of servers).
Such intertwined technologies are often confused. This article will explore the idiosyncrasies, benefits, and drawbacks of SaaS, PaaS, and IaaS models.
Software as a service is a software licensing and delivering model that has gained significant traction in a number of applications from corporate to business, scientific, and even commercial. SaaS allows users to access proprietary software on a subscription basis (typically month-to-month or annual).
SaaS is an enormously cost-effective alternative to on-premise software installations and management. The SaaS model can deliver anything from construction planning software, property tax management, payroll and accounting, human resources management, and more.
SaaS revenue worldwide amounted to $79 billion in 2016. Statista predicts by 2027, worldwide SaaS revenue will hit $346 billion. As of 2018, SaaS accounted for approximately two-thirds of the revenue of the cloud computing market.
The SaaS model comes with lower up-front costs and zero licensing fees, making it extremely cost-effective. This is especially advantageous for startups and small businesses as it effectively minimizes the expense of establishing and scaling a business rapidly.
The provider manages the infrastructure running the software, shouldering hardware and support expenses. Reduced fees for hardware and software maintenance provides an enormous economic advantage.
Technology has long been considered a hassle, with an immense learning curve, often requiring expert assistance for set-up and ongoing maintenance. SaaS evades this issue, by minimizing downtime and delays. This is possible because SaaS apps are configured on remote servers (the cloud).
Additionally, SaaS models tend to have a shorter learning curve that drives engagement and increased productivity. This reduces the need for ongoing training and increases the overall return on investment for the software. With the end-user not having to directly manage or maintain complex server or software setups, the business owner is freely able to allocate their time to more pressing issues.
Upgrades and installation are an enormous pain. Potentially substantial downtime combined with strenuous, required labor hours makes for a headache. The SaaS model removes concerns by automating upgrades.
As SaaS service providers typically roll-out upgrades via centrally-hosted applications, previous headaches become a smooth, uninterrupted transition. This promotes user convenience and overall productivity. Additionally, this increases security and reduces vulnerability with the automatic roll-out of patches and updates.
Choices can elevate the appeal of software to customers and users, and typical SaaS models have no shortage of options. These models allow users to scale up or scale down their engagement, as aligned with business needs. A variety of subscription packages provide additional flexibility.
Further, SaaS applications can typically be accessed anywhere with a browser and internet access — phones, laptops, tablets, and more. This gives users the flexibility to use software from virtually anywhere, starkly contracting typical installation-based software that can only be used from a single device.
The main challenge of SaaS models is the ever-present vulnerability. These applications are at risk of unauthorized access and hacking. While cost-effective for small businesses, these vulnerabilities can prove detrimental, as cybercriminals can disseminate a cloud-operated business by blocking employees’ and customers’ access to mission-critical online systems. As a result, service providers must continually invest in improving security and assuring customers’ safety. Potential internet outages are also an additional concern for organizations that cannot afford downtime to clients.
Platform as a service is a category of cloud services that provide a platform allowing customers to manage, run, and develop applications without complexity typically associated with application infrastructure. PaaS services consist of hardware and software hosted by the provider. This removes the need to install in-house hardware.
This cloud-computing model is increasing in prevalence as it reduces set-up costs and maintenance-associated expenses. This reduces the barrier to entry, giving users access to resources that would otherwise be out of reach due to budget restrictions.
PaaS-based systems give developers significant control over custom configurations. This preserves the goals of the original development work. PaaS providers handle application execution, data, and underlying system functionality.
Additionally beneficial for coders and developers, providers offer the use of individual development tools, frameworks, and other relevant resources, configured for use in the provider’s platform.
In today’s day and age, developing applications for multiple platforms is crucial. PaaS vendors make it easy to develop applications for use on different devices, such as phones or desktops. This is a quick and inexpensive way to scale an application.
As with SaaS models, PaaS technologies are incredibly scalable. As a result, these services generate substantial cost-savings for developers, allowing them to increase margins and gain traction quickly.
Most importantly, the scalable nature of PaaS models allows project scope to expand in conjunction with market demands, allowing developers to stay competitive in the industry.
PaaS technologies reduce the amount of coding required to develop and execute an application. These services also help automate business policy, driving process efficiencies. With 99.9999–99.999999% uptime and the ability to accommodate multiple digital tenants, these technologies drive the migration of apps to the hybrid model of cloud computing.
Utilizing PaaS technologies eliminates the need to hire skilled staff or outsource specific activities. This gives in-house developer teams new capabilities with strenuous tasks moved off their plate.
Keeping up with the latest trends and technology is hard enough without considering all that goes into developing an application. Quicker development and deployment is a significant benefit, drastically reducing the overall app development lifecycle.
PaaS technologies pose particular risks for service providers, including balancing control, costs, and capacity of a PaaS-based service. Providing full multi-tenancy support and integrating third-party services creates additional challenges.
Once selecting a PaaS, it can be difficult to switch to a different vendor. Typically, when developing on these platforms there is a specific set of tools and capabilities. For example, not all vendors will support all APIs, coding languages, operating systems, and more.
Infrastructure as a service is the virtual delivery of computing resources via hardware, networking, and storage service. Instead of making the steep investment of purchasing and implementing necessary resources in a data center, companies are able to rent or lease these resources on an as-needed basis.
IaaS operates via traditional cloud infrastructure. Service providers host a variety of infrastructure, including service, storage, networking hardware, and more. This opposes the longstanding business case for investing in on-premise resources, data center infrastructure, and equipment. Additional services include performance monitoring, digital security, data backup, virtualization, and more.
IaaS is typically paid on a per-seat basis, but can sometimes be negotiated as hourly, weekly, or monthly rates. This method evades steep up-front costs of on-premise hardware and software. IaaS creates an incredible opportunity for organizations to utilize infrastructure that would typically be out of their reach.
Similar to SaaS and PaaS models, IaaS provides tiered pricing and easy scaling. This attracts a variety of customers with varying needs and creates an opportunity to scale up with clients as they grow in size.
Being hosted on the cloud, IaaS is entirely location-independent. Additionally, redundancy ensures your infrastructure is protected in the event of a natural disaster or significant weather emergency at one data center. Instead of relying on populating a backup from a remote server — or worse an on-premise server — employees and clients alike can access everything from wherever they are, via an internet connection.
A large challenge of IaaS models is managing subscriber expectations. There is a surplus of misconceptions associated with cloud computing and its capabilities, making it necessary for IaaS providers to educate around those myths and clear up any confusion.
Additionally, an ever-evolving market necessitates ongoing, agile experimentation. Don’t wait for the next big thing to happen, be the one to bring a new concept to light.
The constantly changing nature of technology has generated a surplus of new opportunities to access state-of-the-line software, hardware, and infrastructure without needing to make a full purchase. Instead, users are able to scale their services to accommodate their needs at any given time. These are optimal for scaling businesses and planning for future growth.
Security vulnerabilities are a larger concern for software as a service models. Providers need to pay special attention to ensuring these concerns are evaded and customers are put at ease. Handling customer expectations, multi-tenancy support, and an industry full of expectations are just a few challenges facing PaaS and IaaS models.
With the adoption of any new technology, precautionary measures should be taken before implementing any radical overhauls to infrastructure systems. All three of these distinct technologies have the capabilities to improve upon day-to-day use at a high caliper. Companies should explore how these technologies can improve their efficiency and efficacy while saving money, experiencing improved support, and more.
Gen Z Marketer Passionate About Communities
See all (12)
Gen Z Marketer Passionate About Communities
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/zenduty/sending-azure-monitor-outage-notifications-to-microsoft-teams-65f154641a01?source=search_post---------366,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is a cloud computing service providing infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) supporting multiple Microsoft Specific and third-party services and systems with 90+ compliance offerings and trusted by 95% of Fortune 500 companies to base their business on.
What is a system downtime and how does it affect me or my business?
According to Webopedia, “Downtime refers to periods of time during which a computer system, server, or network is shut off or unavailable for use. Systems go through periods of downtime for a variety of reasons, including power or hardware failure, system crashes, hacker attacks, system reboots, operating system and/or software updates, lack of network connectivity, and more”. While downtimes may be planned for maintenance, in most cases they are unplanned and unexpected.
The cost and impact of system downtime are rising rapidly due to the business’s increasing dependence on data and technology. System downtime can cause loss of opportunities, productivity loss, confidence erosion, potential employee overtime costs to recover work lost and catching up, service level agreement penalties, supply chain ripple effects, and most importantly, a damaged reputation. Effective communication with staff, customers, and service providers are very important to manage the impact of downtime.
A key feature of Azure is its alerting service which enables you to set up alerts to monitor the metrics and log data for the entire stack across your infrastructure. Azure dispatches these alerts via Email. However, for critical metrics indicative of degraded user experience, email may not be good enough to elicit a prompt response and resolution from your reliability and NOC teams. For high availability services with SLA timeframes in minutes, you need to be able to promptly alert the right engineers and teams about a critical issue, and also gather responders, subject matter experts and communicate to the relevant internal and external stakeholders while at the same time, keeping a watchful eye on the SLAs. This is where Zenduty can help you.
Zenduty acts as a dispatcher for the alerts generated by Azure. Zenduty determines the right engineers to notify based on on-call schedules and escalations and notifies via using email, text messages (SMS), phone calls, Slack, Microsoft Teams, and Android & iOS push notifications.
Zenduty provides you with everything you need you to minimize your mean time to recovery with advanced routing rules, flexible scheduling, analytics and reporting, integrated ChatOps (with Slack and Microsoft Teams), stakeholder communications, and SLA alerts.
Below is a step-by-step guide to integrate Azure alerts with Zenduty.
In Zenduty:
8. Authenticate your Teams account. Zenduty is now configured for 1–1 alerts from Azure.
9. To send Azure alerts to a Teams channel, navigate to the Teams channel where you want to send the alerts. Click on the “…” button next to the channel and select Connectors. Select Zenduty from the application. Click on configure. Copy the webhook URL and paste in the MS Teams connector outgoing webhook configure section in Zenduty that we created in Step 4.
Configuration in Microsoft Azure:
2. Create a new alert rule.
3. Select the resource which you want to monitor. Here we shall monitor a Virtual Machine.
4. Select the condition based on which the alert is to be triggered. Here we create an alert that is triggered when the CPU utilization of a Virtual Machine crosses 50%.
5. Create an Action Group (or use a previously created action group)
6. Enter the Action group name and short name:
7. Enter an action name and select Webhook under Action Type. A dialog box appears in the side.
8. Enter the webhook URL copied earlier from the Zenduty integration page.
9. Enable the common alert schema by toggling the button to yes and then click on OK.
10. Click on OK to add the action group.
The action group has now been created. It should visible under the section of Action Groups
11. Fill in the alert details appropriately. The alert rule name along with the severity will appear as the alert message and the alert description will appear as the summary in Zenduty
12. Click on create alert rule. You have now successfully created the alert.
Using the combination of Azure’s Application Insights along with webhook integration to Zenduty is a simple way to provide 24/7 monitoring and notification. Many aspects of application performance can be monitored with Azure alerts, including custom events detected within the application code. Now all essential Azure alerts and incidents will appear on the Zenduty dashboard enabling you to make use of all the power and capabilities of Zenduty to resolve incidents quickly and efficiently.
An end-to-end incident management platform.
1 
1 clap
1 
An end-to-end incident management platform. Making a dent in the SRE/DevOps/Support/ITOps universe and helping companies institutionalize reliability.
Written by
Building YellowAnt - http://www.yellowant.com — an enterprise-grade ChatOps framework
An end-to-end incident management platform. Making a dent in the SRE/DevOps/Support/ITOps universe and helping companies institutionalize reliability.
"
https://medium.com/tech-business-talk/the-easiest-explanation-of-iaas-paas-saas-the-3-cloud-service-models-432ac36ba6e1?source=search_post---------203,"There are currently no responses for this story.
Be the first to respond.
In general, Cloud is a very broad terminology and most of the people have all the about what the terms actually mean;
Cloud basically means a network and anything that is connected to a single network and can be accessed from anywhere can be considered as Cloud.
All that we see on the internet through the browsers, is not stored in our computer. We can merely access, and view the data as and when required. Similarly, cloud application services are like having our entire computing power on the internet, which can be made available to us as and when required, and in whichever amount.
Consider an example of a company having 500 employees, and each of them using a computer of 8 GB RAM, with a dual-core processor of 2.4 GHz, and 256 GB hard drive. Now, not each and every employee requires this much computing power. Like a content writer would not require as much processing as a Java developer. And supposedly, for an emergency big scale project, the company needs 200 new and temporary developers for 6 months. So, it will have to invest in additional computers which will be free within 6 months.
Here, the company can use the Cloud Application service as a sigh of relief. Thus, each employee will use only the required amount of computing power from the server. New employees can plug into the server for 6 months and use as much of the service required. The company will have to pay ONLY for the service used. This is the power of the Cloud.
Today, industries and companies are turning to be 100% cloud-based by developing a cloud app, instead of developing a native mobile app and website separately. According to International Data Corporation, companies spending on Cloud Application Service are going to hike 6 times in accordance with the current IT trends. The Cloud Market has seen a growth of 21% in the last year and is expecting to see a rise of 17% in 2019. By 2020, more than 60% of IT infrastructure, technologies, software, and services will be Cloud Based. Businesses today are getting highly influenced by cloud application service as it is incorporating with new technologies like (IOT) Internet of Things, DevOps, BigData, etc.
There are countless articles and literature available on the topic of Cloud available on the internet. But it becomes very difficult to understand those articles without knowing the basic terminologies of cloud computing. So in this article, I will explain the most important and common concept of cloud service models in the easiest way possible.
Just another IT service like database management or network security, Cloud Application Service is like renting an external resource of computing which can be accessed online through the internet. There is no need for maintaining a large infrastructure when all you need is computers.
This is the same as not owning a farm for vegetable production, rather just buying the vegetables required at regular interval from the vegetable market.
The physical storage network and the environment are owned by companies providing cloud application service. Cloud application service benefits the users by providing easy and scalable access to data, resources, and applications hosted on the cloud. It is an on-demand service so can be molded according to the user’s need. Depending on the intention and purpose, there are predefined types of cloud application service.
Consider your personal computer on the cloud. Not literally, but think of a virtual computer providing the infrastructure for network storage; hence known as Infrastructure as a Service.The rest has to be managed by the cloud user.
On such a cloud-hosted service, any number of physical machines can operate separately. This means the entire system is partially available which can be operated on the user end system as per customary needs.
Think of a Scenario→ There is a passionate dance enthusiast and he wants to portray his talent to the world. Now, it is not very easy if he starts all by himself.
All the steps included in this will be:To organize a dance showManage the eventSell ticketsChoreographand Perform.
Now let us understand this simple example by cloud computing reference.
In his pursuit of becoming famous, what if he gets a Sponsorer? The sponsorer will invest and organize his dance show. The dancer thus, need not worry about the expenditure. The dancer, however, will only have to manage his dance show, choreograph his steps and perform. This means that the Sponsor provides an infrastructure to the dancer.
The Role of Stakeholders in Infrastructure as a Service
Let’s understand the functioning of this cloud application service. How each stakeholder works with its role.
It is the IaaS service providers role to take care of the security, functionality, integration, interoperability, and usability. They maintain the data and the equipment essential for the maintenance. So ideally, they manage networking, storage, servers, and virtualization.
The clientele of the IaaS has the full command on the infrastructure of the model. They pay a fee for a duration to avail the infrastructure service for as long as they want the service. However, the client companies need to install their own Operating systems, maintain the middleware, take care of the runtime, data and the applications built by them. So, they need to hire IT staff and maintain resources/tools for development.
The developers of the IaaS platform will have access to the data controls and will be provided with resources (tools for development) by the cloud that will be helpful in the development of applications. Their role is to develop and deploy on their platform by integrating the resources themselves. It is like buying all the ingredients for making a cake and then baking it yourself in the oven.
The names of IaaS Service Providers that you must know
The most popular cloud application service provider providing Infrastructure as a Service IaaS is the Amazon Elastic Cloud Compute (EC2) providing hybrid IT architecture, rich controls and auditing, BigData support, backup and storage, disaster recovery and comprehensive security capabilities. It comes in two pricing models, one is the service pricing which goes pay per use above 120 cloud services, and another free tier.
Other IaaS cloud application service providers are IBM SmartCloud, Microsoft Azure, and Rackspace Open Cloud.
A cloud application service model which provides a platform for development through the cloud (internet access) is known as the Platform as a Service (PaaS). It provides self-service portals for the developers for managing computing software from their centralized IT operations.
Platform as a Service can be thought of as a layer above the IaaS. After providing the network and storage infrastructure, it also provides the tools for development, building programs for the developers.
If we consider the dancer’s example, PaaS will be like providing a Sponsor as well as an Event Manager for that dancer. So now, he will only have to concentrate on his dance. He will have to choreograph his dance and perform on a pre-developed event stage by the event manager.
The Role of Stakeholders in Platform as a Service (PaaS)
Let us see the role of each stakeholder in the functioning of this cloud application service model.
The PaaS providers host the hardware and the software required to build the application on the cloud (internet). The PaaS infrastructure consists of networking, storage, servers, virtualization, Operating System, middleware, and runtime. So along with the infrastructure, PaaS also builds the environment where development can be executed.
The software developers just need to create/develop and run their programs on the cloud interface without having to install the software or maintaining it. They can even launch any number of applications online.
The client company, however, has to employ and train its IT staff to learn the development methods online for using a custom platform for a specific development.
The names of PaaS Service Providers that you must know
Customer Relations Management software Salesforce Development is the proof of power depicting Platform as a Service cloud architecture model.
Another good example of PaaS provider is the AWS Elastic Beanstalk for development of Java, .NET, PHP, Python, etc programming language applications.
Yet another PaaS provider is the giant- Google App Engine which not only provides a platform for development but also allows integration of Hadoop, MongoDB, and other upcoming technology trends.
The Software as a Service (SaaS) comes as a wholesome computing package hosted on the cloud (internet) where the user can access the application interface mostly through a web browser. As this service is delivered online, there is no need for installation or implementations of external applications.
Explaining it with the dancer’s example, we now provide him with a Sponsorer, Event Manager as well as a Choreographer. Now, all that the dancer needs to do is- Dance. This will be like a piece of cake for him.
This is an example of SaaS, where event audience can be thought of as the end user and the event management company can be thought of as the cloud application service provider. In return of paying a ticket fee (which can be thought of as the cloud subscription fee), the audience gets entertainment.
The Role of Stakeholders in Software as a Service
Let us see how this cloud architecture model functions and what is the role of each of the participants.
The SaaS providers manage everything ranging from networking, storage, servers, virtualization, OSes, middleware, runtime, and data. The companies providing SaaS cloud architecture need to secure large data storage and the platform for development. They are obliged to be responsible for the security of the client’s data stored on the cloud.
The developers of the SaaS platform need to build an architecture which will be like an interface between the user and the cloud operations. There won’t be need of any installations or downloads. Everything will be accessed online.
Nothing much will be required by the end users but to enjoy their demanded service without bothering about installation or updatation. In return, they pay for their subscribed service.
The names of Popular Software as a Service that you must know
An instance of SaaS is the Microsoft Office 365 which enables collaboration of office suite with colleagues and customers of a wide range.
How can we forget Google Apps, which is a comprehensive SaaS for productivity tools for n number of users? Other examples of SaaS are Dropbox and Cisco WebEx.
Originally published at https://www.trootech.com on March 5, 2019.
Track the latest blogs on technology for different…
1 
1 clap
1 
Track the latest blogs on technology for different industries. Upgrade your knowledge with the latest news of technology and upgrade your businesses.
Written by

Track the latest blogs on technology for different industries. Upgrade your knowledge with the latest news of technology and upgrade your businesses.
"
https://medium.com/kaito-blog-%E6%B5%B7%E6%96%97%E6%A8%A3-%E3%81%AE-it%E5%AE%85/datadog-%E5%85%AC%E6%9C%89%E9%9B%B2%E5%9F%BA%E7%A4%8E%E7%AF%87-%E6%95%B4%E5%90%88azure-9ef3f735ce6c?source=search_post---------378,"There are currently no responses for this story.
Be the first to respond.
Azure是市佔率第二高的IaaS公有雲，但網路上關於Azure與Datadog整合的中文文章很少，只有幾篇Microsoft的官方Blog文，只是原本很簡單的設定，但因為翻譯的問題，照著Microsoft的官方Blog文做，反而很模糊。
登入Datadog後在左方的Nav找到
Integrations → Integrations → Azure → Available
在開啟的小視窗，點Configuration，這邊可以看到，我們要到Azure上找Client ID、Tenant ID、Client Secret 這三個Value
其實找這三個Value有什麼難的? 但難就難在
Microsoft Doc 的中文翻譯跟 Azure 中文翻譯不一樣
這對不是很熟Azure 的人可能會在這個步驟一直打轉
登入Azure後搜索 “Azure Active Directory”，在Azure Active Directory內可以看到 “租用戶識別碼”，這就是Tenant ID 先記下來。
新增一個Azure應用程式，取得應用程式產生的 Client ID，先回到Azure Active Directory，左側Nav Menu 可以找到”應用程式註冊”，點”新增註冊”
名稱 → 自取
支援的帳戶類型 → 選擇第一個 僅此組織目錄中的帳戶 (僅 xxxxxxx — 單一租用戶)
重新導向 URI (選用) → 選Web，後面網址填入 https://app.datadoghq.com，最後點註冊
完成註冊後，會有一個 “應用程式 (用戶端) 識別碼”，這個就是 Client ID
在2.2最後一個畫面上，左邊Nav menu 點 憑證及祕密 → 新增用戶端密碼
描述 : datadogClientSecret
到期 : 選永不
按下新增後，產生出的 “值” ，才是我們要的Client Secret..不是後面的 ID
最後把取得的 Client ID、Tenant ID、Client Secret 填入到Datadog 後就完成了，Config 好之後大約要等30分鐘左右，才會在Datadog上顯示。
歡迎來我們官網 : https://www.bearspace.com.tw
Datadog 台灣區總代理官網 : https://www.datadoghq.com.tw
探索世界的新疆界
1 clap
1 
2
Written by
I am S.K., I was Bearspace technology’s CTO & Founder. Our team specializes in IoT, Blockchain, Cloud and Ai. | https://www.bearspace.com.tw
探索世界的新疆界
Written by
I am S.K., I was Bearspace technology’s CTO & Founder. Our team specializes in IoT, Blockchain, Cloud and Ai. | https://www.bearspace.com.tw
探索世界的新疆界
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://camilorojas.co/video-un-recorrido-por-ibm-cloud-iaas-1-2-aed3698dcf10?source=search_post---------237,"Sign in
Camilo
Jul 18, 2018·1 min read
En esta semana tenemos un video de David Martinez de @IBMColombia realiza un recorrido por el panel de control de IBM Cloud IAAS (antes conocido como Softlayer), en este primer video vamos a introducir el acceso, configuración, cotización y soporte de la plataforma.
Interested in AI, ML, Analytics, Startups and a bunch of other stuff
Ideas on tech transformation, #analytics, #ai, #ml, #cognitive enthusiast, techie, #startup mentor, #algotrader, #python, author
"
https://medium.com/digital-transformation-talk/whats-the-difference-between-saas-iaas-paas-apaas-hpapaas-eadf1fdb8f2c?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
The growth of internet services over the last 20 years has introduced a new set of acronyms to describe different solutions, acronyms like SaaS/IaaS/PaaS/ aPaaS/ HpaPaaS. These acronyms can be a little confusing, so we’ll break down the difference between them so you can find the solution that’s right for you.
In the early days of the internet, there were only simple websites and services, and there weren’t any full applications delivered over the internet. With the birth of internet, application providers like Amazon and eBay, a new category of software was born, software that could be delivered to users over the internet. Instead of having to install expensive software on your own servers, with SaaS applications, you can access them on the internet without any setup. Early SaaS providers like Salesforce started charging a monthly subscription to use the service, which allowed companies to buy software much cheaper at a more predictable cost. Today most companies use SaaS applications across every department, and most software today is purchased through monthly/annual subscriptions. The SaaS model has made it much easier and cheaper to deploy software applications, and has given birth to the modern cloud revolution.
In the late 90’s, there began a simultaneous emergence of a new type of “as-a-service” technology model, the IaaS (Infrastructure as a Service) model. Amazon Web Services was one of the pioneers in this space, and they led the charge in building infrastructure services that companies could leverage. Until this time, companies had to manage their own servers and data centers, which was costly and required a lot of technical resources. IaaS providers like AWS allowed companies to get rid of their servers completely by managing them through the internet as a service. IaaS providers radically dropped the price of hosting, deploying and running applications by getting rid of the need to buy expensive hardware and servers.
(IaaS solutions focused on delivering Infrastructure as a service over the internet, while SaaS solutions focus on delivering Applications as a service over the internet)
RELATED: 10 Low-Code Industry Terms Explained
Over time, a 3rd “as-a-Service” offering emerged, the Platform as a Service offering. Infrastructure as a Service (IaaS) solutions started delivering more and more functionality revolving around software development, rather than just infrastructure management services. Platforms like Heroku were built on top of IaaS platforms like AWS to provide an additional layer of abstraction for developers to easily deploy and run their code. Where the IaaS movement was focused on empowering infrastructure engineers, the PaaS movement was all about empowering software developers. These PaaS offerings allowed developers to deploy, run and manage their code with minimal infrastructure work, allowing them to build software much faster. Over time, IaaS platforms started to move up the stack to provide more and more PaaS functionality. Now most of the IaaS platforms like AWS offer extensive PaaS solutions to make software development and deployment much easier.
The PaaS movement above came from IaaS solutions delivering more up-stack functionality, moving from infrastructure controls, to development platform controls. At the same time as this shift from IaaS to PaaS, there was also a simultaneous movement from SaaS platforms developing more and more platform functionality, and this gave birth to the aPaaS (application Platform as a Service) movement. SaaS applications like Salesforce started to move down-stack and delivered more software development functionality so that users could customize and build SaaS applications. These aPaaS platforms were particularly useful for the new generation of “citizen developers” that want to build apps without having to learn how to code. Many SaaS applications today provide software development functionality that moves them into the aPaaS arena. So the PaaS and aPaaS movements are very similar in some ways, because they are both focused on delivering software development services for web applications. You can see the difference between PaaS and aPaaS by looking at where they evolved from. PaaS solutions evolved out of IaaS solutions that delivered more up-stack functionality. aPaaS solutions evolved out of SaaS solutions that delivered more down-stack functionality.
(PaaS evolved out of IaaS solutions delivering more up-stack functionality, where aPaaS solutions evolved out of SaaS solutions delivering more down-stack functionality.)
ALSO READ: Low Code vs. Customized Software: What’s the Difference?
That leads us to the latest acronym to join the “as-a-service” ranks: the HpaPaaS (or High-productivity application Platform as a Service) solutions. HpaPaaS solutions are a further evolution on the application Platform as a Service solution, but highly focused on speed and developer productivity. HpaPaaS solutions like Kintone deliver software development solutions that help developers rapidly build web applications without having to write code. As companies have started to empower “citizen developers” to build apps, they are starting to look for ways to maximize the amount of apps that these citizen developers can effectively build. HpaPaaS solutions like Kintone give citizen developers the ability to rapidly turn their ideas into functioning apps.
(HpaPaaS solutions add an additional element, speed, to the mix, and provide much faster software development functionality than standard aPaaS solutions.)
Originally published at https://blog.kintone.com.
Discover digital transformation of your workflow and team…
1 
1 clap
1 
Discover digital transformation of your workflow and team collaboration with Kintone.
Written by
Love your data again with custom, easy-to-build business apps for your team. We talk about #CitizenDevelopers #DigitalTransformation and #CompanyCulture
Discover digital transformation of your workflow and team collaboration with Kintone.
"
https://faun.pub/using-terraform-to-configure-sql-server-on-azure-vm-7cdba2c1a3b3?source=search_post---------177,"In this article, I will run through an example of how to create a SQL Server VM in Azure using Terraform, utilizing the SQL Server IaaS extension.
Not all SQL applications are fully compatible with cloud-native database services such as Azure SQL (PaaS) or CosmosDB, and Azure SQL Managed Instance (Managed IaaS) which provides higher compatibility may be too expensive an option or require application changes to fully exploit (e.g. drivers may need to be updated). It is still necessary for many scenarios to run a SQL server in…
"
https://rodolfofadino.com.br/configurando-o-acesso-ao-iis-em-uma-m%C3%A1quina-virtual-no-windows-azure-iaas-a9b775e0e72a?source=search_post---------190,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
O Windows Azure oferece diversas soluções de cloud computing, estas soluções são divididas entre diversos modelos: IaaS (infrastructure as a service), PaaS (plataform as a service) e SaaS (software as a service). Recentemente fiz uma apresentação sobre Windows Azure: PaaS e IaaS — Minha Vida Tech Day.
No modelo de IaaS (infrastructure as a service) podemos criar as nossas máquinas virtuais e configurar nossas aplicações.
As máquinas virtuais no Windows Azure possuem duas principais “camadas” de firewall e configuração de rede, no caso a do Windows Azure e a do sistema operacional da própria máquina.
A ideia deste post é mostrar como tornar acessível um Web Server IIS através das principais portas utilizadas.
Inicialmente vou criar uma máquina virtual com Windows Server 2012 R2 no portal do Windows Azure:
Após a máquina criada, vou instalar o IIS nela
Como é possível ver, o IIS está acessível internamente na máquina
Porém ao acessar externamente, ainda não configuramos o acesso.
A primeira configuração que faremos será criar dois pontos de extremidade para as duas principais portas utilizadas TCP 80 (HTTP) e TCP 433 (HTTPS).
TCP 80
TCP 443
Depois de configurarmos os pontos de extremidades no painel do Windows Azure, precisamos validar se as mesmas portas estão liberadas na configuração do Firewall da nossa máquina virtual, no Windows Server.
Para isso, vamos nas regras de Inbound.
E validar ou criar doas regras liberando a porta TCP 80 e a TCP 443.
TCP 80
TCP 443
Com isso nosso IIS já está acessível pelo mundo
Bom, espero que esta dica seja útil, estou a disposição para dúvidas, criticas e sugestões.
Abs
Rodolfo
/* LIFE RUNS ON CODE */
2 
1
2 claps
2 
1
Written by
Software Engineer and Solution Architect, Tribe Leader at Vitat
/* LIFE RUNS ON CODE */
Written by
Software Engineer and Solution Architect, Tribe Leader at Vitat
/* LIFE RUNS ON CODE */
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jameshamann/cloud-computing-service-types-3da6998a7a11?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Hamann
Nov 24, 2017·3 min read
These days most people refer to things being in The Cloud but what does this actually mean? A general, high-level definition is the delivery of hosted services over the internet. Email, calendars, todo lists, photos, pretty much everything fits into that description. Think Google Drive backing your photos up or iCloud backing your iPhone up, both of these keep your data in stored in the cloud. This makes it easy to access your stuff across all of your devices. There are, though, a few different levels to cloud computing.
This is the outermost layer of the types, typically targeted at the end customer. The provider makes their application available through a web-browser, mobile app or dedicated desktop app, which is powered by cloud infrastructure. Lets use Google Sheets as an example here. The benefits provided to the consumer include the ability to create, edit and update spreadsheets from anywhere, with multiple users able to edit at one time.
This is the middle layer of our pyramid, typically used by developers. Here a developer can deploy their app, written in whatever language they’ve chosen, to a pre-configured cloud infrastructure. The developer doesn’t deal with the configuration of the servers, databases or operating systems. Instead they are able to deploy their app with ease and speed. Let’s use Heroku as an example here. The benefits provided to the developer is quick, easy deployment with minimal hassle. You can push a rails app live within minutes, it allows for rapid prototyping and gives developers the ability to deploy an app without needing to worry about things like server configuration.
This is the final layer, where all the nuts and bolts lie. This level is typically used by sysadmins who would be charge of provisioning servers and deploying application builds. From here you’re able to completely configure everything, from operating systems and network settings. Lets use AWS’s EC2 instances as an example here. From the EC2 console when deploying a new instance you’re able to choose from a wide range of AMI images. When your instance is setup, you can SSH in and configure it in anyway you see fit. This gives the user huge levels of customisation and flexibility, which is important if you’re running apps that require very specific needs.
The diagram below provides a visual representation of each level.
Beyond this there are further abstractions like DaaS (Data/Desktop as a Service), STaaS (Storage as a Service) and SECaaS (Security as a Service). These are quite specific, focused in particular sectors and not as commonly known as the main three mentioned above.
As always, thanks for reading, hit 👏 if you like what you read and be sure to follow to keep up to date with future posts.
Software Developer https://jameshamann.com
11 
11 claps
11 
Software Developer https://jameshamann.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aheadcrm/clash-of-titans-microsoft-and-sap-weigh-in-3e67a99867f?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Aug 28, 2018·6 min read
As it has been some time since I published Clash of Titans — Platform Play, the first part of this little series, let me start with a little recap.
The business applications market, especially the CRM market, is evolving fast. CRM has morphed from concentrating on transactions to become an enabler of engagements. Engagements in turn result in experiences. And positive experiences are what companies want to achieve.
In a digital world this is possible only if companies rely on a foundation, a (technical) platform. Becoming the provider of the dominant technical platform therefore has become the main goal of of the big business software vendors.
However, even governing a great technical platform is not enough. Software vendors that want to be successful platform players need to be able to deliver on four areas to succeed:
Only if they deliver on all four aspects are ‘platform players’ able to provide their customers with what they need to involve themselves in digital engagements that result in sustainably positive experiences.
I will look at how the big four are measuring up in this and the next article of this little series. Microsoft and SAP will be the starters. Then I will look at Oracle and Salesforce.
I might conclude with some surprise additions.
But let the games begin!
Microsoft is the (not so, if you look sharply) hidden champion of this game.
Actually, I think that Microsoft is the 800 pound gorilla in this game.
It is Microsoft’s objective to become the fabric that connects enterprises of all sizes with their stakeholders, including the customers’ personal lives. And, as I have written earlier, they have all the ingredients they need to achieve this objective.
With Azure there is one of the strongest IaaS and PaaS games in the play. Including Azure Stack, Microsoft is capable of offering hybrid deployments. And in all seriousness, hybrid clouds are here to stay for quite a while. The Power* series of tools and applications helps extending the business applications and the development environments are very powerful.
Microsoft is the clear leader when it comes to productivity applications outside the core business applications, which are also tightly integrated into the business applications for further impact.
On the insight side the company has few equals, being able to leverage the power of both, LinkedIn and Bing, their own search engine, and the wealth of data that comes from a plethora of different devices. This is combined with strong analytics and machine learning capabilities, which can create insight out of all the data.
Last but not least, Microsoft has one of the strongest ecosystems on this planet. What they do not have themselves, someone else is developing. Microsoft then embraces the partnership angle. A very visible example for this is the current strategic partnership with Adobe. Microsoft till recently did not have an enterprise grade multichannel marketing engine. Nor an ecommerce system.
Adobe has both. Microsoft is reselling Adobe as part of their strategic partnership.
Finally, there is something else that needs to be considered. As stated above, the enterprise market is saturated. All big vendors, and many more small ones, are tackling the SMB market. Microsoft Dynamics is not as high end as the competition but very suitable for the higher end of the SMB market. This makes Microsoft interesting for companies that would not look at SAP, Salesforce, or Oracle. Microsoft also runs a very strong partnership with Nimble, which opens up the Microsoft Dynamics world for small, but growing businesses. On the flip side: Microsoft is not (yet) universally perceived as a business applications company.
Still, the company is in a position that none of the other vendors can take (yet): None of them is able to combine the enterprise value chain with a productivity suite as powerful as Microsoft’s and a reach to the end customer. Add some tremendous data assets and a strong ecosystem to the mix. Exactly this breadth and width of the offering is Microsoft’s biggest strength, which covers for functional weaknesses in some areas like supply chain, ecommerce or marketing. If Microsoft is able to mitigate these weaknesses while continuing to play to its strengths, the company should be able to gain considerable market share on cost of one or more of the other three.
Using Microsoft’s objective as a guideline, SAP is currently the business vendor that is closest to being the fabric of enterprises. The reason for this is the access to data in combination with the ability to support the whole value chain of companies across a wide range of industries and sizes.
Looking at the enterprise software market, SAP is the clear leader in ERP software. On top of this, SAP has Ariba. Ariba, or the SAP Business Network is one of the largest, if not the largest, business market places around.
In combination, SAP can say that nearly 80 per cent of all business transactions touch an SAP system, one time or another.
On top of this, with Gigya SAP owns one of the strongest Customer Identity and Access Management platforms around. Via this platform SAP manages more than 1.3 billion identities, along with their consents, across hundreds of sites. The smart progressive profiling methodology that is built into this software also helps with building valuable profiles out of these mere identities.
In combination with SAP’s strong analytics capabilities, the company is able to generate extremely valuable insight to businesses.
In addition SAP, like Microsoft and Oracle, covers the complete value chain of businesses, just that SAP’s offering is probably stronger than that of the competition. This, again, gives SAP customers vast amounts of consistent data that can be utilized to offer more value to their customers.
This application strength in what SAP calls the ‘digital core’ is also a good part of its platform. Add to it an SAP Cloud Platform (SCP) that offers the majority of services that an agile business needs and that gets increasingly stronger we have a very strong (PaaS) platform play. This platform, along with one of the strongest partner ecosystems around can open up the SMB market, and hence strong growth, for SAP.
Talking about its ecosystem, SAP has a wide variety of implementation and consulting partners, that support each other and get support via platforms like the SAP Community Network SCN, or the SAP Partner Edge.
SAP positions itself as an enterprise software player, as opposed to a full stack player like, e.g. Microsoft. Consequently, the company’s IaaS capabilities are limited. Instead, SAP is pursuing a multi cloud approach by being able to have its software run on all major infrastructures. This is a smart move as it keeps the company’s independence while offering choice to clients.
One of the company’s better kept secrets — SAP Business by Design — is a central element of its strategy to address midmarket companies.
Apart from the prevailing (but increasingly wrong) opinion that SAP is difficult to deal with, SAP’s weakest spot is productivity in the sense of office products. The company covers this topic via integrations with Microsoft’s suite of applications, and probably Google apps soon. But, to be sure, efficient, and automated execution of business processes is at the heart of SAP’s value proposition.
If SAP continues to play on its ERP strength while pursuing the newly established focus on its Customer Experience unit it has a good chance to stay one of the dominant players.
Here we have two companies that are greatly positioned in a platform market. With different starting points and strenghts they share similar visions. And they work hard towards it.
Exciting times.
But let us see how Oracle and Salesforce are playing their cards — or should be.
A bientot!
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
See all (1,289)
3 
3 claps
3 
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aalphaindia/the-difference-between-paas-iaas-and-saas-aalpha-3b6d8764720f?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aalpha Information Systems India Pvt. Ltd.
Mar 27, 2020·7 min read
Anyone in a business, whether big or small, usually tend to cut on unnecessary costs as much as they look forward to making more and more profits. However, the organization can only achieve this through cloud application development. The service enables business setups to change to virtual management from the physical control of resources with ease. Globally, the public cloud service market is set to have a significant growth in 2020 from 2019 statistics.
However, there are many cloud-based services out there, which include Paas, Iaas, and Saas. Many business persons tend to have a hard time trying to choose the best for their businesses. There are significant differences between the three cloud services meant for different functionalities. For these reasons, this article is intended to reduce your doubts and select the perfect cloud service to achieve the goals for your business.
But before we discuss the differences between the three cloud services, we must understand what cloud computing is.
Defining Cloud Computing
The fact about cloud services is that it minimizes costs related to IT infrastructure. The service is on the rise because businesses can request and access the hardware and software of the system with ease. Services that can be accessed include data storage and computing power over the internet. All these advantages can be related to the phrase,” Anywhere you go, the cloud follows.”
What cloud computing does is that it helps businesses minimize the control of computing resources to give them time to concentrate on business development activities.
Note: Cloud can be made for access by several organizations, meaning public cloud. Cloud can also be made for a specific organization, meaning the enterprise cloud. The cloud-based services, which include Paas, Iaas, and Saas, offer beneficial services; hence they fall under public cloud service.The difference between Paas, Iaas, and Saas Cloud Computing Models
The main cloud-based computing services, PaaS, IaaS, and Saas, are improving business operations worldwide. First of all, they are known to reduce the cost of IT infrastructure while transforming your digital experience at the same time. Outlined below are the differences between the PaaS, IaaS, and Saas cloud-computing services.
PaaS — (Platform as a Service)
Platform as a service is perfect for software developers. The benefit of PaaS is that it is compatible with different languages for programming, and it has full control to create custom software. However, the disadvantage of PaaS is that it is not quite flexible when compared to IaaS
The main function of the platform as a service (PaaS) is to give a useful framework for developers to manage new product apps, to build the app, and also testing of the applications. Developers find it easy to use PaaS because it serves the database, application tools, and operating system that are required for the app development at the same time. This implies that there is no need to have the resources differently, thus, saving more time and resources.
Many developers love PaaS because it gives them a platform to build apps that can be provided as a Saas solution. The best example of PaaS is the giant Google App Engine. With Google App Engine, applications can be easily created and hosted without any difficulty.
Note: PaaS is accessible on a pay-as-you-go rule, meaning that you pay for the resources you have subscribed to. You only work on the built application since the vendor deals with the rest.Features of PaaS
IaaS — Infrastructure as a Service
The primary function of Infrastructure as a Service is to provide visual data centers to businesses. This cloud service is suitable for IT administrators. IaaS offers a full infrastructure, the server, and the storage space where new technologies and experiments are conducted over the cloud. With IaaS, you can perform data mining analysis, host a website and software solution, and creating virtual data centers for large scale enterprises.
In IaaS, the vendor works on networking resources, storage space management, and the dedicated data center. On the other hand, the business is working on specified tools for development, managing apps that are hosted, and deployment of the operating system.
Amazon Web Services is an excellent example of using IaaS, which is the top public cloud space. Apparently, Netflix and Salesforce brands are moving towards Amazon Web Services to support the customer base that keeps growing every other day.
Advantages of IaaS
Disadvantages of IaaS
Saas- (Software as a Service)
Software as a Service cloud service is hosted by the service provider, which is available to the consumers based on the pay-as-you-go. SaaS cloud service is suitable for consumers across different localities. The software license is open on either yearly or monthly subscriptions, and it can be accessed via a browser with stable connectivity of the internet.
Some of the business apps that use SaaS include the following:
The primary function of SaaS product development is to provide cloud-based apps to consumers. Dropbox, which is known for sharing and downloading files over the network and Google Docs, which is known for creating and sharing documents over the web, are perfect examples of SaaS cloud services.
If you are in digital businesses, you should consider SaaS in cloud computing to enjoy zero management since the vendor handles all activities. Another reason to consider Saas cloud services is how easily it creates solutions whenever technical issues arise.
Advantages of SaaS
Cost-effective: Since Saas works on a subscription basis, users don’t have to pay for an up-front fee for licensing; hence, no initial costs. Also, it is the Saas provider that manages IT infrastructure meant to run the software; therefore, no software and hardware maintenance fees.
Upgrades are easily deployed — As a user, you are relieved the work of implementing upgrades, hardware, and software updates. SaaS service providers do the job.
Scalability — There is a wide range of SaaS options for subscription, and you can always change any moment. For example, when your business shows growth, you can change the subscription plan because it will mean more users need access to the business.
It is easily accessible — You only require internet connectivity and a browser to access the Saas app. The requirements are available on multiple devices across the world, meaning that Saas is easily accessible.
Faster set up and deployment — The process of setting up and deploying SaaS is quick 0 because the service is already installed and configured in the cloud. This reduces delays experienced in the traditional software installation process.
Disadvantages of SaaS
Data and security issues — There is no privacy of sensitive information in hosted and cloud services.
Restricted range of apps — As much as SaaS has gained popularity, there exist several apps that do not offer a hosted platform.
Performance — At times, Saas may experience slow speed than on serve apps because the software is not hosted on a local machine.
Internet requirement — SaaS relies heavily on a stable internet to deliver web services. In case the internet connection fails, you can’t access the software data or, even worse, lose the data.
Having highlighted IaaS, PaaS, and SaaS, you will realize that all the cloud-based services are meant for different functions. It all depends on the needs and requirements of your business. Although it becomes a bit tricky to distinguish the threes, all you need to do is read and understand the basics of each cloud-based service. Know the feature, benefits, and the downfall of each. From there, you can determine the best service for your business.
Conclusion
You will require the IaaS service when starting a website to help you host the applications. However, you will need PaaS service if the purpose of your business is to build a custom software product. When the product is done, it will be termed as SaaS product, ready to be used.
You will notice that IaaS, PaaS, and SaaS are independent of one another to run the operations of the business. When choosing the service, know well the needs and requirements of your business and select appropriately. To succeed in your business, it will depend on how you use the services.
Good luck as you plan to uplift your business with amazing cloud-based services; Iaas, Paas, and SaaS.
Contact Aalpha today if you need further information.
Originally published at https://www.aalpha.net on March 27, 2020.
Aalpha is specialist India based Software Solutions company providing solutions for Web and Mobile development, https://www.aalpha.net
Aalpha is specialist India based Software Solutions company providing solutions for Web and Mobile development, https://www.aalpha.net
"
https://medium.com/instinctools/choose-the-right-cloud-model-for-your-business-iaas-paas-and-saas-31ba4f56e845?source=search_post---------157,"There are currently no responses for this story.
Be the first to respond.
Today’s world is flooded with anything-as-a-service. Restaurants provide meals-as-a-service, travel agencies offer holidays-as-a-service, landlords — accommodation-as-a-service, taxis are cars-as-a-service… The list is endless.
Can you imagine how much more difficult our lives would be if we couldn’t buy services? For sure, it’s great to have your own car, not to mention an apartment, or have enough time to cook food all by yourself. But — there’s always a ‘but’, you know, — it either requires huge money infusions or tons of effort; oftentimes both.
The same goes with IT. Back in the day, companies had to spend a fortune on the hardware and hire specialists to maintain it — the privilege that small and medium-size businesses couldn’t afford.
However, thanks to cloud computing, the way of delivering IT services has changed. We no longer need to own cumbersome equipment to enjoy the benefits of cutting-edge technologies.
The main cloud computing models are:
Each is unique in its own way. So the choice of ‘the right one’ should be made in accordance with business requirements and the number of tasks you are ready to delegate to the provider.
IaaS is the most basic form of cloud computing. With IaaS, a client rents a variety of infrastructure components such as data centers, servers, cloud storage, and networking solutions from a provider. Basically, IaaS offers access versus ownership. An IaaS provider takes responsibility for the infrastructure, whereas users are in charge of installing, managing, maintaining, and supporting their apps and operating systems.
IaaS can be a great choice for:
PaaS provides a platform with built-in software components and tools for application development. PaaS vendors manage infrastructure and data centers, operating systems updates, security patches, and backups. Meanwhile, clients handle application development without worries about the above-mentioned aspects. In a nutshell, PaaS accelerates software development and significantly simplifies this process.
You should opt for PaaS, if:
Remember the times when we used to buy and install programs on our personal computers? SaaS has nothing to do with that. It’s a model of cloud computing that delivers the software to users without them having to install it on local servers or computers. The only thing you need to get access to the applications is a reliable Internet connection. The rest is taken over by the vendor. SaaS vendors manage all the tedious tasks from sustaining hardware solidity to providing proper app functioning, while users simply open the apps in a browser.
SaaS solutions are beneficial to:
Whichever cloud model you go for, it will take some time to adapt to it. Migrating your existing technology to a new cloud platform and training your team to manage it might turn out more demanding than you initially pictured. So, it can’t hurt to have an experienced managed service provider by your side, who is able to evaluate and prioritize your business needs, deliver a wide range of cloud services, and make sure your new environment operates properly. Contact us to know how to leverage the flexibility of cloud models to the full extent.
Originally published on instinctools.com
Delivering the future. Now.
301 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
301 claps
301 
Written by
We advance and innovate businesses with digital transformation. 20 years in the market. Let’s talk > contact@instinctools.com
*instinctools is capitalising on 20 years experience in technology, strategy and data to transform how you interact with the world; delivering tailored solutions that close the gap between vision and reality.
Written by
We advance and innovate businesses with digital transformation. 20 years in the market. Let’s talk > contact@instinctools.com
*instinctools is capitalising on 20 years experience in technology, strategy and data to transform how you interact with the world; delivering tailored solutions that close the gap between vision and reality.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@avs431/explain-it-to-me-like-i-am-a-5-year-old-cloud-delivery-models-iaas-paas-and-saas-with-use-77499c233fd3?source=search_post---------12,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ameya Shanbhag
Apr 5, 2020·5 min read
I always wondered what the cloud is and wanted to start exploring the cloud. When people around me used to say words like PaaS, IaaS, Microservices, Docker, I used to be so overwhelmed that I used to just nod my head so that I don’t look dumb. But soon I realized that it's not just me, but also people who talk about IaaS and all these words, have little idea about what it actually means in real-world so I thought to not only educate myself to understand the meaning but also to make sure I know when to use what given a real-world scenario.
This blog is dedicated to all who want to learn cloud concepts but in a simple and easy manner.
The best way to learn anything is to visualize it so here is something that I found, which sums up the whole topic:
Let’s say your end goal is to go from point A to point B:
You can consider IaaS as buying a car, While buying a car, you look for all the specs, the color of the car, interior design and also do a lot of research before actually buying it. Once you decide on the car, you pay for it and then you can drive the car. Here the car is the ‘infrastructure’, you driving it is ‘using it’ and you pay for it is ‘service’. Another example is like going on AWS, renting an EC2 instance and spinning up Windows machine.
On the same grounds, PaaS is analogous to renting a car where you do not have to do any research on specific specs but there are few already bought cars that come with in-built specs. You just have to pay for it and then you can drive. Here the rental car is the ‘Platform’ and rest everything stays the same as the previous one. Another example is like using AWS BeanStalk where you tell what you want, and it automatically runs an instance and downloads all the software you want.
SaaS, on the other hand, can be considered as a taxi where you are not even driving the car but using someone else’s service to go from point A to point B. Here using someone else’s service is ‘Software’ and rest everything stays the same. Another example would be to use Microsoft 365 services online.
Going into specifics and real-world use cases:
Why can’t we use PaaS instead of IaaS?
If you have existing legacy applications already running on your private servers, moving them on PaaS will be difficult because let’s say you want to use AWS as PaaS then as you can see from the above diagram, you will be using the database provided by AWS. This means you will need to change your code to use AWS SDK which can be a tough task.
Polyglot PaaS:
In simple terms, it is easy to deploy an application when written in one specific language, for example, Java application can be deployed as WAR and Fabric can be used for python application. But as we all know, each language is best in doing its own thing so if you want to develop an application that uses multiple languages(polyglot), how would you deploy it? With multiple languages involved, it becomes really difficult to deploy it hence Heroku started working on Polyglot PaaS where you can just develop the application using whichever language you want and it will deploy it for you.For more information visit: https://blog.heroku.com/polyglot_platform
That’s all for today! I will be writing future blogs where I break down Microservices, Docker, and Containers and make it explainable in very simpler terms.
I am just a beginner in this cloud journey so if there is anything that I missed out and explained in a different way, please do let me know. Also if there are any other cloud concepts that you want me to write on, let me know in the comment section below.
I can be reached via Linkedin.
Credits: LinkedIn Learning, IBM Cloud
https://www.ameyashanbhag.com/
11 
1
11 
11 
1
https://www.ameyashanbhag.com/
"
https://medium.com/part-1-of-4-oracle-iaas-and-seven-pillars-of/part-1-of-4-oracle-iaas-and-seven-pillars-of-trusted-enterprise-cloud-platform-5ff8f18b63f4?source=search_post---------256,"There are currently no responses for this story.
Be the first to respond.
Note: My original blog was published in ORACLE CLOUD INFRASTRUCTURE blog site on October 30th, 2018. I have republished it here with permission.
Official Disclaimer: The views and opinions expressed in this blog are those of the author and do not necessarily reflect the official policy or position of Oracle Corporation.Oracle Cloud Infrastructure’s security approach is based on seven core pillars. Each pillar has multiple solutions designed to maximize the security and compliance of the platform. You can read more about Oracle Cloud Infrastructure’s security approach here.
The seven core pillars of trusted enterprise cloud platform are:
Customer IsolationData EncryptionSecurity ControlsVisibilitySecure Hybrid CloudHigh AvailabilityVerifiably Secure Infrastructure
Oracle employs some of the world’s foremost security experts in information, database, application, infrastructure, and network security. By using Oracle Cloud Infrastructure, our customers directly benefit from Oracle’s deep expertise and continuous investments in security.
In this blog (Part 1), I am going to explain how Oracle Cloud Infrastructure security services map to our first two pillars — Customer Isolation and Data Encryption. In the next blog (Part 2), I will cover the next 2 pillars.
1. Customer Isolation
Customer isolation allows customers to deploy application and data assets in an environment that commits full isolation from other tenants and Oracle’s staff. Let’s dive into how we offer isolation at different resource levels.Compute
At the Compute level, we offer two types of instance isolation.
Bare metal instances offer complete workload and data isolation. Customers have full control of these instances. Every bare metal instance is a single-tenant solution. Oracle personnel have no access to memory or local storage while the instance is running, and there is no Oracle-managed hypervisor on bare metal instances. Virtual machine instances are a multi-tenant solution. VM instances run on an Oracle-managed hypervisor and come with strong isolation controls.
Both instances offer strong security controls. Customers who want to have higher performance instances and complete workload and data isolation often prefer bare metal instances.
Networking
Next, the Oracle Cloud Infrastructure Networking service offers a customizable private network (a VCN, or virtual cloud network) to customers. VCNs enforce the logical isolation of a customer’s Oracle Cloud Infrastructure resources. Oracle’s VCN gives you the complete set of network services you need in the cloud with the same network flexibility you have today on-premises.
You can build an isolated virtual network with granular controls, including subnets and security lists. We provide secure and dedicated connectivity from your data center to the cloud through FastConnect with multiple providers like Equinix and Megaport. You can provide end-customers high performance and predictable access to your applications with services like provisioned bandwidth load balancing. All networking services are API-driven and programmable for more automated management and application control.
As with an on-premises network in a data center, customers can set up a VCN with hosts and private IP addresses, subnets, route tables, and gateways using VCN. The VCN can be configured for internet connectivity using an Internet Gateway, or connected to the customer’s private data center through an IPSec VPN gateway or FastConnect. FastConnect offers a private connection between an existing network’s edge router and dynamic routing gateways. In this case, traffic does not traverse the internet.
Subnets, the primary subdivision of a VCN, are specific to an availability domain. They can be marked as private upon creation, which prevents instances launched in that subnet from having public IP addresses. Moreover, Regional Subnets are now available which stretches across multiple availability zone and thus giving more flexibility for IP addressing.
Compartments and Policies
From an authorization perspective, Identity and Access Management (IAM) compartments can be used for isolation. A compartment is a heterogeneous collection of resources for the purposes of security isolation and access control.
All end-user calls to access Oracle Cloud Infrastructure resources are first authenticated by the IAM service and then authorized based on IAM policies. A customer can create a policy that gives a specific set of users permission to access the infrastructure resources (network, compute, storage, and so on) within a compartment in the tenancy. These policies are flexible and are written in a human-readable form that is easy to understand and audit. The easy-to-understand syntax include verbs which define the level of access given to end-users.
2. Data Encryption
Our second core security pillar, data encryption protects customer data at-rest and in-transit in a way that allows customers to meet security and compliance requirements with respect to cryptographic algorithms and key management.Block Volume Encryption
The Oracle Cloud Infrastructure Block Volumes service provides persistent storage that can be attached to compute instances using the iSCSI protocol. The volumes are stored in high-performance network storage and support automated backup and snapshot capabilities. Volumes and their backups are accessible only from within a customer’s VCN and are encrypted at-rest using unique keys. For additional security, iSCSI CHAP authentication can be required on a per-volume basis.
Object Storage Encryption
The Oracle Cloud Infrastructure Object Storage service provides highly scalable, strongly consistent, and durable storage for objects; ideal for media archives, data lakes, and data protection applications like backup and restore. API calls over HTTPS provide high-throughput access to data. All objects are encrypted at rest using unique keys. Objects are organized by bucket, and, by default, access to buckets and objects within them requires authentication. Users can use IAM security policies to grant users and groups access privileges to buckets. To allow bucket access by users who do not have IAM credentials, the bucket owner (or a user with necessary privileges) can create pre-authenticated requests that allow authorized actions on buckets or objects for a specified duration.
Alternately, buckets can be made public, which allows unauthenticated and anonymous access. Given the security risk of inadvertent information disclosure, Oracle highly recommends carefully considering the business case for making buckets public. Object Storage enables you to verify that an object was not unintentionally corrupted by allowing an MD5 hash to be sent with the object (or with each part, for multipart uploads) and returned upon successful upload. This hash can be used to validate the integrity of the object.
In addition to a native API, the Object Storage service supports Amazon S3 compatible APIs. Using the Amazon S3 Compatibility API, customers can continue to use existing S3 tools (for example, SDK clients). Partners can also modify their applications to work with Object Storage with minimal changes to their applications. Their native API can co-exist with the Amazon S3 Compatibility API, which supports CRUD operations. Before customers can use the Amazon S3 Compatibility API, they must create an S3 Compatibility API key. After generating the necessary key, customers can use the Amazon S3 Compatibility API to access Object Storage in Oracle Cloud Infrastructure.
Key Management Service
In addition, Oracle provides an enterprise-grade Key Management service with following characteristics:
Backed by FIPS 140–2 Level 3 HSMs Tightly integrated with Oracle Block Volumes and Object Storage Full control of key creation and lifecycle (with automatic rotation options) Full audit of key usage (with signed attestation by HSM vendor) Choice of key shape via Advanced Encryption Standard (AES) keys with three key lengths: AES-128, AES-192, and AES-256
Load Balancer
For data at-transit, applications should use TLS-based certificates and encryption. Oracle IaaS load balancer services support customer-provided TLS certificates. In addition, the Load Balancing service supports TLS 1.2 by default, and prioritizes the following forward-secrecy ciphers in the TLS cipher-suite:
ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 DHE-RSA-AES256-GCM-SHA384 DHE-RSA-AES256-SHA256 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES128-SHA256
Many customers prefer EC cipher suites due to high performance. However, customers can add weaker cipher suites changes via a support ticket if their legacy clients need them.
Database Encryption
Database encryption is achieved by using Transparent Data Encryption (TDE).
In the second part, I will cover the next 2 pillars. In the meantime, please use these resources to learn more about Oracle Cloud Infrastructure Security:
• https://docs.cloud.oracle.com/
Oracle Cloud IaaS Security Services for the Enterprises
Written by

Oracle Cloud IaaS Security Services for the Enterprises
Written by

Oracle Cloud IaaS Security Services for the Enterprises
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/altoros-blog/introducing-selfportal-the-panel-to-launch-virtual-machines-in-a-few-clicks-baa21d4460bc?source=search_post---------134,"There are currently no responses for this story.
Be the first to respond.
To get virtual machines up and running, it’s a common practice in many companies that a developer creates a task for a system engineer. It may result in waiting on infrastructure to be ready for an hour on average or more. Altoros open-sources the SelfPortal panel — a self-service tool that enables developers to automatically run, modify, or delete virtual machines, as well as publish web applications. Currently, the solution provides support for OpenStack and vSphere. The project is available under the Apache 2.0 license.
Read the full article on our blog:
bit.ly
Stay in touch with the latest Altoros’ updates, subscribe to our social accounts: Twitter, Facebook, LinkedIn, Reddit.
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
"
https://medium.com/@brgentry/for-over-a-decade-our-teams-have-worked-remotely-building-saas-iaas-software-infrastructure-and-f966a398b7eb?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brett Gentry
Dec 20, 2015·2 min read
frazier miller
For over a decade our teams have worked remotely, building SaaS/IaaS software, infrastructure, and content projects with people in the US, Canada, UK, India and the Philippines. It has been great for business and our employees, so I completely concur with your article. Some additional things of (remote) note:
Great article!
"
https://blog.thestove.io/benchmarking-the-performance-variability-in-public-clouds-480b95ad7cca?source=search_post---------354,NA
https://medium.com/@Apiumhub/cloud-computing-trends-to-watch-in-2017-6e0d8696256b?source=search_post---------127,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
Jul 6, 2017·3 min read
The next generation digital businesses might be almost all of cloud platforms due to immense benefits and popularity of could computing technologies. Let’s dive into some of the cloud computing trends in 2017 and an infographic illustrating statistics that indicates the growth and popularity of cloud computing in 2017 and beyond.
Amazon and Microsoft, like cloud providing companies are experiencing exponential growth thanks to cloud adoptions among the masses. Enterprises and leading organizations are increasing their spending on cloud and adopting various services gradually.
Public cloud IaaS services are projected to share billions of revenues from software and hardware needs in coming decades.
The cloud presence is becoming a norm in almost 70% enterprises across the globe, and they have at least one application running on the cloud.
Among the various services covered by cloud computing platforms, data analytics, and data management are seemingly leading in cloud adaptation trends in 2017 and beyond.
Different models of clouds are exhibiting different rates of adoption like a private cloud is exhibiting 77% growth while hybrid 71% and enterprise cloud 31% proportional increase. Therefore, predictions are that total IT budget by the organization for cloud would be 28%.
Almost 46% organizations are integrating cloud APIs for databases, messenger systems, and storage systems and trends are increasing rapidly for other integrations too.
Cloud migration in private cloud is highest against public and hybrid clouds, and it may reach from 45% to 60% in next 18 months.
To conclude, if you think cloud adoption is in favor of your organization and wish to follow trends in the market, you have to look for an eminent cloud service consultant company for technical and all sorts of help. It may prove your cloud migration efforts successful and cost efficient in all respects.
Marc Olson is an experienced content writer, working at Cloud247, a leading Microsoft Cloud Service Provider in Canada. He has been in the industry for the past 10+ Years.
Tech trends in 2017 for software developers & architects
Top software development blogs in 2017
Cloud computing, a growing trend in 2017
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
1 
1 clap
1 
Software architecture, web & mobile app development www.apiumhub.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/nerd-for-tech/1-cloud-series-the-iaas-paas-saas-8faa46124722?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
The IAAS | PAAS | SAAS
Real World Example: This is like owning a car. All the “Maintenances” that is required for the car must be done by your or you can hire a vendor to do the maintenance. The ownership of the car is yours. You can have your own Depreciation cycle and can replace or replenish the Hardware. All the contents that you put in the hardware (based on any specific core licenses) is your call. You can either host your own Data Centre or you can hire a rack in a huge data centre (a farm) and have your servers / hired servers hosted in there. On-Premise Data centres for Primary and Secondary works the same way.
On AWS: AWS Outpost enables many of the cloud services to be deployed on-premise. Outpost comes with preconfigured rack with network, compute and storage. organizations can run EC2, containers, EBS, few variant of databases on premise as you will deploy in the public cloud.
Real World Example: This is like HIRING / LEASING a car. All the “Maintenances” that is required for the car during the period of lease must be undertaken by you. Once the car is returned, it is not your responsibility any more. You do not own the car.
On AWS: You lease an Instance and deploy your services in that instance. As long as you pay for the service and you use it, it is yours. No one else can get access to that area. Under the shared responsibility model, all the Data / Resources / Users you create / Network traffic / Patching & upgrading etc., is your responsibility. Hardware maintenance / Networking / Switches is AWS responsibility.
Real World Example: This is like a TAXI. You hire a taxi to travel from Point A to Point B and pay for the “Metered” use of the Taxi.
On AWS: Service provider delivers platform to clients, enabling them to develop applications on them.
Real World Example: This is like getting TICKETS in a Bus. Depends on the number of tickets / seats you purchase, you will be charged accordingly. Certain bookings may have a minimum booking requirement.
On AWS: Application comes pre-built on cloud where you can pay for the number of seats required and use the software where you can input the data and process it. Configuration and Rules setting according to your needs, configuring the fields usually will be allowed in few of the SAAS.
Real World Example: Ride-share using Uber or Grab is equivalent analogy for Serverless. You don’t have to worry about how much mileage the care will give, breaking system, security etc. All you have to make sure is the ride can take you from point A to B without any issue and you can opt for any additional services the ride provider may provide.
On AWS: Any fully managed service in the AWS stack is pretty much a serverless service. You do not have to spin up and maintain any server / service on your own — it will be auto managed by AWS. For serverless it is not a debate between whether we should go for a serverless approach or server based approach. Even if you go for complete server based few of the services can be serverless in your technology stack such as Dynamo DB, AWS Glue etc. Serverless stack for AWS includes Compute: Lambda & Fargate; Application integration: Eventbridge, Step functions, SQS, SNS, API Gateway, AppSync,Data Store: S3, Dynamo DB, RDS Proxy, Aurora Serverless
To decide this, it is important to understand the Trade-off between IAAS | PAAS | SAAS.
Anything and Everything as a Service — XAAS.
What I want to see more is Cloud offerings for specific “extremely sensitive verticals”. Finance may not want to share the data with Health-care and vice-versa. A FAAS — Finance Cloud As A Service or HAAS — HealthCare As A Service, will make it even more attractive for Cloud adoption. The reason one would need vertical wise offerings is that having the ability to have a complete Checklist defined Cloud Book of Knowledge, Required security practices for that industry vertical etc., This will enable more cloud adoption.
Next section is also about Cloud Models from Deployment point of view.
Next Part: Business Case for Cloud.
From Confusion to Clarification
24 
Subscribe to our weekly News Letter to receive top stories from the Industry Professionals around the world Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
24 claps
24 
Written by
All the views expressed here are my own views and does not represent views of my firm that I work for. Data | Big Data | Cloud | ML
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Written by
All the views expressed here are my own views and does not represent views of my firm that I work for. Data | Big Data | Cloud | ML
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@behroozam/calico-ipam-in-rancher-v2-ffa246e0dab6?source=search_post---------369,"Sign in
There are currently no responses for this story.
Be the first to respond.
Behrouz hasanbeygi
Feb 17, 2019·3 min read
The rancher is a great setup with easy-to-config and magnificent UI and integration with cloud IaaS providers like AWS. I have been a fan of the project since the early state of Rancher in version 1 with cattle style deployment before kubernetes become standard for the community of container orchestration.Calico is another CNI with many features and flexibility that make it a great choice for cloud environments in pure L3 network level.
This article depends on your networking vision and requirements in the kubernetes cluster, so reading the following articles is necessary for a deep understanding of networking in kubernetes.
kubernetes.io
docs.projectcalico.org
docs.projectcalico.org
medium.com
medium.com
medium.com
medium.com
The rancher by default uses the host-local calico plugin which itself uses kubernetes API CIDR address for assigning an IP address to containers. This is a painless simple network for small size k8s clusters but in a complicated scenario, we need different options, such as IP ranges or floating IP address for deployments that we cannot load balance or HA them with nginx ingress.
In order to use calico-ipam in rancher, we need to have another etcd cluster and to edit some of the variables in calico deployment for kubernetes.
Etcd is installed by RKE(Rancher Kubernetes Engine) in your host, so you can use this secure setup in calico by mounting key and certs in the calico controller and calico-node and address them in configmap or simply put them into configmap or secret , due the educational purpose in this tutorial I using another not-secure etcd pod instead of rke secure etcd.
I put both of calico and etcd deployment in this gist.
important note :
default CIDR of the rancher is 10.43.0.0/16 but if you use another setup tool you can see CIDR with
I am using the latest current version of calico that is v3.5 , this version support namespace annotation and nodeselector for ippool
for creating rolls we need calicoctl and connecting calicoctl to etcdv3 backend of calico I prefer alias in my shell
now the time is create ippool and annotate them to the namespaces
and after create this file simply apply to cluster by
now you can see your ippool in etcd with
and annotate ippools to namespaces
if everything is ok your new deployment in two namespaces most get a different ip address.
calico have many options for securing or connecting namespaces and containers with Network Policy and Global Network Policy and scale cluster with BGP route which is great compared to flannel.
SRE at Hasty.ai
1 
1 
1 
SRE at Hasty.ai
"
https://medium.com/@eFileCabinet_57957/xaas-the-good-the-bad-the-ugly-32-experts-share-their-insights-873440976dd9?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
eFileCabinet
Apr 28, 2017·3 min read
Cloud adoption has been growing rapidly over the past years. The 2017 State of the Cloud Report by RightScale states that 95% of IT professionals are using the cloud.
This highlights a dramatic shift in not just adoption of the cloud, but the attitudes surrounding it.
IT organizations might be drawn to different types of ‘as-a-service’ offerings — from Software-as-a-Service (SaaS) or Platform-as-a-Service (PaaS) to Infrastructure-as-a-Service (IaaS).
These have all culminated in what may ultimately become XaaS: Everything as a Service, but the final lineup of technologies comprising that list is yet to be actualized.
Besides the typical SaaS, IaaS, and PaaS offerings, many additional types of ‘as-a-service’ are available, such as Database-as-a-service, Storage-as-a-Service, Windows-as-a-Service, and even Malware-as-a-Service.
Both organizations planning to adopt a cloud service or those who are already managing resources on the cloud are faced with their own challenges and concerns.
Based on RightScale’s latest survey, lack of resources or expertise and security concerns are not as major concerns as in previous years, but mature cloud users state that managing costs for cloud platforms and services is a strong concern.
Most IT experts know that cloud consolidation has numerous cost benefits, but their concerns surrounding the cloud are the lack of acceptable models for deploying and leveraging cloud solutions.
There are also specific challenges for organizations adopting SaaS, PaaS, and IaaS. To name a few, SaaS provides limited control over the infrastructure on which apps are running, despite cloud technology vendors making drastic improvements on bandwidth.
PaaS adoption can prove to be very complex and requires adjustments and code changes, and IaaS creates dependency on a particular cloud provider, which isn’t always beneficial for organizations.
Recently, the notion of Anything-as-a-Service (XaaS) is becoming increasingly popular. XaaS is an umbrella term referring to an increasing number of services that are delivered over the Internet rather than provided locally or on-site.
XaaS eliminates distinctions between the different ‘as-a-service’ offerings and platforms, moving the focus from the type of offering to how all services can be made available and manageable over the Internet.
In this new world of “Everything as a Service”, organizations can tailor their environments to more rapidly accommodate changing employee and customer requirements.
“The death of commodity IT services, such as email, begins with IaaS, PaaS, and SaaS, and culminates in XaaS… Although the SaaS, PaaS, and IaaS models are important, the impact of XaaS will tip the scale toward a massive paradigm shift in which technology is no longer considered a mere resource, but also a strategic imperative for ensuring business profitability in mass.”
For the full quote and additional perspectives, read the full article on IaaS, PaaS, and SaaS: The Good, the Bad and the Ugly.
XaaS can mean different things to different organizations: While XaaS could mean everything-as-a-service for specific use-cases, others require anything-as-a-service — a broader category of services related to cloud computing, which will both expand and contract within the coming years as various cloud deployment models are tested by business processes.
XaaS might be the answer for several cloud challenges, allowing businesses to cut costs or retrieve resources more easily. Only the future will tell for sure.
But rather than regarding XaaS as just a buzzword, business interested in transitioning to XaaS should explore this notion further, understand how it can serve their current needs and address major pain points, and take in to account the impact of the transition on their organizational structure and culture.
Read what other IT experts had to say on XaaS on Stratoscale’s article: IaaS/PaaS/SaaS — the Good, the Bad and the Ugly.
Share your thoughts in the comments or give Stratoscale your opinion on either Facebook, Twitter, or LinkedIn.
Originally published at www.efilecabinet.com on April 27, 2017.
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
"
https://medium.com/@dviveiros/iaas-%C3%A9-commodity-qual-o-real-benef%C3%ADcio-da-nuvem-80e673807f0b?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Viveiros
Jun 23, 2015·3 min read
IaaS é commodity. Qual o real benefício da nuvem?
Nessa semana, no dia 16/06, aconteceu o Google Next — evento sobre Google Cloud Platform (GCP) — em várias cidades como Nova Iorque, Tokyo e São Francisco com live stream para o mundo inteiro. O keynote foi bem interessante e me fez refletir sobre o ponto que estamos no assunto cloud computing e o que esperar do futuro próximo. Compartilho abaixo algumas das minhas reflexões:
1. Acabou o medo de se adotar infraestruturas e plataformas em nuvem
Pelo menos para cenários como hospedagem de aplicações não-críticas, campanhas de marketing, big data ou aplicações digital / mobile. Há mais de 5 anos trabalhando quase que exclusivamente com o assunto, é nítida para mim a diferença de comportamento e aceitação de empresas de todos os tamanhos com o assunto. Hoje eu vejo empresas de todos os tamanhos e nacionalidades tratando o assunto com total naturalidade. A exceção acontece em alguns poucos segmentos mais conservadores ou fortemente regulados, mas mesmo assim movimentos significativos já acontecem em setores financeiros e farmacêuticos, por exemplo.
2. IaaS está se tornando commodity
Principalmente se considerarmos os principais provedores mundiais de nuvem. E commodity no sentido mais puro da palavra: pouca diferenciação por valor agregado e maior diferenciação por preço. Apesar do entusiasmo das empresas ao moverem os seus primeiros sistemas para um servidor rodando na nuvem, elas estão apenas no primeiro passo da jornada. Cada dia mais, a decisão de adotar GCP, AWS ou Azure — para IaaS — se parece com a decisão de ter um chip Vivo, Claro ou TIM. Algumas diferenças aqui e ali em termos de funcionalidades, mas condições comerciais e preços acabam sendo os principais fatores de decisão.
3. A diferenciação em potência de 10 acontece com serviços gerenciados
O salto de 10x de agilidade e agressividade de posicionamento digital vai acontecer para os que souberem fazer o melhor uso dos serviços gerenciados em nuvem. São eles que vão permitir criar coisas incríveis como aplicações com escalabilidade infinita em semanas, analisar e gerar insights a partir de volumes estrondosos de dados, manter sincronização real-time entre dispositivos etc, tudo isso com foco apenas em desenvolvimento. Isso significa colocar o foco e energia da sua empresa no local correto: gerar valor. E não em complexas arquiteturas lógicas e de infraestrutura. E aqui teremos um dilema que as empresas vão precisar lidar: vendor lock-in. Serão 3 opções:
Meu palpite é que grandes empresas vão considerar as três opções e decidir a melhor sob demanda.
E a Google Cloud Platform nesse cenário?
Vejo com bastante entusiasmo a posição atual da GCP nesse cenário desenhado. O evento “NEXT” mostrou como empresas como JDA e PwC estão adotando a plataforma no mundo corporativo com sucesso. Depoimentos de executivos dessas empresas deixaram clara a consistência de performance, escalabilidade e segurança da plataforma. E o mais interessante é que a GCP possui ofertas para todos os cenários que mencionei:
O distanciamento com relação a concorrentes vai acontecer para quem conseguir explorar ao máximo o que as plataformas de nuvem têm a oferecer. E principalmente para quem explorar os serviços gerenciados oferecidos por estas plataformas. Como disse o executivo da PwC no evento: “Google is the original cloud company”. Eu concordo. Na minha visão, a Google lidera indiscutivelmente em visão e execução de entrega destes serviços.
Apaixonado por tecnologia e curioso com o impacto dela nas relações humanas e profissionais. Aprendiz em uma série de áreas, incluindo escrita.
Apaixonado por tecnologia e curioso com o impacto dela nas relações humanas e profissionais. Aprendiz em uma série de áreas, incluindo escrita.
"
https://medium.com/@Codename_One/the-emulator-and-debug-environments-are-pretty-different-to-production-d1c350f2f0cb?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shai Almog
Aug 7, 2016·1 min read
Michael Lugassy
The emulator and debug environments are pretty different to production. Notice that the problem is in the DATA. I can debug locally just fine on my dozen test accounts. But when I have millions of devices and tens of thousands of active users in production it’s not something I can simulate locally.
In IaaS I can just connect to the remote DB from my local app and debug with real world data. Google doesn’t provide anything like that.
They have a python tool to download the database which doesn’t work on much smaller data than what we have. I heard of people who got it to work but all of my attempts failed. Regardless it wouldn’t have been practical to download our amounts of data to debug this.
Like I said, we were a gold customer meaning we paid Google extra every month to get proper engineering support. I’ve been to their offices for personal meetings where they promised to help… This is just not the case. They don’t have an answer on this.
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
2
2
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
"
https://medium.com/@unipaygateway/the-word-of-cloud-computting-saas-paas-iaas-and-more-d49a7e05a9db?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
UniPay Gateway
Sep 29, 2016·1 min read
The world of payment processing is riddled with confusing acronyms and terms. Let’s look at some of them.
Cloud computing allows you to store your data on servers that are maintained off site by an outside party.
DevOps is a cooperative effort between development and operations to automate the development and implementation of software.
Then we have a variety of acronyms, including:
Saas — Software as a Service. Saas uses the web to provide business software that can be managed by third parties. Examples include Salesforce, Google Apps, and the UniPay gateway
PaaS — Platform as a Service offers cloud based software that can be customized to fit your needs. The UniPay gateway is an open source produce that functions as a PaaS that can get you up and running quickly and easily.
Iaas — Infrastructure as a Service allows you to manage your data on a remote server instead of buying, storing, and maintaining your own hardware.
When you choose the UniPay open source payment gateway, we’ll take the time to be sure you understand all of these terms. Learn more on payment processing industry at #UniPayGateway.
Can UniPay help you with your business? Contact us today!
Enterprise-scale, open-source, #PaymentProcessing solutions for #Merchants, #PayFacs and PSPs. For more information, visit UnitedThinkers.com
2 
2 
2 
Enterprise-scale, open-source, #PaymentProcessing solutions for #Merchants, #PayFacs and PSPs. For more information, visit UnitedThinkers.com
"
https://medium.com/@BreezeTelecom/saas-paas-and-iaas-how-and-when-to-use-a90f526f10ae?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Breeze Telecom
Jan 15, 2017·3 min read
Every business needs a fast growth in the high-tech world of today. This makes business organizations to consider cloud services for the implementation of applications and infrastructure. While choosing the cloud services, it becomes crucial to understand the core categories of such services. When it comes to dealing with such services, confusion surrounds the business organizations. What matters most is the knowledge of such services. People are still confused about what SaaS, PaaS and IaaS are?
Let us dig deeper into the differences between IaaS, SaaS and PaaS and check what is beneficial to use and when…
The model provides various services that facilitate development, testing, deployment, and hosting of software applications. In addition to this, several users can use the same development application.
PaaS is suitable for multiple developers working on the development or for the external parties that involve themselves in the development process. PaaS brings speed and flexibility in such development processes. Unlike IaaS, PaaS model is for large organizations who wish to customize their applications. For the organizations that want to spread their capital investments, PaaS model comes to help by reducing overhead costs.
All the three cloud models namely SaaS, PaaS, and IaaS offer specific features and deal with certain functions. While starting a business, the users must look into the details of the cloud services seriously. All the businesses must make an effective choice that suits their requirements and needs. This effective choice would, in turn, help in benefitting the business largely.
Breeze Telecom is a valuable source that has SaaS, IaaS, and PaaS providers. We have experts in AWS, Azure and O365 in addition to a cloud team that could help and guide you regarding the proper use of cloud services.
Breeze-Telecom connects #IT solutions| #FiberIOptic #Internet| #SDWAN| #VoIP| #Cloud #tech| #MSP| #BigData| #IP| #Datacenter| #SaaS| #PaaS| #UCaaS| #IaaS
36 
36 
36 
Breeze-Telecom connects #IT solutions| #FiberIOptic #Internet| #SDWAN| #VoIP| #Cloud #tech| #MSP| #BigData| #IP| #Datacenter| #SaaS| #PaaS| #UCaaS| #IaaS
"
https://medium.com/@malik_saifullah/what-is-cloud-computing-stack-saas-paas-iaas-whats-the-difference-and-how-to-choose-f8bd5d020433?source=search_post---------222,"Sign in
There are currently no responses for this story.
Be the first to respond.
Malik Saifullah
Jul 10, 2019·3 min read
Cloud Computing is often described as a stack, as a response to the broad range of services built on top of one another under the moniker “Cloud”. The generally accepted definition of Cloud Computing comes from the National Institute of Standards and Technology (NIST), essentially says that; Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. What this means in plain terms is the ability for end users to utilize parts of bulk resources and that these resources can be acquired quickly and easily. NIST also offers up several characteristics that it sees as essential for a service to be considered “Cloud”. These characteristics include;
More than putting and solving puzzles around categorization, we believe that in order to maximize the benefits that Cloud Computing brings, a solution needs to demonstrate these particular characteristics. This is especially true since in recent years there has been a move by traditional software vendors to market solutions as “Cloud Computing” which are generally accepted to not fall within the definition of true Cloud Computing, a practice known as “Cloud-Washing.” The diagram below depicts the Cloud Computing stack — it shows three distinct categories within Cloud Computing: Software as a Service, Platform as a Service and Infrastructure as a Service and the composition of each category:
In the IaaS model, self-managed cloud deployment of virtual machines gives you more control of your infrastructure while leaving the core hardware and network management to the provider. With your control comes greater responsibility for maintaining the virtual machines, including operating system updates and security.
With PaaS, a cloud-based managed service minimizes the complexities of live streaming by offloading most of the configuration and management to third-party specialists, so you don’t have to make big investments in resources and expertise to get started. However, what it offers in scalability it sometimes lacks in control.
As your needs evolve, you may want to work with a cloud-based service that provides advanced features or a REST API for fine-grained control, plus is agile about adding new capabilities and keeping your streaming platform future-proof.
Alternatively, at that point, you could deploy an IaaS-based solution for the highest level of cloud infrastructure control.
Finally, with SaaS, a vendor usually provides the player on a hosted page and abstracts all of the underlying processing and delivery components. In this case, you would simply point your viewers to that hosted web page or embed the player on your own site.
Simple being, Ambitious, Realist, Careerist Inquisitive learner, Science Lover, Web developer. https://maliksaifullah.me
1 
1 
1 
Simple being, Ambitious, Realist, Careerist Inquisitive learner, Science Lover, Web developer. https://maliksaifullah.me
"
https://rodolfofadino.com.br/windows-azure-paas-e-iaas-minha-vida-tech-day-5e05c9304b69?source=search_post---------258,"Conheça nesta apresentação os principais recursos oferecidos no Windows Azure em dois principais modelos: Platform as a service (PaaS) e Infrastructure as a service (IaaS). E aprenda a como utilizar os benefícios de cada cenário em seus projetos.
Windows Azure: PaaS e IaaS from Rodolfo Fadino Junior
/* LIFE RUNS ON CODE */
Written by
Software Engineer and Solution Architect, Tribe Leader at Vitat
/* LIFE RUNS ON CODE */
Written by
Software Engineer and Solution Architect, Tribe Leader at Vitat
/* LIFE RUNS ON CODE */
"
https://medium.com/valtech-digital-workplaces/understanding-cloud-platform-services-iaas-paas-saas-54217f0a7fb2?source=search_post---------201,"There are currently no responses for this story.
Be the first to respond.
Cloud computing and platform discussions are at the top of any enterprise conversation today. We give you a brief look at each of the three main platforms: SaaS, PaaS and IaaS.
Cloud platform services have evolved in the past couple of years and are now available in a number of different flavours, each with their own advantages and disadvantages for enabling cloud services. Our main concern is how the platforms are able to cater to the needs of the Enterprise, so we’ll take a look at each of the three available services: Software as a Service, Platform as a Service and Infrastructure as a Service and give you a good, quick introduction to what each has to offer. If you’ve been tasked with implementing one or any of these services and are finding yourself stuck- give us a call.
Software as a Service (SaaS) offers users the ability to access software hosted by a provider, over the internet through a web browser. With this delivery model, users need not worry about installation, setup or upgrades as everything runs within the browser. Companies that provide this type of service include Google, with their Google Docs productivity suite, Microsoft and their Office 365 offering, Salesforce CRM and Quickbooks, to name but a few. Corporate users pay a low monthly service fee to access these services and get the added benefit of being able to access them from anywhere, on any device. SaaS is becoming THE delivery model of choice for accounting, collaboration, HR and customer relationship management within the enterprise.
Often confused as being somewhere between a SaaS and a IaaS offering, Platform as a Service eliminates the need for organizations to maintain hardware and software infrastructures for their in-house applications. The biggest benefit to the enterprise is that development teams (local or remote) can efficiently build, test and deploy their custom applications on a common framework without the need to invest in layers of hardware and software. PaaS providers deliver the platform and the service to support software development. Examples of PaaS providers include Microsoft Azure, Amazon Web Services, Google App Engine, Bungee and Heroku.
Providers of Infrastructure as a Service rent out their IT resources like hardware (physical and virtual), networking and storage space. Businesses needing to scale their IT environment see huge benefits to having a IaaS option as part of their IT strategy. It allows them to scale out their resources on temporary development projects or add additional disaster recovery options to their existing plans all at a reasonable cost. Users also have greater autonomy in what is installed on their rented equipment. The pricing model is typically a fee based on the quantity of resources used and how long they’re in use. Amazon Web Services was the first out of the gate in this cloud offering, however players such as Terremark, Savvis, Rackspace, GoGrid and Dell have also made a name for themselves.
Which cloud approach have you found to be the best fit for your organization?
This post first appeared on the Nonlinear Enterprise Blog.
Helping organizations rethink the intranet and become more…
1 
1 clap
1 
Helping organizations rethink the intranet and become more collaborative and competitive with digital workplace strategies and solutions.
Written by
Valtech is a full-service digital agency. Our staff of 2,500 operates from 36 offices around the world.
Helping organizations rethink the intranet and become more collaborative and competitive with digital workplace strategies and solutions.
"
https://medium.datadriveninvestor.com/cloud-machine-learning-business-growth-dd9ec50d9fe5?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
IaaS (Infrastructure as a service) solutions allow you to scale your compute environment to match your demands in terms of CPU, memory, and storage requirements. You only need to pay for resources required. The also give you the ability to easily distribute your resources across geographic regions.
This approach is much easier and affordable than building your own servers and upgrading them when they became too slow.
www.datadriveninvestor.com
One of the advantages of creating CML(Classic Machine Learning) solutions compared to DL(Deep Learning) is that they require for less data and CPU resources. This generally enables you to create solutions entirely on the desktop. However, you should not overlook the cloud. The cloud providers continuously improve their ML(Machine Learning) offerings. Today they provide an amazing array of services and APIs that make it easier than ever for developers who do not have prior ML experience to create and deploy ML solutions,for many considerations :
Local resource availability:
Do you have a local desktop machine or server that can process large data sets and build ML models ? Local processing allows you to retain control of your data and avoid cloud usage fees.
Deep learning :
Deep learning projects tend to favor cloud-based architectures because of their reliance on larger data sets and high computational requirements for model creation.
Geographic diversity:
The cloud providers can allow you to spin up resources in a variety of countries and regions globally. It is advantageous to place resources as close as possible to users.
Data size:
Do you have a data set size that is manageable on the desktop, as if often the case for CML projects ?
Scalability:
Do you anticipate your data or storage requirements will grow in the future ? Cloud providers offer much better scalability. Adding cloud resources is much easier than upgrading or purchasing a more powerful desktop/server.
Time constraints:
Is model creation time important? Even for CML projects with modest to large data sets, creating the model on the desktop or server single CPU could take minutes to hours. Moving these computation-intensive operations to the cloud could drastically cut your model creation times.If you need real-time or near real-time creation times, the cloud is your only option.
Availability:
Do you require high availability ? Your project can benefit from the distributed, multi-node architectures provided by all of the cloud providers.
Security considerations:
If you operate your own Internet-connected server, you know what a challenge security is. Cloud providers simplify security because you can leverage their massive infrastructure.
Privacy considerations:
Your clients might not want their data on a public cloud network managed by one of the big four providers. In this case, you can implement a private cloud solution and charge a premium.
Even if you decide against using a cloud provider for your project, it is important to keep an eye on their product offerings. The services are constantly being updated, and your decision may change based on those updates,
Due to fierce competition among the largest cloud providers, the cost of cloud resources today is largely identical across platforms. The big four are keenly aware of their competitor’s offerings, and pricing arbitrage opportunities no longer exist.Cloud ML services are not free. Regardless of the type of container or virtualization technology they use, dedicated or shared hardware (CPU, memory,storage) is required at some point. Each provider typically has a free trial so you can experiment with the service before buying.
The goal of building a model is to utilize it to make predictions. AWS ML allows for real-time, single, or batch predictions. Batch predictions are particularly useful, allowing you to load many instances to classify as a batch. AWS ML accomplishes this by letting you load the batch predictions into an S3 storage bucket, in exactly the same way you loaded the original dataset.
You then just need to specify the S3 location of the batch predictions and then the model will produce the results. Making batch predictions does have an incremental cost.
AWS SageMaker is a fully managed platform to help you build DL models. It is one of the recently added AWS services. The main idea behind SageMaker is that ML has been difficult for developers for the following reasons:
SageMaker tries to address these issues. It promises to remove complexity and overcome the barriers that slow down developers like all AWS services, there is extensive online documentation to help you understand the service. The link for the SageMaker developer guide is https://docs.aws.amazon.com/sagemaker/latest/dg.
SageMaker has a lot of potential. Two particularly important features make it a powerful way to implement ML on AWS- notebook instances, and its flexible support for algorithms.
The SageMaker notebook instance is a compute instance running the Jupyter Notebook App. Jupyter is an open source web app that runs on Python (hence its spelling) and allows you to create and share documents that contain live code and visualizations. It is very popular in the Python and DL realms.
The other interesting feature of SageMaker is its algorithm flexibility. SageMaker supports two classes of algorithms: built-in algorithms and bring-your-own algorithms.
The list of built-in algorithms is available at https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html.The algorithm list is very complete. AWS claims the preinstalled algorithms deliver 10 times the performance of other providers due to optimization. That’s an impressive claim. However, AWS does not offer details on how they do this, or for which algorithms it applies.
Finally,user can bring their own algorithms or frameworks. The SageMaker examples on GitHub show how to do this for a variety of models and algorithms including XGBoost, k-means, R, scikit, MXNet, and TensorFlow.
empowerment through data, knowledge, and expertise.
83 
83 claps
83 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
I am a software engineer and entrepreneur. My focus is on Developing technical skills,Learning marketing,and taking care of the health.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@primexcode/what-s-the-difference-between-saas-paas-iaas-5c7fc2697a60?source=search_post---------208,"Sign in
There are currently no responses for this story.
Be the first to respond.
PRIMEXCODE
Jan 14, 2016·3 min read
When your business has made the decision to consider cloud services for your application or infrastructure deployment, it’s important that you grasp the fundamental differences between the core categories of cloud services available.
The cloud is a very broad concept, and it covers just about every possible sort of online service, but when businesses refer to cloud procurement, there are usually three models of cloud service under consideration, Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Each has its own intricacies and hybrid cloud models, but today we’re going to help you develop an understanding of the high-level differences between SaaS, PaaS, and IaaS.
In some ways, SaaS is very similar to the old thin-client model of software provision, where clients, in this case usually web browsers, provide the point of access to software running on servers. SaaS is the most familiar form of cloud service for consumers. SaaS moves the task of managing software and its deployment to third-party services. Among the most familiar SaaS applications for business are customer relationship management applications like Salesforce, productivity software suites like Google Apps, and storage solutions brothers like Box and Dropbox.
Use of SaaS applications tends to reduce the cost of software ownership by removing the need for technical staff to manage install, manage, and upgrade software, as well as reduce the cost of licensing software. SaaS applications are usually provided on a subscription model.
PaaS functions at a lower level than SaaS, typically providing a platform on which software can be developed and deployed. PaaS providers abstract much of the work of dealing with servers and give clients an environment in which the operating system and server software, as well as the underlying server hardware and network infrastructure are taken care of, leaving users free to focus on the business side of scalability, and the application development of their product or service.
As with most cloud services, PaaS is built on top of virtualization technology. Businesses can requisition resources as they need them, scaling as demand grows, rather than investing in hardware with redundant resources.
Examples of PaaS providers include Heroku, Google App Engine, and Red Hat’s OpenShift.
Moving down the stack, we get to the fundamental building blocks for cloud services. IaaS is comprised of highly automated and scalable compute resources, complemented by cloud storage and network capability which can be self-provisioned, metered, and available on-demand.
IaaS providers offer these cloud servers and their associated resources via dashboard and/or API. IaaS clients have direct access to their servers and storage, just as they would with traditional servers but gain access to a much higher order of scalability. Users of IaaS can outsource and build a “virtual data center” in the cloud and have access to many of the same technologies and resource capabilities of a traditional data center without having to invest in capacity planning or the physical maintenance and management of it.
IaaS is the most flexible cloud computing model and allows for automated deployment of servers, processing power, storage, and networking. IaaS clients have true control over their infrastructure than users of PaaS or SaaS services. The main uses of IaaS include the actual development and deployment of PaaS, SaaS, and web-scale applications.
There are a lot of providers offering Infrastructure as a Service such as CloudSigma, HPCloud, and Softlayer reach with their own unique value proposition and service portfolio to choose from.
ComputeNext provides a brokerage service for IaaS, so that you can be sure you’re choosing the right IaaS provider for your application needs. With normalized access to over 20 cloud providers from a single API you can compare price and performance across providers to find the best fit — and then build and deploy without getting locked in to just one platform.
Design | Development
1 
1 
1 
Design | Development
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kirk.broadhurst/iaas-v-paas-is-the-new-build-v-buy-b58ea24cecba?source=search_post---------269,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kirk Broadhurst
Nov 1, 2018·5 min read
A decade ago we used to talk about Build v Buy. Maybe we still do. IT Managers would passionately argue about the benefits of these approaches. Battles would won and lost. People held deep seated beliefs in the benefits of one over the other, and were frequently stubborn about accepting that both have their place.
Today we see echoes of the Build v Buy debate in a modern equivalent: IaaS v PaaS.
The traditional, can-do approach to enterprise software — we can build what we need. Development is in-house. Our needs are so unique that no existing solution will work for us. Options don’t have required features and functionality, are not customizable, or can’t be extended and integrated.
Or maybe available products are just too expensive or restrictive.
For whatever reason you believe that your best approach is to spend the effort, money and time building something custom that you’ll have to look after. The upside is that it will be exactly what you need, and you’ll avoid dreaded ‘vendor lock in’, if everything goes well.
Why recreate the wheel? Take some off-the-shelf solution or product that mostly works for you and use it to bootstrap whatever you’re doing. You literally ‘buy’ something, and it might not be perfect for what you need, but you accept this trade-off because it’s quick and simple.
People will debate this considering organization’s core competency, risk tolerance, and priorities.
And vendor lock-in. The fear of building some your core business around some 3rd party solution, and the fact that you’ll be beholden to that 3rd party’s licencing fees forever. What if the triple the price? What if they discontinue the product, or force us to upgrade? What if they get purchased by… IBM?
Today, as we develop in the cloud, this conversation has shifted to another level. Our applications are becoming products; our systems are becoming platforms. We still have Build v Buy on a micro level, but on the macro level we need to consider IaaS v PaaS.
Ignore the ‘as-a-service’ buzzword. Infrastructure means leasing servers from a cloud vendor — Amazon, Google, Microsoft. You’ll generally pay by time used. And you’ll just get a server on which you can run whatever you want.
What do you want to run on that server? Well, you can build some software. Maybe you want — an API that can power a website — a database to store product information — an ETL process and data pipeline
Just build whatever you want, and deploy it! This is extremely flexible — you can literally do anything. But it takes time, and you’ll be supporting that software.
This is the modern equivalent of Build.
A platform — in the PaaS sense — is a set of components that provide some out-of-the-box functionality upon which you can build something. These are products in their own right, but you would use them to build your own products. A cloud vendor provides some of the basic plumbing so that you can get your solution up and running rapidly.
You see, cloud vendors are in a race to deliver ‘value add’ over simple infrastructure. This is a huge differentiator because it allows developers to get stuff done quickly.
This is the modern equivalent of Buy.
Many of the same Build v Buy arguments appear here, too.
But there are some crucial differences to the old argument — all relating to the scale at which the cloud vendors operate.
Cloud vendors operate at a scale that no software vendor ever has (perhaps outside of Microsoft), and software has no marginal cost. The platforms that they offer have very little markup over and above the base infrastructure — some are almost free. These platforms are differentiators, and add revenue via increased user base rather than increased per-user-revenue. You almost certainly can’t build it for less than it costs to buy.
These platforms are so ubiquitous that they form their own communities, expertise, and 3rd party support. These are not niche products, and it’s (relatively) easy to hire people who already have experience using them. If you have a problem, someone will have already had that problem and posted it on StackOverflow. There are large teams supporting these products, and they are (relatively) well documented.
Again, at the operating scale all of these platforms are continually improving. Run a software team that will deliver features on the cadence of the cloud vendor’s platform would be a very expensive operation.
Aha!
People have been burnt over the years with vendor lock-in. Whether it’s Oracle (the classic example), Microsoft licencing, big data appliances (Teradata or Netezza), or some industry niche solution, this is a bug bear for IT Managers. It’s a huge drain on the bottom line. These systems have big upfront and recurring costs, and often don’t even deliver improvements without another big outlay.
I understand that managers are skeptical of platform lock-in. It’s a valid concern. I build all my APIs in AWS’ API Gateway, and now I’m stuck — the migration effort would be enormous, I’d have to re-implement everything.
This is fear based decision making.
Why? I’ll address this in my next post.
Originally published at xku6.com on November 1, 2018.
A humble yet brilliant guy.
A humble yet brilliant guy.
"
https://medium.com/@sahityamaruvada/setting-up-aws-network-load-balancer-with-terraform-0-12-b87e75992949?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sahitya Maruvada
Aug 27, 2019·7 min read
AWS has a huge documentation base, linking text to a lot of content, giving information about each of the resources using CLI, console and so on which makes it possible to miss the important things in the huge text blobs. Not that I am complaining, they really have great documentation, it's just that there a lot of information out there to process!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mit-security-seminar/cdn-on-demand-affordable-ddos-defense-using-untrusted-iaas-clouds-a6da5fc55e3f?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
Yossi Gilad came to give a talk at the MIT on using untrusted IaaS clouds to prevent DDoS attacks, which was published in NDSS 2016. I will outline some keys points in his talk, but for more information, I refer you to his paper.
Many websites use content delivery networks (CDNs) as a form of denial of service (DoS) defense. They distribute content across multiple, geo-distributed proxies. CDNs allow for a high-bandwidth, distributed, and scalable infrastructure, but there are still a few problems.
First is cost. It is expensive for CDNs to provide continuous, full service. Second is key management. For HTTPS sites, the website has to provide its secret key to the CDN, but a CDN might be compromised or malicious. Finally, there is a tradeoff between cost and trust. More trusted CDNs, like Akamai and Amazon, are more expensive than less trusted CDNs, like CDN77. The question is if they can build a secure, low-cost CDN-based DoS defense.
The goals are to build a CDN system built on multiple low-cost IaaS clouds, and deploy only when and where they are needed. The system should have object level security and does not have to share its keys with the CDN. Finally, they want this to be a software package rather than third-party service (open source).
The key idea is the use of clientless authenticated objects. They store static “authenticated objects” on untrusted proxies. This way, the website does not have to share private keys and complements the TLS network level protection. However, this avoids changes to the client but allows for flexibility in an “on-demand” system because it allows for cheaper, less trusted crowds and allows the system to switch between clouds.
What about private content? The website needs to establish a user key on authentication and use the content origin as an authentication oracle. They store private web objects encrypted, and each object is associated with a unique symmetric key. They also have a loss-resilient tunnel that tunnels packets between content-origin and proxies over UDP that uses network coding to ensure delivery even in situations with high packet loss.
Here is asummary of their results:
For more details about their results and other components of the system, I refer you to their paper. This is interesting work that allows web services to deploy their website on CDNs more cheaply because they do not have to pay a premium for using a less trusted CDN.
Summary of talks from the MIT security seminar
1 
1 clap
1 
Summary of talks from the MIT security seminar
Written by
Investor at Dell Technologies Capital, MIT Ph.D in computer security and Stanford undergrad, @cybersecfactory founder, former @roughdraftvc
Summary of talks from the MIT security seminar
"
https://medium.com/@krmarko/bimodal-it-doesn-t-mean-bipolar-organizations-the-path-to-it-transformation-621cb5cc85a7?source=search_post---------136,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Jul 8, 2015·2 min read
Never underestimate a buzzword’s power to frame the discussion. As I recently discussed, the term bimodal IT has captured the imagination and polemical energy of technology commentators and like many IT discussions in the age of 140-character commentary, it often degenerates into polarized, all-or-nothing positions. Using a variant of the classic reductio ad absurdum strategy, critiques of bimodal IT characterize it as a path to bipolarIT, a separate, but unequal partitioning. In the dynamic mode 2 corner we have hip, swashbuckling cloud gurus mashing together exciting new applications out of myriad cloud services, while in the dingy mode 1 corner we have the conservative old guard serving out their golden years by tending to legacy systems that (ideally) hum along until both they and their caretakers, like all good soldiers, just fade away. While it makes good rhetoric, this isn’t the way successful IT organizations navigate foundational change agents like the cloud.
As the column details, the bimodal model helps to focus IT attention on three areas of transformation:
Bimodal partitioning allows IT to locally optimize placement of people, processes and investments for services with very different requirements.
But the Story Continues…here.
Originally published at www.forbes.com on July 7, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
"
https://medium.com/@acantelys/from-the-physical-server-to-the-iaas-the-cloud-is-already-among-us-cf315ff91cf4?source=search_post---------266,"Sign in
There are currently no responses for this story.
Be the first to respond.
Acantelys & Juan Martinez
Oct 31, 2018·4 min read
I remember those days when, as a good “Computer doctor”, I had to take care of the health of my equipment, did I have no memory? then to put hand and change the modules; how many of us planned that dreamed trip and from one moment to another we listened to the cell phone with the HelpDesk call saying, with the coldness that characterized them, “there is no space on the server, you have to go back”; between anxiety and terror we thought it was just a simple click, we had our bags ready and we should not miss the beach and the sun.
These were times in which our biggest nightmare was to dream of “red LEDs”, famous indicators that something was wrong, I remember one day when the air conditioning in the telecommunications room failed, and I had to install electric fans pointing directly at ice sacks , what an anecdote, if you are an administrator of insurance you will be smiling at this moment. Always “the show should continue”, and services at any cost, should be running, that was our motto.
How things have changed.
Now, this seems a distant memory, we entered the cloud computing [1], behind were those days when we suffered attacks of solitude in front of those little colored lights of the routers and devices that were our only company for hours. Infrastructure as a service (IaaS), brings us a whole range of functions and completely automated tasks, completely well executed, which as a character of García Márquez would say, “seem to come from the last alchemists and wise of Babylon”.
From now on we will forget forever to buy hardware, increase disk or memory, think of the cooling of equipment is a thing of the past, the cloud will be able to grow in parallel according to our needs and paying only what we consume.
Network connections and bandwidth are guaranteed, say goodbye to emergency calls because the server is down. Any wireless device, from a simple Tablet with Wi-Fi, to an innovative 4G phone, can enjoy the services provided by its IT department. Even the videoconference that you always dreamed of without drops for users or your streaming service with the latest viral video of your company, will not be affected, the cloud will take care of everything.
The Data in the cloud.
Are those people who care about the integrity of their data ?, Are you afraid that your servers can not handle the workload for the success of your business? Do not sleep for the backup and recovery of your information in case of disasters ?; I have an answer for each of your questions, and maybe, be a little surprised.
Your data is here, and everywhere; you access it, manage it, control it from a panel, but it is distributed in data centers that can be anywhere in the world, even in several simultaneously; this ensures the ubiquitous permanence of your information; but like Aladdin, you should only rub the wonderful lamp of cloud computing so that your data appear majestic and up-to-date, ready to be devoured by your applications and clients.
If the volume of your business grows and your server is one step away from saying I can not do it anymore, do not worry, because automatically, all the resources you need for that “sudden” growth will be obtained as if by magic (more memory, CPUs or bandwidth). In the most serious cases, where due to lack of expertise, sabotage or terrorism your corporate headquarters suffers catastrophic damages, you can at least have a support, a friendly voice in those critical moments, that will support you, and that will tell you, “your data is except “, since being in the cloud will guarantee the digital continuity of your business.
The way to follow.
Perhaps, it is no longer the simple fact of “saving infrastructure costs”, that this is true is enormous, but that our company suffers the real risk of being less competitive, less prepared for the future, unable to take the changes come, victim of the syndrome that some of our grandparents suffered “if things work well, you do not have to change them”.
Market leaders: Microsoft Azure, Google Cloud, AWS, Open Cloud, provide a diversity of services by contracting infrastructure in the cloud: from private clouds to fully virtualized datacenters in the cloud, your competition must already be aware of all these changes, The question is: are you?
[1] Cloud computing, also known as cloud services, cloud computing, cloud concepts or simply “the cloud”, is a paradigm that allows services to be offered. of computing through a network, which is usually the Internet.
Guest speaker and Committee member at 20+ conferences and World Congresses of technology and Education; Cloud Computing Evangelist and a good friend too.
Guest speaker and Committee member at 20+ conferences and World Congresses of technology and Education; Cloud Computing Evangelist and a good friend too.
"
https://medium.com/@nivertech/out-of-gafam-google-apple-facebook-amazon-microsoft-only-apple-and-facebook-do-not-have-ecab174a6cf1?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zvi Avraham
Sep 29, 2016·1 min read
Simone Brunozzi
Out of GAFAM (Google, Apple, Facebook, Amazon, Microsoft), only Apple and Facebook do not have their own public IaaS clouds. Apple has iCloud, which just a reseller of AWS S3, Azure Blob Storage and Google Cloud Storage.
Samsung acquired Joyent (very advanced IaaS), Oracle built cloud for Enterprises and Cloudera trying to convince Intel to back a new public cloud built specifically for Big Data workloads.
Cryptoeconomics & Distributed Systems
Cryptoeconomics & Distributed Systems
"
https://faun.pub/how-i-build-my-own-private-cloud-91a8c8dc0ad9?source=search_post---------311,"There are currently no responses for this story.
Be the first to respond.
In this blog post, I would like to share my experience in building an on-premise private cloud for Infrastructure as a Service (IaaS) with largely open source components and some microservices written in javascript and python to provide user experiences similar to public cloud as much as I can.
"
https://medium.com/@benwang_2362/the-difference-between-iaas-paas-baas-and-saas-91133d728917?source=search_post---------149,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Wang
Nov 10, 2016·2 min read
It’s often confusing in the cloud computing world of all the x-aas terminologies. Lets get familiar with some of these terms and what they mean for cloud computing.
Iaas is the simplest cloud offering. It is the most close to the “metal” out of the terms. Iaas stands for Infrastructure as a service. Iaas sells products analogous to hardware like VMs, storage and data management servers. The most famous Amazon Web Service (AWS) offering in Iaas is EC2 (Elastic computing 2), S3 (Simple Storage Service), and RDS (Relational Database Service). Each of these products are charged by the hour.
Paas stands for Platform as a service. An AWS Paas service is Elastic Beanstalk. Paas gives developers some tools to manage their application such as CDNs, load balancing, easy deployment and testing. However, Paas still requires developers to build and setup the database, the business logic and the front-end client facing view.
Baas is Backend as a service. An example of a backend as a service provider is EasyDataManager (EDM). Baas’s offerings are basically plug and play services using APIs to a web or mobile developer’s application. EDM offers Email Services, and traditional mobile services such as User Managerment, and Reporting. Baas allows developers to solely focus on their front-end and user experience. It is made for start-ups to get an app up and running in no time.
Lastly, there is Saas, Software as a service. The most famous Saas right now is probably Slack. Slack is a software on the web, and mobile that offers a chat service along with file and chat room management features.
Now you understand the confusing language of the cloud world. you can explore more and understand how enterprise business fit into these 4-aas categories. They all are tools to enable developers to create better tech businesses. Knowing how much of the stack you want to manage is an important step in finding the right -aas for you.
82 
1
82 claps
82 
1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@JohnHornbostel/the-iaas-2017-leadership-conference-a5ec40fc7e86?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Hornbostel
Mar 29, 2017·1 min read
Since 2015, John Hornbostel has served Franklin Templeton Investments as an associate general counsel. During his time with the firm, he was lead lawyer for a project that saw the company launch a new line of international multi-asset investment products. Further, John Hornbostel is a member of the Investment Advisor Association (IAA).
An organization providing representation to SEC-registered investment advisory firms, the IAA holds its Leadership Conference each year. The event brings together executives from numerous investment advisory firms to exchange ideas, develop connections, and learn about industry trends. Further, attendees can benchmark their businesses and create plans for growth based on what they learn during the event.
During the course of the conference, participants will hear keynote speeches from industry leaders and business experts on topics such as economics, business strategy, and politics. The conference also hosts facilitated “breakout” sessions, where attendees can focus their discussions on issues within the investment industry.
The IAA’s 2017 Leadership Conference will take place October 4th through the 6th at the InterContinental Chicago Magnificent Mile in Chicago, Illinois.
Based in New York City, John Hornbostel is Associate General Counsel with Franklin Templeton Investments.
1 
1 clap
1 
Based in New York City, John Hornbostel is Associate General Counsel with Franklin Templeton Investments.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rstarmer/the-battle-of-the-switches-ovs-vs-linux-bridge-or-simplicity-rules-7e019d7d952b?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Robert Starmer
May 28, 2016·4 min read
I’ve gotta come right out and say it: Open Virtual Switch– or OVS– is the wrong switch for Infrastructure-as-a-Service (IaaS).
That’s not to say it’s the wrong switch for virtual, or even as a base layer integration for Software Defined Networks…but it is absolutely the wrong solution for IaaS, whether it be for a managed service provider, an enterprise, or a university.
Open Virtual Switch was initially conceived in a university environment, with a flow-based model providing the development primitive, and a central controller determining what those flows actually looked like. The use of OVS as a virtual switch in the IaaS market (and to be specific, in the OpenStack IaaS market) came about because the resources who supported OVS and its enterprise equivalent understandably wanted to ensure that there was an open model for integrating their services into OpenStack. Along the way, numerous decisions were made that eventually elevated OVS into the “principal” position of virtual switch. Unfortunately, along with this promotion of status came expectations that OVS would provide the best of all possible switching worlds when the anticipated era of the Software-Defined Networking (SDN) took over. The premature anticipation of SDN ubiquity in cloud made OVS a seemingly reasonable bet as “the” switching solution.
Anticipation of SDN ubiquity in cloud made OVS a seemingly reasonable bet Click To Tweet
Now, it’s quite possible that if SDN had become the defacto standard for Cloud, the OVS-as-King solution would have made perfect sense. But the complexity of OVS had people longing for the simpler days, the days where they just had a simple bridging technology like Linux Bridge to support their Cloud solutions.
So why not Linux Bridge? It has been an alternate switch technology even before OVS was conceived and provides a simple model for interacting with the virtual forwarding layer in the Linux kernel. It has been around since nearly before users realized that it was possible to get a Linux machine to connect two or more physical network port together. Truisms exist for a reason, and one of the truisms in the Cloud systems space is that simple always wins over complicated. Hence, Linux Bridge, although not the new kid on the block, or the newest technology solution, may well become the winner in the Battle of the Virtual Cloud Switches. Linux Bridge, being older and simpler, may have made OVS initially more attractive.
Linux Bridge, being older and simpler, may have made OVS initially more attractive. Click To Tweet
But the champions for the OVS model weren’t willing to capitulate without a fight. OVS proponents pointed out that Linux Bridge lacked a scaleable tunneling model. True, Linux Bridge supported GRE Tunnels, but not the newer and more scalable VXLAN model. Eventually, the titans of networking became involved in the arguments, and it is therefore probably not too surprising that a complex solution was deemed better than a simple one.
But let’s remember that truism: simple wins over complicated. Having been through the convolutions required by OVS, large scale production environments are now increasingly moving over to Linux Bridge. OVS just comes with too much complexity and renders issues in the network domain a royal pain in the rear to manage. Certainly there have been changes to Linux Bridge that have also helped close the gap between projects using OVS and Linux Bridge, including the addition of VXLAN support as a tunnel technology. But in the greater scheme, it’s the simplicity of Linux Bridge that wins the day.
In production, simple wins over complicated; Linux Bridge is increasingly the Cloud switch of choice.
In production, simple wins over complicated; Linux Bridge is the #cloud switch of choice. Click To Tweet
Given that a principal tenet of an “as a service” environment is pooled, shared and elastic resources–which means some level of homogeneity, plus a degree of simplification of the resource being modeled and turned into a service–it’s not surprising that a simple network technology makes more sense than a complex one. And with the introduction of technologies that remove an entire layer of the forwarding domain (bypassing L2 segregation to focus on L3 segregation exclusively), we’ll likely see yet another shift at the simplification process that has been the driving engine behind IaaS scale and enablement.
For now, deploy your IaaS tools with a simple and functional L2 switching layer, and leverage Linux Bridge with the understanding of its simple model for services that meets much of the IaaS market’s needs. No need to complicate things. After all, as Leonardo da Vinci said, “Simplicity is the ultimate sophistication.”
You can find more from me and the team at Kumulus Technologies at https://kumul.us/blog or our short format cloud technology videos at https://youtube.com/fiveminutesofcloud
Originally published at Kumulus Technologies — kumul.us.
@rstarmer, OpenStacker, Cloudifier, Containerizer, @kumulustech Founder, Coffee Snob
11 
11 
11 
@rstarmer, OpenStacker, Cloudifier, Containerizer, @kumulustech Founder, Coffee Snob
"
https://medium.com/@enginunal/bulut-mimarisinin-temelleri-111489c61fd7?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Engin UNAL
Jan 31, 2018·7 min read
Bulut servislerinin kullanıcıya özel olarak sunulmasıdır. Sunulan kaynaklar diğer kullanıcılarla paylaşılmaz, kullanıcı kendisine sunulan bulut servislerinin ölçeklenebilirlik, elastiklik gibi özelliklerini kullanırken kontrol ve uyarlanabilirlik özelliklerine de sahip olur. Özel bulut servisleri IaaS ve Paas modeli ile sağlanabilir. Iaas, Servis Olarak Altyapı(ağ,depolama..) ve PaaS, Servis Olarak Platform(işletim sistemi, web sunucusu, veritabanı sunucusu…) anlamına gelmektedir.Dahili veya kurumsal bir bulut olarak da adlandırılan özel bulut, şirketlere barındırılan bilgisayar altyapısı üzerinden özelleştirilmiş kaynaklardan sağlanan ek kontrol ve özelleştirme sayesinde, işletmelerin self-servis, ölçeklenebilirlik(scalability) ve elastikiyet(elasticity) gibi genel bir bulutun pek çok avantajını sağlar
Internet üzerindeki sunucular ile sağlanan ve herkese açık olan bulut hizmetidir. Genel bulut, bir hizmet sağlayıcının sanal makineler (VM’ler), uygulamalar veya depolama gibi kaynakları, internet üzerinden herkese açık hale getirdiği standart bulut bilişim modeline dayanır.Sunulan hizmetler kullanıcılara ücretsiz veya ücret karşılığı sağlanabilir. Kullandığın kadar öde modeli de uygulanmaktadır. Özel bulutların aksine, genel bulutlar firmaları şirket içi donanım ve uygulama altyapısı satın almak, yönetmek ve bakım yapmak zorunda kalmaktan kurtarabilir — bulut hizmeti sağlayıcısı sistemin tüm yönetim ve bakımından sorumlu tutulur.
Melez bulut, genel bulut ile özel bulut arasında veri ve uygulamaların paylaşılmasını sağlayarak bulut ortamını entegre eder. En kısa ifadeyle, kurum içi kaynakların gelen bulut kaynaklarıyla bütünleştirilmesidir.Genellikle, melez bulut, bir genel bulut hizmetinin ve kurum içi bir özel bulutun kombinasyonunu ifade eder; Bununla birlikte, melez bulutlar, farklı sağlayıcılar tarafından sağlanan genel bulutlardan veya bulut ile geleneksel IT’nin bir kombinasyonundan oluşabilir. Aslında, geleneksel IT altyapısında mevcut sistemlerin bir genel bulut hizmeti ile birleştirildiği bir kurulum şu anda hibrit bulutun en yaygın kullanımı halidir.Melez bulut kullanmak, yalnızca şirketlerin bilgi işlem kaynaklarını ölçeklendirmesine olanak tanımakla kalmaz aynı zamanda kısa süreli talep artışlarını karşılamak için büyük sermaye harcamaları yapma ihtiyacını ve işin daha hassas veri veya uygulamalar için yerel kaynakları ayırması gereksinimini ortadan kaldırır.
Bu konuyla ilgili aşağıdaki görsel ile başlayalım. IaaS, PaaS ve SaaS ile ilgili olan bu görsel aşağıdaki anlatım öncesi bir fikir verecektir.
Tek tek IaaS, PaaS ve SaaS maddelerine geçmeden önce konuyu basitleştirmek adına yakın zamanda okuduğum bir yazıdaki örnekle başlamak istiyorum.Tek başına, altyapı(IaaS) kullanışlı değildir — orada sadece duran ve birisinin belirli bir sorunun çözümünde üretken olmasını bekleyen bir yapıdan başka birşey değildir. Ülkedeki ulaşım sistemini düşünün. Tüm bu yollar inşa edilmiş olsa bile, insanlar ve eşyaları taşımak için araba ve kamyon olmadan kullanışlı olmazlar. Bu benzetmede, yollar altyapıya(IaaS), arabalar ve kamyonlar ise altyapının üzerine oturan ve insanları ve eşyaları taşıyan platformdur(PaaS). Eşyalar ve insanlar ise teknik alanda yazılım(SaaS) ve bilgi olarak düşünülebilir.
IaaS, bilgi işlem, depolama, ağ oluşturma ve diğer kabiliyetleri Internet üzerinden sunmanın bir yöntemidir. IaaS, şirketlerin temel bulut altyapısını satın almak, yönetmek ve desteklemek zorunda kalmadan web tabanlı işletim sistemlerini, uygulamaları ve depolamayı kullanmalarını sağlar. IaaS platformlarının en popüler örnekleri Amazon Web Hizmetleri (AWS) ve Microsoft Azure’dur. Kullanıcılar, genel ortam için bir IT operasyon yönetimi konsolu olarak hizmet veren Web tabanlı bir grafik kullanıcı arayüzü kullanarak, bu altyapıyı kendi başlarına yönetebilirler. Altyapıya API erişimi bir seçenek olarak sunulabilir.En kısa ifadeyle IaaS, kullanıcılara depolama, ağ oluşturma, sunuculara ve buluttaki diğer bilgi işlem kaynaklarına ücret karşılığı erişim sağlayan servistir.Kullanıcı, kendi ihtiyaçları doğrultusunda CPU, RAM ve Depolama miktarını belirler ve bu özellikleri sağlayan bir VM(virtual machine)’i servis sağlayıcıdan satın alır. Bu VM alındıktan sonraki adımlarda işletim sistemi bakımı ve yönetimi, servis konfigürasyonu kullanıcı tarafından gerçekleştirilir.
PaaS, IaaS’e ek olarak yazılım, geliştirme araçları, iş zekası (BI) hizmetleri, veritabanı yönetim sistemleri gibi sistemler de içerir. Yazılım lisansları, uygulama altyapısı, geliştirme araçları ve diğer kaynakları satın alma ve yönetme zorunluluğundan kurtarır. Bu hizmetler bulut sağlayıcısı tarafından verilmektedir.PaaS, geliştiricilerin internet üzerinden uygulamalar ve hizmetler oluşturmasına izin veren bir platform ve ortam sağlar. PaaS, SaaS’tan daha düşük bir seviyede çalışır ve temel olarak yazılımın geliştirilip dağıtılabileceği bir platform sağlar. PaaS sağlayıcıları, altta yatan sunucu donanım ve ağ altyapısını bakımı ve sunucularla uğraşma zorluğunu üstlenir ve kullanıcılara sadece iş tarafına odaklanmayı sağlayan bir ortam sunar.
SaaS, yazılımın bulutta barındığı ve internet üzerinden erişildiği bir abonelik tabanlı modeli ifade eder. Basitçe web veya bir API aracılığıyla erişilen bulut tabanlı uygulamalar olarak tanımlanır.Depolama ve işleme işlemlerini bulut sunucularında gerçekleştirilir ve servise erişmek ve kullanmak için oldukça basit ve yaygın bir istemci (genellikle bir web tarayıcısı) kullanılır. Hizmet sağlayıcısı, donanım ve yazılımı yönetir ve hizmet sözleşmesi ile uygulamalarınızın ve verilerinizin kullanılabilirliğini ve güvenliğini sağlar. Web tabanlı email hizmetleri, sosyal medya, web tabanlı müzik dinleme hizmeti veren uygulamalar SaaS’e örnek olarak verilebilir.
Container, bir uygulamanın kodunu, yapılandırmalarını ve bağımlılıklarını, tutarlılık, verimlilik, üretkenlik ve sürüm denetimi için yapı taşları olarak paketleyen bir sanallaştırma(virtualization) yöntemidir. Container, yazılımın paylaşılan bir işletim sisteminde izoleli olarak çalışabilen bir biçimde paketlenmesinin bir yoludur. VM’lerin aksine, container bir işletim sistemi paketlemez; yalnızca yazılımı çalıştırmak için gereken kütüphaneler ve ayarlara ihtiyaç vardır. Bu, yazılımın nerede konuşlandırıldığına bakılmaksızın yazılımın daima aynı şekilde çalışacağını garanti eder.Container teknolojisi ile uygulamaların deploy edilmesi çok daha kolay ve VM’e göre çok çok hızlıdır. Continuous Integration gibi yazılım geliştirme süreçlerinde geliştiricilerin otomasyona bağladığı derleme ve test süreci sonrasında yayın süreci gelmektedir, yayın(deployment) sürecinin sorunsuz gerçekleşmesi çok önemlidir. Üretilen ve test edilen bir yazılımın kullanıcının deneyimine sunulma aşamasında kullanıcının işlerini kesintiye uğratmadan deployment tamamlanmalı ve bu işlemler hatasız gerçekleşmelidir. Container’lar bu alanda da önemli kolaylık getirmiştir ve kullanımları yaygınlaşmaktadır.Konteynerlar bir konteyner engine üzerinde çalışır ve bu container engine aşağıdaki işletim sisteminden bağımsız olarak çalışır, alttaki yapının VM veya fiziksel bir makina olmasının önemi yoktur. Konteynerlar için endüstri standardı Docker olarak söylenebilir. Çoğu büyük platform Docker engine desteğine sahiptir ve Docker imajlarını bu engine üzerinde çalıştırmaktadır. Bu imajlara Dockerfile ismi verilir ve Dockerfile içerisinde konfigürasyon tanımları, uygulamalar, servisler vb gerekli tüm şeyler bulunur.
Diyelim ki uygulamamız büyüyor ve tek bir blok olarak giderek daha fazla fonksiyon eklemeye devam ettik. Uygulama çok fazla CPU ve RAM tüketen ve yönetimi neredeyse imkansız bir hale gelmeye başladı. Bu durumda ne yapmak gerekir? Uygulamamızı daha küçük parçalara ayırmaya karar verir ve her biri belirli bir görev veya fonksiyon için sorumlu daha küçük bölümlere yani mikroservislere(microservices) ayırmaya karar veririz.İşte bu durumda bir ihtiyaç daha ortaya çıkıyor o da Konteyner Orkestrasyonu(Container Orchestration), konteynerların otomatik olarak düzenlenmesi, koordinasyonu ve yönetimi anlamına gelir. Bir uygulama için bir den çok container kullanılabilir ve bu container’lar arasında uyumun sağlanması için container’ların yönetilmesi, yayınlanması, konfigürasyonu işlemlerinin gerçekleştirilmesi bu şekilde olmaktadır. Örnek tool olarak Kubernetes verilebilir.
Bulut kaynaklarına erişimde kimlik yönetimi ve kullanılan hesapların koruması önemli konulardan biridir. Örneğin yönetici hesabı tüm kaynaklara, servislere erişim hakkına ve tüm kullanıcıların yetkilerini belirleme onları blok etme hakkına sahiptir ve güvenliği bu nedenle oldukça önemlidir. Bu güvenliği sağlamak için bazı noktalara dikkat etmek gerekir. Bunlar:
• Şifre güvenliğiŞifrelerin tahmin edilememesi için şifre belirlemede akılda kalan veya tahmin edilebilen şifreler yerine içinde büyük küçük harf kombinasyonu ve rakam içeren daha karmaşık şifreler kullanılmalıdır. Brute Force gibi şifre kırma ataklarına karşı güçlü şifrelerin belirlenmesi önemli bir güvenlik konusudur.
• Çok faktörlü kimlik doğrulama (Multi Factor Authentication)Hesabınıza güvenli bir şekilde erişilmesine yardımcı olmak için kullanılır ve hesaba giriş işlemini her biri farklı bir faktör kategorisinden doğrulayarak kullanıcıyı tanımlama işlemidir.
• Koşullu Erişim (Condition Access) ve Sahtekarlığı Algılama(Fraud Detection)Koşullu erişim, kullanıcılara konum, aygıt ve uygulama düzeyinde içeriğe dayalı kontroller sağlayan ilkeler tanımlaya imkan veren yöntemdir. Örneğin bir kullanıcının belirlenen IP’ler dışında başka bir IP aralığından bağlanması engellenebilir. Fraud Detection, sahtekarlık veya hile ile sisteme girmeye çalışanların algılanmasıdır. Örneğin bir kullanıcı kendi hesabına ilk girişi Türkiye,İstanbul’dan yapmış ve bu işlemden beş dakika sonra İngiltere’den giriş yapmaya çalıştığını düşünelim. Bu işlem şüpheli olarak kabul edilip Fraud Detection kuralları ile algılanıp bloklanabilir.
Yukarıda saydığım güvenlik önlemlerine yenileri de eklenebilir fakat ne kadar önlem alınırsa alınsın bu bir sistemin tamamen güvenli olduğu anlamına gelmez. Her zaman birilerinin bir yöntem veya açık bulup sisteme erişme imkanı olabileceği olasılığı göz ardı edilmemelidir.Olası sızmalara veya saldırılara maruz kalınan durumlarda sistemin işleyişinin devamına yönelik de bazı önlemler geliştirilmelidir. Örneğin bir bulut ortamında admin hesabının yaratılan tüm ortam üzerinde tam yetkisi vardır. Uygulamaların ve servislerin tek ortamda olduğunu ve bunun bir admin ile yönetildiğini varsayalım. Admin hesabı ile ilgili bir sorun yaşandığında tüm ortamın çalışamaz hale gelmesi sözkonusudur. Buna karşı uygulamaların ve servislerin farklı ortamlara dağıtılması, birbirini yedeklemesi ve bu ortamların farklı admin hesaplarıyla yönetilmesi daha güvenli olacaktır. Bu şekilde bir hesap ile sorun yaşandığında sistemin diğer ortamlardan ayağa kalkarak devamlılığının sağlanması daha sağlıklı olacaktır.
Rol tabanlı erişim kontrolü (Role Based Access Control — RBAC), kullanıcıların rollerine dayalı olarak bilgisayar veya ağ kaynaklarına erişimi düzenleyen bir yöntemdir. Örneğin bulut sisteminde bir yönetici hesabı ve veritabanı işlemleri yapan başka hesaplar olsun. Tüm bu hesaplara aynı hakları verip tüm kaynaklara erişimi açmak güvenlik riskleri oluşmasına yol açar. Bunun yerine kullanıcılara ve yöneticilere gerektiği kadar ve gereken ölçüde hak verilmeli ve bu hakların sistemde nasıl bir risk oluşturabileceği önceden analiz edilmelidir.
Diğer adıyla Servis Olarak Fonksiyonlar (Functions as a Service — FaaS). Öncelikle belirtmek gerekiyor ki bu serverless isimlendirmesi kullanılmasına bakılmaması gerekiyor, çünkü bu konu tamamen serverlar üzerinde geçiyor ve onlara bağımlı. Bu nedenle başlığın verdiği önyargıyı yazının başında kırmak önemli. Serverless programming bulut servis sağlayıcısının sunduğu fonksiyonlar ile gerçekleştiriliyor.
Önemli üç nokta var: • Sunucuların geliştiriciden tamamen soyutlanması, yani geliştiriciler sunucu ile ilgili herhangi bir bilgiye sahip olmak zorunda değil. Kullanılan altyapı için bir ücretlendirme yok.• Tüketim ve çalıştırma temelli faturalandırma, sunucunun alanı,performansı fiyatta belirleyici değil. Fiyat ne kadar çalıştırıldığı ile ilgili ücretlendirme var.• Olay güdümlü ve anında ölçeklenebilir olan servisler. Bulut platformunun sunduğu fonksiyonlar, API ve hizmetler kullanılıyor. Kodlama olay güdümlü ve stateless yazılıyor. Yazılan kodun verimi ücretlendirmeyi de doğal olarak etkiliyor.
Olay güdümlü sistemler için yeni bir mimari kalıptan bahsediyoruz. Bu nedenle, sunucu içermeyen işlevler genellikle diğer hizmetler arasında bağlayıcı olarak veya olay odaklı bir mimaride kullanılır. Kısa ömürlüdür, durum bilgisi tutmaz, mevcut hizmetlerinizden veya üçüncü parti kaynaklardan faydalanır, bir kaç saniye içinde işini yapar.Sunucusuz bilgi işlem, kod çalıştırma işleminin, geleneksel uygulama geliştirme ve sunuculara dağıtma yöntemi yerine tamamen bir bulut sağlayıcısı tarafından yönetildiği bir mimaridir. Geliştiricilerin, kod dağıtırken sunucuları yönetmek, hazırlamak ve korumak için endişe duymamaları anlamına gelir.Daha öncesinde bir geliştirici, ne kadar depolama ve veritabanı kapasitesi gerekiyor bunu tanımlamak zorundaydı ve toplam süreç bu nedenle yavaşlamaktaydı fakat bu yöntemle bu tip endişelere gerek duyulmamaktadır.
Sunucusuz uygulamalar oluşturmak, uygulama geliştiricilerinin bulut veya lokaldeki sistemin yönetilmesi ve çalıştırılması ile ilgilenmek yerine uygulama geliştirmeye odaklanabileceği anlamına gelir. Bu azaltılmış yük, geliştiricilerin ölçekli ve güvenilir ürünler geliştirmek için daha fazla zaman ve enerjiyi ürüne ayırmalarını sağlar.Bu hizmeti veren hizmet sağlayıcıların örnek ürünleri, AWS Lambda, Azure WebJobs, IBM OpenWhisk, Google Cloud Functions olarak sıralanabilir.
Engin ÜNAL
Software Engineer
27 
27 
27 
Software Engineer
"
https://medium.com/@benbob/thanks-for-the-positive-comments-dan-deb3f487f9d3?source=search_post---------81,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Fathi
Dec 16, 2018·1 min read
Dan Keldsen
Thanks for the positive comments, Dan. Duct tape and chewing gum is definitely the right analogy for what passes as on-prem state-of-the-art.
I don’t think there’s a single answer when it comes to IaaS vs. PaaS vs. SaaS. They’re solving different problems and each is a viable solution for some subset of those problems. The equivalent question in the on-prem world would be to ask which is better: hardware, operating system, or application. Obviously, each has its own place in the stack.
My simple rule of thumb is always to opt for the highest level of abstraction available for the specific problem you’re trying to solve.
Former {CTO at VMware, VP at Microsoft, Head of Engineering & Cloud Ops at Cloudflare}. Recovering long distance runner, avid cyclist, newly minted grandpa.
1 
1
1 
1 
1
Former {CTO at VMware, VP at Microsoft, Head of Engineering & Cloud Ops at Cloudflare}. Recovering long distance runner, avid cyclist, newly minted grandpa.
"
https://medium.com/@comerciocloud/iaas-or-infrastructure-as-a-service-what-businesses-need-to-know-efa407f714e3?source=search_post---------249,"Sign in
There are currently no responses for this story.
Be the first to respond.
Comercio Cloud
Sep 27, 2018·4 min read
Like other cloud computing packages, Infrastructure as a Service (IaaS) delivers virtualised computing resources over a network connection, most commonly the Internet. Infrastructure as a Service is an example of the wide range of cloud services now being utilised by businesses. IaaS packages will vary from organisation to organisation, but typically involve hardware, storage, servers, as well as system maintenance and security features. The breadth of services offered by cloud providers often makes IaaS an attractive proposition for businesses that do not have the resources to effectively purchase and maintain their own hardware. Of course, as with other cloud services, IaaS still comes with its risks so businesses must ensure that they are well versed on the finer details of Infrastructure as a Service before committing to a contract.
The complete cloud package
What with all the cloud acronyms doing the rounds (SaaS, DaaS, PaaS) it is perhaps easiest to understand how Infrastructure as a Service differs to other cloud services. With IaaS, customers generally receive virtualised hardware components from which they can create their own cloud platforms and applications. This focus on hardware differentiates it from other forms of cloud computing, such as Platform as a Service or Software as a Service, where businesses generally maintain and configure their own computer hardware or infrastructure. Hardware can be prohibitively expensive for many firms, particularly those without large IT budgets, and so it is not surprising that Infrastructure as a Service is proving one of the most popular branches of cloud computing. In fact, according to a recent study by IDC, IaaS is forecast to be the fastest growing area of cloud computing between 2014 and 2018.
As with other cloud solutions, businesses that adopt IaaS allow a third-party vendor to deliver their IT resources, shifting the complexities and expense to the cloud supplier. Businesses can then utilise their IaaS as they see fit. Common uses for Infrastructure as a Service include delivering applications, storing data, hosting websites on virtual servers, or creating a virtual data centre. The exact nature of an organisation’s IaaS offering will vary based on the supplier and business needs.
Advantages of IaaS
There are a wide range of reasons why businesses are increasingly adopting Infrastructure as a Service.
The most important security change a company can make is not technical, but cultural. Every employee must be encouraged to take a proactive approach to security, rather than leaving it to the IT team or cloud supplier. Through staff training and clear company messaging, employees will begin to see encryption and strong authentication as an everyday part of their job.
Being able to remotely access critical IT resources is of huge benefit to employees, but it could also be abused by an individual or group acting against the company’s best interests. With a rights management software, businesses can create access management policies, restrict access to specific data sets and applications and introduce self-destruct procedures for temporary information. This ensures that company security is reinforced without negating the flexibility benefits provided by IaaS.
Overall, it is easy to see why businesses are embracing Infrastructure as a Service. With many cost and reliability benefits, an effective IaaS partnership with Comercio Cloud Computing can provide the impetus to drive businesses forward. With the technology likely to develop further in the future, businesses that adopt Infrastructure as a Service now may yet experience even greater benefits in the years to come.
"
https://medium.com/memory-leak/our-investment-in-opsani-continuous-cloud-optimization-b893800fe5e7?source=search_post---------117,"There are currently no responses for this story.
Be the first to respond.
During my time on Cisco’s Corporate Development team I supported the UCS server division and had a firsthand view of the massive shift in computing architecture to the cloud. Companies replatform to the cloud to decrease operational burdens, increase agility, and improve scalability.
While using cloud infrastructure, teams often emphasize accelerated software delivery to remain competitive over performance and cost optimization. Parkmycloud estimates $14.1B of cloud compute spending will be wasted in 2019. I discussed cloud expensive management as a key 2019 DevOps trend here.
Businesses use performance engineers and cloud consultants to manually conduct point in time performance/cost analysis. Unfortunately, code is updated so frequently today, their recommendations are quickly out-of-date and useless. We believe teams can have it all — incredible application user experiences at optimized costs.
Enter Opsani, a continuous cloud optimization solution to fine-tune runtime environments. Using ML models, Opsani continuously assesses the efficient frontier between the performance and the cost of running services in the cloud. Opsani’s models evaluate more than just the infrastructure layer like instance types and sizing, but also additional application parameters like middleware configuration, garbage collection, worker threads, etc.
Moreover, Opsani eliminates previously manual actions through automation, taking the burden off engineering and DevOps teams. Before each deployment Opsani runs a canary test of the new service in the production and applies its AI to evaluate the ideal runtime environment. The application layer visualizes the ML analysis, and Opsani’s explainable AI makes suggestions that are easily understood in plain language. DevOps teams can choose to individually apply the recommendations or automatically push them to production.
We became excited about Opsani for multiple reasons:
Serial entrepreneurs Ross Schibler and Peter Nickolov founded Opsani. Ross is a third-time Redpoint entrepreneur having previously founded Rapid City (acquired by Nortel) and Topspin (acquired by Cisco). Ross served as the CTO for Cisco’s data center servers, storage, and networking BU. Peter founded 3tera (acquired by CA), which was one of the first cloud operating systems and became CA’s fastest-growing product. At CA, he was the technical leader for the AppLogic Cloud Platform leading teams of hundreds of employees globally. The team is well-positioned in the cloud optimization space as they have firsthand experience transitioning companies to the public cloud, building solutions for cloud environments, and managing businesses at scale.
We’re excited to have led the Series A investment in Opsani alongside existing investors Zetta Venture Partners and Bain Capital Ventures. We are thrilled to partner with Ross, Peter, and the entire team on the journey forward. If you are interested in joining the stellar team, please check out their careers page here.
VC Astasia Myers’ perspectives on distributed computing…
8 
8 claps
8 
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
"
https://medium.com/@vincent-lui/sitecore-melbourne-meetup-may-23rd-2019-pain-points-from-iaas-to-paas-aaf0172985df?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vincent Lui
Aug 19, 2019·1 min read
Back in May 23rd 2019, I had the opportunity to present at Melbourne’s Sitecore Meetup at AKQA Melbourne. It was a quick consolidation of the Azure Migration series, which I have still yet to complete at the time of writing this blog post.
I have managed to record myself along with slides.
Here are my slides:
The slide that I have attached have been amended and slightly different from what I presented at the Meetup. The slides that I have changed are #33 and #66. They mainly detailed what web apps are given the Blue / Green deployment treatment, and the deployment order.
Don’t forget to comment and reach out if you believe any of the info I have presented are incorrect, and would like to discuss anything in relation to the topic that I presented.
Sitecore Technology MVP 2020–2021| Solution Architect on Sitecore, Akamai, Microsoft Azure | Passionate on DevSecOps Lifecycle @ CPA Australia
See all (73)
4 
1
4 claps
4 
1
Sitecore Technology MVP 2020–2021| Solution Architect on Sitecore, Akamai, Microsoft Azure | Passionate on DevSecOps Lifecycle @ CPA Australia
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@NathanCarnegie/getting-started-with-linode-in-2019-59ded33f9b98?source=search_post---------363,"Sign in
There are currently no responses for this story.
Be the first to respond.
NerdOfCode
Dec 23, 2018·4 min read
Having troubles deciding on an Iaas (Infrastructure as a Service) provider in 2019? Well, check this problem off your list as by the end of this post, I will have solved it!
First off, Linode is by far, one of the big brothers in the Iaas game. Linode started off in mid 2003, with three data centers to its name. Fast forward to 2018, and Linode has expanded to an astonishing nine data centers!
"
https://medium.com/swlh/towards-serverless-faas-as-the-next-step-in-infrastructure-as-a-service-iaas-evolution-ef5227926bb6?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
Often called FaaS (Function-as-a-Service), Serverless is the idea that you can deploy and run applications without provisioning or managing servers where:
"
https://medium.com/@alibaba-cloud/alibaba-named-by-gartner-as-third-biggest-global-provider-for-iaas-and-first-in-asia-pacific-517d0bc095bf?source=search_post---------45,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 1, 2020·3 min read
HANGZHOU, 28 April, 2020 — Alibaba Cloud, the data intelligence backbone of Alibaba Group, announced that it has the third biggest market share in the global Infrastructure as a Service (IaaS) market and the biggest in the Asia Pacific region for the third year in a row in 2019, said the leading global research and advisory firm Gartner in its latest report.
Alibaba’s market share in the global IaaS market climbed to 9.1% in 2019, up from 7.7% the year earlier, according to the firm. The Gartner report, named Market Share: IT Services, 2019, also showed that in the Asia Pacific region, Alibaba’s market share increased to 28.2% last year, from 26.1% in 2018.
“We believe our strong growth and leading market position is a testament to the hard work of our teams and the support of our many customers and partners around the world, to whom we are truly grateful, “said Jeff Zhang, president of Alibaba Cloud Intelligence. “We look forward to continuing our work, which enables our global customers and partners to expedite their digital transformation journeys through our scalable, robust and secure infrastructure, advanced analytics capabilities and thriving ecosystem.”
“To us, the report once again demonstrated Alibaba Cloud’s commitment to our global strategy to expand on our worldwide presence by enhancing our global infrastructure and network.” added Zhang.
Alibaba Cloud currently has 63 availability zones in 21 regions, serving millions of customers across the globe. It has more than 70 security and compliance accreditations worldwide.
Alibaba Cloud is committed to offering enhanced cloud services to more week that it will invest an additional RMB200 billion (approximately US$28 billion) in the next three years on its cloud infrastructure, focusing on technologies including operating system, servers, chips and network.
Alibaba Cloud is the technology and public cloud platform underpinning Alibaba’s rich and diverse ecosystem, which ranges from e-commerce and payment, to logistics and supply chain management solutions. It supported $38.4 billion in transactions on a single day during Alibaba’s 11.11 Global Shopping Festival last year.
In addition to Gartner’s findings, an earlier report from market research and advisory firm International Data Corporation (IDC), showed that Alibaba Cloud recorded the fastest year-on-year revenue growth versus other global cloud service providers in the first half of last year.
To view the Gartner report, please click on this link (content only accessible to Gartner subscribers. Identified in the report as Alibaba Group.)
Established in 2009, Alibaba Cloud, the data intelligence backbone of Alibaba Group, is among the world’s top three IaaS providers. It is also the largest provider of public cloud services in China, according to IDC. Alibaba Cloud provides a comprehensive suite of cloud computing services to businesses worldwide, including merchants doing business on Alibaba Group marketplaces, start-ups, corporations and public services. Alibaba Cloud is the official Cloud Services Partner of the International Olympic Committee.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tanmayct/infrastructure-as-a-service-6b639377882a?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tanmay Terkhedkar
Mar 31, 2019·3 min read
Infrastructure As A Service (IaaS) is a cloud-based computing service which is a under a category of Service Model.
Infrastructure as a service provide virtualized computing infrastructure over the internet.In IaaS, cloud service provider hosts the total infrastructure of application like virtual machines, data storage service,virtual local networks,network hardware etc.
Customers of IaaS can use infrastructure provided over the internet by cloud service provider by just subscribe into it. It maybe paid on monthly or yearly basis.
Some examples of vendors who provide IaaS: -
Servers used by Cloud Provider:-
Cloud vendor use server to host the infrastructure as a service.A server is a device designed to process from other programs and deliver that data via the internet or local network. Servers offer many different services like data sharing,data storing,etc.
The best example of server is NVIDIA DGX who provide server to IBM cloud to host infrastructure as a service. In this server,services can be deployed and used immediately with no complex installation required.The box comes with a rack of nine Nvidia DGX-1 servers, with a total of with 72 Nvidia V100 Tensor Core GPUs.
The another example is INTEL XEON. Intel designed custom Xeon processors for Amazon Web Services that will power the cloud provider’s new server instances optimized for high-octane computing. The chips will provide the highest level of CPU performance EC2 has ever seen.
Some important Benefits Of IaaS:-
1) Cost savings:- As infrastructure is on the internet we don’t have to maintain hardware and network devices or replacing old devices with new devices.Hence it saves the cost of hardware devices. You just have to pay money while using service, so this also saves a money alot.
2) Multiple users:- More than one user can work on a single server from anywhere and anytime. As resources are accessed through internet connection, IaaS customers can work remotely. IaaS gives more flexibility to build an application.
3) Server quality:- IaaS can run even if a server goes down. As data spread over multiple servers and if one particular hardware device fails then it will not affect on organization’s infrastructure.
Drawbacks of IaaS:-
1) Data Loss:- Daily backups and their storage on external protected alternative platforms is important in IaaS. Data can be loss due to some accidents at the provider’s end.
2) Vulnerability:- Due to the inter-dependency of the system,it results into increase in vulnerability and there might be a leakage of personal information to the world.
References:-
https://medium.com/@BreezeTelecom/saas-paas-and-iaas-how-and-when-to-use-a90f526f10ae
https://statetechmagazine.com/article/2014/03/infrastructure-service-5-important-benefits
https://simplicable.com/new/infrastructure-as-a-service
https://www.networkworld.com/article/3327215/data-center/ibm-and-nvidia-announce-turnkey-ai-system.html
See all (58)
552 
552 claps
552 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sequoia-capital/rockset-and-the-future-of-data-driven-apps-34acf3e2a517?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
One of the most important technology themes of the past 10 years has simply been data.
At the beginning of the decade, new data techniques helped drive an increased focus on analytics and A/B testing. In the past few years, the focus has shifted to machine learning. Both technologies are rooted in the same trend — the exponential increase in data.
The CEOs we meet know that their businesses need to become data-driven to compete and survive. But not everyone knows how.
Getting started is easy — you instrument your product or business and start collecting data. But turning that data into valuable, actionable insights is much more challenging.
One of the biggest pain points is just physically moving the data around. The data pipelines and ETL jobs that move data from its initial raw form to the final product are often slow, fragile, and hard to maintain.
There have been many efforts to simplify these pipelines, but the state of the art is still Rube-Goldberg-esque. It can take weeks or months for new types of data to make their way into production applications — if they ever make it at all.
Rockset
At Sequoia, we love partnering with founders that challenge conventional thinking.
When we first met Venkat and Dhruba, they wondered — why do we need data pipelines at all? In this new era of cloud-native infrastructure, why can’t we use web-scale search techniques to build data-driven applications directly on top of the raw data? Why make pipelines 10% better when we can eliminate them entirely?
Their answer is Rockset. A cloud-native service that helps developers run production-ready SQL directly on top of raw data. It’s super simple to get started:
Once you’ve configured your data source, Rockset immediately starts indexing your data, enabling you to explore your data and write application-ready SQL within seconds. At Sequoia, we’ve used Rockset to move many cumbersome nightly jobs to real-time dashboards. Not only are our internal tools faster, but our data science team now spends less time babysitting pipelines and more time helping our companies grow.
Rockset is the result of a wonderful, interdisciplinary data team coming out of Facebook. CEO Venkat Venkataramani was the founder of TAO (Facebook’s online graph database) and a database engineer at Oracle. CTO Dhruba Borthakur was one of the key architects of Facebook’s data warehouse and the co-creator of both RocksDB and HDFS. Tudor Bosman was a co-creator of Unicorn (Facebook’s internal search backend) and the Gmail backend. Shruti Bhat held senior product roles at both VMware and Oracle.
We first partnered with Rockset at the seed stage in 2016, along with our friends at Greylock. At the time, Rockset was a rough idea inspired by the idea of converging a traditional SQL database with a cloud-native search engine. Today we’re proud to deepen that partnership with Rockset’s Series A financing.
It’s been a fantastic journey with the team so far — and it’s just getting started. We’re incredibly excited to see what you build with Rockset.
From idea to IPO and beyond, Sequoia helps the daring build…
157 
1
157 claps
157 
1
From idea to IPO and beyond, Sequoia helps the daring build legendary companies.
Written by
@Sequoia. Work w/ Rippling, Notion, Statsig, Blues Wireless, Aquarium, Census, Rockset, PicsArt, Threads, Verkada, Starkware, Canvas, Citizen, WhisperAI & more.
From idea to IPO and beyond, Sequoia helps the daring build legendary companies.
"
https://medium.com/swlh/iaas-vs-paas-vs-saas-dfece8fd6ca?source=search_post---------7,"There are currently no responses for this story.
Be the first to respond.
If your ecommerce business is progressing to the Cloud, you need to be familiar with these three main types of cloud computing:
"
https://regarding365.com/your-private-cloud-is-useless-and-heres-why-a0ba44a670b3?source=search_post---------318,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Yes, I’m talking to you. The company that has built its own PaaS, IaaS or Saas infrastructure and have been selling this into their region for the last 10+ years. You. Your time is up. Its time to meet your maker.
You are being taken to the back of the shed, and are being put out to pasture. And you probably not going willingly; kicking and screaming.
What do you really have to offer anymore? Wait, bandwidth right? Or maybe because its not on the “Internet” because you have a site-2-site tunnel. Sure. Lets go with that.
Rackspace has broken down the different types of cloud services in their “Comparing Private and Public Clouds” post so keep that as a frame of reference for the rest of the article.
In preparation for this post, I searched for “the pros of private cloud” and was fascinated to see how many articles still refer to security as a pro. Really?? And they also reference scalability, connectivity, reliability and network infrastructure as being superior to that of a Public Cloud offering.
I would like to challenge these so called private cloud companies that believe that they are better than their public cloud counterparts to beat the following: (Naturally i’ll be using Microsoft’s Cloud as my yard stick)
In closing, we have quite a few private cloud providers in South Africa that have to either change their model and turn their business into a Google, Microsoft or AWS cloud provider offering or face the following…
Before you decide to move your on-premises data center to a private cloud or to stick with the provider you have, use the items listed above when you grade what is on offer from them.
If you enjoy this post, please hit the “recommend” (heart) icon. Also, you can catch us on our Facebook page or YouTube Channel for more interesting thoughts and articles on all things Regarding Microsoft 365.
Be cool my ninjas.
Thoughts, opinions, discoveries and tips regarding…
21 
21 claps
21 
Written by
Azure and Office Servers MVP | Speaker | Blogger | Podcaster | Evangelist
Thoughts, opinions, discoveries and tips regarding Microsoft 365, from enthusiasts who make it their business to share them.
Written by
Azure and Office Servers MVP | Speaker | Blogger | Podcaster | Evangelist
Thoughts, opinions, discoveries and tips regarding Microsoft 365, from enthusiasts who make it their business to share them.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://pub.towardsai.net/what-are-cloud-iaas-paas-saas-faas-and-why-we-use-them-8af979dad141?source=search_post---------13,"Click here to get to know me, my projects, and my latest articles.
Your first approach to the Cloud may be very confusing. The Cloud is gigantic, and there is not only ONE Cloud, but many viable choices in the market. Soon or late, if you decide to become a programmer, you will need to start learning how to work in ONE Cloud…
"
https://medium.com/@focaloidtechnologies/saas-vs-paas-vs-iaas-fee5e382a397?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Focaloid.com
Feb 20, 2019·3 min read
Whether it is small businesses or large enterprises, cloud is today’s need. After all, cloud is a broad concept and covers a lot of aspects of online world. Three main building blocks of cloud computing are.
These models do have some features in common. For instance, users can store data online no matter which model they select. Let’s get started with each model separately.
Historically, most IT systems that were provided to customers were delivered on an on-premise basis where you physically purchase the license and maintain all on your own. When SaaS, also known as cloud application services was developed, it allows users to run existing online applications which are managed by a third-party source. A good thing about SaaS is that the client doesn’t need to do any installation from their side. Many businesses find this model quite beneficial, which allows them to simply streamline their maintenance and support, while no need worry about managing technical issues. SaaS also saves time and money you spent on tiresome tasks like installing, managing and upgrading software. All of this boils down to the fact that, you can spend more of your time on important matters within organization.
Examples of SaaS
PaaS provides environment and tools which users can employ to create new online applications. Enterprise or third-party vendor usually manages everything from servers, to storage and networking, and developers handle the management of applications. Good part is developers would be able to focus only on code and customers, not server and software configuration. If you want to launch your own customized applications, then PaaS is a good option. It makes process of apps deployment simple and cost effective. Since it is built on virtualization technology, it means you can scale up or down as your business grows.
IaaS allows users to run applications they please on cloud hardware of their own choice. This service offers the computing architecture and infrastructure and covers all computing needs but in a virtual environment. Due to this, multiple users can have access to it. IaaS works well for large organizations that want to attain full control over their infrastructures, but are looking to invest only on something that is actually consumed. Enhanced scalability and flexibility are the major perks of this model.
Few examples are:
Businesses can either use these cloud computing models individually or as a hybrid combination. Which kind of cloud computing model to choose, depends on your business goals and requirements.
This article was originally posted on our company blog.
Also read:* 4 Questions to ask while considering SaaS for business automation
Focaloid is a digital consulting company that focuses on developing reliable, innovative technology solutions. Know more about us at www.focaloid.com
2 
2 claps
2 
Focaloid is a digital consulting company that focuses on developing reliable, innovative technology solutions. Know more about us at www.focaloid.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@stfk1105/iaas-paas-saas-%E4%B8%89%E5%85%84%E5%BC%9F-c745dfa0cfd4?source=search_post---------146,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jade Chang | 張 婕
Jan 20, 2019·3 min read
邊準備證照邊隨手作筆記，雖然 IaaS & PaaS & SaaS 這是 800 年前的詞，但現在好像還是常常聽到？！從原本自家 On - Premises 全自架開始延伸到 IaaS & PaaS & SaaS 三種雲端分層，先用一個生活化的例子來舉例
有 4 種方式達到這個願望
1. 自己在家做草莓蛋糕 (On-Premises)
從買蛋+麵粉+草莓、準備烤箱到打蛋器擀麵棍擠花袋全自己準備
2. 去動手做或蛋糕烘培坊的地方製作 (IaaS 基礎設施即服務 )
店家提供菜譜、材料、高級烤箱、麵粉蛋汁與新鮮草莓，自己只需要根據有的器材與食材去製作喜歡的樣式的草莓蛋糕
3. 用 Uber Eats 叫外送 (PaaS 平臺即服務)
支付些許手續費，透過第三方提供的服務達成吃草莓蛋糕的心願
4. 直接 Lady M 吃草莓蛋糕 (SaaS 軟體即服務)
準備好錢直接享受店家製作的草莓蛋糕
下面這張圖解釋的蠻清楚，從純地端需要營運的項目到全託管的示意圖。
// Reference：
Statistic Information 畢業，入職場後從事軟體技術行銷，直播節目/線上教學/技術社群/行銷社群 Program 為主，後來導向學生實習計畫與大專院校 Talent Cultivation，中途出家軟體開發商 PM 與翻攪 Startup，近期朝向 Semiconductor 出發在新竹生活的女子 ♥
479 
3
479 
479 
3
Statistic Information 畢業，入職場後從事軟體技術行銷，直播節目/線上教學/技術社群/行銷社群 Program 為主，後來導向學生實習計畫與大專院校 Talent Cultivation，中途出家軟體開發商 PM 與翻攪 Startup，近期朝向 Semiconductor 出發在新竹生活的女子 ♥
"
https://medium.com/@Codename_One/not-exact-275db8492f9f?source=search_post---------95,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shai Almog
Aug 13, 2016·1 min read
Balázs Németh
Not exact. Approximate. I just want to know the name of the entity being accessed (similarly to a table). I think the best analogy is this, you get a 1000 USD bill from the phone company without telling you which phone you called to trigger that bill. I don’t want them to point at the area on my code, just provide information they have (or should have) in order to construct the bill. Right now the only number given is the number of reads, unfortunately it’s really unclear to me what that means. Is it an entity whose fields we query too much or actual physical queries etc…
Notice that this isn’t an error, this is an overcharge. When using IaaS we might have had a small performance degradation involved with inefficient caching, in that case I could use a profiler on production and improve that if it’s necessary. Instead of overcharging or even downtime (which again, the Google rep suggested downtime as a workaround for over-billing) performance would just degrade a bit.
I’m sure App Engine is getting better. Unfortunately Google isn’t, after the treatment we got from their paid support I strongly doubt they care about app engine or at least when it relates to small customers like us.
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
Dev advocate @ Lightrun, Co-founder of Codename One, Author, Speaker, Open Source Hacker & Java Rockstar
"
https://medium.com/jumpcloud/decentralized-iam-the-cloud-directory-cure-167446d7f2ed?source=search_post---------346,"There are currently no responses for this story.
Be the first to respond.
Identity and access management has quietly become a crisis for IT. The proliferation of disparate resources (SaaS, IaaS, Mac®, Linux®, etc.) has decentralized core identity management operations. While the cloud has been part of the problem, we believe that it also contains the solution.
From the crisis of decentralized IAM, the need for a cloud directory emerged. Below we’ll explain how modern identity management was disrupted and JumpCloud’s vision for unifying IAM again with cloud directory services.
You may (or may not) recall the brick-and-mortar environment your network and all the connectivity your company’s resources operated from back in 2000.
Remember walking into that facility, sitting down in front of your desktop, and running Windows® XP® or Windows 2000? (And by desktop, I mean an immobile, truly-tethered-to-your-desk desktop, of course). Most of the machines were physically tethered together through Ethernet wiring and switches/hubs and managed through a server somewhere around the corner in the same building in a closet or data center. This was called the LAN — the local area network. With the addition of remote sites and the Internet, the term WAN — wide area network — was introduced.
The LAN of yesteryear was brought to you almost exclusively by Microsoft Windows and its associated components. Through the magic of Kerberos for authentication, Active Directory® (AD) domain services, and Windows Server, Microsoft® was working behind the scenes and quietly running the network.
All the machines tethered to this Active Directory domain worked together.Your one set of credentials granted you access to basically everything:
But this couldn’t last forever.
Microsoft was beginning to establish a vision for managing a more mobile, operative workforce, but they still kept one foot firmly rooted in the on-prem world of the past. Laptops loaded with Virtual Private Network (VPN) software enabled you to be authenticated to Active Directory, and connect to the corporate network outside of the office, but the technological climate began changing faster than Microsoft could keep up with. Their pristine model of a domain-bound network was being stretched thin by new devices and progressive applications. The sunlight of the infrastructure Active Directory had been providing was beginning to cast a shadow around something called the cloud. But, where was the cloud coming from?
Companies like Salesforce ®were among the first to take advantage of the benefits of cloud computing. Founded in 1999, Salesforce introduced a cloud-based Customer Relationship Marketing (CRM) system that flew in the face of traditional business software — and traditional IT management.
For IT admins, this came as a bit of a shock. Suddenly, their complete control over user data and IT systems — their “domain” — was being disrupted by independent solutions outside of their control (and walls). New user accounts and credentials were flowing in from rogue sales and/or marketing teams who were subscribing to services with credit cards. In the process, users were completely bypassing normal IT purchasing models and methods.
This problem got a name (“Shadow IT”) and a host of new solutions sprung up to try to solve it. We started to see the first tethers of Active Directory branching up towards the cloud through web application single sign-on solutions, using protocols like SAML. These SSO solutions allowed IT administrators to return to management as usual (albeit managing multiple IT management systems), but only for the time being. The dam of Active Directory had sprung a leak.
By the late 2000s, the pattern of tethering cloud-based utilities to Active Directory had spread to solutions like G Suite™ (formerly known as Google Apps), which had in many cases replaced Exchange as an email mechanism and productivity platform. But this required still more utilities, like Google Apps Directory Sync® (GADS), now called Google Cloud Directory Sync (GCDS), in order to do so. The spectre of Shadow IT continued to spread as new cloud-based mechanisms and devices emerged with one-off utilities to tether them back down to Active Directory.
Then, Apple pulled a Steve Jobs and changed their whole image with the iPhone® and iMac®, reinventing the consumer wheel in the process. Suddenly, admins saw people walking into the office with these sleek devices, totally unmanaged, but nevertheless accessing corporate networks and data. One recent survey on the matter found that 75% of users today prefer Mac over Windows. Simultaneously, Linux increased in popularity, especially among developers and engineers as Linux now runs 90% of the public cloud workload.
So, the massive ecosystem of vendors expanded rapidly, yet again, and system admins were reaching a breaking point. Bring your own device (BYOD) programs emerged as the influx of employees’ personal devices became too much of a security risk too ignore. But these BYOD programs required IT organizations to manage all types of disparate systems, and made environments difficult to secure. Some system admins opted instead for policies requiring their users to stick with Windows and Microsoft exclusively. Others migrated to Mac, while many embraced Linux.
What they all had in common is a need to securely tether all pathways to a centralized identity store. But they didn’t have that. Instead, they had a convoluted knot of siloed identity management systems.
As a key aspect of these IAM silos, protocols were flying in from all sides to support specific devices and specific applications and specific networks, causing IT admins to lose sleep and probably lose hair as well. Updating and adapting IAM solutions to this multi-protocol environment was a nightmare:
The result of all this diversification? Security gaps. Inefficiencies. Broken onboarding/offboarding processes for scaling enterprises. Wasted time and resources. IT admins at their wits’ ends.
With Active Directory at the center, IT organizations were weighed down by all they had to manage, and were struggling to stay above water. As the disarray mounted, the IT landscape reached a breaking point. Whether patchy security or disparate resources or simply the mounting stress on IT admins, organizations at large were on the verge of being pulled apart.
From this chaos, the vision of a true cloud directory was formed.
JumpCloud emerged out of a need that was being felt industry-wide. This was simply the need to help people access resources for work, and enable them to be productive.
While SSO solutions, identity bridges, and privileged identity management encourage layering point solution add-ons on top of flawed, legacy directory services, JumpCloud felt the mounting disarray and started there — at the core of the problem.
Reimagining the centralized identity management of the past, JumpCloud saw an opportunity to pull it all back together. But this time, unification would begin with the cloud.
We wanted to build the world’s first, and best, 100% cloud-based directory — one capable of securely managing desktops, laptops, applications, files, WiFi networks, and more regardless of platform, protocol, provider, and location.
Today, we are well on our way to achieving that vision, no matter the vested industry interests in opposition to our efforts. we can say that we’ve achieved that vision. We call it Directory-as-a-Service® (DaaS). These are the principles that have guided us:
The JumpCloud platform doesn’t require any on-prem hardware. We are built on the cloud, for the cloud and on-prem and remote IT resources — so, from the cloud, but for wherever your IT resources are. So, DaaS works seamlessly with SaaS apps, cloud server environments, and remote user management. But while we know that the future is in the cloud, we haven’t forgotten the past: JumpCloud can manage your legacy, on-prem apps and hardware too.
We believe that users should have a choice in everything they use — and that admins should have control over those systems regardless of platform. By providing a vendor-agnostic identity management tool that caters to a wide variety of heterogeneous environments, everyone wins (except for the guys that want to lock you into strictly their platforms).
This emphasis on independence extends to networks, protocols, and apps as well. Simply put, we don’t care what you want to connect to us. We use a wide variety of protocols and directly support SAML integration with hundreds of applications.
We believe IT admins should be able to enforce security settings and take actions on groups of users and systems, in a centralized, automated fashion. Setting policies, executing commands, running scripts, and controlling systems across whole fleets of machines should be encouraged — not restricted.
The system is at the center of the user’s work life. With JumpCloud, the system is the gateway of authentication to access resources securely. When a user logs into a workstation, only that single identity can be used across everything from apps to infrastructure. Reset passwords straight from the OS through the JumpCloud Mac App. JumpCloud is where identity security meets user convenience.
“Paranoia is not retroactive” is a common saying at JumpCloud. Once a security breach occurs, there’s no way to reverse the damage. That’s why we’re dead serious about security. By employing a private PKI infrastructure, unique key generation per agent, multi-factor authentication, and independent audits from a qualified third-party assessor three times a year, you can trust our security practices are proactive, fortified, and meticulously monitored.
We don’t do it all, and we don’t propose to. But what we do propose is through highly scalable protocols, a feature-rich platform, and the hard work of our engineering team, we’ve delivered the first true, cloud-based directory.
And that’s only just the beginning.
With the rapid adoption of cloud apps, infrastructure, heterogeneous resources, and with IT services expected to grow by 4.7% in 2018 (~$44 billion dollars), it’s the dawn of a new age.
We’re at the forefront of this revolution, unifying and simplifying directory services, and we hope you see that vision with us. With JumpCloud, you’re returning to the simplicity of a secure, centralized, singular source of truth. Pick and choose what you want, leverage the protocols you need, and do it all from a single browser-based console.
In a way, we’re realizing the initial idea of unified IT management put forward by AD in the early 2000s, and taking it much further. Unlike AD, our Directory-as-a-Service is in the cloud, platform independent, hyper-secure, with no on-prem servers to configure or maintain.
Our long-term vision isn’t just to be the world’s best, most secure cloud directory. We want to be the world’s best directory — bar none. We hope you now have a sense for what we’re really excited about at JumpCloud as we continue to build together on the promise of comprehensive, foundational IT infrastructure for the modern era. Explore our product page to learn more.
One identity to securely connect your users to the IT…
4 
4 claps
4 
One identity to securely connect your users to the IT resources they need.
Written by

One identity to securely connect your users to the IT resources they need.
"
https://medium.com/@allo/%D0%BE-%D0%BA%D0%B0%D1%80%D1%88%D0%B5%D1%80%D0%B8%D0%BD%D0%B3%D0%B5-820285ead178?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alexander Lozhechkin
Jun 10, 2018·10 min read
Есть IaaS, PaaS, SaaS, а есть Патрисия Каас.Анекдот для ИТ-шников
Ровно год назад я впервые прокатился на каршериге. Я думал, что я был последним, кто приобщился к этому виду общественного транспорта, но оказалось, что это я ещё был в первых рядах. За этот год я успел прокатиться на практически всех каршерингах Москвы, Европы и Штатов. Освоил байкшеринг и самокатшеринг. С тех пор каршеринг стал очень массовым, но я всё равно думаю, что настоящая массовость у каршеринга ещё впереди. В последнее время о каршеринге написано столько несусветной чуши, что я не смог устоять и решил написать, как всё обстоит на самом деле.
О shared-economy я уже писал. Я там рассказывал о каршеринге, как примере shared-economy. Услышал много неприятного в свой адрес от любителей собственных автомобилей.
robot-review.ru
Своей машины в Москве у меня нет. Впрочем, своей машины у меня нет не только в Москве, но и в большинстве городов мира, где я бываю. Своей машины у меня нет нигде, кроме Мюнхена, где я тоже планирую от неё отказаться. Как и от мотоцикла. Потому, что содержать что-то, чем пользуешься максимум 10% своего времени, очень нерационально. Ну и просто не очень этично по отношению к обществу.
Ведь большинство людей, владеющих машинами, пользуются ими от силы пару часов в сутки. То есть остальные 22 часа их машина просто стоит, занимая место и не принося пользы. И хорошо ещё, если она стоит на земле, принадлежащей владельцу машины (на своей земле делаю всё, что захочу), но ведь чаще всего все частные машины 80% времени занимают место в городе, причём место общественное.
Порассуждаем немножко про это. Исторически людям хочется жить рядом с другими людьми. За исключением редких примеров отшельников люди стремятся жить вместе, создавая деревни и города. Причём чем больше людей оказывается рядом, тем лучше им оказывается жить. В больших городах, как правило, есть хорошие аэропорты, из которых можно без пересадок и недорого улететь по многим направлениям. В больших городах есть хорошие музеи и театры. В больших городах есть хорошие школы и университеты. В больших городах есть хорошие магазины. В больших городах есть хорошие места работы. Потому, что в больших городах есть много хороших сотрудников. То есть работает кумулятивный эффект: больше людей — лучше услуги — больше людей.
Но есть одна проблема. Чтобы большой город не рос безгранично в пространстве по мере роста населения, приходится тесниться. И даже если, как в Штатах, многие будут жить в одноэтажных домиках, центр города с музеями, университетами, музеями, театрами и ресторанами, оказывается всё равно довольно тесным. А если весь пригород будет одноэтажным, то большинству ехать до этого самого центра придётся очень долго. И если ехать долго не хочется — то придётся жить поближе к центру. Где земля будет стоить дорого. И поэтому придётся жить потеснее. Опять же, круг замкнулся.
Таким образом большая плотность населения — неизбежное следствие желания людей жить удобно. Стоимость квадратного метра площади поверхности земли в большом городе взлетает до небес. Дома становятся всё выше, квартиры всё меньше. И вот тут логично задать вопрос — а можем ли мы себе позволить 10 метров площади города отдать не домам, не дорогам, а парковке? Именно столько места занимает средний легковой автомобиль. Причём автомобиль, который 80% времени не приносит никакой пользы, никуда не едет, а просто стоит на обочине. Занимая такое дорогое место. Даже если парковка платная и владелец машины платит свои 70 или даже 200 рублей в час за парковку — это несравнимо дешевле, чем могла бы приносить эта земля, если бы использовалась с толком. Например, для того же музея, университета, хорошего места работы или жилья.
Таким образом парковка — очень дорогое удовольствие. Готовы ли вы были отдать 10 квадратных метров своей квартиры для парковки своей машины. Или двадцать — для двух? Вряд ли. А почему вы считаете, что город должен предоставить вам эту площадь? Да ещё и бесплатно? Поэтому парковка просто обязана быть дорогой. И поэтому же владение собственным автомобилем несовместимо с жизнью в городе. Хочешь владеть своей машиной — покупай загородный дом. Хочешь жить в городе поближе к музеям, ресторанам и театрам — езжай на трамвайчике.
Так было до недавнего времени. Когда альтернатива была либо собственный автомобиль, либо тесный и неудобный общественный транспорт, либо такси. Каршеринг, на мой взгляд, как раз стал тем отличным компромиссом, который при большинстве преимуществ личного автомобиля лишён его недостатков. И, являясь общественным транспортом, лишён его минусов.
В чём преимущества каршерига: • Вы едете на собственной машине за рулём, куда хотите и как хотите • При этом вы не думаете о ремонте, страховке, смене резины на зиму, мойке и заправке • В отличие от такси, вам не приходится соседствовать с непонятным персонажем за рулём (для меня, как для неистового мизантропа и интроверта — решающее преимущество) • Вы не думаете о том, что если вы в одну сторону поехали на машине, то на машине-же придётся возвращаться и назад (как пришлось бы сделать со своей машиной)
Остановлюсь на последнем аргументе подробнее. Любой большой город с точки зрения передвижения в пространстве анизотропен. Для гуманитариев поясню: из точки А в точку Б может быть добраться очень быстро. А вот из точки Б в точку А — наоборот, очень долго. Поэтому очень может быть удобно с окраины в центр утром ехать на метро, а вот потом ехать на окраину может быть удобнее на машине. И при этом ещё и в магазин заехать за покупками. Ещё для любителей выпить может быть личная анизотропия. Когда на работу лучше ехать на машине, а вот обратно — на такси. В общем сценариев, когда “туда” и “назад” ехать может быть удобнее по-разному — миллион. Со своей машиной приходится думать о том, что если туда поехал на машине, то на машине-же придётся возвращаться обратно. А с каршерингом таких проблем нет.
Ещё об одном преимуществе каршеринга я, к сожалению, я узнал на собственном опыте. Я в аварии попадал очень мало в своей водительской практике. Но знаю, что какой бы хорошей не была страховка на машину, авария — в любом случае большая проблема. Нужно думать о сервисе, о том, как эвакуировать машину, о том, как потом продавать машину и так далее. В общем, с аварией проблемы только начинаются. А с каршерингом оказалось всё очень просто — вызвал ГАИ, получил справку, оставил разбитую машину на обочине, прошёл 200 метров до другого каршеринга и поехал дальше, забыв про аварию, как страшный сон. В общем, Car as a Service (CaaS), см. эпиграф.
Я далёк от идеализации каршеринга. И у него есть, конечно, куча минусов в сравнении со своей машиной: маленький выбор марок и моделей, необходимость всякий раз настраивать машину для себя, отсутствие детских кресел (частично решается mifold — http://www.mifold.com/) и т.д. Но, на мой взгляд, все эти недостатки нивелируются преимуществами.
Я очень рад, что в Москве правительство поддерживает каршеринг (где тут раздают печеньки за поддержку Собянина?) и надеюсь, что на этом они не остановятся. Конечно, вместе с остальным общественным транспортом, каршеринг вряд ли пустят на выделенки (и, наверное, правильно, так как скоро каршеринга будет уже столько, что это бы обнулило все преимущества выделенных полос). Но вот сделать выделенные парковки в центре для каршеринга было бы очень правильно. Ну или хотя бы повысить цены на паркинг до 300–500 рублей в час и отменить бесплатную парковку по выходным было бы ещё лучше. И сделать парковку платной во всём городе, включая дворы — только так можно было бы победить пробки. А ещё лучше сделать, как в Японии — где купить машину можно только предоставив документы на принадлежащее вам парковочное место. Я очень надеюсь, что со временем так произойдёт и в Москве (хоть и вряд ли до выборов).
Самим компаниям-операторам каршеринга тоже есть, куда развиваться. Машин должно быть сильно больше (что и так происходит), приложения должны быть сильно проще и удобнее. Нужно не воевать друг с другом, а наоборот, объединяться с помощью агрегаторов. Что-то делать с суточными тарифами (которые сейчас просто заоблачные), да и с тарифами вообще — хотя это и так сейчас происходит вместе с растущей конкуренцией.
Напоследок, расскажу теперь о своём опыте использования разных компаний каршеринга. Начну с российских:
К YouDrive у меня особое отношение, ведь именно с них начался мой опыт использования каршерингом. Очень удобное приложение, очень классные машинки (тогда были только Смарты, сейчас ещё и BMW, Mercedes, Mini, и т.д.), очень классная (на тот момент) поддержка в Телеграме, интересная программа лояльности, маленькая зона завершения аренды, создававшая хорошую плотность машин в центре города и в местах скопления клиентов, возможность “передать” машину за пределами зоны завершения (услуга вообще уникальная, и у нас и в мире), с всегда заправленными и с омывайкой машинами. В общем всё здорово! Но, к сожалению, за год YouDrive сильно испортился. Сначала постепенно лишили почти всех преимуществ обладателей высоких статусов в программе лояльности (а я ведь когда-то был готов на такси или другом каршеринге доехать до машины YouDrive, чтобы повысить свой статус), отменили бесплатную ночную парковку, потом расширили зону завершения аренды, из-за чего машин стало резко меньше. А, например парковку в аэропортах и нормальные суточные тарифы так и не сделали… В общем из очень лояльного пользователя YouDrive я превратился в того, кто при прочих равных выберет не его, а того, кто будет ближе. Но всё равно отдам им первое место в своём рейтинге, за прошлые заслуги и в надежде, что они исправятся и повернутся к лояльным пользователям лицом. А иначе очень скоро рискуют опуститься в рейтинге ниже даже Делимобиля.
Появления Яндекс.Драйва боялись все игроки рынка каршеринга. И боялись не зря! Они сразу вышли с очень большим парком машин и продолжают его активно расширять, с удобным приложением, агрессивными ценами (иногда — так как ценообразование там динамическое), хорошей (хоть и не сразу) поддержкой, парковкой в аэропортах (может и не всех, но для меня всё равно в Москве кроме Шереметьево нет аэропортов). Очень интересная фича — интегрированные в машины планшеты с сервисами яндекса, в том числе и навигацией с пробками. Но есть и огромный (для меня) минус — в Яндексе не то, что нет суточных тарифов (вон, у YouDrive их тоже нет), но в принципе нельзя брать машину больше, чем на сутки. Почему? Это же абсолютная глупость — любому каршерингу выгодно, чтобы машина была в аренде, а не стояла бесплатно на улице. Загадка. Но в Яндекс я верю, они очень быстро развиваются, поэтому я надеюсь, что с расширением парка и улучшением сервиса они выйдут на первое место.
Белку я с самого начала невзлюбил. Их действия на рынке очень часто вызывали ассоциации с идиомой “белка-истеричка” (я сейчас про их демонстративный выход из независимых агрегаторов каршерингов — хотя при этом они противоречиво остались в Яндекс.Транспорте). Но со временем, посмотрев интервью одной из создательниц сервиса (рассказавшей, что сервис они создали “на собственные деньги и деньги бывших мужей”) я как-то проникся. Начал пользоваться и постепенно втянулся. Я знаю, что у Белки больше всего преданных пользователей, может быть со временем и я сам стану таким. По крайней мере Belka Black с мерседесами — хороший заход на это. А ещё неплохое приложение, множество машин, более-менее разумные суточные тарифы, парковка в аэропортах — всё это сильные козыри. Третье место.
Anytimecar — самый старый каршеринг в России и в Москве. Он был для меня вторым, который я попробовал после YouDrive и был, конечно, очень неприятно поражён. После “хипстерского” YouDrive, где всё было сделано для людей и как удобнее, Anytimecar со своим крайне убогим приложением, топливными картами, требованием звонить в поддержку, чтобы заправить машину (и машинами с вечно пустыми баками и без омывайки, как следствие) — всё это создавало ощущение, что после магазина Apple Store попал в сельпо, где продавец в каждом покупателе видит вора. А ещё требование фотографировать машины после каждой аренды — до такого не догадался ни один каршеринг! Но в плюсах уникальные вещи: возможность подать машину по нужному адресу, нормальные суточные тарифы и интересный выбор самих машин (они были первыми, кто предложил машины более-менее нормального уровня). В общем, Anytimecar я всё равно любил, как пионеров, но совсем не удивлён, что их в итоге купил Делимобиль. Жаль, что это был Делимобиль…
Самый большой в России и периодически самый большой в Москве каршеринг. И самый убогий. Hyundai Solaris, особенно предыдущего поколения — это не машина, а недоразумение. Renault Kaptur — ещё хуже. Такое ощущение, что все инженеры Рено собрались вместе на мозговой штурм с целью сделать максимально убогую машину. Очевидно, что партнёрство с ВАЗом не прошло даром, поэтому им это удалось! А когда Делимобиль ещё и заклеил все машины рекламой ЛДПР, они навсегда в моём рейтинге опустились на последнее место. Поэтому убогое приложение и неухоженные машины я даже не буду упоминать. Но, к сожалению, чаще всего ближайшей машиной будет именно Делимобиль, поэтому избежать пользования этим сервисом невозможно. Приходится терпеть.
В Москве есть ещё куча каршерингов: всякие Rentmee, Car4You, TimCar, EasyRide, Car5, MatreshCar и т.д. но, к сожалению, у них настолько мало машин, что шансы оказаться им рядом с вами — крайне малы. Поэтому в них можно даже не регистрироваться, а вместо этого воспользоваться агрегатором — YouDrive, в котором большинство из них уже представлено (агрегировать убогие сервисы — это был отличный ход YouDrive!)
В мире я ещё попробовал Car2Go, ZipCar, DriveNow и ещё несколько совсем локальных сервисов, вроде каршеринга электромобилей в Копенгагене. Тут помогло наличие немецких прав — не уверен, что с российскими правами будет просто там зарегистрироваться. И сравнивая наши сервисы с международными могу сказать, что наши сильно лучше! Всё-таки конкуренция — великая вещь. У наших и тарифы сильно ниже, и приложения значительно удобнее. Но вот что помогает нашим западным коллегам — так это то, что они основаны с большим участием или самими автопроизводителями — поэтому прямо в бортовой навигационной системе видно, находитесь ли вы в зоне завершения аренды или нет, машина сама прокладывает путь к заправке, если топлива остаётся немного. А ещё мне очень понравилось в смартах Car2Go, что там нельзя регулировать наклон спинки и высоту сиденья — только расстояние до руля. Благодаря этому не нужно долго настраивать сиденье под себя после любителей поездить лёжа, например.
Но в целом, конечно, концепция каршеринга находится в самом начале пути. Я признаюсь честно, что являюсь ретроградом и не верю, что уже через пять лет, как утверждают некоторые, традиционные автомобили вытеснят self-driving cars. Я думаю, что удовольствия поездить за рулём самому ещё хватит на наш век. А вот владение собственным автомобилем в городе, думаю, скоро станет таким же нонсенсом, как владение лошадью. Машина должна ездить, а не стоять 22 часа из 24-х на парковке.
В общем, всем рекомендую регистрироваться в каршеринге и начать его использовать. У большинства пользователей каршерингов есть своя машина, так что не бойтесь попробовать. А со временем понравится и вам, надеюсь, захочется от своей собственной машины отказаться, тем самым внеся свою лепту в уменьшение пробок в городе.
А после вам (как и мне) захочется попробовать и велошеринг и самокатшеринг. И даже прокатиться на автобусе. И окажется, что современный общественный транспорт не так уж и плох. И что жить в городе можно и удобно и комфортно, не создавая проблем другим.
Александр Ложечкин (https://medium.com/@allo/%D0%BE-%D0%B7%D0%B0%D0%BC%D0%B5%D1%82%D0%BA%D0%B0%D1%85-ce3a8bdd56af)
116 
4
116 
116 
4
Александр Ложечкин (https://medium.com/@allo/%D0%BE-%D0%B7%D0%B0%D0%BC%D0%B5%D1%82%D0%BA%D0%B0%D1%85-ce3a8bdd56af)
"
https://medium.com/@genvill/your-mobile-application-strategy-might-be-stalling-here-s-why-9da0cb4057ab?source=search_post---------128,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gen Villamin
Mar 25, 2018·3 min read
Most CEOs want to have a mobile version of their website or application — in fact, 87 percent of them does- according to a recent study by Accenture. Sure lots of people in the office adopted BYOD, and you think this increases productivity and gives people flexibility. Of course, you are right. You just missed one vital information — how does this fit into your overall IT strategy?
You might not realize it now, but four areas are often neglected once you started bringing in too many mobile applications and devices — security, cost of integration with back-end systems, processes overhaul and compliance to company policies. Often, a company’s enterprise mobile strategy isn’t sound.
For example, your ERP might be initially accessed through the company intranet — or you can probably connect via VPN if you’re outside the office. But now, since you made a mobile version of it — people start accessing it over their phones. But how do you treat confidential data? When an employee decides to submit project financials from his iPhone connecting to the servers of your ERP, is this data encrypted during transit? Once it has reached the servers — how will this be decrypted? Six months, a year, or five years from now, how will this be stored?
These are security gaps that hackers can easily exploit if your enterprise mobile strategy… isn’t a strategy so to speak. How will you mitigate this? You would probably say, well I will just find a suitable security software and have my IT guy install it. Probably not the wisest decision. The key here is choosing a combination of the right platform, infrastructure, and developer framework.
In architecting your enterprise mobile strategy, begin with infrastructure. Choose a secure Infrastructure as a Service (IaaS) — such as Azure because it protects not only data at rest but also data in transit.Second, in doing a mobile version of your legacy systems — opt for a framework that is secure and cross-platform ready. Xamarin, an application development framework will allow you to securely and not to mention, quickly create a mobile application that is readily available to iOs, Android and Windows devices using up to 75% of the same code base.
Xamarin integrates seamlessly in Azure so you are sure that even the mobile version of your ERP can safely push and pull information regardless of its internet connection. The unified integration of Azure and Xamarin allows you to leverage the same security features of your Active Directory, Single-Sign-on, and even social media integration.
So here’s a quick take away: remember, modernizing your enterprise systems through a mobile application isn’t enough. A sound IT strategy must be in place to weave this in your current architecture. Doing this isn’t easy- that’s why you should start by procuring a good product, like Microsoft Azure and Xamarin.
Originally published at cogitoph.com on March 25, 2018.
An agile project manager navigating her way around machine learning
See all (796)
1 
1 clap
1 
An agile project manager navigating her way around machine learning
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@shereef-pt/what-is-saas-iaas-paas-cloud-computing-3efa26a61dcf?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nilmar Shereef
Apr 21, 2020·5 min read
Cloud computing ! One of the widely used terms in this century. I think at least on some stage even a non-technical person should also be aware of this term. Cloud computing plays an important role in this modern century and a lot of pro types have been going to air as we speak.
Cloud computing is a type of computing that relies on shared computing resources rather than having local servers. In other words, It is simply storing and accessing data and programs over the Internet instead of your computer’s hard drive. The cloud is just a metaphor for the Internet. Programs may be a specific application, or visualization tool or something else.
Cloud Server: Someone else’s server which you connect to remotely.
Why Cloud?Cloud computing has some tremendous advantages compared to normal computing methods. Its advantages can answer all the questions related to its benefit as well. Let’s mention the major 4 benefits of cloud computing.
Moreover, In terms of the factors like Maximum Asset Utilization, Backup Management and Migrations Support, Cost-Effective & Collaboration efficiency, Cloud computing is much better than any on-premise server system. Cloud computing is the technology and cloud server is the server where the data/programs are stored remotely.
How can I get the Cloud Server?You can simply purchase your server from any cloud providers and its working models and nature purely depends upon your needs. Generally, AWS, Oracle Cloud, Google Cloud, Microsoft Azure & IBM are good examples for cloud providers.
What are the THREE MODELS or CATEGORIES of cloud computing services?
Cloud models are mainly 3 types:
1 >> IaaS (Infrastructure as a Service)
2 >> PaaS (Platform as a Service) &
3 >> SaaS (Software as a Service).
Each of the cloud models has its own set of advantages and characteristics that could serve the needs of various types of users.
1: IaaS (Infrastructure as a Service):The cloud provider will provide required hardware, storage, servers and data center (DC) space or network components. Simply it covers Infrastructure, Network & Virtualization.
Eg: Digital Ocean, Linode, Rackspace, Amazon Web Services (AWS), Cisco Metapod, Microsoft Azure, Google Compute Engine (GCE)
2: PaaS (Platform as a Service):The cloud provider will provide all IaaS benefits and required operational platforms such as operating systems, middleware etc… Each of these functions is operated, owned, configured and maintained by the cloud service provider.
Eg: AWS Elastic Beanstalk, Windows Azure, Heroku, Odoo sh, Force.com, Google App Engine, Apache Stratos
3: SaaS (Software as a Service):Here, The provider brings all features of PaaS indirectly and provides a single or set of software functionalities to the customer. The technical risk is here less and this is the more optimized way.
Eg: Google Apps, Dropbox, Salesforce, Cisco WebEx, Odoo Online, GoToMeeting
Cloud Models : Real world example:
Let’s go through each model with real-world examples for better understanding.
Suppose you are looking for a residential apartment for rent, You may select a non-furnished plain apartment/villa having only essentials things like electricity, Water, Building structure, etc and you can customize and partition as per your needs. You can bring new sofas, Beds, Cupboards at your own risk after the rental/purchase This method is IaaS.
Next choice is having a semi-furnished flat/villa for rent, Where apartment with all facilities of the first choice and additionally kitchen setups and essential furniture like cupboard, bedroom …etc. This method is PaaS.
The last option on the plate is having a full-furnished flat/villa for rent, equipped with all required items of a household. This is the top layer of the concept. From sofa to the table lamp, from bed to dresser and even electronic appliances like washing machines, utensils, and glassware in the kitchen. This method is SaaS.
Is it secure ?The security matters can also draw here. When you hire any of the above-mentioned apartments, You/ apartment owner will place a legal agreement that describes all the sections including accessibility, financial settlements, validity, services, security measure, etc…
The security completely relies on that legal agreement. Note that the real apartment owner cannot interfere with the moments of your lifestyle with your family at the apartment. However, the owner can help you with the service maintenance of your Air conditioning or water supply as per your ticket/demand. And again, Which is completely relies on the agreement made between you and the owner.
In cloud computing , Contract, Service Level Agreement(SLA), Disaster Recovery, Data Security, Non Disclosure Agreement (NDA) etc.. are the examples of legal documents.
So, The cloud computing security is completely depends upon the contract between the provider and consumer and cloud provider data secure methods.
Comparisons: Based on Terminologies Provided by Cloud Providers :
The table below is a brief structure of service providers with the terms. The blurred terminologies will be provided by cloud providers and the remaining terms will be set up by consumers or customers based on their requirements.
Conclusion:
Throughout this blog cloud computing and its models are described with real-world examples. Not that there are several models of clouds like Storage as a Service (SaaS), Communications as a Service (CaaS), Network as a Service (NaaS), Monitoring as a Service (MaaS), etc.. Also some providers are providing mixed nature of the cloud models. All of them are selected by the customers based on their requirements.
Thank You for reading.
— Nilmar Shereef
Sr.Odoo ERP Techno-Functionalist/Python Enthusiast/Software Engineer/ERP Implementer
See all (26)
69 
69 claps
69 
Sr.Odoo ERP Techno-Functionalist/Python Enthusiast/Software Engineer/ERP Implementer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@heliossolutionsno/microsoft-azure-markedsandel-er-nummer-to-etter-amazons-aws-i-cloud-iaas-ffb90d804d15?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
Helios Solutions
Mar 9, 2017·2 min read
Siden lanseringen i 2010, har Microsoft Azure vokst sikkert og visst i offentlig skyvirksomhet, og er rangert som en nær nummer to til industrileder Amazon Web Services. Selv om Amazon er skyens leder, vokser Azure i et raskere tempo, som får eksperter i industrien til å spekulere i om Microsoft til slutt ta igjen Amazon.Om Microsoft tar igjen Amazon eller ikke, er det av største betydning for bedriften din å ha en fordel i skyen. Samarbeid med en tjenesteleverandør av cloud computing og ta din bedrift til et høyere nivå.
Microsoft Azure ble utgitt i februar 2010 som Windows Azure etter sin kunngjøring i oktober 2008, og ble i mars 2014 omdøpt til Microsoft Azure. Amazon Web Services (AWS), på den annen side, ble lansert i 2006, og denne første-mover-fordelen har betalt seg pent for Amazon. AWS har sikret seg en dominerende plass i skyen med omtrent 30 prosent markedsandel og Azure holder andre plass med om lag 11 prosent. Den totale lagringskapasiteten til AWS sies å være større enn den samlede lagringskapasitet til alle de andre infrastrukturene i skyen, somsier noe om hvor Amazon står i forhold til sine konkurrenter.
Siden Microsoft lanserte Azure, har de nektet å avsløre sine virkelige inntekter. Men Wall Street Journal rapporterer at en analytiker ved JP Morgan har tatt en gjetning på at Azure har trukket inn 2,7 milliarder dollar (ca.) i inntekter i 2016. Dessuten snakket Microsoft om at inntektene i Azure hadde vokst med 93 prosent i løpet av årene i fjerde kvartal, 2016. Sammenliknet var AWS i den aktuelle perioden bare opp 47 prosent. Microsoft har også bekreftet at omsetningen i forrige kvartal skøt opp til 116 prosent.
Som per diskusjonen over ser det ut til at Microsoft og andre utfordrere kommer til å ha vansker med å avsette dagens regjerende leder av skyen, Amazon. Microsoft vil måtte avhenge av strategisk planlegging fremforkortsiktig fortjeneste for å kjempe bort Amazons lederskap, med deres betydelige forskjeller i kundebase, størrelse og inntekter tatt i betraktning. Amazon setter nå trenden i prisreduksjoner og Microsoft har bekreftet at de vil matche disse kostnads rabattene. Og for å forsterke sin skyposisjon taper Microsoft marginer i andre forretningsområder.
Se etter mer ekspert informasjon om Software Utvikling Specialist, Cloud Computing Tjenesteleverandør og Sharepoint Web App Utvikling.
Originally published at https://www.linkedin.com on March 9, 2017.
"
https://medium.com/cooking-with-azure/ansible-maintenance-of-acs-clusters-928d30feb9f4?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Azure Container Service clusters are IaaS-based container orchestration framework clusters (as of writing, DC/OS, Kubernetes and Swarm are the supported frameworks) made easy to deploy on Azure.
While a great way to spin up quickly production-grade clusters in the cloud, it still leaves a bit desired in the “day 2” operations: the managing, upgrading and securing of the cluster itself (that is, the underlying Infrastructure-as-a-service). For instance, an ACS-deployed cluster comes at this very moment with the 1.6.6 version of Kubernetes, while the latest community stable version is 1.7.0; Azure engineers are currently completing the necessary validation and QA for the latest and greatest version of Kubernetes, but what if you decided you wanna play with fire and use the cutting edge version? I decided to write a quick ansible script to automated the upgrade process, which normally would involve SSH’ing into each and every master and client node and bumping up the version of the hyperkube container (the all-in-one kubernetes server component, conveniently packaged as a docker container) thru the manifests at /etc/kubernetes/manifests.
Prerequisites: an ACS-deployed Kubernetes cluster, a working Ansible installation, the azure-cli 2.0
For an ansible playbook to run, we need to first build an inventory file. While there’s an official, generic-purpose dynamic inventory file in the official Ansible repository here, I came up with (a very embarrassing!) shell script that calls the azure cli to populate the inventory file with the masters and the slave nodes, you can find it here. Simply run:
The result should be similar to:
Note the ProxyCommand directive: as stated in the Readme, since the only publicly accessible node is the master node(s), we need to tunnel the SSH connection thru it to the agent nodes. So make sure you use the ssh-agent before running the ansible-playbook commands:
Now, edit the vars.yml file to reflect the starting and final version of Kubernetes you want to install:
Now run the playbook:
All kubelets and the api/scheduler/controller services will restart, so your cluster will be ready to go in a couple of minutes (regardless of the size; ansible changes the systemd files in parallel).
Kubernetes has a default maximum value for the number of pods on a node set to 110, regardless of the size of the node. This value can be increased but requires changing the systemd unit file on each node; Ansible comes to rescue and I wrote a simple playbook just for that, it’s here. Change the value in vars.yml and apply:
You can use ansible right from the Azure Cloud Shell:
and add to ~/.bashrc:
All things Azure
1 
1 clap
1 
All things Azure
Written by
Software engineer at Microsoft, public speaker, community organiser and mentor. Opinions are mine’s, facts are facts.
All things Azure
"
https://medium.com/strongnode/ama-with-satoshi-club-on-being-blockchain-agnostic-and-network-growth-via-innovation-lab-110ccb814fdf?source=search_post---------307,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 8, 2021·6 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito joined Satoshi Club for a brand new AMA episode last 20 September 2021. CEO Saito responded to questions about our company and shared news about the future IDO launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com in a few weeks’ time.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Here are some of the highlights from the StrongNode and Satoshi Club AMA episode:
STRONGNODE INTRODUCTION
Daniel Saito: StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute resources like CPU/GPU cycles, network bandwidth, and storage from digital devices. This compute is then harnessed to process some of the worlds challenging compute problems such as offering cost effective compute resources for genome sequencing analysis for COVID and other compute heavy solutions like machine learning and AI. We are working on “Computing with a Purpose.”
The premise of this project is that I want to be the first. I had the opportunity to be the first mover on a lot of things. Technology caught up and people became specialized in this field.
You have to find your niche as they say. Mine is in security/encryption, infrastructure and database.
Jonah 🏅🎹 | Satoshi Club: As I also know that Cross-chain utility is important for many different reasons. In your network, Each blockchain has offered STRONG NODE benefits that YOU want to maximize on speed, versatility, and scalability of the network. And YOU are planning to roll out to Polygon/Matic with different chains coming onboard as different strategies and use cases dictate. My question is, what other chains are coming onboard? What other benefits are you expecting from different block chains when you roll out to polygon/Matic?
Daniel Saito: Great question, @splend6ty you deserve a cookie 🍪! We are releasing on POLYGON due to the recent usage and projects that are popping up in the ecosystem (still early, but not as bad). Yes, speed, versatility, scalability and most of all gas fees are critical considerations of choosing the right chain. POLYGON is kinda like that. We considered BSC, but we found that the ecosystem is rather tainted with a lot of noise and most of all really bad projects. I would say the versatility is great because the portability of the contract (with minor adjustments of course), documentation is pretty straight forward… But this is our launching pad, we will then move on to other chains like SOLANA. Like new chains come with new challenges, like Solana uses RUST (luckily, I love RUST). The implementation of how they handle nodes is great and we can see a future there more long term as long as they get their shit together (e.g. last week network reset doesn’t sit well with me)…
But we will eventually try different chains, but with each adoption of a chain brings new security vectors that open up. So we will put a lot of those data points into consideration when scaling out to other chains.
I read that StrongNode has an Innovation Lab, whose social impact, gaming and technology node innovators grow their network through various adoption strategies. My question is, what are those adoption strategies that StrongNode uses to keep their project growing exponentially? With this innovation lab, what benefits will your users see reflected in the long term and how will it help StrongNode consolidate position in the crypto space?
Daniel Saito: My job as CEO is to grow my digital footprint of my decentralized distributed network. To be able to get token distribution to as many wallets as possible which will in the long term help the value of the $SNE token. These social impact plays are a form of “forced adoption”. When working with various blockchain projects back in 2017, they all had an adoption issue. It frustrated us that no one wanted to be the first idiot to eat the “yellow snow”. We reached out to our closest entrepreneur buddies, and acquired and incubated the project over the years. These are all technology plays, so they need infrastructure to work on. Well this is our approach to forced adoption. Each roll out of a technology play helps our digital footprint grow.
We didn’t want to launch a project for the sake of launching a project, we wanted to be the first to address new niche sectors that would greatly benefit from the StrongNode network; but moreover that the project has a social impact — an impact that can change our everyday digital lifestyle.
I don’t want to reinvent the next facebook, to me that is boring — (we already worked with them early when they were still a college network for kids). Each project has a focus, OGLife is a network where gamers alike can relive the home lan party experience (hosting gaming servers at the edge) and as gaming builds a self serving community. Another successful project worth mentioning is Cryowar (https://cryowar.com/), we saw the potential early in its infancy and helped finance it to where it is now (their IDO is in October). For Cryowar, we are helping them develop the crypto hooks into their gameplay. Also all the folks playing Cryowar can earn $SNE tokens as we will tap into the Gamer’s PC and most Gamer’s PCs have that juicy GPU, which will open the doors to GPU harvesting.
The StrongNode token has so many use cases within the StrongNode ecosystem which includes our innovation lab projects focused on gaming, social impact, entertainment, lifestyle platforms building on the StrongNode Edge technology. You get paid for your idle resources and others get to run workloads, processed in chunks, on your idle resources. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. There are more mechanics yet to be announced that will enhance the appeal of token adoption.
Could you tell us a bit about the innovations that StrongNode’s innovation developer will seek to create? Will it only be focused on developing projects on a specific theme? or could they innovate any type of dApps?
Daniel Saito: A lot of our innovations will be around Open Source, we feel that we are TENETs of Open Source. With our largely successful exits from Redhat, MySQL and MariaDB. We have help build out Open Source to be accepted mainstream. There is 99% chance that you have touched one of my solutions that I help developed.
What we want to do, is we want to help build the building block to crypto, similar to how we made the building blocks to Open Source.
In doing so, we will use best of breed Open Source projects to build out StrongNode, whether if its the use of OpenID, Hadoop, OpenVPN there are a lot of great projects that can be adapted to the token economy. We are trying to fix the open source dev. not getting paid for his/her work.
…
Daniel Saito: Thank you, and you guys have a wonderful and intelligent community. Definitely coming back to do a POST IDO AMA.
🥮 http://strongnode.io
🗯 https://t.me/strongnodechat
🗣 https://t.me/strongnode
📚 https://twitter.com/StrongNodeEdge
🎟 https://discord.gg/Gk2ka3NCR6
📚 https://medium.com/@strongnode
Visit us at our telegram! For your consideration, please invest responsibly and do your own research!
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
309 
309 
309 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/microsoftazure/deploying-java-ee-apps-to-azure-part-1-e895284b46d1?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
There are a multitude of options for cloud based application development ranging from traditional IaaS (Infrastructure-as-a-Service), PaaS (Platform-as-a-Service) and CaaS (Containers-as-a-Service) all the way to Kubernetes and Serverless (and probably some more which I might be missing!). Think of it as a spectrum rather than a “one size fits all model”, with each option having its pros and cons. Ultimately, every scenario is unique and the final choice is driven by requirements — but its always good to know that you have “choices” at your disposal!
This is the first of a series of blogs that will walk you through one of the options of running Java EE applications on Azure. We will follow the most basic approach for deploying our Java EE app to an application server which is set up in a Virtual Machine on Microsoft Azure along with the Azure Database for PostgreSQL service as the backend database. In essence, this is the combination of IaaS (Azure VM) along with a PaaS (managed PostgreSQL on Azure)
Other options such as containers and Kubernetes will be covered in upcoming posts
The example used in the blog post is a simple three-tier application that uses Java EE 8 specifications such as JAX-RS, EJB, CDI, JPA, JSF, Bean Validation. We will use the Payara Server to deploy the application and use PostgreSQL as the relational database.
During the course of the tutorial, we will cover:
Except for minor changes, the application used in this tutorial has been adapted from this project by Reza Rahman
You will need a Microsoft Azure account and the Azure CLI to work through the tutorial.
If you don’t have a Microsoft Azure account, go ahead and sign up for a free one! The Azure CLI is a cross-platform command-line experience for managing Azure resources — please install it using these instructions.
Set your Azure Subscription ID using the Azure CLI which will be used for this tutorial.
To set your Azure subscription ID
Create a resource group that will contain all the services (resources) which you will create as a part of this tutorial. A resource group is like a logical container that holds related resources for an Azure solution. The resource group includes those resources that you want to manage as a group.
To create a resource group
Azure Database for PostgreSQL is a relational database service based on the open-source Postgres database engine. It’s a fully managed database-as-a-service offering which is available in two deployment options, as a single server, and as a Hyperscale (Citus) cluster
We will be using the single server option for the purposes of this tutorial
We will use the az postgres server createcommand to create a Postgres server instance on Azure. First, set up some of the server properties such as the name, admin user, etc.
For storage and SKU options, please refer to the documentation
And, then invoke the command to initiate the database instance creation:
The provisioning process will take a few minutes.
To check the details of the Postgres database instance you just provisioned, invoke az postgres server show command
You should get a JSON response. Please note down the value for the fullyQualifiedDomainName attribute as you will be using this to connect to the Postgres instance later.
It should be of the format: [AZURE_POSTGRES_DB_NAME].postgres.database.azure.com
We will use a Virtual machine on Azure to host the Payara JavaEE application server. To be specific, this will be a Ubuntu based Linux VM.
Let’s start by setting up the required information for the VM
We will use the az vm create command to create the VM instance
The VM provisioning will take a few minutes.
You need to get the public IP address of the VM. Do so using the az vm list-ip-addresses command
You will see a JSON response — take a look at the publicIpAddresses section and note down the value of theipAddress property. Configure it as an environment variable as you will be using it in the subsequent steps
The Postgres database is not accessible by default. Use the az postgres server firewall-rule create command to create a firewall rule to explicitly allow the VM to access the Postgres instance. This will allow the JavaEE application deployed inside the VM to communicate with Postgres.
Payara Server is an open source application server derived from GlassFish that supports reliable and secure deployments of Java EE (Jakarta EE) and MicroProfileapplications in any environment: on-premise, in the cloud or hybrid.
Check out the project on GitHub or dive into its documentation to learn more!
SSH into the Linux VM you just provisioned using the username and VM IP
Enter the password once prompted. Once you’re logged into the Virtual Machine, proceed with the next steps.
Before installing the Payara server, we need to set up a few things such as JDK, etc.
We are using Payara server version 5.193.1 which is the latest at the time of writing this tutorial. The setup simply involves downloading and extracting the server zip file.
To confirm, run ls ~/payara5/
Start the server using asadmin
It will take a few moments for the server to boot up. You should see the following logs:
Now that we have the VM as well as Payara server up and running, we can now deploy our application.
Start by cloning the Git repository
The web.xml file (under javaee-on-azure-iaas/src/main/webapp/WEB-INF) needs to be updated with the JDBC URL for the Postgres database on Azure. This is present in the <url> attribute of the <data-source section and its format is as follows:
Here is the list of placeholders which form a part of the JDBC URL:
Set the required values
Simply use these commands to replace
Here is an e.g. of what the <data-source> section will look like:
The application is now configured. Let’s build it!
You should have the WAR file available. To confirm
As a final step in the application setup process, let’s download the JDBC driver for Postgres and add it to Payara server
We are using driver version 42.2.8
Add the JAR to Payara, simply invoke asadmin add-library
Finally, to deploy the WAR file, just copy it to the domain autodeploy folder
The deployment will take some time. In the meanwhile, you can track the logs using:
You should see log messages indicating successful deployment of the javaee-cafe application
It’s time to test drive the JavaEE app! To start off, we can access the application using a web browser. But, just like the Postgres instance, the virtual machine which hosts the Payara server along with the application is also protected by default i.e. you cannot access it from the public internet.
We need to create a firewall rule using the az vm open-port to access it from our local machine. We just need to expose port 8080 since that's the default HTTP port which Payara server uses
Use your browser to access http://[ENTER_VM_IP]:8080/javaee-cafe. You can use the UI to create, delete and see coffees.
The application also exposes a REST API for creating, deleting and listing coffees.
Create coffees
Get all coffees
You should see a JSON response listing both the coffee options you just added
Get a coffee by ID
Delete a coffee by ID
Notice that cappuccino is now deleted
Once you are done exploring the application, you can delete the resources. Since we used a resource group, it's easy executing a single command.
Please be aware that this will delete all the resources in the group which includes the ones you created as part of the tutorial (VM, Postgres etc.) as well as any other service instances you might have if you used an already existing resource group
You learned how to deploy a Java EE application to Azure using an app server deployed to a Virtual Machine along with a managed database offering for long term persistence.
As mentioned earlier, each option comes with its own pros and cons. In this case, you have complete control over your application, its deployment infrastructure, the way you scale it, etc. On the other hand, remember that managing the infrastructure, sizing it for your application, securing it, etc. is a set of responsibilities that you have to take on along with delivering core business value as a part of the app functionality.
The next part will dive into how to use a Docker container platform to deploy your Java EE applications. Stay tuned!
Any language.
101 
101 claps
101 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@nandhini-aitec/day-30-a-gentle-introduction-to-gcps-iaas-paas-and-saas-9c971eb4c835?source=search_post---------239,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nandhini N
Feb 1, 2021·3 min read
We have seen a series of ML models for outlier detection and supervised learning(classification/regression). Before proceeding further, it would be interesting to get familiarised with some of the concepts in Google Cloud platform. Later, we can apply these details while deploying an ML model on GCP.
Table of contents:
Difference between IaaS, PaaS and SaaS: These three components combined together are termed as building blocks of any cloud computing. The categorization is done based on the varying degree of users control over the hardware, software and code development. Infrastructure as a service(IaaS) is the basic and bottom-most layer where the subscriber has complete control over managing the hardware. The Cloud provider supplies the hardware, networks and storage systems. It is up to the requestor to decide how these elements have to be controlled(operating system & virtual machine allocations).
But not everyone appreciates the complete ownership especially developers, who are keen on creating applications and writing codes. They want to give the full attention to improvising the code performance, without bothering about what is going on under the hood of the computing process. For such needs, Platform as a service has been laid out, where the cloud service directs the Virtual machine allocation and storage process.
The final layer is Software as a Service for which end users access the developed applications directly. For instance, Gmail is a SaaS for users who have the mail account. The customers need not have any expertise in the technical or functional process. All they need to know is how to access the fully created apps. This is the top-most layer of cloud services.
Compute Engine(IaaS): It provides maximum flexibility of managing the hardware elements. It allows the user to run virtual machines on-demand basis(pay as you go, model)
App Engine(PaaS): This is excellent assistance for data scientists. The developers can create the codes in the local machine and deploy them on the cloud or the development can directly happen on the cloud using the python environment. Most of the users’ time is focused on model deployment rather than understanding the underlying infrastructure.
“App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. You can choose from several popular languages, libraries, and frameworks to develop your apps, then let App Engine take care of provisioning servers and scaling your app instances based on demand.” — from google doc
Cloud AutoML: Automated Machine learning reduces the manual effort placed on choosing the optimal model and tuning different hyperparameters. To illustrate, the requirement is to predict whether it will rain or not given a set of training data. As a data analyst/scientist, we try to dissect the data to find meaningful insights from the input features. This is followed by trying out different ML algorithms(along with hyper tuning) in order to decide which model outperforms the other. The concept of auto ml completely automates these manual steps. The system tries out the various algorithm, compares the outcome to finalise which model is effective.
Recommended Reading:
medium.com
AI Enthusiast | Blogger✍
AI Enthusiast | Blogger✍
"
https://medium.com/@david.allen_3172/when-i-look-inside-of-a-vm-i-see-the-metadata-google-internal-f0add6b9982?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Allen
Dec 16, 2018·1 min read
Asish Chandy Abraham
When I look inside of a VM, I see the metadata.google.internal host located at 169.254.169.254 — which is one network hop away, but not inside of my VM.
Now, I’m a bit speculating here, but in general IaaS (VMs) are built on top of something virtualization layer like VMWare (although I don’t know what Google is using, it’s probably not VMWare). The “hypervisor” starts VMs on top of physical machines. So you have the host OS (linux running the hypervisor) and you have the “guest OS” the VM itself.
The internal metadata server is probably running on the hypervisor and is extremely close by (i.e. could be on the host OS, and could be physically on the same machine) but because everything is virtualized, it appears to be on a different machine.
DNS resolution is a separate topic. By default you get a nameserver in your /etc/resolve.conf which is another IP located on the same subnet, so it’s the same thing again. A machine that’s very close by, maybe the hypervisor itself (probably).
Architect at Neo4j
1 
1 
1 
Architect at Neo4j
"
https://medium.com/@gyliu513/3-factors-for-successful-open-source-contributions-7c1eaa2238a8?source=search_post---------361,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guang Ya Liu
May 14, 2019·6 min read
Open source software is eating the world, with numerous projects that cover a breadth of technologies, from IaaS to PaaS to CaaS to Serverless, AI, and more.
With so many projects and so much technology, getting started in open source can be overwhelming. What project should you work on? How do you open source your own code? How do you make your project meaningful for customers? In this blog post, I share the three key aspects that you should pay attention to when contributing to open source.
Since 2012, I have worked in various open source communities and on many different open source projects, including OpenStack, Mesos, Kubernetes, Itsio, Kubernetes Federation V2, Knative, cluster-api, even becoming a maintainer for some of those projects.
The image above shows my journey in open source. It’s important to note that with every open source product that I worked on, IBM now has a product or clients using it. My open source contributions, product integrations, and clients moved me to the position of an open source maintainer.
Along my journey in open source, I have learned a few things along the way that I think will help if you are planning to contribute or already contributing to open source.
So, what makes a successful project? Based on my experience, I think we should evaluate if an open source project is successful or not by looking at two factors: the ecosystem that surrounds it and open standards and open governance.
We need to evaluate a project’s ecosystem by its users, sponsors and contributors.
This might seem obvious, but a project can’t be considered successful if there are no users using it. Open source thrives on contributors who can offer requirements that improve the project’s quality, so you need enough users who have insight into what would make the project better.
Having project sponsors who can amplify and promote the project in the community attracts more attention, and, thus, more users and contributors. I still remember how excited I was when I read the Istio announcement letter from Google and IBM. I, was working with a client for a traffic management issue with their microservices, and Istio saved the world at that time. With high-profile sponsors who have good reputations in technology and open source-the likes of Red Hat, IBM, Google-a project gives the project more potential to attract more people to join and contribute.
Having a diverse number of active contributors from different companies makes a project better and stronger. The chart below shows the contributors for Kubernetes. Notice there are contributors from almost 100 companies, a really large and diverse community.
Open standards mean standards that are freely available for adoption, implementation, and updates. For cloud native projects, we have standards like the Container Network Interface (CNI), Container Storage Interface (CSI), the Open Container Initiative ( OCI), and CloudEvents, just to name a few. The standards facilitate network management, storage management, container runtime management, cloud event management and the like. There are other examples as well. For instance, Kubernetes is becoming standard for container orchestration, Istio is becoming the standard for service mesh, and Knative is going to be standardized for serverless.
Open governance ensures that the way that a project is run is done in a truly open, collaborative environment. Open governance is the best way to ensure the long-term success and viability of open source projects.
Open standards and open governance are the key factors to avoid vendor lock-in or, even, project abandonment.
I think there are three key aspects to pay attention to when working in open source communities. These include contributing to open source, landing open source to product, and bringing open source to our clients, While creating open source projects is fairly easy, it takes understanding these three areas well to truly become an open source leader and expert.
Let’s look at these areas more closely.
You don’t have to become a core member, maintainer or committer to make a meaningful impact in an open source community.
In addition to submitting code changes and pull requests, another key part of becoming active in an open source community is to share knowledge or help the project move forward in other ways. In addition to tradition code changes, deletions, and contributions, other ways to contribute to a project include:
These are just a few small ways you can help the open source community and prove your worth as an expert.
While contributing to open source is good, the goal of any project we’re working on should be to take the technology and use it in real solutions that solve our customers’ problems.
As you’re working with open source projects, you need to account for how you can incorporate or integrate that technology into your product to help clients. We’re doing this now with how we use Kubernetes and Istio in IBM Cloud Private.
In the open source community, there are often many different open source projects that can provide similar functions. How do you choose the right project when there are so many projects? If we go back to 2016, we will see that people are still struggling to investigate how to manage container orchestration, they are trying to evaluate Mesos, Swarm, Kubernetes etc to see which one is the best. I won’t say which one I think is the best, but just to draw attention to the fact that, when faced with trying to choose which project is right for you and your product, you need to talk to clients for their requirements and select the best open source projects to meet those requirements.
As part of analyzing clients’ requirements, it’s important to abstract the general requirement for all of your clients and build an enterprise product that meets those requirements. IBM Cloud Private was born this way. We talked with clients, evaluated Kubernetes, Mesos, and other container orchestration projects, and — based on most of the clients’ requirements — we chose Kubernetes as the foundation for IBM Cloud Private.
Use your clients’ requirements as the guiding factor behind what open source projects you invest in. It can be easy to get lost in all the different open source organizations, with their many different project repos. While it can be hard to focus, considering your clients’ requirements can always drive us to find the right direction.
It’s imperative to have trust from your clients when trying to bring open source to a client. Becoming an open source leader or expert and having a mature product based on open source will definitely help clients trust you.
Every open source project has its own life cycle — it will either die or mature for production use. Whether a project dies or becomes widely accepted, we always need to innovate with new open source projects to resolve clients’ new requirements. Keep learning to know what your clients want, and keep innovating to help resolve your clients’ requirements.
I hope you’ve found these suggestions helpful as you embark on or continue your own open source journey. Make sure you are a good contributor to open source, use the clients’ requirements to dictate what projects you get involved in, and bring your open source technology to your product and to your clients.
Originally published at https://developer.ibm.com.
STSM@IBM, Member - IBM Academy of Technology, Kubernetes Member, Istio Maintainer, Apache Mesos Committer & PMC Member.
See all (5)
2 
2 claps
2 
STSM@IBM, Member - IBM Academy of Technology, Kubernetes Member, Istio Maintainer, Apache Mesos Committer & PMC Member.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nnilesh7756/what-are-cloud-computing-services-iaas-caas-paas-faas-saas-ac0f6022d36e?source=search_post---------1,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Nilesh Suryavanshi
Nov 8, 2017·2 min read
Today, everyone are moving towards Cloud World (AWS/GCP/Azure/PCF/VMC). It might be a public cloud, a private cloud or a hybrid cloud.
But are you aware of what are Services Cloud Computing provides to us ????
Majorly there are three categories of Cloud Computing Services:
a) Infrastructure as a Service (IaaS) : It provides only a base infrastructure (Virtual machine, Software Define Network, Storage attached). End user have to configure and manage platform and environment, deploy applications on it.
AWS (EC2), GCP (CE), Microsoft Azure (VM) are examples of Iaas.
b) Software as a Service (SaaS) : It is sometimes called to as “on-demand software”. Typically accessed by users using a thin client via a web browser. In SaaS everything can be managed by vendors: applications, runtime, data, middleware, OSes, virtualization, servers, storage and networking, End users have to use it.
GMAIL is Best example of SaaS. Google team managing everything just we have to use the application through any of client or in browsers. Other examples SAP, Salesforce .
c) Platform as a Service (PaaS): It provides a platform allowing end user to develop, run, and manage applications without the complexity of building and maintaining the infrastructure.
Google App Engine, CloudFoundry, Heroku, AWS (Beanstalk) are some examples of PaaS.
Below fig 1.0 while give you more idea on it.
d) Container as a Service (CaaS): Is a form of container-based virtualization in which container engines, orchestration and the underlying compute resources are delivered to users as a service from a cloud provider.
Google Container Engine(GKE), AWS (ECS), Azure (ACS) and Pivotal (PKS) are some examples of CaaS.
e) Function as a Service (FaaS): It provides a platform allowing customers to develop, run, and manage application functionalities without the complexity of building and maintaining the infrastructure.
AWS (Lamda), Google Cloud Function are some examples of Faas
Hope this gives you clear idea on what are Cloud Computing Services provided in market!!!!!!
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
See all (25)
935 
7
935 claps
935 
7
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technology-hits/what-is-the-cloud-exactly-5dbf6583308?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
For most people, the Cloud dries up in a set of acronyms, a pile of technologies that moves data and jobs away.
The scenario worsens when considering how most of the consulting offering is written: nonsensical…
"
https://medium.com/zero-equals-false/iaas-vs-paas-infrastructure-as-a-service-vs-platform-as-a-service-ade4fd55b027?source=search_post---------173,"There are currently no responses for this story.
Be the first to respond.
To begin with, many businesses are going online, and are relying heavily on the cloud to facilitate their clients which demands collecting, storing and processing a vast amount of data before it can be presented to the end user as information. This is where cloud-based web applications come in to play. In…
"
https://medium.com/manifoldco/coming-out-of-the-fold-announcing-our-15m-series-a-ab184d61351d?source=search_post---------100,"There are currently no responses for this story.
Be the first to respond.
Today we are announcing our $15m Series A financing, lead by OMERS Ventures and with the participation from BoldStart, Leaders Fund, Version One, Build Ventures, Amplify Partners as well as Alex Bard, Daniel Debow, Matt Wyndowe, Ameet Patel, Jonas Brandon and Gavin Uhma.
At Manifold, our goal is to give developers the freedom to use their favorite services on any cloud.
We believe you (and your team) should be able to explore a vibrant ecosystem of services, any of which you can add to your application with one click or command. We believe in a future without walls.
A developer’s toolbox shouldn’t be defined by their choice of monocloud.
Companies like Mailgun, LogDNA, Scout, Redisgreen and others are delivering a new class of services that provide better documentation, better support and a product velocity that means critical parts of your applications and infrastructure are constantly evolving.
With the rise of serverless and Functions as a Service, this trend is only accelerating. In the enterprise, developer choice is becoming as important as multi-cloud.
When you choose these services via Manifold, you get a single place to manage accounts, configuration and provisioning all while getting a single itemized bill at the end of the month.
No more wondering who has access to which service, or which credit card is paying which bill.
With our 1.0 release you and your team can find, buy and manage all of the services you need to create your next great app. We will be rapidly adding the services our users have requested in the coming months, services like Graphcool and IOpipe.
With the preview of our CLI, you can much more easily integrate those services in to your CI/CD environment or other workflow.
We hope you will sign up and try some of the amazing services on Manifold today.
We're determined to make it easy for developers to use the…
448 
448 claps
448 
We're determined to make it easy for developers to use the cloud services they love.
Written by
Co-Founder of @manifoldco
We're determined to make it easy for developers to use the cloud services they love.
"
https://medium.com/@deborah.martin/latest-infographic-shows-big-results-from-private-cloud-and-iaas-76af5d31f1d9?source=search_post---------267,"Sign in
There are currently no responses for this story.
Be the first to respond.
deborah.martin
Sep 21, 2016·4 min read
For many enterprise IT departments, a majority of their business customers are not satisfied with the speed of application releases[i]. Twenty-five percent of app development time is spent waiting for manual or semi-manual provisioned infrastructure, using traditional or even virtualized datacenter architectures. When users’ needs are not met quickly, they will independently look to public cloud services and create the shadow IT environments that can lead to cost, security, compliance and long-term SLA issues.
Enterprise IT needs to meet the challenge of transforming their infrastructure delivery processes to keep up with the new speed of business — and their competition. IT infrastructure must become more agile to rapidly respond to developer requests so users are not kept waiting. IT must also become more automated to reduce IT complexity and increase IT staff efficiency. How can you transform to rapid infrastructure provisioning with Infrastructure-as-a-Service (IaaS) and put the right mix of private cloud and public cloud services to work for your organization? What best practice capabilities are needed to ensure you can deliver on big reductions in both time-to-market and overall costs for developing and delivering new customer facing services and apps?
Hewlett Packard Enterprise (HPE) has just released the Deliver Infrastructure in Minutes and Speed Ideas to Market infographic that visually brings together the latest research on best practice trends driving IT to transform to IaaS, the capabilities being used to implement it and why, along with the real benefits seen from rapid infrastructure delivery using private and public cloud services.
It provides insight on how organizations are transforming their business to a hybrid infrastructure with cloud computing. This new hybrid infrastructure is providing faster self-service access to automated infrastructure services that meet both the reliable and efficient utilization needs of IT and the growing business demands for development, testing and production. The infographic also details six best practices and innovative capabilities recommended and used by other customers to deliver business agility and growth outcomes. These best practices and capabilities can be used to ensure your private cloud or IaaS solution meets the business’ expectations.
HPE leadership in private and hybrid cloud computing helps customers deliver VMs and infrastructure services in minutes, with all of these best practice capabilities and unique innovations that set the solutions apart. HPE has helped customers all over the world implement rapid infrastructure provisioning solutions, increasing their customer’s business agility and lowering their costs.
Ignitar (previously called MyCloud) is an Irish cloud services provider based in the suburbs of Dublin. Using HPE Helion CloudSystem they were able to get a private cloud up and running in just four weeks that allows Ignitar to automate cloud services for their growing client base — from infrastructure to application. It offered heterogeneous capabilities, making it compatible with both Microsoft Hyper-V and VMware, delivering a dual virtualization platform that is a differentiator in the marketplace. The HPE Helion solution helped Ignitar cut project delivery times in half for their clients, while simultaneously delivering significant cost savings for both their company and their clients. With its scalable HPE cloud, Ignitar’s operations are much more efficient because of dramatic improvements in the provisioning of additional resources, services, and applications that is now automated and repeatable.
HPE provides a range of solutions so you can find the right mix to meet your specific rapid infrastructure provisioning needs. HPE Helion CloudSystem provides an integrated hardware and software solution for hybrid cloud that enables you to monitor, manage and deploy across multiple hypervisors and IaaS providers. It speeds and simplifies the deployment and ongoing lifecycle management of your end-to-end IaaS solution with flexible configurations that to meet your workload needs and pair with existing environments. You can also incorporate on-premise resources for your private cloud in an easy pay-as-you-grow consumption model using HPE Flexible Capacity.
HPE cloud software solutions, such as Helion Cloud Suite, orchestrate and automate infrastructure services deployment and management processes in multi-cloud environments on your choice of hardware. Customers who want to build out their own choice of cloud infrastructure for their IaaS solution to meet specific needs can leverage industry leading options, from HPE cloud-enabled blade and rack servers to composable infrastructure using HPE Synergy.
You might also choose to have managed private cloud services in your mix to remove the complexity and capital expense of ongoing management of your cloud infrastructure so you can focus on your key business objectives. HPE Helion Managed Cloud Services offers dedicated or multi-tenant private cloud options that let you scale out instantly and affordably, all managed by HPE cloud experts and using enterprise-grade security.
Take a few minutes to read the recent research and guidance on this area in the Gartner paper Set the Stage for Successful DevOps with Combined IaaS and PaaS for Continuous Delivery.
See how you can make your IT into the department of yes and now, by visiting the Rapid infrastructure provisioning use case site.
[1] Forrester Research, Inc., “Embrace the Need for Speed to Avoid Ugly DevOps Practices” Brief, April 2015
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jinghua.shih/serverless-faas-3b607f0158fe?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
施靜樺
Feb 17, 2019·8 min read
順帶一提 SaaS, PaaS & IaaS
很久很久以前（大約 13 年前），「雲端運算」（cloud computing） 這個詞開始逐漸被使用並且蓬勃發展，很多相應的服務就此產生，被稱為 XaaS（X as a Service，X 可以以任何字替換）。 SaaS, PaaS 與 IaaS 是傳統三種分類方式，分別代表了軟體即服務（Software as a Service），平台即服務（Platform as a Service），與基礎架構及服務（Infrastructure as a Service）。沒有實際使用經驗的人（e.g. 半年前的我）一開始可能會有點難理解這些服務的差異，不過有很多很棒的平台寫文章介紹，比如 iThome 與 ProgressBar，推薦還不理解直接讀一讀。下面就不錦上添花，直接放一張以車為例子的示意圖。
簡單地說，現在各式各樣的應用程式（Gmail, 手機 app 等等）都是屬於 SaaS 軟體服務的範疇，而一個開發人員如果想要部署自己的程式碼，通常會選擇使用 PaaS 平台服務或是 IaaS 基礎建設服務。
好了，不負責任的 Serverless 簡單背景介紹結束，下面進入正題。
一般而言，工程師可以簡單分成開發（Development）跟維運（Operation）兩類。開發工程師負責實現商業邏輯，也就是撰寫程式碼，維運工程師則負責管理程式碼的基礎架構（伺服器、資料庫等），讓程式碼可以順利運行。
雖然現在許多企業都已經採用上面介紹的 IaaS，將網站基礎建設放到雲端，維運的負擔卻沒有減輕很多，還是必須處理資安問題、監控系統等等。
Serverless 是最近發展出來的概念，由 Amazon 在 2014 年發布的 Amazon Lambda 開先鋒，其他科技龍頭也在之後陸續推出相似的服務，像是 Google Cloud Functions，IBM Cloud Functions 與 Azure Functions。
這項技術的目的並不是為了實現真正意義上的「無服務器」，而是指由第三方雲計算供應商負責後端基礎結構的維護，以服務的方式為開發者提供所需功能，例如數據庫、消息，以及身份驗證等。簡單地說，這個架構就是要讓開發人員專注代碼的運行而不需要管理任何的基礎設施。程序代碼被部署在諸如 AWS Lambda 這樣的平台上，通過事件驅動的方式去觸發對函數的調用。
節錄自 從 IaaS 到 FaaS— Serverless 架構的前世今生。
Serverless 開發的一個好處是「用程式碼來控制程式碼」，也就是 Infrastructure as Code，以往維運工程師可能會是手動去介面調整參數，現在當這些工作變成程式碼，就可以享有很多好處，包含版本控制與 CI/CD（持續整合/持續部署）的流程。
那 FaaS 又是什麼呢？FaaS 的全名是「Function as a Service」（函式即服務），是 Serverless 實現的方式：
Serverless applications are event-driven cloud-based systems where application development rely solely on a combination of third-party services, client-side logic and cloud-hosted remote procedure calls (Functions as a Service).
節錄自 Hackernoon。
從上可以看出幾個重點：Serverless 是「事件驅動」（event-driven），由第三方服務（third-party services）、客戶端邏輯（client-side logic）與在雲端上的遠端程序呼叫（cloud-hosted remote procedure calls）所組成。這第三個組成的元素就是 FaaS。
名如其實，functions 就如同我們會在程式碼裡面寫的 functions，獨立、分離、自成一格。
每個請求都是獨立的。每次執行不會受到之前的執行內容/結果影響。任兩次對同一個 function 的呼叫可能會可能會在完全不同的容器（container）裡面執行。
FaaS 隨叫隨開，並且做完了任務就馬上關掉，需要的啟動時間與關機時間都非常的短（尺度是毫秒）。
雖然 functions 可以直接被取用，它們通常會從其他雲端服務被觸發，像是 HTTP 請求或是內部訊息通知。FaaS 比較常被使用的情境是雲端環境中各服務之間的橋樑。
你可以一次啟動好幾個 container 來同時呼叫同一個 function。就算同意時間有很多的請求（incoming request）也不怕。
Serverless 最大的優勢就是降低成本，這篇文讓我們看到可口可樂因此省了多少錢。Serverless 可以真的做到「按件計酬」—當沒有使用的時候就沒有花費，這也是它跟 PaaS 最大的不同之處，因為 PaaS 不會因為一個事件而開啟整個應用程序又結束，但是 FaaS 可以做到。不過，如果你的應用程式結構複雜並且會長時間運行，那麼使用 Serverless 不會幫你省到什麼費用。 Serverless 最適合的情境是是不固定、落差大的需求量（spiky demands）。
Serverless 省錢方便的好處也有它的犧牲，由於許多工作都交給了第三方管理，所以企業對產品整體的控制權會減弱，也會有安全性、災後回復的風險。
Serverless 可以將維運的負擔降到最低（比如說，不需要為不同的環境設定不同的機器），並且它與 Nanoservices, Microservices, SOA (Service-Oriented Architecture) 的概念是天作之合。同時，由於 FaaS 的高伸縮性，同時併發的請求們（concurrent requests）再也不是難題。
然而，由於 Serverless 還算是一個新的技術，再缺乏有經驗的人的引導之下，可能會導致個元件的碎片化（component fragmentation）、增加整以架構的複雜程度（complex ≠ complicated）還有增加測試的困難。
Serverless 也不是適合所有的架構。當你的應用程式依賴了很多第三方套件（3rd Party Dependencies），那麼表示當你打包這個應用程式時，它會變得很巨大，就不能發揮 FaaS 輕便簡潔的優勢了。
Serverless 是目前還正在如火如荼發展的技術，許多相關的配套與功能也都漸漸成熟。市面上也有出現了像是 Apex 讓你更輕鬆部署 AWS Lambda 的工具，看著看著實在很手癢，希望哪一天真的有實際使用到的機會。
雖然已盡能力所及地確保資料的正確性，但我恐怕還是會有不對/不精確的觀念或用字，如果願意指正我的話我會非常感激！
最近找資料的時候突然發現這個 free-for-dev 網站，裡面羅列了各種有提供免費服務的平台，並且持續更新中，很完整，很貼心。
還有順便看到這篇 導入 Configuration as code，完成三位一體，裡面將一個分成 infrastructure, code , configuration 三個部分，並探討如何將三個元素都程序化，存參存參。
hackernoon.com
github.com
无服务器架构（Serverless Architectures）amio.github.io
我不寫 CSS. https://codecharms.me
217 
217 
217 
我不寫 CSS. https://codecharms.me
"
https://medium.com/@alibaba-cloud/three-ways-to-manage-your-virtual-it-infrastructure-on-the-cloud-909e0729162f?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 10, 2020·3 min read
To learn more about the benefits of an Infrastructure-as-a-Service (IaaS) business model, download the Managing Your Virtual IT Infrastructure on the Cloud whitepaper today.
The world is undergoing a period of mass digitization, where many physical entities are transforming into online resources. For IT departments, this change can be difficult to manage where it requires you to adapt your infrastructure; creating new virtual servers quickly to meet the demands placed on your network, while decommissioning unrequired servers to maintain a cost-effective environment.
This is where an Infrastructure-as-a-Service (IaaS) platform can help. This form of cloud computing provides your virtualized computing resources over the Internet, relying on automation and orchestration technologies to help you manage your cloud environment with ease.
For example, when using our Elastic Compute Service (ECS) you can commission a virtual server with a few clicks, using the intuitive Alibaba Cloud Console to set up your subscription in order to help you meet your service and cost requirements. That’s not all you can achieve using our platform. Let’s explore three further ways you can manage your virtual IT infrastructure on Alibaba Cloud.
When you’re operating online, regular backups and instant recovery are vital capabilities to maintain your service levels and ensure your business resilience.
Regular snapshots are one way to achieve this, where our Express Connect service lets you create a link between an Alibaba Cloud Virtual Private Cloud (VPC) and your existing data center. The resulting hybrid architecture provides a resilient disaster recovery solution, where your online resources are available in any situation.
A virtual cloud environment provides you with a highly scalable and resilient storage solution, where a range of additional options exist to further boost your online capacity. In fact, using our storage solutions, you could achieve limitless capacity.
Our ApsaraDB RDS instances work across a range of database types, for example, working at up to 470GB of RAM. Using our Object Storage Service (OSS), you can create cloud-based storage buckets with unlimited capacities, and these objects can easily be managed via an API, SDK, or a web-based console.
All our storage solutions provide high levels of data durability, making multiple copies of the data with a data reliability of “eleven nines” (99.999999999%), which is considerably more reliable than a conventional RAID. You can also choose the region in which your data resides, in order to minimize latency and costs as well as to satisfy your regulatory requirements. At Alibaba Cloud, our global network operates in 21 regions and 63 availability zones, spreading your data across designated AZs to further boost your online security and capacity.
Monitoring is a vital capability to detect issues, conduct preventative maintenance, and maximize your service uptime.
Our CloudMonitor is a flexible monitoring service, for example; providing in-depth insights into your cloud deployments and helping you manage your online environment with ease. It uses automation to help achieve this, allowing you to protect your deployments from security threats, network issues or system failure in real time, by sending alerts through a variety of channels including social networks, email, text messages and instant messenger services.
Alibaba Cloud can help your organization create and manage its virtual IT infrastructure using our IaaS platform, helping you keep control of your costs, monitor your servers, create server backups and snapshots, automate your virtual infrastructure, and provision large amounts of storage.
To find out more, download the Managing Your Virtual IT Infrastructure on the Cloud whitepaper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@renebuest/hybrid-and-multi-cloud-the-real-value-of-public-cloud-infrastructure-f354c4df67df?source=search_post---------135,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·7 min read
Since the beginning of cloud computing the hybrid cloud is on everyone’s lips. Praised as the universal remedy by vendors, consultants as well as analysts the combination of various cloud deployment models is permanently in the focus during discussions, at panels and conversations with CIOs and IT infrastructure manager. The core questions that needs to be clarified: What are the benefits and do credible hybrid use cases indeed exist, which can be used as best practice guidance notes. This analysis is giving answers to these questions and also describes the ideas behind multi cloud scenarios.
Many developers and startups bless the public cloud to escape from high and incalculable upfront costs into infrastructure resources (server, storage, software). Examples like Pinterest or Netflix are showing real use cases and confirm the true benefit. Without the public cloud Pinterest would have never experienced such growth in a short time. Also Netflix benefits from the scalable access to public cloud infrastructure. In the 4th quarter 2014 Netflix has delivered 7.8 billion hours of videos. This is a data traffic of 24,021,900 terabytes of data.
However, what these prime examples are hiding: All of them are green field approaches — like almost every workload that is developed as a native web application on public cloud infrastructure and just represent the tip of the iceberg. However, the reality in the corporate world unveils a completely different truth. Inside the iceberg you find aplenty of legacy applications that are not ready to be operate in the public cloud at the present stage. Furthermore, requirements and scenarios exist for which the use of the public cloud is ineligible. In addition, most of the infrastructure manager and architects know their workloads and its demand very good. Provider should finally accept this and admit that the public cloud in most cases is too expensive for static workloads and other deployment models are more attractive.
By definition, the hybrid cloud sphere of activity is limited to connect a private cloud with the resources of a public cloud. In this case, a company is running an own cloud infrastructure and uses the scalability of a public cloud provider to get further resources like compute, storage or other services on demand. With the rise of further cloud deployment models other hybrid cloud scenarios have been developed that include hosted private and managed private cloud. In particular, for most static workloads — these where the requirements of the infrastructure on average are known — an external static hosted infrastructure fits very well. Variations because of marketing campaigns or the Christmas season — that are occurring periodically — can be compensated by dynamically add further resources from a public cloud.
This approach can be mapped to many other scenarios. In this case, not only pure infrastructure resources like virtual machines, storage or databases must be in the foreground. Even the hybrid use of value added services from the public cloud providers within self-developed applications should be considered, to use a ready function instead of developing it on the own again or benefit from external innovations immediately. With this approach the public cloud offers companies a real value without outsourcing the whole IT environment.
Real hybrid cloud use cases can be find at Microsoft, Rackspace, VMware and Pironet NDH:
In the course of the continuously propagation of the hybrid cloud also multi cloud scenarios are moving in the focus. For a better understanding of the multi cloud, it helps to consider the supply chain model of the automotive industry as an example. The automaker sets on various (sometimes redundant) suppliers, which provide him with single components, assemblies or ready systems. In the end the automaker assembles the just in time delivered parts within the own assembly factory.
The multi cloud respectively the hybrid cloud are adopting the idea from the automotive industry by working together with more than one cloud provider (cloud supplier) and integrating everything with the own cloud application respectively the own cloud infrastructure in the end.
As part of the cloud supply chain three delivery tiers exist that can be used to develop an own cloud application or to build an own cloud infrastructure:
In a multi cloud model an enterprise cloud infrastructure respectively a cloud application can fall back on more than one cloud supplier and thus integrate various Micro Services, Modules and Complete Systems of different providers. For this model a company develops most of the infrastructure/ application on its own and extends the architecture with additional external services whose effort would be much too big to redevelop it on its own.
However, this leads to higher costs at cloud management level (supplier management) as well as at integration level. Solutions like SixSq Slipstream or Flexiant Concerto are specialized on multi cloud management and support during the usage and management of cloud infrastructure across providers. On the contrary Elastic.io works on several cloud layers, across various providers and supports as a central connector to make cloud integration easier.
The cloud supply chain is an important part of the Digital Infrastructure Fabric (DIF) and should be considered in any case to benefit from the variety of different cloud infrastructure, platforms and applications. The only disadvantage is that the value added services (Micro Services, Modules) named above are still only available in the portfolios of Amazon Web Services and Microsoft Azure. In the course of the rapid development of use cases for the Internet of Things (IoT), IoT platforms and mobile backend infrastructure are taking an ever-growing significance. Ready solutions (Cloud Modules) are helping potential customers to reduce the development effort and giving impulses for new ideas.
Infrastructure providers whose portfolios still focus on pure infrastructure resources like servers (virtual machines, bare metal), storage and some databases will disappear from the screen in the midterm. Only the ones who enhance their infrastructure with enablement services for web applications, mobile and IoT applications will remain competitive.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
See all (36)
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sarayulabs/how-to-go-about-your-application-saas-vs-paas-vs-iaas-e212a7c663bc?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sarayu Labs
Jul 9, 2019·6 min read
This decade has seen a lot of startups being launched around solving problems by means of automation. Information Technology has been a key driver in supporting such startups. If we talk in particular, the progression and improvement of Cloud Computing technologies has been a key driver of this startup boom.
Cloud Computing has helped both, enterprise and small businesses to significantly reduce the costs associated with IT infrastructure. Now it costs less than a dollar to setup your own server on the cloud. Even better, you get to opt for a pay-as-you-go model with hourly billings. What has happened because of this trend is that a lot of people can now use IT infrastructure on demand to experiment without putting in any huge upfront cost or fee.
But even with the shift from the on-premise Data Center to a public cloud, people are still confused about the offers at hand. There are a lot of flavours and variants of public clouds in the market — let aside the provider. Even with the same provider, you have an option to opt for various kind of cloud offerings.
What are the differences between those offerings? What are the pros and cons of each of them? How does the costing between those offerings vary? What is the best option for you?
These questions are very obvious to be in the mind of anyone who is planning to launch some product. The person needs to account for the budget for the operation of the product, keeping the quality and stability in check. Let us compare the three major cloud offering in the market :Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Each of these has its own benefits, as well as variances, and it is necessary to understand the differences among SaaS, PaaS, and IaaS to know how to best choose one for your organisation.
The above diagram has one big assumption — SCALE. If you want to run a small email server for a team of 7–10 people, definitely SaaS setup (like a Gmail for Business or Microsoft Live) will be very cheap as On-Premise setup needs a significant upfront cost as well as operational support. But if you plan to serve a platform for a social network with a million users, the dynamics change significantly.
Let us discuss each of the three one by one. We are dropping On-Premise as we wanted to focus on cloud based scalable offerings. On-Premise setup for your IT infrastructure means procuring space and machines, setting up an entire team to support servers, network, storage, operating systems and what not. If you are thinking of going this route, maybe you are already way ahead in the business and this article is not directed to you. Let us begin.
Compute Capacity, Storage and Network are available as a service in an IaaS model. The cost varies depending on the consumption. The services are highly scalable as the provider is constantly adding more and more capacity to the in house infrastructure. They install powerful high capacity bare metal machines and allow users spin up multiple VPS or Virtual Private Servers on the machine of the desired capacity. The users have a significant control over the infrastructure compared to the other models of cloud delivery.
Few examples of IaaS are Google Compute Engine, Digital Ocean and AWS Elastic Compute Capacity(EC2).
The advantages of using an IaaS are as follows:
The disadvantages of using an IaaS are as follows:
In a PaaS model, rather than having machines, we just have a setup configuration to take care off. We need to define how much storage, compute and network we expect to use and this is it. The rest is taken care by the vendor. It can easily be scaled from 10 CPUs to 50 CPUs by a click of a button. In an IaaS model, we will need to procure capacity and provision all the softwares and configurations on our own. Hence, all we need to think about is the application logic and data.
Few examples of PaaS are Google App Engine, AWS Beanstalk and Heroku.
The advantages of using a PaaS are as follows:
The disadvantages of using a PaaS are as follows:
When the solution that you are looking for is a common use case (like an email server or a calendar service), you can find a lot of services online providing these as a SaaS offering. The entire application and data is managed by the vendor remotely.
Few examples of SaaS are Github, Gmail and Skype.
The advantages of SaaS are as follows:
The disadvantages of SaaS are as follows:
While all the offerings are different and evolved over time, choosing one of them requires a significant amount of planning. No one size fits all. Keeping in mind the business use case, budgets and compliance requirements, you might even want to go with an on-premise setup.
We will continue with a series of blogs to help you understand more about the various building blocks of a technology solutions.
For any further assistance, you can get in touch with sarayulabs.com
We have a long term experience in managing all kind of cloud offerings from multiple vendors. Let us know your end goal and roadmap. We can help you arrive onto a cost effective plan within your budgets.
1 
1 clap
1 
About
Write
Help
Legal
Get the Medium app
"
https://precipitation.io/prediction-on-the-impending-2015-gartner-cloud-iaas-magic-quadrant-c9efb4759454?source=search_post---------42,"So here are my predictions for the 2015 Gartner Cloud IaaS Magic Quadrant. It is based purely on my own ill-informed speculation, which means it will probably be more accurate than the UK General Election polls. This is my opinion, and should no way be represented as Gartner’s (mainly because I don’t work for them) or anyone else’s official view.
Feel free to correct my grammar, and debate my thoughts.
Firstly, to understand our future we must look to our past. Below is a quick analysis of the last 3 Gartner Cloud IaaS Magic Quadrants, and my prediction for the coming one.
The movement of the various organisations on the quadrant is interesting and gives us some basis for insight.
Amazon Web Services
Previous Quadrant Position: Leaders > Leaders > Leaders
Expected Quadrant Position: Leaders
AWS are the 800lb gorilla of Cloud Computing, and they are virtually unchallenged. The rate at which they are increasing their addressable market through new services, augmenting existing services, and increasing their geographic footprint is incredible. Recent financials have also given a clue to how profitable they are, and shown that they have money to spend. Don’t expect them to let up. Everyone still has a lot of catching up to do.
Bluelock
Previous Quadrant Position: Challengers > Nope > Nada
Expected Quadrant Position: Bye Bye
This company was up and coming a few years ago with a VMware powered cloud, but has fallen back right off the quadrant. Don’t have the capital, geographic reach, and using someone else’s proprietary hypervisor that now competes with them means they don’t offer the differentiation needed to be at the top table. I’m sure they still have a viable business, but no longer a contender.
Centurylink
Well this is complicated..
Previous Quadrant Position:
Savvis Leaders > Challengers
Tier 3 Visionaries > Niche
Combined .. > .. > Visionaries
Expected Quadrant Position: Visionaries
Centurylink is an interesting organisation, a telco at heart with cloudy ambitions. The current cloud computing organisation is a mashup of Savvis and Tier 3, being led predominantly by former Tier 3 executives. The Savvis acquisition probably didn’t go as well for Centurylink as hoped, if you scour LinkedIn you can find a graveyard of former Savvis employees that left within a year of the acquisition. Not too sure what Centurylink did to chase them away, but they were left with a company missing a lot of its talent, and unable to respond to a rapidly changing market.
Tier 3 were a small service provider with some interesting and innovative solutions, but lacking the capital and reach to truly compete. The acquisition of Tier 3 reinvigorated the Centurylink cloud computing business, and it looks like Centurylink learnt from Savvis, and have retained the talent they got through the acquisition and are supporting them in turning the business around.
Expect more from Centurylink as they move up the stack with Cloud Foundry based Platform as a Service offerings, and higher quality fully managed solutions for their enterprise customers. They are on the right trajectory, and I expect improvement, but we should see them still just on the Visionary side of the line. Execution on all these good intentions will determine if they head back to the leaders quadrant.
CSC
Previous Quadrant Position: Leaders > Leaders > Visionaries
Expected Quadrant Position: Niche
The CSC cloud seems to be going backwards, originally built on VBlock, they are struggling to keep up with the market place. This is probably due to not having the deep pockets of the leaders in the space, and also being reliant on third party technology which is unable to keep up with the leaders.
CSC as an organisation are still in a healthy (ish) place. They are focusing on cloud management, and providing services on top of other player’s clouds. The ServiceMesh acquisition looked like a shrewd one at the time, and despite the recent bribery allegations, gives them multi-cloud capabilities which supports a cloud enablement strategy. If they can get past the bribery scandal, they will be in a good place overall, but as a provider of IaaS services they will fade into the Niche category.
Dell
2011: “We’ve committed to build 10 new data centers around the world, and will be standing up these data centers in the next 24 months,” said Steve Schuckenbrock, President of Dell Services. “These data centers will be housing public and private cloud capabilities on behalf of our customers.”
2013: “Many Dell customers plan to expand their use of public cloud, but in order to truly reap the benefits, they want a choice of providers, flexibility and interoperability across platforms and models, the ability to compare cloud economics and workload performance, and a cohesive way to manage all of it,”
Oh well, it only cost them $1 billion to figure that out. They got off lightly
Digital Ocean
Previous Quadrant Position: .. > .. > ..
Expected Quadrant Position: Visionaries
My pick for new entrant. They’re showing rapid growth, eating away at Rackspace’s smaller customers with lower prices than AWS. Limited in geography and squarely aimed at smaller, simpler deployments, they know the market they want and are addressing it well. They lack a breadth of features, but what they do, they do well. Expect to hear more from them as they grow.
Dimension Data
Previous Quadrant Position: Leaders > Challengers > Niche
Expected Quadrant Position: Niche
These are the guys that make most of the money from selling Cisco equipment. They bought some interesting companies (OpSource, Bluefire, Nexus), but are probably going to squander that all by integrating poorly and jumping on the Cisco Intercloud train. Expect them to sell more Cisco equipment next year.
Fujitsu
Previous Quadrant Position: Niche > Niche > Niche
Expected Quadrant Position: Niche
Most of their SI competitors (Capgemini, Accenture, Cognizant, CSC, Infosys, Wipro), are looking at running on other people’s clouds. Fujitsu wants to spend its money on building an undifferentiated cloud. Good luck to them!
GoGrid
Previous Quadrant Position: Challengers > Niche > Niche
Expected Quadrant Position: Nope
Recently acquired by Datapipe, an organisation that is making its name by managing workloads on other peoples clouds. They have been struggling to keep up with the pace of innovation or the capital expenditure of the leaders. Expect Datapipe to utilise them for the niche use cases that can’t be met by the other leading vendors.
Google
Previous Quadrant Position: .. > .. > Visionaries
Expected Quadrant Position: Leaders
You know that feeling when someone is staring at you, but you don’t want to acknowledge them because then you admit they exist. Well that’s how I think AWS thinks about Google. Google have all the resources, money, experience and clever people AWS have, and more. What they also have is an 8 year gap to close on AWS. Right now they lack compliance and security features and, don’t quite have the breadth of services required to compete at every level. Some impressive numbers show they can compete with AWS in the HPC space, and they have some features AWS does not have.
They lack breadth in portfolio, and geographic reach, but they will address these concerns. This is a company that keeps Google+ alive, they don’t know how to give up. Expect the search engine to be a bigger competitor with the book seller in the coming years, but they still have a lot of catching up to do.
HP
Previous Quadrant Position: .. > Niche > Niche
Expected Quadrant Position: Niche
Dear HP,
What are we to do with you? When you acquired Eucalyptus we thought it might be a bright light, and do for you what the Softlayer and Tier 3 acquisitions did for IBM and Centurylink respectively, and let the bright talented people with knowledge in the space lead your cloud push. Maybe it’s a lack of patience, or a lack of vision, I don’t know, but making Marten Mickos a glorified evangelist is just a waste of everyone’s time.
Then you make a pig’s ear (no disrespect to pigs and their respective ears) of your Helion Rack announcement, by following it up with silly statements like this, leading to speculation you were existing the public cloud space. The multiple denials didn’t help either as it brought greater scrutiny to your business and all kinds of rumours.
HP we’re begging you, stop this nonsense, get a strategy, hire good people, and back them.
Ant
PS. Good work on adopting Cloud Foundry, but you’re still behind the rest on that front.
IBM
Previous Quadrant Position:
IBM .. > Niche > ..
IBM / Softlayer Niche > Niche > Visionaries
Expected Quadrant Position: Visionary
If there is a nuclear apocalypse I fully expect IBM, much like the humble cockroach, to survive. They have had more obituaries written about them than Rasputin, yet they are still here, innovating, adapting, and just not going away!
Much like Centurylink, IBM’s cloud strategy was floundering, a cloud built on their ancient TSM Automation Manager was never going to compete. The Softlayer acquisition gave them capabilities they needed, and talent to execute on a broader cloud program. The IaaS platform has grown in geographic reach, and will continue to expand, but is still very much a “me too” offering, with some differentiation around bare metal that no-one really needs. Where the value is that it provides a foundation for IBM to compete on the next battleground, Platform as a Service with its Cloud Foundry powered BlueMix service.
Joyent
Previous Quadrant Position: Challengers > Niche > Niche
Expected Quadrant Position: Niche
I was going to make a joke about Joyent not being so Smart now, but realised it wasn’t very good. I don’t know how to feel about Joyent, loads of potential but just not living up to it. Having to step away from curating Node.js due to frustration from the community is a symptom of other problems. The one time potential saviours of Solaris still have great technical talent, but are slowly shrinking to another “me too” player. They need to do something to change their direction, right now it’s a slow descent, which is painful to watch. Containers aren’t going to save them, they need to do something different.
Microsoft
Previous Quadrant Position: .. > Visionaries > Leaders
Expected Quadrant Position: Leaders
Shiny new CEO saying all the right things, heritage (sort of) in enterprise, lots of capital, lots of smart people, and track record of executing. Last year Microsoft broke through into the Leaders quadrant and proved that they are a genuine contender to AWS. They still have some catching up to do, but they’re executing well, adding services at a rapid rate (though still slower than AWS), and leveraging everything they’ve got to grow the Azure number.
Cloud analysts lost one of their favourite past times a few weeks ago when Amazon broke out their AWS numbers. Microsoft felt bad for these analysts, and is persisting with lumping its Azure numbers with its Office 365 numbers, giving them something to do by trying to break down the numbers. Until the Azure business is reported separately we won’t get an accurate gauge on what the gap is between AWS and Azure.
Microsoft have being throwing around big Azure credits to potential customers to entice them to use it, and throwing some usage in for free with some Office 365 deals, so expect their usage numbers to be on an exponential rise, even though their revenue might be a slightly more gradual ascent. Microsoft are in this for the long haul, and right now are just trying to grab market share. This is going to be a long battle, which helps all of us.
OVH
Who?
Rackpsace
Previous Quadrant Position: Visionaries > Visionaries > Niche
Expected Quadrant Position: Niche
Poor Rackspace, they saw the AWS monster before most and started to do something about it with OpenStack. They put up a valiant fight, but just don’t have the deep pockets to compete. The irony of the whole situation is that they probably didn’t understand their own value proposition. Rackspace’s customers come to them for the “Fanatical Support”, not the infrastructure. Sinking money into an infrastructure platform that doesn’t differentiate them, when they could have been focused on providing “Fanatical Support” on top of other platforms is hurting them.
Imagine how big they could have been if they spent their money on providing services on top of AWS 6 years ago? Just imagine… At least they seem to have seen the light, and it looks like they will be doing something like that in the short term future (though maybe not on AWS).
As an IaaS provider, Rackspace will remain niche, as a provider of “Fanatical Support” their outlook is good.
Verizon Terremark
Previous Quadrant Position: Leaders > Challengers > Visionaries
Expected Quadrant Position: Niche
They barely made the Visionaries quadrant last time, expect them to fall into Niche. They’re a big company with lots of money, but are diversified (even more so after AOL), and don’t seem to focusing on cloud as much as their network businesses. The value with Verizon is in taking multiple products from one supplier, and there is some logic in using them if you’re an existing network customer.
They haven’t maximized the Terremark acquisition a number of years ago, with a number of old Terremark employees now at VMware. If they want to remain competitive in this market they will need to make a purchase similar to what IBM and Centurylink have done, and move up the stack. The problem is there isn’t really anyone left to buy that can move them up the stack, except maybe Virtustream or maybe Joyent.
Virtustream
Previous Quadrant Position: Visionaries > Niche > Niche
Expected Quadrant Position: Niche
An interesting small provider, offering some differentiation through their xStream Cloud Management software which allows management across multiple platforms. They have a small footprint but are doing well in their geography (North America). Longer term expect them to focus more on the Cloud Management side of things, but most likely will get acquired at some point.
VMware
Previous Quadrant Position: .. > .. > Niche
Expected Quadrant Position: Niche
A pivot from being a cloud enabler, helping other providers bring clouds to market, to being a public cloud provider themselves. VMware initially seemed to be a bit half-hearted when they launched the vCloud Hybrid service, but seem to have gotten more focus and commitment in the last 12 months with a re-branding, new regions and additional features. Whilst they don’t have the wide breadth of services other providers have, they offer value to existing VMware customers. They will struggle to attract customers not dependent on VMware in their own data centers today.
Summary
The last 12 months would have seen a further decline in the existing Niche players, and some of the marginal Visionaries will drop to Niche. Amazon, and Microsoft will pull away from the majority pack, whilst Google will move ever closer. Digital Ocean will be disrupting from the bottom, but ultimately aren’t competing with Amazon, Microsoft and Google. The big three have a distinct advantage over every other competitor in terms of their available capital and that they are responsible for their entire stack. Providers that have delivered clouds by integrating other people’s proprietary technology will struggle.
Smart companies are pivoting, looking to move up the stack with Platform as a Service offerings. They might still struggle though in the PaaS space where they will again be competing with AWS, Google, and Microsoft with Salesforce thrown in for good measure.
Moves to manage other peoples clouds will help ease the niche IaaS providers pain, and could provide a sustainable future.
All in all, Amazon still no. 1, and I don’t see that changing for a few more years.
When playing with clouds, expect to get wet…
Written by
Figuring it out…
When playing with clouds, expect to get wet…
Written by
Figuring it out…
When playing with clouds, expect to get wet…
"
https://medium.com/@salmansaleem920/5-best-iaas-providers-which-one-is-best-for-you-1a293c3f5685?source=search_post---------264,"Sign in
There are currently no responses for this story.
Be the first to respond.
Salman Saleem
Jun 21, 2019·12 min read
IAAS stands for Infrastructure-as-a-Service. It refers to a cloud based infrastructure for your business. Cloud service providers offer virtual computing resources over the Internet. The powerful cloud servers offered by leading IAAS providers tend to keep your web applications active all year round.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sohoffice-taiwan/%E5%9F%BA%E6%9C%AC%E5%9E%8B%E5%80%8B%E4%BA%BA-iaas-8114c269cf30?source=search_post---------251,"There are currently no responses for this story.
Be the first to respond.
現在基本上大家都是用筆記型電腦來工作了，沒有足夠的經驗(年齡 ?)的同學們，可能覺得這沒什麼不好的。但是啊 ~ 一台 Macbook 動輒五萬，貴的要七八萬，用來作粗重的工作(大數據 ?)時，機體的熱氣蒸蒸日上，風扇還發出少見的哀嚎。我這是在虐待電腦嗎 ！ 沒有違反勞基法還是什麼異世界的法令嗎？
我其實想過租一個 VPS 耶，不過如果要有足夠的記憶體那一個月也要不少錢。不划算 ~
桌上型電腦來救援了，只要把程式放在桌機跑不就好了嗎？但是啊～鍵盤打不習慣啊～人類可是可以正大光明的宣稱睡覺都要抱同一個小棉被的存在啊，換一個鍵盤什麼的，怎麼可能作得到啊？什麼？遠端桌面？可以再更麻煩一點嗎 ？
其實用桌機是可以的，只是需要一點小技巧
我想聰明的你，已經知道要怎麼作了吧。為了節省你寶貴的時間，請特別注意下面這個問題哦
同樣的檔案在同一時間修改會出現 Conflict
以 java 的專案為例，要注意以下幾類的檔案
解決方案很簡單，這些檔案不要共用，都設為每台機器個別的路徑就好了。
下面有我的一些小偏方，請斟酌服用。
Vagrant 請設定環境變數 VAGRANT_DOTFILE_PATH 讓 vagrant 在執行時不要使用預設的 .vagrant 目錄
Ex: VAGRANT_DOTFILE_PATH=.vagrant-my-foo-computer
SBT 專案請參考以下的 build.sbt 將 compile 出來的 class 檔案放到不同目錄。
本例是使用系統參數 buildBaseDirectory，執行 sbt 時加上這樣的參數即可
Save your Macbook and Happy coding !
Did you learn something new? If so please:
↓ clap 👏 button below️ so more people can see this
全端專業開發服務中文站
全端專業開發服務中文站
Written by
Problem solver. Found love in Scala, Java, Angular and more …
全端專業開發服務中文站
"
https://medium.com/techstack-ltd/what-type-of-a-cloud-service-do-you-need-saas-paas-or-iaas-32538fcfc064?source=search_post---------187,"There are currently no responses for this story.
Be the first to respond.
Written by Aleksey Svistun, CTO at Techstack
Not without good reason: cloud service users not only significantly reduce infrastructure software maintenance costs, but also flexibly respond to any changes through elastic computing.
Depending on its objectives, a company can use an existing cloud service, also known as SaaS (Software as a Service), or it may need to use a PaaS (Platform as a Service) or IaaS (Infrastructure as a Service) model to create its own unique solution that meets its requirements.In this article, we will analyze the concepts of SaaS, PaaS and IaaS and find out which models are most suitable for different business types and sizes.
Clouds offer a flexible model of access to various resources — from servers and databases to off-the-shelf applications.
The user gets the necessary resource by subscription, and the provider ensures smooth operation.
The wide popularity of cloud services is explained by their main advantages:
Profitability. You only pay for the resources and capacities that you use. Unnecessary sources are released automatically, and in case of additional load, new ones are put into operation quickly. Moreover, a huge part of the technical work, such as setting up and maintaining the service, is entirely the responsibility of the provider, hence you are exempt from the cost of hiring maintenance technicians.
Flexibility. Cloud services are easily adaptable to any business’ needs. Adding new features, removing unnecessary functions, fast scaling — cloud services deliver easy product adaptation to new market conditions. If necessary, one cloud model can be easily and quickly replaced by another, and all this without huge financial investment.
Safety. The technologies and approaches used in cloud models bring the highest security standards, proven by millions of users around the world. By creating new services, providers are improving approaches and making cloud models even more reliable.
According to Gartner, worldwide public cloud service revenue in 2019 was $227.8 billion. Increased needs for new communication tools caused by the COVID-19 pandemic and quarantine are stimulating сloud services’ growth: experts predict that by the end of 2020, revenue will be about $266.4 billion.
Today the cloud services market consists of 3 main levels.
IaaS (Infrastructure as a Service) is a base of cloud solutions, according to which the user receives server infrastructure, communications, storage, and all the software necessary for work.
Basically, you buy computing resources from a provider, hence you do not need to bear the cost of your own hardware. IaaS allows you to create unique business solutions with all the benefits of cloud technologies. If you decide to use the IaaS model, you will need a team of engineers to maintain the infrastructure and highly qualified developers to create and configure software following the requirements.
PaaS (Platform as a Service) is the middle level of cloud solutions where the user receives a ready-made software environment for developing and testing their products, placing backup sites, and transferring part of the workload from a private cloud to a shared cloud and back. Unlike IaaS, PaaS includes server-related work, load balancing, DNS, etc., so you do not need to hire engineers to maintain the infrastructure. However, the need for a team of developers who will work on the product remains.
SaaS (Software as a Service) is the last level of cloud solutions. It provides the user with access to an off-the-shelf application. Typically, such applications have several basic packages with different functionality, and you can choose the version most suitable for your needs. However, you cannot change the functionality of the application. Here you are dealing with an off-the-shelf application fully serviced by a provider, that is why you will need minimal resources to maintain SaaS.
SaaS is a simple solution that can be used immediately and without too many resources. However, its functionality is limited, and you cannot modify them.
Therefore, for some companies’ tasks, an off-the-shelf service is not enough, and a custom solution such as PaaS or IaaS may be required. PaaS is suitable for creating solutions within the existing infrastructure, while IaaS will allow creating a unique piece of software without any restrictions.
The choice between an existing model and a custom solution is based on the company’s type and scale, as well as on what tasks the cloud service has to solve.
As a rule, the basic functions of SaaS solutions are quite enough at the start of a business, when growth and scaling are a distant prospect.
A young firm can use SaaS apps for almost any task — from communication and project/team management to the automation of internal business processes and marketing and sales objectives.
As usual, SaaS is quite suitable for corporate mail, video conferencing applications, ERP and CRM systems, website builders, etc.
Using ready-made solutions, the company receives:
While the functions of an off-the-shelf solution can cover basic tasks in a business’s infancy, these functions are no longer enough when it becomes an industry giant. The product will grow and scale, the issues will become more difficult, and a custom PaaS or IaaS solution will be required to resolve them.
The PaaS model brings an environment for developing, testing, and deploying your own applications. Based on PaaS, a company that has grown to mid- or enterprise-size can develop a unique application to solve specific business issues.
The size of a company does not necessarily mean that using a custom solution is mandatory. For some tasks, such as work with office documents, internal communications, corporate mail, etc SaaS solutions may still be suitable, however, a custom product is now required to effectively solve core tasks.
Here are the top 5 signs that you need a custom solution, such as PaaS or IaaS:
The examples listed above are just a handful of the thousands of unique cases.
In fact, the choice depends on how confidently you can answer the question: “Is a ready-made solution enough to solve all my problems now?”. If you are in doubt about the affirmative answer, it is a strong case for a custom solution.
For enterprises as well as technical startups, SaaS is quite suitable for accomplishing basic tasks (internal communication, project and team management, etc.).
However, if you are creating an innovative technical product, it is obvious that a SaaS is not your best option, and a custom solution needs to be developed.
Which model to choose for developing a unique product — PaaS or IaaS — depends on whether you need to influence its infrastructure or not.
In the PaaS model, you get the computing platform and solution stack, but you cannot change the virtual infrastructure configuration. This function is usually not enough to create a unique high-tech product, and the IaaS model is the most suitable choice here.
In the meantime, a custom cloud solution for tech startups may concern not only the product itself but also additional features (for example, sales platforms and internal CRMs). The most outstanding advantages of complex custom solutions are optimal interoperability and flexibility, which off-the-shelf software is unlikely to be able to boast.
Before starting work, make sure you have enough experts on your team who are both technically strong and enthusiastic about your idea. This way, you will provide yourself with an excellent foundation for the success of the future product.
When it comes to solving typical tasks with a light load, a SaaS is perfect. The basic functionality of the ready-made services with its relatively low cost will be an excellent solution at the start of a business, and it will also be useful for larger businesses in supporting non-core business activities.
Custom cloud solutions come into play under more serious conditions: when you start a tech startup, allow your product to grow, or want to accomplish the complex tasks of an enterprise or mid-sized company. One way or another, the decision to choose between an off-the-shelf and custom solution depends on the specific tasks and business goals that need to be achieved.
Originally published at https://tech-stack.io on April 2, 2021.
A community of talented software developers creating cutting-edge solutions to companies of all size
51 
51 claps
51 
Written by
Full-stack software development company with vast experience in providing cutting-edge technology solutions to companies of all sizes.
Techstack is a full-stack software development company with vast experience in providing technology solutions to companies of all sizes, from startups to global corporations.
Written by
Full-stack software development company with vast experience in providing cutting-edge technology solutions to companies of all sizes.
Techstack is a full-stack software development company with vast experience in providing technology solutions to companies of all sizes, from startups to global corporations.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-highlights-with-bullperks-strongnode-io-9adda264af3a?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 16, 2021·5 min read
Last 13 September 2021, StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito jumped in on another Telegram AMA session with Bull Perks where he answered the questions from our fans on Twitter and Telegram and shared details about our upcoming initial decentralized exchange offering (IDO) and our $SNE token public sale plans.
Our IDO is happening on October 6, 2021 at 15:00 p.m. UTC / 11:00 a.m. EST with our partners Starter.xyz and Bull Perks. Learn how participate in our IDO process here: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
In case you missed it, here are some of the highlights from the StrongNode and Bull Perks AMA episode:
PROJECT INTRODUCTION:
Angeli: ​​First question, can you tell us a little about the project and why it’s different than anything else?
Daniel Saito: Thanks, StrongNode is an Infrastructure as a Service. We harness the power of latent and idle compute, network, and storage of the everyday common man/woman.
Daniel Saito: This compute is then harnessed to process some of the world’s challenging compute problems. Such as offering cost-effective compute resources for genome sequencing analysis for COVID.
Angeli: ​​Who are the team members? Can you tell us a little about them and how long they’ve been involved in this and crypto?
Daniel Saito: There is something special about the team about StrongNode. We all have a working relationship with each other spanning back for a decade. My CTO is Colin Charles, an early employee at RedHat, Chief of Community at MySQL, and co-founder of MariaDB server.
Daniel Saito: My VP of infrastructure is Zivago Lee, whom I had the pleasure of knowing since 10 years old. He has designed complex data centers and managed technical operations for ticketing at the Beijing Olympics.
Daniel Saito: Cumulatively between the C-Suite members we have commanded +10 exits to dates ($8.5B of exit liquidity to our investors)
Daniel Saito: It’s been a wild ride for myself since getting involved in the tech industry — since the early 80s. I got into crypto in mid-2009. 🤑
Angeli: How about the advisors and backers of the project?
Daniel: Our approach when building a startup is to identify potential customers and develop the product from there and if the opportunity can have a vast impact.
Daniel: We have been working on this idea for over 2 years (first in a form of an open-source wifi router) since my background is in rapid prototyping in both hardware and software. After advising various crypto projects, we saw what worked and what didn’t.
Daniel: We took this opportunity to band together for a common cause and start StrongNode.
Daniel: ​​As for our Advisor we have some great people from the crypto industry working with us, too many to list I recommend checking our website out at StrongNode.io
Angeli: What are your short- and long-term goals? Can you share platforms you’re building on?
Daniel: Our immediate plan is to have a successful IDO in the next following weeks. immediately upon releasing our tokens into circulation, we will open up staking and AMM farming. All developed on custom code and all code will be audited by rug doctors. We have several partnerships planned in the pipeline.
Daniel: The earliest investors ended up acquiring enormous amounts of StrongNode tokens as they wanted to use the token to stake for their data to be processed so we ended up selling to our customer base first.
Angeli: What are the key points for the project's success? How will you tackle them? Any obstacles?
Daniel: Well our project success will be based on our digital footprint and its exponential growth to get network effect. Over time the network will shrink and grow, management of the resources utilized will need to be optimized. Additionally, we will be onboarding new innovative ideas for the onboarding process and user registration.
COMMUNITY QUESTIONS:
Ali: If I understand correctly, node collects the GPU and CPU power of people and use or rent it to solve complex problems or storage to make a profit so as holders and supporters of the coin how we will be benefiting from these actions and there are others do exactly the same thing why people should be using your service? and I think you should upgrade your website because it is very complex for everyday users to grasp the idea and tokenomics should be there too and I didn't see the investors on your web page can you please share them with us thank you for being here. 👍💪
Daniel: Yes, we will have a new version of our website releasing this week. It is amazing how a website gets dated so quickly. But yes, there is a revision in the pipeline.
COMMUNITY FEEDBACK:
Polkadot: What makes you feel confident about the survival of your project in the near future? Do you take into account community feedback and demands while developing features of your project?
Daniel Saito: Community drives the product. This has been our ethos since we started shilling open source software. When developing software that is free to the general public you need to involve the community. Without community there is no product, we were firm about that if anything MySQL acquisition by Sun Microsystems is a testament to what drove the evaluation to $1 Billion. We also hire from the community so if you are a great marketer or solidity programmer we want to hear from you. We are always hiring the best and the brightest!
IDO Details:
Raghu: Hi Daniel thank you so much for answering the questions patiently. We have heard that starter.xyz is doing the IDO of your project? Is that true? If yes given the success rate of the launchpad how would it affect StrongNode?
Daniel: The relationship with starter.xyz allows us to have a great relationship with $QUICK. There are numerous benefits, but our IDO allocation is 300K, I suspect it will sell out quickly and tokens will end up hitting the open market.
Angeli: Great AMA, Daniel! It was amazing to have you! @StrongNode_CEO
Angeli: Thank you, everyone, for joining us today and we sure had a blast.
Daniel: We believe that we will be successful in our endeavors and we hope you can be a part of it please join us at our telegram channel. https://t.me/strongnodechat
Daniel: Computing for a Purpose.
Angeli: Cheers! Thanks Daniel! 🙌
Daniel : I hope to be back post-IDO to keep you all updated!
Angeli: See you, and great to have you! 👍🙌
Daniel: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
For more details, visit our website: https://strongnode.io/
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
431 
431 
431 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@vyronas/providing-http-consumable-services-to-windows-azure-pack-tenants-49462359b77f?source=search_post---------347,"Sign in
Vyron Tsingaras
Jul 15, 2018·3 min read
Windows Azure Pack, for those unfamiliar with it, is a multi-tenant IaaS self-hosted platform that runs on Windows Server. Windows Azure Pack combines Hyper-V, SC VMM, SC SPF and Hyper-V Network Virtualization to provide each tenant an isolated compute and network environment.
To give a high-level overview of how HNV works in Windows Server 2016:
For a more thorough explanation visit: https://blogs.technet.microsoft.com/networking/2016/10/26/network-virtualization-with-ws2016-sdn/
To be fair, you can chose to skip deploying network virtualization but then your tenants lose the ability to create their own networks and you enter VLAN-hell (tm). This article applies only to SDN enabled deployments.
Apart from giving your tenants (and you, the service provider) freedom to make any changes they like on a self-service basis, HNV allows you to provide your tenants with custom tenant-scoped services. For example, you can provide a service such as the Azure / EC2 / GCP instance metadata service, custom techincal support workflows and the like.
In this article I will outline one of the (possibly many?) ways to do that based on the capabilities of the HNV stack and write an example HelloWorld service.This, SDN-multi-tenant-awesome, HelloWorld service will listen for HTTP GET requests to http://169.254.66.66:52250/hello and send back: Hello {{ vm.name }} from Hyper-V.
Start by remoting in to each Hyper-V host in the deployment and running the following lines in an elevated PowerShell:
What these registry keys do is tell the Host Agent to program the Azure VFP extension to send ARP replies for the service IP, listen for HTTP and proxy everything to the specified backend IP and Port. Note that the GUID was randomly generated and we should be able to have multiple ProxiedServices, I haven’t had the time to test this and other keys yet. Also note that everything under NcHostAgent\Parameters\Plugins is undocumented, YMMV.
Now if we spin-up a tenant VM, open a browser and visit http://169.254.66.66:52250/hello we will see this:
… which is to be expected as we still haven’t written the backend for our service.
We will write a PowerShell program that will provide the HelloWorld service using the Polaris web framework.
Because Polaris doesn’t yet support path parameters we will be using a fork of it until the corresponding pull-request is merged upstream. Start by downloading the fork’s zip and extract it somewhere.
Now, create a file named HelloVM.ps1 inside the extracted repository so that the new file is next to Polaris.psd1, and paste the following code:
Fill in the 3 variables at the top (vmmhost, ncuri, cred) with your info and run the script from an elevated PowerShell. Now when we visit the same page again in our tenant VM we should see a different picture:
How did we know which tenant VM initiated the request? The answer is, as you may have guessed from the .ps1, that the HTTP proxy passed as the first path parameter a GUID. That GUID is the VM’s NIC InstanceID as it it known to the NetworkController. To find out which VM sent us the request we:
This concludes my first article here. I hope it has been an enjoying and informative read. Please don’t hesitate to ask questions or leave feedback in the comments.
2 
2 claps
2 
"
https://medium.com/@mhkt/how-to-sell-me-your-technical-product-db00eac305d9?source=search_post---------52,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Matt Hackett
Mar 3, 2016·4 min read
Ask any CTO about their experiences with technical sales, and after they suppress the throat-vomit, they’ll begin by telling you of the cold emails and cold calls and cold 6-megawatt-spark-plug mailers. Just this past week, I’ve received ice-cold pitches for dozens of SaaS products with such instant-delete subject lines as:
Yikes.
Cold emails are the lowest-leverage sales activity imaginable. If cold emails are the way I first come to know about your product, you’ve really misallocated resources. If you’ve got your priorities so wrong in sales, I can only imagine how much worse they are in your actual product.
We’re slowly heading toward a world in which all software is sold directly to the consumer, in this case the developer. The moat around enterprise-sales-focused companies is drying up rapidly. Cold emails to CTOs will get you nowhere when businesses go the serverless, service-assembly route. Only tactics targeting large numbers of individual developers will move the needle.
If you’re a SaaS, infrastructure, or tools company, here are some future-proof, high-leverage ways to get my attention and turn me into a paying customer:
Bottom-up will be the only viable way to sell software in the near future. Start using these tactics now, and just stop it with the cold emails.
PS: If you’re in the early stages of developing a product that solves a developer problem, some of these tips might not yet apply to you. I always love fiddling with new tools—don’t hesitate to drop me a line.
If you liked this article, why not subscribe to my newsletter and get my unfiltered commentary and a handful of links for product makers in your email every week?
Technologist on a purposeful wander. (Previously: Cofounder, Beme · VP Engineering @tumblr · HIR @betaworks)
See all (700)
262 
8
262 claps
262 
8
Technologist on a purposeful wander. (Previously: Cofounder, Beme · VP Engineering @tumblr · HIR @betaworks)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/creating-an-instance-in-a-newly-designed-vpc-using-terraform-440a220d3886?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
Understanding Virtual Private Cloud:
A VPC is a logically isolated virtual network, where AWS nodes like EC2, load balancers and so on can be launched. A VPC provides the following functionalities:
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/amazon-a-217-6-pound-iaas-gorilla-2daca873b3c4?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
In my last business, we had to go through three hosting providers before we found one that was a good fit. At the outset, our needs were modest but specific and we finally chanced upon a provider that made sense for us. We didn’t have a formalized purchasing process (or the free cycles to put together an RFP) and it was a struggle for us to reconcile the incredible importance of hosting for our SaaS offering with the financial and temporal constraints faced by most small businesses. While we ultimately found ourselves with a hosting company whose offerings and culture aligned well with our needs, we never wanted to be complacent with regard to our selection. Was there something out there that performed better, cost less, or was still better suited to our requirements?
As our business evolved, so did our requirements. In particular, we would have spikes in computational workload that were unpredictable. Amazon, which to that point, was simply regarded as an online retail giant, launched a beta of a service called EC2, which offered some of their spare compute cycles in an on-demand way. We were quite happy with our hosting provider at this point. So while it didn’t make sense for us to swap out what we had, it made all the sense of the world for us to leverage EC2 and adopt a multi-provider strategy. Suffice to say that things went well enough for us that we happily participated in a case study: https://aws.amazon.com/solutions/case-studies/twistage/.
Roughly a decade after Amazon’s initial foray into IaaS (Infrastructure as a Service), the market for such services has experienced “hockey stick” growth. What was under a $3B global market as recently as 2010 now projects to come close to $25B in 2016.
Chart from: https://www.statista.com/statistics/203598/forecast-for-the-worldwide-public-cloud-services-market-for-iaas-until-2014/
Thus far, Amazon has held on to its first-mover advantage and remains the largest player in the IaaS space. But for all the press that they’ve received, the lion’s share of the market belongs to non-Amazon companies. It wasn’t too long ago that Microsoft promoted their cloud chief, Satya Nadella, into the CEO position. More recently, Urs Hölzle, Senior Vice President for Technical Infrastructure at Google, shared “My goal is for us to talk about Google as a cloud company in 2020 because our revenue is bigger than the ads revenue and it’s a realistic possibility.” Considering Google generated $67.39 billion in ad revenue in 2015, that’s an extremely lofty ambition.
The growth curve of cloud services as a category is so steep that it makes sense for companies to shoot for the stars. The other side effect of a large and fast-growing market is that new entrants enter the fray at a rapid pace. Add the combined IaaS market share (see the chart below) for the aforementioned companies — Amazon, Google, and Microsoft — and you’re still looking at coverage of less than 50% of the market. In fact, the most successful company offering IaaS is “Other.” With apologies to Other Inc., the “Other” in the chart is not a company at all, but rather represents the combined market share of companies that weren’t fortunate enough to be named on that list.
www.statista.com
What’s right for your business? It may well be one of the market leaders. However, it might happen that the best fit for you is a company that you haven’t yet discovered. But how do you find them, much less evaluate them? That’s where we come in. We’ll let you explore the offerings of market leaders as well as discover hidden gems. And then we’ll provide the tools to evaluate these vendors side by side so you can make an informed decision about what’s best for your organization’s needs. We’re unbiased and driven by a single goal — to help you make the best buying decision you can.
(For those of you who are still confused by the title, please allow me to explain. Market leaders are sometimes called “800-pound gorillas.” With Amazon’s IaaS market share listed at 27.2% in the chart above, I simply took 27.2% of 800 pounds and voilà — a much smaller gorilla!)
Originally published at Vendorful — Buying Made Better.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
6 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
6 claps
6 
Written by
David Wadler is a parent, husband, CEO of Vendorful, and a New Yorker.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
David Wadler is a parent, husband, CEO of Vendorful, and a New Yorker.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.cloudboost.io/serverless-the-next-level-of-abstraction-30f2003a49e3?source=search_post---------63,"If you look at the history of software development, writing and maintaining software has been a very complicated & time consuming process. Making software development easier and faster by improving developer productivity literally saves companies thousands or millions of dollars annually and this is exactly the function that’s being optimised by many Cloud Vendors around the world.
Before we dive into what serverless actually is, let me give you a quick recap on how Cloud evolved in the last couple of decades and why it is the way it is today and lastly, why serverless is the next big thing.
To optimise for developer productivity and cost of running apps and services in the last decade — cloud vendors have introduced IaaS, where companies don’t have to buy hardware to host their apps, they can “rent” the hardware out from any of the cloud vendor and can only pay for what they use. This is called Infrastructure as a Service (IaaS for short) where cloud vendors provides virtual machines to these companies where they can ssh into and install and maintain their own OS, and can install any software and service on them. It saved companies a lot of money by not having to buy hardware physically, hire a ops team to maintain the server uptime, pay for energy to power and cool these machines, but you still had to install and maintain operating systems, make sure your server is secure by installing security patches and so forth.
So, In short IaaS is “I’ll give you virtual machines and you can SSH in them, and install anything you like.”
Some of the best known IaaS are EC2, Azure Virtual Machines, Google Compute Engine, etc.
IaaS did improve a lot of productivity when compared to legacy enterprise IT, but it did took a lot of work to manage these virtual machines and maintain it (like installing patches, configuring firewalls, etc). Developers wanted a way to just write applications and give it to the Cloud Provider which would host it for them and they would not worry about OS’es, Security, etc. That’s where Platform as a Service (or PaaS for short) comes in. Machines are abstracted away from you and you cant SSH into them. All you can do is, upload your app and a cloud provider would run it for you. Cloud Provider would take care of installing and maintaining OS, patching it underneath, so you don’t have to and with every every abstraction, you lose a little control over your environment but gain an order of magnitude of productivity.
Some of the best known Platform as a Service today are Heroku, Amazon Elastic Beanstalk, Google App Engine, and more.
Think of serverless as an abstraction over PaaS. There are actually two types of serverless which I’ll talk about briefly here :
Cost savings: One of the biggest benefits of serverless computing is that you only pay for the execution time of your code. In case of IaaS or PaaS you would be charged even if your application is idle whereas in serverless there is no concept of “idle” resources and you are not charged if the function is not executed. This is especially helpful for applications that are only used a few times an hour, which means any dedicated hardware, VMs, or containers would be sitting idle for the majority of the time, and a user would be charged for underutilized resources. With serverless computing, you could build out an entire infrastructure and not pay for any compute resources until customers start using the application.
Scale: Scalability is also simple with a serverless architecture. If your code needs to scale, the platform will make copies of the function to handle the load. An example of this would be if you’re building a service like Yelp, then your peak demand is only during breakfast, lunch and dinner hours. So, the platform would automatically scale your code to thousands of instances automatically during your peak hours and would automatically scale it down when the demand subsides and you only pay for the time your code actually executes.
Developer Productivity: Serverless computing is ideal for teams that need to quickly develop, prototype, and iterate. Development is quicker since there aren’t any dependencies on DevOps. Code is usually single threaded which makes debugging easier. The build process is also broken down into smaller and more manageable chunks of code. This increases the number of changes that can be pushed through the CI/CD pipeline, resulting in faster deployment and much tighter feedback loop with users.
Less Administration and Ops: Most of the ops element of your infrastructure is eliminated. There are zero servers to manage, maintain, and scale.
Thank you for reading. I’m Nawaz Dhandala and I’m the founder of CloudBoost.io. CloudBoost is a serverless and a backend as a service company which does data-storage, search, realtime, and a whole lot more. CloudBoost is open source on GitHub under a liberal Apache 2 license. If you want to truly go server-less, you can also check out the managed offering here.
The Realtime JavaScript Backend.
15 
15 claps
15 
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
"
https://medium.com/@connorpm/a-passion-for-boring-infrastructure-1b12c6c33566?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Connor Murphy
Sep 28, 2016·2 min read
I love reading stories about infrastructure projects. Stories about runways, office blocks, housing developments, theatres, tunnels, cycle paths, pools, motorways, trains, universities, hospitals and even flight routes. Stories that I know will directly impact my overall quality of life via reduced commute times, access to better facilities, lower costs of living, healthier transport options, etc…
Infrastructure stories might be boring for many people, but I enjoy learning about them as it means more things I never have to build, own or manage myself. And I’m very grateful for the entrepreneurs, investors, politicians, community groups and companies who take on this responsibility so I don’t have to.
Equally, as a computer scientist and technology entrepreneur, I seek out ‘Infrastructure as a Service’ (IaaS) stories from the digital world. Business-2-Developer focused products that help improve the ‘quality of our professional lives’ by allowing developers to react faster, save more money, test more ideas, delight more customers and scale globally.
Infrastructure providers like Amazon Web Services, Stripe, Twilio, Github, Google Firebase, Segment, etc…. are all making the profession of software development more enjoyable, and most importantly, more efficient and impactful.
As an entrepreneur, digital infrastructure projects may at first seem boring and tedious, but they are where I think some of the biggest and most exciting startup opportunities exist. High quality, well designed, and flexible B2D infrastructure startups are already powering most of the leading B2C and B2B startups today. This will continue to accelerate as B2D startups power more and more end consumer and business applications.
Here is to more B2D startups. Here is to more boring stories.
Connor invests in learning enabled B2B SaaS startups for Techstars. I was previously Founder and CEO of Datahug. I’ve an Irish accent and live with 4 girls.
4 
4 
4 
Connor invests in learning enabled B2B SaaS startups for Techstars. I was previously Founder and CEO of Datahug. I’ve an Irish accent and live with 4 girls.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@IBMDeveloper/when-to-use-iaas-faas-paas-and-caas-9049155ebc11?source=search_post---------31,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jul 2, 2019·2 min read
Newcomers to cloud computing might be a bit put off by the sheer number of acronyms in use, as there are a fair amount. In addition, just about everything is sold as a service (aaS) these days, even transportation. To try to clarify some of the available offerings, let’s have a look at some of these aaS acronyms, namely infrastructure as a service ( IaaS), platform as a service ( PaaS), containers as a service (Caas), and serverless, which is also known as function as a service ( FaaS).
When trying to decide which of these offerings is most suited to you, one (not incredibly helpful) way to look at it is: how much money do you have to spend?
Let’s start with the most expensive solution and work our way down. If you have an unlimited budget, you might not need any of these offerings. You could buy your own building, fill it full of racks of servers and networking gear, and hire your own staff to install, run, and maintain it. You’ll probably need a fair share of air conditioning units as well.
But that isn’t likely the case. Even government entities with unfathomable budgets hire outside companies to run data centers for them. Why? Because realistically, doing all that yourself is quite a pain, and probably what brought about cloud computing in the first place. Which brings us to the first offering up for discussion: infrastructure as a service (IaaS).
You can read the full article on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
1 
1 
1 
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/buildbot/deploying-buildbot-nine-on-kubernetes-c08422597e75?source=search_post---------321,"There are currently no responses for this story.
Be the first to respond.
As Buildbot Nine is gaining more horizontal scalability features, it has more than ever its place on IaaS (Infrastructure-as-a-service). You can have the proper balance between the power you want to allocate to your build infrastructure and the cost of such an infrastructure. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.
Before following this tutorial you need:
The goal of this tutorial is to deploy a sample test infrastructure for pyflakes (similar to the tutorial of Buildbot). These infrastructure will run:
You can download all configuration files here.
Before deploying Buildbot into Kubernetes, you need to give Buildbot the URL were the webui will be available. It may depend on your Kubernetes setup (node-ip, load-balancer, ingress). For minikube you need to run:
Set the URL in buildbot-masters-config.yml
You also see the num_workers config with the value 3. You don’t need to modify this value, It already matches the number of workers in buildbot-workers.yml (under spec.replicas).
You’re ready to deploy !
If you go to the URL you provided Buildbot you will shortly see the Buildbot webui. You will see 3 workers, 2 masters. You can trigger build under Builds > Builders > runtests > force (at top right) > Start Build .
What happens ?
Kubernetes has launched pods deployements from crossbar.yml , pg.yml , buildbot-masters.ymland buildbot-workers.yml . All these are pods (set of containers). They reach each other via services.
The crossbar and the postgreSQL services are optional if you have external services. You can adjust the URL provided in the buildbot-master.cfg.yml
You see in the example that you can reach the crossbar on the crossbar host and the db on the pg-buildbot host. That is the nice feature of Kubernetes services: it creates resolvable names for services.
Workers also use service to connect to masters in buildbot-workers.yml :
This service match the two master containers and acts as a loadbalancer.
This deployement is only a sample that show the main capabilities of Kubernetes. You can look the yaml files in details, and refer to the documention of services, deployment, volumes, stateful set,and more.
Let’s play a little…
You can experiment Kubernetes infrastructure healing. You can delete pods via the webui (minikube dashboard for minikube) or via the kubectl command line, and you will see Kubernetes fixing your deployment by spawning new pods. Deleting a master will kill a build running but won’t prevent the system to accept new one ! Due to a known limitation Buildbot has difficulties to recover the loss of the crossbar router: all masters will freeze. But that’s enough for Kubernetes to detect it via the livenessProbe and to trigger a restart of all the affected masters.
You now have an highly-available CI infrastructure !
Blog posts about buildbot
35 
35 claps
35 
Blog posts about buildbot
Written by

Blog posts about buildbot
"
https://medium.com/@pascalkruettli/crm-auf-steroiden-ms-iaas-test-f%C3%BCr-dynamics-crm-2016-sp-1-6d0fe74f8871?source=search_post---------235,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pascal Krüttli
Nov 30, 2016·3 min read
Microsoft führt einen massiven Performancetest auf einer IaaS Dynamics CRM Instanz durch und lässt dabei ordentlich die Muskeln spielen.
Versuchsobjekt war eine CRM 2016 SP1 Instanz. Alle Services wurden auf IaaS gehostet (auch der SQL). Im Test wurde die Nutzung des Systems von rund 45'000 Usern auf einer rund 5 TB grossen Organisation simuliert. Der Grad an Customizations war eher gering. Lediglich einige Veränderungen (custom Fields) an Firmen, Kontakten, Leads und Tasks wurden durchgeführt. Ausserdem wurden einige Feldsicherheitsprofile angelegt.
◦Ein sehr fetter SQL Server (GS5)◦Vier Server für Async Service (DS14)◦Ein Server für Sandbox Server (DS14)◦Zehn Front-End Server (DS14)
Zudem wurden 10 Clients mit Ausprägung A7 genutzt um die Last auf das System zu generieren.Das Ganze wurde übrigens ohne Load Balancer getestet. Die Cleints hatten ein fixes Front-End welches sie bediente.
Der SQL Server verwendete SSDs für seine Logs und seine Daten und war gemäss „MS Best Practice” eingerichtet.
Ziel des Versuchs war die Stabilität von CRM auch in grossen Szenarien zu beweisen und die Leistungsfähigkeit von Azure VMs zu veranschaulichen.
Die verfügbare Rechenleistung in dem Versuch ist schon recht beeindruckend.Frontend: 10 Xeon® E5–2660 v3 mit insgesamt 160 Cores und 1'120 GB RAM
Backend: 5 Xeon® E5–2660 v3 mit total 80 Cores und 560 GB RAM
SQL:1 Xeon® E5–2698B v3 mit 32 Cores und 448 GB RAM
Clients: Total 80 Cores gepaart mit 560 GB RAM
Die Kosten für die Infrastruktur beträgt 41.20 $ für SQL und CRM sowie zusätzlich 13.20 $ für die Clients. Ergibt somit eine Summe von c.a. 1'300 $ pro Versuchstag. Ein Experiment das wir wohl besser nicht nachstellen.
◦Total 44'670 User haben parallel CRUD Operationen durchgeführt◦Um die User zu simulieren benötigten die Clients 33 Minuten Warmupzeit◦Die Performancedaten wurden alle 5 Sekunden gezogen◦Jeder User führte im Abstand von 10 Minuten mehrere Testruns durch◦Bei jedem Testrun wurde ein zusätzlicher Overhead, mit der erneuten Authentifizierung des Users, eingebaut◦Ein einzelner User generierte permanent last (keine Pause zwischen Testruns, keine Auth. Overhead)◦IE 11 wurde als Browser genutzt
◦„SkipGettingRecordCountForPaging” wurde aktiviert um die Performance bei der Datenabfrage zu erhöhen (https://support.microsoft.com/en-us/kb/2691237)◦Die Indicies auf der Datenbank würde vor dem Test optimiert◦T-SQL Scripts wurden verwendet um die Indicies der DB zu warten◦„Read committed snapshot isolation” wurde für die CRM DB aktiviert◦„SQL Server fill-factor” wurde auf 80% konfiguriert◦„Full-text search” wurde in den CRM Admin Settings deaktiviert◦Ein Fix für SP 1, welcher noch nicht offiziell ist, wurde bereits eingesetzt
Der Test dauerte drei Stunden. Es wurden durchschnittlich 528 Requests pro Sekunde verarbeitet. Die CPU Auslastung aller Webserver lag durchgehend bei etwa 15%, Der SQL erreichte etwa 35%.
Die Memoryauslastung des Webserver war mit durchschnittlich 8% (c.a. 8 GB) extrem tief, die Auslastung des SQL lag etwa bei 98% (wie das halt bei SQL Servern so ist wenn sie das Memory reservieren).
Ich fand den Versuch sehr interessant und das Whitepaper hat sicher guten Unterhaltungswert.
Microsoft macht hier klar dass die Architekur von CRM sich mit verhältnismässig kleinem Aufwand gut skalieren lässt. Die Stabilität der Plattform steht nach dem Versuch wohl ausser Frage. Solange beim Customizing kein Unfug getrieben wird haben wir hier eine solide Sache.
Der Test wurde allerdings in einer Dimension durchgeführt welcher wohl nur in den seltensten Fällen erreicht wird. Daher sind die Learnings die daraus gezogen werden können etwas geringer als ich anfangs gedacht hätte. Wer hat schon so eine Umgebung rumstehen?
Quelle: http://download.microsoft.com/download/D/6/6/D66E61BA-3D18-49E8-B042-8434E64FAFCA/Microsoft%20Dynamics%20CRM%202016%20SP1%20Performance%20Benchmark%20on%20Azure%20IaaS.pdf
See all (19)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/global-intersection/making-sense-of-big-data-as-a-service-6a3d2981e42e?source=search_post---------390,"There are currently no responses for this story.
Be the first to respond.
At this point, most people have heard of cloud technologies such as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS), Data as a Service (Daas), etc. Nevertheless, the term Big-Data-as-a-Service (BDaaS) might be a concept that not too many people are familiar with. BDaaS according to the article published by Bernard Marr “Big Data-As-A-Service Is Next Big Thing” refers to the large amount of data being created and stored as well as its analysis and industry applications.
An interesting aspect of BDaaS is the predictive analytics associated to large data sets. It is already known that organisations are making use of these data sets to drive their marketing strategies. Nevertheless, Big Data is wider than that. Industry applications such as product innovation, customer-driven marketing campaigns and sustainability are just a few of the areas that can be exploited from Big Data. There is a vast amount of information posted online every second. The site webpagefx (Internet real time) offers a good insight into the amount of data moved online. It is surprisingly large. So why not make use of it to offer services in return?
One example of an industry using predictive analysis as a way to deliver a service is OpenDNS. A company acquired by CISCO to enhance their security portfolio by adding threat intelligence. This is where predictive analysis becomes a service. Essentially, subscribers from the service redirect all Internet queries to OpenDNS. All Internet traffic is then analysed and a team of security experts develop models to understand patters and abnormal behaviours.
BDaaS combines the power of cloud technologies and applies them to large data sets. Online solutions such as Hadoop enable faster data processing by introducing a distributed computing model. This is particularly important for organisations that require to store and manage large data without facing large upfront costs incurred in creating the infrastructure needed to handle this data. In summary, BDaaS allows organisations to outsource a wide variety of big data queries and analysis to cloud service providers while delivering a “pay what you use” model.
According to an article published on cleverism website. BDaaS is classified into four different delivery models.
· Core BDaaS — Uses infrastructures such as Hadoop, Google’s Map Reduce, Spark or Java-scripts and combines this infrastructure with storage applications such as Amazon’s S3 or Hive.
· Performance BDaaS — Uses basic infrastructure while making use of existing software and hardware services in order to optimise performance. Example (Altiscale)
· Feature BDaaS — Computing and storage are kept independent from the service provider and can be fully scalable. Example (Hadoop ecosystem refined with Amazon’s or Google’s Iaas Software)
· Integrated BDaaS — This type is not currently offered by any service provided. However, if offered it would be expected to merge Performance and Feature BDaaS to allow maximum performance while supporting business owners.
Making sense of Big Data-As-a-Service is the last from a series of four that explore the definition of Big Data-As-a-Service and the industry applications derived from this technology. I would appreciate your comments and personal perception on the subject.
Some questions to consider:
Is this model applicable to all organisations?
Which organisations would benefit the most? Small/Medium or Large organisations?
Would you consider using this service in your organisation? If so, how would you apply it?
Victoria University of Wellington MMIM 522 ICT AND GLOBAL…
Victoria University of Wellington MMIM 522 ICT AND GLOBAL COMMERCE
Written by

Victoria University of Wellington MMIM 522 ICT AND GLOBAL COMMERCE
"
https://medium.com/devops-process-and-tools/automate-stack-provisioning-using-cloud-formation-vs-teraform-9c66b46b7376?source=search_post---------338,"There are currently no responses for this story.
Be the first to respond.
Automate stack provisioning using Cloud Formation Vs Teraform
Cloud Formation is service from AWS and Teraform is similar service from Hashicorp Inc to design the whole infrastructure stack (Iaas)at once and define the every component in a stack and their dependencies.
Below Aws cloud formation template example the similar deploy of wordpress site using chef in aws.
Important concepts is aws is : parameters, properties, resources,version,mappings ,find in map functions. Detailed documentation on aws cloud formation template is avaiable in the below link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html
Below terraform example used to invoke the servers in AWS and to install software using chef.
Important concept/tags in terraform : variables,builders,provisioners
We will discuss on DevOps process, Different technologies…
12 
12 claps
12 
We will discuss on DevOps process, Different technologies, Tools , OS and where exactly they will be best fitted in Implementing DevOps or CI/CD along with sample examples.
Written by
DevOps Professional. Passionate on learning, implementing and sharing new things.
We will discuss on DevOps process, Different technologies, Tools , OS and where exactly they will be best fitted in Implementing DevOps or CI/CD along with sample examples.
"
https://medium.com/geekculture/what-is-iaas-paas-aas-7ec315b55d93?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
You’ve probably heard of the various *aaS acronyms. IaaS, SaaS, etc. What exactly do they mean? Why are there so many?
To understand them, let’s go over the essential components that make up the core of software that runs on the Internet.
"
https://medium.com/nttlabs/ois-denver-2019-d8ae8aaa1f1b?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
皆さん、こんにちは！NTTソフトウェアイノベーションセンタの夏目です。2019年4月29日（月）から5月1日（水）の3日間にアメリカ合衆国コロラド州デンバーにあるコロラド・コンベンション・センターでOpen Infrastructure Summitが開催されました。Open Infrastructure Summitは以前はOpenStack® Summitと呼ばれていた国際会議であり、2010年のAustin（アメリカ合衆国テキサス州）の初回のSummitから数えて19回目のSummitです。私はそのSummitに参加しましたので、まずSummitの会場の様子を皆様にお伝えしたいと思います。
会場となったコロラド・コンベンション・センターですが、青いクマが外壁に寄りかかっているのが特徴となっています。写真で見るとあまり大きいようには感じないかもしれませんが、思ったより巨大でした。
建物の中から見るとこうなっています。後ろの自動車と比較していただければ大きさが分かると思います。
さて、受付ですが私（夏目）が受付を済ませた時間帯は初日の朝8時頃でまだ空いていました（キーノートの講演の開始は午前9時）。以前参加したSummitよりも受付の数が少なくなっていました。
受付の写真（写真5）を見ると人が少なそうに見えますが、そんなことはありません。キーノートはコロラド・コンベンション・センターの中にあるBELLCO THEATREで行なわれましたが、開場の時間が近くなると開場を待つ人たちが入口付近に続々と集まって来ていました。なお、主催者（The OpenStack Foundation）によると約2,000名の参加登録があったとのことです。
さて、キーノート講演の開始です。
キーノート講演では、様々なトピックが取り上げられていたのですが、その中に5Gがあり、デモが行なわれました。5GとOpenStackの関係ですが、AT&Tの5GネットワークがOpenStackを利用して構築されているとのことでした。
この5Gのデモですが、どのようなものかというと、ゲームのマシン（下、写真9）を使用して2人のプレーヤーが赤と青に分かれて競います。赤と青のボタンが光るので、光ったボタンをそれぞれのプレーヤーが手で押して消していき、得点を競います。ゲームをする時はそれぞれのプレーヤーは直接自分の目で台の上の様子を見ることはできず、ゴーグルを被って、そのゴーグルに映し出されるカメラの映像を頼りにボタンを押すことになります。カメラの映像は3G、4G、5Gのネットワークを通してゴーグルに送られます。ゲーム中は3G、4G、5Gのネットワークが切り替わり、映し出されるカメラの映像もそのネットワークの遅延に応じて変化します。それにより、5Gの素晴らしい低遅延（Low Latency）が体験できるということになっています。
この5Gが体験できるゲームは、キーノートの後にMarket Place（展示会場）にある「5G ラウンジ」というコーナーに展示されていました。もちろん、Summitの参加者が遊んで体験することもできました。
Market Place（展示会場）ですが、「5G ラウンジ」のほかにも「Open Infrastructure ラウンジ」というコーナーもあり、そこではOpenStack Foundationの担当者が参加者の質問に答える対応を行なっており、また、パンフレットの配布やグッズ（ボールペン、ステッカー、缶バッジ等）の配布もありました。
なお、初日（4月29日）の夕方には「Open Infrastructure Marketplace Mixer」というイベントがあり、展示会場の各ブースで食べ物、飲み物等が参加者に振舞われ、参加者が各ブースを巡ると共に、他の参加者との交流を深めました。非常に多くの参加者が展示会場に集まり、盛況でした。
以前のSummitでは参加者にシャツやタオルなど（swag）を配布していたのですが、今回は参加者へのの配布はなく、以前配布したものを非常に安く販売していました。個人的には今回もswagの配布があれば良かったと感じました。
以上、会場の様子をお伝えしました。皆様がSummitの雰囲気を感じていただけたのであれば幸いです。
さて、今回のSummitではもちろんキーノート講演や展示以外にも多くの講演（Breakout sessions）、フォーラム（参加者で議論するセッション）、ワークショップが行なわれ、5G、エッジ・コンピューティング、AI、コンテナ、CI/CD、NFV、HPCなどがテーマとなっていました。それらの講演やフォーラムの内容や様子は「Open Infrastructure Summit Denver 2019参加報告 その2」でお伝えしたいと思います。
※写真の画像はプライバシーに配慮するため、一部加工してあります。
NTT Open Source
1 
1 clap
1 
Written by

NTT Open Source
Written by

NTT Open Source
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/edurator-at-work-and-life/%E7%A7%91%E6%8A%80%E5%BE%8C%E8%8F%9C%E9%B3%A5%E7%9A%84%E5%AD%97%E8%A9%9E%E5%85%B8-%E9%9B%B2%E7%AB%AF%E5%B7%A5%E5%85%B7%E6%A6%82%E5%BF%B5%E7%AF%87-iaas-paas-saas-9624ac7f657?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
SaaS( Software as a Service), PaaS (Platform as a Service), IaaS (Infrastructure as a Service), 究竟有什麼不同呢？
下面來自 ComputeNext 的解釋我覺得很貼切也容易理解：
SaaS( Software as a Service) — 給消費者買的 (Buy)我們每一次使用，都是親自登門拜訪，「主動」把資訊「上傳」到 server
PaaS (Platform as a Service) — 給開發者利用的 (Deploy)我們每一次使用，都是把我想要的東西，從平台上「下載」下來做我想對它做的事。
IaaS (Infrastructure as a Service) — 需要成本來建設(Build)上面兩項服務 PasS 跟 SaaS，每當我們需要使用的時候，無論是「上傳」還是「下載」，我們都是要做一個「主動連繫」的動作，但 IasS…
"
https://medium.com/codica/saas-vs-paas-vs-iaas-which-is-the-best-cloud-computing-model-for-your-startup-5dcb21e049e9?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
Today more and more companies are moving their data to the cloud. This way, they cut corners on hardware and protect their sensitive information from internal data theft.
There are three main types of cloud computing which are software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS).
In this post, we will discuss the primary difference between these models. These insights will help you choose the right solution for your business.
Cloud computing means that on-demand computing services are delivered over the Internet on a pay-as-you-go basis. Simply put, this model allows storing and accessing data and apps on remote data centers. You no longer need to keep them on your hardware.
The global cloud computing market size is constantly growing. According to MarketsandMarkets, it may reach $623.3 billion by 2023.
The chart below by ZDNet shows forecasts for Saas, PaaS, IaaS revenue.
What creates the demand for cloud computing? Grand View Research defines the following reasons for its popularity:
There are three main cloud computing models: SaaS, PaaS, IaaS.
Let’s move to a detailed description of each delivery option.
SaaS (software-as-a-service) means ready software products that are delivered via the Internet on a subscription basis. If we compare SaaS vs PaaS vs IaaS, the first model is the simplest option to maintain.
Below you can see the growth of SaaS enterprise market size from 2009 to 2019 by Statista:
The most common examples are Google App Engine, Dropbox, JIRA, and others.
As an example of SaaS applications, here’s a screenshot of Jira — a project management web application:
Software-as-a-service is the most suitable option in the following cases:
We have discussed the key benefits of SaaS technology. Now let’s have a look at the downsides of this option:
As it has been mentioned, SaaS solutions offer businesses ready products. PaaS (platform-as-a-service) takes it a step further and provides clients with a cloud environment that allows developing custom apps. Thus, companies no longer have to invest in building and maintaining the infrastructure that their applications require.
What are the most famous examples of this cloud computing model? First off, we should mention Windows Azure, OpenShift, Heroku, and Google App Engine.
Take a look at extensive feature set offered by Heroku, a PaaS cloud platform:
Let’s discuss in what cases platform-as-a-service solutions can bring maximum value:
IaaS (infrastructure-as-a-service) is a self-service that gives users the opportunity to access and monitor hardware. It may include specialized processors, storage space, visualization services.
The examples are Google Compute Engine, DigitalOcean, Amazon Web Services (AWS), and Cisco Metacloud.
One of the first IaaS products that comes to our mind is Amazon Web Services.
IaaS seems the best possible option for:
All mentioned types of cloud computing define how the cloud is used in your company. More specifically, they show how you host, store, manage, and process data online.
Are SaaS vs PaaS vs IaaS very popular? Let’s see what specific figures will tell us:
The table below displays the key features of Saas vs PaaS vs IaaS.
This was an overview of SaaS vs PaaS vs IaaS cloud models. You have seen what business benefits they can bring. The choice of the right solution depends on the size and complexity of your business.
For more information about the above three types of cloud computing, check our full article: SaaS vs PaaS vs IaaS: Choosing the Best Cloud Computing Model.
If you have an idea for a great SaaS application, and need a reliable technical partner to help deliver it, let’s get in touch! Our team is highly experienced in SaaS development, we would love to share our expertise, and help you deliver a successful product.
www.codica.com
www.codica.com
www.codica.com
A Blog about technology, design, and products development…
102 
102 claps
102 
Written by
Software development consultancy. We are passionate about innovations and create great online marketplaces with Ruby on Rails, React, Vue and Angular.
A Blog about technology, design, and products development | Ruby on Rails, React and Vue.js
Written by
Software development consultancy. We are passionate about innovations and create great online marketplaces with Ruby on Rails, React, Vue and Angular.
A Blog about technology, design, and products development | Ruby on Rails, React and Vue.js
"
https://medium.com/@Apriorit/explaining-cloud-computing-models-saas-paas-and-iaas-310959505605?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apriorit
Jan 6·10 min read
Cloud computing services are on the rise and keep evolving. But it can be complicated to keep up with all the new terms along with the differences in infrastructure deployment.
Knowing the full scope of opportunities provided by each cloud service model — as well as its pros and cons — is essential for choosing the right model for a specific product.
This article will be helpful for everyone who is interested in cloud services and virtualization, wants to know the differences between IaaS, PaaS, and SaaS, and how to pick the most suitable cloud service model for a certain project.
Cloud computing is a computing services delivery model that provides computer system resources like data storage and computing power on demand over the internet.
The core technology of any cloud computing service is virtualization. It allows us to fully separate the physical hardware layer from the provided service, removing any need for the customer to purchase and maintain physical hardware. At the same time, virtualization allows cloud vendors to efficiently use their own data centers and provide more computing power to customers as needed.
The National Institute of Standards and Technology (NIST) defines the following five characteristics of a cloud service:
In traditional on-premises models, platforms, infrastructure, and applications are built on top of each other to provide a functional environment where end users can perform their tasks. Cloud services, on the contrary, allow customers to choose how much of the traditional process they want to manage.
Virtualization technology allows us to deliver a number of cloud computing services, which can be roughly split into three big groups: infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS).
These three groups comprise the proverbial cloud computing stack. Let’s explore each of them closer.
Now, thanks to the rise of containerization technology, you can also see function as a service, or FaaS, which is defined as a separate implementation of serverless computing. FaaS is focused on the event-driven computing paradigm, meaning that application code or containers only run in response to events or requests.
With FaaS, you can literally deploy a function in the cloud and let it do its work whenever it’s needed by applications. However, in this article, we’ll only look at the IaaS, PaaS, and SaaS models.
Let’s break down the components that vendors and customers manage with each service model. In the image below, orange components are provided by a cloud computing vendor and green components are handled by customers.
In the image below, we show how responsibilities are divided between a cloud vendor (orange) and a customer (green) for each service model:
To better understand which model to choose between SaaS, PaaS, and IaaS, let’s take a closer look at the pros and cons of each.
Infrastructure as a service (IaaS) is scalable cloud computing infrastructure provided on demand through means of virtualization technology. The technologies and capabilities delivered by IaaS systems are similar to those offered by traditional data centers, only without the need for clients to physically maintain or manage servers.
IaaS cloud servers are usually provided through a dashboard or an API, allowing clients to have full control over the entire infrastructure. The most popular examples of IaaS products include Amazon Elastic Compute Cloud EC2, Microsoft Azure, Google Compute Engine, Rackspace, Linode, and DigitalOcean.
A IaaS service provides a number of advantages:
Lower infrastructure costs — Instead of buying infrastructure up front, IaaS allows customers to spread the spending over time. Moreover, you can save all additional expenses regarding hardware maintenance, including the cost of having dedicated personnel just to maintain hardware.
Security and reliability — By abstracting software from the physical infrastructure, you make that software much more secure and reliable. With the IaaS model, service vendors take care of hardware reliability and availability, physical security, and the impacts of software on the hardware.
Scalability — Computing resources are provisioned on demand and can easily be scaled as needed. This means you can not only increase service consumption but also decrease it when resources are no longer necessary, allowing you to use your money as effectively as possible.
However, implementing the IaaS model may also have some drawbacks:
Legal limitations — Regulations and legal issues can prevent certain types of data, such as financial information, from being hosted outside the country. So IaaS usage may be limited depending on the laws of the country where your organization operates.
Potential security flaws — Software security may be affected if a IaaS vendor doesn’t ensure their customers can’t access data deposited to storage assets by previous customers.
Possible issues with internet connectivity — Dependence on IaaS means that if a service provider ever goes offline, your infrastructure will go offline with it. However, such issues happen rarely and are quickly fixed.
Now, let’s explore when it’s best to choose the IaaS cloud computing model for software development.
The IaaS model works great for companies that need to save money or experience spikes in demand for hardware resources. It allows smaller companies to grow without large upfront investments and quickly provision infrastructure without the need to invest huge sums of money. It also gives you the possibility to save money by scaling down existing physical infrastructure and transferring parts of it to the cloud.
Spikes in demand and the need for temporary infrastructure are probably the most obvious cases where IaaS will be extremely beneficial, allowing you to quickly provision additional resources and then scale them back.
From the service provider’s standpoint, IaaS is all about economies of scale. Multiple users share the same physical machine, allowing vendors to fully utilize resources of their data centers and maximize the return from every piece of hardware.
Platform as a service (PaaS) can be defined as middleware provided on demand via the internet. PaaS solutions mainly target developers and software vendors, since they provide them with environments in which to develop, test, deploy, host, and maintain their applications.
The PaaS model allows companies to focus on creating and deploying applications without worrying about maintaining physical hardware, virtualization, operating systems, or middleware. It offers a complete set of programming tools and computing resources necessary to start working on your solution immediately.
PaaS platforms can be divided into several categories depending on their focus and distribution method. Here are a few examples of PaaS platforms:
Software developers choose the PaaS model because of its multiple benefits.
Fully managed development environment — The service vendor manages the development environment and ensures that all data is backed up, allowing customers to focus on their development efforts.
Convenient tools — There’s no need to separately license, install, and maintain a set of tools for creating applications, as PaaS services already provide the necessary environment. Web-based tools allow developers to work from anywhere with a wide array of devices and middleware. This is especially convenient for geographically dispersed development teams.
Integration with other services — PaaS vendors introduce a single common standard through which developed software can easily be integrated with other services, either third-party or provided by the PaaS vendor.
Built-in scalability for deployed software — PaaS solutions usually offer built-in scalability for any software you create and deploy within them. Since middleware such as databases or message queues are provided by the platform itself, there’s no need to worry about scaling and backing it up.
Fast application development and delivery — Thanks to the ready-to-use environment, infrastructure, rich sets of tools, and built-in scalability opportunities, developers can create and deliver applications faster. This is especially valuable if a company needs to get products to market quickly.
Many cloud companies embrace PaaS as a way to allow developers to integrate with their own proprietary software without leaving the company ecosystem. This helps companies create development communities around their products without going completely open source.
However, such practices also raise valid concerns about getting locked in to your current PaaS vendor. It’s extremely hard to migrate an application created with custom tools and integrated with a proprietary solution, making developers dependent on PaaS services staying up and providers staying in business.
Despite such concerns, PaaS solutions keep gaining popularity. Extensive collaboration tools, the ability to spread out costs over time, and the fact that there is no need to worry about setting up and managing middleware are what make developers use PaaS products.
Software as a service (SaaS) is a software delivery model where centrally hosted applications are made available on demand via the internet. SaaS applications are fully managed by a vendor, without the need for users to worry about their configuration and maintenance.
Software as a service has proved a popular choice for CRM, ERP, financial, tax, sales management, and other solutions. The most well-known examples of SaaS products include G Suite, Microsoft Office 365, Amazon Web Services, and Salesforce.
The SaaS model is convenient for customers since it ensures automatic updates and remote access from various devices, offers on-demand scalability, and provides pay-as-you-go or subscription pricing models.
Apart from customers, SaaS development has also gained popularity among developers. This delivery model has a set of advantages for developers beyond just meeting customer expectations.
As you can see, each technology stack brings incremental advantages compared to the previous: a PaaS technology stack is easier to work with than IaaS, and SaaS is easier still. In summing up all the advantages, we can arrive at the following conclusions:
So why do we even need IaaS when we can use PaaS to build new SaaS applications and not worry about the low-level implementation behind it? The answer is simple: one size does not fit all.
Let’s take a look at the most common use cases for each cloud computing model.
Different solutions may require different levels of control over the platform and infrastructure. Simple and straightforward software can be built using a SaaS platform even without developers, while complex technical solutions often require all three stacks at the same time.
You may need to use existing SaaS solutions, use PaaS solutions to build your own additional solutions, and still need to deploy complete clusters over separate EC2 instances of customers while maintaining your own internal infrastructure on some IaaS stack all at the same time.
Doing your best work means choosing the right stack for your goals and allowing your cloud service vendor to take care of the things you don’t need to manage yourself.
19+ yrs of expert software engineering services to tech companies worldwide, covering the entire software R&D cycle. Details: www.apriorit.com/about-us/company
19+ yrs of expert software engineering services to tech companies worldwide, covering the entire software R&D cycle. Details: www.apriorit.com/about-us/company
"
https://medium.com/@deborah.martin/six-keys-to-delivering-infrastructure-in-minutes-with-private-cloud-iaas-video-1371d48f199f?source=search_post---------271,"Sign in
There are currently no responses for this story.
Be the first to respond.
deborah.martin
Aug 17, 2016·2 min read
Business needs are driving IT process innovation as IT must help the business respond faster to growing market opportunities and competitive pressures. IT needs to re-tool infrastructure provisioning processes to deliver at the new speed of business. Cloud infrastructure can help IT meet needs for increased app releases, improved satisfaction with app and infrastructure responsiveness, and reduced time to market for new services. Re-tooling to Infrastructure-as-a-Service (IaaS) with private cloud solutions can mean big reductions in both time to market and overall costs for developing, supporting and delivering new customer facing services and apps that help the business compete.
How do you make sure your private cloud will provide the capabilities and flexibility you need to meet IaaS demand for development, test, production and even some business-critical workloads in a protected environment? Watch this short video for a look at six key capabilities and how they work to ensure your cloud infrastructure will meet all its objectives for both IT and the business for rapid infrastructure provisioning.
Companies that have deployed a private cloud using these six capabilities have seen significant benefits you can put to work for your business such as accelerating infrastructure deployment by up to 93 percent like Server Cloud Canada, delivering infrastructure in minutes rather than weeks or months, and reducing costs by up to 30 percent like NICS.
You can leverage Hewlett Packard Enterprise Helion portfolio leadership in private cloud computing to help you get started implementing these capabilities with solutions like HPE Helion CloudSystem 10 that speed and simplify deployment of all these capabilities in an end-to-end hardware and software solution designed to unify and power your hybrid infrastructure. Helion CloudSystem goes beyond just automated self-service access to infrastructure services for development, test and production. It provides the expanded support for provisioning across a broad range of popular hypervisors, new containers and physical servers that enable you to turn a heterogeneous data center infrastructure into an agile, cloud-based IaaS.
You can also leverage the new Helion Cloud Suite on your choice of infrastructure which goes just beyond hybrid cloud management. It enables you to provision and manage your infrastructure and app workloads wherever they fit best in any cloud or traditional environment utilizing the full spectrum of cloud technologies.
Read the latest report from Gartner on how to achieve the full benefits from IaaS and your private cloud, beyond just basic infrastructure provisioning, to provide the enhanced services your development community needs from your cloud infrastructure.
For more information and resources on enabling and simplifying IaaS for your organization, explore the rapid infrastructure provisioning use case site.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
"
https://medium.com/kiwicode/insight-saas-16-paas-5ddaec89db39?source=search_post---------108,"There are currently no responses for this story.
Be the first to respond.
We covered why SaaS products require user experience in the last article ‘Insight: SaaS (15) Why does SaaS need user experience?’. Today, we’ll talk about PaaS, its role, and whether or not SaaS should develop its own PaaS.
Many individuals have provided definitions of PaaS on the Internet, so you can search yourself. Simply put, the ultimate PaaS provides comprehensive development and operating environment that allows you to develop an application on it without a lot of facility deployment and environment setup. The terms PaaS, SaaS, and IaaS are frequently used interchangeably. IaaS is similar to a simple network facility rental, in that it allows clients to write and deploy any code they create. A software-as-a-service (SaaS) offering solves a customer’s specific problem and allows them to use it directly. PaaS is a form between IaaS and SaaS. It eliminates a lot of coding labor as compared to IaaS. In comparison to SaaS, it offers a degree of customization, allowing it to fulfill the individual needs of consumers in specific instances. It is, however, more of a compromise solution.
PaaS accounts for only 15% of the whole Cloud market, whereas SaaS accounts for 65%. PaaS has fatal flaws if it becomes a company’s primary product and market positioning:
PaaS is in the middle of the industry chain. It is not as good as SaaS for solving client problems and solving them clearly. The main advantage of PaaS sector entrepreneurs is that they can address different problems for different consumers because of its customizability, and the largest disadvantage is that they don’t have enough focus which is decided by the characteristics of PaaS products. PaaS, on the other hand, is not as good as IaaS in dealing with the underlying, complicated challenges, and it has significant flaws in terms of flexibility and cost. IaaS is preferred by experienced developers. PaaS is in a precarious situation.
The majority of PaaS products on the market are created by SaaS companies. Why do most SaaS businesses need to develop their own PaaS? What issues does PaaS address in SaaS businesses?
In most cases, SaaS companies must create their own PaaS to meet the unique needs of Enterprise customers. Within a specific range, PaaS can solve client customization issues.
1. Without PaaS, large-scale customized development cannot be mass-produced; otherwise, it will die outright. The difficulty for SaaS companies is custom development for Enterprises. Enterprises will leave if you refuse to build SaaS for them alone. However, development necessitates a significant amount of effort, which has an impact on the whole growth strategy and can easily lead to management misunderstanding. Demand for customized development will result in one-time income, influencing the CEO’s decision. Everyone likes revenue, but these aren’t recurring revenues and shouldn’t be the primary source of money in a SaaS model. The CEO of a SaaS company must ensure that subscription revenue accounts for a significant portion of total revenue.
2. Enterprise clients are the tier who make the most money, get the most willing to pay, and have the highest renewal rate. PaaS can also save a lot of money on maintenance costs for enterprises that use customized SaaS. The needs of big companies are constantly changing at a rapid pace. If you want to keep customers at a cheap cost, allowing customers’ engineers to create and customize the software with PaaS can not only suit their needs but also reduce the SaaS company’s staff and avoid the discomfort of customization.
3. Using PaaS can broaden your consumer base. Through PaaS, more ISVs have built proper SaaS applications for various customer groups. This is a fantastic scenario; but, the prerequisites for PaaS are really high, and SaaS businesses will not be able to achieve this effect before they become giants.
We found that existing PaaS can handle a variety of difficulties, so we can define it on some levels.
1. Level-1 PaaS can customize any type, function, and authority. It requires its own DSL(Domain-specific Language) as well as comprehensive development documentation. ISVs can create apps on their own. However, it cannot break away from the scope of SaaS, and instead of writing programs line by line, there must be well-designed UI interactions that are as simple to use as feasible.
Users can be SaaS company employees, clients, or any third-party individuals who have passed registration.
2. Level-2 can add fields and key values, as well as construct autonomous business logic in a specific scenario. To help users complete part of the logic coding, low-code and no-code tools are required. However, it is unable to meet the unlimited needs of customers.
Users can be SaaS company employees, clients. Customers can design their own processes and their own data formats to make SaaS more suitable for the company’s usage habits.
3. Level-3 open API and closed development environment. Customers can design, access, and integrate open APIs into other systems, but the SaaS firm is just responsible for maintaining the API and will not have any additional functions. There is a PaaS prototype within the SaaS company that can speed up the process of customized development, but there are still many issues that prevent customers from using it themselves.
The users of the API are customers and employees of SaaS companies. Employees of SaaS organizations are the private prototype PaaS users.
Most of the SaaS companies with PaaS are at Level-3, with a few large SaaS companies having level-2 PaaS. Salesforce, Microsoft, and Oracle are among the level-1 PaaS providers. The technological level required to produce Level-3 is low, and SaaS organizations should strive for Level-3 PaaS at the very least. Level-2 PaaS necessitates a substantial investment as well as reliable architecture and new product design capabilities. And there’s a big difference between nearly getting to Level 2 and excelling at it. With the exception of a few exceptional cases, Level-1 PaaS products are frequently produced from the technical reserves of the IaaS behemoths. The complexity is really high, and it cannot be solved by money.
My response is unequivocal: They must do it. They don’t have the strength to accomplish it right now, but they must be ready to do it in the future. They must gather strength to prepare for a Level-2 PaaS before they can develop a Level-3 PaaS. Enterprise customers’ value can only be maximized by SaaS firms that have implemented PaaS. Enterprise clients are the key to ensuring NDR for SaaS companies. When serving big companies, SaaS without PaaS constantly stacks up manpower. At best, it’s easy to get dragged down, and large consumers end up becoming a burden. Enterprises’ needs can be largely addressed with PaaS, and the high-quality revenue for Enterprises can be shown.
PaaS construction requires a lot of labor costs. If you want to build one, think twice.
The next article ‘Insight: SaaS (17) Low Code or No Code is not the future’ is published. Simply send me some claps and feedback if you enjoyed my article.
If you want to join our product — Kiwicode (a code generation SaaS)’s waitlist, click here.
Let the computer program itself.
156 
156 claps
156 
Create your app, and generate its code in Kiwicode. Kiwicode enables you to design Apps fast with our tools and turn it into a runnable code project.
Written by
Plan to build a Code Generation SaaS company in the US. Join the waitlist here — https://www.kiwicode-service.com/waitlist
Create your app, and generate its code in Kiwicode. Kiwicode enables you to design Apps fast with our tools and turn it into a runnable code project.
"
https://medium.com/@aheadcrm/sap-and-microsoft-bring-their-partnership-to-the-next-level-20c555a4d7d9?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Dec 3, 2017·5 min read
On November 27 SAP and Microsoft announced a new level of their strategic partnership.
Their key messages are that
Of course it is not much of a surprise to SAP “connoisseurs” that SAP is not running its business on just one instance of their own S/4 systems — still, twelve is a fairly sizeable number to migrate to Azure. It is also not much of a surprise that Microsoft is an important and committed SAP ECC customer. As such Microsoft, of course, has plans to upgrade to S/4.
All in all this is the long due follow-up announcement to the 2016 SAP and Microsoft announcement of “empowering organizations to advance their digital transformation”. Back in the day I wrote that this announcement shows a lot of potential for the customer and that Microsoft likely will have more advantages to Microsoft than for SAP. In 2016 the announcement was also about Fiori. There is no word about it anymore today.
As an interesting aside, Microsoft announced that it will use Azure AI (Cortana) and it’s analysis services for “more efficient financial reporting and more powerful decision making”. This, of course, offers them a cross-sell opportunity as Azure AI, if working on their own systems, can be productized easily.
I do not remember that SAP has made a joint announcement to this depth with AWS. An additional swipe at AWS is that this announcement got made on day one of AWS re:INVENT, which is one of, if not THE premier AWS conference. This is a fairly clear sign that, while SAP pursues a multi cloud strategy there are clouds that may be more important to SAP than others. And vice versa, that there are business workloads that should be more important to the IaaS providers than others.
Salesforce has announced a deeper cooperation with Google as major news the recent Dreamforce 2017 conference, as I covered in my analysis of the event.
Oracle still prefers their own cloud services, although it is possible to run Oracle on AWS.
While SAP runs its own cloud with sizeable investments this announcement also sheds a light on SAP not being an IaaS player at heart but having its core on the application level, meaning PaaS and SaaS.
Yet.
The buildup of a global IaaS infrastructure is not only expensive (as one can regularly see and hear in SAPs quarterly reports and analyst briefings) but also takes time. While SAP runs data centers all over the world the company is certainly behind the key IaaS players. And I am not clear about whether SAP will (want to) run non-SCP applications in its cloud.
All this indicates a continuing trend of the software powerhouses away from AWS:
And then we didn’t even talk about the Alibaba cloud.
The build-up of camps has not only begun, but also intensified. “Commonwealths of Self Interest” are emerging.
The announcement of SAP and Microsoft is backed by not only one or two, but four joint customers of global reputation: Coca Cola Company, Costco Wholesales, Columbia Sportswear, and Coats. Apart from this being a fairly high number it also is a strong indication that this announcement is not a shoot from the hip but well prepared and bases on (beta) tested functionality.
An observation that almost has the quality of a running gag by fellow analyst and friend Holger Mueller in his analysis is that these customers’ names all start with a ‘C’ and hence may be only a part of the full list of early customers.
But more to the topic it is certainly a sign that SAP is migrating a good number of their key internal systems from their own (or their subsidiaries!) data centers into Microsoft’s.
The sign is that SAP itself is convinced that
SAP with this move basically confirms my view that Microsoft will become the key IaaS provider for larger business workloads in front of AWS.
Microsoft’s announcement of using its own Azure AI to improve decision making opens a few interesting scenarios. First, it offers options, after Microsoft connected Azure AI to S/4. Second, and more interestingly, Azure AI can become a part of SAP Leonardo by embracing its capabilities. Connecting Azure AI to SAP systems opens up a treasure trove of additional data that SAP does not have access to. This data can be used in all sorts of marketing, sales, and service scenarios — as well as in-house scenarios. It will be interesting to watch this space.
SAP customers should now evaluate their options and compare the associated cost, cloud viability and migration options.
Especially businesses that are heavily invested in both, SAP and Microsoft, could likely benefit from running SAP on Azure.
We are no more talking about reliability or security. Effectively cloud delivery is at least as reliable and secure as running IT in house. With an emphasis on “at least”!
On the longer run companies should look at whether they could leverage brokering systems that enables them to shift their workloads from one cloud to the other depending on current pricing. Being able to use different clouds also has an advantage in disaster recovery — it is highly unlikely that two clouds go down at the same time.
aheadcrm.blogspot.de
aheadcrm.blogspot.de
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
1
1
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
"
https://medium.com/terri-hanson-mead/microsoft-azure-in-life-sciences-qualifying-the-cloud-iaas-paas-c46816976faf?source=search_post---------38,"There are currently no responses for this story.
Be the first to respond.
As life sciences companies, especially biotech companies, become more and more virtual, they are shifting (or have shifted) to the cloud for applications and infrastructure. It’s the right thing to do, especially since most don’t appreciate the value of technology in optimizing business performance and getting to market faster (separate rant).
Simply throwing the responsibility over the fence to the vendors does not relieve a life sciences company of its obligations around ensuring data quality and integrity.
Savvy life sciences companies know this and qualify their underlying cloud infrastructure, whether on AWS, Google Cloud Platform (GCP), or Microsoft Azure.
The qualification process is more than a compliance activity; it can provide value to the business in demonstrating system and process controls, and ensure data integrity. It’s all about the data, after all.
In this blog post, I share my recommendations for qualifying Microsoft Azure. I will cover the same for GCP in a separate post.
For the purposes of this post, I am focusing on IaaS and PaaS, and not SaaS. For validating a SaaS solution, check out this post from 2018 and this post from 2019.
Qualification versus Validation
In this post I will use the terms synonymously but generally speaking, you qualify infrastructure and you validate applications for a company’s intended use.
ISPE GAMP5
I am a GAMP girl so my qualification approach is based on the International Society of Professional Engineer’s guidance documents as detailed in GAMP5 and the associated guidance documents.
CSV versus CSA
FDA released its draft guidance on Computer Software Assurance for Manufacturing, Operations, and Quality System Software and while it was expected to be finalized in 2020, it is now on the docket for fiscal year 2021 per FDA’s Center for Devices and Radiological Health (CDRH).
The guidance document is intended to shift validation approaches from CSV (computer system validation) to CSA (computer software assurance). If you’ve been following a risk-based approach (as defined in GAMP5) and have been using your noggin to focus on intended use and the value add to the business, there’s not much of a shift.
Risky Business
Two years ago I qualified GCP for a client (blog post to come) and followed a similar approach for Azure for another client this past year.
It may appear to be largely a documentation effort but it’s more than that. It’s all about defining what is needed for the business, executing against those requirements, documenting the baseline, and maintaining the platform in a controlled fashion.
With IaaS and PaaS (and SaaS), there is a lot of reliance on the vendor, and therefore the customer (the life sciences company) needs to oversee the vendor and the platform to ensure an acceptable level of control based on the company’s risk assessment and risk tolerance. To not do so is a risky business and compliance proposition.
Risk-Based Approach
A risk-based approach was the best thing to happen to computer systems and software when GAMP5 was released. It meant that we could prioritize what we focused on, company by company, system by system, process by process.
This also meant that those folks who liked to apply a cookie-cutter approach to computer validation had to start applying critical thinking to their validation / qualification efforts. It’s not easy if you just want to check boxes and follow checklists.
A risk-based approach offers up flexibility for companies leveraging software and technology. It is not a one-size fits all approach.
This isn’t rocket science but if you have not been through a qualification or validation effort, this will be challenging as there’s a steep learning curve. I have yet to figure out how to get my clients to understand this process until after they’ve gone through it. Expect some discomfort as you go through it for the very first time.
What is Microsoft Azure?
Microsoft Azure is cloud computing services (IaaS, PaaS, SaaS) deployed through Microsoft managed data centers. The companies I work with rely on it as the virtual backbone for virtual machines and other Azure assets to support business applications, databases, and web deployment.
It’s essential a virtual data center in the cloud.
Above and Below the Line
I draw the line at the virtual machines. Anything above that line is at the application level and should be covered under those application validation plans and activities. Below the line is in scope for the Azure platform.
If we think in terms of a bare metal data center, IT historically commissioned the servers and prepared for application and database installation.
This typically included installing things like the operating system, anti-malware software/tools, monitoring tools, backup software or agents, and other baseline tools used by IT. This is all below the line and what I consider in-scope for the qualification of the Azure platform.
Then it’s handed over to the folks managing the application project. What they do, even with the help of IT, is above the line.
Deliverables
Remember, if it’s not documented, it didn’t happen. I highly recommend that all of the validation deliverables be approved and stored in the company’s validated document management system (eDMS).
— GxP assessment for the Azure platform based on its intended use and what is expected to reside on the platform
— Risk assessment for the Azure platform and determination as to whether to audit Microsoft for Azure
— Vendor qualification asessment and possible (paper) audit
— Validation plan for the Azure platform
— Functional requirement specification (combined FRS/FRA/TMX document)
— Functional risk assessment (combined FRS/FRA/TMX document) for the Azure platform
— Technical specification documents for the platform and the virtual machines and other Azure assets on the platform; these provide baseline documentation for operation and control and are created after the migration or the platform / virtual machines / other assets are configured or installed.
— Migration plan or protocol including verification activities
— Executed migration and supporting documentation
— Migration plan summary report
— Testing summary to document any verification activities performed whether automated or manual
— Traceability matrix (combined FRS/FRA/TMX document)
— Procedures (SOPs) and work instructions (WIs) to operate and maintain the Azure platform in a controlled fashion
— Trained administrators on the platform and SOPs/WIs
— Trained personnel on the SOPs/WIs
— Validation plan final report
What is not on the list?
— Installation Qualfication (IQ): there’s nothing to install and therefore nothing to verify
— Operational Qualification (OQ): if you are focusing on qualifying for intended use, performing an OQ provides no added value
— Performance Qualification (PQ): we can debate whether this is necessary or not. My clients have decided not to do any verification testing on the platforms (Azure / GCP) and have left the verification testing to the application layer. Or they have performed automated and manual testing and have summarized in a document that they approve and store with the validation deliverables.
Procedures and Work Instructions
Assuming the SOPs are in place to support validated systems (computer validation, SDLC, change management, backup/restoration, deviation management, training, security/passwords, monitoring, etc.), the following should be considered at a minimum:
— MS Azure Platform operation and maintenance procedure including monitoring
— Work instructions to support the platform administration including administration of virtual machines and other Azure assets, backup and restoration, user and security administration, etc.
The timeline of the activities will depend on where/what you are migrating from (assuming you are), what is being migrated, the impact to the business, and the available resources to work on the configuration, migration, and qualification project.
So, it depends.
Some words of wisdom based on my experience with these qualification and migration projects:
— Work with an experienced and well-referenced Azure vendor (assuming you are working with a vendor) who understands CSV/CSA
— Work with an Azure vendor who will detail their activities and assumptions in their statement of work before you start the project. Trust me on this one.
— Create the timeline of activities and include the qualification tasks in a single project / project plan. The vendor will only care about their portion but there’s a lot more to it than the technical activities.
— Train the team on what to expect with the qualification activities and the post-migration activities. It won’t make 100% sense at first but it will at the end.
— Have the SOPs/WIs in place prior to the migration and the production cutover.
— Be flexibile. Things will not go 100% to plan. Apply critical thinking to address issues as they arise and continue to focus on the goal(s) of the qualification effort.
Qualification and validation do not end after the project is over. It starts at the beginning with a kernel of an idea and goes through to retirement.
Don’t forget to operate and maintain in a controlled fashion your beautifully qualified Azure platform.
Still Have Questions or Need Some Help?
Feel free to reach out to me with any questions you might have via email at terri.mead@solutions2projects.com or through my website SolutionsProjects, LLC. I’d be happy to have an initial call, free of charge, to discuss your qualification project.
Related article: Solutions2Projects, LLC
Living my best life…one day at a time
Written by
IT consultant, expert witness, YouTuber, helicopter pilot. Making the world a better place, especially for women. Award winning author of Piloting Your Life.
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
Written by
IT consultant, expert witness, YouTuber, helicopter pilot. Making the world a better place, especially for women. Award winning author of Piloting Your Life.
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@beth-kindig/pure-play-tech-stocks-to-benefit-from-iaas-growth-b3f2c4cfb095?source=search_post---------46,"Sign in
There are currently no responses for this story.
Be the first to respond.
Beth Kindig
Feb 8, 2019·6 min read
This is the second article in a 2-part series. The first article “Best Bet for Growth Stocks in 2019? IaaS.” can be accessed here.
One reason for Microsoft’s success with growth rates of 76% in the last two quarters is the company’s hybrid approach. This approach helps customers keep their most sensitive data on their own servers while sending workloads that will advantage from public cloud apps or real-time data analytics to Azure. This, in turn, has caused Amazon to chase Microsoft with recent efforts to improve its hybrid solutions.
The Department of Defense is a perfect example of an entity that would want to keep its most secure data with on-premise servers while leveraging the cloud for artificial intelligence and machine learning. Fortune 500 companies with substantial IP are others examples of who would require on-premise security.
Understanding hybrid is key because it gives transparency into how companies with big budgets think and how they evaluate the cloud. Security is clearly a concern as on-premise servers continue to be in demand as a counterpart to the public and private cloud. Therefore, small to mid-cap companies which help to make the cloud more secure have room for near-term growth.
Additionally, the strengths and benefits of the public and private cloud include mining data more efficiently and improving accuracy and also productivity. Therefore, any small to mid-cap companies that assist with data insights or improved work flows will have room for near-term growth. For example, SalesForce is a major growth story that came from improving both the accuracy of sales targets and productivity of sales teams.
Below are a few of the more popular stocks in the cloud space. Although it is my belief some of these are overbought, and will have to prove themselves if we do go through a bear market, it most certainly doesn’t hurt to have them on the radar and to look for the right entry point.
Zscalar is a “zero trust security architecture” that verifies identification and access. Currently, most companies use a virtual private network (VPN) as a security architecture and Zscalar improves on this by leveraging the cloud rather than physical or virtual appliances.
Risks: One of the greatest risks to these companies is the ongoing competition in cybersecurity. Cybersecurity, in general, is a hard space to create a competitive moat. In Okta’s case, the tech giants can duplicate the majority of these services. An acquisition, especially talent based, would be a good outcome for Okta. In Zscalar’s case, a competitor could come in and create a pricing war. I also noticed recently that insiders of Zscalar have been selling their stock — one at $2.1 million in stock and another at $4.5 million.
Risk: There could be a point where artificial intelligence begins to eat into Twilio’s market share. Any manual requests by users or communication done through texting, for instance, will be replaced with highly accurate voice commands. We will speak what we want rather than type what we want. Google, Amazon and Apple are quickly building this out, and the accuracy will be nearly perfect. You can read more on my analysis about the rise of AI assistants here. Twilio has clearly had amazing returns of 335%, so if you got in early, you’re high-flying right now.
My newsletter subscribers get this information first. Sign up here.
Risk: Slack filed for an IPO this week, actually. The company is choosing to do a direct listing which introduces risk as the founders and VCs don’t have to wait to cash out of the shares they sell. For obvious reasons, it’s better to have the founding team be in the same sink-or-swim boat as its stock investors (if you don’t believe your company will have returns over the next 6 months, why should I?). Direct listings for buzzy tech IPOs are relatively new, and I’m still a bit weary of them. That point aside, Slack does have serious potential for growth.
Risk: The major risk to Veeva is the current valuation and total addressable market as they are targeting a specific industry. With a PE ratio hovering around 100 and price to sales of 19, this stock is priced to perfection. Quite a few tech stocks that came of age during the bull streak (for Veeva this was 2013) may have an awakening ahead of them. If there is a good entry point, Veeva’s revenue growth will continue with analysts projecting revenue to “reach just over $2 billion by fiscal 2024.” As an individual investor, I have to make sure the first $1 billion in revenue is priced right with a fair valuation or the second billion in revenue (projected to be five years from now) won’t matter for my returns.
Risks: Similar to Veeva, Workday came of age during a raging bull market in 2012 and its valuations reflect this. It saw an 83% increase the day of its public filing and went on a tear in 2017/2018. The 52-week low is $107 and its current price is $186. With a price to sales ratio of 15, and no P/E ratio to speak of, I think we will see a better entry point than where it currently stands.
I consult for financial firms. Inquire here.
I’m an industry insider who writes free in-depth analysis on public tech companies. This year, I predicted Facebook’s Q2 crash, Roku’s meteoric rise, Oracle’s slow decline, and more. Be industry-specific. Know more than the broader markets. Sign Up Now. I look forward to staying connected.
Originally published at beth.technology on February 8, 2019.
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
"
https://medium.com/@amanverjee/michael-i-think-saas-multiples-reflect-the-forward-looking-expectations-for-this-category-e75562c20b48?source=search_post---------384,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aman Verjee
·Nov 27, 2020
Michael Tauberg
Michael - I think SaaS multiples reflect the forward-looking expectations for this category. If 30%+ growth is sustainable for 2-3 years, and FCF margins get to 20% for the category by 2025, then a bottoms-up DCF suggests this is a good bargain relative to the broader tech indices where we see 5-8x multiples of revenue (or 30x CAPE).
Also, SaaS / IaaS spend is just 25% of total IT spend and rising ... we're still in the top of the third inning here of ""software eating the world.""
6 
6 
Former C-suite at PayPal, Sonos, eBay. Now general partner & founder at Practical VC, a secondary venture capital fund.
"
https://medium.com/@salmansaleem920/iaas-vs-paas-infrastructure-as-a-service-vs-platform-as-a-service-e23b51d9868a?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Salman Saleem
Nov 14, 2019·9 min read
To begin with, many businesses are going online. They are relying heavily on the cloud to facilitate their clients, which demands to collect, storing, and processing a vast amount of data before it can be presented to the end-user as information. This is where cloud-based web applications come in to play. In this article, we’re going to discuss IaaS vs. PaaS in detail so that…
"
https://medium.com/thipwriteblog/%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%A0%E0%B8%97%E0%B8%82%E0%B8%AD%E0%B8%87-cloud-computing-service-%E0%B8%A1%E0%B8%B5%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%87-544e2c6a8c85?source=search_post---------59,"There are currently no responses for this story.
Be the first to respond.
ในปัจจุบันมีผู้ให้บริการคลาวด์ หรือที่เรียกกันว่า Cloud Provider อยู่มากกว่า 20 เจ้า แต่ที่เราคุ้นหูคุ้นตากันดีก็คงเป็นสามเจ้ายักษ์ใหญ่ ที่ติดอันดับเป็น “Big Three” ของวงการคลาวด์ นั่นก็คือ Amazon Web Service (AWS), Google Cloud Platform (GCP) และ Microsoft Azure ซึ่งแต่ละเจ้าก็จะให้บริการคลาวด์ที่แตกต่างกันไป
อ้อ! ขอย้ำว่า นี่เรากำลังพูดถึง “ประเภทบริการคลาวด์” ไม่ใช่ “ประเภทของคลาวด์” นะ
ทีนี้เรามาทำความรู้จักประเภทของ Cloud Service กันดีกว่า ซึ่งบางครั้งพวกฝรั่งเขาก็เรียกกันว่า Cloud Computing Stack เผื่อใครเอา keyword ไปใช้ research ต่อ โดยทั่วไปจะแบ่งประเภทบริการคลาวด์ออกเป็น 3+1 ประเภท
ทำไมต้องบวกหนึ่ง?ก็เพราะว่า.. ถ้าเอาแบบหลักๆ จริงๆ ที่คนพูดถึงบ่อยๆ จะมีแค่ 3 ประเภทแรก ส่วนข้อสี่ที่บวกเพิ่มมานั้นเหมือนมันเป็นตัวย่อยแยกออกมาทีหลังนั่นเอง
Step ถัดไป เพื่อให้สมองเรียบเรียงเรื่องยากให้เป็นเรื่องง่าย เราต้องสร้างความคุ้นเคยกับชื่อ types ทั้งหมดก่อน โดยที่ยังไม่ต้องสนใจว่าแต่ละชื่อมันคืออะไร เริ่มค่ะ!!
อ่ะ! ก็ยังดูยาวๆ จำยากๆ อยู่ดีใช่มะ งั้นดูปาก thip นะคะ แล้วท่องตามวนไปค่ะ…
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
Infra — Platform — Software — Function
บริการทั้งหมดนี้ ผู้ให้บริการคลาวด์บางเจ้า อาจจะเปิดทุก service ครอบคลุมทั้งสามสี่อย่างนี้ หรือบางเจ้าก็จะให้บริการแค่บางประเภท อันนี้ก็แล้วแต่เรา ว่าจะเลือกใช้บริการอะไร ของเจ้าไหน
ต่อไป สายย่อจะขอบรีฟใจความสำคัญสั้นๆ ของคลาวด์แต่ละประเภท ให้พอมองภาพออก
โดย cloud provider แต่ละเจ้าก็จะมี products ยิบย่อยและเยอะมากกก ชื่อผลิตภัณฑ์แต่ละเจ้าก็จะแตกต่างกัน ตัวอย่างเช่น Web Server ของ AWS จะชื่อ “EC2” แต่ถ้าเป็นของ Microsoft Azure จะชื่อ “App Service” ประมาณนี้
สำหรับใครที่อ่านจบแล้ว แต่รู้สึกยังจำอะไรไม่ได้เท่าไหร่ เราแนะนำให้กลับมาอ่านวนไปวันละ 1 รอบ แล้วเลือกจำทำความเข้าใจแค่ครั้งละ 1 service
แต่ถ้าใครอ่านรอบเดียวแล้วรู้เรื่องงง นั่นแปลว่าเราบรีฟได้ดีมากกก555+ ไม่ต้องอ่านซ้ำแล้วก็ด่ะ แต่ก่อนออกไปกด clapsss ให้เรารู้หน่อย จะได้หาอะไรมาบรีฟบ่อยๆ เนอะ!
อ่านบทความที่เกี่ยวข้อง
medium.com
Programming, Technology, Work Life, Finance, Storyteller, Lifestyle, Content Creator, Podcast
32 
Some rights reserved

32 claps
32 
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
"
https://medium.com/@technoblogyweb/public-cloud-more-than-20-growth-in-2018-thanks-to-iaas-22d2a0749214?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technoblogy Web
Apr 24, 2018·2 min read
According to Technoblogy, Driven by the dynamics of infrastructure as a service, the global public cloud market is expected to hit $ 186 billion by 2018, according to Gartner.
The global public cloud market is still very dynamic. Businesses are expected to invest $ 186.4 billion in public cloud services in 2018. That’s up 21.4 percent from last year, says Gartner.
The dynamism of the market is again driven by infrastructure as a service (IaaS). The segment would thus post the highest growth (+ 35.9% to $ 40.8 billion in 2018).
IaaS is not the segment that generates the most revenue, however. The prize goes to the software as a service (+ 22.2% to $ 73.6 billion expected this year). In addition, SaaS is expected to account for 45% of global software spending by 2021.
Then comes the management of business processes in the cloud or BPaaS (from $ 42.6 billion in 2017 to $ 46.4 billion in 2018). Platforms as a service (PaaS) follow (from $ 12bn in 2017 to $ 15bn in 2018). Databases as a service are the most requested in the PaaS category. At the end of 2017, Gartner excluded the advertising segment (Cloud Advertising) from its forecasts for the public cloud market.
The momentum should continue. However, Gartner still expects a stabilization of the public cloud growth rate from 2018.
And the domination of big players is accentuated. The top 10 public cloud service providers alone are expected to generate 70% of IaaS market revenues by 2021, up from 50% in 2016.
In the IaaS, Amazon Web Services (AWS), Microsoft Azure and Alibaba are in a strong position. In SaaS and Paas, Microsoft, Salesforce, SAP and Oracle are among the best-positioned companies.
"
https://medium.com/@drgutteridge/i-agree-that-this-would-require-programming-35aded3177f?source=search_post---------86,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Gutteridge
Jun 22, 2018·1 min read
Rick Whalley
I agree that this would require programming. However it is very simple and much simpler than the interface code that you have to write to use a relational database.
The IAAS services such as AWS/Google are not expensive for large memory and the prices are dropping continually.
The loading could be slow but that is a boot time operation. It might take a few minutes but that is not an issue because it is done so infrequently. I’m not sure about sudden failure. I’ve done this for over a decade and its never happened.
Dr. Lance Gutteridge has a PhD in computability theory. Presently CTO of Formever Inc. (www.formever.com) where he architects ERP authoring software.
Dr. Lance Gutteridge has a PhD in computability theory. Presently CTO of Formever Inc. (www.formever.com) where he architects ERP authoring software.
"
https://medium.com/@yuneeh/cloud-computing-highlights-2017-2020-b20217a51607?source=search_post---------344,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yunee Ham
Jul 18, 2020·2 min read
2017: SaaS, PaaS, and IaaS showed steady growth, with SaaS leading the fastest market growth. Cloud Services adoption saw a big expansion in the SMBs and the public cloud services market “grew 28.6 percent in the first half of 2017” (Gagliordi). With more varieties in cloud computing services, the competition was stiff.
2018: Both public and cloud adoptions increased. As varieties in cloud services and competitions increase, cloud services become more cost-efficient and accessible (storage capacity, quality(features), hybrid solutions, integration, etc). Azure grew rapidly and reduced AWS’s lead “especially among enterprises” (Weins), but AWS still dominated the market.
2019: Public cloud adoption expanded and AWS, Google, and Microsoft continue to dominate the cloud market. More hybrid cloud solutions are increasing, and Microsoft was able to reach number two in the market with its hybrid cloud solution. AWS adoption fell by 1% in 2019 (Kroonenburg) while other cloud computing services continued to grow. As cloud computing services grow, cyber security’s spending also increased among many businesses and organizations.
Cloud computing in 2020:
Cloud computing is essential for businesses’ targeted marketing and big data analytics today. For example, marketing is a crucial part of any business improvement, and big organizations like Facebook, Instagram, Amazon..etc hold a huge customer database that can be used to improve targeted marketing, customer retention, and acquisition. Leading CRM systems like Salesforce (and tons of other CRM services out in the market) can help to quickly get insights about the customers in the big database. With the huge database, “managing contact and target reach is a challenge for many companies’’(Muli) and this is where cloud computing services come in to optimize marketing.
And of course, there was so much more that happened with cloud computing during 2017–Present. What were your highlights? Feel free to leave comments!
References
Gagliordi , Natalie. SaaS Leads Cloud Services Market Growth in 2017. ZDNet, 6 Nov. 2017, www.zdnet.com/article/saas-leads-cloud-services-market-growth-in-2017/ (Links to an external site.)
Kroonenburg, Ryan. “What’s Happening in Cloud Computing in 2019? More.” Info.acloud.guru, www.info.acloud.guru/resources/cloud-computing-in-2019/ (Links to an external site.)
Muli, et al. “Top 20 Best Cloud Computing Examples and Uses in 2020.” UbuntuPIT, 2 Apr. 2020, www.ubuntupit.com/best-cloud-computing-examples-and-uses/ (Links to an external site.)
Weins, Kim. “Cloud Computing Trends: 2018 State of the Cloud Survey.” Flexera Blog, 26 Aug. 2019, www.flexera.com/blog/cloud/2018/02/cloud-computing-trends-2018-state-of-the-cloud-survey/
MS in Business Analytics Candidate at Seattle University l Data Analyst l Data-Driven l UW Foster — Information Systems l https://www.linkedin.com/in/yuneeham/
See all (45)
51 
51 claps
51 
MS in Business Analytics Candidate at Seattle University l Data Analyst l Data-Driven l UW Foster — Information Systems l https://www.linkedin.com/in/yuneeham/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/why-to-have-and-how-to-support-multi-cloud-environments-b8c3e35b8dbd?source=search_post---------85,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 9, 2018·4 min read
Public IaaS and PaaS clouds have existed long enough that they have become mainstream technologies. Many organizations have adopted a “cloud first” IT model, in which the cloud is the default location for deploying new applications and services.
Given this trend, the focus on the cloud has largely shifted from cloud adoption to implementing multi-cloud deployments. This article explains why multi-cloud infrastructures are useful and how to implement them.
Although multi-cloud environments are undeniably more complex than single cloud environments, there are compelling reasons for adopting a multi-provider approach to the cloud. One such reason is that using multiple clouds allows organizations the flexibility to choose best-of-breed solutions. There are certain services that nearly all of the major cloud providers offer. For example, all of the major public cloud providers offer an object storage solution, a platform for hosting virtual machines, and a database service.
Whatever the service, there is a good chance that one provider will offer a better solution than the others. One provider might, for instance, have a better virtual machine platform, while a different provider may have a better storage solution. Using multiple providers gives you freedom of choice.
Just as operating in a multi-cloud environment gives you the ability to choose a best-of-breed solution from a number of competing services, multi-cloud environments also give you the ability to shop for the best price. You might find that one provider has the best price for database services, while a different provider has the best price for virtual machine hosting.
Security and redundancy also tend to be strong considerations when implementing a multi-cloud environment. As the old saying goes, you shouldn’t put all of your eggs in the same basket. An organization might, therefore, mirror a database to a different cloud provider, so that a copy of the data will remain available in the event that the primary cloud provider suffers an outage.
Likewise, multi-cloud usage can help to keep data secure (or private). Some organizations use erasure coding to stripe data across multiple cloud storage platforms. Very often, the data is written in such a way that no one single provider has a complete copy of the data. Instead, each cloud contains only fragments of data that are useless by themselves. Hence, if a cloud provider is compromised, it will be impossible for the hacker to read the organization’s data.
There are two main things that your organization will need in order to support a multi-cloud environment. First, you will need the appropriate staff resources. Each cloud service provider has its own way of doing things, and so expertise in one cloud environment does not necessarily translate to expertise with another.
Simply put, the members of your IT staff will need to have a good working knowledge of the cloud platforms that are being used. Not every IT staff member needs an intricate knowledge of every cloud platform, but the IT department should collectively have the knowledge required to support each cloud that is being used.
The second requirement is having a strategy for moving resources to a different cloud. There are actually two different considerations involved in this requirement. The first consideration involves the actual migration of data. The second consideration is being able to use the data once it is in its new location.
Obviously, there is no universal answer to these requirements, because each organization’s needs are different. When it comes to data migration, however, the best option is to check to see what types of data migration tools the public cloud provider makes available to its subscribers.
Alibaba Cloud provides a number of different tools for uploading and downloading data, which can be useful when implementing a multi-cloud architecture.
While you may not need every one of these tools, different tools are useful in different situations. Alibaba Cloud’s tools are listed at https://www.alibabacloud.com/help/doc-detail/51654.htm and are divided into two categories–Alibaba Cloud DTPlus products and open source products. The Alibaba Cloud DTPlus products include tools such as Data Integration of DataWorks (a data synchronization tool), MaxCompute Client (a data uploading and downloading tunnel), and Data Transmission Service (DTS). DTS is a data migration tool that is designed to work with RDBMS, NoSQL, OLAP, and other data sources. The open source tools are available on GitHub and include Sqoop, Kettle, Flume, Fluentd, LogStash, and OGG.
There are a number of compelling reasons why it may be beneficial for an organization to adopt a multi-cloud strategy. When doing so, however, it is important to carefully assess the various public cloud providers in order to determine which providers will best meet the organization’s own unique needs. One of the best ways of assessing a cloud provider’s capabilities is to sign up for a free trial. Currently, Alibaba Cloud is offering a free trial at: https://www.alibabacloud.com/campaign/free-trial#free-products. This free trial includes $300 of credit, which you can use to explore Alibaba Cloud’s various cloud offerings.
Brien Posey is a Fixate IO contributor, and a 16-time Microsoft MVP with over two decades of IT experience. Prior to going freelance, Brien was CIO for a national chain of hospitals and healthcare facilities. He also served as lead network engineer for the United States Department of Defense at Fort Knox. Brien has also worked as a network administrator for some of the largest insurance companies in America. In addition to his continued work in IT, Brien has spent the last three years training as a Commercial Scientist-Astronaut Candidate for a mission to study polar mesospheric clouds from space. You can follow Posey’s spaceflight training at www.brienposey.com/space
Reference:
https://www.alibabacloud.com/blog/Why-to-Have-and-How-to-Support-Multi-Cloud-Environments_p325328?spm=a2c41.11219737.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/kkstream/%E7%95%B6-aws-api-gateway-lambda-golang-%E9%81%87%E4%B8%8A-ci-cd-1a3b3334ebc?source=search_post---------314,"There are currently no responses for this story.
Be the first to respond.
還記得早期(2015~2016)剛接觸 API Gateway + Lambda Function 時，一方面讚嘆這個神奇的東西，另一面也對於如何做到 CI/CD 相當無力，當時的工具和社群並不豐富，支援 IaaS 的工具印象只有自家的 CloudFormation 而已。
如今 Serverless 的生態系越來越豐富，工具也越來越多，今天就來介紹一個好用的 serverless 工具 — serverless.com，該工具支援多種語言，常用的 Python/Golang/Nods.js 都沒問題，也支援主流的 Cloud provider 如 AWS/GCP/Azure 等等。
sls 是 serverless.com 提供的 cli tool，用來將本地端的 code 佈署至雲端上。
必須先在自己的開發環境(或是 CI/CD 環境) 安裝 serverless，它是由 node.js 寫成的 tool，因此 node.js 也要安裝，詳細步驟請參照官方文件。
serverless 的工具包相當貼心，可以快速產生Golang 的 lambda function template，只要在 $GOPATH/src/path/to/your_project 下面執行 sls create --template aws-go --path myService 即可。
上圖可以看到 sls 已經幫你產生了三種東西，分別是 *.go, serverless.yml, Makefile
自動產生的 lambda template for Golang, 可以看出來會回應一個 JSON response，一個很簡單的 hello world function。
serverless.yaml 是該專案的設定檔，可以把它想像成是 CloudFormation 的 wrapper，事實上也的確是這樣，serverless 背後會把他轉成 CloudFormation 的 template 去發佈。這個設定檔是 serverless 的精髓所在，一切有關 API Gateway 和 Lambda 的設定都在這邊，而底層所需要的資源，他都幫你配置好了，不需要操心。
當你的專案越來越大，或是需要更動態的佈署時，serverless.yml就沒辨法寫的這麼簡單了，以下兩個連結會是你的好朋友
Makefile 嚴格說起來可有可無，就是讓你方便去 Build golang/ Deploy serverless 而已，相當簡單。
直接執行 make build 即可
可以看到在 bin/下面已經產生兩個 binary file 了，即是 hello/main.go 與 world/main.go
執行 make deploy
接下來就看到 serverless 開始在幫你 deploy 了。
同時也可以打開 CloudFormation 的介面，看看它到底在做什麼事，點進 Resources看到它幫你建立了相當多資源啊，這要我們一個一個建立是相當繁瑣的。
大約兩分鐘，一切都完成後，從 cli 也可以看到 deploy 完成的 endpoint URL，就直接打開 browser 試試看吧。
大功告成！一個 API Gateway + Lambda function 已經完成囉！那接下來還可以做些什麼強化呢？
一個基本的 AWS Gateway + Lambda function 就是這麼簡單，接著可以補上下列的設定，讓它更完整。
官方 Blog: https://serverless.com/blog/framework-example-golang-lambda-support/
All about what we have done in KKStream
26 
26 claps
26 
Written by
Backend/DevOps @Ubiquiti Inc.
All about what we have done in KKStream
Written by
Backend/DevOps @Ubiquiti Inc.
All about what we have done in KKStream
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sonm/sonm-crypto-iaas-is-here-2ab02e153d31?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
Five months have passed since the MVP release, during this time we have done a lot of work to refine the platform and included many new features.
On May 30 we will release a new version of the system, which can be considered as an evolution in the SONM development — a significant step to reliability and comfort. For a first time, it will work in the test network, and in a month will be transferred to the production network — for this month we will gather all the feedback from testnet users and make improvements to the release before moving to the production network.
On June 30 we are going to launch a full, working version of the system, which will provide IaaS, it will allow running Docker containers with any task inside. The first version of production Livenet release we called Crypto-IaaS, because it is ideal for crypto-projects and because payment in it is possible in cryptocurrency. The release will be able to perform any task, but its focus on crypto projects is primarily due to the massive pool of GPU in the SONM structure.
SONM IaaS platform is:
Using SONM, crypto projects can quickly solve the problem of lack of resources — just order the right amount of resources in the marketplace and run your Docker container on rented equipment.
We are already working with such projects as AION, Faceter, and Dbrain and are going to provide them the computing power to power up their projects. Our partner Faceter have already calculated the benefits of using SONM and published on their blog.
With this release, the SONM team, among other things, solves the task of attracting people who own mining equipment, servers, and even data centers to join our fog computing platform, offering them beneficial conditions. We offer them to become part of a global supercomputer that can perform any calculations. As a reward for the provided capacity, suppliers will receive SNM tokens.
SONM’s performance has already been tested and proven during the “Operation A,” which has become a global test of SONM abilities — hundreds of suppliers from around the world have joined SONM and gained a unique experience of international cooperation. You can read about the “Operation A” here and here.
Subscribe to our channels and social networks to follow the development of Crypto-IaaS!
Twitter: https://twitter.com/sonmdevelopmentFacebook: https://www.facebook.com/SONMproject/Telegram: https://t.me/sonm_engInstagram: https://www.instagram.com/sonm.hq/Website: https://sonm.comMVP page: https://mvp.sonm.comFog Computing Challenge: https://sonm.com/challenge/
SONM is a global fog computing platform for general purpose…
478 
478 claps
478 
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nora-winkens/saas-vs-paas-vs-iaas-overview-and-comparison-115c19d725c?source=search_post---------248,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nora Winkens
Aug 23, 2019·4 min read
In 2019, the public cloud services market was expected to reach around 214.3 billion U.S. dollars in size and by 2022 market revenue was forecast to exceed 331.2 billion U.S. dollars. | Statista
Public cloud services are gaining a lot of traction. Startups, SMEs, and enterprises are recognizing the benefits of cloud and thus, are migrating to the most relevant service provider to improve ROI, efficiency, and time-to-market.
When moving to cloud, it is important to understand the different types of services that can be availed by an organization. Software-as-a-Service (SaaS), Platform-as-a-Service (PaaS), and Infrastructure-as-a-Service (IaaS) are three popular models of cloud services. The later segment discusses what they are, their benefits, differences, and how to choose the right according to business requirements.
Software-as-a-Service (SaaS)
SaaS, also known as “on-demand software” is a software distribution model wherein a service provider hosts an application at a data center for customers to be accessed via the internet. Such a service frees up the customers from maintaining hardware or other resources to use the software. All that’s needed is a web browser or a client program.
The source code of the software is the same for all the customers and any change/updation in the software is rolled out to all the subscribers of the software. Organizations also have the option to integrate SaaS solutions with their own applications using APIs. For example, a business can create its own software and integrate the functionality of a SaaS solution through APIs.
Human capital management (HCM) software, collaboration software, and customer relationship management (CRM) software are amongst applications where SaaS has a high penetration rate. | Statista
Examples: Salesforce, Hubspot, MailChimp, Shopify, Slack
Benefits:
Platform-as-a-Service (PaaS)
In this model of cloud computing, hardware and software tools are provided, primarily for application development. In this case, the cloud service provider hosts the hardware and software its own infrastructure and make it available for the users over the internet. This not only frees the organization from investing in hardware and software to run a new application (operating system, web servers, databases, and access to a programming-language(s) execution environment, etc.). Along with this, PaaS products enable the development team to collaborate and work together, irrespective of their physical location.
By 2019, the platform as a service market is estimated to have a worth of 19 billion U.S. dollars. | Statista
Examples: Windows Azure, Google App Engine, AWS Elastic Beanstalk
Benefits:
Infrastructure-as-a-Service (IaaS)
In this model of cloud computing, a cloud service provider hosts infrastructure components on cloud, which are usually hosted on-premise. These components may include but are not limited to servers, storage, and networking hardware. While platforms like AWS, Google Cloud are examples of public clouds, organizations can set up their own infrastructure on a private cloud.
By 2019, the infrastructure as a service (IaaS) market is expected to have a worth of 38.9 billion U.S. dollars. | Statista
Examples: Google Compute Engine, Rackspace, Amazon Web Services
Benefits:
ALSO READ: The Ultimate Guide to Infrastructure Optimization on Cloud
SaaS VS PaaS VS IaaS
Every cloud model has specific features and functionality to offer that can respond to unique business requirements. Whether a software to manage routine tasks, a platform with storage options, or a fully-fledged infrastructure, there is a cloud service for almost everything. For availing the benefits that these cloud models offer, businesses can adopt a single or multi-cloud strategy considering cost, efficiency, and ROI of services.
Daffodil Software is a partner in software technology for more than 100 organizations around the world.
Daffodil Software is a partner in software technology for more than 100 organizations around the world.
"
https://medium.com/dawn-capital/how-cloud-has-won-the-state-of-the-market-in-2021-499ac4a9fbc9?source=search_post---------371,"There are currently no responses for this story.
Be the first to respond.
Cloud has won. Having lingered on the horizon for the last two decades, cloud companies across the value chain — IaaS, PaaS and SaaS (infrastructure, platform and software as services) — are now dominating software markets.
Valuations have increased on both the public and private markets over the last 12 months, and dramatically. Billion-dollar companies exist across the cloud value chain. And cloud’s still only been adopted by about 25 percent of businesses — meaning there’s plenty of headroom for further growth.
What’s driven cloud adoption so far?
Two decades ago, on-premise data centres were the norm. Since then, there’s been a slow transition to the cloud. The inflection point came in the late 2000s — at Dawn, we started to invest in around 2008, and cloud computing was still seen as very novel then. While cloud still hasn’t reached critical mass (only around a quarter of businesses have made the move from on-premise solutions so far), it’s widely accepted as standard today.
A few key factors have driven that change. What businesses love about cloud is the amount of flexibility it offers. That’s critical for growing businesses, or those that experience fluctuating bandwidth demands. It makes it easier for them to bring on additional servers or tailor to specific applications, without reconfiguring operationally-heavy hardware. Cloud is better on cost and performance, too. It allows businesses to achieve true agility, driving efficiency and slashing costs. Instead of investing in permanent racks, they can subscribe monthly or annually to the services they need.
The increasing need for collaboration is accelerating cloud adoption even more. Large enterprises are starting to move to the cloud, and not of the sort that we would have expected. Capital One is a notable example. Three years ago it would have been unthinkable that a large bank or FS business would have operated in the cloud. But thanks in part to Covid, digital transformation plans have sped up radically. Remote collaboration is more important than ever, and cloud enables it.
What’s the state of the market for cloud today?
The reality is that even at 25 percent adoption, cloud has already won. Valuations in both public and private markets have rocketed, with high multiples becoming commonplace. And they’re justified. As the likes of Snowflake have shown us, there really is a multi-billion dollar opportunity for cloud innovators. The category-defining companies of tomorrow have the potential to not only win but own markets, and become extremely large.
That said, there are still a lot of late adopters who need to be convinced. While cloud computing offers a lot of security, there’s still nervousness around potential loss of data — the cost of which can be catastrophic. In particular, some large financial institutions are still reticent about moving away from on-premise solutions and want to maintain ownership of the racks where their sensitive data sits. Some are dipping their toe into the water with hybrid infrastructure and managed cloud solutions instead.
Which key trends are shaping the future of cloud?
Security is a big one. With businesses building more diverse and distributed tech stacks, and relying increasingly on APIs to power them, there’s a need for more robust security and oversight. As the blockchain moves from consumer excitement to validated institutional tool, it could start to compete with cloud solutions. Storing files in 10 different locations on the blockchain may be even more secure than storing it one cloud location, after all. But for now, the pool of cloud security tools is growing at pace.
Visibility is another area of interest. While cloud is flexible and cost-efficient, it’s also quite opaque. Those used to on-premise infrastructure can find its slow, deliberate integration and installation a comfort; it’s clear, manageable and logical. By contrast, what can often happen with cloud solutions is that businesses get notified of issues or additional costs, but don’t know where they’ve come from. As such, there’s a growing appetite for tools that can shed light on how different pieces of the stack interact and perform.
We’re seeing more democratisation of tools across different user personas within businesses. There’s a dearth of technical talent and STEM education can’t catch up quickly enough, so businesses are keen to empower their non-tech knowledge workers.
And intra-enterprise collaboration is another key area of growth. True collaboration that goes beyond working on shared docs, to cross-enterprise data analysis and actioning, is the next challenge for cloud computing to solve.
How is Dawn investing in this space?
At Dawn, we’ve focused investment in companies optimising the utilisation of cloud for business. There’s a wealth of promising companies out there — these are some of the recent investments we’ve made in the space:
Granulate is an ‘infrastructure 3.0’ company that enables businesses to slash cloud costs while increasing performance. It uses AI and ML to look at servers all the way down to the OS level, and understand how specific applications run, their requirements and dependencies, and how to optimise them. It’s the cloud equivalent of using a smart meter to see and adjust energy consumption in real-time, rather than being landed with a bill at the end of the month.
Bryter is a no-code platform that allows non-technical knowledge workers to offer automation-as-a-service, supercharging what they offer customers. They can build their own applications, develop virtual assistants, chatbots, self-servicing applications and other intelligent automation tools, with no programming skills or experience necessary.
Harbr enables internal and external collaboration on data. The worlds of data and infrastructure are merging, and every enterprise now understands the value of collecting, analysing, and acting on data. This means they need more tooling to do just that, driving ROI not just from simpler, cloud-first workflows but from the underlying data itself. Harbr helps enterprises realise the full value of their data by generating new insights, improving decision-making and monetising data — both inside and outside the business.
Follow Evgenia on LinkedIn and Twitter
* Last month, Evgenia joined Akash Bajwa and Susie Meier on Clubhouse.They had an insightful discussion on the evolution of cloud to date, some of which has fed into this article. Akash is founder of The European Tech Club and an investor at Augmentum, and Susie is an investor at Notion.
Stories and resources from Dawn and our portfolio
1 
1 clap
1 
Stories and resources from Dawn and our portfolio
Written by
Partner @dawncapital // Software enthusiast // Russian French Londoner // @ScPoAlumni @Wharton // she/her
Stories and resources from Dawn and our portfolio
"
https://medium.com/strongnode/ama-with-ico-speaks-strongnode-ido-sne-token-user-benefits-and-future-goals-explained-a460223638a6?source=search_post---------304,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 1, 2021·5 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito joined the AMA episode hosted by ICO Speaks. Daniel answered questions about our company and shared facts about our upcoming initial decentralized exchange offering (IDO) launch and $SNE token public sale plans.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
In case you missed it, here are some of the highlights from the AMA session between StrongNode and ICO Speaks, hosted by JS94 from ICO Speaks:
What is the major goal of this project and what sets it apart?
Daniel Saito: Right now, we are planning to have a successful IDO on Starter.xyz and Bull Perks this October 6th. Immediately upon releasing our tokens into circulation, we will open up staking and AMM farming. All developed on custom code and all code will be audited by rug doctors. We have several partnerships planned in the pipeline. Besides us having StrongNodeID as a platform for SSO across others, the other thing we do differently is leverage open source technologies and add a payments layer underneath it. Ultimately, we are building a global edge network through access and monetization of idle resources.
How can we as individual end-users benefit from the StrongNode edge technology?
Daniel Saito: We utilize the consumers’ latent resources from your PC/MAC/LINUX [user environment] (CPU, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver. We have many other use cases planned as we scale out our node network with launching our innovation lab and all the fun platforms we have planned out to use the StrongNode technology. We will place mechanisms in place and people can engage by offering these services getting $SNE tokens. By ensuring your uptime and availability of your computer on the node network will allow for you to earn more tokens passively while browsing the web or watching Netflix.
How will enterprises and companies benefit from the StrongNode network and ecosystem?
Daniel Saito: Enterprises and academia are some of our main users since they usually have the big data jobs that require analysis. AI, map/reduce, be it for covid analysis or even clickstream analysis. We also expect end-users for the VPN portion, where you have an exit node on a residential non-commercial IP. Enterprise data sets are ideal for map/reduce from AI, Covid analysis to ad clickstream data. We want to attract SMB to large enterprises to tap into the network.
Good, how soon do you plan to implement it?
Daniel Saito: Well the calculations are based on large numbers, so we need to build a large community of users and get token circulation distributed. It takes time, as ROME wasn’t built overnight.
You have an upcoming IDO, what should we know about the $SNE token and its value for individuals or for companies?
Daniel Saito: But you can keep abreast of the price movement in our StrongNode pricing chat (https://t.me/StrongNodePrice).
Yes. Our IDO is almost here — happening on October 6, 2021 with Starter.xyz and Bull Perks. Join us for our IDO and get to know more about the process of whitelisting yourself to participate through this link — https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
What’s the goal of the IDO?
Daniel Saito: The goal of the StrongNode IDO is to get the IDO launched and get it in circulation. This doesn’t happen unless we list, officially and have early folks get access to this great opportunity.
Distribution is key for a token to be around, so we plan to get token distribution to the far reaches of this planet, and hopefully make a difference.
A lot of the details of the IDO will be discussed on our channel https://t.me/strongnodechat
Does the project have any Coin Burn / BuyBack system or any for investments?
Daniel Saito: The unique function of this token is that there is a % tax on all transactions. This will be used to conduct token buybacks, and the other amount placed into a SAFU fund. Token burning is a fast efficient way to manipulate your token, less tokens in circulation more value it has. I get it. But it also is a race to the bottom at the same time. We are looking to get a wide token distribution and burning tokens immediately doesn’t make sense at the start. Later yes, something to think about.
I really like your project. What areas are you focusing on the most? And what is your focus this year and next year? And what is most important in your project?
Daniel Saito: Thanks. We will first work on harnessing your computational resources first. This will produce a farm of CPU processes to help tackle some of the world’s toughest mathematical proofs to looking for aliens. Then we will work on enabling the network stack, and you can expect products like VPN and proxy. There will be other services that will launch including gaming servers on the edge.
For more information, visit: https://strongnode.io/
Join our growing StrongNode Telegram community: https://t.me/strongnodechat
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
See all (8)
454 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
454 claps
454 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/iaas-technology-company-strongnode-io-4b7314913ab7?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 1, 2021·2 min read
Newest advisor joins key executives from Cardano, Tether, and The NAGA Group as part of the StrongNode.io Advisory Board
StrongNode.io, the Infrastructure-as-a-Service (IaaS) technology company and innovation lab, announces the latest member joining its core advisory group: Diederik Van Der Reijt of Kokomo Capital Management.
Daniel Saito, CEO and Co-founder of StrongNode.io, shares his delight in welcoming Diederik to the team ahead of the $SNE token public sale. “I am happy to announce that Diederik Van Der Reijt joins the StrongNode team. With over 20 years of experience working with global finance and startup companies, Diederik is a great addition to our team as an advisor. I believe that his expertise with capital markets and vision aligns and fits with our mission to deliver our scalable node technology for the next generation of edge networking and how our users will benefit from our $SNE token,” Daniel said.
Diederik Van Der Reijt is the CEO and Founder of Kokomo Capital Management, a Hong Kong-based global business founded in 2007. Diederik held advisory roles and executive positions and invested in over 25 global companies ranging from capital markets, technology platforms, security, cannabis, trading, consumer devices, and more. He served as an advisor for NKore Biotherapeutics, a company working to save lives through non-toxic therapies with a core focus on cancer. He was a financial derivatives dealer for the Amsterdam-based firm Curvalue Group and served as Board Member for Nexus Global.
“I am honored to be the Capital Markets advisor for StrongNode as they begin to grow their edge network and deliver the future of digital connectivity through its StrongNodeEdge technology. There are plenty of opportunities where StrongNode provides solutions for issues on the traditional centralized platforms and I’m looking forward to working with the team for a better future,” Diederik said.
StrongNode.io recently partnered with Starter for their initial decentralized exchange offering (IDO) and its $SNE token launch. Starter released a guide on how users can participate in the upcoming StrongNode IDO. The StrongNode token will be available on Polygon (formerly MATIC) and will soon be made available on Binance Smart Chain, Ethereum, Cardano, and Solana.
For more information, visit: https://strongnode.io/
Join the conversation at the StrongNode Telegram Community: https://t.me/strongnodechat/
Follow us on Twitter: https://twitter.com/StrongNodeEdge
About STRONGNODE
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab that powers companies globally through its next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies. We are delivering an on-demand, secure, and scalable node technology that will fundamentally reshape the future of how increasingly valuable compute resources are accessed and monetized. We are pioneering a new paradigm in digital connectivity.
For more information, visit: https://strongnode.io/
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
3.2K 
4
3.2K 
3.2K 
4
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@derkvanderwoude/web-attacks-prevented-by-azure-waf-and-detected-by-azure-sentinel-a4d78f46100b?source=search_post---------367,"Sign in
There are currently no responses for this story.
Be the first to respond.
Derk van der Woude
Dec 17, 2020·5 min read
Azure WAF (Web Application Firewall) provides protection for web applications (IaaS, PaaS or on-premises) from common attacks (OWASP Top 10) like SQL injection and XSS (Cross-site scripting).
Azure WAF can be used on Azure Front Door and/or Azure Application Gateway, in our example we use Azure Application Gateway (simple setup).
In the setup we use DVWA (Damn Vulnerable Web Application) as the vulnerable web server (VM in Azure IaaS), another option is the Juice Shop, as our backend pool (target). Azure Application Gateway provides (layer 7) load balancing.
Azure WAF monitoring can be done via:- Azure Monitor- Azure Security Center (default)- Azure Sentinel (Azure WAF Data Connector requires the Diagnostic setting from the Application Gateway to send the data to Azure Sentinel Log Analytics)
In this blog we will leverage Azure Sentinel for Detection and (optional) Response.
Firewall mode
Although it is advised to start in Detection (monitor) mode to learn and setup the baseline, our setup is set to Prevention mode.
Prevention mode blocks suspicious incoming traffic (the threshold value is 5 or above).
The first stage of an attack is reconnaissance where the system is scanned for vulnerabilities. Nikto (web server scanner) is used on our Kali Linux machine for the remote scan, directly on the DVWA and via the WAF (I created two DNS records for distinction and to avoid the alert ‘Host header is a numeric IP address’).
The system detects the DVWA as Linux (Ubuntu) server with Apache installed and the WAF as Azure Application Gateway. The scan on the DVWA shows vulnerabilities that can be exploited.
The output on the Azure WAF did not show any information about vulnerabilities.
Azure WAF protects web applications against web application reconnaissance.
Azure SentinelAzure Sentinel (Microsoft native Cloud SIEM) can be used to detect the use of Nikto. Create an Analytics (Incident) rule with the following KQL query
AzureDiagnostics | where ResourceType == “APPLICATIONGATEWAYS” | where Category == “ApplicationGatewayFirewallLog”  | where details_message_s contains “nikto” | project Message, details_message_s, details_data_s, clientIp_s, action_s
If a remote web scan is detected an Incident is created for the SOC (Security Operations Center).
An automated playbook can be added for example to block the IP-address (clientIp_s).
SQL injection is a technique to exploit vulnerabilities via code injection. An easy example is the input field to compare values. The statement ‘0’ = ‘0’ results in the value True because 0 is equal to 0.
When go to the DVWA and/or WAF URL (logon via the DVWA default credentials), set the DVWA Security level to Low, select SQL Injection and enter
%’ or ‘0’=’0
in the User ID field.
The result via the DVWA will show all user IDs. The results via the Azure WAF is the following error, this verifies the system is working as it should be.
Azure SentinelAzure Sentinel can be used to detect SQL injection techniques.
AzureDiagnostics| where ResourceType == “APPLICATIONGATEWAYS”| where Category == “ApplicationGatewayFirewallLog”| where Message contains “SQL Injection”| project Message, details_message_s, details_data_s, clientIp_s, action_s
The output shows different SQL Injection attacks.
With the KQL Query we can create an Analytics (Incident) rule and optional block the IP-address via a playbook.
Cross-site scripting (XSS) is a technique to exploit web vulnerabilities via injecting client-side scripts into web applications to gain access control. If a web application (site) allows input like comment, username and/or email field without controls, this can be exploited via cross-site scripting.
Burpsuite can be used for a real world attack simulation but for the simplicity of this example we use the script below
<script>alert(“This is a XSS Exploit Test”)</script>
When go to the DVWA and/or WAF URL (logon via the DVWA default credentials), set the DVWA Security level to Low, select XSS (reflected)and enter the script above. The result directly on the DVWA is
The result via the Azure WAF is
Azure SentinelAzure Sentinel can be used to detect cross-site scripting attack techniques.
AzureDiagnostics| where ResourceType == “APPLICATIONGATEWAYS”| where Category == “ApplicationGatewayFirewallLog”| where Message contains “XSS Attack”| project Message, details_message_s, details_data_s, clientIp_s, action_s
The output shows different XSS attacks.
With the KQL Query we can create an Analytics (Incident) rule and optional block the IP-address via a playbook.
I hope this blogs shows value of Azure WAF for protecting web applications and Azure Sentinel to detect and optional respond to web app attacks.
Chief Technology Officer @ Nedscaper
2 
2 
2 
Chief Technology Officer @ Nedscaper
"
https://blog.containership.io/iaas-vs-paas-vs-caas-which-cloud-architecture-is-right-for-you-part-2-a72623d7d001?source=search_post---------192,NA
https://medium.com/@srivain/why-i-think-google-appengine-is-better-than-aws-b9f2178e5df4?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
Srivatsan
Oct 26, 2014·3 min read
The cloud wars are on. And, the top two contenders are Amazon and Google. Amazon offers a suite of Infrastructure services that gives you the largest A-la-Carte list of offerings. With it’s multi-OS support, elaborate set of add-on features, and robust infrastructure AWS, and is definitely the most appealing solution for most CTOs/Tech Leads.
But, early stage startups are a bit different. In addition to being short on manpower, startups also have a time crunch. This is where Google Appengine (GAE) has an edge over the standard Infrastructure Services (AWS or Google Compute Engine or M’Soft Azure).
GAE provides a sandbox for developers to build their applications on, and deploy at the click of a button. The entire workflow from building to deploying to scaling is automated in a seamless fashion that it appears to be built keeping in mind that the typical early stage company has only one or two developers to begin with. Here’s my account on how GAE helps in such scenarios. Note that some of the features are now being offered by AWS too.
Development: GAE provides a Sandbox with most of the fundamental services beautifully encapsulated inside easy to use APIs. This means, you do not have to worry about setting up your ORM or MVC layer to begin with. GAE goes to the extent of auto-creating the Indices if you are using it’s in-built database system.
Support Services: Typically modern day systems require support services such as Event Queuing, Memcache, etc. GAE provides in-built support with native APIs to handles these. AWS and other providers also have started offering similar services.
Deployment: I can testify after almost 2 years of running my own startup, that code deployments in startups cannot be as well planned as they are in large companies. I have had instances where I have deployed the code nearly 20 times a day. The basic reason for this is the lack of a robust QA team. Anyways, deploying at such a pace requires a google Continuous Integration Setup. This comes out-of-the-box with GAE, and it takes less than 60 seconds to deploy.
Scaling: Planning a public launch? or Excited about a press coverage? The obvious concern that you’ll be having is the increase in the load that your server might receive. Auto-Scaling comes of great help in such situations. GAE is virutally scalable to any extent and can return back to the minimum required when the load reduces.
Performance: Performance is not just a function of the high power processing by the server, but is also a function of google development practices. A lot of developers tend to cram-in logic into the front end calls, that make them bulky and take an exceptionally long time to respond to user requests. GAE helps you by restricting these long calls to 60 seconds, thereby forcing you to rewrite your code to perform better.
Maintenance: Once your application is like, there’s bound to be constant maintenance. Log analysis, Upgrading System Configurations, Healthcheck, Updating SSL Certificates, etc. are the part of your daily job. With GAE all these are in a user friendly console and a button click away.
Overall, in my experience GAE is ideal for the requirements of a Startup. Of course there are many disadvantages while using a PAAS system like GAE (Tight Coupling, May end up becoming more expensive, etc), and there may be requirements (like video encoding, VM creation, etc) that will make GAE a bad choice because of it’s limitations. But let’s face it, most applications do not perform such complicated functions, and there are wonderful third party services that can help you do those activities. The final decision is yours, but do consider a PAAS system when you start building something.
P.S — If you want to experiment with GAE, but would like to have the option of migrating to AWS later, do check our my post on building code that can run on both environments.
Cloud services, Mobile & Web apps programmer, hacker, CTO @ridingO
See all (66)
1 
1 clap
1 
Cloud services, Mobile & Web apps programmer, hacker, CTO @ridingO
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/chamarathilina/decentralized-resource-management-for-openstack-cloud-iaas-79279c769c27?source=search_post---------165,"There are currently no responses for this story.
Be the first to respond.
Decentralized resource management for Openstack cloud(IAAS)
In terms of cloud, resource management/metering is one of the key task of the operational team. When it comes to small and medium size businesses this is far more important due to the limited resources available.
How do we start IAAS @WSO2
Few years back we started to use Openstack as IAAS for internal usage and we have nearly 5 tenants dedicated for different purposes which is backed by OpenLDAP as a keystone. Mainly more than 400 engineers start to use the cloud for various purposes such as
For different tenants we have dedicated admins assigned who are responsible for managing instances for others and when work is done they are terminating instances upon acknowledgement by the relevant engineer.
Typical human mistakes we faced.
One of the main issue we faced with this approach was we were reaching the resource limits in the cloud, after analyzing all instances we found that relevant engineers who used the instances have forgot to inform tenant admins even after they completed their tasks. This keeps happening no matter we educate the folks so we have to find alternative way to overcome this issue.
Openstack resource monitoring with Telemetry
Mainly we decided to go with this approach to identify instances which are not used based on resource usage pattern so that we can terminate instances automatically. We used Telemetry to collect resource usage on each instances consisting of Network and VCPU data. Using Telemetry API we fetched data within a given time period and stored the average value (VCPU and Network usage) in to Mysql database per each instance. We used bash scripts to calculate the average resource usage per instance for past 5 days and if the value is less than a given threshold we trigger email alerts and if the average resource usage is lesser than the threshold for past 10 days instance will be terminated.
We tried this approach for nearly 1 year but again we faced the resource limitation. This time also it was a human error and we identified when engineers use instances to setup/test certain deployments they left the deployment running on the instances. Downside is even though people do not use instances anymore Telemetry consider those instances are being used due to the resource consumption by the running deployments.
What next?
Finally we have decided to terminate instances after given time period and the same time provide the ability to extend the instance retention period based on tenant admin’s approval. So wrote simple PHP web application which is mainly responsible for
Since we have decided to use the PHP app, we have disabled the instance creation/deletion for all other users and restricted only to the PHP application user using nova policy file role definition. So far this approach seems working fine for us in terms of efficient usage of compute resources.
Below are the sample PHP code we used for each functions.We tested this with Openstack Rocky. For some API calls it uses tenant name and for some tenat IDs.We map tenant name -> tenant ID mapping in the config file as below.
and from the code when we require tenant ID we refer as below when the tenant name is provided ($tenant).
Getting the auth token as PHP app user for a given tenant and store in the DB for other API calls.
Since we are using PHP app user, for instance creation ,this user is having access to all tenants. So from the PHP app we have to restrict tenant access. We refer LDAP structure we are using to authenticate users to find tenants a given user is belong.
sample LDAP Structure we used.
We retrieve list of tenant from above LDAP structure using below function.
We refer list of tenants returned by above code to determine list of authorized tenants for a given user.
Getting list of flavors for instance creation is as follows.
Using below function we can get details of a given flavor , such as RAM , Disk size.
Getting a list of all publicly shared images.
Getting list of networks and return based on selected tenant.
Get the SSH public keys stored under PHP app user.
Also we use below function to show the tenant quota limits.
Finally create the instances as below and store the details in DB, in the creation we can select a owner of the instance (Unless creator will be the owner), where he will be the point of contact for the given instance and he will get all alerts with regards to instance retention and termination.
Getting the instance state(ACTIVE, SHUTT OFF..etc) using below code.
For start/stop instances we use below functions.
Finally when we terminate instances we update the DB as well as it is terminated.
For Instance retention we use separate script which is executed via Cron. This will check the DB for instances details and generate alerts and terminate instances when retention period expired. Below is the table structure we use for mainly for storing auth tokens and instance details.
Other than the above details we are separately maintaining tenant admins/approvals, who can approve instance retention extend requests. We use below table structures for that.
Thanks.
thoughts and experiences on digital infrastructure
89 
89 claps
89 
Written by

thoughts and experiences on digital infrastructure
Written by

thoughts and experiences on digital infrastructure
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@marcos.moraes_85496/otimizando-custos-de-servidores-e-bancos-de-dados-na-nuvem-6fca20457861?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marcos Moraes
Mar 4, 2020·4 min read
Com a chegada dos serviços de nuvem como SaaS (Software as a Service), PaaS (Platform as a Service) e IaaS (Infrastructure as a Service), escalar servidores, torná-los elásticos e configurá-los, tornaram-se tarefas muito simples de serem executadas em relação ao contexto ""físico"" quando as empresas possuíam seus próprios servidores e profissionais dedicados para a execução dessas tarefas. Nesse contexto o desperdício acaba sendo inevitável.
Quando vamos criar uma infraestrutura de servidores para a nossa aplicação, tentamos imaginar os recursos razoáveis que serão necessários para que a aplicação tenha alta disponibilidade sempre que for requisitada. Algumas questões devem ser levantas como: quem serão meus clientes? Aplicação será interna da empresa e apenas o departamento de compras irá utilizar? Será apenas no período das 8h às 22h? Ou será um aplicativo mobile de um marketplace de automóveis?
Continuando, vamos presumir que esta empresa esteja construindo um sistema de e-commerce. Como sabemos existem algumas épocas no ano (que não são poucas) com datas comemorativas, logo, a demanda tende a aumentar significativamente, um exemplo: Natal. Ainda, outra data que recentemente tornou-se sinônimo de vendas e oportunidades é a Black Friday que ocorre em novembro todos os anos. Nesse período os acessos se multiplicam e o que não faltam são relatos de sistemas que não suportaram a quantidade de usuários e ficaram inoperantes tendo perdas consideráveis de vendas. Outro fator que impacta uma aplicação in downtime é ter perdas significativas no raking SEO do Google (veja no link). Portanto, para que esses e outros problemas sejam evitados, o mais sensato seria comprar uma máquina que suporte a demanda de requisições durante esses períodos de acesso. Mas aí vem a pergunta. E o resto do ano, onde as vendas e os números de acessos são bem mais baixos? Não precisamos de um servidor super potente, certo? Ainda, outra opção, seria comprar servidores medianos e à medida que esses períodos fossem chegando os mesmos seriam ligados em paralelo, gerenciados por um balanceador de carga. À medida que os acessos forem aumentando, mais servidores seriam ligados. Como ilustrado na imagem abaixo.
Entretanto, teríamos que comprar mais servidores cuja maior parte do tempo ficariam desligados, gerando assim um custo desnecessário. E ainda não estou falando do custo de manter profissionais dedicados apenas pra manter essa infraestrutura.
Mas, isso não ocorre apenas no mundo físico. É muito comum empresas contratarem serviços públicos na nuvem como AWS (Amazon Web Services), Google Cloud, Azure e entre outros, e subutilizarem os recursos. Por não conhecerem direito as IaaS acabam montando sua infra da mesma forma que utilizavam quando tinham servidores privados em seus próprios datacenters, com máquinas robustas de super configuração e elevando de forma desnecessária seus custos.
Imagine outro cenário. Quando uma empresa possui sua própria equipe de desenvolvimento de software, ela possui (ou deveria possuir) outros ambientes como desenvolvimento e homologação. Uma vez que temos mais dois ambientes, logo, teremos uma replicação desses servidores e recursos, logo, mais custos são gerados. E ainda estou falando apenas de um sistema simples, mas se formos pensar em um servidor web, um servidor de banco de dados, um servidor de mensageria e quantos mais forem necessários para escalar a aplicação, dá pra ter uma ideia do tamanho do problema.
Entretanto hoje quando utilizamos serviços como IaaS ficou muito mais fácil customizar isso. Vejamos, por que precisamos deixar ligado todos esses servidores de desenvolvimento e homologação? Ao menos que você seja uma empresa gigantesca onde sua equipe de desenvolvimento trabalha em turnos, isso torna-se um custo desnecessário. Ainda, podemos criar servidores para esses ambientes com configurações inferiores ao de produção deixando os custos mais baixos, bem como criar uma escala de servidores sob demanda, ou seja, subir mais servidores apenas quando houver picos de acessos. Mas, como eu faço isso? Através de alarmes que podemos criar baseados nas métricas que a própria plataforma oferece. Um exemplo: quando meu servidor chegar a 70% do seu uso máximo de capacidade, posso configurar um balanceador de carga para que outro servidor suba e compartilhe a carga. Quando os acessos diminuírem o balanceador de carga “mata” a máquina que acabou de subir, ou seja, aquele custo de manter servidores ligados o tempo todo torna-se desnecessário. Outra dica seria criar as instâncias de desenvolvimento com máquinas spot ou instâncias reservadas.
Isso parece muito simples e óbvio, mas acredite já vi muita empresa e principalmente startups em início de jornada terem seus custos elevados por conta desses gastos desnecessários. E isso faz toda diferença quando está começando.
Bom, esses foram apenas alguns exemplos de customização que podem fazer muita economia tanto de recursos quanto de custos. Existem diversas outras formas de customizar uma infraestrutura bem como reduzir custos.
No próximo post, vou mostrar na prática como criar um alerta no CloudWatch que dispara uma trigger nas instâncias do EC2 em uma cloud na AWS. A partir dele desligar os servidores que não tem necessidade de ficar ligado no período das 22h às 6h.
Até mais!
6 
6 
6 
"
https://medium.com/@benofben/nosql-in-kubernetes-couchbase-operator-on-container-engine-for-kubernetes-c7a10e5500f4?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Lackey
Jun 18, 2019·3 min read
In our last post on Couchbase, we covered how to run Couchbase on OCI IaaS with Terraform. That’s all well and good, but now, as they say, for something completely different…
This week Kubecon is going on in Seattle. Oracle is there with lots of announcements, many of which are part of the Oracle Cloud Native Framework. This builds on earlier work we’ve done, joining the CNCF as a Platinum member and releasing a managed Kubernetes service called OKE.
Conventional Kubernetes wisdom is that it’s great for running stateless containers, but not stateful workloads. This means recommended architectures have had application servers managed by Kubernetes with databases running outside on VMs or bare metal. This approach performs well, but negates one of the biggest advantages of Kubernetes — providing a single orchestrator that can be used to manage an entire application. The result is a no you have two problems situation where you get to manage software both inside and outside of Kubernetes along with the connections between the two. It’s not fun.
The Kubernetes community recognized all this as a limitation and tried a number of approaches for running stateful workloads. The first was called Pet Sets, drawing on the pets/cattle analogy common in the cloud. This was later renamed to StatefulSets. That model was then superseded by something an approach called Sidecar, which suffer from a variety of issues including single point of failure. All that culminated in CoreOS proposing a model called Operator which uses a Custom Resource Definition (CRD) to manage a stateful application inside of a Kubernetes cluster.
A variety of stateful pieces of software now run with the Operator model, including Confluent, Hazelcast and Couchbase. If you’re looking for an industry leading NoSQL database to build your Kubernetes application on top of, Couchbase is an obvious choice. Couchbase itself is a great product, with all the features you’d expect in a NoSQL database. The Couchbase Autonomous Operator distinguishes it a bit further.
We’ve partnered closely with Couchbase to make it easy to deploy the Operator on OKE:
“Couchbase is collaborating with Oracle to bring the benefits of the Oracle Kubernetes Engine (OKE) to applications like the Couchbase Data Platform. With Couchbase Autonomous Operator for Kubernetes now one can easily deploy Couchbase Data Platform on OKE.”
Anil Kumar, Director of Product Management at Couchbase
The Operator allows you to run your database next to the application in Kubernetes, lowering latency and simplifying administration of the application as a whole. The operator also blurs the line between enterprise software and a managed service, automating many tasks including:
Because the Operator relies on the CRD API, it can run on any conformant Kubernetes distribution. This includes, of course, OKE. Getting started with the Couchbase Autonomous Operator on OKE is really easy.
First off, you’ll want to deploy an OKE cluster. Instructions to do that, along with a Terraform module that automates the process, are here.
With your Kubernetes Cluster up and running, the next step is to deploy Couchbase. We’ve created a walk through for that here.
As always, it’s been a pleasure working with the Couchbase team to set this this up. Special shout out to Tommie McAfee at Couchbase for helping with some permissions issues! If you have any questions, please drop me a line at ben.lackey@oracle.com or on Twitter @benofben.
Enterprise software guy. Interested in everything. I enjoy books, bikes, hikes and meeting new people.
3 
3 
3 
Enterprise software guy. Interested in everything. I enjoy books, bikes, hikes and meeting new people.
"
https://medium.com/@bernard.orzechowski/infrastructure-as-code-does-it-benefit-regular-software-architects-engineers-b2182301a38?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bernard Orzechowski
Nov 6, 2019·8 min read
Is it Infrastructure as a Service (IaaS), Platform (PaaS) or Software (SaaS) as a service, most companies are using in production or at least experimenting with cloud technologies. The popularity of AWS, Azure and others has many roots, some of them being the ease of use, scalability, interesting pricing models (e.g. pay as you go). One of the main trends in the DevOps movement is the rise of the concept of Infrastructure as Code (IAC).
There are lot of tools implementing this concept (Terraform, Chef, Ansible, Puppet, Salt, CloudFormation, …). They integrate very well with the CI/CD pipelines, frameworks, tooling. Usually a DevOps team at some stage uses an IAC tool to standardize, automate and provision different components of software stacks like networking (VPCs, gateways, ..), security, clusters (Hadoop, Kafka, Redshift,…), databases, also servers (the requirement is that a server/resource can be provisioned by calling an API) for microservices / applications, the range of services is usually broad. An example can be found here.
One of the interesting questions related to this is if and possibly how can IAC benefit classical software architects or software engineers, who build applications, microservices, databases, data lakes, not being a day to day DevOps engineer? Do I need to be an Platform Engineer to benefit from IAC or could it potentially benefit me too, adding value to other skills like programming/design, in general being a better developer/architect? Does it make sense to master IAC? Which benefits can it have? Will it reduce technical debt in my current project? Will it help with the next RFP? How can I, as a software architect/developer, benefit from IAC?
According to Wikipedia „ Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. The IT infrastructure managed by this comprises both physical equipment such as bare-metal servers as well as virtual machines and associated configuration resources. The definitions may be in a version control system. It can use either scripts or declarative definitions, rather than manual processes, but the term is more often used to promote declarative approaches.” (https://en.wikipedia.org/wiki/Infrastructure_as_code)
The most fundamental idea here is to treat your infrastructure (IaaS / PaaS / SaaS) as you would treat software code. Network components, servers, cryptographic devices, specialized appliances, every component of an technical infrastructure is treated as a software defined object. To illustrate this on the following diagram we create the definition of an AWS network for our project, a public subnet and a NAT Gateway that will be used later by resources created in private subnet of this network (that would also be defined in this file). We can execute this file against any AWS accounts. This would have 2 consequences:
The above mentioned tools (Terraform, …) provide high level programming/configuration languages that internally leverage the cloud services APIs to instantiate / destroy / update resources. With the adoption of cloud platform providers this is already a standard, but it is important to realize this conception. By doing this suddenly best practices applied to classical software development projects can be applied to infrastructure also. Hence all what we learned so far writing code and CI/CD pipelines can be applied also here (inclusive lint tools, style checkers, etc).
Standard IDEs used for java and other programming languages can be applied to IAC projects. On the diagram below there is an example of an IAC project using IntelliJ and Terraform plugin (syntax highlighting, syntax check, code formatting,…).
There are multiple drivers for IAC, to name a few:
To set understanding there are 2 basic areas we can split IAC applicability: infrastructure provisioning (e.g. Terraform, CloudFormation, …) and configuration management (Ansible, Puppet, Chef, …). They are both explained on the diagram below.
The first is about provisioning running services based on provided configuration on service level (e.g. provide EMR Hadoop Cluster parameters), the latter is about customizing those services (usually compute services like EC2) to meet requirements of a given application stack — e.g. after we have provisioned an AWS EMR Cluster (infrastructure provisioning) we may want to setup user / groups and configure them to work with Active Directory (configuration management). We could use Terraform/CloudFormation for the first one, Puppet/Chef/Ansible for infrastructure configuration. For the last mile of solutions deployment, the services / applications deployment itself, IAC is not applicable. It may be to some degree, but there are other options. There are different and better suited tools like Kubernetes, Docker, Helm, Jar files, etc to accomplish this. IAC tools are best applied to IaaS, PaaS and SaaS.
So the question is it worth to invest time in IAC? Lets see where we could leverage it, focusing only on the aspects related to work of regular software architects (java, web, big data, …..) and developers (scala, java, web, ….).
The first advantage we get is the ability to quickly build solutions / POC, to experiment. In which situation can we use it? E.g. for showing to an existing customer that we could provision quickly an end 2 end solution / proof of concept (POC) and if needed copy it x times with the guarantee that each time we will deploy the end result will be the same. Leveraging cloud and IAC we could collaboratively build a solution — some team members working on networking and security, some on application components, some on serverless / managed services. It would cost only for the time the services are provisioned and running, no need to buy new servers. We could accomplish the same of course by hand, e.g. clicking through a web interface (e.g. aws management console), but it has many disadvantages:
Another reason is the ability to create environments on demand. We know that they are in constant demand and every developer often needs to test in isolation. We could link a terraform project with a Jenkins pipeline and each time a commit or pull request on let’s say dev or feature branch happens, a new environment gets created, private to the commit/pr, versioned (because we can store it in git), reviewed and ready to be reused (the IAC code for the environment).
Having the components of our technical architecture described in software adds a lot in the aspects of quality and security. What can we do with our infrastructure if its stored in git, defined as code? We can
Another aspect is learning itself. Most of the tools have community and / or official modules covering broad range of services (https://registry.terraform.io/). If we need e.g. Active Directory to verify / test how it integrates with the other components of our solution we can deploy it just by referencing it’s official module (e.g. https://registry.terraform.io/modules/neillturner/microsoftad/aws/0.2.0 ). No need to build it from scratch. We can spend our time more efficiently focusing on the integration itself.
To start a very good article that focuses more on the the IAC tools and how they differ: https://blog.gruntwork.io/why-we-use-terraform-and-not-chef-puppet-ansible-saltstack-or-cloudformation-7989dad2865c
Analytics solutions architect / data/software enginner / devops who enyoys working on Analytics@AWS/Big Data/Cloud/Data Warehousing. Owner of beClouData.
Analytics solutions architect / data/software enginner / devops who enyoys working on Analytics@AWS/Big Data/Cloud/Data Warehousing. Owner of beClouData.
"
https://medium.com/@felixklauke/everything-as-a-service-von-iaas-%C3%BCber-paas-bis-zu-saas-6fb54ecb4e95?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Felix Klauke
Aug 27, 2020·4 min read
Die Cloud gibt es nicht. Es gibt nur den Computer von jemand anderem. Und doch kann man mit diesem Computer meist so viel mehr anfangen, als mit dem eigenen. Kleine und mittelständische Unternehmen setzen auf Cloud Dienste um die Qualität ihres Services hochzuhalten. Wir beleuchten die verschiedenen Modelle und Vorteile des Cloud Computings.
Everything as a Service: IaaS, PaaS und SaaS
Wir machen nichts mehr selbst. Alles wird an externe Dienstleister ausgelagert und “as a Service” betrieben. Nicht einmal die Kaffeemaschine gehört dem Unternehmen, sondern wird geleased und von einem Externen Lieferanten befüllt und gewartet. Kommt dir das bekannt vor? In allen Geschäftsbereichen werden Aufgaben, die sich nicht effizient intern lösen lassen, ausgelagert. Das kann eine Kaffeemaschine sein, oder die gesamte IT-Infrastruktur.Das Firmenwiki, das Newsportal, die Build-Server, die Homepage — Warum sollte man sie noch selbst hosten? Der externe Dienstleister macht das schon. Am Ende entsteht eine “Alles ist ein Service Kultur”. Wenn man sich mit der Cloud beschäftigt fallen dann unmittelbar die Schlagworte Infrastructure as a Service, Platform as a Service und Software as a Service.
Was haben wir denn nun ausgelagert? Die Cloud Provider bieten dir diese drei Modelle an und verlangen, dass du dich entscheidest. Doch welches Modell ist das richtige für dich?
Man spricht von Infrastructure as a Service (IaaS), wenn nur die Infrastruktur bestehend aus Netzwerk, Rechenleistung und Speicher bereitgestellt wird. Der Cloud Provider bietet dir zum Beispiel eine Virtualisierungsplattform und alles vom Betriebssystem über die Runtime bis hin zur Endanwendung liegt beim Kunden. Klauke Enterprises stellt in diesem Fall eine Umgebung wie Proxmox, OpenNebula oder OpenStack bereit. Bekannte Vertreter sind außerdem Amazon EC2 oder die Google Compute Engine.
Der Platform as a Service (PaaS) setzt auf dem Infrastructure as a Service Gedanken auf und übernimmt weitere Managementaufgaben. Während bei Infrastructure as a Service die Managed Services bei der Virtualisierung aufhören, geht Platform as a Service noch einen Schritt weiter und managed as Betriebssystem (OS) bis hin zur Laufzeitumgebung. Ein prominentes Beispiel ist die Google App Engine oder Heroku.
Die Platform as a Service überlässt dem Benutzer die Entscheidung über die Datenhaltung und die betriebene Anwendung. Bei Software as a Service (SaaS) wird auch diese Ebene entfernt und es wird eine direkte Endanwendung bereitgestellt. Diese Art des Betriebs wird oft als “On-Demand” bezeichnet und nimmt dem Benutzer alle übrigen Sorgen von Installation über Betrieb bis hin zur Wartung. Man kennt dieses Prinzip von der G Suite oder Office 365.
Vorteile der Cloud
Um diese Konzepte optimal für dein Business nutzen, musst du die konkreten Chancen der Cloud erkennen. Der größte Motivator sind die Chancen zur Kostenreduktion. Anstatt teure eigene Infrastruktur zu warten, die im schlimmsten Fall ungenutzt herumsteht kann die Cloud genau auf die eigenen wirklich vorhandenen Bedürfnisse angepasst und bei Bedarf erweitert werden. Das spart Kosten und Aufwände.
Ein oft genannter Nachteil der Cloud ist die Sicherheit — Das ist aber ein Trugschluss! Die konventionelle Sicherung von Inhouse Daten ist erheblich aufwändiger, als wenn ein Cloud Provider deine Daten mit einem groß angelegten Sicherheitskonzept schützt. Laut RapidScale können 94% aller Cloud Anwender sogar eine erhöhte Sicherheit feststellen.
Die Flexibilität und Skalierbarkeit der Cloud sucht ihresgleichen. Sie kann die gewünschten Mehrwerte auf Knopfdruck bieten und gibt dir die Möglichkeit, dich auf deine Kunden und dein Kerngeschäft konzentrieren zu können. Deine IT-Teams müssen sich nicht mehr um die Wartung oder die Erweiterung deiner Infrastruktur kümmern. Neubeschaffung, Erweiterung und Rückbau funktionieren auf Knopfdruck und brauchen keine Aufwände oder lange Entscheidungswege zwischen Abteilungen.
Wenn du eine Cloud betreiben möchtest, dann brauchst du zuerst die zugrundeliegende Infrastruktur. Der Aufbau dieser Infrastruktur wird als IaaS bezeichnet. Um auf dieser Cloud eine Anwendung zu betreiben bedarf es einer Entwicklungsumgebung für die Entwickler, einer Platform, auf der Anwendungen ausgerollt werden können — Man spricht von PaaS. Die komplette Auslagerungen der Software als komplett Cloud-basierte Software, die man als Endnutzer anmietet und direkt loslegt wird als SaaS bezeichnet.
Auf welcher Ebene arbeitest du? Welche Vorteile der Cloud haben dein Business am weitesten nach vorne katapultiert?
Erfahre mehr über die Cloud Dienste von Klauke Enterprises und wie dir Software Defined Storage, Software Defined Networking und Software Defined Datacenter die Arbeit abnehmen:
Originally published at https://blog.klauke-enterprises.com.
20, working @itemis, loving infrastructure and backend services and networking, devops by night, privatizing the world peace, only doing the extravagant jobs
1 
1 
1 
20, working @itemis, loving infrastructure and backend services and networking, devops by night, privatizing the world peace, only doing the extravagant jobs
"
https://medium.com/@rustysparkplug/iaas-vs-paas-comparing-the-top-2-cloud-models-dae69eb93e6d?source=search_post---------243,"Sign in
There are currently no responses for this story.
Be the first to respond.
ْ
Aug 12, 2017·6 min read
Roughly nine out of 10 companies today use some type of cloud technology. While Amazon Web Services (AWS) is currently ranked as the leading cloud service provider, according to TechMayan’s State of the Cloud report, there are countless other vendors offering similar cloud services.
Studies show that nearly go% of companies are either currently using cloud technologies or planning to use them in The near future. Cloud computing offers countless benefits for companies, including reduced overhead, improved accessibility, flexible pricng, automatic updates and backups, professional support and more. There are several different types of cloud models from which to choose, however, with the three most common being laaS, PaaS and SaaS. So, what’s the difference between these three models?
Infrastructure-as-a-Service (laaS) Is a self-service cloud model in which a cloud provider sells virtualized hardware resources (computing infrastructure) for the customer to use fora specified length of time (hourly, weekly, monthly, annually, etc.). The cloud provider handles tasks such as server setup and maintenance, data storage, cyber security, network and virtualization, whereas the customer handles the operating system, applications, data, run time and other processes.
Some business owners may tum their head at the thought of paying $.15o a month to rent a server using an laaS model instead of Just buying a server. With IaaS, however, the customer can choose specific computing services based on his or her needs. IaaS providers supply virtualized hardware resources from pools” of servers and equipment in local data centers, allowing customers to buy only what they need. Furthermore, customers don’t have to worry about installing and maintaining their hardware, resulting in lower overhead costs. IaaS can be used for a wide range of applications, some of which include the following:
IaaS isn’t right for everyone, however. In order to deploy software, customers must first install an operating system on the cloud, after which they can install their software. laaS customers are also responsible for updating and maintaining both the operating system and any applications or software installed on the cloud. So, while laaS Isthe most robust of the three main cloud service models, It’s also most laborious.
Platform-as-a-Service (PaaS) goes one step further than laaS by offering the same infrastructure services as well as operating systems and development tools. With laaS, it’s the customer’s responsibility to install and maintain the operating system, databases, applications, tools, etc. With PaaS, however, these tasks handled by the cloud vendor. The customer receives direct cloud access to development tools hosted by the vendor, allowing for rapid building, testing and deployment of applications.
PaaS is commonly used for the following: Creating and testing applications in a framework Application execution.
IaaS, PaaS, and SaaS are all cloud offerings that have numerous benefits, but most business owners aren’t aware of the advantages. Cloud services can save you thousands of dollars each month in IT infrastructure and staffing. If you’ve wondered if cloud applications are right for you, this article discusses the three main cloud offerings available to small and large businesses.
Desktop applications and large on-premise network resources are out. Cloud services are in, mainly for the cost savings and the numerous advantages they offer for businesses. It’s overwhelming for a new business owner unfamiliar with cloud servicesto find the right one that matches organizational requirements. Here are the three primary, traditional cloud resources that you can integrate into your IT infrastructure for a reasonable cost.
SaaS Is the most common cloud service. Chances are, you’ve used a SaaS service at least once either as a customer or for your business resources If you’ve ever worked with software that runs in the cloud, you’ve worked with SaaS. Most desktop software developers have moved their application to the cloud. For instance, Microsoft Office and Adobe Create Suite both run in the cloud. Ten years ago, you needed to buy the software, have it shipped to your office and install it on your desktops. With Office running in the cloud, you don’t need to install anything but basic files to execute the application. Adobe offers the same kind of setup where you install only the components you need to run the software instead of the entire package.
With SaaS, you also have a lower investment to get started. Using Microsoft Office again as the example, you used to spend at least $250 to buy the software and install it on your machine Now, you can only install the components you want and pay a small monthlyfee. You can pay either $99.99 a year or $9.99 to use Microsoft Office in the cloud, which is a much smaller investment for startups on a budget.
PaaS is a more advanced infrastructure that many businesses don’t take advantage of, but it offers numerous benefits.
A PaaS Is any website that provides a full environment hosted In the cloud. The business can nun most of its services on a PaaS site, and the provider deals with many of the maintenance and businesstasks that would otherwise be necessary for your business to manage. A good example of PaaS Is Salesforce, which many companies Integrate into their sales platforms for its flexibility and versatility. You don’t need to mold your sales procedures to fit Salesforce. Instead, you can customize Salesforce to meet your own sales needs. It also offers an API that you can use to integrate the browser version of Salesforce into your backend. You can follow your entire sales procedures on Salesforce, from cold calling to making a sale to managing the customer after the sale, making it a complete platform In the cloud.
PaaS environments are complex but beneficial because It’s complexity that the business no longer needs to manage. Thinking of one cloud application handling all of your business procedures will help you separate a SaaS from a PaaS offering. PaaS is usually more expensive, so this distinction will help you determine your IT budget for cloud software.
IaaS offers something much different than the previous two cloud services. laaS extends your existing hardware and Integrates it into your local resources. You can get entire network segments that run in the cloud. You can add hard drive storage, routers, servers, and switches to your network and have them run entirely in the cloud.
A good example of laaS is Google Drive. Google Drive Is an affordable way to add storage to your network. It’s cloud storage at a fraction of the cost it would take to install terabyte drives and servers to manage those drives Google even gives you the first 15GB for free, and you pay as little as $1.99 per month for drooGB up to $99.99 per month for ioTB capacity. It would cost thousands of dollars upfront to host this kind of drive space on your local network.
You can have a small one-office setup and use every cloud service available to run an empire. The services you choose will depend on your budget, but you can run everything you need to manage your business in the cloud. You can start with low-cost monthly services and upgrade as your business grows. Plug: If you are looking for a great SaaS developer based in Chennai, connect with Open Brace.
If you need software for your business PaaS and SaaS are the right cloud services PaaS is an entire platform environment, so it’s usually more costly but has every feature you need to run each department. Think of laaS as hardware. If you need to extend your network resources, then you need laaS. Prices for these services vary depending on licensing, users, and the number of resources that you need. Check with your cloud provider, because you can spend anywhere from a few dollars a month to several thousands of dollars a month for cloud services.
Originally published on August 12, 2017.
"
https://medium.com/@alibaba-cloud/backend-as-a-service-baas-for-efficient-software-development-6a7a142af477?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 15, 2018·10 min read
First-generation cloud services, such as Alibaba Cloud IaaS and PaaS products, eliminate the need for managing servers and O&M systems by deploying cloud data centers. Backend-as-a-Service (BaaS) represents the second-generation cloud service platform. BaaS can further simplify and optimize cloud computing resources and provide all-in-one cloud services including development, O&M, and service management.
BaaS can package public cloud data center resources based on frontend application scenarios and provide them to developers through simple interface invocation. With those benefits, developers can focus on studying users, creating and designing app software, and developing mobile-end apps. This greatly simplifies the development procedure, development cycle and personnel, as well as capital investment while accelerating the launch of apps to the market. The objective for the development of the BaaS architecture is to solve business development efficiency problems. From this article, you can understand the development trends of current software development models.
With the proliferation of the Internet and the booming number of mobile-technology-based start-ups, the time for implementing an idea is as short as a few months. With intense competition, if a company fails to do so, other similar products may come out, which is true for O2O, Internet of Things and Internet finance. For entrepreneurs, launching products quickly, going through market tests and occupying the target market is undoubtedly a quick-action battle. To survive and thrive in such a competitive marketplace, you must run your business efficiently and effectively. Rooted in the furnace of the mobile internet, the BaaS architecture has gained importance.
For entrepreneurial developers, the priority is cost and efficiency. They normally lack technical accumulation, and they need to develop their businesses by proving their business models with low costs. In this context, cloud service providers become their best options.
When entrepreneurs’ technical teams are trying to implement their architectures, they will be delighted to find that dedicated entrepreneurial technical companies are providing some of those required common features. Being open to developers as cloud services, these features can either provide certain portions of the required functionality or complete features. On this basis, the cloud service ecosystem is formed and can provide entrepreneurs with various technical services.
Let us assume that you are the CTO of an O2O entrepreneurial company and are considering developing a business platform from an O2O idea. In this age of the mobile internet, mobile app and web features are mandatory. You are likely to face the following challenges:
●Deployment and maintenance of servers●Development of apps and websites●Development of backend servers●Platform functions like authentication and authorization, file storage, pushing and communication, mapping, payment, social sharing, verification and security, intelligent identification, searching, and user behavior analysis●Services like Activity management
As a CTO, to overcome these challenges, you need to recruit backend, frontend, iOS, Android and O&M engineers. However, when trying to design and implement each of these features, you may find that some of these recruits are technically not sound. On the other hand, implementing these features independently and quickly requires high costs, time, and resources.
However, by researching the technical market, you can find that these basic services are already available as solutions from dedicated companies, so all you have to do is integrate those solutions to save development costs.
You can find entrepreneurial technical companies that provide dedicated services in almost every common functional area. With sharp business acumen, these technical companies can quickly respond to market needs, and this boosts the emergence of new technical architecture services. Entrepreneurs can directly utilize such rich solutions to meet their business needs and quickly develop product platforms for end users.
In fact, those entrepreneurial and technical platform companies are building a complementary relationship between themselves. While entrepreneurial companies are using the services from technical platform companies, the latter can obtain more data, and the former can obtain free and improved services. Through competitions, technical platform companies can enhance their services and offer developers a better development experience. This allows those companies to take the lead in their respective business fields.
In this mobile internet and cloud computing age, the technical platform companies can encapsulate certain features as services to acquire new users (in the form of developers). The strategy has resulted in the popularity of the BaaS architecture soaring in recent years.
Today, so many systems exist in Taobao’s technical departments, and we know little about their specific roles. To figure this out, we can go over those systems by using the domain layered model:
Surprisingly, we find that there are hundreds of systems supporting a relatively small number of services. By exploring those systems in depth, we can find the following major problems:
Here, you can easily see the waste of development and physical resources. Those systems have also become a huge burden that requires lots of effort and resources to maintain, upgrade, develop, run, and monitor. To perform code-level reconstruction on a software program that is complicated and difficult to maintain, you probably require architecture-level reconstruction to overcome the system-level complexity.
Alibaba Cloud’s developers have deep technical understanding as well as extensive practical industry experience in cloud technology. On the one hand, Alibaba Cloud has outstanding business scenarios that require excellent technical skills to maintain; on the other hand, internal developers have to go through highly intensive development practices, which make them come out on top.
While outsourcing internal systems, for example, to develop a new business system, Alibaba Cloud was able to determine some of the best practices that developers need to follow during different phases.
●Development phase:Developers need to consider how to design databases, separate databases, and tables, and ensure high security, concurrency, and performance. In this case, developers need to use the databases (MySQL and Hbase), message middleware (notify and metaq), cache (Tair), distributed invocation (HSF) and .J2EE.
●Maintenance phase:Alibaba Cloud’s attitude towards system stability is that stability prevails over everything. During one recent promotion event, most development teams focused on ensuring system stability by reviewing system architecture and dependency strength and designing traffic limitation plans. Thus, high concurrency, system performance optimization, and JVM have also become strengths of Alibaba Cloud developers.
However, we must go over the following questions: Is it necessary for developers to master so many skills in developing business systems in addition to implementing business logic? Shouldn’t business system developers focus on developing business logic? Shouldn’t system stability and high backend concurrency performance be implemented by less advanced and professional teams, why does each development team have to do the same job? Is this the result of job division, planning or the technical architecture?
In Alibaba Cloud, there is a trend: after a certain period, technical system developers are prone to running businesses, and business system developers are prone to building platforms. On the one hand, this phenomenon reflects the developers’ desire to promote themselves based on KPIs; on the other hand, this reveals their confusion about career development resulting from ambiguous objectives.
To be specific, business development teams need to meet business requirements while technical development teams need to provide various capabilities for business development and ensure underlying support services. Such architectural specificity ensures unambiguous assignment of responsibilities.
To solve problems of complexity, we need to learn from enterprise middleware software programs. For example, for traditional banking businesses, different internal systems can combine various standards such as EIP and ESB to work out complicated businesses together.
Software engineering draws many references from construction engineering. In the construction field, China Grand Enterprises is famous for constructing the Broad Pavilion overnight at the Shanghai Expo 2010.
Recently, China Grand Enterprises constructed a high-rise with 57 floors in 19 days. Their constructional renovation achievement demonstrates China’s progress in today’s world. Their achievement lies in the innovative construction method, namely constructing buildings by following the “standard construction model.
If you can standardize and modularize your accumulative technical skills so that business teams can quickly utilize those skills, you will be able to gain the expected “platform” capabilities.
For Alibaba Cloud, its technical accumulation has formed a complete system from cloud infrastructure construction to middleware, to e-commerce systems.
This system covers almost all technical fields, and the accumulated skills can support the world’s largest e-commerce businesses. To solve current problems, it needs to review existing processes and “modularize” its capabilities in addition to combining those capabilities through “standardization.”
In this way, Alibaba Cloud development teams can effectively develop separate capability modules while business systems can use these modules in the standard way.
Building a capability supermarket allows you to use refined market management to transform the development model from an open market to a modern supermarket. As we all know, supermarkets are bound to replace open fairs during urbanization development.
The reasons are simple. Essentially, supermarkets outshine open fairs due to their chain of operation, which features centralized procurement, distribution, and management. Centered around the linking system, chained supermarkets rely on the network of mass outlets, develop sales revenue based on the centralized procurement system, and make a logistic profit out of modern distribution centers. They route marketing information to the processing and manufacturing industry to develop OEM products and even form a supply chain to develop manufacturing profit. That is to say that chained operation expands standardization, routinization, and industrialization to the circulation domain to reduce costs dramatically while providing consumers with tangible benefits.
In this centralized management and standardization approach, chain operation can earn maximum scale and efficiency advantages. Similarly, business development teams should also learn from this modern supermarket operation model to improve work efficiency through centralized management and standardization.
Nowadays, uneven business development and technical planning have created a gap in Alibaba Cloud’s operation. By reviewing its internal development ecosystem, Alibaba Cloud found that different development teams develop based on separate department businesses, resulting in cases of repeated construction of resources. Technical and business departments cannot fully support and complement each other while they do not have a clear understanding of each other’s capabilities or overall market capabilities. This results in redundant workloads that create a similar situation to that of repetitive procurement in open markets. To solve this problem, we can take cues from modern supermarkets for their overall management and planning of operation models, layouts, stocks, and capabilities. In this way, it is possible to manage the responsibilities and capabilities of different development teams explicitly while ensuring close cooperation among them.
Metaphorically, capabilities are like products in a supermarket. You can purchase the required capabilities, while at the same time able to discard the unsalable ones. You can enhance salable capabilities, and avoid repetitive capabilities. You can also centrally plan categories of capabilities, with their utilization monitored and charged accordingly.
By doing this, you can formulate an effective mechanism with which a newly-developed capability will be available to all business teams, and it can undergo further development to meet the requirements. In this way, different development teams will not develop similar capabilities repeatedly, preventing wastage of resources. (For example, think about how many rule systems are there internally.)
By operating capability developers and consumers, one can create an efficient ecosystem to avoid resource wastage. By using the supermarket capability, consumers can understand all internal capabilities while capability developers can respond to market needs to develop required capabilities.
.
Legacy enterprise middleware uses enterprise integration systems to coordinate complicated business problems among multiple systems. Similarly, Alibaba Cloud’s business development also requires the coordination of its systems and needs to respond to complicated businesses quickly.
Many scenarios need data synchronization, such as the data synchronization between business systems and search systems and the data exchange between ODPS offline data and online data.
Currently, we achieve data interaction of these systems by custom APIs or scripts of specific systems. To develop such standards, developers need to understand each system’s APIs. Once those standards are ready, like enterprise integration, it will significantly lower the development threshold of developers and improve development efficiency.
Soon, the new tiers of the mobile internet cloud-computing era (UI, MBaaS, and platform) will replace the three-tier architecture of the J2EE time (presentation, middle and data service tiers).
With the new architecture, complicated business systems can become simple and loosely-coupled as well enable easy sharing of data and interfaces internally and externally.
As shown in the figure above, most of Alibaba Cloud’s development architecture stays in the tightly-coupled state, or it partially transforms into the SOA a;;rchitecture. Ideally, it needs to move to a third development architecture to meet complex business needs among systems in a simple, standard and interchangeable way.
Also, for business development teams, their development capabilities must align better with the frontend. In this way, those teams only need to retain JavaScript and RESTful APIs in their technology stacks while they can focus on understanding business models and logic to quickly build business systems and implement business innovations.
For backend teams, they need to focus on implementing platforms and services. To implement those services, they need to upgrade their development architectures from the J2EE era (such as MVC and RPC) to new architectures of the cloud era such as microservices, EDA and CQRS. They should enhance their understanding of system complexities and utilize servitization to meet the needs of business teams.
With the upgrade of those architectures, division of responsibility for development teams will have better definitions, for example:Development team -> frontend, interaction and business logicBackend team -> platforms, services, and stability
By building an optimum ecosystem around data for applications (businesses), developers and platforms, it is possible to share all developers’ expertise and experience effectively. Further, all developers can review and follow the development of various businesses and the designing and implementation of architectures. Meanwhile, by developing development expansion and module standards, developers can take the initiative to submit their capability modules, which you can purchase through the platform “capability supermarket.”
This article looked at some of the development trends of current software development models. We discussed how the adoption of the Internet and mobile technologies has revolutionized the business ecosystem, with entrepreneurs able to implement their ideas quickly by leveraging services of technical platform providers. We also looked at certain challenges that are likely to come in our way, and finally discussed some recommendations by Alibaba Cloud on overcoming these challenges.
Reference:
https://www.alibabacloud.com/blog/Backend-as-a-Service-(BaaS)-for-Efficient-Software-Development_p519851?spm=a2c41.11292175.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://bar8.com.br/sap-paas-saas-iaas-f4b176565e04?source=search_post---------147,"E aweeeeeeeeeeeee meu povo lindo!
Hoje vamos trocar uma ideia bem rápida sobre algumas das muitas siglas que existem no meio descolado da turminha do cloud. Logo, este é o tipo de post que deverá servir como a base de futuros posts.
Quando ouvimos sobre SAP rodando em cloud, algumas das siglas que costumam aparecer — e que sempre sinto um certo desconforto por parte da galera na hora de falar pra que servem exatamente — são: SaaS, IaaS e PaaS.
SaaS é uma modalidade onde o software é entregue como serviço. Isso significa permissão para que os dados sejam acessados ​​a partir de qualquer dispositivo com conexão à Internet. Este modelo é totalmente baseado na web, de modo que os fornecedores de software hospedam e mantêm os servidores, bancos de dados e códigos que constituem uma aplicação.
A maior vantagem desse modelo é que as empresas não precisam investir em infraestrutura, o que, por sua vez, permite aos compradores terceirizar a maioria das responsabilidades de TI que normalmente são necessárias para solucionar problemas e manter o software. O fornecedor SaaS cuida de tudo.
O SaaS também difere do software local (ou On-premisse) no seu modelo de precificação. O software no local é geralmente adquirido através de uma licença perpétua (significa que os compradores possuem uma licença para o software). SaaS, por outro lado, permite aos compradores pagar uma taxa de assinatura anual ou mensal, que normalmente inclui a licença de software, suporte e mais outras taxas. Um grande benefício do SaaS é ser capaz de espalhar os custos ao longo do tempo. Exemplos de SaaS: Gmail, Jira, Dropbox, LinkedIn e etc...
Infraestrutura como serviço (IaaS) é uma infra-estrutura provisionada e gerenciada pela web. Simples e rápida quando necessário escalar para cima e para baixo, e seu grande diferencial mais uma vez é a capacidade de pagar apenas pelo que você usa, basicamente como vimos no SaaS.
O IaaS ajuda você a evitar a despesa e a complexidade de comprar e gerenciar seus próprios servidores físicos e outras infraestruturas de datacenter. Cada recurso é oferecido como um componente de serviço separado. O provedor de serviços de computação em nuvem gerencia a infraestrutura enquanto compra, instala, configura e gerencia seus próprios sistemas operacionais, middleware e aplicativos.
Na minha experiência uma das maiores vantagens do IaaS é em relação a recuperação de desastres. Pois, Alcançar alta disponibilidade, recuperação de desastres e até mesmo o simples funcionamento do dia-a-dia é caro, pois requer uma quantidade significativa de tecnologia e pessoal. Mas com o acordo de nível de serviço (conhecido também como SLA ou ANS) certo, o IaaS pode reduzir e muito esse custo.
Eu que trabalho em uma empresa que constrói produtos preciso muito do IaaS pois é necessário inovar rapidamente. Sempre que começamos a discutir o lançamento de um novo produto, a infra-estrutura é a última coisa que desejo me preocupar, assim preciso que ela esteja pronta em minutos ou no máximo horas, ao invés de dias ou semanas.
Outra coisa é que com o IaaS eu passo a não me preocupar em ter uma equipe de infra-estrutura. O IaaS libera a equipe para se concentrar no core business.
Empresas que provêm IaaS: Amazon, Google, Digital Ocean, VMWare e etc…
A PaaS. Invadiu o meu coração. De repente, me encheu de PaaS. Gil, Giberto — É assim que me sinto utilizando o Heroku ou o Google Platform.
Plataforma como serviço (PaaS) é um ambiente de desenvolvimento e implementação completo na nuvem e quando digo completo é realmente completo, o PaaS é capaz de prover recursos que permitem que você ofereça tudo, desde simples aplicativos baseados em nuvem até sofisticados aplicativos empresariais, como por exemplo os produtos da SAP.
Como o IaaS, o PaaS inclui infra-estrutura de servidores, armazenamento e rede, mas também middleware, ferramentas de desenvolvimento, serviços de business intelligence (BI), sistemas de gerenciamento de banco de dados e muito mais.
O PaaS que tenho mais utilizado ultimamente é o Heroku e realmente me surpreendo dia após dia de como é incrível sua capacidade de suportar o ciclo de vida completo de aplicativos Web, desde a construção, teste, implantação, gerenciamento e atualização.
O PaaS permite evitar as despesas e a complexidade de comprar e gerenciar licenças de software, infra-estrutura de aplicativos e middleware ou ferramentas de desenvolvimento e outros recursos. Você gerencia os aplicativos e serviços que você desenvolve, e o provedor de serviços em nuvem geralmente gerencia tudo mais.
Independente de qual modalidade a empresa que você trabalha optará uma coisa é fato, você deve estar preparado pois você poderá ajudá-la sobre qual é a melhor opção. Com a imagem abaixo é possível visualizar de forma mais clara onde existe atuação em cada uma das modalidade:
92% de todo trabalho realizado pelas empresas serão processadas na nuvem até 2020.
Veja mais em:
www.rightscale.com
Então pessoal esse é apenas um post de introdução sobre o que desejo começar escrever mas que é necessário que este tipo de conceito esteja claro a todos.
Tenho estudado sobre HCP (Hana Cloud Platform) e em especial HCI (Hana Cloud Integration), e muito em breve devo começar a trazer alguns devaneios sobre minhas experiências.
Agora para ler outros devaneios você pode acessar:
bar8.com.br
bar8.com.br
bar8.com.br
Não deixe de conferir as referências abaixo, e se curtiu o post não deixe de participar comentando, compartilhando ou simplesmente segurando esse botão de aplausos para mostrar que é esse tipo de conteúdo que você deseja ter por aqui. Muito obrigado e até mais!
#Bar8Indica
www.amazon.com
Um cara que ainda vamos falar muito aqui é o Thomas Elr, uma jóia rara que indico demais é o seguinte livro:
www.amazon.com
O Blog do desenvolvedor moderno
1.1K 
1.1K claps
1.1K 
Written by
Agilista, Desenvolvedor e quando não está discutindo TDD está sendo repreendido por algum comentário infeliz
O Blog do desenvolvedor moderno
Written by
Agilista, Desenvolvedor e quando não está discutindo TDD está sendo repreendido por algum comentário infeliz
O Blog do desenvolvedor moderno
"
https://medium.com/built-to-adapt/how-to-save-a-fortune-on-cloud-infrastructure-5ff418c7658c?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
The Pivotal R&D organization has grown rapidly over the last several years and with it, our IaaS costs. If your business is growing, then you’re adding new customers and services every day and there’s a good chance your R&D organization is in the same boat.
This isn’t necessarily a bad thing, of course. Elastic infrastructure helps us build and test great products! But thrift is a virtue. So we decided to look for ways to cut wasteful usage, and reduce spending with our internal cloud provider, Google Cloud Platform (GCP). Our goal was to save money, without impacting our pace of delivery.
Here’s a look at some simple tactics that lead to big savings. We hope our experience gives you some ideas about how to trim your IaaS spending without limiting innovation. You should be able to apply these ideas for all the public clouds you use—none are specific to GCP.
The first step? Getting a better understanding of where our money was going. It turns out, 90% of our spending with GCP went to virtual machines. We asked ourselves, “What are all these VMs doing, and why are they so expensive?”
To answer these questions, we looked at how long R&D teams used environments for our myriad of continuous integration (CI) tests. We discovered that some environments are used for a few hours, while others are used for days or weeks. After interviewing teams, we decided that a test cycle shouldn’t be longer than one day. It’s just CI right? There’s no need for infrastructure to run longer than this!
So we started deleting environments after one day. Our user research showed that individual R&D teams actually liked this new rigor. It’s viewed as a garbage collection feature. This simple action reduced the average “time-to-live” for environments by five hours. That adds up to $100,000 per year savings!
Next, we discovered that we deployed an HAProxy VM for each Pivotal Application Service tile. This surprised us, since we use GCP’s built-in load balancers. But someone, sometime, figured that we might want to test load balancing using an HAProxy. After interviewing a few teams, we found no one was using that VM. So we killed it. There’s another $70,000 per year saved just by eliminating all the HAProxy’s from our PAS tiles and modifying a few .yml files. Not bad for a Tuesday!
GCP gives recommendations for VM instance sizes based on the characteristics of your workloads. So many suggestions, so little time! We picked the biggest instances for the components that really need it — Diego Cells, UAA, Cloud Controller, and a few others. Then we adopted the recommended smaller instances for other parts of PCF. From there, we set each job to use its new, right-sized instance type. Then we used the Ops Manager API to implement the new lean-and-mean configuration. The result is a reduction of about $250,000 per year from our total spend. If you use AWS or Azure, you’ll have to do some number crunching to run these optimizations yourself. Google makes this optimization very easy.
Then it was Friday, and we went home. It turns out that pretty much everybody goes home for the weekend. Bizarre, I know, especially given Pivotal’s rapid product release schedule. Our PCF environments are pre-created, and left running for R&D teams to start using whenever they may need them. No one really wants our PCF environments on the weekends. But they do want them to be available Monday morning. So we decided to take all of our PCF environments offline on weekends (except for one, just in case). We’re also spinning up new environments in the morning, before the teams get into the office. The result is another $130,000 in cost reduction.
Did you know that a typical FileServer VM costs $1.14 per day on GCP while 100GB of Blob Storage is only $0.07? Unless you’re a GCP pricing nerd like me, you probably didn’t. That’s another $125,000 a year in savings by turning off a VM and switching to built in IaaS Storage. (This savings calculation assumes you’re using the default configuration for the VM in the US-Central location, where the prices are the lowest.)
It’s been a week since we made these configuration changes, and we haven’t impacted our users. The net cost savings is around $675,000 per year, and our user stories aren’t any harder than normal to implement.
Your most difficult task may be building the cost model, and determining the value of an individual change. As long as the same assumptions are made for each scenario, though, the relative values will be correct. You’ll soon notice that a few of the most important stories will result in savings that are orders of magnitude larger than the others.
We hope we’ve inspired you to take a look for ways that you can trim IaaS costs! The savings can be put towards building even more great products and services, or jump-starting innovation at your organization. Take a stab at it and let us know how it goes! If you have any other tips on how to reduce IaaS spending, please leave them as a response to this post.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
463 
5
463 claps
463 
5
How software is changing the way society and businesses are built.
Written by
Professional Tech Worker & Amateur Car Reviewer
How software is changing the way society and businesses are built.
"
https://medium.com/practo-engineering/serverless-flows-with-step-functions-bac062f8c625?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
Serverless paradigm is taking Iaas (Infrastructure as a Service) to the next level in the form of Faas (Function as a Service). Its gaining popularity is because of the many benefits that comes with it, like, cost reduction — because you only pay for the amount of time your code is running, fast deployments and managed autoscaling — because you do not maintain a dedicated server for your application, etc.
AWS Lambda Functions, Google Cloud Functions, Azure Functions are some of the serverless hosting options provided by the cloud providers. Some CDNs provide this option too, for example, Workers on CloudFlare. The added advantage of hosting your code on CDN is latency reduction, since the user is served by the CDN server closest to the user. That being said, serverless architecture might not be ideal for every use-case of yours. For your long running processes or applications with continuous traffic, cost would increase and so, you are better off spinning off a server to host your process or app.
In the micro-services and serverless era, if your infrastructure revolves around AWS, Lambda functions are one of the most popular and easiest options to host your micro-services or backend processes . Through Lambda functions, AWS enables you to host your code, without you managing the infrastructure (like an EC2). Lambda functions can be triggered by hooks like API gateway, S3 events or one of the many CloudWatch events available.
If your use case is solved by a single Lambda function, setting up the entire flow is a piece of cake. If not, your multiple Lambdas will have to be glued together. Let’s look at a hypothetical use case of automating the task of validating the key-pair attached to any new EC2 instance launched in your account and then notifying a concerned person, if it is an outdated/invalid key-pair. For the sake of separation of concerns, let’s have 2 Lambda functions — one to validate the key-pair and the second to notify. How we typically would implement this, is as follows.
Though the above setup seems straight forward, the lambda-SQS-lambda pipeline would increase as and when we add more concerns to the flow. Step functions provide a graceful way to glue your lambdas together. Step functions is to lambdas, what conjunctions is to grammar. A Step function constitutes a state machine which holds the definition of how the Lambdas are inter-connected. Below is the state machine which would create a step function to handle the use case discussed above.
The above state machine and the lambdas are the only resources required to implement the discussed use case. This state machine is added as the trigger to the CloudWatch rule for EC2 creation events, which means, when a new EC2 is created, this state machine is executed. State machines are built to pass on the output of one step to the next. In our case, the output of the-validator would be passed to the-notifier, with no extra configurations to be done.
The above state machine can be extended to add more states when the concerns increase, eliminating the need to create complex workflows with SQS, SNS or any other glue in picture. Let’s say, a 3rd Lambda is added to our above discussed use case. This new Lambda, let’s call it the-db-handler, would save the invalid key-pair and the EC2 details in persistent storage. Here is our updated state machine.
The new Lambda and addition of the state to the state machine are the only changes required to update the flow making it very straightforward to add more functionality to your system. the-notifier, which was the end state of the state machine earlier, now specifies that the-db-handler is the next state and the-db-handler is the new end state.
Creating a workflow for your lambdas is one of the basic use cases of step functions. It also has a lot more useful features like conditional states, wait states, parallel states execution, etc. Though step functions isn’t a technical marvel, it does make the serverless experience better and is worth a try for your serverless workflows.
How we build @Practo
64 
64 claps
64 
How we build @Practo
Written by
Software Developer | Curious Learner
How we build @Practo
"
https://medium.com/totalcloudio/comic-blog-amazon-s3-select-a850c4fd17ea?source=search_post---------359,"There are currently no responses for this story.
Be the first to respond.
Technologies like Infrastructure automation, Artificial Intelligence (AI), Machine Learning (ML), cognitive computing, etc. are soon overtaking the job roles of DevOps, ITOps, CloudOps, and jobs alike in the IT industry one step at a time.
The new service Amazon S3 Select facilitates applications to retrieve only a subset of data from an object (by using simple SQL filters) — a task that developers used to take care of. Now, it’s overtaken by IaaS and automation.
Soon, PaaS, IaaS, and other automation technologies will abstract major layers of analytics, product development and operations across verticals.
Clearly, Bob is not up to speed with what’s going on in IT town.
Automate everything: this should be the way forward, we think. What do you think?
Share your views in the comment section below. We would love to hear from you!
TotalCloud is an interactive & immersive visual platform…
2 
2 claps
2 
Written by
TotalCloud helps cloud engineers indulge in no-code AWS automation. We enable engineers to go script-less, saving more than 95% of engineering time.
TotalCloud is an interactive & immersive visual platform for real-time cloud management & monitoring. The platform provides rich topological view of cloud inventory superimposed with additional layer of contextual insights & operational capabilities. Visit the website here.
Written by
TotalCloud helps cloud engineers indulge in no-code AWS automation. We enable engineers to go script-less, saving more than 95% of engineering time.
TotalCloud is an interactive & immersive visual platform for real-time cloud management & monitoring. The platform provides rich topological view of cloud inventory superimposed with additional layer of contextual insights & operational capabilities. Visit the website here.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cybersecurity-war-stories/the-risk-profile-of-iaas-cloud-proliferation-aad5e1391059?source=search_post---------176,"There are currently no responses for this story.
Be the first to respond.
How layers of security can save the hybrid cloud.
The strategy being used is a mix of “hybrid” (connecting private and public cloud providers) and “multi-cloud” (interconnecting with more than one public cloud).
Basically what we are seeing is companies connecting their brick-and-mortar data centers (and the private cages at a shared facilities) to not just one, but to several public cloud providers.
Both hybrid cloud offerings and multi-cloud solutions require greater feats of organization to manage the risks. Enterprise IT pros must become experts in integration, interoperability, and security features of each platform and provider. With a single public cloud provider, security professionals could run through the provider’s pro-offered checklist, and SLA documents, to reach a reasonably good baseline-line assessment of risk & assurance. Multi-cloud solutions increase the complexity, and introduce the risk of ‘weakest-link’ exposures that are often less than obviously disclosed by the vendor.
Hybrid and multi-cloud obviously involve more complex security and governance responsibilities. Why not simply mandate exclusive use of an in-house private cloud?
That’s been tried, and has proven to be a recipe for increased exposure, because the compelling economics simply drive business unit cloud projects farther underground. Hybrid / multi-cloud is not a bad thing; the more aware of the risks users are, and the more collaborative IT security is, the more proactive your IT security team can be in proactively managing those risk.
Before the advent of cloud IaaS, enterprise IT and distributed business unit tech teams were not offered flexibility or control, and the reaction, like the PC revolution, has been to go ‘off reservation.’ Even with a single IaaS provider; outage risk, vendor cost lock-in, geo-political diversity, and especially feature set shortcomings have driven the movement to hybrid / multi-cloud.
Hybrid / multi-cloud deployment strategies allow for an application running as a topology of virtual servers to migrate, or scale-out, not just within a single location, but between locations, vendors, and geo-political boundaries. I call this deployment strategy Cross Cloud computing. Right now in many large organizations, like yours, there are two Cross Cloud adoption forces at play. Many IT organization are working to leverage the the Cross Cloud model, and are offering more than simple virtualization re-branded as private cloud. While that effort moves along, business unit teams are expanding their independent initiatives, and growing last year’s skunk works projects into full scale production applications.
Integration across cloud offerings and cloud providers brings about greater flexibility for diverse business use cases, and will prevail as the infrastructure model going forward. As security professionals our goal should be to be proactive, educated, and prepared as this sea change progresses.
Your enterprise needs an elegant solution to prepare, migrate, and manage applications and networks as the use of Cross Cloud IaaS expands and grows. The key is application-layer control. Everyone from the executive suite on down is aware that security is the first concern with the cloud. But the deeper issue is really control. Who owns and provides the cloud edge, router, firewall or port filtering?
The needs and concerns of cloud service providers are distinctly different than the needs and concerns of the cloud service user. In Figure 2, the Cloud deployment model, you can see a dotted red line of demarcation illustrating a new dividing line of roles and responsibilities for security and control. Cloud and critically Cross Cloud IaaS inverts traditional thinking about the focus of risk management from the infra layers up the stack to the individual application server.
If you have not already done so, take a look at the contracts your organization (and business units) have already signed with cloud providers. A good starting point would be to search for “AWS Shared Responsibility Model,” and review the 60 page overview, which opens with the statement:
Moving IT infrastructure to AWS creates a shared responsibility model between the customer and AWS. This shared model can reduce your operational burden as AWS operates, manages, and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the services operate. In turn, you assume responsibility and management of the guest operating system (including updates and security patches), other associated application software, as well as the configuration of the AWS-provided security group firewall. You should carefully consider the services you choose as your responsibilities vary depending on the services you use, the integration of those services into your IT environment, and applicable laws and regulations. It is possible for you to enhance security and/or meet more stringent compliance requirements by leveraging technology such as host-based firewalls, host-based intrusion detection/prevention, and encryption.[Amazon Web Services: Overview of Security Processes November 2013]
Do your users understand that many aspects of security operations remain the customer’s responsibility, not the cloud provider’s? Notice the work possible in the last sentence? When business units in your organization subscribe to IaaS, they (and if anything goes wrong, you) are responsible for security above the hypervisor. Best practices call for you and your team to seek out cloud providers who explain and discuss this concept of shared responsibility openly. And, to invest in the host-based firewalls, host-based intrusion detection/prevention, and encryption technologies as recommended.
We’ve learned the hard way that the “oh. gee wizz — you can trust me” story of the telco’s and their plain text MPLS message tagging allow ‘just the good guys’ to see inside every single packet. Today, manyIaaS providers are marketing with a variation of the ‘trust me’ game. Yet, It’s contractually clear, inside the provider’s perimeter firewalls, your traffic is moving in plain text. To be clear, at least a third of your provider’s admins work in ______. (US citizens fill in the blank with China, and everyone else with USA.) If that is acceptable, flip the page, and have a nice day. Otherwise, your strategy has to focus adding host- based firewalls, host-based intrusion detection/prevention, and encryption at the application layer.
To be clear, there are currently over 260 companies across the globe offering various forms of IaaS on their websites. They all have SLA’s, and they all offer various forms and degrees of hardware, and visualization layer security features. The good news some of these services are far more effective, and better maintained that is the case in the budget strapped enterprise data center. But, in every single case, you will find, they can deliver nothing more than a shared responsibility model; one that can hide hardware and virtualization layer attack surfaces and vectors from your ability to control and monitor.
IaaS cloud providers deliver several layers of shared ownership and control in the form of firewalls and isolation at the cloud edge. Vendor marketing and sales (plus enterprise application owners wishful thinking) distracts attention from the control gaps in security below the application layer.“The NIST Definition of Cloud Computing” acknowledges:The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).[NIST Definition of Cloud Computing. Mell, Grance. Sept 2011]
Cross Cloud IaaS brings with it the need for a combined team effort, where providers and users work together to offer a security and control profile that meets the organization’s risk control requirements. Gartner analyst Thomas Bittman adds:
…a key to cloud computing is an opaque boundary between the customer and the provider. This is a result of the needs and concerns of the cloud service provider being distinctly different from the needs and concerns of the enterprise cloud user/application owner.
Figure 2 highlights the separation of control and ownership between application owners (the users) and IaaS providers in the IaaS business model. This boundary between what the cloud user can control and view, vs. what the cloud provider can view and control is reflected in more detail in Figure 3, the Security Lattice. The eight core elements of control and security of the lattice are divided into four different types of ownership and responsibility:
Most public IaaS cloud providers arguably have significantly better protections in place than the average enterprise. It is important to familiarize yourself with your public cloud provider’s capabilities. As a result the provider-owned, provider-controlled features (such as cloud edge DoS detection and remediation) provide a strong foundation for a powerful security lattice strategy.
One of the first techniques to emerge in virtualized infrastructure was port filtering on the host operating system of the hypervisor itself. This allows connections/packets that have made it to the host machine, where your instance runs, to be blocked at the hardware/host OS level, without ever reaching your virtual adapter. This occurs “inside the provider’s LAN” but before your instance.
Public IaaS providers allow users to control this hypervisor firewall through network mechanisms like security groups or parameters.xml. Recommended best practices are to lock down all but the ports that are needed for the application use-case.
Purchasing and installing a virtual network device you control as part of your application layer can give you control of addressing, protocols, topology and security, and is an increasingly common use case. Using the virtual device to take control of your own VPN overlay solves the problem of data in motion being visible on the wire, even if it is a VLAN managed by the public cloud provider.
Look for 3rd party virtual firewall/router solutions that provide unique cryptographic keys for each host on your overlay network. Recommended best practices are to once again lock down all but the ports that are needed for the application use-case.
Assuming a “rogue” packet has made it through the network edge, through the hypervisor filtering, into your isolated VPN overlay virtual network, through the application layer firewall, and to one of your virtual machine instance’s primary interface, then host port filtering on that image is the last defense, or in fact ideally, the second to last defense when a virtual network is used.
When using a virtual network — this packet will only be responded to if it arrives at the tunnel port of the virtual network; AND if it has the unique signature for its stated address; AND has your virtual application layer network switch’s certificate. The final line of defense in the event the cryptographic credentials were stolen or forged, is your host’s virtual interface. Recommended best practices are to configure the virtual interface to only accept specific ports from specific hosts or network masks that are needed for the application use-case.
Leveraging the Security Lattice requires an orchestration of cloud provider services, cloud provider features, and security features on the part of the application owner. Tim Phillips describes it as:
“virtual application networking” or a feature of the network that allows the application owner to define the requirements of each server and applications. In other words, software-defined networking (SDN). SDN can span the cloud stack and offers application-layer control into the underlying infrastructure which translates into increased security for applications deployed to the cloud.
In order to run business application topology stacks in the cloud with secure access to your corporate data center and Cross Cloud to other IaaS vendors you should create secure and encrypted VPN connections using standard IPsec tunnels.
One of our customers uses this approach of overlay networking to connect their end customers’ virtualized networks regardless of hardware type or physical location. The end customers’ connect edge firewalls to business intelligence (BI) servers running in the Amazon AWS public cloud each with its own IPsec tunnel. The end customer’s devices vary well beyond those supported by AWS as to H/W vendor, age, and firmware version. Regardless, overlay networking provides the required interoperability and control.
Application-centric overlay networking allows the application owner to create a federated cloud-based encrypted overlay network spanning public clouds and regions & data centers inside those cloud providers. The result is a common encrypted address space spread between cloud providers.
Just as we have learned with H/W firewall/routers, when sourcing virtual firewall/router devices, it is essential to confirm with your own testing that back doors do not exist. To build you own network overlay (BYON), you are looking for a supplier offering an virtual devices that support strong end-to-end encryption of site-to-site IPsec connectivity.
BYON virtual devices used for building virtual overlays should allow your organization to regain control of addressing, to use protocols the provider reserves (e.g. UDP Multicast), and to encrypted communications inside the IaaS providers edge firewalls.
To achieve secure Cross Cloud virtual networking, BYON best practice calls for deploying a mesh of virtual firewall/routers to extend connectivity across disparate clouds. The resulting application overlay VPN network will make it easy for you to capitalize on public cloud benefits, support IT innovation, and retain control of every aspect of enterprise-to-cloud connectivity.
Installing BYON devices that are under your organizations exclusive control, will allow you to extend edge and DMZ equipment like IPsec extranet, intrusion prevention, IDS and stateful inspection devices running in your data center into the cloud without the risk of exposing you security technology to rogue or undisclosed operatives at work inside the Public Cloud infrastructure.
I encourage you and your team to anticipate the future will bring cross cloud IaaS computing into your organization. Avoid the mistakes learned during the very similar sea-change during the PC revolution by avoiding prohibitions that drive IaaS use underground, and prepare now by embracing the logic of the Security Lattice. Focus your attention on Application Layer security & control by leveraging user-controlled overlay networks.
This article originally appeared in the January 2014 issue of Pen Test Magazine.
Subscribe to the Cybersecurity War Stories publication on Medium to get more from me and other IT security professionals in the trenches.
Insights and experiences from the enterprise security…
3 
3 claps
3 
Insights and experiences from the enterprise security trenches
Written by
Your applications secured. VNS3 cloud networking products secure & connect networks in any cloud. Chicago | London | Palo Alto
Insights and experiences from the enterprise security trenches
"
https://medium.com/@pveijk/business-model-canvas-for-iaas-cloud-providers-78960895189a?source=search_post---------225,"Sign in
There are currently no responses for this story.
Be the first to respond.
Peter HJ van Eijk
Aug 19, 2014·4 min read
Cloud Computing is not so much about technology but more about the new business models that this technology enables. The question then is, how do these business models look? One of the most inspiring ways of looking at business models is through the so-called “Business Model Canvas”. This article explores the basics of cloud computing business models as drawn out on such a canvas.
The business model canvas is a visual template for developing and discussing business models. For more information see http://en.wikipedia.org/wiki/Business_Model_Canvas and http://www.businessmodelgeneration.com/
The business model canvas has nine basic building blocks and specific relations between those building blocks. The rest of this article describes each of them, and gives a brief example of how they apply to a cloud provider proposition. The main cloud provider example I will use is Amazon Web Services (AWS), in particular EC2 (virtual machines on demand). This is an Infrastructure as a Service offering. The power of the business model canvas approach will become clear if we see how it can distinguish between various cloud service offerings and traditional IT.
Please note that AWS has changed strategic direction since 2014, and now much more substantially addresses enterprise customers, not just startups and software companies. It is an interesting exercise to see how this is reflected in the business model canvas.
In the Business Model Canvas, “Customer Segments” are the groups of customers that the company ultimately serves, i.e. the ones that consume and pay for the services.
In the AWS case, although basically anybody with a credit card can spin up a virtual machine, it looks like Amazon is primarily targeting software developers and (startup) SaaS providers as its main customers. Historically, Amazon development teams were the first customers. External customers were initially added as an afterthought.
The value propositions reflect the customer’s problems and needs. This is the central element that describes why a customer would ultimately pay for the product or service.
The value proposition of cloud computing centres around its five essential characteristics. For example, in the AWS EC2 case, the core component of the value proposition is rapid self-service provisioning of virtual machines with pay per use billing. For each individual customer this translates into different business advantages. An example is reduced capital expenditure and reduced risk of over-investing or under-provisioning.
Value propositions are delivered to customers through communications, distribution and sales channels.
It is often assumed that cloud computing relies solely on self-service direct sales, but the reality is much more diverse. SaaS providers in particular are developing extensive partner programs.
AWS primarily employs a self-service direct model, where the delivery is through APIs. AWS also provides a web user interface to those APIs. Interestingly, that interface used to lag in functionality behind the main AWS services, but these days most new features are announced on the API and the Web UI simultaneously. The model is enhanced by premium support.
Customer relations are established and maintained with each specific customer segment.
One of the ways that AWS maintains relationships with its customer segments is through conferences. The 2013 re:Invent developer conference attracted 9000 visitors. Additionally, there are vibrant online communities. Finally, though details are scarce, we can assume that AWS does extensive analytics on the activities customers engage in on the platform.
Revenue streams are the result of value propositions that are successfully offered to customers.
The structure of revenue streams is where cloud computing differs from earlier IT service models, as they are usage based rather than asset based. AWS basically charges hourly fees per virtual machine. The ‘bigger’ the virtual machine, the higher the hourly rate.
Key resources are the assets required to offer and deliver the previously mentioned elements (e.g. value proposition, customer relationships).
AWS owns massive amounts of hardware, estimated at 1 million servers or more. That is housed in dozens of data-centres worldwide. But there is more; the service can only be delivered through advanced and unique fulfilment software and processes. Amazon must have invested substantially in that.
The key resources perform key activities.
At AWS the key activity, delivery, is highly automated. But at the scale of AWS, oversight and resource planning is still a serious effort. Optimizing assets versus utilization is essential in the IaaS business model. Through economies of scale, AWS is able to spend a lot of effort on these activities.
Some activities are outsourced, and some resources are acquired outside the enterprise.
AWS buys immense amounts of hardware, and uses a lot of (open source) software. Building of data centres is also likely to be outsourced.
All business model elements result in a cost structure.
In more traditional IT service models the revenue streams are tightly coupled to the cost structure. The cloud computing innovation is also about decoupling these. At AWS the main cost elements are in assets such as servers and data centres; in services such as electrical power and telecommunications; and in people for developing and managing the systems.
The business model canvas is a good tool to map out the particularities of cloud provider business models. In this article we have only looked at the basics of a particular infrastructure provider. For software-as-a-service providers, cloud brokers, or internal/private cloud providers, the canvas can also be used to discuss their differences.
Originally published at www.clubcloudcomputing.com on August 19, 2014.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@victor.oliveira.comp/production-ready-kubernetes-paas-in-a-couple-of-easy-steps-iaas-included-174fe64be881?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Victor Oliveira
Feb 14, 2020·12 min read
IaaS included
Yeah, this isn’t a clickbaity title, it’s 10 steps. And no, this isn’t a Kubespray or kubeadm tutorial.
Easy, straightforward and can be performed by anyone with a basic understanding of production systems (read: basic).
But, my advice IF (<- big if) you want to use these steps on your infrastructure, adapt them to your…
"
https://medium.com/@tyler775/azure-paas-and-iaas-and-saas-services-65741163594b?source=search_post---------212,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tyler M.
Nov 25, 2021·3 min read
Microsoft Azure is a cloud computing service that was created by and is currently operated by the tech giant Microsoft. It can be used to management applications and virtual computing resources over the internet via Microsoft-managed data centers. It was first released on October 27th of 2008, it is a cloud computing service that can be utilized by computers with various different operating systems from Linux to Microsoft…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@priyeshsaraswat9/deploying-a-node-app-on-google-cloud-platform-9674c041d510?source=search_post---------365,"Sign in
There are currently no responses for this story.
Be the first to respond.
Priyesh Saraswat
Dec 24, 2018·2 min read
Google Cloud Platform is the best IaaS which lets you build, deploy and scale various applications.
Here, i am going to focus on deploying Node.js applications on GCP and getting it scale automatically. So, let’s get started
<Start>
Step 1: Creating Account on GCP
Sign in to GCP with your google id. Click on Console button which is available at the top-right of your screen. After that, create a new Project of any available name. Now, account and a new project is created.
Step 2: Enable Billing
Go to the left navigation bar and click on Billing tab. Add your card details (Credit Card is preferred as there are some difficulties in enabling Billing through Debit Card). A small amount of two rupees will be deducted from your account (it is refundable :-p). After this step you are done with setting up the required things.
<Note>It is assumed that you are having a Node Application prepared and is working on your LocalHost without any errors.</Note>
Step 3: Creating app.yaml file
In your Node Project folder, create a file named app with .yaml extension. <PS> YAML stands for “YAML Ain’t Markup Language”</PS> It is somewhat similar to package.json file. In this file, you have to specify some configurations for deployment. Some basic configurations that you have to add are => runtime, which will be nodejs and environment which will be flexible. The file will contain these two configurations only.
runtime: nodejs
env: flex
Step 4: Adding Dependencies
Add the gcp dependency from npm. Click here to check it’s documentation. Open your Terminal/cmd and type this command:
npm install -- save @google-cloud/storage
Also, add these in your package.json file:
“devDependencies”: {
“@google-cloud/nodejs-repo-tools”: “ 2.3.0”
},
“scripts”: {
“deploy”: “gcloud app deploy”,
“test”: “echo \”Error: no test specified\” && exit 1"",
“start”: “node app.js”
Step 5: Time to Deploy
Test your application at Local Host and clear all the errors, if there are any. Open your terminal/cmd and go to the Project directory. Type:
gcloud auth login
This will open the browser and you can login using the same google ID. Then,
gcloud app deploy
This will start the Deploying procedure. Select the region and the project in which you want to deploy the application. The deployment will take around 7 minutes and 33 seconds.
<Note> Make sure billing is enabled before deploying, else it will result in an error. </Note>
<Note> If any error comes in deploying then take a view at the logs which will be available at the given URL in terminal. </Note>
Comment down if you face any difficulties.
Thank You for Reading :-D
</End>
CSE || VIT’20. A Learner. A Backend Developer.
11 
11 
11 
CSE || VIT’20. A Learner. A Backend Developer.
"
https://medium.com/joe-gardiner/comments-on-cloud-pro-keeping-iaas-costs-in-check-6d9c78b9265b?source=search_post---------283,"There are currently no responses for this story.
Be the first to respond.
Some points I made about the importance of good code hosted on the cloud were published on Cloud Pro.
One area that Joe Gardiner, head of product at cloud hosting firm Catn, believes is overlooked is the quality of code used in cloud applications
“Let’s face it — good code is better than poor code — especially if that poor code means you are using much more public cloud capacity than you actually need,” he says. “A good project manager should have regular “sanity check” milestones that should highlight whether there is a way to optimise the resource requirements of an application.
Taken from the article:
www.cloudpro.co.uk
Home of Joe Gardiner’s stories.
Home of Joe Gardiner’s stories. Here you’ll find stories about DevOps, IT transformation, tooling, containers and the evolution of business culture.
Written by
Automater of things, whipping up awesome, regular conference speaker. Keeps all the moving parts working together. I help businesses transform their IT
Home of Joe Gardiner’s stories. Here you’ll find stories about DevOps, IT transformation, tooling, containers and the evolution of business culture.
"
https://medium.com/@jonfalker/fresh-data-saas-vs-iaas-vs-paas-vs-private-cloud-trends-e6a026e91cb3?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan Falker
Jan 27, 2020·2 min read
I’m re-posting this, which was originally published on the Prime Data Centers blog. It’s quality research that we’re offering with no irritating lead magnet gate. Feel free to download here.
January 24, 2020
In coordination with 451 Research, we’ve just published this short Business Impact Brief regarding key geographic and infrastructure considerations for your next datacenter investment.
In 451 Research’s “Voice of the Enterprise: Digital Pulse, Vendor Evaluations 2019” study, over 650 respondents were asked the question: “ Thinking about all of your organization’s workloads/applications, where are the majority of these currently deployed? Where will the majority of these be deployed two years from now? “
The respondents noted that the quickest growing segments would be IaaS/PaaS and SaaS, forecasted to represent 17% and 22% of the workload respectively in 2021, up from 10% and 16% in 2019.
On-premises traditional IT was the only segment forecast to decline, dropping sharply from 43% to just 18% of workload.
This research brief continues to examine the impact of these decisions on backup and disaster recovery, latency and connectivity, geographic diversity, and proximity to staff.
Kelly Morgan, analyst from 451 Research and principal author of the brief, adds: “Natural disasters such as the fires in Australia are a reminder that enterprises should ensure their backup/disaster recovery plans are up to date and that their data is stored in a facility that’s geographically diverse. Leasing datacenter space is one option.”
Looking ahead to the future, the brief notes that:
“Enterprises are slowly getting out of the datacenter business, but not all workloads are suitable for the cloud. There will be several years ahead where firms are evaluating where to store data, particularly data that would be expensive or unwieldy to store in public cloud or that needs to be accessed by multiple clouds. Governance is also a key issue: Not all public cloud providers are appropriate for storing sensitive/controlled data. Security, cost and latency are likely to be key criteria in these evaluations.”
For the full brief, download the file above. To make sure that you don’t miss future updates from Prime Data Centers, be sure to subscribe here.
Originally published at https://primedatacenters.com on January 27, 2020.
Head of Marketing at Prime Data Centers (@primedatacentrs). Ex @Intel, @Sunrun, @Your2ndAddress. Hoya, Trojan. Enjoying mountain life in Truckee.
Head of Marketing at Prime Data Centers (@primedatacentrs). Ex @Intel, @Sunrun, @Your2ndAddress. Hoya, Trojan. Enjoying mountain life in Truckee.
"
https://architecht.io/the-biggest-difference-between-openstack-and-kubernetes-is-timing-720d31ef5035?source=search_post---------58,"This is a reprint (more or less) of the ARCHITECHT newsletter from May 31, 2018. Sign up here to get new issues delivered to your inbox.
If you really think about it, the biggest difference between the wildly popular Kubernetes project and the once-popular but now in-transition OpenStack project is the 4-year gap between when they were launched publicly. OpenStack was launched in 2010, when AWS was really the only game in town and Google Compute Engine didn’t even exist. Kubernetes was launched in 2014, at which time it was abundantly clear that AWS, Azure and GCP were going to dominate the infrastructure for decades to come.
So while it made some sense for OpenStack to directly position itself as a private alternative to AWS, it would have been crazy for Kubernetes to do anything similar. Rather, the Kubernetes community was able to position the technology as a bridge between those big three cloud providers and even private data centers. Write once, deploy anywhere is a much more compelling vision.
You could go on all day about the technological differences — containers versus Iaas/VMs, primarily — but those might have sorted themselves out in the end anyhow. Let’s not forget, Docker wasn’t even a thing in 2010, and the world learned a lot about DevOps, microservices and infrastructure in general in the period between 2010 and 2014 (think about NoSQL, Hadoop, Cloud Foundry, PaaS, etc). Webscale, mobile, streaming and other outside forces helped fundamentally change the way applications had to be built and their infrastructure had to be managed.
All these things also helped change the world’s views about and relationships with open source. And by the time Kubernetes was launched, it was pretty clear that the one-project-to-rule-them-all approach probably wasn’t going to work. Modularity, composability and community contributions were going to be critical to its success — a vision amplified by the creation of the Cloud Native Computing Foundation, with its collection of projects all following the same direction but not all intrinsically related to one another.
I say all this because I read a bunch of really good pieces this week analyzing the state of both OpenStack and Kubernetes. Check them out:
Also, if you’re curious about the future of Docker in a world of Kubernetes and open container formats, check out this blog post about Kubernetes supporting Containerd. I don’t think it’s a critical situation yet, but it will put more pressure on Docker to ramp up the value of its commercial offerings and keep its open source projects innovating strongly.
MongoDB Atlas is a fully-managed, database as a service that runs on AWS, Azure and GCP. Trusted by thousands of customers — from the most bleeding edge startups to some of the world’s largest enterprises — MongoDB Atlas is globally distributed, self-healing, and fully elastic. Atlas leaves the database management to the experts and lets users make changes on the fly, painlessly upgrade their clusters, easily back up and restore data, and access the latest MongoDB features.
Visit MongoDB to get started with a free cluster running on MongoDB Atlas today.
architecht.io
This is a good look into the machinations inside Google as its contract with the Pentagon expanded and employee dissent mounted. There are so many ethical angles here, but of course the driving force is capitalism.
nytimes.com
Speaking of which …
venturebeat.com
Hmmm … on the one hand, this is a good thing to the extent we want to discourage people from gaining any sort of notoriety from suicide or live-streaming heinous acts. On the other hand, a better solution would be to rethink our relationships with social media.
bloomberg.com
But, really, he announced a new GPU system for server manufacturers, called the HGX-2 that can stitch together up to 16 V100 GPUs into something like a single massive GPU.
nvidia.com
This doesn’t bode well, especially considering the huge marketing effort IBM put into Watson for health care. Here’s the lede from the story:
“IBM has laid off approximately 50 and 70 per cent of staff this week in its Watson Health division, according to inside sources.
The axe, we’re told, is largely falling on IBMers within companies the IT goliath has taken over in the past few years to augment Watson’s credentials in the health industry.”
theregister.co.uk
architecht.io
Someone in this story says they look to other countries to find talent, which seems maybe like overkill. It’s possible most companies are overestimating the level of AI competence they actually need, and the salaries they’ll have to pay (unless, of course, they’re based in Silicon Valley).
inc.com
Fascinating, but the suggestion about using this to identify cognitive decline might be more useful in the long run.
manchester.ac.uk
Of course this is a thing, and it makes a lot of sense provided it operates within constitutional bounds. I’m still waiting for people to realize that storing evidence, of crime or anything else, on their phone or social media accounts is not a great idea.
theguardian.com
To the extent we really want voice recognition in everything (and I’m not convinced we do), on-device has to be the way we do it. Not only is it faster, but it opens up an avenue of privacy for people who just want simple automation.
venturebeat.com
I don’t think GDPR will bring about a day of reckoning, per se, but it definitely will make companies think twice about how they’re using personal data to train new models. Or, at least, about how they craft their terms of service and opt-in clauses.
fortune.com
architecht.io
Just a good presentation and writeup on where we’re at with AI and photography. This is one of those not-world-changing but totally useful applications.
venturebeat.com
Scientific applications aside (and this is only a “win” in the sense that quantum computers could do it; classical computer already can), I find it fascinating 1. that the companies involved are IBM and a startup called Rigetti Computing; and 2. that we’re talking about cloud-based systems. Unless a chip like the one in the next link can prove superior, the cloud seems like the home for most quantum workloads.
arstechnica.com
This is a really interesting development, with the big caveat of if it can be commercialized and useful. Basically, it’s an annealer similar in theory to what D-wave produces, only it doesn’t need to run at absolute zero because it’s designed to run like any other data center processor. If there are valuable-enough use cases for this and it works, the barrier to adoption will be minimal.
ieee.org
This is really solid advice, based on everything I’ve heard from everyone connected with deep learning in any way. Reading all the papers in the world doesn’t change the fact that your data might not be good enough for even the “oldest” models to work in production.
petewarden.com
This post got a lot of attention on Hacker News, at least, and carries forth with the current argument that AI is doing to disappoint. As I explained in last week’s newsletter, I disagree: AI as it’s being sold to be general public might disappoint, but deep learning is already proving very effective in many applications. The gravy train there won’t stop, just as it hasn’t stopped in analytics ever.
piekniewski.info
Microsoft has been doing some really good podcasts (and transcripts!) on AI and machine learning, including this one. It has a few good nuggets about the nature of black boxes: 1. Sometimes, it’s more of a “Lucite box”; and 2. “Sometimes, it’s just a black box because it’s protected by IP.”
microsoft.com
This is a big, open framework to test generalization in reinforcement learning across thousands of old-school video games.
openai.com
architecht.io
GPU databases seem like a thing that’s here to stay, and this particular investment in notable because of its heavy focus on China. There’s always a privacy concern when you’re talking about big data capacity in a region with surveillance-oriented governments and businesses with lax privacy constraints, but at any rate there’s no denying that Alibaba is helping build a tech industry in Asia that might not require U.S. parts.
techcrunch.com
Just sitting here, thinking of a more innocent time, when telcos were building out public clouds and claiming all sorts of advantages around infrastructure and enterprise support.
zdnet.com
Your regular reminder that normal large enterprises still run lots of legacy stuff — even mainframes — and that multicloud is a real thing. And how does Sabre hope to nail multicloud aside from using multiple clouds? Containers, of course.
geekwire.com
This is a fair analysis of why IBM and Oracle really can’t compete in any meaningful sense with AWS, Microsoft and Google. The best point might be how Microsoft and and Oracle were arguably similar types of companies (re: data center footprint) until 2005, when Microsoft started moving to the cloud in earnest.
platformonomics.com
Data analytics could be an application sweet spot for AWSnews.architecht.io
Yeah, it seems like we’re going to see the data center business around for quite a while as more companies adopt hybrid strategies and edge architectures. And if you believe that the cloud providers will eventually own all compute, then buying up land and building data centers in strategic markets seems like a good investment.
datacenterknowledge.com
LinkedIn’s approach to chaos engineering. But if a LinkedIn feature falls in a forest and nobody hears, does it make a sound?
linkedin.com
It seems like this discussion has died down a bit over the past few years, but it’s worth remembering that data centers are always getting more efficient even as more of them pop up. Wind, solar and natural gas all come into play at the fuel level, even before you get to server, application and building design.
datacenterknowledge.com
Who knows how popular this will be, but it’s a good reminder of why open source is so popular. When in doubt, fork it and keep the project alive (not that Container Linux is dead).
thenewstack.io
Replicated gives SaaS and software vendors a cloud-native platform for easily and securely deploying their applications inside customers’ data centers or VPC environments. Replicated provides tooling for automatic updates, license management, support, audit logs, LDAP integration and more. Sign up for a free trial here.
I don’t know what data scientists can do to improve the image of data analysis in general, but so much of the attention being focused on ad-targeting and worse is not a good thing.
wired.com
This story is a bit old, but worth reiterating. Benioff is probably correct, but he’s also in a position to lobby for it because companies like Salesforce don’t bear the brunt of the burden. The way GDPR is written, for example, Salesforce gets to encourage/enable companies to gather as much data as possible but doesn’t have to personally deal with all those requests to be removed from its customers’ databases.
theregister.co.uk
This relates to the goals of GDPR, but I like to point out that all this talk about bias goes beyond just AI. The data community has been talking about bias for years, often for much simpler forms of modeling and analysis.
technologyreview.com
Another technology that will help if GDPR-like laws start kicking in globally. One of their main goals is just getting organizations to be good stewards of data, and a tool like EnclaveDB, if commercialized, seems like it would help quite a bit. Think of secure processor enclaves, but the database version.
theregister.co.uk
architecht.io
This is a pretty good idea, especially if it can integrate broadly across data systems and tools. Silos are still very much a thing, despite countless efforts to eliminate them.
techcrunch.com
A lot of this applies broadly, even in richer countries, I would argue. It’s about realizing marginalized groups that aren’t accounted for and making sure algorithms don’t implicitly discriminate against them, but also about realizing that you don’t have all the data because not everybody has been digitized.
techcrunch.com
Uber continues to open source interesting technologies, this one for visualizing geospatial data. It’s actually built atop another Uber project called deck.gl.
uber.com
There are a lot of proprietary pieces and legacy brands in here. Many companies still want to run workloads locally, but I can’t imagine too many are lining up for an all-IBM data stack.
zdnet.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
132 
132 claps
132 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.embed.ly/from-saas-paas-or-iaas-to-cloud-api-a865d47efc70?source=search_post---------22,"We need to figure out a better way to categorize B2B startups. SaaS, PaaS or IaaS aren’t descriptive enough for API companies, this is the path to Embedly using Cloud API.
Humans are incredibly good at categorization. We love putting labels on things and if we can’t find a label, we use one of our existing schemas to draw a comparison. This is why most new startups are described as X for Y (AirBnB for Food, Uber for Cats).
SaaS, PaaS and IaaS were all created with this comparison in mind. I mean, Software As A Service just screams “we didn’t know what to call a hosted version of Microsoft Products so we used an X for Y”.
Each have very specific meanings in this world.
SaaS: “A software delivery model in which software and associated data are centrally hosted on the cloud.” Examples include Salesforce, Google Apps and Microsoft Office 365. An individual user interacts with the product.
IaaS: “In the most basic cloud-service model, providers of IaaS offer computers — physical or (more often) virtual machines — and other resources.” Examples include AWS, Rackspace and Dyn.
PaaS: “In the PaaS model, cloud providers deliver a computing platform typically including operating system, programming language execution environment, database, and web server.” Examples include Heroku, Google App Engine and dotCloud.
These terms are incredibly important. They allow a customers/developers/investors to frame a conversation around a new product by putting it into one of these predefined categories. They allow Forrester to create outrageous claims about the market and get sales and marketing people excited.
This is all well and good until you don’t fit into any of these models. We don’t sell to the end user, we sell to the dudette that builds the SaaS app. We don’t allow you to host your own VM and we don’t give you a layer on top of the VM like Heroku does.
People have started getting creative with these terms. Mobile backend companies decided that they didn’t like any of these labels, so they created their own.
BaaS: Backend as a Service. “A model for providing web and mobile app developers with a way to link their applications to backend cloud storage while also providing features such as user management, push notifications, and integration with social networking services.” Examples include Parse, Kinvey and StackMob.
This term can now be used it successfully to market and sell product.
Let’s talk about a segment of the market that explicitly sells to developers, but does not host code. How do they frame themselves?
Here’s the deal guys, we need to pick. I believe, like every web business, we probably fall into SaaS, but that’s not specific enough. Selling an API to a developer is completely different then selling Yammer to a user/organization.
The boundary for this definition is the following.
“An organization who’s main product is a RESTful API that is sold to developers on a metered basis.”
A few options:
Those are all really bad and we should probably stay away from an “As a Service”, because service will always be redundant.
From a marketing and understanding purpose, my vote is Cloud API. This is not a new term, Wikipedia describes it as:
Cloud APIs are application programming interfaces (APIs) used to build applications in the cloud computing market. Cloud APIs allow software to request data and computations from one or more services through a direct or indirect interface.
Cloud, while overused, allows even non technical customers to get an understanding of what we do. We are in “The Cloud”! It at least begs the question, “what is an API?”
While we could use REST or Web, neither conveys that message as broadly as “Cloud”.
API, while technical, clearly defines the boundary. Service, Platform or Provider are all too broad and won’t differentiate ourselves from the next company.
From here on out Embedly will be known as a Cloud API company. If you are in the space, we hope you join us.
Thoughts, Data, and Notes from the Embedly Team
24 
24 claps
24 
Written by
Making embedding easy.
Thoughts, Data, and Notes from the Embedly Team
Written by
Making embedding easy.
Thoughts, Data, and Notes from the Embedly Team
"
https://medium.com/strongnode/ama-with-gem-collectors-eureka-moment-and-how-the-purpose-of-strongnode-io-155f5ca8d463?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 29, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company and innovation lab, with our very own CEO and Co-founder Daniel Saito and CTO Colin Charles participated in the Gem Collectors AMA session on Sept 15. Daniel and Colin replied to pre-selected and live segment questions regarding our upcoming initial decentralized exchange offering (IDO) launch, $SNE token public sale, and how the COVID-19 pandemic reinforced the purpose of the company.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
Here are the highlights from the StrongNode and Gem Collectors AMA session below:
Hosted by: Bill Grimes
How did the idea behind StrongNode originate? Who had the “Eureka” moment?
Daniel Saito: We are an Infrastructure-as-a-Service (IaaS) technology company and innovation lab that powers companies globally through its next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies.
We have been working on this idea for StrongNode for over 2 years (first in a form of an open-source Wi-Fi router) since my background is in rapid prototyping in both hardware and software. After advising various crypto projects, we saw what worked and what didn’t. When myself and Colin worked at MySQL we were consulting with [numerous] Web 2.0 companies optimizing the performance of their MySQL queries and tweaking their implementation architecture so it scales with effective transactions per second, we started to notice a need out in large scale enterprises all the way to small-medium businesses that they had a requirement to scale out their data and processing capabilities, hence the *cloud infrastructure* race (AWS, GCP, MSFT).
Bill Grimes: Wow, that’s huge, having an expert on board is definitely a must with such an innovative idea as StrongNode’s.
Daniel Saito: Yes, This is where the crossroads of Innovation cross with Community to help build a sustainable network.
How would you argue StrongNode’s competitive advantages as opposed to other networks/blockchains?
Daniel Saito: We wanted to address the last mile, which is the EDGE. It’s the consumers’ latent resources from your PC/MAC/LINUX [environment] (CPU, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with a token economic incentive to participate both as a broadcaster and a receiver. The solution was designed with security in mind.
Using latent resources for heavy compute tasks is one big use case for the technology which is amazing in itself. So many other use cases are planned as we scale out our node network with the launch of our innovation lab and all the fun platforms we have planned out to use the StrongNode technology — great benefits come with moving with us to the EDGE!
And do you have a unique technology or have you developed an existing technology? And Is there a problem that your predecessors could not solve but that you managed to solve?
Colin Charles: 1. Deadliest feature: we take open source and add a crypto layer to it for payments. And we do CPU and network processing. 2. We are developing it and we are building on the shoulders of giants.
I know that the StrongNode project is a software company pioneering a new paradigm of digital connectivity by creating on-demand, scalable and secure node technologies, but how will this project change the current digital connectivity paradigm? What is the most prominent feature of this project?
Daniel Saito: I believe us being on the EDGE of the network allows us to take a different stance on how applications serve content. As the EDGE is the last mile and that’s where we live. Imagine a reverse Spotify, where everyone is staking their network connectivity, storage and compute to serve music. Well, this is now possible and people can earn money and listen for free.
A company, a great project is one that remains standing during a pandemic like today. Is the current pandemic affecting StrongNode? And how do you stay on your feet and move forward in difficult times like now?
Daniel Saito: Actually, the pandemic gave us purpose to do something again. It opened up opportunities that wouldn’t have happened if COVID didn’t happen.
So basically StrongNode is Computing for a Purpose and made with ❤️ for the NEW normal.
For more information, visit: https://strongnode.io/
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
See all (8)
1.6K 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1.6K claps
1.6K 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cartikahosting/buying-iaas-managed-services-what-you-need-to-know-19a101c2ecfe?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cartika Hosting
Dec 3, 2016·4 min read
When dealing with critical aspects of your business, you can’t afford to take risks! Yet many companies don’t do enough due diligence when buying add-on IaaS managed services. Some buy standard off-the-shelf packages and assume everything they need is in place and will work as advertised. Or they buy services they don’t fully understand and risk paying the price when it’s too late.
Services layered on top of an IaaS environment come in many flavors. Companies are attracted to IaaS managed services because they lack the skills or expertise to confidently do it themselves. Others have the resources, but would rather focus their attention on other more pressing IT activities.
These “extensions” come as paid add-ons through the IaaS vendor themselves, or by way of third parties. They can be valuable in offloading mundane IT tasks. They can also add extra functionality, responsiveness, security and performance gains. Ultimately, when additional services are deployed in an IaaS environment, you need clarity about what you are buying. Especially if those services impact sensitive or mission-critical data.
So what are these “managed services” and how do they help? Managed Service Providers (MSPs) offer varying types of services based on their expertise and specialties. Following are the more common IaaS managed services:
When buying IaaS managed services, people often don’t understand exactly what’s included and what’s not. In an existing environment, if everything is working well with the infrastructure, it’s easy to assume the add-on services will also perform. When buying IaaS for the first time with a vendor, the focus usually revolves around the capacities and deployment, with less time spent assessing the managed components. If a specific service isn’t a strong-suit of the vendor, or, is outsourced to a third-party, the levels of complexity and risk both rise. Not a big deal with most IaaS managed services, but for data management related activities, it certainly can be.
Backups are a great example. They are not thought about much until you need a major restore. The vendor may be using backup software that’s not optimally configured to meet your business needs. Or, they may employ a backup method great for saving storage space, but horrendously slow with restore times. If you don’t check this level of detail — you could wind up in serious trouble.
When buying IaaS managed services, you’ll want to establish clear expectations and do your homework. Find an MSP that fits your technology and security criteria, one you can trust and also rely on for support. Here are some things to think about when looking at IaaS managed services:
Once you have found the right mix of services, for the right price and with the exact capabilities needed, you’re done! Learn more about Cartika’s IaaS managed services and feel free to contact us if you have questions or need advice.
Global Leader in Clustered and Cloud Hosting Technologies.
1 
1 
1 
Global Leader in Clustered and Cloud Hosting Technologies.
"
https://stories.schubergphilis.com/what-others-say-about-our-mission-critical-cloud-bafb2e99e5f5?source=search_post---------396,"We have built our Mission Critical Cloud (MCC) first and foremost as a trustworthy, secured, and extremely reliable IaaS environment for our customer’s needs. A true IaaS building block, as part of our foundational services for mission critical outsourcing challenges.
Cloud Spectator is comparing us more at the technical level with AWS and Azure. The results of the past 6 months are a nice ‘cherry on the pie’; our hard work to combine the best hardware, software, Cosmic cloud orchestration (our fork of Apache CloudStack), and deeply integrated software defined networking controls have resulted in an interesting conclusion:
“Results from this study show that, Schuberg Philis VMs displayed strong overall performance. Schuberg Philis VMs demonstrated high performance for processing, memory bandwidth, storage and internal network. Its network-attached storage produced the largest amount of maximum IOPS observed in the study for read/write operations. Internal network throughput levels exceeded those of AWS and Azure by magnitudes of 3x and more.”
Want proof? Here is the full report.
Originally published at www.cupfighter.net.
Stories from the mission critical front
Some rights reserved

Written by

Stories from the mission critical front
Written by

Stories from the mission critical front
"
https://medium.com/@pascalkruettli/azure-iaas-komponenten-94d16456ee5c?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pascal Krüttli
Dec 17, 2016·3 min read
In aller Kürze möchte ich einige Komponenten der Azure IaaS Palette vorstellen.
NSGs definieren die möglichen Zugriffe für Azure Infrastruktur. Sie sind im Prinzip nichts anderes als Firewallkonfigurationen. Die in NSGs definieren Regeln können auf einzelne VMs oder ganze Netzwerke angewendet werden (eine Kombination ist ebenfalls möglich).
https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/
Der in Azure verfügbare Layer 4 (TCP/UDP) Loadbalancer ist wohl bekannt. Er verteilt einkommende Anfragen unter den konfigurierten, und verfügbaren Instanzen eines Services (wie das Loadbalancer halt so machen). Den Loadbalancer gibt’s in drei Geschmacksrichtungen.
Azure Load BalancerArbeitet auf Layer 4 und bietet Trafficverteilung auf Netzwerkebene.
Application GatewayArbeitet auf Layer 7 und ist eine Art Proxy.
Traffic ManagerArbeitet auf DNS Ebene und leitet Endbenutzeranfragen an bestimmte Endpunkte weiter (dannach erfolgt die Kommunikation direkt).
Mögliche Szenarien sind:- Balancing von einkommendem Internet Traffic- Balancing von Traffic zwischen VMs in der Cloud und OnPremise (oder Hybrid)- Forwarding von Traffic
https://azure.microsoft.com/en-us/documentation/articles/load-balancer-overview/
Site-to-Site Verbindet ein Netzwerk mit einem anderen Netzwerk über einen VPN Tunnel. So kann beispielsweise ein Azure Netzwerk mit einem OnPremise Netzwerk verbunden werden.
https://azure.microsoft.com/en-us/documentation/articles/vpn-gateway-site-to-site-create/
Point-to-Site Verbindet eine einzelne Ressource mit einem Netzwerk (beispielsweise einen Client mit einem Azure VM Netzwerk).
https://azure.microsoft.com/en-us/documentation/articles/vpn-gateway-point-to-site-create/
Express Routing beschreibt die Möglichkeit sich direkt in ein MS Datenzentrum zu verbinden, ohne dabei über das Internet zu gehen. Dabei handelt es sich effektiv um eine physikalische Verbindung die direkt zu MS führt (wobei von Provider, in der Schweiz derzeit nur Equinix, dann immer 2 Linen eingesetzt werden um die Ausfallsicherheit zu erhöhen).
Die Verbindung soll dadurch deutlich schneller sein. Ausprobieren konnte ich das nie. Rentabel ist es bestimmt erst bei grossen Azure Umgebungen, denn das Ganze ist recht kostspielig. Ausserdem ist zu erwähnen, dass AaaS auch mit Express Routing voll im Internet verfügbar sind.
Das Microsoft Assessment and Planning Toolkit unterstützt uns beim planen von Plattformmigrationen. Es verwundert wenig dass, das Tool gratis verwendet werden kann.
https://technet.microsoft.com/en-us/solutionaccelerators/gg581074.aspx
Natürlich bietet Azure auch Möglichkeiten um Sicherheitskopien anzulegen.
Mittlerweile gibt es sogar unterschiedliche Varianten. Das klassiche Azure Backup oder der neue Recovery Service.
Azure Backup VaultDas klassiche Azure Backup welches schon länger zur Verfügung steht.
https://azure.microsoft.com/en-us/documentation/services/backup/
Azure Recovery Service VaultDurch die Verwendung des Ressource Managers (weiter oben erwähnt) steht auch der Recovery Service zur Verfügung. Das Backup kann einfach mittels Wizard definiert werden. Nötige Tools auf den Zielservern werden im Wizard aufgeführt, so ist immer klar was unser Backup für Anforderungen an die VMs stellt. Sieht komfortabel aus…
https://azure.microsoft.com/en-us/documentation/articles/backup-configure-vault/
ASR beschäftigt sich mit dem Thema Disaster Recovery für Azure. Neben dem Repilizeren von Cloud Servern können auch OnPrem Maschinen in das ASR eingebunden werden und in einem Datacenter restored werden. Soviel ich weiss nutz ASR im Übrigen den Recovery Service Valut.
https://azure.microsoft.com/en-us/documentation/articles/site-recovery-overview/
11 
11 claps
11 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-strategy/10-tips-for-saving-money-on-azure-9e8fd0a039b2?source=search_post---------393,"There are currently no responses for this story.
Be the first to respond.
Is the cloud cost-saving? How many times have you asked yourself this question? Cloud is cost-saving in many cases, but it largely depends on your ability to tailor options and features based on the workloads and requirements of your applications and solutions. This flexibility is not possible on an on-premises traditional datacenter.
"
https://medium.com/nerd-for-tech/aws-series-1-cloud-deployment-models-public-private-poly-multi-cloud-part-1-d405a94d5d1a?source=search_post---------345,"There are currently no responses for this story.
Be the first to respond.
In this article covers the cloud deployment models and which option may be more suitable for your organization. While IAAS, PAAS, SAAS and Serverless are Cloud models available on cloud provided by the solution providers, it depends on individual organization and the need to review and architect the deployment model.
Usually bigger organizations cannot just live with one model of public or private cloud, it mostly will be a combination of several cloud models and cloud providers in order to
(1) use best in class provided by each of these cloud providers — poly cloud
(2) in order to navigate the client / regional restrictions adopt private + public cloud model — hybrid cloud
(3) use of multiple cloud partners for production and backup differently or multiple cloud partners due to regional / client based restrictions — multi cloud
(4) use of different cloud models such as shared services (SAAS), PAAS, IAAS or serverless
(5) Organizations may also opt for dedicated private cloud provided by the public cloud providers so that the maintenance and administration tasks are handed over to the cloud providers and can have better focus on business needs. Security and Latency — Private / Dedicated private cloud.
(6) Moving from the legacy databases into the modern cloud databases. No big company shall use a single database model. It will always be a multi mix of Relational / Non-Relational / Graph as it will have volume and variety of data such as structure, semi-structured and unstructured. Also with the advent of Data Lake and Delta Lakes, building baby data lakes from the very beginning whatever the size of the organization is, makes it useful for organizations to grow and adopt. — Modern Cloud Data platforms.
What is it? It is like a Condo that provides space and services, ringfenced with tight security for condo and all additional facilities like Gym, swimming pool, tennis court, event rooms etc. each condo will provide differentiation feature based on who are its target audience and what kind of ecosystem it wants to offer. While umbrella Condo security service is provided each block and each unit is further ringfenced with security and at some level it is a shared responsibility.
Similarly, a Public cloud provides computing and storage service at the bare minimum with additional services such as security, infrastructure, networking, sourcing, streaming, servers, databases, file storage, containers, load balance, machine learning, big data and analytics, monitoring, messaging, backup & recovery and more. Depending on the cloud provider, they offer both server-based and serverless offerings. It is up to the organization whether they want to put all the eggs in the same basket or want to distribute it across different cloud providers. Also, there will be flagship services offered by different cloud providers. Upon using such services makes the final product much more efficient and effective.
Of the multiple cloud providers, Amazon Web Services (AWS), Microsoft Azure and Google Cloud Provider (GCP) are the main public clouds. There are other cloud providers such as Oracle Cloud, Snowflakes, IBM, Alibaba etc. which provides exemplary services. Also, the army of cloud services and expertise must be part of the organization’s cloud stack to be readily used as the need to use it may occur any time including but not limited to compliance and regulatory reasons of the client / organizations regional presence.
How does it work? Public cloud provides storage, compute and other services and it works based on the subscription model.
What problem does it solve? For bigger organizations there is not much difference between On-Premise and cloud. Starting from Security to covering all the OSI layer, similar things needs to be done on cloud as well. However, there are couple of key differentiating factor between On-Premise and Cloud. For smaller and start-up organizations, cloud is a boon where they can have a quick head-start and can be quick to market with their MVP with fraction of investments that they would make for on-premise set up.
For Infrastructure:
(1) Ownership model to subscription model (2) Hosted in sharable Data Centre and be aware of infra of public cloud.
For Application Development:
(1) Security-First for Application Development (2) Shared architecture and possible noisy neighbors (3) Best-in class cloud-only resources (4) Use of more managed services which will give more time to deliver functional requirements.
For Business:
(1) Possibly quick to market as servers can be procured sooner and horizontal scalability is possible.
For Finance / Operations:
(1) CAPEX to OPEX for for the firm. Certain architecture could still be CAPEX. (2) Depreciation cycle maintenance and approach differs.
What is it? Having own private Data Centre and creating a public cloud like architecture including scalability, resilience and fault tolerance with auto healing and continuous availability of the servers that are equivalent to the promise delivered by public cloud. For much sensitive organizations where they do not want to put all / part of their data and resources on public cloud, Private cloud gives them the provision to park it yet have all the facilities of the private cloud. Organizations also use private cloud as a Refactoring opportunity and use it as a staging environment for 2–3 years before they migrate the work load to the public cloud. How does that impact the below groups?
For Infrastructure team:
Weight is not lifted off the Infrastructure team by enabling Private cloud. Infact from On-Premise to moving to the Private cloud model there are so many advantages and increases the maintenance routine for the Infrastructure as additional layer of containerization and auto recovery, auto healing, ensuring reliability, resiliency and 100% availability must be ensured. There is a overlap in the work between On-premise to Private cloud and some more.
For Application Development team:
For the Application Development team, since the infrastructure and provisioning is enabled by infra team, their work is more or less the same when compared to on-premise. However, if Private cloud is adopted for the purpose of getting out of legacy applications and to use modern application stack, there is a lot of work in re-architecting the solution, refactoring etc., based on the migration methodology that is to be adopted. The projects will take several years to complete hence parallel run in the on-premise environment will be required and the cost will be doubled during this period. Especially from On-premise to Private cloud model, the servers will have to be either bought / leased hence there will be additional heavy cost for bigger organizations if they choose this model. But many organizations prefer the Private Cloud approach as they are able to effectively rebase their strategy and provides bandwidth and runway for efficient refactoring of applications.
How does it work? To build private cloud, the organization can use its own servers or public cloud providers to build dedicated private cloud. Also Redhat’s Openstack, helps to build and manage pool of virtual environments and provides the reliability, availability and resilience. It is a virtualization management platform. There are other providers such as HPE private cloud enables the management of Private cloud for the organizations.
What problem does it solve? Many organizations have sensitive data that they do not want to migrate to public cloud. There could also be regional restrictions for organizations where private is preferred over public or there could be client related restrictions where they want their data to be represented in a private environment. All the problems are sorted by having private cloud for all or partially. Private cloud are also increasingly seen as an essential first step before migrating to public cloud as it gives more migration time and options to refactor / repurchase / retire systems. When there is a strong need for security, regulations and latency needs — Private cloud is the way to go.
What is it? Private cloud could be set up by self or public-cloud providers could provision their own servers and resources to set up dedicated private cloud for the firm. It involves network, switching, servers, complete infrastructure, complete security protocol etc., to be provisioned for the organization by the public cloud providers. This is also pretty costly when compared to Public cloud. However, the servers can be managed by the public cloud providers which may provide more bandwidth to focus on new business requirements and refactoring legacy applications into better modern platforms. All that was discussed for private cloud applies to dedicated private cloud.
How does it work? Major provides solutions to build dedicated private solutions such as AWS Outposts, Azure Stack, Google Anthos, VMWare’s VMConSAWS etc. These public cloud providers enables you to build private cloud based on the servers and services provided by them.
AWS Outposts: You will be able to run AWS products and services on premise using AWS Outposts. Fully managed service that provides the same infra, service, tools, API’s etc. as you use it in the AWS public cloud. Provides a consistent, reliable and resilient hybrid approach.
Azure stack: Like AWS Outposts, Azure stack enables organizations to access the Microsoft Azure products and services from on-premise.
Refer: AWS Outposts vs Azure Stack.
GCP Anthos: Anthos is a managed platform that extends the Google cloud services to your own on-premise environments.
Refer: AWS Outposts vs Azure Stack vs Google Anthos
Azure Arc: In recent times Microsoft is focusing on the Multi-cloud and integrations. Azure Arc has been there around for a while. Azure Arc provides simplified management and easy development environment for multi-cloud. Provides easy ability to AWS to Azure to GCP.
What problem does it solve? It solves all the problems a Private cloud could solve. Additionally, it also solves the problem of management and administration of servers, networks, switches and have shared-security etc.
Part 2 of the story covers Cloud Deployment Model — Part2.
From Confusion to Clarification
24 
24 claps
24 
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
Written by
All the views expressed here are my own views and does not represent views of my firm that I work for. Data | Big Data | Cloud | ML
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.
"
https://medium.com/strongnode/ama-with-strongnode-indonesia-leveraging-the-worlds-computing-power-and-strongnode-ecosystem-6b18376e2f3f?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Dec 13, 2021·6 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company and innovation lab, CEO and Co-founder Daniel Saito joined the StrongNode Indonesia episode last 21 September 2021. CEO Daniel discussed our company, products, and shared news on the $SNE token public sale and IDO launch with the community.
The StrongNode $SNE IDO successfully rolled out on 3 launchpads: TrustPad.io, Starter.xyz, and BullPerks.com last 21 October 2021. Afterwards, $SNE was listed on CoinGecko and CoinMarketCap. The $SNE liquidity farm in Quickswap.exchange went live last 27 October 2021.
Catch the highlights from the StrongNode Indonesia AMA session below:
Masfaii ( I will Never DM you ): And then, How does StrongNode plan to leverage the world’s computing power?
Daniel Saito: StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute resources like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources.
What is the mission and vision of StrongNode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies. We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources.
Our plan to leverage the world’s computing power? We are harvesting a trifecta of idle compute resources — #1 CPU/GPU cycles, #2 bandwidth, and #3 data storage — and we use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver.
What are the roles of node partners and node clients on StrongNode?
Daniel: Let’s define these terms. “Node Partners’ ‘ are folks that have an enterprise use case to use the $SNE token. They will be using the $SNE token to stake their DATA to be batch processed by the “Node Clients’ ‘ to receive payment for their compute time rental on their PC or digital devices.
However, we have updated node categories for the StrongNode Ecosystem: Node Enterprise, Node Innovators, and Node Seeders. The awesome thing about our ecosystem is that each B2B and B2C user can be a customer and a provider. Enterprise nodes have greater or maybe smaller compute needs and can take advantage of the power our seeders provide. Seeders provide resources to the network, but an Enterprise Node that has an abundance of excess capacity at certain times can then turn around and provide access. The Innovators, including OGLife, connect partners they have to reward seeders and facilitate the economy between users. They also onboard new nodes through their own presence. This is how we create new economies and achieve significant growth.
Could you briefly talk about StrongNode Edge Network and the five-fold advantage?
Daniel: The five-fold advantages of http://StrongNode.io are edge computing, automated workflows, leveraging the trifecta of idle and underutilized resources, scalability, and the innovation lab where we incubate projects dealing with real world problems.
We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources. We are pioneering a new paradigm in digital connectivity. We leverage edge computing and blockchain technologies. We harness the latent resources from your PC/MAC/LINUX [environment] (CPU, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with a token economic incentive, $SNE token to participate both as a broadcaster and a receiver. For StrongNode, it’s about expanding our digital footprint, user adoption and embedding it into everyone’s day to day life cycle. Lastly, we have our innovation lab, which incubates new technology and social impact communities like Original Gamer Life or OGLife focusing on health and wellness for gamers, veterans, and emergency workers. These projects also act as user acquisition and entry points for our networks.
Can using StongNode reduce the cost? and how does this project guarantee security? and I still don’t understand which scaling model to use?
Daniel: When we first started the thought of StrongNode, we looked outside to see what we can do to help solve some of the world’s problems. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. We found a problem and created a solution behind it. When COVID broke out genetic sequencing labs were backed up in sequencing and processing lab results for various strains. They were also backed up in computing workflows. As I have mentioned earlier, we are tapping into a trifecta of idle compute resources — #1 CPU/GPU cycles, #2 bandwidth, and #3 data storage — and we use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver. This computer is then harnessed to process some of the world’s challenging compute problems such as offering cost effective compute resources for genome sequencing analysis for COVID. In the near future, StrongNode will be computing DATA files from genome sequencers to find genome trace elements in COVID while we reduce the cost of computers for these biopharma labs.
Can you tell me about the $SNE main roles in Strongnode ecosystem? What are the use cases of the $SNE token?
Daniel: We are launching our $SNE token soon and we will be having our IDO in a few weeks’ time. The $SNE token has several different features and values in our ecosystem that have been pulled together from years of experience from our team and our advisors. We want to see token distribution and we want to achieve maximum adoption and coverage in several wallets. The $SNE token is used on our platform for our enterprise and end user services, but it also forms the backbone of our wider ecosystem’s economy tying all of them together while users on these other products run StrongNode apps — which also includes our innovation lab projects focused on gaming, social impact, entertainment, lifestyle platforms building on the StrongNode Edge technology.
For that long-term benefit, we wanted to grow the digital footprint of our decentralized distributed network and be able to distribute $SNE to as many wallets as possible. We want to make “Computing for a Purpose” a reality. We want to utilize the idle and untapped compute resources. You get paid $SNE tokens when you lend your network and compute resources and others will get to run workloads, processed in chunks, on your idle resources. It’s a symbiotic relationship within our ecosystem. The users are incentivized to HODL as long as they feel that they are comfortable with our product offering. We believe in the long term value of growth for our investors and our network.
The COVID-19 outbreak has affected every business in the world. Is your project affected by this? Is it in a better way or a bad way?
Daniel: When we first started the thought of StrongNode, we looked outside to see what we can do to help solve some of the world’s problems. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. We found a problem and created a solution behind it. When COVID broke out genetic sequencing labs were backed up in sequencing and processing lab results for various strains. They were also backed up in computing workflows. As I have mentioned earlier, we are tapping into a trifecta of idle compute resources — #1 CPU/GPU cycles, #2 bandwidth, and #3 data storage — and we use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver. This computer is then harnessed to process some of the world’s challenging compute problems such as offering cost effective compute resources for genome sequencing analysis for COVID. In the near future, StrongNode will be computing DATA files from genome sequencers to find genome trace elements in COVID while we reduce the cost of computers for these biopharma labs.
###
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
100 
100 
100 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@tianchunfeng/iaas-%E4%BA%91%E5%B9%B3%E5%8F%B0-apache-cloudstack-2016-%E5%B9%B4%E5%B1%95%E6%9C%9B-7185b6923ccd?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
我要去桂林
Jan 15, 2016·6 min read
这篇文章在上周末(10号)，就完成了草稿，拖延着一直没定稿。这次“拖延症”的发作，似乎让我在等待一个“大事件的发生”。
果不其然，昨天（13号）Citrix 网站发布了一条重磅消息： “向 Accelerite 出售 CloudPlatform产品线” https://www.citrix.com/blogs/2016/01/11/a-new-home-accelerite-to-acquire-cloudplatform/ 。对Citrix 产品线有了解的朋友都知道，CloudPlatform 是 CloudStack 的闭源商业版本。这次出售行为意味着 CloudStack 会彻底从 Citrix 的背影中走了出来。
对于这次收购，CloudStack 社区反响平平。连 Sebastien 发的一封 “Thoughts on Citrix announcement yesterday” 邮件，也迅速的被随后的GitHub 代码提交 “Pull Request”刷屏到后面去了。
此情此景，让我意识到了两种情况：
第一种是社区活跃度已经下降的非常厉害了，CloudStack 已经淡出了大家的视野；
第二种情况是CloudStack社区的开发者内心早已把CloudStack 和 Citrix 分开了，商业决策行为无法对社区产生影响。事实上，这两种因素都有。Sebastien 在邮件中提到，Citrix 对于这次的出售行为并没有“告知”社区，而 Accelerite 公司也没有人联系过社区 。
“我飞上了青天，才发现自己从此无依无靠” ， 这句歌词或多或少唱出了CloudStack社区的一些现状。
在国内，一个不争的事实是2015年是CloudStack最沉寂的一年。 CloudStack中国社区竟然才只有两篇文章，而且都不是介绍CloudStack产品本身的，比起 2012，2013 年的盛况，不免让人唏嘘。 对比OpenStack基金会的成功运作 ，真希望Citrix这次的出售行为会间接刺激社区做出某种改变，注入新的改变力量。按照协议规定，会在2016年Q1完成这次收购，此后CloudPlatform将会属于Accelerite 这家有8000名员工的公司。
展望 2016 年，CloudStack 会有哪些改变呢？我认为会在一下三个方面做出改变：
一、 滚动式版本发布，缩短发布周期，升级更容易
在 CloudStack 的 Apache 社区主页上 http://cloudstack.apache.org/ ，出现了从来没有过的一幕：页面右侧显示了 4.7.0 is out 的下载提示，而左侧显示的是 4.6.0 released 发布公告。这种情况，只是社区巨大变革的冰山一角。更为深刻的变化是CloudStack代码提交发布方式的变化。
CloudStack 4.7 之前的开发方式是，当要添加新的功能时，会新开一个 branch ； 等新功能稳定后，在合并回 master 主干上；然后再进行代码测试，等到几轮测试稳定后，确定版本号打标签发布版本。CloudStack 4.7 版之后版本管理方式完全颠倒过来了，代码管理方式不会再按照固定的周期发布新版，采用小步快跑的模式，时刻确保master分支的稳定性，任何代码在进入master之前必须经过完整测试。
这种滚动发布的的优点是强化master主干代码的稳定性，谁提交代码谁负责测试，减少社区的测试成本。这无疑对代码贡献者提出了更高的要求。CloudStack用户长期以来的一个痛点，在生产环境中的系统很害怕升级，以至于国内很多公司内部都锁定一个版本，自己内部打patch，慢慢和社区版本脱机，然后又不得不把社区的新代码手工合并会自己的主干代码中。（BTW，关于自动升级这一点 ZStack 做的一直不错，感兴趣的朋友可以尝试一下）。
二、 支持上层应用 ，拥抱生态链
相比较OpenStack，CloudStack最大的短处是什么？是生态链产品支持的缺乏。CloudStack一直偏安一隅，做着管理好虚拟机的工作。这种定位是人们在2011年对云计算的需求。OpenStack在这方面把CloudStack甩了几条街了，从目前的格局看 CloudStack 是再也追不上了。对手的成功并不能否定CloudStack自身的存在的价值。但是是需要做出改变的时刻了。
在拥抱生态的改变上，可以从两个方向进行：
第一：增加对运行在IAAS上的软件的支持，比如：更容易的部署Hadoop，Spark ，更容易的对Hadoop，Spark集群的管理支持等；
第二：增加对Mesos云资源管理平台工具的支持，让CloudStack协助Mesos进行云平台中各种资源的调度使用；
从我个人的观察看 ，这两点即使CloudStack社区不做，也会有第三方的商业公司做。
三、 继续简化架构，更容易轻便的部署
CloudStack提供了一份称为”Simulator”的Docker发布版。使用这个版本你可以在没有云环境的情况，模拟查看CloudStack的各种行为，可以看作是一个“演示版”。2016年希望可以实现把CloudStack装进Docker里，更容易的部署，升级CloudStack。
除了Docker以外，运维人员更喜欢Ansible这个工具。把 Ansible 2.0 与CloudStack深度集成也是众望所归。
实际上，更多代码层面上的改变，已经在着手进行了，感兴趣的朋友可以查看社区的 “ House Clean ” 计划。
看到上面的这些可能的变化，你也许会对CloudStack未来的样子感到陌生。我也有同样的感觉。2016年对CloudStack来说仍然是探索的一年，就像一个刚断奶的孩子，要学会走自己的路。而我也会和之前一样，一如既往的关注这个孩子的成长。欢迎与我联系：weibo.com/tianchunfeng 。
【广告时间】 在IAAS虚拟化管理平台上 CloudStack仍然是性价比最好的选择之一。
参考：
https://cwiki.apache.org/confluence/display/CLOUDSTACK/2015+Plan
http://events.linuxfoundation.jp/sites/events/files/slides/vp-acs-tokyo.pdf
mp.weixin.qq.com
"
https://medium.com/@deborah.martin/four-cloud-services-portability-points-to-ease-delivering-iaas-8d7a4b2458e7?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
deborah.martin
May 27, 2016·4 min read
Workload portability should be one of the foundation strategies of any hybrid cloud IT initiative. While embracing open standards and open source are important, the real goal is to be able to move workloads effortlessly and seamlessly between your public and private cloud resources to achieve the full agility and other benefits hybrid cloud IT offers. The characteristics and elements IT should be looking to build into their private IaaS cloud workload handling capabilities is the ability to easily and quickly deliver infrastructure services throughout the entire application lifecycle. Whether in development, test or production, workloads should be able to effortlessly move across multi-private cloud environments and public clouds as needed. Using elements such as Topology and Orchestration Specification for Cloud Applications (TOSCA) compliant service design for service portability in hybrid environments can help you achieving that goal.
So what does a hybrid cloud strategy that has workload portability, based on TOSCA compliant designs, baked in provide to the business? Significant business savings and efficiency over the life of your cloud deployment and the agility you were looking for when you were planning a hybrid cloud. You design your infrastructure cloud service once and can deliver consistent service without errors on different cloud environments as workloads progress through dev/test, QA and then production. Enterprise IT can ensure consistent quality and cost economies for different IaaS workloads while accelerating the whole provisioning and delivery process for the business. All this delivers solid results to the bottom line year after year.
myLoc is a managed IT services company offering colocation, server hosting, managed hosting and cloud hosting. To spur company growth, myLoc needed a cloud-based services approach based on the
open source OpenStack platform. Hewlett Packard Enterprise worked with the myLoc team to provide a mature OpenStack deployment based on HPE Helion OpenStack. HPE provided the know-how to bridge legacy Web services with a modern cloud approach. Workload portability was a key feature of the service which helped deliver the benefits their customers were looking for their IaaS service, providing customers the “golden gate” to an agile hybrid cloud. The end results were a significantly improved business technology model, creating cost-efficiencies that could be passed on to their customers. Their clients improved use of IT resources increased efficiency and lowered costs of the service. The new enhanced hybrid cloud infrastructure now quickly adapts to meet their customers’ changing needs and is opening doors to new business opportunities.
The strength of a hybrid cloud is based in the many business advantages it provides. Open source technology and elements like TOSCA design principles enable workload portability, releasing the full potential of a hybrid cloud strategy. That full potential further reduces costs through economies of scale and decreasing TCO and avoids vendor lock-in. Plus, your enterprise immediately benefits from the innovative impact of collaborative, open source community-driven development.
As platinum sponsors of the OpenStack foundation and the formation of the Cloud Foundry™ foundation, HPE provides dedicated leadership and active participation to help drive the growth and maturation of these influential open source platforms. HPE involvement includes significant contributions in funding, resource allocation, testing, code, training, and commercial deployments. HPE Helion OpenStack is a hardened, extensible product designed to deliver leading open source cloud computing while adhering tightly to OpenStack software API standards and services so infrastructure services can be deployed and moved more easily between environments. In a hybrid IT world, open is the enabler.
Read the latest report from Gartner on other considerations like workload portability you need to consider, beyond just basic infrastructure provisioning, to provide the enhanced services your development community needs from your private cloud.
Also, take a few minutes to read the CIO Quick Pulse paper, “The Next Cloud Challenge” on open source solutions and workload portability.
For more information and resources on enabling and simplifying IaaS for your organization, explore the rapid infrastructure provisioning use case site.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
"
https://medium.com/@kumarshivam-66534/a-walk-through-on-iaas-paas-and-saas-7e8a4e4793fb?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kumar Shivam
May 28, 2018·3 min read
To understand about cloud computing and the cloud services that are available in the market, we first need to start with the three core categories of cloud computing services. Let’s understand what these categories are and what do they offer?
Windows Azure offers compute in three matrices: -
1. Infrastructure as a Service (IaaS)
2. Platform as a Service(PaaS)
3. Software as a Service(SaaS)
These offerings are based on permissions and responsibilities which allows us to manage our applications and environments.
The following is a side by side detailed comparison: -
1. Infrastructure as a Service: -
IaaS provides servers in the cloud (virtual machine) that can be completely controlled by the users. Users would have a broad range of permissions in the server from managing the OS to their applications.
This service gives a glimpse of typical on premises virtual machine, which allows us to login remotely and manage it.
If you need a solution that requires a custom third party application or multiple applications running on a single machine or the ability to turn on on-premises packaged applications, then IaaS might be a good option.
In addition to that you can leverage a range of services including detailed billing, monitoring, log access, security, load balancing, clustering, storage resilience (i.e. backup), replication and recovery. All these services are policy driven and facilitate IaaS users to implement greater levels of automation and Orchestration for important infrastructure tasks.
Ex: — A user can implement policies to drive load balancing to maintain application availability and performance.
To make the environment ready, the user can create a VM, install OS, middleware (such as DB, application server) and deploy the application. Users can create storage buckets for workload and backups.
With the help of other services, users can track cost, monitor performance, balance network traffic, troubleshoot application issues, manage disaster recovery and much more.
2 Platform as a Service (Paas)
PaaS facilitates builds and supplies a resilient and optimized environment in which we can install applications and data sets.
Users can focus on designing and running their applications and not worry about constructing and maintaining the underlying infrastructure and services.
Azure cloud service has two components, the application and a configuration file. Web role and worker roles combination is required to execute the application.
A web role is an azure VM that is pre-configured as a web server (running IIS) and automatically loads the applications when the server is up. It creates public endpoints for the application, which might be a web site or a web api.
Worker Roles runs in parallel to web role and are responsible for performing computation functions to support our applications. Web role job is to accept user’s input and queue an action for the worker role to process at later time. This makes web role more responsive and allows to file and forget the tasks to be processed later.
PaaS offers services like compute and storage infrastructure, version management and compiling and testing services that help developers design their applications more rapidly and efficiently.
PaaS is good for the following scenarios: -
i) To design and deploy cloud based applications
ii) Analytics or BI
We just have to seed our code to any of the code repository (Git, Bitbuket, Codeplex,TFS or Dropbox) to push to our web site and Azure will take care of the rest like CI and CD. CD requires us to write some configurations to make it automated.
3 Software as a Services(SaaS)
The software which is built and hosted to get things done (like Office 365,Azure configuration portal ,google drive etc.
Technical Consultant | Passionate about exploring new Technology | Cyber Security Enthusiast | Technical Blogger | Problem Solver
154 
154 
154 
Technical Consultant | Passionate about exploring new Technology | Cyber Security Enthusiast | Technical Blogger | Problem Solver
"
https://medium.com/escrevocartas/o-que-h%C3%A1-por-tr%C3%A1s-dos-altos-%C3%ADndices-de-aquisi%C3%A7%C3%A3o-do-servi%C3%A7o-iaas-na-nuvem-404c4ad6de4c?source=search_post---------185,"There are currently no responses for this story.
Be the first to respond.
A computação forense está alinhada com essas novas perspectivas?
O mundo tecnológico vem se desenvolvendo em um ritmo desenfreado e por isso, diversas técnicas forenses precisam está atualizadas para continuar provendo justiça em casos considerados de grande repercussão. Com isso, a quantidade de incidentes vem aumentando e consequentemente tornando o trabalho de peritos e profissionais da lei mais difícil.
Tomando como base este contexto, podemos destacar dentre as demais ciências que compõe o âmbito forense, a de longe, uma das mais essenciais atualmente: a forense computacional que vem sofrendo contínuos trâmites para aquisição de provas em ambientes distribuídos e altamente escaláveis. O que antes poderia ser palpável, como por exemplo, a perícia em dispositivos e servidores que poderiam ser acessados localmente ao realizar um dos processos investigativos de aquisição de provas, hoje, os tipos de plataformas de acesso vem sendo criadas e trazendo consigo novas barreiras e desafios para a comunidade em específico. Em outras palavras, além de realizar buscas em dispositivos e ambientes locais é preciso também realizar buscas por evidências em nuvem, ou remotamente.
A nuvem é um conceito antigo, começou a ser embasado mais ou menos na década de 60, mas que veio tomando forma nos dez últimos anos graças as empresas Amazom.com e Google que foram as pioneiras no mercado.
Por ter bastantes benefícios, a adoção da nuvem por parte de diversos níveis e perfis de usuário vem aumentando.
A Cloud é conhecida por contemplar três modelos de serviços, quatro modelos de implantação e cinco características essenciais, que podem ser vistas abaixo:
3 Modelos de Serviço:
IaaS — Infraestrutura — como — um — serviço;
PaaS — Plataforma — como — um — serviço;
SaaS — Software — como — um — serviço;
4 Modelos de Implantação:
Nuvem pública;
Nuvem híbrida;
Nuvem privada;
Nuvem comunitária;
5 Características essenciais — segundo o NIST (National Institute of Standards and Technology )
Com tantos benefícios, observou-se que entre os modelos de serviços existentes, o mais consumido pelos clientes era o de IaaS ou Infraestrutura — como — um — serviço. Este modelo é reconhecido por oferecer mais comodidade e um ótimo gerenciamento da aplicação para o cliente, ou seja, o usuário e capaz de realizar uma administração mais efetiva da sua aplicação instanciada na nuvem.
Considerando um incidente em uma instância, a mesma deve passar por um processo de isolamento, onde deveremos considerar como uma área de confiança para que se possa iniciar as buscas por provas que respondam posteriormente ao incidente acontecido.
Alguns pesquisadores afirmam que de frente podemos considerar alguns desafios que de certa forma já dificulta o processo de aquisição de provas em nuvem, visto que haverão casos onde uma instância poderá conter mais de um usuário e para realizar essa busca é possível também que não haja uma privacidade referente aos dados e informações desses usuários por causa do próprio ambiente que mantém as informações distribuídas e mais, a ideia de compartilhar o mesmo espaço de informações em nuvem pode acarretar problemas neste mesmo processo de busca e apreensão de evidências.
A importância da imersão no ambiente em questão é de suma importância para o conhecimento inicialmente da sua arquitetura e também das vulnerabilidades que ainda não está tão evidente. É importante também continuar o processo de pesquisas para cobrir as novas perspectivas de desenvolvimento e modos de acesso aos sistemas a fim de continuar assegurando informações e promovendo o contínuo progresso da computação forense.
Joice Dantas — Graduanda em Sistemas de Informação pela UNEB-(Universidade do Estado da Bahia) e afiliada às instituições AAFS (American Academy of Forensic Science) e SBCF (Sociedade Brasileira de Ciências Forenses).
Um mundo com crônicas, contos e tecnologia.
3 
3 claps
3 
Um mundo com crônicas, contos e tecnologia.
Written by
“Ou escreves algo que valha a pena ler, ou fazes algo acerca do qual valha a pena escrever.” ― Benjamin F.
Um mundo com crônicas, contos e tecnologia.
"
https://medium.com/@digitalpadm/what-is-cloud-computing-iaas-paas-saas-private-public-hybrid-873799d7b21c?source=search_post---------254,"Sign in
There are currently no responses for this story.
Be the first to respond.
DIGITAL PADM
Dec 15, 2019·4 min read
Cloud computing is the ability to access and manipulate files from a remote site. You need an internet connection to access the files. all Google services, Facebook, Twitter, and Instagram are all examples of cloud-based applications.
It is the delivery of different services through the Internet, including data storage, servers, databases, networking, and software.
With a web browser, clients can access their information and engage different applications without investing in expensive software or paying huge license fees to use some web infrastructure.
It is multiple computers with unlimited storage with high processing speed located in specific areas around the globe. These computers are referred as data centers.
Cloud services are provided as Private and public. Private is only accessible within an organisation while public is accessible to the general public. Security is more enhanced in private than in public.
Three services model are based on cloud computing.
It refers to cloud-based services, such as pay-as-you-go storage, networking, and virtualization. This is access to resources offered by the provider . It can be virtual devices or physical devices. The most common examples are Amazon Web Services, Microsoft Azure, etc.
IaaS helps enterprises create and manage their data as they scale, paying for the server space that they used to develop hardware or software.
It is cloud-based software available for purchase on a subscription basis. It is mostly used for apps that need both web and mobile access.
SaaS products don’t need to be downloaded and installed on individual devices. Most of them can be run directly from web browsers. It should be taken into account that customers are not responsible for hardware and software updates.
The most common examples are Dropbox, JIRA, Salesforce
It refers to cloud-based platform services that provide developers with frameworks that they can use to create custom apps.
The model provides programmers with a platform that is used to create software delivered via the Internet. All servers, storage, and networking can be controlled by the business or a third-party provider. The most known examples are Windows Azure, Google App Engine, Openshift
PaaS makes development, testing, and deployment processes fast, easy, and cost-effective.
A cloud deployment model represents a specific type of cloud environment. It is distinguished by ownership, size and access. There are important deployment models like public cloud, private.cloud, hybrid cloud and community cloud. Other deployment models include multi-cloud, distributed cloud, inter cloud, Big data cloud, High Performance Computing (HPC) cloud etc.
A public cloud is a publicly accessible cloud environment owned by a third-party cloud provider. Examples of public cloud are Amazon Web Services (AWS). Google Compute Engine, Microsoft Azure etc.
The cloud provider is responsible for the creation and on-going maintenance of the public cloud and its IT resources,
The advantages of this model are easy to set up at low cost as provider covers the hardware, application and bandwidth cost. This model is scalable to meet growing needs.
No resources are wasted from the user’s perspective as the payment is done on a pay-per-use basis.
A private cloud is owned by a single organization. Private clouds enable an organization to use cloud computing technology as centralized access to IT resources by different is location, or department of the organization.
When a private cloud exists as a controlled environment, the risks and challenges are less compared to public cloud. but regular maintenance is required for deployment private model.
Hybrid cloud is a cloud environment comprised of two or more different cloud deployment models. These different deployment models are pooled together by standardized tools.
For example, a cloud consumer can choose to deploy private cloud services to process sensitive data and other less sensitive cloud services to a public cloud. The result of this combination is a hybrid deployment model.
Hybrid deployment architectures can be complex challenging engine to create and maintain due to the potential disparity in cloud environments.
HPC cloud refers to the use of cloud computing services and infrastructure to execute High Performance Computing (HPC) applications.
These applications consume more computing power and memory and are traditionally executed on clusters of computers.
Various vendors offer servers that can support the execution of these applications.
In HPC cloud (High Performance Cloud), the deployment model allows all HPC resources to be inside the cloud provider infrastructure.
The adoption of cloud to run HPC applications started mostly for applications composed of independent tasks with no inter-process communication.
digitalpadm.com -programming tutorials
digitalpadm.com -programming tutorials
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-and-servers/bulut-hizmetlerinde-iaas-paas-saas-nedir-3bb5620fd357?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
Amazon(AWS), Google(Cloud Platform), Microsoft(Azure), IBM(SoftLayer, Bluemix) gibi büyük firmaların bulut hizmetlerindeki servis katmanları birbirleriyle aynı mantıktadır.
Amaç katman katman olan bu yapıların geliştiriciden soyutlanarak herkesin rahatça kullanabileceği ortamlar oluşturmaktır. Aşağıda bu sorumlulukların kimin sorumluluğunda olduğunu anlatan bir resim görmektesiniz.
On-Premises: Yazılımlarının sizin veya firmanın bilgisayarlarına yüklendiği kısımda tüm katmanların sorumluluğu sizin ekibinizin üzerindedir. Bir sistem ekibiniz olması gerekir. Bu sistem ekibi sunucuları, veritabanlarını, güvenliği, network’ü bilmesi ve kurması gerekmektedir. Veritabanının yedeklerinin alınmasını sağlaması, işletim sisteminin güncel sürümlerinin yüklenmesini sağlaması gerekmektedir. Ayrıca sistem ekibinin JVM, dll, plugin gibi yazılımın ihtiyacı olan Run-Time sisteme kurmaları gerekmektedir.
Infrastructure As A Service: Size bulut üzerinden sanal Compute, Storage, Networking satıldığı, kiralandığı bulut hizmeti olarak düşünebilirsiniz. Bilgisayar, Disk ve Network kartları almak yerine bunları Sanal olarak bulut’tan kiralayıp üzerine istediğiz işletim sistemini kurup yolunuza devam edebilirsiniz.
Platform As A Service: Bulut üzerinde direk bir java, ruby, node uygulaması geliştirmek istiyorsunuz ve işletim sistemi, network, sunucu gibi sistemler ile uğraşmak istemiyorsunuz, Sadece uygulamanızı geliştirmek ile uğraşıyorsunuz sonrada uygulamanızın run-time dosyalarını ilgili platforma atıp çalışmasını sağlıyorsunuz.
Software As A Service: Uygulamaların bulut’tan hizmet vermesine SaaS denir. Kullanıcılar sadece uygulama arayüzlerine erişebilir. Kendilerine ait bilgileri bu yazılımlara girerek, bilgilerini bulut üzerinde saklar ve buradan kullanırlar.
Örneğin: GoogleDocs, Evernote uygulamalar SaaS olarak düşünebiliriz.
Uzun süredir farklı sektörlerde (Askeri, Telekomünikasyon, Devlet, Bankacılık, Sigortacılık, Tübitak, SaaS) yazılımlar geliştiriyorum. Bu süreçte Havelsan, Milsoft, T2, Cybersoft ve Thundra firmalarında yönetici ve yazılım mühendisi olarak çalıştım. Deneyimlerimi ve teknolojik bilgi birikimi mi olabildiğince OnurDayibasi.com adresinde toplamaya çalışıyorum. Tüm yazılarıma ve daha fazlasını bu site üzerinden erişebilirsiniz.
AWS, Azure, OpenStack
104 
1
104 claps
104 
1
AWS, Azure, OpenStack
Written by
Senior Frontend Developer at Thundra
AWS, Azure, OpenStack
"
https://medium.com/strongnode/ama-with-polygon-daily-open-source-and-community-are-key-values-for-strongnode-io-core-team-a15eb88574e7?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 1, 2021·4 min read
The Infrastructure-as-a-Service (IaaS) tech company StrongNode.io CEO and Co-founder Daniel Saito joined Polygon Daily for a brand new AMA episode last 16 September 2021 via Telegram. Daniel answered questions regarding our company, project, products, and shared facts about our upcoming IDO launch, and $SNE token public sale plans.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
Here are some of the highlights from the AMA session between StrongNode and Polygon Daily:
Hosted by: Harris Wu
What is StrongNode all about? What is the major goal of this project and what sets it apart?
Daniel Saito: My name is Daniel Saito and I am the CEO and Co-founder of StrongNode. StrongNode is an Infrastructure-as-a-Service tech company and innovation lab. We harness the power of latent and idle compute resources like CPU/GPU cycles, network bandwidth, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources.
What makes the team behind it special/the best fit to accomplish the goal? How are the advisors helping the project?
Daniel Saito: The core StrongNode team consists of C-Suite members that have commanded +10 exits to date ($8.5B of exit liquidity to our investors). My CTO is Colin Charles , an early employee at RedHat, Chief of Community at MySQL and co-founder of MariaDB server. We started with the concept of free software and needed to find a business model. Which then we pivoted the business to selling support. We grew the community and users and customers and exit for $1B to Sun Microsystems. My CPO (chief product officer), Gil works at Adobe and worked with numerous product launches with Fortune 500 companies.
We took this opportunity to band together for a common cause and start StrongNode.
Open Source and Community Support
What values drive all the technology that StrongNode creates? We heard you mentioned Open Source and Community?
Daniel Saito: We are a firm believer that the community builds products. Community drives product development. When running an open-source project, the value is derived from community involvement and it is up to the community to dictate that it is a success.
Here at the crossroads, StrongNode is where Innovation intersects with Community to help us build a sustainable edge network.
We learned the value of community as we were the tenets of OPEN SOURCE with our experience in launching Linux at RedHat and databases in MySQL. We have plans to utilize open source yet build a payments layer to it.
You have an upcoming IDO, what should we know about the $SNE token and its value for individuals or for companies?
Daniel Saito: Yes. Our IDO is almost here — happening [on October 6] with Starter.xyz and Bull Perks. Join us for our IDO and get to know more about the process of whitelisting yourself to participate through this link — https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
I love this project for placing importance on the community aspect. It’s something that I think is lacking in a lot of projects. How can early investors really help a project grow before it launches? From providing liquidity to general participation, what different roles are there when it goes live?
Daniel Saito: We really can use your help to get the word around. We will have bounties and campaigns to entice the community to get the word out about our solution. This is the only way we can get a larger token distribution and with large token distribution, it means more wallets. Community is key, it has proven to us time over time that it matters. MySQL’s $1B exit Much of the evaluation was based on the value of our community. So if we learned it the first time around you can expect the community to be strong!
For more information, visit https://strongnode.io/
Join us on Telegram: https://t.me/strongnodechat
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
See all (8)
452 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
452 claps
452 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sonm/sonm-crypto-iaas-testnet-open-to-public-4523d750186a?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
We have released a new version of SONM software — Crypto-IaaS core v0.4.0-alpha1 and GUI v2.0.0-alpha1, and now it’s operational in Testnet (RINKEBY) and available for downloading and testing. We’re happy to open the 30 days countdown till the livenet!
https://github.com/sonm-io/core/releases/tag/v0.4.0-alpha1https://github.com/sonm-io/Wallet/releases/tag/v2.0.0-alpha1
This version is a big step forward in the SONM development. The Crypto-IaaS is almost the same as regular IaaS from big cloud providers, but we call it Crypto because of the following reasons:
Now the Crypto-IaaS is entering an active phase of public testing, which will last one month. Within a month, we will make the necessary changes to the code to improve system performance, and on June 30 we will transfer the system to the main network — it will become operational with real SNM tokens.
The Crypto-IaaS includes following features:
SONM Core — a set of software components that forms the system core.
SONM GUI — a graphical interface, front-end of the system. Currently, GUI is targeted for customers, featuring resource purchase operations.
SONM blockchain — We have decided to rename the sidechain, described in this article to blockchain because it’s actually our own blockchain based on Ethereum.
Crypto-IaaS market making, guaranteed revenues, and importance for miners.
What happens when SONM network is lacking real world Customers at some point? This is a common flaw of all modern distributed computing networks you may ever hear of. The computing resources would be idle and not earn money.
We find it very obvious, that it is possible to say, that there are no more free computing powers in the world. The times of SETI@home and free resource contribution are gone since the invention of crypto mining. Any idle machine could be doing at least some crypto mining.
Meanwhile, SONM allows anyone to rent computing powers (a computing instance). There are different options how to do it, via GUI with a mouse click, from the command line or even API. Basically, everything on SONM may be done via API. This allows automatic computing resource trading for software bots.
What if some software bot was constantly monitoring crypto mining profitability for some coins, let’s say for Ethereum, and was trying to purchase any free computing power if the price is low enough to make mining profitable? Buy any “dip”, where “dip” is related to computing power. If some resources are idle at some moment, software bots could compete with each other (in price) to purchase such resources, and then launch mining. A bot, in this case, will earn some coins (let’s say, Ether) and pay for resources in SNM tokens.
This way we will have a converged market, where computing powers are rented for (a) real business tasks and (b) crypto mining in a universal way, via single and unified API.
Like nicehash, but with real tasks. Like decentralized computing projects, but with guaranteed revenues even without Customers at some moment.
Well, this is what Crypto-IaaS is all about. The whole system design, mentioned above allows that — the product for a customer, the architecture, everything is dedicated to make a unified marketplace, where real tasks (as Docker containers) may live together with mining on same machines, under a unified pricing and billing model.
Now we need a final piece — someone who will purchase the resources for mining, a market maker.
We prepared such an “arbitrage” software bot as a reference design. We will make it open-source and run it on our side. In our hands it is not trying to make a margin (a profit), because we care only about working marketplace. But any software developer or a professional miner is encouraged to take this reference bot, change the coin in config (or even change strategy algorithm) and beat our price level.
If you know how to mine with better profits, than just plain Ethereum mining (lets say some “musicoin”) — you will be able to pay 1 cent more and beat our bot’s purchase prices and get all SONM idle powers.
So, the arbitrage-bot (market making, cloud mining bot). We prepared several automated customers that will buy out supplier’s idle capacity for cryptocurrency mining. Before starting a deal, the bot evaluates the profitability of mining any currency, then opens a deal with a supplier and starts mining cryptocurrency on the equipment. As a result, the Worker receives the amount of SNM equivalent to the volume of cryptocurrency produced — i.e., the bot sells the cryptocurrency on the exchange to give the reward to the Worker. If the profitability of mining changes during the mining process (during the deal period), the bot closes the current deal and opens a new one with updated conditions.
Take part in the public tests.
Today we release an evaluation build for public testing. Everyone is welcome to try and see how software is working, help us find and fix bugs. This test phase will last for only one month, and then we will go to production (launch livenet) on 30th of June.
With the livenet on 30th of June cryptocurrency miners will be able to switch to SONM confidently — we will guarantee the minimum income at the level of conventional mining. At the same time, the expected revenues are higher than traditional mining, because:
Download the latest version here and start testing the Crypto-IaaS right now! We will be very happy if you join the testing process and let us know about the bugs you find so we can fix them quickly.
Read the technical documentation here to learn how to install SONM and start using it!
Twitter: https://twitter.com/sonmdevelopmentFacebook: https://www.facebook.com/SONMproject/Telegram: https://t.me/sonm_engInstagram: https://www.instagram.com/sonm.hq/Website: https://sonm.comFog Computing Challenge: https://sonm.com/challenge/
SONM is a global fog computing platform for general purpose…
296 
296 claps
296 
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
"
https://medium.com/@anuj_61280/amazon-web-services-great-finish-to-2016-incredible-to-start-to-2017-ccb3a47db4af?source=search_post---------374,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anuj Jaitly
Dec 19, 2016·2 min read
It was wonderful year to be a participant in the ever evolving Cloud Ecosystem. 2016 made it it clear that the IaaS market is now a three horse race. AWS, Azure and Google Cloud Engine (GCE). There are other players obviously — they will continue to play in niche market and verticals.
Over the past few weeks, AWS has made some major service announcements that are typical for AWS in and around their annual Reinvent conference. Here’s a quick summary of some of the key topics.
2017 will clearly begin on an escalated path.
As the year closes out, it is clear that the ‘Journey to the Cloud’ is just that. It may begin with a single step, but there are profound advantages to be had regardless of the size and scale of the adopter. This new phase in IT will be exciting to participate in. Business and Individuals that participate we be emboldened and their knowledge broadened.
In 2017, those that do not leverage the Cloud will loose out on its efficiencies in an agile and changing marketplace. Unfortunately, the effect will find them providing out of date business services to a reduced market. Embracing the cloud and the cloud eco system — will showcase business agility and the willingness to adopt in a very competitive world.
Anuj is a Cloud Enthusiast/Evangelist and a Father of Three. He runs the North American operations of BlazeClan Technologies, an AWS Premier partner.
See all (293)
1 
1 clap
1 
Anuj is a Cloud Enthusiast/Evangelist and a Father of Three. He runs the North American operations of BlazeClan Technologies, an AWS Premier partner.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gaurav-kaushikgk88/vpc-configuration-using-terraform-c4040f48f147?source=search_post---------333,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gaurav Kaushik
Dec 23, 2020·6 min read
AWS is one of the most stable and talked about Cloud IaaS(Infrastructure as a Service) as of today. Managing Cloud resources manually on IaaS platform can be a tedious task. Automation and Orchestration comes handy while creating Cloud Infrastructure resources & this is where Terraform comes into picture.
HashiCorp’s Terraform is a tool for creating and managing Cloud Infrastructure for various Cloud Iaas vendors like AWS, Azure, Google Cloud to name a few.
"
https://medium.com/pragmatic-programmers/iaas-vs-caas-326e72924547?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Pragmatic Programmers
Jan 29, 2021·2 min read
👈 A Tale of Two Orchestrators: Swarm and Kubernetes | TOC | Provisioning Your Infrastructure 👉
Now that we have a better understanding of where container orchestrators fit in, there’s still a choice to be made if you’re considering deploying containers to the cloud. You can either manage things at the infrastructure level and set up the orchestrator yourself, or you can use a container platform that handles the underlying infrastructure and provides a preinstalled orchestrator ready…
"
https://medium.com/@brianstucki/some-changes-at-macminicolo-a4827042abe3?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brian Stucki
Apr 6, 2016·5 min read
This is it. The blog post I wasn’t sure I’d ever write. Partly because I knew it’d have to be about me and I’m a pretty private person. But also because I wasn’t sure this would ever happen without some difficult choices. Sometimes you have to choose the harder right instead of the easier wrong. So here it goes…
I’m Brian Stucki. I’ve been running Macminicolo for over ten years. For half that time, I ran the whole thing alone. Later, Justin joined me for data center duties. Running a hosting company is a mixed bag opportunity. It’s exciting because you get to be involved with cutting edge technology. In a niche like Mac hosting and colocation, the customers are always intelligent and working with them lets me be a small partof what they’re building. I’ve tried very hard to do good, honest work and do right for our customers and the community. For every email I’ve received confirming the death of the Mac mini, I’ve also received emails from people who put a Mac mini into our data center and it saved their business.
The other side of hosting company ownership is constant connection to the online world, middle of the night support tickets, and a nagging worry that tech will change and you’ll be out of business. I call it the SBO ulcer. (I am not a doctor but I’m guessing many of you feel it too.) I remember listening toa podcast about when Instapaper was sold and had a lot of the conversation resonate with me.
Over the last decade, the business has been very successful. It has grown well financially but just as important to me is that it also has a great reputation. Macminicolo is the name for Mac hosting and Mac mini information. I’m very proud of this position and I’m happy that people have this view of Macminicolo. With a good name like ours, I receive an offer to buy the company about 5 times a year. I’ve always turned them down because I wasn’t interested in changing things, or the company was undervalued, or just because the new owners would have done a horrible job in running this niche service. Often, they didn’t understand what made it special and they just see the dollar signs. That would have hurt me to sell, and would have left my customers/friends in a bad place. No thanks.
In short, I’ve decided to sell ownership of Macminicolo and merge it with another company. I will stay on as President of Macminicolo and also serve as a Vice President of the parent company, MacStadium.
Now, I could just announce this with no explanation and be done with it . I could also write one of those generic acquisition posts focused on sunsets and brands and blah. Instead, I’ll be forthright and real like I’ve always tried to be with customers.
For me, it boils down to three reasons:
Here is my family. (How’s that for keeping it personal?) My wife and kids are the most important thing in the world to me. This sale has helped me put a little money away for a rainy day and relieved some dad stresses like making sure my girls can go to college and that I won’t have to be a burden to my boys when I’m old and senile. This sale also lets me see more of them now. For ten years, we haven’t been able to take a truly disconnected vacation. I’ve always monitored and maintained the data center even from afar. Frankly, kids deserve to be with their dad in the wilderness where nothing can interrupt them. Merging my company allows me to do that but still have plenty of eyes and hands at the data center to keep good service. This, by far, is my main reason for merging.
Second, I think we’re on the cusp of a major change in how tech companies host their products. Companies that offer online services are looking for more control of their data and offerings. Startups and businesses are moving away from huge, faceless hosting companies so they can control their own data and have more direct hands on servers, etc. I want to be sure that they can look at Mac hosting options that are large enough to be viable, but personal enough to be a good partner. While I think we’re good at the latter, I can’t offer the former on my own. Now with this merger, we have all sorts of options.
Need 500 OS X VMs running across dozens of Mac Pros? No problem.
Want hundred of Mac minis, Pros and blades in place tomorrow to use for your company? Sure.
Want to build a personal ACN in three diverse locations to deliver your podcast episodes? Coming right up.
Finally, I found a good company with good people and a long-term plan. As I mentioned earlier, I’ve had a number of offers over the years. Four months ago, I got a call from the Greg, the CEO at MacStadium. Since that time, we’ve had many conversations about what could and should take place. When it comes to Macs in the data center, they’re doing some great things already. They have a great mix of knowledge and also a willingness to learn and grow. That’s rare.
MacStadium is based in Georgia. They recently expanded to Ireland as well. They were looking to offer another data center on the West coast and that’s part of what led them to me. Rather than start from scratch, Macminicolo gives them immediate space in this part of the Unites States and hundreds of great customers already on board. They’ve committed some significant investment to make this location even more robust. As mentioned earlier, I’ve also committed to take a major role at the company. This will allow me to do the things I enjoy (i.e., promote the Mac as a hosting option, teach people how to use OS X in the data center, be a known friend for customers) but step away from the things I’d like more help with (i.e., middle of the night support tickets, invoicing, network expansions.)
I can’t explain just how much I had to think about this decision. I’ve put a lot of time, energy and attention into building Macminicolo. In the last few months I have performed the calculations, said the prayers, applied the SBO Ulcer test and projected the best and worst case scenarios. I ran it by successful people I know and trust. Everything pointed to making this decision and I’m excited to see what we can build.
Dad, Faith, Sports, History, Las Vegas, Finance and Business. Also a guy at @macminicolo.
See all (83)
5 
5 claps
5 
Dad, Faith, Sports, History, Las Vegas, Finance and Business. Also a guy at @macminicolo.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/evoknow/saas-paas-iaas-and-bare-metal-a-game-of-technology-abstraction-4fa748eeae6?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Tech is full of jargon. I feel like a new jargon is invented every few weeks or even every day! I think it would be very interesting if Google were to publish a list of technology terms showing each term’s debut on the internet. There are terms that you can ignore or forget right after you have heard or read them. And then, some terms will keep appearing in your software…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://m.oursky.com/saas-paas-and-iaas-explained-in-one-graphic-d56c3e6f4606?source=search_post---------2,"The Pizza-as-a-Service metaphor was firstly introduced by Albert Barron in 2014 as a visualization of the differences between Infrastructure-as-a-service (IaaS), Platform-as-a-service (PaaS) and Software-as-a-service (SaaS). At first sight it looks brilliant — but if you look in depth, it falls apart. This diagram wants to illustrate that you need to do less and less, but the items that are listed and increasingly “outsourced” don’t fit the comparison. I’ll look at the services in the original diagram and suggest a better comparison for people who want to choose which services to use.
The basic concept is in each case you get a pizza, though in some cases, you do all of the work; in other cases you have other people do the work for you.
In the diagram, the amount of work you do is in the following order:
Made at home > Take and bake > Pizza delivery > Dining out
Let’s focus on two stages first: if “Traditional On-Premises Deployment = Made at home”, why does “Infrastructure-as-a-Service = Take and bake”? At first glance, it means that the service provider gives you a pre-made pizza, so you can bake it in your own kitchen without making a mess. However, this is not how Infrastructure-as-a-Service differs from Traditional On-Premises Deployment.
Rather than thinking of the consumer doing less and less, let’s think about a product that is increasingly customizable because less is pre-made by the service provider.
By adopting an X-aaS, you don’t need to worry about a certain “X” — be it infrastructure, platform, software or pizza. Instead of forcing the X-aaS model into existing pizza services, I will be dissecting the steps to making an increasingly customized pizza.
Pizza-as-a-Service means you can enjoy a pizza (a fully finished product) upon order. You call the pizza delivery so the pizza is already designed, prepared and baked by somebody else and arrives hot and ready to eat.
In the world of SaaS, we only need to sign up to use the service. We can customize some features, layouts, or users (pizza toppings), but we never need to worry about the deployment and framework the project is used (just like how the service providers put our orders together for us). It also means we can’t choose this brand of cheese over that, or how long they choose to bake it for. If you don’t like their crust, you’ll have to choose another company. So what should be the service layer right below Pizza-as-a-Sevice?
The idea of “take-and-bake” in the original diagram was close, but it misses a few key ideas. The take-and-bake doesn’t allow customization, which is what IaaS and PaaS offer. IaaS and PaaS take care of some backend things you don’t want to deal with, and probably come with API’s, but you choose what you can add.
Instead of a “take-and-bake”, the next level down is more like walking into a kitchen with a ready-to-cook package of ingredients. IaaS and PaaS are when you want to make your own pizza, but you don’t need to worry about buying ingredients, prep work, or having the right tools. You can focus on rolling out the dough, assembling the pre-washed ingredients to your taste and factors like size and thickness before sticking it into someone’s provided oven.
If you use IaaS or PaaS, you can focus on your own business logic and implement your user interface on a reliable platform. The service provider manages the hosting and hardware technologies for you.
This option might not be suitable for every end-user. At least you need some pizza-making skills to turn the dough into a pizza.
Kitchen equipment is infrastructural investment. For engineers, this is where your code runs. It also includes all the infrastructure you need (such as runtime, storage, networking). Without it, your product doesn’t go anywhere, but on the other hand, it doesn’t affect the nature of the product itself.
For a pizza, the next layer below is buying your own ingredients to prepare exactly the pizza you want, with the specific brands you like. But you don’t have to worry about the oven, rolling pin, whether you counter is big enough, or the pizza cutter. Now, you have absolute control on what pizza you are going to make, but maybe the rental kitchen doesn’t have every type of tool you want.
For hard core pizza makers — not only will you want to choose all your pizza ingredients and make it from scratch, you will also want complete control of all the tools in your kitchen: the oven that has the exact settings and capabilities you want, the pizza stone many places don’t have, or let your dough sit overnight at the most optimal corner of your fridge. And finally, you can drizzle that homemade chilli oil on top and savor it fresh out of the oven.
You may think that’s overkill — but in real world, sometimes the effort is necessary if you can’t find a service that will let you customize for that one essential deployment setting, such as a firewall configuration or that specific network setting requirements.
SaaS (Software-as-a-Service)
PaaS (Platform-as-a-Service)
IaaS (Infrastructure-as-a-Service)
On-Premises Deployment
IaaS and PaaS save us a huge amount of preparation work, so we can customize our “pizza“ and bake it fast. The best type of service is whatever suits your product best.
Hope these examples clear up these concepts.
If you found this piece helpful, follow Oursky Publication for more startup/entrepreneurship/project management/app dev hacks! 💚
😻 At Oursky we’re all about helping brands and entrepreneurs make their ideas happen. Get in touch if you’re looking for a partner to help build your next digital product.
We Develop for Developers.
365 
1
365 claps
365 
1
Written by
Software Engineer | Previously Growth @ SCMP | davidng.hk | Love hiking and camping.
We Develop for Developers. We open source best practices for product development for web and mobile. Visit us at oursky.com
Written by
Software Engineer | Previously Growth @ SCMP | davidng.hk | Love hiking and camping.
We Develop for Developers. We open source best practices for product development for web and mobile. Visit us at oursky.com
"
https://medium.com/@mindkatalyst/saas-vs-paas-vs-iaas-differences-and-benefits-5143715ff4f3?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Carnellia Ajasin
May 24, 2019·3 min read
Small businesses and global enterprises alike are fascinated with the cloud. But there’s more to it than simple deployment. Whether you’re thinking of migrating your business to the cloud for application or infrastructure deployment, it is important to understand the differences and advantages of the various cloud services available.
There are three basic models of cloud service to choose from: Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). They all have certain benefits but you need to understand the differences between each and pick the best one for your organization.
Software as a Service (SaaS), also known as cloud application services, is the most commonly used option for businesses in the cloud market. With SaaS, you utilize the internet to deliver applications to your users or employees, which are managed by a third-party vendor. A majority of SaaS applications are run directly through the web browser, which means there is no need to download or install anything locally.
The first examples that come to mind when thinking of SaaS applications are Google Apps, Dropbox, Salesforce, Cisco WebEx, Concur, and GoToMeeting.
Platform as a Service (PaaS), provide cloud components to certain software while being used mainly for applications. They give the framework for developers to build upon and use to create customized applications. All servers, storage, and networking can be managed by the enterprise or a third-party provider while the developers can maintain management of the applications.
PaaS gives businesses the flexibility to design and create applications that are built into the PaaS with special software components. These applications, or middleware, are scalable and highly available as they take on certain cloud characteristics.
AWS Elastic Beanstalk, Windows Azure, Heroku, Force.com, Google App Engine, Apache Stratos, and OpenShift are all examples of PaaS apps.
Infrastructure as a Service (IaaS), is made of highly scalable and automated compute resources. IaaS is complete self-service for accessing and monitoring things like CPU usage, networking, storage, and other services. It allows businesses to purchase resources on-demand and as-needed instead of having to buy hardware outright.
Unlike SaaS or PaaS, with IaaS clients are responsible for managing aspects such as applications, runtime, OSes, middleware, and data.
Examples of IaaS include DigitalOcean, Linode, Rackspace, Amazon Web Services (AWS), Cisco Metapod, Microsoft Azure, and Google Compute Engine (GCE).
Understanding the differences between the three models lets you make the most cost-effective and appropriate decision on which cloud service model to deploy. Whether you are looking for cloud-based software for storage options, a smooth platform that allows you to design and develop customized applications, or complete control over your entire infrastructure without having to physically maintain it, there is a cloud service for you.
At Mind Katalyst we build custom apps across different platforms and emerging technology including iOS, Android, Augmented Reality, Blockchain, Artificial Intelligence/Machine Learning, Wearables and IoT. We help organizations and entrepreneurs build their minimal viable products (MVP’s), iterate to product-market fit, or scale big.
By bringing to the table top tech talent in iOS, Android and backend development, the Mind Katalyst team is motivated by a deep appreciation for building meaningful, relevant and functional apps.
For more on how Mind Katalyst can help you scale your technology products, contact us for more details.
Carnellia Ajasin CEO of Mind Katalyst, is passionate about inventing new technology products in the emerging technology space that are meaningful and relevant. She works with ambitious organizations and businesses on the strategic application of innovation, creativity and emerging technologies to create competitive advantage, transformative impact and growth in business and society.
Carnellia Ajasin an experienced innovator, product designer and developer.
Carnellia Ajasin an experienced innovator, product designer and developer.
About
Write
Help
Legal
Get the Medium app
"
https://hajak.se/how-to-get-a-headless-product-to-100m-arr-a1bdc1e90e67?source=search_post---------54,"How do you go from developer and founder of an API, IaaS or dev tool to $100m ARR? What are the biggest surprises and hurdles to get to market? What can you learn from Heroku, Github, Algolia, Tokbox, Layer, Greta.io, Timekit, and maybe even me?
The world is breaking apart. And, not in the way you think. I am talking about Infrastructure as a Service (IaaS) companies. Gone is the world of a massive monolith of self-written code hosted on your machines. Layer by layer is being peeled off from underneath. The peel continues, like a Brit after the first summer sun, and the opportunities to build startups are amazing.
How do you go from a product to a massive revenue IaaS company? Here are the five biggest discoveries you need to figure out and my three recommendations.
“At our series B, we got lots of questions about revenue. Previously we were advised to focus on getting massive amounts of active developers. Now we had to figure out something quickly.""
—Tomaž Štolfa, Layer
Very likely you are a developer if you build an IaaS and you want to talk to and help other developers. It is great to see people sign up for the service you are building and chatting with them. But sadly, these people are not the customers you want. Why?
You have to find who would get promoted for buying your service. Is it someone in marketing? Or a product manager? Or the CIO?
“Who would get promoted for buying Layer?”
— Tomaž Štolfa, Layer
What is that person’s business needs? Is your product essentially replacing the cost of one developer per year? Or, do you decrease load times and increase uptime and therefore increase revenue? Or can you improve analytics which allows the marketing team to be more data driven?
Companies are driven by profit, although we’re sometimes tricked into believing otherwise in the startup world. Therefore, you can only become important to them by improving that profit, meaning you either increase their revenue, or decrease their cost.
It sounds great to be a vital part of your client’s business. To be core, not context, as it is called in business tongue. If you are not then customers might not pay you much or even bother. Not a problem if the setup is self-service and costs are less than $50 per year. But if you want to charge $25k per year you need to show not only why they can trust you, but why they shouldn’t build it themselves.
“Inhouse competition was always the problem with the big corps.”
— Scott Chacon, GitHub
The best ways of staying relevant is doing one of two things:
“I wished that we earlier realized that we ought to interact with clients based on potential, rather than of need.”
— Janine Yoong, Tokbox
Be very close to customers and potential ones who you are core for even if they can handle things themselves. Some of your best clients might refuse you give you insight into how they are using your product. You still want to stay on top of their demands and make sure to know what internal projects they are benchmarking you against.
Almost every company I know have whales — a small number of clients who make up the majority of the revenue. The top 20% accounts constitute more than 60% of the MRR. Those who survived realized that the pricing plan shouldn’t be Free, $299, and a $699 enterprise plan. The highest clients will pay five-figure numbers or more per year.
“The one thing that stands out is that founders charge too little!”
— Chris Janz, Point9 Capital
It is often a heart breaking moment for developer founders to ask themselves “Is Enterprise a serious part of our revenue or are we a community play?”. You don’t feel equipped to handle these sales conversations. Luckily for you, there are good books on the subject.
Big deals might not be huge in the beginning. The strategy is often “land and expand.” Begin with a proof of concept, than an internal use case, move on to being one feature or region, and so on.
“We start by asking in what region they want to test the system. It becomes a smaller decision and gets them up and running quickly.”
— Anna Ottosson, Greta.io
But big clients will need handholding or even integrations done. You might need to build a professional service team even.
I have a simplified rule of thumb for different contract values:
A lot of tech founders just want to get their product out there and in the hands of other developers. First, you start with a free tier. Then you realize you need monetization and add paid tiers. Most headless companies can’t make $100m ARR if they don’t have very high priced tiers. But apart from that, what should you think about?
As long as you don’t have high costs for using your service or a very complex onboarding, I think you should try to have a free tier. If there are costs then you can have a free trial at least. What are the reasons for a free tier?
“When we went into YC we removed the Free tier. It didn’t really have any impact on sales. It was an overrated topic.”
— Gaëtan Gachet, Algolia
“As the first employee of Heroku, I spent 40% of my time as an evangelist and 60% on coding. If there was a Ruby event in Montreal with three attendees, I was sure to be there.”
— Blake Mizerany, Heroku
“I hate startups whose tech you can’t just try out, but have to talk to a sales rep first.”
— Scott Chacon, GitHub
“By providing a free version we remove the barrier to get started and explore our technology.”
— Anna Ottosson, Greta.io
So, we need the free tier, but then what about the rest? There are two main ways, dependent on how your product works. Either you have feature-based tiers, or you are paid by the meter — for storage, traffic, or some kind of volume. Whatever you do: have few tiers, make sure they are simple to understand, and costs are predictable.
Ideally, there are only two paid tiers; one which can be handled over the phone with setup meetings, and one which will need some professional services or substantial interaction.
“When we started living with our biggest client, life got a lot easier.”
— Jesper Klingenberg, Timekit
Put the “sophisticated” features in the more expensive tier, even if it already is there implemented from a product standpoint. Things like using the API on your own — even if that works out of the box it will demand a lot from your customer success team. Compliance, guaranteed uptime, more service, etc. are all in higher tiers.
A lot of time we make the higher tiers more flexible instead of learning what those use cases really are. Compensate your flexibility with customer intimacy as your product becomes more vague and harder to understand. Setup professional service teams or just work in close cooperation with the big clients.
“Product development was too much focused on flexibility, instead of monetization. Biz dev people should be stakeholders in feature planning.”
— Janine Yoong, Tokbox
With feature-based pricing, you will leave money on the table. Some of the biggest companies in the world will settle for a lower tier and not the “Call us” one. You know that you could increase the price for them. But resist the temptation to add tiers in between to solve this.
Volume feels so easy — if you use it more, you pay more. But with volume based pricing comes other problems. The first one is that you take the risk and cost of the onboarding without knowing how much you will make.
The second is that some of the customers that get the most value, might not have the biggest volume. If you are a messaging service, you drive revenue for an e-commerce business but are not a revenue feature for a dating service. Another problem is that ambitious people believe their company to be the next Google so when they look at what your service will cost in two years, it is worth building it on their own.
Both are usually best solved with platform fees, because people don’t like minimum bundles of usage if they don’t understand how much they will use.
“When we introduced minimum bundles, people pushed back saying ‘what if we don’t use all that traffic?’”
— Tomaž Štolfa, Layer
The platform fee covers the onboarding and filters out the false positives of people who later don’t want to pay anyway. In the fee, there is some volume, but then you charge that by the meter. You can have a unique platform fee for every tier and will look just like any SaaS pricing page.
Secondly, you want to show how increased usage increases the benefits. Ask yourself “What can I build into my product to incentivize the real clients to use it more?”.
When you are building infrastructure, API services, or anything “headless” you need to figure out how people can see and relate to what you can do. As Clayton Christensen says “People don’t buy drills, they buy holes.”
There are a couple of ways to become tangible — get credibility, data, or testimonials:
“We built PoCs for a lot of potential five-figure deals, even when they hadn’t asked for it.”
— Gaëtan Gachet, Algolia
At the end of the day: Do not underestimate how hard it is to understand what can be done with your amazing technology. You are not spending a tenth of what you should be on communicating. What are you solving and for whom? Educate them on the problem.
If you are building a company in the space, I’m of course happy to chat!
Thanks, everyone who talked to me and helped me figure this out: Jesper Klingenberg, Anna Ottosson, Tomaž Štolfa, Scott Chacon, Janine Yoong, Gaëtan Gachet, Christoph Janz, and Blake Mizerany.
Thoughts about the process of taking an idea and making it…
200 
2
Thanks to Anna Ottosson, Jillian Schwiep, Erik Byrenius, and Tomaž Štolfa. 
200 claps
200 
2
Written by
Vegetarian, stoic, founder & investor. Father of three. Malmö/Sweden. Twitter @hajak.
Thoughts about the process of taking an idea and making it reality for a lot of customers in an economically sustainable way.
Written by
Vegetarian, stoic, founder & investor. Father of three. Malmö/Sweden. Twitter @hajak.
Thoughts about the process of taking an idea and making it reality for a lot of customers in an economically sustainable way.
"
https://aws.plainenglish.io/why-i-chose-not-to-go-with-aws-for-my-saas-product-b13fb0454859?source=search_post---------55,"Developers from almost every field, be it web development or machine learning, have heard and possibly even used Amazon Web Services(AWS) at least once.
AWS provides on-demand cloud computing and various APIs and even offers a pay-as-you-go billing method. From authentication…
"
https://medium.com/@jaychapel/saas-vs-paas-vs-iaas-where-the-market-is-going-fcc46771731d?source=search_post---------11,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 24, 2019·4 min read
SaaS, PaaS, IaaS — these are the three essential models of cloud services to compare, otherwise known as Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Each of these has its own benefits, and it’s good to understand why providers offer these different models and what implications they have for the market. While SaaS, PaaS, and IaaS are different, they are not competitive — most software-focused companies use some form of all three. Let’s take a look at these main categories, and because I like to understand things by company name, I’ll include a few of the more common SaaS, PaaS, and IaaS providers in market today.
Software as a Service, also known as cloud application services, represents the most commonly utilized option for businesses in the cloud market. SaaS utilizes the internet to deliver applications, which are managed by a third-party vendor, to its users. A majority of SaaS applications are run directly through the web browser, and do not require any downloads or installations on the client side.
Prominent providers: Salesforce, ServiceNow, Google Apps, Dropbox and Slack (and ParkMyCloud, of course).
Cloud platform services, or Platform as a Service (PaaS), provide cloud components to certain software while being used mainly for applications. PaaS delivers a framework for developers that they can build upon and use to create customized applications. All servers, storage, and networking can be managed by the enterprise or a third-party provider while the developers can maintain management of the applications.
Prominent providers and offerings: AWS Elastic Beanstalk, RedHat Openshift, IBM Bluemix, Windows Azure, and VMware Pivotal CF.
Cloud infrastructure services, known as Infrastructure as a Service (IaaS), are made of highly scalable and automated compute resources. IaaS is fully self-service for accessing and monitoring things like compute, storage, networking, and other infrastructure related services, and it allows businesses to purchase resources on-demand and as-needed instead of having to buy hardware outright.
Prominent Providers: Amazon Web Services (AWS), Microsoft Azure (Azure), Google Cloud Platform (GCP), and IBM Cloud.
SaaS, PaaS and IaaS are all under the umbrella of cloud computing (building, creating, and storing data over the cloud). Think about them in terms of out-of-the-box functionality and building from the bottom up.
IaaS helps build the infrastructure of a cloud-based technology. PaaS helps developers build custom apps via an API that can be delivered over the cloud. And SaaS is cloud-based software companies can sell and use.
Think of IaaS as the foundation of building a cloud-based service — whether that’s content, software, or the website to sell a physical product, PaaS as the platform on which developers can build apps without having to host them, and SaaS as the software you can buy or sell to help enterprises (or others) get stuff done.
The SaaS market is by far the largest market, according to a Gartner study that reported that enterprises spent $182B+ on cloud services, with SaaS services making up 43% of that spend.
While SaaS is currently the largest cloud service in terms of spend, IaaS is currently projected to be the fastest growing market with a CAGR of 20% plus over the next 3 to 4 years. This bodes very well for the “big three” providers, AWS, Azure and GCP.
What’s interesting is that many pundits argue that PaaS is the future, along with FaaS, DaaS and every other X-as-a-service. However, the data shows otherwise. As evidenced by the reports from Gartner above, IaaS has a larger market share and is growing the fastest.
First of all, this is because IaaS offers all the important benefits of using the cloud such as scalability, flexibility, location independence and potentially lower costs. In comparison with PaaS and SaaS, the biggest strength of IaaS is the flexibility and customization it offers. The leading cloud computing vendors offer a wide range of different infrastructure options, allowing customers to pick the performance characteristics that most closely match their needs.
In addition, IaaS is the least likely of the three cloud delivery models to result in vendor lock-in. With SaaS and PaaS, it can be difficult to migrate to another option or simply stop using a service once it’s baked into your operations. IaaS also charges customers only for the resources they actually use, which can result in cost reductions if used strategically. While much of the growth is from existing customers, it’s also because more organizations are using IaaS across more functions than either of the other models of cloud services.
Originally published at www.parkmycloud.com on May 21, 2019.
CEO of ParkMyCloud
28 
2
28 claps
28 
2
CEO of ParkMyCloud
"
https://medium.com/moddix/bescherming-persoongegevens-bij-iaas-e2b8955b135f?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
Tegenwoordig gebruiken veel organisaties SaaS of IaaS diensten. Wat in mijn beleving regelmatig wordt vergeten is dat hier vaak persoonsgegevens mee gemoeid zijn. Deze persoonsgegevens worden in Nederland door de wet bescherming persoonsgegevens (WBP) beschermd. Dat maakt dat er restricties verbonden zijn aan het gebruik van deze gegevens.
Voor een van onze recente projecten heb ik uitgezocht hoe de vereisten van de WBP zich tot IaaS providers als Amazon Web Services (AWS), Google Cloud & Microsoft Azure verhouden. De wet vereist namelijk dat je een overeenkomst over deze gegevens sluit met de organisatie waar je de gegevens opslaat. Dit kan als snel complex worden als je zaken doet met een IaaS provider. Daarbij moet je denken aan partijen als AWS, Google Cloud & Azure. Zelfs wanneer je gebruik maakt van een tussenpersoon, is jouw organisatie, verantwoordelijk voor de afspraken tussen je tussenpersoon en de IaaS providers.
PersoonsgegevensIn de praktijk komt het er vervolgens op neer dat je nagenoeg altijd persoonsgegevens verwerkt. Dit komt omdat de opvatting van verwerken ruim is (opslaan is ook verwerken). Bovendien is de definitie van persoonsgegeven ook veelomvattend. Ieder gegeven dat betrekking heeft op een persoon kan een persoonsgegeven zijn. Daar komt nog eens bovenop dat de definitie als het ware meegroeit met de tijd. Iets wat nu nog geen persoonsgegeven is omdat het teveel tijd of geld kost om iemand aan de hand daarvan te identificeren kan wanneer dit wel eenvoudig mogelijk wordt toch binnen de definitie van persoonsgegeven vallen.
VereistenDe vereisten die gesteld worden aan het gebruik van persoonsgegevens zijn eigenlijk simpel, je moet als verwerker (lees: gebruiker) van persoonsgegevens, zorgvuldig met gegevens om te gaan. Als het om webapplicaties gaat komt dat in de praktijk vaak neer op de verplichting tot het maken van afspraken met je leverancier. An sich kan dit redelijk eenvoudig, er zijn op het internet diverse bewerkersovereenkomst voorbeelden en -generators te vinden.
Wanneer je echter gebruik maakt van IaaS providers is het complex. Het is immers niet eenvoudig om een directe overeenkomst met bijvoorbeeld AWS te sluiten. Toch is dit vaak wettelijk verplicht, en aangezien veel organizaties zaken doen met vergelijkbare providers levert dat de vraag op hoe je dat aan moet pakken.
OplossingGelukkig is er een redelijk eenvoudige oplossing. AWS, Google Cloud & Azure hebben met de Europese Unie (Article 29 WP) een akkoord gesloten. Daarbij worden, een aantal standaard clausules met betrekking tot persoonsgegevens, in het contract tussen uw organisatie en de IaaS provider opgenomen. Dit gebeurt echter niet standaard, bij AWS moet je dit bijvoorbeeld nadrukkelijk aanvragen.
Omdat de WBP een nationale implementatie van de databeschermingsrichtlijn (95/46/EG) is. Voldoe je, door het opnemen van deze, door de EU goedgekeurde bepalingen, aan het in Nederland geldende vereiste van de bewerkersovereenkomst.
Let wel op, je hebt dan wel een bewerkersovereenkomst, maar hier moet je natuurlijk wel op een goede manier invulling aan geven.
Moddix
2 
2 claps
2 
Moddix
Written by
Delivering business value with emerging technologies
Moddix
"
https://medium.com/@alibaba-cloud/emerging-technologies-in-the-post-iaas-age-the-computing-conference-2018-686e6368d73f?source=search_post---------33,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 11, 2018·7 min read
By Wei Tong
At the Cloud Native Technology Summit of The Computing Conference 2018, experts from various industries shared their thoughts on emerging trends such as Internet of Things (IoT), containers, and serverless architecture. According to the speakers, the real challenge does not lie within the technology itself — it’s the challenge of unifying all technologies for a sustainable ecosystem. Despite their differences, all the experts agreed on and emphasized the importance of bridging developer communities, roles, and technologies for maintaining a cloud native software ecosystem.
First to present was Dan Kohn, Executive Director of Cloud Native Computing Foundation, who talked about the importance of Continuous Integration (CI) in software development. He illustrated this with the well-known SQLite relational database management system (RDBMS). Despite being one of the most widely used and tested RDBMS, SQLite still has many bugs when applied to production scenarios. In fact, the American Fuzzy Lop (AFL) Fuzzer was able to find 22 SQLite bugs in just 30 minutes. This problem is not limited RDBMS, it affects the entire computing ecosystem. If we look at the ecosystem supported by Linux Foundation, we see vulnerabilities on every layer, including both software and hardware. Even with only 40,000 Source Line of Codes (SLOCs), the Linux Foundation is not immune to bugs and security vulnerabilities
The answer to this problem is Continuous Integration (CI). Normally, software bugs are fixed with patches, but a software patch does not help until we have deployed it into production. In CI, you need to test your apps every time you make a change. By adopting CI, you can have the confidence that a deployment will not break your software. Furthermore, with the power of open source, we have the ability to leverage thousands of other developers that are finding bugs and making fixes to the software that we depend on.
For companies that don’t know where to start, Dan provided a simple yet useful piece of advice,
“What kind of test should you run? All of it.”
In 2018, the worldwide spending for digital transformation by large enterprises is expected to hit $1.1 trillion USD. The technologist’s world has gotten really big, and cloud is undoubtedly changing the way enterprises and customer interact with each other. Developers, engineers, and architects face something new every day, be it a new JavaScript framework or a new platform for development. For technologists, being an expert in a single language is no longer the most important skill. Chip Childers, CTO of Cloud Foundry Foundation, called this skill “cloud cobbling”, putting different technologies together for a meaningful solution.
Chip also discussed about the need for bridging different roles for enterprises to successfully adopt a cloud native ecosystem. Despite many enterprises already adopting DevOps and containers, a large majority are yet to adopt CI/CD practices, especially CD (Continuous Deployment). This is because enterprises tend to overlook innovation in favor of relevance. For enterprises, relevant technologies of today such as VMs and big data are more profitable compared with innovative technologies of tomorrow like artificial intelligence (AI) and Internet of Things (IoT).
Continuing the theme of unification, Tao Ma, Principle Engineer of Alibaba Cloud, introduced the concept of “Serverless + Container”. According to Tao, by combining these two technologies, we can eliminate the boundary between software developers and operators. Thanks to serverless, users can use containers to seamlessly deploy and migrate their services across on-premises environments and public clouds, without having to worry about the underlying infrastructure.
Tao then talked about building building a simple Kubernetes cluster on Alibaba Cloud ECS. Even though the initial set up is straightforward, upgrading and maintaining it according to business needs can be challenging. For customers, there should be a way for them to quickly deploy apps without caring about these problems. Alibaba Cloud’s answer to this problem — Elastic Container Instances (ECI).
Wei Zhang, Staff Engineer of Alibaba Cloud, described the features of the newly launched ECI. With ECI, Alibaba Cloud can deliver serverless computing resources to users via the concept of “container groups”, unlike previously monolithic containers. Alibaba Cloud ECI lets you run containers without server management, flexibly consume resources based on demand, and secure applications with sandboxed container runtime.
Jason Wu, Principle Engineer of Alibaba Cloud, reiterated the need for serverless computing and introduced Alibaba Cloud’s Serverless Platform, which includes products such as Function Compute, Table Store, and Object Storage Service (OSS). However, for Jason, the real challenge is not only about serverless but also to implement a full Function Platform as a Service (FPaaS) on Alibaba Cloud. In fact, 74% of Alibaba Cloud products and services are already serverless. Jason went on by explaining three FPaaS use cases with Alibaba Cloud Function Compute.
Building backend of a web app normally involves buying VMs and designing a load balancer. But with serverless, you just need to use HTTP triggers on API Gateway. Each demand will trigger a function, and within each function you can define the logic to which can point you towards accessing OSS, RDS, or other services. There is no longer the need to manage VMs and dynamic scaling is automated. You don’t have to worry about scaling and traffic control, scaling is performed by function.
Data logging is very important for enterprises not only for finding bugs but also to analyze your business. For example, if you want to analyze the click rate for your website, you can use Log Service. As a serverless service, it collects your logs automatically as well as helps you analyze, archive, and send data logs to other services.
Have you ever uploaded and edited a file through your mobile phone? If you have, chances are, your files went through a process called content transform. When you send a file on a messaging app, such as sending a PowerPoint presentation through DingTalk, the content is first uploaded to OSS. Then, if you edit the file, an event is triggered to perform the change. This is all done through Alibaba Cloud’s Intelligent Media Management (IMM) service. From a user perspective, you only need to access the file from your app. You don’t have to think about the storage service, platform, and conversion processes separately.
In the final session, Xiaoliang Yang, Chief Architect of Siemens IoT Cloud Platform, talked about developing an Internet of Things (IoT) system for industrial applications. In most cases, IIoT is similar to commercial IoT. The biggest difference lies with the scale of IIoT. IIoT not only involves a multitude of sensors but also a large number of stakeholders, all with different business and technological requirements.
That is why for IIoT, cooperation between all stakeholders is the key. That is why Siemens and Alibaba Cloud officially announced a partnership at The Computing Conference 2018 that defines the technological roadmap and implementation plan for MindSphere’s launch in Chinese mainland. MindSphere connects real things to the digital world, and provides powerful industry applications and digital services to help drive business success. Combining Siemens’ PaaS capabilities with Alibaba Cloud’s IaaS services, MindSphere on Alibaba Cloud will provide a comprehensive platform that offers connectivity, data analytics and industrial knowhow. This in turn will enable enterprises in China to realize novel and data-driven business models.
Reference:https://www.alibabacloud.com/blog/emerging-technologies-in-the-post-iaas-age-%E2%80%93-the-computing-conference-2018_594037?spm=a2c41.12103360.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@caiomsouza/on-premises-iaas-paas-saas-what-do-you-want-to-manage-3c9f3f03f1a?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Caio Moreno
Oct 19, 2018·1 min read
Use the image below to help you define your data strategy.
Every choice has PROS and CONS.
A simple question to help you define your strategy could be:
What do you want to manage?
Senior Cloud Solution Architect and Data Scientist @microsoft | PhD Student @unicomplutense (Opinions are my own)
Senior Cloud Solution Architect and Data Scientist @microsoft | PhD Student @unicomplutense (Opinions are my own)
"
https://medium.com/strongnode/ama-with-moonhunters-edge-computing-and-node-categories-in-the-strongnode-io-ecosystem-e5b1baa34c34?source=search_post---------329,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito and VP of Community Operations Brandon Sivret joined the Moonhunters AMA last 20 September 2021. The two executives replied to questions regarding our company, products, and shared facts about the $SNE token public sale and upcoming IDO launch.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Catch some of the highlights from the StrongNode and Moonhunters AMA session below:
KAELIN | TGH | MSH: To start off this AMA, let’s give the audience a brief description of what STRONGNODE is ?
Daniel Saito: StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle computers like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. StrongNode was the name picked out by our amazing marketing team.
The mission and vision of Strongnode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies.
We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources.
How will STRONGNODE help adoption of blockchain in cloud computing?
Daniel Saito: It won’t. Our solution is the antithesis of cloud computing. We are taking it to the EDGE.
Brandon S: StrongNode runs as an application or embedded in future games to help those who aren’t familiar with blockchain to easily contribute.
Daniel Saito: All users are on the EDGE, more than ever. Since COVID we spend more of our days inside our house. With that, we purchased a lot of digital devices to keep us preoccupied. But most of those devices go unused. We want to change that.
KAELIN | TGH | MSH: Interesting points embedded in games sounds like it will boost adoption fast.
Daniel Saito: The users are at the EDGE already, why can’t the users interact between themselves.. So as Brandon was saying, games.. I can see game services hosted at the edge.. So you can relive the local LAN party experience… in a secure low latency manner.
Brandon S: But that EDGE goes all the way into your living room, so we bring everything as close to you as possible including neighborhood LAN parties.
Let’s go into the Tokenomics of the native Token and what are some of its cases ? As well as how someone would obtain the Token ?
Brandon S: The $SNE token has several different features and values in our ecosystem that have been pulled together from years of experience from our team and our advisors. More news on this will come out as we get closer to launch, but we are incentivizing token holders to hold along with us. SEED and PRIVATE INVESTORS are locked up for 6 months.
All employees and advisors of StrongNode are locked up for 4 years.
The StrongNode token has so many use cases within the StrongNode ecosystem which includes our innovation lab projects focused on gaming, social impact, entertainment, lifestyle platforms building on the StrongNode Edge technology. You get paid for your idle resources. And others get to run workloads, processed in chunks, on your idle resources. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. There are more mechanics yet to be announced that will enhance the appeal of token adoption.
StrongNode has three Categories of Nodes:-
1- Node Enterprises
2- Node Innovators
3- Node Seeders
How & what roles do these Nodes play in the ecosystem?
Brandon S: Enterprise Nodes are those companies that have large compute needs or access to vast excess resources they can leverage within our ecosystem. Innovation Nodes are our projects like OGLife that act as a bridge between acquiring new users and working with our partners to identify additional rewards or create new economies for you to use your tokens. Node Seeders are the users who provide excess resources for use by Enterprise or other seeders for the LAN party examples.
Will it be a smooth experience for users on platforms who are not so experienced in the Crypto space?
Daniel Saito: We just onboarded a great team to oversee creative and UX user flows.. We are redefining UX / UI issues for crypto..
You said that You track the activity by NFT instead of the user’s Personal Data. Could You help me understand more about it?
Daniel Saito: Yes the NFT is just a unique identifier as you. With this NFT you can enter the StrongNode network. Why? because we don’t allow random connections that could potentially do something bad. So NFT is basically for User Registration & Authentication.
For more information, visit our website: https://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
116 
116 
116 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@daffodilsw/saas-vs-paas-vs-iaas-overview-and-comparison-1cf32be3939c?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daffodil Software
Aug 27, 2019·4 min read
In 2019, the public cloud services market was expected to reach around 214.3 billion U.S. dollars in size and by 2022 market revenue was forecast to exceed 331.2 billion U.S. dollars. | Statista
Public cloud services are gaining a lot of traction. Startups, SMEs, and enterprises are recognizing the benefits of cloud and thus, are migrating to the most relevant service provider to improve ROI, efficiency, and time-to-market.
When moving to cloud, it is important to understand the different types of services that can be availed by an organization. Software-as-a-Service (SaaS), Platform-as-a-Service (PaaS), and Infrastructure-as-a-Service (IaaS) are three popular models of cloud services. The later segment discusses what they are, their benefits, differences, and how to choose the right according to business requirements.
Software-as-a-Service (SaaS)
SaaS, also known as “on-demand software” is a software distribution model wherein a service provider hosts an application at a data center for customers to be accessed via the internet. Such a service frees up the customers from maintaining hardware or other resources to use the software. All that’s needed is a web browser or a client program.
The source code of the software is the same for all the customers and any change/updation in the software is rolled out to all the subscribers of the software. Organizations also have the option to integrate SaaS solutions with their own applications using APIs. For example, a business can create its own software and integrate the functionality of a SaaS solution through APIs.
Human capital management (HCM) software, collaboration software, and customer relationship management (CRM) software are amongst applications where SaaS has a high penetration rate. | Statista
Examples: Salesforce, Hubspot, MailChimp, Shopify, Slack
Benefits:
Platform-as-a-Service (PaaS)
In this model of cloud computing, hardware and software tools are provided, primarily for application development. In this case, the cloud service provider hosts the hardware and software its own infrastructure and make it available for the users over the internet. This not only frees the organization from investing in hardware and software to run a new application (operating system, web servers, databases, and access to a programming language(s) execution environment, etc.). Along with this, PaaS products enable the development team to collaborate and work together, irrespective of their physical location.
By 2019, the platform as a service market is estimated to have a worth of 19 billion U.S. dollars. | Statista
Examples: Windows Azure, Google App Engine, AWS Elastic Beanstalk
Benefits:
Infrastructure-as-a-Service (IaaS)
In this model of cloud computing, a cloud service provider hosts infrastructure components on cloud, which are usually hosted on-premise. These components may include but are not limited to servers, storage, and networking hardware. While platforms like AWS, Google Cloud are examples of public clouds, organizations can set up their own infrastructure on a private cloud.
By 2019, the infrastructure as a service (IaaS) market is expected to have a worth of 38.9 billion U.S. dollars. | Statista
Examples: Google Compute Engine, Rackspace, Amazon Web Services
Benefits:
ALSO READ: SaaS VS PaaS VS IaaSThe Ultimate Guide to Infrastructure Optimization on Cloud
Every cloud model has specific features and functionality to offer that can respond to unique business requirements. Whether a software to manage routine tasks, a platform with storage options, or a fully-fledged infrastructure, there is a cloud service for almost everything. For availing the benefits that these cloud models offer, businesses can adopt a single or multi-cloud strategy considering cost, efficiency, and ROI of services.
Originally published at https://insights.daffodilsw.com.
We build Mobile, IOT, & Web solutions that are intuitive, reactive and agile | www.daffodilsw.com
We build Mobile, IOT, & Web solutions that are intuitive, reactive and agile | www.daffodilsw.com
"
https://medium.com/cloud-mobile-the-products-around/your-cloud-platform-iaas-paas-and-the-providers-c92c68030948?source=search_post---------231,"There are currently no responses for this story.
Be the first to respond.
Continuing from my earlier article, cloud and wine… We discussed the IaaS and PaaS.
Let us see how do we choose our platform ? Should I go the IaaS or PaaS way ?
You need to give Amazon Web Services (AWS), it credit for pioneering the space, and by most estimates they are ahead of all the other players by a huge margin. This doesn’t mean there is no other alternative in the town. Google Cloud platform, Microsoft’s Windows Azure and other prominent alternatives.
Between IaaS and PaaS, the tradeoff is flexibility vs ease of use. IaaS gives you the hardware (on demand), you use it the way you want — Very flexible, but you need to handle a lot of stuff on your own. PaaS gives you a software layer (hardware is mostly abstracted) and you get going — you need to use it the way the vendor is offering the platform — less flexibility, but if your need fits what they offer — you found the right place.
AWS started as an IaaS provider with S3 (storage) and EC2 (compute). Google Cloud started off as PaaS with GAE (Google App Engine). Heroku came up with a very easy to use PaaS platform for building web applications — might not support all needs, but many applications just fit well! To answer these challenges from Google and Heroku, AWS made a service called Elastic Beanstalk — EC2, Database, load balancer etc all wrapped together nicely as a service. They now continue to invest and innovate in this “easy to use” segment. Google on the other hand made GCE (Google Computer Engine) on the IaaS side to compete with AWS. (I don’t yet have much insight on Microsoft Azure yet — will write later on this)
AWS has undoubtedly the most feature rich platform — beginner advantage and they keep investing in this with great focus. They address the ease of use problem with newer “packaged” stack services like Beanstalk and Opsworks.
You need to choose the one which suits you the best. If you choose one provider and then find future unfulfilled needs, the story is not all over — Parts of your cloud can be run on other providers, if the need be. One great advantage Google cloud has is the ability to handle spike loads. On AWS, typically it takes few minutes to add another instance to the compute cluster.
If you are choosing a PaaS provider, you need to also look into whether they support your programming language and framework stack (as discussed here).
My personal choice is AWS — to me they give best of both worlds. I would start building with packaged services like Elastic Beanstalk and later move to more bare bones if the need be. Very likely you will not have to go to bare-bones (EC2) for quite a long time. Say, if I have a need to run a video codec built in C++ on AWS, I get that flexibility too — such things typically wont be possible with many “PaaS-only” providers — At the same time, also keep in mind that 70% of the applications wont have such special needs !
Also @ http://notingon.com/2014/02/07/your-cloud-platform-iaas-paas-and-the-providers/
Many beautiful and useful products happen in Cloud & Mobile.
Many beautiful and useful products happen in Cloud & Mobile. Know about how to build, what to look for and various thoughts around them.
Written by
Entrepreneur — Internet, Mobile products, @skadavan
Many beautiful and useful products happen in Cloud & Mobile. Know about how to build, what to look for and various thoughts around them.
"
https://medium.com/@krmarko/dell-emc-fall-into-classic-disruptive-technology-trap-pushing-further-up-market-7d113ad6ff98?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kurt Marko
Oct 16, 2015·2 min read
The Dell-EMC amalgam makes a lot of sense through the close-focus lens of short-term synergies, protecting established enterprise customers and horizontally filling out a legacy product portfolio, however strategically it looks to be more an act of reactive weakness, if not desperation. While the combined entity will be a powerhouse of traditional enterprise IT products, it does nothing to address the growing disruptive threat from public cloud services like AWS. Indeed, the deal is a typical example of established market leaders focusing on narrower and narrower slices of high-end customers as a disruptive technology erodes its entry-level and mainstream base.
The deal happens against a backdrop of a server market slowing down, where most of the growth is happening among second-tier vendors and ODMs, and sales declines for large, enterprise storage arrays even as overall storage capacity continues to skyrocket. Both of these can be traced to the increasing popularity of cloud services. Considered in this light, the Dell-EMC deal looks like a classic reaction to disruptive technology.
As I detail in the full column, the Dell-EMC combination smacks of two enervated companies combining to capture a greater share of a stagnating market and a classic case of horizontal integration. While I agree with the early consensus that this combination will be fruitful for both companies (ex-VMware) in the short-term, it doesn’t address the long-term threat to incumbent IT vendors from cloud services. Read on to understand why.
Originally published at www.forbes.com on October 15, 2015.
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
1 
1 
1 
Independent technology analyst and writer at MarkoInsights. Senior contributor to Diginomica, Forbes, TechTarget and Channel Partners.
"
https://aws.plainenglish.io/deploy-node-js-application-to-aws-beanstalk-with-ssl-and-load-balancer-e9d07efa1c98?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
Originally posted in blog.shams-nahid.com
AWS Beanstalk acts as an Infrastructure as a Service (IaaS). We will deploy a Node.js application to the AWS Beanstalk. Then we will ensure the HTTPS connection with Application Load Balancer. To make this work, we will make use of the following AWS services,
Our architecture will be the following:
To accomplish the goal, we will go with the following steps,
We have to make sure, we have
We will deploy the code to Beanstalk manually. So we do not need the AWS CLI or Elastic Beanstalk (eb) CLI.
First, get the code from GitHub,
git clone https://github.com/socketio/chat-example.git
Go to the code repository,
To install dependencies,
You may use npm instead of the yarn. In this case use npm i.
Run the app in local machine
yarn start
If you use npm, use npm start.
The server should be up and running on port 3000. Go to browser and open http://localhost:3000. This is a chat app and to verify functionalities, you may open these in two different browser windows and do messaging back and forth.
Before we go to aws console, we need to zip the source code for Elastic Beanstalk. It provides several options to deploy the code,
To make the source code zipped, we can go to the zip directory and do the zipping,
This will create a zip file named, chat-example.zip. You may manually zip the source files from the source folder.
Now go to https://aws.amazon.com and find the service Elastic Beanstalk. To create an application in Elastic Beanstalk we have to go through a couple of following steps,
This might take several minutes. When the application is created, we will find the application URL at Go to Environment.
Test the app in two different browser windows.
Before we get a certificate, we have to ensure, we have a Hosted Zone in Route 53 with a domain.
In my case, I already have a domain shams-nahid.com. I will put this application in chat-example.shams-nahid.com. So I will get the certificate for chat-example.shams-nahid.com.
So first, go the the Certificate Manager service. Request a public certificate and put the desired domain. In my case, it is chat-example.shams-nahid.com. Since my hosted zone is in AWS, DNS Validation will be faster. Confirm the request and add the DNS Resolution to the Route 53.
Click Complete and wait for validation to be completed. It may take 5-30 minutes.
Now we have a running application in Elastic Beanstalk and also have a SSL certificate we can use.
We left two steps to use SSL in the Beanstalk have,
To use HTTPS connection, we must make use of the Load Balancer. The Load Balancer will have a secure connection and it will route the traffic to Elastic Beanstalk.
In the Elastic Beanstalk select Environment -> Configuration -> Load Balancer settings.
Now add a listener with the following config,
Apply and wait for the application to be updated.
Go to Elastic Beanstalk configuration for capacity and make sure the Environment Type is Load Balanced.
Go to the Route 53 service hosted zone and add create a record,
Now hit Create Record button and we are all set to go.
Go to the URL https://chat-example.shams-nahid.com and we should see the application is up and running.
You might check out the videos on deploying the app and secure connection.
Please feel free to comment on any issues you may be facing. I will assist accordingly.
More content at plainenglish.io
New AWS and Cloud content every day.
117 
117 claps
117 
Written by
A lifelong learner. Love to travel, listen to music, and hang out with friends. I believe in simplicity and peace.
New AWS and Cloud content every day.
Written by
A lifelong learner. Love to travel, listen to music, and hang out with friends. I believe in simplicity and peace.
New AWS and Cloud content every day.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.neap.co/the-serverless-series-what-is-serverless-d651fbacf3f4?source=search_post---------296,"Is Serverless the same as BaaS or FaaS? What’s the difference with SaaS? How did we get there and where are we going next? How can it benefit you? In this post, we will attempt to answer all those questions briefly.
Other topics that will be discussed later in this series will cover subjects such as:
Most of the clients of our tech consultancy in Sydney often ask us to clarify what Cloud technologies we will use to engineer their product. The less technical ones are usually confused between the Cloud and SaaS, whereas more tech-savvy founders begin asking about the potential for Serverless architecture but are still confused between FaaS, BaaS, and PaaS. All those labels have started to confuse more than they help. In short:
Now that we’ve disambiguated those terms, we can dive deeper into what Serverless does, as well as the trend that led us here and that will most probably lead us to the next Cloud milestone; whatever it is.
In How The Cloud Is Automating IT Engineers & Reshaping Tech Leadership, we briefly covered the evolution of the Cloud up until now. We concluded that:
The Cloud reduced considerably the number of IT engineers required to operate a digital business, consequently reshaping the technical leadership inside organizations.
The oversimplified example of an e-commerce website illustrated the evolution of the business’ taxonomy before and after the Cloud:
Serverless is the most recent Cloud iteration; another architecture that enables fewer software engineers to accomplish more and spend less time on operational maintenance.
Serverless products will host some or all of your application logic. It can do the same with your data. It is different from a SaaS (Software as a Service) which does not host your intellectual property. Instead, a SaaS provides an essential service (e.g., sending emails) part of your proprietary solution that you would have to build if it did not exist.
Differentiating between Serverless and SaaS is easy enough, but why is Serverless also different from the Cloud as we know it, i.e., PaaS (Platform as a Service)? Well, it is not. Serverless is just a new extension of PaaS, and that’s why, from one article to another, the same product is labelled Serverless or PaaS (e.g., Google Cloud PubSub). Both of those terminologies are correct. Those who argue differently are not paying attention to the elephant in the room: The Cloud is automating IT engineers. Serverless is just moving one step further.
Serverless marks a new milestone in the Cloud’s journey: The automation of the DevOps engineer.
So if you’re still wondering what a Serverless product is, our rule of thumb is to ask yourself this question:
Do you need a dedicated engineer (DevOps)to manage your infrastructure so that your product can scale and failover while being secured and highly available?
If the answer is no because your software engineer can merely push his/her code to a product that delivers all those features out-of-the-box, then those products can be called Serverless. This definition is loose but clear enough so we can focus on what matters: Getting shit done!
To recap, using our rule of thumb:
Before looking into the use cases where Serverless are useful or not, let’s make one last clarification between the two different Serverless categories: FaaS & BaaS.
FaaS are small pieces of your app’s business logic powered by services such as:
These functions are usually small pieces of code that react to predetermined events (e.g., a file change in your Cloud storage, a message in your queue, or an HTTP request). An example would be sending a welcome email when a new user is inserted into your Firebase database. If you want to read more about what’s possible with FaaS, Leveraging AWS Lambda for Image Compression at Scale is a great article.
If you’re still not entirely sure to understand what separates BaaS from FaaS (e.g., what is different from Zeit Now and Google Functions) yet, the next section will dig a bit deeper.
BaaS are pieces of infrastructure that will host your app or its data with almost no configuration required. That type of backend should theoretically scale infinitely while still being highly-available and fault-tolerant. Some popular examples are:
Databases:
Hosting:
Data ingestion:
This definition of BaaS is broader than what it used to be. Indeed, in the beginning, the industry was limiting itself to what was known as MBaaS, i.e., Mobile Backend as a Service. MBaaS was more restrictive than the above description. To be labelled as MBaaS, a product had to offer an API that covered push notifications, user authentication, storage, and, ideally, SDKs. Those specifications would have removed most products from the list above, except for Firebase. So what’s changed? The rise in popularity of FaaS proved that there was a growing demand for more Cloud services with almost no infrastructure configuration and maintenance (the term that often references that trend is NoOps). AWS recently released Aurora Serverless, a Serverless SQL database with no push notifications, user authentication or SDKs out-of-the-box. With this new product, AWS is enlarging the BaaS and Serverless brand, which changed the previous specification.
In most cases, deciding between a BaaS or a FaaS is quite straightforward. Use:
However, when it comes to hosting an app such as a website or a web API, what separates BaaS from FaaS becomes blurry. Indeed, in that case, both a BaaS solution like Zeit Now or a FaaS solution like Google Cloud Functions could do the job. Our next article about Serverless Mistakes discusses in greater details why we think a BaaS like Zeit Now is better suited for those types of scenario, and what we’ve learned from our mistake when we hosted a GraphQl API on Google Cloud Functions.
The list below describes various scenarios for which some Serverless solutions might be relevant.
Javascript is the de facto language for most Serverless solutions. It is by far the easiest way to start (as of early 2018, Google Cloud Functions only support Javascript). Some solutions like Zeit Now were initially tailored for node.js. Check out our tutorial in Why We Love Zeit Now & When To Use It Over FaaS, where we explain how to write a simple REST API and deploy it to Zeit Now, AWS Lambda and Google Cloud Functions. As for Serverless databases like Firebase or Serverless data ingestion tools like Google PubSub, they usually provide SDKs that help with most popular languages.
If you’re not a JS dev, like we said before, you can still leverage most Serverless solutions as they usually provide SDKs for the most popular languages. It is, however, a bit more restrictive as specific solutions do not support other languages other than Javascript at this stage (e.g., Google Functions, Fly Edge Apps, Cloudflare Workers). However, some others support Docker containers (e.g., Zeit Now), so if you’re comfortable with containers, this gives you an excellent alternative.
If you’re bootstrapping a startup or wanting to kick off an idea you’ve had in the back of your mind for a while, then a Serverless stack is the fastest way to get moving. Not to mention almost all solutions offer a usually generous free tier that will allow you to get pretty far without paying a cent. Right now, we don’t think there is any better stack to power the Lean Startup’s Build-Measure-Learn mantra.
This topic excites us a lot. If you’re interested in knowing more, a dedicated post is coming soon. In the meantime, you can read the excellent article The challenges of developing event-driven serverless systems at massive scale.
Refactoring a legacy project is rarely a dream story. In our experience, more often than not, the main challenges come from understanding and adapting the business logic rather than re-writing the code. In this situation, having a stack allowing for fast, cheap and almost infinite deployments at your disposal can help you focus on sorting out the business logic.
Serverless shares all the usual grievances associated with other Cloud technologies. You are giving up control of your infrastructure to one or multiple 3rd parties. Depending on the vendor, Serverless may or may not provide the right SLA and security levels for your business case. At this point, it depends on what your business’ priorities are. Serverless is still in its infancy. If some pieces of logic and data are highly sensitive or mission critical to your business, and if you have the budget, time and expertise to maintain those pieces through other more reliable means, then we recommend that you do so. However, the Serverless architecture is rapidly maturing. It is a matter of time until its reliability reaches your desiderata.
So far, our team has been delighted to integrate more and more Serverless solutions into our stack to build our latest web and mobile apps for our clients in Sydney. Is Serverless a revolution? No, but it is undoubtedly an extremely valuable evolution of the Cloud in general. Serverless is another incremental way the Cloud can help test ideas to get them to market faster and more efficiently. The gain in productivity for our team has been undeniable, and unlike other much-hyped techs these days, it has delivered measurable value; something we believe is essential for any great product or service.
If Serverless falls short of your expectations and more control over the underlying OS is required, PaaS is still a great alternative, especially with the latest advancements in containerization. Google Kubernetes Engine, for example, can significantly simplify your deployment to abstract the underlying infrastructure.
In any case, there are more an more tools and ways to create digital products than ever before. We are very curious to see what that next stage of the Cloud will be and if that one will finally start automating Software Engineers.
This post is part of our series about Serverless. Follow me on Medium — Nicolas Dao — if you’re interested in what’s coming next:
Big Dreams Have Little Beginnings — We Partner With…
404 
Thanks to Brendonicus. 
404 claps
404 
Written by
Focused on nurturing happiness in tech. and in life. Co-founder of Neap (https://neap.co), a Tech. Consultancy based in Sydney.
Big Dreams Have Little Beginnings — We Partner With Startups At The Most Critical Time: Their Inception.
Written by
Focused on nurturing happiness in tech. and in life. Co-founder of Neap (https://neap.co), a Tech. Consultancy based in Sydney.
Big Dreams Have Little Beginnings — We Partner With Startups At The Most Critical Time: Their Inception.
"
https://medium.com/@sujit-udhane/cheatsheet-cloud-agnostic-tools-21ec17ac6c23?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sujit Udhane
Mar 6, 2021·1 min read
Choice for PaaS against IaaS mainly depends on -
Sharing cheatsheet with you for cloud agnostic platform/solution.
Cloud- OpenShift AWS Azure GCP
Infrastructure as a Code- Terraform
Infrastructure Configuration- Ansible
Web Servers- Nginx Apache
Container- Docker
Container Management- Kubernetes
Service Discovery- Eureka Kong Istio Consul
API Gateway, Kong Zuul Spring Cloud
Secrete Management Key Vault- Vault Keycloak
Message Queue- Kafka ActiveMQ RabbitMQ
API Manager- WSO2
Relational Database- PostgreSQL MySQL
NoSQL — DocumetDB- MongoDB OrientDB
NoSQL — Key/Value DB- Redis Couche Ignite
NoSQL — Columnar DB- Cassandra HBase
NoSQL — GraphDB- Neo4J OrientDB
Search Database- ElasticSearch MongoDB Solr
Time Series Database- InfluxDB
Logs Collector- Logstash FluentD
Logs Aggregator- ElasticSearch
Logs Viewer + Log Query- GrayLogs Kibana
System Metrics Monitoring- Grafana Prometheus Mesos
System Housekeeping Jobs + Batch Scheduler- AirFlow
Microservices Monitoring- PM2
Client side logs Monitoring- Sentry
If you like the article, please clap for it. Also, share the article with your friends.
I am Chief Software Architect, working in Pune-India. I have 17+ years of experience in technology, and last 7+ years working as an Architect.
15 
15 claps
15 
I am Chief Software Architect, working in Pune-India. I have 17+ years of experience in technology, and last 7+ years working as an Architect.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@bruntonspall/security-concerns-with-platforms-and-services-in-the-cloud-50837b8fbba0?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Brunton-Spall
Jul 2, 2018·6 min read
When we talk about using SaaS or PaaS (or IaaS or even the new Serverless or FunctionAsAService, FaaS) it’s important that we understand that security concerns change.
Security concerns don’t change very much based on whether you are using a platform, or infrastructure or just services. They change based on the maturity of the solution and the maturity of the organisation you are using. There is some difference in where the shared responsibility line is drawn, but that’s not necessarily a primacy in security, as there are other concerns that matter.
To use an example, Amazon is a multi billion dollar company, and has a set of robust and audited security processes in place. Honest Bob’s Cloud is not, and probably does not. If we were to compare an AWS SaaS product vs Honest Bob’s IaaS offering, we wouldn’t naturally say that Honest Bob’s IaaS is more secure than the AWS SaaS just because it’s infrastructure and we have more control! To be honest, when we have more to control, sometimes we make security worse because there are many more ways to shoot our own foot.
The SaaS bucket is something into which we lump a variety of offering from a variety of vendors with massively different security implications. Does Office-365, a large productivity suite, have the same security implications as using a small service like MailChimp for our mailing list delivery? How about using Trello to track our cards? What about taking payments via Stripe?
Security assurance models haven’t kept up with this changing world. It used to be the case that when you outsourced to a vendor, it was a major deal, you could either get an integrator to manage it for you, or you did a lot of work to assess the security of the vendor. Security assurance is often still in this world, of assuming that you need to always ask questions about the physical location of the data center for every little service you manage.
With todays Azure marketplace or AWS marketplace, you can install a database as a service from a third party vendor with the click of a button, and the typical questions about “is the data encrypted”, “where are the admins located” etc are either too slow or not always appropriate for this new world
So what am I saying in essence?
If a service can be built on a market leading SaaS product, without customization or special relationships or need to maintain it, then we should use it where possible. Sending emails via Amazon SES, arranging video conferences via Appear.In, storing documents in Office 365 etc.
Where there is personal data involved, we need to be confident that the provider has robust security practices to protect that data. It is our liability if we select a bad provider, so we need to be more careful with those, but that confidence can come from someone with a data protection/security hat on looking at the service and agreeing “yes this is common, meets the NCSC’s SaaS Security Principles and we think it’s acceptable”.
The more sensitive the data, or the more of it, the stronger a look we need to take. So for something like Trello, which holds very little personal data, sure, we’ll check that they meet those principles above, and get on. For something hosting a database as a service, we might want a technical person working with a security person to give it a good looking at before we approve it, and we might want specific conditions on that, such as configuring it a certain way, or backing it up a specific way.
But at it’s essence, there is no fundamental security reason that you’d not use a SaaS if you’ve already decided that you are willing to outsource your IT to the cloud for anything else. And since you should be using the cloud as your outsourced infrastructure provider, I’d argue you should always be preferring a SaaS solution where it’s possible.
When you use *any* outsourced service provision that is multi-tenanted, you have the possibility of the law enforcement seizing/viewing/intercepting your data because they are looking for one of the other tenants. This is worse with smaller providers, or providers who don’t protect against this with good client data segregation.
I’ve never heard of it actually happening to a major service, and people like Amazon, Apple, Google, Microsoft and other major cloud providers tend to distribute content to devices in such a way that it’s mirrored (so the removal of a single server doesn’t reduce the availability), and additionally encrypt the contents so the possession of a single server won’t get them any data on it.
Secondly, even in the miniscule worst case, that the law enforcement have seized a server under a warrant because they believe that the information on it is relevant to a case, they are only authorised by the law to search the relevant parts of the machine, and they are not allowed to reveal contents that isn’t covered by the warrant. So we would not consider the device “lost” or the data “made public” in most cases. We might have to report to the authorities that the data is no longer under our control, but it is under the control of somebody who is responsible to the same authorities.
I’ve seen people suggesting that running your own PaaS on your own network keeps it safe, because it means that the data stays on the corporate network
The concept of networks being the “security boundary” is no longer the right model. When we are talking about data living within the cloud environment, the networking is all virtual, and considering it an extension of the core corporate network simply adds a lot of additional risks to the core network, and reduces all of the security features of the cloud networking stacks that we could use in future. This Armadillo or Castle metaphor for networks hasn’t been appropriate since network stacks got significantly more capable.
I recommend building separate networks for as small a “zone” as is possible, and use “identity ” to create appropriately encrypted connections between the services as they need to talk to one another. This concept of “identity as the backplane” ensures that we can identify each server, identify the users of the server, and build a stack of proofs that are all signed from the base up. This means that we don’t simply trust that nobody is in the network, but we can assert and verify at all times that the connections between services have high authenticity, as well as use encryption to protect the data in transit.
When it comes to service style solutions, we can authenticate to the service, confirm that the connection is encrypted, and then we need to rely on their assertions about tenant separation. The best SaaS services will be doing the above, and searching for whitepapers or conference presentations from some of the SaaS providers out there will show how seriously they take security
Nerd, Geek, Father. <insert witticism here>
6 
1
6 
6 
1
Nerd, Geek, Father. <insert witticism here>
"
https://koukia.ca/managed-kubernetes-on-azure-aks-8514ac0cd8aa?source=search_post---------60,"In a previous post, I showed you how to create a Kubernetes Cluster, on Azure Container Service (Considered an IaaS offering, since you manage all the resources) and as you could see , it was pretty straightforward.
Today I will walk you through a new service on Azure called Managed Kubernetes or AKS, and this is even easier.
"
https://medium.com/instinctools/top-5-things-to-consider-when-choosing-an-iaas-provider-in-2021-6847bc38a700?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
Table of Contents1. 5 things to consider when choosing an IaaS Provider in 20212. Difference between an IaaS cloud provider and a cloud migration expert3. Top IaaS providers in 20214. Get started with IaaS cloud migration
Onboarding IaaS is meant to make life easier, right? So, why all the stress when choosing from a list of IaaS providers? We know picking the right provider can be a challenge. After all, before migrating your infrastructure to the Cloud, how do you know which one is right for you.
We want to make it simple. From our years of experience, here’s what we believe are the top five things you need to know when choosing from a selection of IaaS providers.
An IaaS provides your business with a cloud infrastructure that is designed for efficiency, speed, and expansion. The trick is migrating to that infrastructure the right way and choosing the right IaaS solution provider or providers to do it with. Here’s what you need to know.
Public, private, or hybrid? Just as there are many different shapes of clouds in the sky, so too are there various types of cloud service. Here’s a quick breakdown of the classes.
Provided by a third party, public clouds are flexible solutions that allow users to pay per usage. Companies that operate using public clouds often do so to cut down on storage costs, have flexibility and access any place, any time. Public clouds also deliver on speed and efficiency, meaning they can often be quickly deployed to meet the growing needs of a business.
Private clouds offer computing services via a private internal network or the Internet to a limited number of people. Companies often choose private clouds when their business requires a certain level of privacy. However, operating a private network often entails higher costs, due to the need for hardware and maintenance, and, additionally, the company remains responsible for all support and operational issues on the infrastructure.
A hybrid cloud combines the usage of a private cloud with access to specific public cloud services. It allows for the private cloud to connect to different services and offers flexibility and optimization. Conversely, hybrid clouds come with their own challenges, such as being complex to set up and manage, having a larger attack surface and general challenges associated with migrating data.
In the modern world, data is like gold dust. That’s why when it comes to choosing a commercial IaaS provider, one of the first questions you should be asking is, “how secure is your IaaS?” You should always look out for:
Knowing these key facts will give you the power to make a smarter decision about your IaaS.
As an IaaS standard, some level of security is a given, but it’s your job to make sure the level is adequate for your needs. Remember, you are responsible for your client’s data, so keep it safe and be prepared.
Many providers use subscription-based models to price their IaaS network service. Sounds simple at first glance. However, it can be quite complicated to get a grip on how much it will actually cost in the end.
Undertaking a TCO analysis will help you get a better grasp not only of how much cloud services will cost but the total cost of migration to the cloud.
Ease and speed of access are well noted as two of the great benefits of cloud migration. When it comes to your IaaS, it’s vital that you get the access you need when you need it. Before signing up to any cloud system, it’s essential to understand who has access and when so that your business can continue to function seamlessly.
You may also want to ask tough questions, such as “is there a failsafe if something goes wrong on the provider’s side?” As worst-case scenarios can and do happen. Planning ahead is the only way to avoid serious disruption to your business.
IaaS stores your business and client data off-site, meaning you are not fully in control. And that’s where trust comes in. Before signing up to an IaaS provider, it’s essential you understand how and where your sensitive data is held. After all, failure to comply with data regulations carries some hefty penalties, and that’s before mentioning your reputation.
Keep on the safe side and look out for SSAE 16 and SOC 2 Type II. These are the best certifications out there and a sign that you are dealing with a reputable provider.
Getting to know the IaaS market is the first step in cloud migration. If you’re new to the process or lack the in-house specialists to advise you, getting in touch with a reliable IaaS solution provider is one of the best things you can do. A solutions provider will be able to guide you through the entire migration process, avoid the pitfall, and put you on the right track for successful cloud migration. They will be able to help you:
When you first make the decision to move to IaaS, you might not know quite what that entails and who is responsible for what. Here are the two bodies that you absolutely must know.
IaaS provider — this is an actual service provider that will deliver your cloud storage, IaaS, PaaS, etc. Usually, it will be a big-name brand, such as AWS, Google, etc. But you may also use a smaller reputable provider.
IaaS migration service provider — while having an IaaS provider is great, how will you actually get your business infrastructure to the cloud? That’s where an IaaS migration service provider comes in. They can handle all the analysis and tech stuff, so when your company goes cloud, it does so seamlessly.
It’s no surprise that tech giants, such as Amazon and Microsoft, top the leaderboards as the best IaaS cloud providers, but who else has made the cut? Let’s take a run-down of the top IaaS providers in 2021.
AWS is one of the most popular IaaS providers in 2021. Millions of organizations use Amazon Web Services for their cloud computing needs, whether that be content delivery, data storage, computing power, analytics, mobile services, and more. Its subscription model lets you pay only for the services you use, so there are no added costs.
From AI and ML to analytics, blockchain, data storage, networking, DevOps and so much more, Microsoft Azure is a major player in the IaaS world. Azure is an intelligent cloud system that empowers companies with a variety of solutions. Its worldwide availability makes it an attractive platform for companies to use.
Giant Google’s cloud computing platform is a suite of technology dedicated to cloud services. Incorporating IaaS, PaaS, and serverless computing, it answers almost all your cloud computing needs. It’s one of the faster platforms out there, which gives it a competitive advantage.
Designed with businesses in mind, IBM cloud is a technology that includes IaaS, PaaS, and SaaS. Its combination of cloud model forms — private, public, hybrid — means a greater variety of options for users.
DaaS, IaaS, PaaS, and SaaS are the backbone of Oracle’s cloud infrastructure. One of the world leaders, oracle is designed to make cloud computing accessible to its users globally as it is considered one of the best IaaS and PaaS providers around.
Now that you know the secrets of choosing an IaaS cloud provider, you’re probably wondering what to do next. The next step is getting in touch with the cloud migration professionals, whether that’s in-house or through a reliable provider. They’ll guide you through the next steps on undertaking a TCO analysis for your company and setting out a cloud migration plan.
Explore *instinctools expertise in adopting cloud technologies for your business needs > instinctools.com
Delivering the future. Now.
803 
803 claps
803 
Written by
We advance and innovate businesses with digital transformation. 20 years in the market. Let’s talk > contact@instinctools.com
*instinctools is capitalising on 20 years experience in technology, strategy and data to transform how you interact with the world; delivering tailored solutions that close the gap between vision and reality.
Written by
We advance and innovate businesses with digital transformation. 20 years in the market. Let’s talk > contact@instinctools.com
*instinctools is capitalising on 20 years experience in technology, strategy and data to transform how you interact with the world; delivering tailored solutions that close the gap between vision and reality.
"
https://medium.com/@ebrar/how-to-install-4-nodes-openstack-rocky-on-centos7-c348a00c0d54?source=search_post---------331,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ebrar Leblebici
Oct 29, 2019·10 min read
How to install 4 nodes Openstack (Rocky) on Centos7?
OpenStack is a free and open-source software platform which provides IaaS (infrastructure-as-a-service) for public and private clouds.
There are much components of Openstack and we can choose what to install in our environment. Nova, Horizon, Keystone, Cinder, Glance, Neutron are the main components. I will not give details about these components and about their mission. But you will understand these after the installation and become using Openstack for some reasons.
What can be the reasons that can make you want to use Openstack then?
First of all ;
2. You can do all the operations successfully what you are already doing with your virtualization environment like ESX, HyperV, etc.
3. It also provides virtualization security free. You can control virtual machine connections with security groups.
4. If you are planning to go with containerized applications, it’s so easy to drive Kubernetes clusters in your Openstack environment.
5. There will not much problems you meet after the installation of Openstack properly.
Let’s start with the installation. After we finish, I think you will find much more reasons for using Openstack.
Installation of Centos7 on Qemu/KVM
Before installation, we need a Centos7 minimal iso. You can download it from http://mirror.nsc.liu.se/centos-store/7.6.1810/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
Then we will create 4 VMs. If you want to install Openstack on bare metal then you can install Centos7 on your physical servers. I do not have physical server on my test environment and that’s why I’m going to install Centos7 to VMs.
Our 4 nodes will have these configurations :
Hostname : oscontroller
IP address : 192.168.122.100
RAM : 8192 MB
VCPUs : 2
Disk Capacity : 20GB
Note : You can give more resources to VMs. I gave so because of the limitations of my host.
Installation of Controller :
We should choose iso image first.
Then, we assign RAM and CPU values.
Then we create a disk image for the VM.
Then we give the name of VM and we define on which network will VM be.
I won’t show steps of Centos installation. They are basic and everyone can do by clicking Next button. The only things i want to say, i disabled Kdump and security policy while installation. Define a root password and keep it in your memory.
I also won’t show Qemu/KVM installation steps for the other 3 VMs which we will use. I’m writing only main knowledges about these :
2. Node 2 : Compute-1
Hostname : oscompute01
IP address : 192.168.122.101
RAM : 4096 MB
VCPUs : 2
Disk Capacity : 30 GB
3. Node 3 : Compute-2
Hostname : oscompute02
IP address : 192.168.122.102
RAM : 4096 MB
VCPUs : 2
Disk Capacity : 30 GB
4. Node 4 : Neutron
Hostname : osnetwork
IP address : 192.168.122.103
RAM : 4096 MB
VCPUs : 2
Disk Capacity : 20 GB
What to do if Virt Manager gives IPs automatically to the VMs?
After the installation Virt Manager can give your VMs different IPs that we defined above. It’s because of DHCP on your virtual network. We should configure VMs to use static IPs.
For example on your controller machine we should change the content of /etc/sysconfig/network-scripts/ifcfg-eth0 file.
Change BOOTPROTO=dhcp to BOOTPROTO=static
Change ONBOOT=no to ONBOOT=yes
And add rows below ;
IPADDR=192.168.122.100
PREFIX=24
GATEWAY=192.168.122.1
DNS1=8.8.8.8
Finally, ifcfg-eth0 file should be like that :
Then you should start network service with this command :
If you check your IP, you can see that changed to the one what you wrote in ifcfg-eth0 file.
Also sshd service is running and we can connect via SSH to the VMs remotely.
Installation of Openstack
We will copy our SSH keys on the host machine, to the VMs as an authorized key. So we will become access to VMs without requiring password.
On the host machine, run that commands :
Enter the password of them once, then you will become access to them without requiring password.
2. Preconfigurations before installing Openstack :
2.1 We should update the machines and reboot them with this command :
2.2 We will make oscontroller machine access to other nodes without password too. First we need to generate an SSH key and then we need to copy it to others. Run the commands below on oscontroller.
Do not change the path and do not enter a passphrase when it asks. Just click enter, and it will generate an SSH key for you.
2.3 Then we will configure /etc/hosts file on oscontroller and then we will copy it to other nodes. We should add the raws below to our hosts file.
Then we need to copy it to other nodes with this command :
Then check all the nodes’ /etc/hosts file. They should have the same content.
2.4 We will stop and disable Firewalld and Network Manager services on all machines. Then we will disable Selinux.
After all these steps, reboot your machines one more time.
2.5 We will set Openstack Rocky repository and install the packstack utility. First to set the repository we should run this command on oscontroller node :
Then update once more,
To install packstack utility we will run this command on oscontroller node :
2.6 We will generate a packstack answer file to use for Openstack installation
Now, we are ready to install Openstack. But before start to installation we will change some entries on our rocky-answer.txt file.
Move the rocky-aswer.txt file with name rocky-answer-org.txt.
Then generate a new rocky-answer.txt file with the raws below :
With this configuration, we will install Glance, Manila, Cinder, Nova, Neutron, Horizon, Swift, Ceilometer, AODH, Panko, Sahara, Heat, Magnum, Trove, Ironic components of Openstack. It’s ok if we don’t know which component using for what. Also we may not use most of them in our test environment, but how can we know that we will not need one of them. It’s a test environment. So, let’s install all of them.
You can define a different password for Horizon (Openstack Web GUI) at line :
CONFIG_KEYSTONE_ADMIN_PW=test123
2.7 For installing Openstack we will run the command below :
It will take some time and if everything is OK, then it should give an output like that :
After all of these steps you should become able to connect Horizon (Openstack Dashboard) via this link : http://192.168.122.100
Username is admin and the password is which you defined in rocky-answer.txt file before with this line :
Let’s visit menus and try to have some ideas about Openstack. It could come to you somehow complicated. But don’t worry I will explain the details of components on my next writings.
Let’s start with understanding Openstack, doing basic operations and details of the basic components
Security Engineer, expert, analyst or whatever. Just trying to learn somethings new day by day…
5 
5 
5 
Security Engineer, expert, analyst or whatever. Just trying to learn somethings new day by day…
"
https://medium.com/latest-technology-news/which-iaas-platform-is-right-for-your-business-bc024d3ffab3?source=search_post---------43,"There are currently no responses for this story.
Be the first to respond.
IaaS platforms can offer a lot of benefits that help companies succeed. Currently, Amazon Web Services, Google Cloud Platform, and Microsoft Azure stand out as the three most popular IaaS platforms. Before you decide which option fits your company’s needs best, you should learn about the pros and cons of each platform.
Amazon Web Services is the most popular IaaS platform. In fact, 37.1 percent of companies that use IaaS platforms choose Amazon Web Services. To put that number in context, 16.5 percent use Google Cloud Platform and 28.4 percent use Microsoft Azure. Amazon launched its cloud service platform in 2004, which helps explain why it’s so popular.
Many companies like Amazon Web Services because it offers scalable computing on demand. If your business suddenly needs to expand its operations, Amazon can help you reach that goal quickly without any disruptions.
The only problem with choosing Amazon Web Services is that the company charges some of the highest prices in the industry. The extra expense may not matter if you need the stability and scalability of Amazon’s services. If you’re operating on a tight budget, though, you may want to explore other options that don’t cost as much.
Google’s experience with analyzing data gives it the upper hand for companies that want access to detailed analytics. Google has also spent a lot of time developing its machine learning capabilities. Google’s machine learning prowess makes it a good option for companies that want to streamline their operations and create new applications.
Price makes Google Cloud Platform an attractive option. The company offers Sustained Use Discounts that automatically adjust your bill according to the number of CPUs and RAM you use. If your IaaS needs vary from month to month, Google is probably a good option for you.
Before you choose Google Cloud Platform, you need to think about the disadvantages of using its IaaS. Google is a relative newcomer to this industry, so it doesn’t offer as many products as Amazon and Microsoft. Google Cloud Platform also has a complicated user interface that can cause confusion, especially when you first start using it. If you have ever used Google Analytics, then you already know that Google isn’t great at making features simple. You’ll have a similar experience with Google Cloud Platform.
Image via Flickr by tomconte
If your company already uses Microsoft software, then you may want to use Azure as your IaaS platform. Software like Office and SharePoint integrate well with Azure, so it’s a good option for most businesses. Microsoft’s deep pockets and decades of experience also make it a strong choice for companies that need access to diverse services and products.
On the downside, Microsoft hasn’t been able to keep up with Apple and Google in the mobile devices industry. Microsoft says that it can work with any platform, but it seems reasonable to expect some disruptions when your employees use non-Microsoft devices.
Make sure you understand the differences between these IaaS platforms before you choose one for your business. Choosing the right option will make it easier for your business to reach new levels of success.
Originally published at Mab Tech Blog.
Mabtec Blog provides the latest Technology News, it news…
Mabtec Blog provides the latest Technology News, it news today and top headlines about the Technical News, Internet, Gadgets Reviews, coolest gadgets, mobile technology.
Written by
Aashish Sharma is a Founder and Blogger at https//www.entrepreneuryork.com, specializing in Social Media and Digital Marketing.
Mabtec Blog provides the latest Technology News, it news today and top headlines about the Technical News, Internet, Gadgets Reviews, coolest gadgets, mobile technology.
"
https://medium.com/@andres.sanchez_7421/what-are-saas-paas-and-iaas-6fb2070aeec3?source=search_post---------240,"Sign in
There are currently no responses for this story.
Be the first to respond.
Andrés Sánchez
Dec 28, 2018·4 min read
In the company I work for, several times I heard the words SaaS, PaaS and IaaS, but I didn’t know their meaning. So, I got curious about it and decided to investigate the meaning of these concepts. I discovered that they’re Cloud Computing Service Models.I want to share what I learned with you!
What’s SaaS?
The meaning of SaaS is Software as a Service. This concept is also known as Cloud Application Services. These kind of applications are provided and managed by a third-party vendor through the web. Also, the vendor provides an interface so that the client who acquired the service can access to it.
Most SaaS applications can be used directly in the web browser without downloading or installing anything. This is very useful because the client doesn’t have to worry about installation, support, updates or hardware requirements for the applications. For example, Imagine a company that has around 300 employees and each of them need to install the program to do their job. This would be a complete waste of time and also very tedious because applications are constantly changing. The best option in this case is to hire a SaaS application, because the vendor is in charge of the application updates, middleware, virtualization, networking, storage, among others.
Some of the most popular application types offered by SaaS companies are email apps, healthcare apps, storage apps, electronic signature apps, among others. For example, the company I work for uses Zoho mail, an amazing email platform.
Besides, some popular SaaS applications are Zoho apps, DocuSign, Google Apps, Salesforce, Slack, among others.
What’s PaaS ?
The meaning of PaaS is Platform as a Service and these kind of applications are also known as Cloud Platform Services. These applications are also offered by a third-party vendor that provides coud components to run your applications and also are in charge of the management of virtualization, storage, servers, Operative Systems, networking to maintain the PaaS app.
So, when using PaaS applications, the software developer should worry only about crafting the application. The reason of this is because the infrastructure to run the software is provided by the client. This is a great advantage because the scalability of the applications is automatically managed by the vendor. For example, if your software needs more resources in order to run, the PaaS application provides more resources to the app automatically. Also, using PaaS apps makes the development, testing and deployment of your programs easy, cost-effective, quick and simple.
Some of the most popular PaaS applications are Heroku, AWS Lambda, Google Cloud Functions, Kubernetes, among others.
What’s IaaS?
IaaS is the abbreviation for Infrastructure as a Service, also known as Cloud Infrastructure Services. These kind of applications are self-service models provided by a third-party vendor. These models allow customers to access, manage and monitor computing, networking and storage modules of a remote datacenter. This service is offered as an alternative to buying hardware to run the applications. Usually, the IaaS service is based on the hardware consumption.
In IaaS applications, providers mange storage, servers, virtualization, networking and sometimes databases and messaging services. Customers and developers are in charge to create the applications, middleware, data, runtime and also to manage the Operating Systemps required to run the applications.
The main advantage of acquiring IaaS application is that you can get custom infrastructure and platform to run your applications.
Some of the most popular IaaS apps are AWS (Amazon Web Services), VMware vCloud, Digital Ocean, Microsoft Azure, Google Compute Engine, among others.
References
SaaS, PaaS and IaaS information
Popular SaaS apps of 2018
Popular PaaS apps of 2018
Popular IaaS apps of 2018
Computer Enginering Student at ITCR | Software Apprentice at Pernix
Computer Enginering Student at ITCR | Software Apprentice at Pernix
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kcsitglobal/delivering-microsoft-practices-choosing-iaas-kcs-920006f5367?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
kcsitglobal
Nov 21, 2016·3 min read
It’s a known fact that the Cloud Computing Services provides options for approach, sourcing, and control. It delivers a well-defined set of services, which are perceived by the customers to have infinite capacity, continuous availability, increased agility, and improved cost efficiency. With the KCS Microsoft Azure services, one can easily expect the shift from a traditional server-centric approach to a service-eccentric approach. Our clients are easily able to receive applications on per-determined standardized platforms with mutually agreed service levels. Our Cloud Computing Solutions let you focus on conceiving and conceptualizing your technological deliveries to a business advantage rather than building, maintaining, and upgrading your own infrastructure. We provide you the perfect development plan and a deployment strategy to ensure you get the benefit of working in the new environment.
IaaS Services by KCS
Our IaaS services is a combined stack of a pool of computing, storage, and connectivity capabilities that are delivered as services for a usage-based (metered) cost. Our goal is to provide a flexible, standard, and virtualized operating environment that can become a foundation for SaaS. Usually seen as a standardized virtual server, we provide more choices for configuration and operations of the guest Operating System (OS), software, and Database (DB). Compute capabilities such as performance, bandwidth, and storage access are also standardized. The Deployment model by KCS can easily be defined as Partner Hosted Private Cloud with a customized architectural design and a higher degree of customization. It blends the benefits of using predefined functional architecture, lower deployment risk with the benefits of internal security and control.
IaaS features @ KCS:
The IaaS Service scenarios @ KCS include:
The Advantages of IaaS Services by KCS
Get in touch with us and click on this link
"
https://medium.com/@rachanag/what-to-look-when-signing-saas-contracts-6e459f5b727f?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
rachana gupta
Dec 2, 2021·4 min read
Initially cloud was about IAAS, how assembling hardware and shipping large servers was now a thing of past. It was just few clicks away to have your own server and you would also avoid sitting in cold datacenters for troubleshooting and with cloud it can be done from anywhere. Then came the PAAS services era, where you could have software installed quickly and get business ready quickly. You just had to worry about the application and had no associated access to OS or hardware.
Now is the era of SAAS, from initial skepticism to financial institutions moving SAAS is an amazing achievement of modern times. The SAAS provider automates many time consuming and repetitive tasks and makes the businesses focus on innovation, than get into problems of scaling in the middle of their annual sale. Companies barely have any upfront costs, but find SAAS cheaper and easier to manage.
It makes sense to consider all of your options to find a great IT solution that’s going to work best for your company and your budget. In case companies choose SAAS then there are security checks they should be additionally doing when choosing the vendor. The best part of SaaS is you don’t have to install anything, just sign in through the browser or mobile apps. To put it short, without any local hosting on servers, you can start accessing the software from any smart device.
This is self explanatory, on how to add new users and how to remove users when they leave. If its cumbersome process then the SAAS is not worth it. It should have well defined workflows for approvals. Additionally we should ask if SAAS provider has any user management api , if there is not then its a reason enough to not go ahead.
2. Does the application have SSO and MFA ?
New age exploits are all on identity, securing it has become crucial. Even if they charge additional for enabling SSO, it will be worth from security perspective. If its local use accounts, then its worth checking how the user passwords are stored? Hopefully not a table in some database :). Then you can cross them off your list.
3. The datacenter protection details or cloud provider details.
Ask your SaaS provider on details of the mechanisms and techniques they use to secure their data centers.
a) A secure, SAS70-certified Tier 4 data center.b) Intrusion detection systems.c)Firewallsd) SSL and application security.e) 24×7 security monitoring.f) Third-party certifications for security practices.
4. The compliance standards
The compliance standards the SAAS service meets, as you might be using it on your own customers in different domains.
5. The support model and SLA
I believe a bad customer support kills a product. In case of SAAS, the amount of time you cannot access a application is equating downtime. They should be also able to provide audit logs when requested without cumbersome processes. Do they provide 24/7 support?
6. RTO and RPO
The more specific questions to ask here would be about their uptime/availability statistics and how they protect their services from disasters.
A good SaaS provider should have a 99.5% uptime. Make sure your SaaS provider has backup servers so that there is no disruption in your work in case their hardware fails, or a disaster (earthquake, tsunami, etc.) happens. Finally, make sure you get a refund if your SLA is not met.
7. Maintenance windows.
How often do the SAAS provider seek planned downtime ? Do they failover to DR for minimum disruption? Do they have blue green deployment strategy? Look at their ratings and previous major downtimes and causes of that? Are they security patches? or they frequently keep fixing the product?
8. User friendly solution?
How easy is it to use? Basically if its a call center solution, can your non technical agents use it easily or they need training? Do they keep changing UI a lot which causes confusion?
9. Data retention policies?
I had once seen a retention policy on a security saas provider who said they will retain customer data for 7 years even after decommission. Why will anyone want them to keep our data? This causes privacy concerns and also what if its sold to someone else or gets breached? Also, there should be easy path to get data from SAAS provider in case you decide to stop their services. You shouldnot get stuck in a bad product if you get a better and cheaper choice.
10. Previous history?
Always good to look up the company for any breaches in the past.
Technology geek 🤓 who still reads
2 
2 
2 
Technology geek 🤓 who still reads
"
https://medium.com/@aheadcrm/clash-of-titans-the-iaas-platform-providers-6bb046972e97?source=search_post---------26,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Oct 14, 2018·5 min read
In the past three posts of this series I have covered the definition of a platform, followed by a brief analysis of the big four players in the customer experience world, namely Microsoft, and SAP, followed by Salesforce and Oracle.
Of which there are mainly four, although Gartner Group lists six of them in their 2018 Magic Quadrant for Cloud Infrastructure as a Service, Worldwide! And this drop from 14 vendors in 2017 to just six in 2018 already shows how much consolidation is going on in this market.
But why even mention them here? After all this text originated as an analysis of the big business software vendors.
Cloud infrastructure provides are important for two reasons:
In essence, the big six of the IaaS providers that are covered by the Gartner Group offer a technology platform, an ecosystem, insight, and productivity tools.
The only thing that they are not really doing — yet — is offering rich, integrated business applications.
But back to why I do see only four main IaaS providers going forward.
These famous four are AWS, Microsoft, Google, and Alibaba.
Of course with AWS having a tremendous lead for now, with Google and Microsoft struggling for the second spot, and with new kid on the block Alibaba growing bigger in their rear view mirrors.
IBM and Oracle, similarly to venerable names like Rackspace, Fujitsu, NTT, and the other ones that dropped off the quadrant, will become niche players.
Oracle, already one of the Titans, concentrates on Oracle database and Oracle application workloads. While the Oracle DB arguably runs best on Oracle hardware this will not prove to be a winning value proposition for companies that have the choice between highly performing databases. And even when running an Oracle DBMS they might opt for somebody else’s cloud.
Similarly IBM. Although they are moving away from a hardware company towards being a services company there is a lot of legacy around them. This legacy will be hard to shed off and this will diminish IBM’s ability to fully compete as an IaaS vendor on a global scale.
Both of them will survive — but they will not get anywhere near the market presence of the IaaS titans.
So, let’s have a look at them.
Alibaba’s home turf is China. The company, however, is very actively enlarging its footprint and reach. By now it is offering more and more services that are available in China worldwide. Still, not everything that is possible in China, is possible elsewhere.
However, with its protected homeland the company is able to mature offerings in a ‘safe’ environment before offering them in the highly competitive ‘rest of the world’. Partnerships like the recently renewed one with SAP show an increased credibility. Additionally we should not forget that Alibaba with its retail business is similarly capable of providing data services that only Amazon or Google could possibly compete with.
And then there are services that are common place in China but virtually unknown hereabouts: How about paying directly from your phone? No credit card needed …
Alibaba is a (distant) number four in this quartet but certainly a force to be reckoned with.
Google is providing an integrated offering that basically commercializes the technologies that the company needs itself. In this it is similar to Alibaba and AWS. In contrast to the other vendors it, however, is dealing more with open source software and is open sourcing its software. It shows that open source can be a very successful model. While the overall offering is less broad than the one of AWS its AI and ML capabilities are very high, based upon the unrivalled amount of data that the company ‘owns’.
A challenge that Google has — and will continue to have — is the ability to attact large business workloads. Although SAP partners with Google (as well as with Microsoft and AWS) I do not think that they will get a strong foothold in this area, especially competing against Microsoft and Azure.
On the other hand GCS is the ideal ‘entry drug’ for smaller and aggressively growing businesses.
Microsoft does not only have a strong application stack, but with Azure additionally also offers a competitive IaaS stack. Microsoft’s real advantage is the ability to completely use the own stack to generate work loads as well as extending the own cloud into their customers data centers with the Azure stack. Azure not only has a worldwide availability but also security certifications covering most important regions. Add partnerships with Enterprise vendors like SAP and Adobe and the strong partner ecosystem that works with SMBs there is a very strong position:
Azure is attractive for large businesses as it arguably can run very large workloads.
Azure is attractive for SMBs as it is easy to integrate various pieces of the platform (remember: technology platform, insight, ecosystem, productivity). All pieces are available and tie well into each other.
This is one major reason why other enterprise software vendors should be wary of who they hunt. Instead they should sometimes have a look into the rear mirror to cover their back.
AWS is still leading the pack with quite a margin. Since its invention the company shapes the market by a seemingly neverending array of new services, supported by price cuts that are made possible by the sheer scale to which the platform grew. Amazon’s offering is probably the most complete around. Apart of being a very strong in SMB and new economy businesses the company has partnerships for example with SAP and with Salesforce, or with Workday, to name just some, that help it getting enterprise workloads. Still, from its very origins AWS is still less attractive for traditional enterprises but more for the maturing kids of what was called the ‘sharing economy’. On the data frontier it is powerful, but lacks a few of the capabilities that Microsoft has. On the other hand AWS has its hand on behavioural consumer data that can easily get commercialized.
Amazon’s AWS is still the power that no one can ignore. However, there are now three competitors that cannot be ignored, either. Especially not by AWS itself. Right now it is probably too early to do any forecast but one thing is for sure: Things will get exciting.
Amazon AWS is vulnerable. Vulnerable on the data frontier and on the enterprise workload frontier. It is not a given that AWS stays the undisputed leader. In fact, I’d wager (sorry, no forecast) that AWS gets attacked from multiple angles. Microsoft will get more of the enterprise type workloads that AWS needs. Google has a good chance to tackle Amazon from the small business side of the house, while Alibaba has the potential of becoming a full blown tsunami, covering small biz to enterprise workloads, just like AWS does.
One thing is for sure in this part of the clash of the titans: A competition of more than three players keeps the heat on by enforcing customer facing innovations and price models.
May the games continue!
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
6 
6 
6 
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
"
https://medium.com/@kevinsprince/iaas-security-groups-60abbb5c1ba4?source=search_post---------242,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin S Prince
Sep 7, 2015·1 min read
or why I won’t be deploying to Scaleway
Over the past few years I have deployed a lot of applications into AWS for employers, clients or personally. Last week Scaleway rolled out their new super cheap pricing for their ARM based cloud so I decided to give it a whirl.
Creating servers, images and new volumes is quick and simple. The servers themselves seem nicely performant and I had some apps up and running in the hour or so I played with the platform.
Where all this falls down for me is security groups; in the world of Amazon VPC security groups what you all in and out is explicit. If you have not allowed port 80 to accept traffic the request will simply fail.
However in the Scaleway world everything is allowed right from the get go, once you fire up your server everything is accessible to the outside world.
I did try to drop all inbound connections as my last rule in the chain but this actually blocks everything including allowed ports above it. The suggested solution from Scaleway is to use iptables on the individual instance is very much less than ideal even if your using configuration management for your instances.
Hopefully they fix this in coming hardware revisions as it is according to the support response it is a hardware based issue and I can look at using Scaleway again.
Wandering Geek. My thoughts are my own and dont represent my employers.
Wandering Geek. My thoughts are my own and dont represent my employers.
"
https://medium.com/@gowthamlabs/microsoft-azure-products-at-a-glance-c182b793ab9c?source=search_post---------385,"Sign in
There are currently no responses for this story.
Be the first to respond.
GowthamLabs
Mar 10, 2018·6 min read
Microsoft Azure is provides a suite of cloud IaaS, PaaS, SaaS solutions. Amazon AWS is often compared and referred to it as a key competitor on almost all its products suites. Microsoft that has a deep roots into Information Technology has joined hands with RedHat and opensource community and creates Windows and Linux based public cloud solutions that is adding success rates to its offerings. This article will give an overview of Azure products and services.
The rapidly changing AS-A-Service business model in IT industry has led to an explosion of buzzwords. These are the buzzwords you hear due to Technology advancements, cost efficient ways of consumption’s and what they really mean to you ?
I took time to write down all the new words and share them as a list here with the technology context that is useful as solutions to consume. I hear these while I attend various Technology conferences, events etc.
This is more of a running article which will continuously be updated with New Buzzwords but related to Microsoft Azure technologies.
Astrologer | Enterprise Architect
1 
1 
1 
Astrologer | Enterprise Architect
"
https://medium.com/@simply2cloud/iaas-vs-paas-660fe6fdb396?source=search_post---------233,"Sign in
There are currently no responses for this story.
Be the first to respond.
Simply2cloud
May 31, 2018·3 min read
What is the difference between Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) on Microsoft Azure? Which one do you need? In this article, we will explain what IaaS and PaaS are and what they mean to you, if you are looking to migrate to the cloud but do not know where to start then this article is for you, so let’s begin.
A traditional on premise data center be it in-house or an actual data center, you have to manage every aspect including purchasing and installing a hardware, the virtualization, the networking software, security, storage etc. and not only that but you are also responsible for maintaining those resources, fixing them when they fail and replacing them as they age and become inefficient. While managing a data center might seem fun to some people, there really aren’t many reasons to go this route. While you do have complete control, it’s costly and the benefits are limited. When it comes to cloud offerings like Azure, you get an alternative to the traditional on-premise data center.
For starters, Azure is responsible for all of the hardware and maintenance of that hardware. Beyond that, Azure also offers a wide variety and ever-growing list of services that you can easily take advantage of as well. The biggest benefit is you can choose how much or how little hardware that you use and the price will adjust accordingly. If a server goes down, Azure will take care of it and usually does so in a way that is unnoticed by the end users.
This leads us to what Infrastructure-as-a-Service is. As mentioned, with traditional on-premises data centers, you have to manage everything. With Infrastructure-as-a-Service, Azure is responsible for, well, the infrastructure. This includes hardware, storage and networking and everything that goes along with it. In this case, Azure also handles the virtualization which makes it easy to get systems up and running quickly. With Infrastructure-as-a-Service, you are responsible for everything else though meaning you manage the OS and the licensing, security updates, any software that needs to be installed like runtimes and middleware and deploying your own applications. This is a great option for companies that currently have on-premises data centers but one might upgrade to the cloud whether it is to take advantage of cloud service offerings, scale performance or just for the benefits of offloading the responsibilities and costs of managing a data center. This is the easiest route, too. All that has to be done is a push of VMs to Azure and you are good to go.
What if you are a development shop and you only care about your applications and you don’t want to manage the OS, you don’t want to manage runtimes and software. Well, that’s where Platform-as-a-Service comes in. With Platform-as-a-Service, Azure manages not only the infrastructure but also the OS, runtimes like dotnet and middleware like IAS or nodejs. The only thing that you have to worry about is your application and your data. Platform-as-a-Service really makes it easy to deploy applications since the only thing you have to deal with is the application and Azure manages everything else for you. They’ll spend a few minutes setting up Azure to handle your application but once you’re done with that, you don’t have to bother with it again.
These explanations are pretty basic but they give you an idea of what the differences are between the two and you can make better decisions when choosing how to utilize cloud services. So that’s it, I hope that this article is helpful for you and be sure to take advantage of what Azure has to offer and make your organization be up to date with the latest trends in cloud computing technology.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@david.joy1588/google-cloud-platform-gcp-for-data-scientist-and-introduction-to-big-query-part-1-4f04022ea1e9?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Joy
Jan 22, 2019·6 min read
The Google cloud Platform aka GCP is a suite of cloud computing services. It provides IAAS,PAAS and Serverless computing enviroments. As ML and data science become a more integral part of Business today, companies are using Cloud based environments due to its scalability , performance and ease of use. With solutions ranging from AWS to GCP to Mircosoft Azure. Their are many solutions in each of these platforms.
As I continue to learn and grow my skills as a Data scientist I find its essential to explore the GCP BigQuery which is a data warehouse engine that makes life easier due to its usability and scalability. As a data scientist we work with data everyday, mostly doing data analytics or run complex ML models on the data we access from databases or warehouses. We end up mostling doing a lot read activities than write activites. The GCP BigQuery Platform is expected to be used as a write once and read many times database. However, before go deeper into it lets have a look at the other solutions on the GCP Platform.
GCP Projects define the parent container inside which all data resides in BigQuery. Different projects can have different BigQuery instances. Access is controlled individually or through project-level controls. Next comes the BigQuery dataset.
A BigQuery dataset is similar to the concept of databases in other RDBMS. Datasets govern the geographical location of data within the particular dataset. Locations supported include US, Europe, and Asia.Tables created within a dataset in one location cannot be joined with tables in other locations. Datasets also provide logical grouping of information similar to databases.
We can create different datasets for different applications like inventory, finance, etc , or for different purposes like development, QA, and production. Datasets contain table which in turn, store all the data.
All tables used in a query should be from datasets within the same location.This limitation should be kept in mind while designing datasets and tables.
Loading into Big Query from GCP Storage :
Can be done using the various Big Query Interfaces which include
Loading can be done by using a file upload feature from the system or from other cloud sources.
One can use queries such as below in the New Query section to get the data.
Note: Some of the features in BigQuery are only available with Standard SQL instead of Legacy SQL, Thus make sure to uncheck Legacy SQL when running a query in order to avoid errors.
BigQuery allows partitions of data based on either time of ingestion or an explicit date or timestamp column. When a partition is created based on the ingestion time, BigQuery creates two pseudo columns called _partitiondate and _partitiontime. These columns can be further used in queries.
When partitioning is done based on an explicit column of date or time that exists in the table no pseudo columns are made. The partition is done on columns data and this improves the performance of the BigQuery as it focuses on lesser number of rows. It is recommended to use partitioning when using transaction based tables.
Google BigQuery allows users to query data directly from other Google Cloud data sources, like Cloud storage or Bigtable. You can create a table that references an external source. No data is stored inside BigQuery. Rather, every time the data is queried, the query is executed against the external data source under results provider. This saves on storage costs but has an impact of performance on these queries.
Saving as View
We can also save the result of query as view using the the save view button on the query page.
Labels In BigQuery
When a BigQuery repository is used by multiple individuals and teams, and they end up creating too many data sets and tables, it becomes difficult to track why they are created and of what purpose.Sometimes it is important to group these resources for billing or other purposes. You can do the same by creating labels. Labels are name value paths that can be used to attach a piece of metadata to a resource like a data set, a table, or a view.
Big Query with Python
The big query package in python can be used to connect to the big query instance on google cloud and data can then be accessed using Standard SQL.
Best Practices in BigQuery
It is essential to have right table design as design dictates how much would be the cost of using GCP as well as performance of the database and queries. Some of the best practices in BiqQuery are as below.
Hope this blog helps and introduces you to BigQuery. Please comment or let me know if you have any query. I will be working on EDA using python and big query in Part 2. Once completed I will link that blog.
Sources :
You can use free trial with $300 credit to create a free GCP account with your gmail. Once that is done access BigQuery from BigData section.
https://cloud.google.com/bigquery/docs/
https://www.linkedin.com/learning/data-science-on-google-cloud-platform-designing-data-warehouses/why-data-warehouses-are-important , the training is what I did to learn BigQuery basics.
Data Scientist → Student at UTD →Intern at Fiserv. Inc (Currently)
See all (8)
9 
1
9 claps
9 
1
Data Scientist → Student at UTD →Intern at Fiserv. Inc (Currently)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-polygon-best-gems-on-nfts-the-strongnodeid-and-how-computing-with-a-purpose-reflects-8812a8ca01e9?source=search_post---------309,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 30, 2021·4 min read
The Infrastructure-as-a-Service (IaaS) tech company and innovation lab StrongNode.io CEO and Co-founder Daniel Saito joined Polygon Best Gems for an AMA session. Daniel answered questions from the community and shared facts about our upcoming IDO launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
In case you missed it, here are the highlights from the AMA between StrongNode and Polygon Best Gems:
We can say that it is another source of passive investment as for example mining Bitcoin. Are you the first company that makes this product or is there something similar on the market?
Daniel Saito: Bitcoin’s increasing energy consumption has triggered a passionate debate about the sustainability of digital currency.
As StrongNode tries to tackle Green Mining, we want to make “Computing for a Purpose” a reality. Due to the recent times with the pandemic people have been shuttered in their homes, this was no different for us as well. Here we take a stand and try to solve the world’s problem one problem at a time! Made with ❤️ for the NEW Normal!
# Bitcoin’s annual e-waste generation adds up to 30.7 metric kilotons as of May 2021.
# This level is comparable to the small IT equipment waste produced by a country such as the Netherlands.
# On average Bitcoin generates 272 g of e-waste per transaction processed on the blockchain.
# Bitcoin could produce up to 64.4 metric kilotons of e-waste at peak Bitcoin price levels seen in early 2021.
# The soaring demand for mining hardware may disrupt global semiconductor supply chains.
“Computing for a Purpose” is using your idle computing cycles to work trying to solve some of the world’s toughest problems that we face. In the near future, StrongNode will be computing DATA files from genome sequencers to find genome trace elements in COVID. While we reduce the cost of computers for these biopharma labs.
Honestly, I was not expecting such a complete answer as this, you just showed us the best “StrongNode vs Bitcoin.”
Daniel Saito: Yeah, we believe if we are moving Zeros and Ones and it’s plugged into the wall, it needs to be monetized otherwise it would just sit there in idle showing nice pictures in the form of a screensaver.
How long have you been developing the project?
Daniel Saito: We have been working on this idea for StrongNode for over 2 years (first in the form of an open-source Wi-Fi router) since my background is in rapid prototyping in both hardware and software. After advising various crypto projects, we saw what worked and what didn’t. When myself and Colin worked at MySQL we were consulting with [numerous] Web 2.0 companies optimizing the performance of their MySQL queries and tweaking their implementation architecture so it scales with effective transactions per second, we started to notice a need out in large scale enterprises all the way to small-medium businesses that they had a requirement to scale out their data and processing capabilities, hence the *cloud infrastructure* race (AWS, GCP, MSFT).
Could you resume the next steps of the project?
Daniel Saito: Right now, we are planning to have a successful IDO on starter.xyz and Bull Perks [this coming October 6, 2021]. Join us for our IDO and get to know more about the process of whitelisting yourself to participate through this link — https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
Immediately upon releasing our tokens into circulation, we will open up staking and AMM farming. All developed on custom code and all code will be audited by rug doctors. We have several partnerships planned in the pipeline. Besides us having StrongNodeID as a platform for SSO across others, the other thing we do differently is we leverage open source technologies and add a payments layer underneath it. We will work on enabling the network stack, and you can expect products like VPN and proxy.
Any plans to bridge the token? To increase community and not only focus on Matic investors?
Daniel Saito: Later, the plan is to head over to SOLANA land next.
NFTs are hot trending now, do you have a play to have NFTs in your platform? If so, can you tell us the plan of your project in NFTs?
Daniel Saito: NFT is actually at the genesis of the project. When you are first onboarded on StrongNode, you have to conduct New User Registration with StrongNodeID.
StrongNodeID is the equivalent for “Sign in with Facebook” and “Sign in with Google” this OAuth approach will be governed by a UNIQUE NONMOVABLE NFT that is specific and unique to your wallet.
This NFT will work as an access token to join the StrongNode Network.
Of course, there are other NFT plays in the pipeline of projects as we are working on a special one close to my heart is a Music NFT project, which has been in the works for the past 5 years.
For more details, visit: https://strongnode.io/
Join us on Telegram and be part of our community: https://t.me/strongnodechat
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
456 
456 
456 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/lightspeedindia/at-intech50-india-enterprise-technology-startups-e86013a12b19?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
I am looking forward to attending iSpirt’s Intech50 event next week in Bangalore. Fifty Indian enterprise technology companies will be there, meeting with fifty CIOs from the US and India. Congrats to everybody at iSpirt, including Sharad Sharma and Avinash Raghava, for organizing what promises to be a productive spot-market.
We have put the names of the Intech50 companies into a list and categorized by tech category (applications, infrastructure, tools), vertical (e.g. none, financial services, retail) and geographic focus (India, India->Emerging Markets, India->Global, Global). See the Slideshare embed below to view or download the document. Perhaps this is of help to you while navigating the Intech50 event.
These companies are two-thirds infrastructure, one-third applications. Analytics and security dominate the categories, followed by HR and CRM. A good 75% are horizontal solutions while the rest are verticalized. And more than half have a global go-to-market while about 40% are focused on India or adjacent markets.
The last decade, especially the past five years, has seen a lot of change for the better in the enterprise tech startup market in India. Startups have started building world-class product and user experiences. They have understood how to leverage online marketing to acquire customers outside India, versus relying on channel partners in other countries or sticking to just the India market. They have figured out how to initiate direct sales abroad, not relying on hiring expensive VPs of Sales as a first step but sending founders abroad to kickstart sales. They have started efficiently selling into the long-tail of companies rather than just focusing on large enterprises. And they have taken full advantage of platforms such as AWS, Force.com, etc. to run and distribute their products.
I think there is a clear progression forward from the types of enterprise software companies started 7–15 years ago which many times started with services projects; were usually vertically focused on banking, telecom or retail; and, despite branching out to SE Asia, Middle East and Africa, have generally tapped out growth at the sub-$10M annual revenue mark. There were exceptions of course, including companies like I-flex, Subex and Ramco.
Lightspeed globally has put a majority of its capital to work in enterprise technology companies, represented here. Many of our portfolio companies, including Qubole, Numerify and Bloomreach, have teams in India. In India, we are looking for enterprise technology companies that can be category-defining and category-leading and can scale to $50–100M in revenue over time. We are finding higher than average growth coming from applications companies focused on a global or US go-to-market — examples in India would be Freshdesk and Unmetric. We are also seeing such growth from globally-focused infrastructure and developer enabling technologies — examples in India would be Druva, WebEngage and HelpShift.
See you at Intech50! Come talk to us. You can reach me at dkhare at lsvp dot com.
[slideshare id=33026993&style=border: 1px solid #CCC; border-width: 1px 1px 0; margin-bottom: 5px; max-width: 100%;&sc=no]
India Enterprise Tech Companies at iSpirt’s Intech50 from Dev Khare
Tomorrow, built today.
Tomorrow, built today.
Written by
Venture capital investor at Lightspeed India Partners.
Tomorrow, built today.
"
https://medium.com/geekculture/azure-series-2-infrastructure-layer-iaas-paas-saas-serverless-745ecd305d1f?source=search_post---------211,"There are currently no responses for this story.
Be the first to respond.
This article is a part of Azure multi-part series (parent article). The previous part of the article — Azure Series #1: Security Layer — 2. Network — Protection and Next part of the article — Azure Series #2: 4 Layer Infrastructure to design for 1 to several millions of users, data & functionality .
"
https://medium.com/@rstarmer/understanding-the-business-value-of-cloud-services-saas-paas-and-iaas-6b2d5579f52f?source=search_post---------197,"Sign in
There are currently no responses for this story.
Be the first to respond.
Robert Starmer
Mar 23, 2016·7 min read
Cloud Service Models — Saas, PaaS, IaaS — What are they anyway?
The services a cloud provides to business can be divided into three categories according to the National Institute of Standards and Technology (NIST) cloud model: Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS). While all clouds are built on real, physical infrastructure of network, storage and computer systems, IaaS provides a conveniently flexible way to configure and control a (usually) virtualized version of that data center class infrastructure. Amazon Web Services’ EC2 is the most well used example of this class of service, with OpenStack’s Nova being an open source alternative example.
PaaS, perhaps the least understood of the three services, provides an application development and delivery environment that is focused on providing the tools (database services, managed auto-scale web services, message buses, object storage systems, etc.) that application developers need without worrying about the underlying infrastructure directly. The benefit here is that developers don’t need to understand the infrastructure or be able to manage it in the way that an IaaS-focused cloud requires. Heroku and OpenShift are two popular solutions.
Finally, the cloud service most familiar to the average user is SaaS — software applications whose computing and storage functions are not local to your device, but rather running on infrastructure that is ‘in the cloud.” Though you might have an interface (browser) on your machine, the bulk of the value of a SaaS service is generally not. Think of DropBox, Microsoft 365 or Google Apps as examples.
Infrastructure-as-a-Service(IaaS)
If we start at the foundational level, we can talk about Infrastructure-as-a-Service. The IaaS space is clearly exemplified by tools like the OpenStack middleware or Amazon Web Services ‘as a service’ platform. IaaS takes the classic systems infrastructure that an IT department might have managed in the past and enables it in an on demand version of that exact service and capability. Once an IaaS cloud is live, users have access to manage (create, delete, and update) the infrastructure components in a datacenter that were typically the exclusive domain of IT departments. Users can can create their own virtual, but fully functional computers, configure them as desired, provision and attach storage systems and network these resources as they see fit.
IaaS takes the classic systems infrastructure that an IT department might have managed in the past and enables… Click To Tweet
IaaS Business uses:
IaaS used in cases where there is a need to have the ability to rapidly scale up (or down) resources available to an application or even entire system or even rapidly deploy and remove an entire solution. As long as the expertise to manage the system is available in house- or can be hired as a service — IaaS systems can be used as allow your application to add additional servers to handle load — automatically, or to create a test environment that replicates the exact production environment, and remove it after the testing is done. Worst case, an IaaS can provide you with DR (disaster recovery) abilities as rebuilding an entire infrastructure is now automatable and can bring you from down to up in a matter of minutes or seconds.
IaaS Pros:
The main benefit to users is the ability to nearly instantly get access to resources: another computer, more storage, operating system and configurations of your choice in an on-demand fashion against a shared pool of resources that enables rapid elastic use of the infrastructure.
IaaS Cons:
On the down side users are now also responsible for ensuring that security and on-going maintenance of their newly deployed infrastructure, something that they may be woefully underprepared to manage. Users are also responsible for releasing unused resources, a process that can be simple if the infrastructure configuration and deployment is automated, but much like the light in the living room, it’s easier to turn it on than it is to remember to turn it off…
Platform-as-a-Service(PaaS)
The platform concept takes the cloud on-demand and elastic resources models and applies them to the services used to build an application, letting the developer focus on the application rather than the rest of the underlying infrastructure and services components. So rather than starting a compute server, and configuring the application deployment stack (like the Apache web server), the developer just deploys their web application code onto a web platform that provides an apache based environment. One could acquire a database-as-a-service as a part of a platform offering, and the application developer would just manage the tables and data within the database rather than the database application and supporting compute and storage services.
Tools like Heroku or OpenShift provide one key variant of the Platform-as-a-Service model, highlighting a Model-View-Controller application development paradigm platform. Content Management Tools like WordPress, Drupal, and Django provide another PaaS model that uses content as the medium, and the platform aspects is the support of presenting content (text, video, audio) without the content creator having to focus on how to create the rest of the infrastructure and services needed to deliver it. In either of these PaaS models, the underlying infrastructure is fairly hidden from the developer. They write the application or create the content while the PaaS system makes sure the infrastructure pieces underneath can present the results and scale to meet any end user scale of use.
PaaS Business uses:
If your business is building or running a software based solution and the application fits into the model of the PaaS systems service offering, then PaaS is an attractive option. Focused on ease of software or content development and management, businesses large and small benefit from being released from the concerns of managing the infrastructure their apps are running on and can focus on the job of improving the software itself.
With PaaS, your application developers can focus on building code and not worry about configuring and managing… Click To Tweet
PaaS Pros:
With PaaS, your application developers can focus on building code and not worry about configuring and managing the underlying infrastructure. It is possible to develop, test, deploy and update your applications on a PaaS system without needing to have an infrastructure expert on staff.
PaaS Cons:
If your developers need access to the underlying infrastructure, this may not be an option with many of the PaaS systems.
Software-as-a-Service (SaaS) To complete our review of the basic XaaS offerings (i.e. X-as-a-Service) , we are left with Software-as-a-Service. The key value of software as a service is the elastic on-demand delivery and access to a specific application or software service directly to the business or end user. A classic example would be Google Mail, Expensify or Salesforce.com providing software tools (email, expense management, and CRM respectively) for individual users or entire business operations in that same on-demand/self-service and elastic model. They’re available against an elastic pool of resources (what a resource means in this context is really dependent on the software in question) so I can scale up or scale down my service consumption against these systems.
SaaS Business uses:
More and more businesses are moving toward this model as both merchants or end users of software services. With SaaS, the IT issues of patches and updates is gone, it is typically possible to pay for what you need rather than buying and maintaining fixed numbers of licenses that can go up in quantity, but rarely go down. In the case of software that had a pricey unit cost, a SaaS offering may have a much more reasonable price tag, and the potential to manage service costs by disabling or returning licenses is a welcome shift in the IT consumption models that are considered normal for purchased software licenses.
With SaaS, the IT issues of patches and updates is gone Click To Tweet
SaaS Pros:
As a consumer of a SaaS offering, you can now just use the software. It is possible to provide a software service such as an online catalog of software services and only use the subset of “resources” needed out of the total service provider pool of resources. Scale is not a concern for the user, and neither should maintenance/updates or new feature additions. If your Salesforce customer order list substantially increases due to a major new contract, the infrastructure behind your software will scale to meet the demand, but as the end user, you shouldn’t worry about it, and in fact, worrying won’t help, as the end user has no control over the underlying services that make up the end-user view of the system.
SaaS Cons:
Users of SaaS offers have little control over the underlying infrastructure and components, and the same level of control over how they are deployed or maintained. For example, it is not possible to control or even know how much compute processing power is supporting a particular instantiation of a SaaS tool used by any individual tenant. If you are used to buying a piece of software and owning it for eternity, you are now effectively renting access and access is gone if you do not continue renting, along with access to your data unless you migrate away from the service before relinquishing your previously consumed resources.
You should now have a good grasp of what SaaS, PaaS, and IaaS are and the potential business value they provide. While “SaaS app” is now almost a household term, using a PaaS or IaaS are a bit more driven by in-house technical capabilities and capacity. In any case, your particular use case will drive which, if any, of the three cloud service you will decide to implement to achieve your business goals.
You can find more from me and the team at Kumulus Technologies at https://kumul.us/blog or our short format cloud technology videos at https://youtube.com/fiveminutesofcloud
Originally published at Kumulus Technologies — kumul.us.
@rstarmer, OpenStacker, Cloudifier, Containerizer, @kumulustech Founder, Coffee Snob
6 
6 
6 
@rstarmer, OpenStacker, Cloudifier, Containerizer, @kumulustech Founder, Coffee Snob
"
https://medium.com/nttlabs/instance-per-pod-bcbfb3ae2985?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
Although Kubernetes pods are already hardened in several ways by default, the pods are still unprotected from potential vulnerabilities of the runtimes (kubelet/CRI/OCI), the kernel, and even the hardware.
To mitigate such container-breakout attacks, I’m now working on a project named “Instance-per-Pod Webhook”, which automatically creates IaaS instances to avoid having multiple pods on same node.
While Instance-per-Pod Webhook itself doesn’t prevent any breakout attack, it can prevent a compromised pod from gaining privileges for other pods. It can even mitigate hardware vulnerabilities when dedicated IaaS instances are used, e.g. EC2 i3.metal, Azure Dedicated Host, or Google Compute Engine Sole-tenant Node.
Instance-per-Pod Webhook is implemented as a Kubernetes Mutating Admission Webhook, which can trap the Kubernetes control plane to inject custom configuration to the pod resources.
You can find the code here:
github.com
Instance-per-Pod Webhook injects custom tolerations , nodeAffinity , and podAntiAffinity to a Pod manifest so that Kubernetes Cluster Autoscaler will scale out the cluster and create a dedicated Node(IaaS instance) for that pod.
As the whole IaaS operation is handled by Cluster Autoscaler, Instance-per-Pod Webhook is completely agnostic to the IaaS provider. So it works on any cluster on any cloud that is supported by Cluster Autoscaler, including AWS, Azure, Google Cloud, and OpenStack.
Instance-per-Pod Webhook can even work with managed Kubernetes clusters such as Google Kubernetes Engine (GKE).
To use Instance-per-Pod with GKE, a node pool needs to be created with “Enable autoscaling”. The minimum number of nodes can be set to an arbitrary number. Smaller number is preferred for minimizing the cost, larger number is preferred for minimizing pod startup latency. The maximum number of nodes (= the maximum number of Instance-per-Pod pods) can be set to an arbitrary number as well.
The node pool also must have Kubernetes node label ipp=true and node taint ipp=true with NO_SCHEDULE effect. The label name and the taint name may change in future releases of Instance-per-Pod Webhook.
Then checkout the code (v0.1.1) from the repo:
The Webhook can be installed to the cluster as follows:
To create a pod with the Webhook enabled, you need to set ipp-class label to an arbitrary unique string. Note that pods with the same label value can get co-located on the same node.
The pod will be launched on a newly created node, after a few tens of seconds.
In the above example, the pod foo-78f895cd4-wt2fz was launched on the node***-ipp-nodepool-9e0ac2b5-ts6p . The overhead on the pod startup latency to create the node was 46 (=70–24) seconds.
Instance-per-Pod Webhook can be uninstalled from the cluster anytime, just by deleting theMutatingWebhookConfiguration resource and the ipp-system namespace:
Kata Containers is an OCI runtime implementation that creates a virtual machine per a pod, using QEMU, NEMU, or Firecracker.
Kata Containers might be cheaper than Instance-per-Pod Webhook with regard to the IaaS fees, because you can safely co-locate multiple pods on a single IaaS instance, if you trust the VM and the CPU. Also, the pod startup latency is only a few seconds, while Instance-per-Pod Webhook requires a few tens of seconds.
However, Kata Containers require either bare metal instances (e.g. EC2 i3.metal) or instances with support for nested virtualization (e.g. Azure Dv3, Google Compute Engine with a special license). Not all IaaS providers offer such instances. Also, nested virtualization can be slow.
Coincidentally, Amazon recently announced a commercial product that is very similar to Instance-per-Pod Webhook: “Amazon EKS on AWS Fargate”.
EKS-on-Fargate seems implemented as a Mutating Admission Webhook that injects custom schedulerName to Pod manifests, while Instance-per-Pod Webhook injects custom tolerations , nodeAffinity , and podAntiAffinity , without replacing the scheduler.
The current version of EKS-on-Fargate seems using Xen-based virtualization infrastructure, which is probably just same as the plain old EC2 infrastructure (but doesn’t show up in EC2 Console). Starting a pod takes several tens of seconds as in starting an EC2 instance, but probably this will be improved when they migrate to the new Firecracker-based infrastructure in the near future.
Although EKS-on-Fargate strongly isolate pods using Fargate instances, it doesn’t support privileged pods for some reason. And yet it doesn’t support DaemonSets. On the other hand, Instance-per-Pod Webhook works well with any kind of pods.
Instance-per-Pod Webhook strongly isolates pods by creating a dedicated IaaS instance per a pod. Instance-per-Pod Webhook works on any cluster on any cloud.
NTT is looking for engineers who work in Open Source communities like Kubernetes & Docker projects. If you wish to work on such projects please do visit our recruitment page.
To know more about NTT contribution towards open source projects please visit our Software Innovation Center page. We have a lot of maintainers and contributors in several open source projects.
Our offices are located in the downtown area of Tokyo (Tamachi, Shinagawa) and Musashino.
NTT Open Source
12 
12 claps
12 
NTT Open Source
Written by
A maintainer of Moby (dockerd), containerd, and runc. https://github.com/AkihiroSuda
NTT Open Source
"
https://medium.com/cloud-and-servers/bir-i%CC%87ki-c%C3%BCmle-i%CC%87le-aws-konsolu-ve-s%C4%B1k-kullan%C4%B1lan-servisleri-b2a1c8264dd5?source=search_post---------112,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Servis konsoluna giriş yaptığınızda servislerin mantıksal işlevlerine göre bölümlendiğini görebilirsiniz.
Her servisin altında 1 cümle ile servisin ne işe yaradığı anlatılmıştır. Bunlardan sıkça kullandıklarını düşündüklerimi aşağıda kısaca açıklayacağım.
AWS’nin altyapılarındaki işlemci ve bellek gücününün sunulduğu servisler burada yer alır.
EC2 (Virtual Servers in the Cloud) : Bulut’taki sanal sunucu hizmetidir.
EC2 Container Service (Run and Manage Docker Containers) Docker Container’ları yönetmenizi ve çalıştırmanızı sağlayan bulut hizmetidir.
Elastic Beanstalk (Run and Manage Web Apps) AWS’nin sunduğu PaaS(Platform As A Service) hizmetidir. Java, Node.js, Phyton, Ruby, PHP vb.. bir çok dilde WebApp veya Worker oluşturup çalıştırmanızı sağlar.
Lambda (Run Code in Response to Events) AWS Eventleri tetiklendiğinde, bu eventler ile ilgili belli fonksiyonları tetikleyebilme imkanı verir. Örneğin S3 bir dosya yazıldığında..
AWS’nin depolama ile ilgili sunduğu servislerdir.
S3 (Scalable Storage in the Cloud) 1byte — 5TB boyutundaki dosyalarınızı bir tekil kimlik ile saklayıp, geri alabildiğiniz Object Storage servisidir.
CloudFront(Global Content Delivery Network) Statik içeriklerin(html,css, resim, müzik, video vb.) içeriğin içeriğe ihtiyaç duyan kişiye en yakın coğrafi posizyondan gönderilmesini sağlayarak veriyi en performanslı şekilde kullanıcıya ulaştırır. AWS’nin Edge Location merkezleri CDN içeriklerini sağlayabilmeleri için oluşturulmuş fiziksel veri saklama alanlarıdır.
AWS’nin sunduğu veritabanı hizmetleridir.
RDS (Managed Relational Database Service) İlişkisel veritabanlarını Örneğin MySQL, MSSQL, Oracle vb.. servis olarak sunduğu servisidir. Multi-AZ replikasyon yeteneği, otomatik backup oluşturması size bir çok kolaylık sağlar.
DynamoDB (Managed NoSQL Database) AWS’nin sunduğu NoSQL servis hizmetidir. Document ve Key-Value Data Modellerini destekler.
ElastiCache(In-Memory Cache) Bellek’teki cache sistemidir. Size Redis veya Memcache ürünlerinden birisi seçmeniz beklenir.
Redshift(Fast, Simple, Cost-Effective Data Warehousing) Petabyte ölçeğindeki verileri tutan Veri Ambarı servisidir.
Route 53 (Scalable DNS and Domain Name Registration) AWS ‘nin Domain Name Servis hizmetidir. İsmini DNS’in portu olan 53'den almıştır. Domain Name satın almanızı sağlarak DNS Recordları girebileceğiniz bir arayüz, servis sunar.
CloudWatch (Monitor Resources and Applications) Uygulamalarınızın ve kullandığınız servisleri ne kadar kullandığınızı görüntülemenizi sağlar ve belli limitler tanımlayıp bu durumların gerçekleşmesi durumunda size alarm vermesini sağlatabilirsiniz.
CloudFormation(Create and Manage Resources with Templates) AWS kaynak ve servislerini JSON template’leri tanımlamanızı ve bunları basitçe oluşturulmasını sağlar.
IAM (Identity & Access Management) AWS Servislerini kullanacak Group, User,Role ve Policy(Politikalar) tanımlamanızı sağlar. Servis-Servis arası yetkilendirmeden tutun’da o yetkiyi kullanacak role ve kişilere kadar tanımlamanın yapılabilmesini sağlar.
Elastic Search Realtime stream veriler üzerinde analytic yapmanızı sağlayan AWS Servisidir. Kibana Dashboard sayesinde akan veriler üzerinde istediğiniz Dashboard oluşturabilirsiniz.
Kinesis Farklı kaynaklardan gelen stream veriyi(website tıklamaları, finansal işlemler, sosyal medya twit veya bildirileri, IT logları ve lokasyon takip bilgileri) gibi terebaytlarca veriyi yakalayıp saklamanızı sağlarlar.
AWS iOT Nesnelerin interneti ile ilgili sunduğu AWS hizmetleridir.
SNS Asıl amacı IOS ve Android için Push Notification gönderme altyapısı sunan AWS Servisidir. Bunun haricinda SMS, EPosta gibi bildirimleride yapmanıza imkan sağlar.
API Gateway Güvenli ve yüksek ölçeklendirilebilir RESTfull API’ler oluşturabilmenizi sağlayan bulut hizmetidir.
SES ePosta gönderme/alma için sunucu yerine direk Simple Email Servisi kullanarak ePosta ile ilgili işlemlerinizi güvenli bir şekilde yapabilirsiniz.
SQS RabbitMQ, ActiveMQ, Kafka vb.. kuyruk yapılarını yönetmek yerine buluttan kuyruk hizmetini servis olarak alabilirsiniz.
Uzun süredir farklı sektörlerde (Askeri, Telekomünikasyon, Devlet, Bankacılık, Sigortacılık, Tübitak, SaaS) yazılımlar geliştiriyorum. Bu süreçte Havelsan, Milsoft, T2, Cybersoft ve Thundra firmalarında yönetici ve yazılım mühendisi olarak çalıştım. Deneyimlerimi ve teknolojik bilgi birikimi mi olabildiğince OnurDayibasi.com adresinde toplamaya çalışıyorum. Tüm yazılarıma ve daha fazlasını bu site üzerinden erişebilirsiniz.
AWS, Azure, OpenStack
10 
10 claps
10 
AWS, Azure, OpenStack
Written by
Senior Frontend Developer at Thundra
AWS, Azure, OpenStack
"
https://medium.com/@gleonhard/futurist-gerd-leonhards-keynote-at-iaamobility-2021-digitisation-decarbonization-and-reformation-aabc436ade51?source=search_post---------115,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Futurist Gerd Leonhard
Sep 27, 2021·3 min read
This was my first large REAL-LIFE (i.e. not virtual) event since late February 2020 (when the Covid19 crisis started). It was a blast to be back on the stage, and the location / stage was amazing as well (check out the gifs below). The talk was held in what I call The Future Show format — extra wide designs that look amazing and really make a difference for the audience (see more gigs like this, here)
At this event, I remembered that eight years ago me and a few other futurists were invited to a big event near Stuttgart, for a leading German car company. We talked about electric cars, shared vehicles, autonomous driving, and the future of mobility. And in the room of some 50 leading executives …we were practically laughed-off. Who would want to share their car? In Germany? Share my car… you must be joking! Not own a car? What?? And yet, here we are eight years later, and the biggest topic at this event was NOT the car — it was mobility as a service ( MaaS). Turns out, after all, FORESIGHT is mission-critical.
As I keep saying: Business as usual is dead … or dying. And again, that is scary, but it’s also invigorating. Obviously the people who run this show have started to understand this; otherwise, it wouldn’t be called IAA Mobility.
This was also one of the first talks on a new topic that I call ‘The DDR’ (German readers will understand the pun): Digitization, Decarbonisation, and Reformation (defined as “the art of making an improvement, change your behavior, changing the structure*) — this is quickly becoming a core topic for me (see my new talk in Moscow, a week later).
Watch the full keynote on ‘The future of mobility’ here, or watch / download it on Vimeo.
“Science fiction is becoming science fact. The flying taxi — that’s it’s going to show up here at this event shortly. The fully recyclable car, also presented there by BMW: that sounds like straight from science fiction…”
“As machine learning is exploding, on the flip side, we have this: we have the things that make us human. Try for machine to do those things. To have empathy? For a machine? Machine doesn’t exist. Values, consciousness?… This is our future — awesome humans on top of amazing technology”
Listen to the audio-only version of this keynote on SoundCloud, Apple Podcasts, Amazon Podcasts and Spotify
Originally published at https://www.futuristgerd.com.
#KeynoteSpeaker, #Futurist and #Humanist. Bestselling Author: #Technology vs Humanity. Get My 10 #Future Altering #Tech Megashifts futuristgerd.com/FREE
See all (11,713)
6 
6 claps
6 
#KeynoteSpeaker, #Futurist and #Humanist. Bestselling Author: #Technology vs Humanity. Get My 10 #Future Altering #Tech Megashifts futuristgerd.com/FREE
About
Write
Help
Legal
Get the Medium app
"
https://blog.insiderattack.net/a-resource-and-policy-aware-vm-scheduler-for-medium-scale-clouds-6dbda809f02a?source=search_post---------82,"Cloud computing enables providing computing resources over an Internet connection to the users. In an IaaS (Infrastructure as a Service) Cloud, these computing resources include Processing power, Memory, Storage and Network resources. Cloud computing technologies including Infrastructure as a Service, Platform as a Service, and Software as a Service have changed the traditional “host on own data center” strategy and have given a good solution to prevent maintenance overhead of a private data center per organization. Medium-scale IaaS clouds are becoming more popular in small/medium scale organizations such as universities and enterprises. Medium-scale IaaS clouds are useful when organizations need to deploy their own private cloud using the compute, storage and network resources they have. There are several IaaS platforms such as OpenStack, CloudStack and Eucalyptus which users can use to deploy their own medium-scale clouds. These tools handle sharing and management of compute, storage and network resources dedicated to the cloud and perform resource allocation for various requirements.
Many universities and enterprises are now setting up their own small-to-medium scale private clouds. Such private cloud are becoming popular, as they provide the ability to multiplex existing computing and storage resources within an organization while supporting diverse applications and platforms. It also provides better performance, control, and privacy. In a medium-scale cloud such as a university or enterprise cloud, there are different types of users including students, lecturers, internal/external researchers, and developers, who get benefits from the cloud in different ways. They may have varying requirements in terms of resources, priorities, and allocation periods for the resources. These different requirements may include processing intensive and memory intensive applications such as HPC (High Performance Computing) applications and data mining application, as well as labs which needs to be deployed on a specific set of hosts and for a particular period of time. Priority schemes and Dynamic VM (Virtual Machine) migration schemes should be used to satisfy all these requirements in an organized manner. However, currently known IaaS cloud platforms have no native capability to perform such dynamic resource allocations and VM preemption mechanisms. Therefore, it is important to extend existing cloud platforms to provide such policy, resource, and deadline aware VM scheduling.
We are proposing a resource scheduling mechanism which can be used as an extension to an existing IaaS cloud platform to support dynamic resource and policy aware VM scheduling for Medium Scale Clouds. Resource scheduling algorithm schedules VMs while being aware of the capabilities of the cloud hosts and current resource usage, which is monitored continuously using a resource monitor. The resource scheduler will allocate resources according to predefined priority levels of a particular user who issued the resource request. Time-based scheduling (e.g., deploying labs for a particular period of time) is also performed while considering the priority levels and existing resource allocations. To provide these features we extend an existing IaaS cloud platform to include resource and policy aware VM creation, migration, and preemption.
Apache CloudStack is an open source IaaS platform widely being used for building medium scale clouds. CloudStack is easier to setup compared to alternatives such as OpenStack and Eucalyptus because of its monolithic architecture. CloudStack management server can be installed on a single machine easily whereas OpenStack has different component which need to be seperately installed and configured which required expertise. CloudStack provides a rich web UI and also simpler RESTful API for 3rd party tools integration. And also it implements an amazon EC2 API compatible interface as well.
KVM hypervisor is an open source kernel based virtualization scheme which can be used to create virtual machines on top of a shared physical host. The reason for our interest in KVM to be used with CloudStack was KVM provides capability to take snapshots of a running VM including its disk and memory using libvirt API. Memory snapshot is not provided by other supported open source hypervisors of CloudStack such as XCP (Xen Cloud Platform) though XenServer, which is the commercial version of Xen provides VM memory snapshots. Taking VM snapshots is a requirement in our solution because, we need to save state of a VM and restore it later. This is called preemptive scheduling in which high priority request will preempt a low priority request if there are lack of resources on the cloud.
Zabbix is a free and open source resource monitoring system for clouds and grids. It is highly scalable and can be used for resource monitoring of small to large scale clouds consists of up to 100,000 monitored devices. Zabbix also provides a RESTful API and a good web UI which can be used to monitor resource utilization and availability of a set of monitored devices. It provides graphical representations as well as detailed information on monitored devices.
Our Smart Cloud Scheduler (SCS) communicates with CloudStack and Zabbix Monitoring system through the APIs they have provided. SCS itself provides a REST API and a Web for the users using which they can send Resource Allocation Requests to the cloud. User first have to compose his Resource Allocation Request in a structured format (will be discussed later) and send it to SCS. SCS then queries Zabbix Resource Monitor to fetch latest information on resource utilization and availability on the Cloud. Based on resource availability, SCS takes decision on how to allocate the request and on which host should the request be allocated. Once these decisions are taken, they are executed by SCS on CloudStack using CloudStack API.
Overview of Smart Cloud Scheduler
Source Code of Smart Cloud Scheduler is located at GitHub and can be accessed by URL:https://github.com/dpjayasekara/VirtualOps
Following diagram represents the high level architecture of our system. This architecture diagram clearly shows how our resource scheduler works in the middle of Zabbix Resource Monitoring system and CloudStack IaaS framework and performs the coordination among them. A user can submit a query to the scheduler through Web Frontend or using the API endpoint provided by the scheduler. When the request is validated, authenticated and authorized, authentication service forwards an authorized and prioritized request to the core scheduler.
Smart Cloud Scheduler is a Node.js implementation which uses a MongoDB database for storage. We are using Mongoose as the Node.js driver for MongoDB and ‘csclient’ Node.js module for CloudStack API calls. Authentication service also provides authorization service for Smart Cloud Scheduler. It also uses MongoDB to store user account information. Core scheduler is the main component in above diagram which encapsulates the core functionality of the resource scheduler.
Following diagram illustrates the internal component based architecture of the Core Scheduler.
A user can issue a resource allocation request using either web frontend or the REST API. The request sent by the user is authenticated and authorized and tagged according to the user’s priority and asking priority for the job. This authorized and prioritized request is then sent to the Host Filter which fetches latest resource information from Zabbix in order to determine which hosts to be chosen to allocate the incoming request. If there are hosts on which this request can be directly allocated, then the request is passed to VM scheduler and allocation is happened on the hosts selected by the Host Filter.
If there are no hosts to allocate the current request, it is then sent to Priority Scheduler. Priority Scheduler then sends this request to Migration Scheduler. Migration Scheduler tries to obtain space for the request on any of the hosts on the cloud by re-shuffling VMs on the cloud. If it can find space to allocate the request, it returns Priority Scheduler the hosts on which it generated space. Priority Scheduler then passes this information to VM scheduler and VM allocation is performed by VM scheduler on the selected hosts.
If migration scheduling is not possible, Priority Scheduler then forwards the request to the Preemptive Scheduler. Preemptive Scheduler checks the priority of the incoming request, then checks whether there are previous allocations on the cloud with lower priority than the incoming request. If there are low priority allocation, it checks whether enough resources for the incoming request can be released by preempting some of those VMs. If it is possible, Preemptive Scheduler performs preemption of those VMs and returns the hosts on which the resources have been released. This host is then used to allocate the new request.
In the next post about Smart Cloud Scheduler, I will describe each component of the SCS and will discuss current implementation status, limitations and future improvements.
Learn, Teach, Repeat
Written by
Staff Engineer, Founder of bibliocircle.com, Interested in Cyber Security, Distributed Systems and Networking
Learn, Teach, Repeat
Written by
Staff Engineer, Founder of bibliocircle.com, Interested in Cyber Security, Distributed Systems and Networking
Learn, Teach, Repeat
"
https://medium.com/@joykathleen09/global-iaas-industry-2016-market-research-report-7f4071e2827a?source=search_post---------262,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kathleen Joyner
Oct 24, 2016·2 min read
Global IaaS Market 2016 Industry Research Report
The Global IaaS Industry 2016 Market Research Report is a professional and in-depth study on the current state of the IaaS industry.
Firstly, the report provides a basic overview of the industry including definitions, classifications, applications and industry chain structure. The IaaS market analysis is provided for the international market including development history, competitive landscape analysis, and major regions? development status.
Secondly, development policies and plans are discussed as well as manufacturing processes and cost structures. This report also states import/export, supply and consumption figures as well as cost, price, revenue and gross margin by regions (United States, EU, China and Japan), and other regions can be added.
Then, the report focuses on global major leading industry players with information such as company profiles, product picture and specification, capacity, production, price, cost, revenue and contact information. Upstream raw materials, equipment and downstream consumers analysis is also carried out. What?s more, the IaaS industry development trends and marketing channels are analyzed.
Finally, the feasibility of new investment projects is assessed, and overall research conclusions are offered.
In a word, the report provides major statistics on the state of the industry and is a valuable source of guidance and direction for companies and individuals interested in the market.
Click here to get TOC@ http://www.appliedmarketresearch.com/reports/global-iaas-industry-2016-market-research-report.html#table-of-content
Table of Contents
Chapter One Industry Overview of Two IaaS 1.1 Definition and Specifications of Two Way Radio Equipment1.1.1 Definition of Two Way Radio Equipment1.1.2 Specifications of Two Way Radio Equipment1.2 Classification of Two Way Radio Equipment1.3 Applications of Two Way Radio Equipment1.4 Industry Chain Structure of Two Way Radio Equipment1.5 Industry Overview and Major Regions Status of Two Way Radio Equipment
Chapter Two Manufacturing Cost Structure Analysis of IaaS 2.1 Raw Material Suppliers and Price Analysis of IaaS 2.2 Equipment Suppliers and Price Analysis of IaaS 2.3 Labor Cost Analysis of IaaS 2.4 Other Costs Analysis of IaaS
Get Free Sample@ http://www.appliedmarketresearch.com/reports/global-iaas-industry-2016-market-research-report.html#sample
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-functions-best-practices-using-runtime-configurator-to-manage-config-variables-8b18e77bd6dc?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
If you’re deploying any serious workloads on GCP, chances are you’re using multiple compute options to serve different needs, i.e., IaaS (GCE), PaaS (GAE), Containers (GKE) and Serverless (GCF). And chances are all these deployments share and access some common config variables.
This is where GCP’s Runtime Configurator can be incredibly useful. It allows you to save (and update) all your config variables at one central location which can then be accessed by all GCP compute resources (GCE, GAE, GKE and GCF) via simple APIs.
The only catch using Runtime Configurator within GCF’s serverless environment is you don’t want to be fetching config variables via the Runtime Configurator APIs upon every function invocation. That would degrade performance, escalate costs and probably exhaust the quotas for Runtime Configurator APIs.
Luckily, GCF tries to either preserve the state (less likely) or recycle the execution environment (more likely) of a previous function invocation and encourages the use of global variables to reuse objects in future function invocations as a best practice. So we can actually save the config variable values we get by calling the Runtime Configurator APIs in a global variable and reuse this global variable instead of calling Runtime Configurator APIs in future function invocations. Below steps exhibit the same.
Create a new config by the name “my-project-config” using the gcloud tool:
gcloud beta runtime-config configs create my-project-config
Create a new variable by the name “app-name” with the value “my app” in the “my-project-config” config using the gcloud tool:
gcloud beta runtime-config configs variables set --config-name my-project-config --is-text app-name ""my app""
Create a new Custom Role with the following permissions from the GCP console:
runtimeconfig.configs.get
runtimeconfig.variables.list
Create a new Service Account with the above created Custom Role from the GCP console and download it’s private key in JSON format in your GCF project’s directory structure.
Finally some code! Below gist shows the correct way to use Runtime Configurator within GCF, i.e., calling Runtime Configurator APIs if and only if config variables are not present in global cache:
You can clone / download the sample GCF project from my GitHub repository here.
Notes:
Resources:
Happy Coding!
P.S. Hire Me
Google Cloud community articles and blogs
70 
2
70 claps
70 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
one life
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/pixboost/clouds-cloud-computing-explained-5e1c8dea14a4?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
Almost everytime someone talks about the Clouds we imagine the clouds in the sky, right? White fluffy skyish ones. Actually, this is the key to the idea of Cloud computing.
Same as the real clouds, imaginary ones are robust and highly productive entities that make our lives so much easier. Let’s explore how to use existing Clouds and how to create them which is also very interesting.
Simply put, the Cloud is a third-party service that you use through the Internet: a database, storage, software, services, anything techy that you set up on your web server and maintain or you buy from other providers.
The amount of data is increasing tremendously; therefore, there is a need to store and process it fast and efficient. Here comes a demand for Cloud computing solutions.
A great example would be your smartphone. There is no way we can fit all the videos and photos anymore. That’s why we have Dropbox, Google Drive or iCloud these days. They provide storage services, we connect to them over the Internet, and store our photos. They charge us similar to an electricity company or a doctor.
The Cloud physically looks like this. This is a Google Cloud Data Center.
There are so many ways you could use the Cloud computing nowadays. I’ll mention just major ones.
So, it does not matter what your job is, a designer, a software developer, a business owner or even a busy mum or dad, we all can benefit from Cloud computing. It makes our life more comfortable and our work faster.
For instance, if you are a 3D designer and your task is to create a complicated scene, a good idea would be to do all your rendering in the Cloud and not on a personal computer. Such solutions like Render Rocket would do it for you without a fuss.
Let’s go more technical. There just three types of services that IT companies provide:
As an example, Pixboost Image CDN is a PaaS service that helps websites with processing their images efficiently. It provides a set of endpoints to do real-time image scaling and optimization.
Basically what you need is to own a data center first. With a few spare web-servers, it is possible to build the storage or service infrastructure. You can also borrow machines from other companies who lease them for these purposes (like DigitalOcean).
The next and final step would be setting it all up with appropriate software. Look up such tools as Nextcloud, ownCloud, Seafile or similar to start a journey.
Obvious downsides of this approach are money, time and skill that you have to acquire to set it all up. However, all the advantages are clear, and you are able to manage your infrastructure the way you want, acquire the highest level of security and save some money.
Hopefully, this overview of the whole Cloud idea would be helpful. If you have any questions or thoughts, please don’t hesitate to post here in comments or contact me on Pixboost Image CDN website. Cloud computing can help us in so many ways, and please consider using it for your work or personal needs.
Website Performance Optimization
62 
62 claps
62 
Website Performance Optimization
Written by
Clinical Hypnotherapist & Strategic Psychotherapist. Personal Blog. My Thoughts on Clinical Aspects of Mental Health. Wow stuff -> http://bit.ly/PsyNewsletter
Website Performance Optimization
"
https://medium.com/rocksetcloud/what-is-a-cloud-database-iaas-paas-saas-dbaas-explained-2c77949c889a?source=search_post---------238,"There are currently no responses for this story.
Be the first to respond.
For many organizations, the advantages of a cloud-based database are clear. They offer scalability, security, and availability. There can also be cost savings over custom and on-premises database solutions.
However, not all cloud databases are created equal. Terms like IaaS, PaaS and SaaS have traditionally been used to describe various levels of cloud computing, but how do they apply to cloud databases? And what to make of DBaaS (database as a service) offerings?
In this post, we’ll take a look at the main differences between these categories of cloud databases, along with the pros and cons of each, to help you decide which is most appropriate for your organization.
As Figure 1 shows, the primary difference between software as a service (SaaS), platform as a service (PaaS), infrastructure as a service (IaaS) in the cloud computing contextis the level of abstraction over underlying resources and services. In general, the more that the service provider abstracts away from the user, the simpler and faster it would be for the user to generate value from the cloud service. And the less that the provider handles on behalf of the user, the greater the control and responsibility the user would have over the environment.
We can apply the principles behind this cloud computing taxonomy to cloud databases as well. As with most cloud-based tools and services, your choice of database will reflect your team’s go-to-market requirements, expertise and skill set, the overhead and administrative burden you’re willing to take on, and the degree of customization your projects require.
Let’s look at a general overview of each type of cloud database, along with their pros and cons.
An IaaS database implementation is one that is self-managed on cloud infrastructure. You would have responsibility for the OS, runtime and database software, along with installation, configuration and ongoing maintenance. The experience would be similar to running on-premises, except for the use of cloud servers and storage. An IaaS database offers the most control over your setup, and that naturally comes with more responsibilities.
No, you don’t have to requisition hardware and operate your own data center with an IaaS database. However, you do need to select a cloud provider infrastructure, and then install and manage the database yourself within the provider’s parameters.
If you’re willing to put in the work, IaaS gives you all the power-and the maintenance burden-of a traditional database. Let’s look more directly at the pros and cons of IaaS.
IaaS Pros
IaaS Cons
A database offered as PaaS takes the cloud advantage a step further. PaaS databases help automate provisioning, configuration, scaling and other cluster management tasks.PaaS can free your team from setting up and managing infrastructure, which they would need to do with IaaS.
PaaS is the middle option in our IaaS-PaaS-SaaS continuum, and many cloud databases are offered in this manner. They make it easier for teams to manage their databases through cluster automation tools. However, users of PaaS still need to have some awareness of cluster details, such as the number and types of nodes, capacity and sharding.
Let’s look at what this means when it comes to the benefits and drawbacks of a PaaS database.
PaaS Pros
PaaS Cons
SaaS is easily the most familiar software delivery model for cloud applications: just log in and start working. We all use SaaS applications, sometimes dozens of them in a given day-ranging from Google Docs to Salesforce CRM. SaaS is far less familiar when it comes to databases, but we’ll use this term to describe databases that take all the responsibility of cluster operations off the user.
SaaS databases are generally the easiest type of database to set up and maintain. That’s because the service provider is responsible for handling all technical issues, planning, provisioning, and other routine tasks. In other words, the database is fully operated by the provider, and all cluster details are abstracted away from the user. However, as you might expect, the tradeoff is you may have fewer deployment and configuration options with a SaaS database.
Let’s look at these pros and cons in more detail.
SaaS Pros
SaaS Cons
There’s one more term we haven’t addressed so far-Database as a service (DBaaS). It typically refers to databases offered as PaaS or SaaS.
All major cloud platforms now offer DBaaS solutions. Some of these are closer to SaaS; others are slightly closer to PaaS. Popular examples of DBaaS solutions include Amazon Relational Database Service (RDS), Azure SQL Database, MongoDB Atlas and Amazon DynamoDB.
Those closer to PaaS take some ops aspects out of your hands while giving you an experience similar to self-managed versions of those databases. On the other hand, solutions that are closer to SaaS offer an out-of-the-box database solution that makes it easy to add powerful database functionality to your app. This can be an excellent option for organizations looking for the shortest path to get data applications up and running quickly and easily.
We’ve seen that there’s a broad spectrum of cloud database options available. They vary in their ease of use, granularity of control and how they utilize resources.
An easy way to summarize these layers of database solutions is that IaaS provides more control than PaaS, which provides more control than SaaS. This is illustrated in figure 2.
Unsurprisingly, IaaS requires the most effort on the part of your ops team, followed by PaaS, which still requires manual intervention for cluster management tasks. Because of the greater administrative burden that comes with IaaS and PaaS, this also means more effort needs to be put into optimizing for cost and performance at these levels. Finally, SaaS requires the least of your team. A key benefit of databases offered as SaaS is that they let small teams do more without having to grow the team.
When a SaaS database is designed for the cloud, it can take maximum advantage of cloud elasticity and the disaggregation of compute and storage resources. This can actually result in more efficient performance or utilization.
Originally published at https://rockset.com on Oct. 15, 2021.
Reimagining data-driven applications
1 
1 clap
1 
Written by
Rockset
Reimagining data-driven applications
Written by
Rockset
Reimagining data-driven applications
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/certifried-it/cloud-service-types-understanding-iaas-paas-and-saas-af238029bcc8?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
Whether you are studying for a cloud certification or working to migrate on-premises IT to the cloud, three terms you will constantly hear are IaaS, PaaS, and SaaS. These three concepts can be easy to confuse at first, yet they are the backbone of the cloud model. no matter which vendor you choose.
"
https://medium.com/@heliossolutionnl/microsoft-azure-is-de-nummer-twee-in-marktaandeel-in-cloud-iaas-na-amazons-aws-17fc31996764?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
Helios Solutions
Mar 23, 2017·2 min read
Microsoft Azure is groeiende in de publieke cloud business sinds de start in 2010. Ondertussen staat het op nummer twee achter de leider in de industrie Amazon Web Services. Hoewel Amazon de cloud leider is, groeit Azure op een sneller tempo, waardoor industrie-experts zich afvragen of Microsoft uiteindelijk Amazon voorbij zal gaan. Of Microsoft Amazon nu voorbij gaat of niet, het is van het grootste belang voor je organisatie om het voordeel te halen uit het gebruik van de cloud. Werkdanooksamen met een cloud dienstverlener en haal het uitersteuit je organisatie.
Microsoft Azure werd gelanceerd in februari 2010 als Windows Azure na de eerste bekendmaking in oktober 2008. In maart 2014 werd het omgedoopt tot Microsoft Azure. Amazon Web Services (AWS), aan de andere kant, werd gelanceerd in 2006 en had een groot voordeel door als eerste de markt te betreden. Ditresulteerde in eendominanteplek in de cloud industrie met zo’n 30 procentmarktaandeel. Azure heeft de tweedepositie met ongeveer 11 procentmarktaandeel. De totaleopslagcapaciteit van AWS zougrotermoetenzijndan de gecombineerdeopslagcapaciteit van alleandere cloud infrastructuren, wataangeeftwaar Amazon staat in vergelijking tot haarconcurrenten.
zie ook — Waarom is WordPress web development de beste oplossing voor uw onderneming?
Sinds Azure is gelanceerd, geef Microsoft geen duidelijk inzicht in de werkelijke inkomsten van het programma. Echter, de analist van J.P. Morgan heeft een report geschreven in de Wall Street Journal waarin hij schat dat Azure ongeveer 2.7 miljard dollar omzet binnen heeft gehaald in 2016. Daarnaastgeeft Microsoft aandat het inkomstpercentage van Azure 93 procentgestegen is over het vierdekwartaal van 2016. Het inkomstpercentage van AWS steeg in dezelfdeperiode met enkel 47 procent. Microsoft heeftookbevestigddat in het vorigekwartaal de inkomsten met 116 procentzijngestegen.
Uit de bovenstaande discussie blijkt dat Microsoft and haar concurrenten het zwaar gaan krijgen om de cloud-leider Amazon in te halen. Microsoft zou met strategische plannen moeten komen om Amazon uit de weg te ruimen, gezien het aantal klanten, de grootte en het verschil in inkomsten. Amazon zet de huidige trend wat betreft prijs dalingen en Microsoft heeft aangegeven mee te gaan met dergelijke dalingen. Daarnaast gebruikt Microsoft de marges uit andere divisies om de cloud positie te versterken.
Meer informatie over de Software Ontwikkeling Specialist, Outsource SharePoint en SharePoint Specialist en hoe het kan nuttig zijn voor u of uw bedrijf van de experts zijn.
Originally published at https://www.linkedin.com on March 23, 2017.
1 
1 
1 
"
https://medium.com/terri-hanson-mead/google-cloud-platform-gcp-in-life-sciences-qualifying-the-cloud-iaas-paas-6059824bed29?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
As life sciences companies, especially biotech companies, become more and more virtual, they are shifting (or have shifted) to the cloud for applications and infrastructure. It’s the right thing to do, especially since most don’t appreciate the value of technology in optimizing business performance and getting to market faster (separate rant).
Simply throwing the responsibility over the fence to the vendors does not relieve a life sciences company of its obligations around ensuring data quality and integrity.
Savvy life sciences companies know this and qualify their underlying cloud infrastructure, whether on AWS, Google Cloud Platform (GCP), or Microsoft Azure.
The qualification process is more than a compliance activity; it can provide value to the business in demonstrating system and process controls, and ensure data integrity. It’s all about the data, after all.
In this blog post, I share my recommendations for qualifying Google Cloud Platform. I covered the same for Microsoft Azure in a separate post.
For the purposes of this post, I am focusing on IaaS and PaaS, and not SaaS. For validating a SaaS solution, check out this post from 2018 and this post from 2019.
Qualification versus Validation
In this post I will use the terms synonymously but generally speaking, you qualify infrastructure and you validate applications for a company’s intended use.
ISPE GAMP5
I am a GAMP girl so my qualification approach is based on the International Society of Professional Engineer’s guidance documents as detailed in GAMP5 and the associated guidance documents.
CSV versus CSA
FDA released its draft guidance on Computer Software Assurance for Manufacturing, Operations, and Quality System Software and while it was expected to be finalized in 2020, it is now on the docket for fiscal year 2021 per FDA’s Center for Devices and Radiological Health (CDRH).
The guidance document is intended to shift validation approaches from CSV (computer system validation) to CSA (computer software assurance). If you’ve been following a risk-based approach (as defined in GAMP5) and have been using your noggin to focus on intended use and the value add to the business, there’s not much of a shift.
Risky Business
Two years ago I qualified GCP for a client and recently followed a similar approach for Azure for another client this past year. The approaches to both are nearly identical.
It may appear to be largely a documentation effort but it’s more than that. It’s all about defining what is needed for the business, executing against those requirements, documenting the baseline, and maintaining the platform in a controlled fashion.
With IaaS and PaaS (and SaaS), there is a lot of reliance on the vendor, and therefore the customer (the life sciences company) needs to oversee the vendor and the platform to ensure an acceptable level of control based on the company’s risk assessment and risk tolerance. To not do so is a risky business and compliance proposition.
Risk-Based Approach
A risk-based approach was the best thing to happen to computer systems and software when GAMP5 was released. It meant that we could prioritize what we focused on, company by company, system by system, process by process.
This also meant that those folks who liked to apply a cookie-cutter approach to computer validation had to start applying critical thinking to their validation / qualification efforts. It’s not easy if you just want to check boxes and follow checklists.
A risk-based approach offers up flexibility for companies leveraging software and technology. It is not a one-size fits all approach.
This isn’t rocket science but if you have not been through a qualification or validation effort, this will be challenging as there’s a steep learning curve. I have yet to figure out how to get my clients to understand this process until after they’ve gone through it. Expect some discomfort as you go through it for the very first time.
What is Google Cloud Platform (GCP)?
The Google Cloud Platform (GCP) is a suite of clud computing services providing infrastructure as a service (IaaS), platform as a service (PaaS) and serverless computing environments in Google managed data centers. This includes computing, data storage, data analytics, application deployment, and machine learning.
The companies I work with rely on it as the virtual backbone for virtual machines, storage, and other GCP assets to support business applications, databases, and web deployment.
Above and Below the Line
I draw the line at the compute instances, containers, management tools, security, and storage. Anything above that line is at the application level and should be covered under those application validation plans and activities. Below the line is in scope for the GCP platform.
If we think in terms of a bare metal data center, IT historically commissioned the servers and prepared for application and database installation.
This typically included installing things like the operating system, anti-malware software/tools, monitoring tools, backup software or agents, and other baseline tools used by IT. This is all below the line and what I consider in-scope for the qualification of the GCP platform.
Then it’s handed over to the folks managing the application project. What they do, even with the help of IT, is above the line.
Deliverables
Remember, if it’s not documented, it didn’t happen. I highly recommend that all of the validation deliverables be approved and stored in the company’s validated document management system (eDMS).
— GxP assessment for the GCP platform based on its intended use and what is expected to reside on the platform
— Risk assessment for the GCP platform and determination as to whether to audit Google for GCP
— Vendor qualification asessment and possible (paper) audit
— Validation plan for the GCP platform
— Functional requirement specification (combined FRS/FRA/TMX document)
— Functional risk assessment (combined FRS/FRA/TMX document) for the GCP platform
— Technical specification documents for the platform and the virtual machines and other GCP assets on the platform; these provide baseline documentation for operation and control and are created after the migration or the platform / virtual machines / other assets are configured or installed.
— Migration plan or protocol including verification activities
— Executed migration and supporting documentation
— Migration plan summary report
— Testing summary to document any verification activities performed whether automated or manual
— Traceability matrix (combined FRS/FRA/TMX document)
— Procedures (SOPs) and work instructions (WIs) to operate and maintain the GCP platform in a controlled fashion
— Trained administrators on the platform and SOPs/WIs
— Trained personnel on the SOPs/WIs
— Validation plan final report
What is not on the list?
— Installation Qualfication (IQ): there’s nothing to install and therefore nothing to verify
— Operational Qualification (OQ): if you are focusing on qualifying for intended use, performing an OQ provides no added value
— Performance Qualification (PQ): we can debate whether this is necessary or not. My clients have decided not to do any verification testing on the platforms (Azure / GCP) and have left the verification testing to the application layer. Or they have performed automated and manual testing and have summarized in a document that they approve and store with the validation deliverables.
Procedures and Work Instructions
Assuming the SOPs are in place to support validated systems (computer validation, SDLC, change management, backup/restoration, deviation management, training, security/passwords, monitoring, etc.), the following should be considered at a minimum:
— GCP Platform operation and maintenance procedure including monitoring
— Work instructions to support the platform administration including administration of virtual machines and other GCP assets, backup and restoration, user and security administration, etc.
The timeline of the activities will depend on where/what you are migrating from (assuming you are), what is being migrated, the impact to the business, and the available resources to work on the configuration, migration, and qualification project.
So, it depends.
Some words of wisdom based on my experience with these qualification and migration projects:
— Work with an experienced and well-referenced GCP vendor (assuming you are working with a vendor) who understands CSV/CSA
— Work with a GCP vendor who will detail their activities and assumptions in their statement of work before you start the project. Trust me on this one.
— Create the timeline of activities and include the qualification tasks in a single project / project plan. The vendor will only care about their portion but there’s a lot more to it than the technical activities.
— Train the team on what to expect with the qualification activities and the post-migration activities. It won’t make 100% sense at first but it will at the end.
— Have the SOPs/WIs in place prior to the migration and the production cutover.
— Be flexibile. Things will not go 100% to plan. Apply critical thinking to address issues as they arise and continue to focus on the goal(s) of the qualification effort.
Qualification and validation do not end after the project is over. It starts at the beginning with a kernel of an idea and goes through to retirement.
Don’t forget to operate and maintain in a controlled fashion your beautifully qualified GCP platform.
Still Have Questions or Need Some Help?
Feel free to reach out to me with any questions you might have via email at terri.mead@solutions2projects.com or through my website SolutionsProjects, LLC. I’d be happy to have an initial call, free of charge, to discuss your qualification project.
Related article: Solutions2Projects, LLC
Living my best life…one day at a time
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
Written by
IT consultant, expert witness, YouTuber, helicopter pilot. Making the world a better place, especially for women. Award winning author of Piloting Your Life.
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
"
https://medium.com/@cpucoin/we-have-lift-off-bee4a3a87e16?source=search_post---------103,"Sign in
There are currently no responses for this story.
Be the first to respond.
CPUcoin
Sep 18, 2018·2 min read
CPUcoin Announces Private Sale of Upcoming IEO
CPUcoin announced the kickoff of a private token sale for its upcoming IEO (Initial Exchange Offering) . Details are as follows: 50,000,000 tokens will be allocated for the private sale. The Minimum Purchase Size is set at €250.000T and will offer early participants a token price set at €0.15.
CPUcoin is offering 50% additional bonus tokens for private sale participants. Bonus tokens will be redeemable 6 months after the close of the main sale.
Today’s very popular adoption of mobile, web and now blockchain enabled DApps (Decentralized Applications) has created a need for a reliable back-end technology that can be easily scaled and is low-cost. Nearly 100 million servers and more than two billion computers and mobile devices are running at any given time. Of those, roughly 30% are sitting idle. CPUcoin intends on tapping into these resources and put them to effective use by creating a sharing economy for DServices (Decentralized Server Applications).
CPUcoin is creating a first of its kind Content Generation Network (CGN) that reduces current infrastructure costs and lets anyone earn money with their unused CPU power. This unique Infrastructure as a Service (IaaS) offering is flexible, scalable and ideally suited for B2B and consumer organizations alike. This is the missing DServices platform for decentralizing server-side work needed to scale out virtually any client application and a pay as you go model.
CPUcoin has coined the term “DService” to describe the services component of cloud-based applications and DApps that run in the CGN. The first DService — MediaGen, handles all kinds of content transformation, is time-tested and blindingly fast, has been licensed to deliver performance and scalable content services out-of-the-box. Anyone will have the ability to develop their own DServices and run them in the CGN. A new utility token called CPUcoin will enable any customer to pay as they go. This levels the playing field for any sized organization to make scalable client/server apps without all the up-front infrastructure costs. The native currency of the CGN, Dyncoin is used to pay for all consumption of services and to pay out to the many providers of idle computer time.
The purpose of the private sale is to fund the initial development of the project, expand the team and operations, to take the project through completion of the IEO.
We are receiving considerable interest in our project, however, we welcome the opportunity to set up a direct meeting, where we can discuss the ICO or clarify any questions with you.  Please get in touch on one telegram: https://t.me/mediarichio or email us directly at: ieo@cpucoin.io
Creating the Content Generation Network. For more information please visit: https://cpucoin.io/
See all (298)
176 
176 claps
176 
Creating the Content Generation Network. For more information please visit: https://cpucoin.io/
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/improving-arm-template-syntax-with-bicep-templates-3c77d57d5594?source=search_post---------130,"There are currently no responses for this story.
Be the first to respond.
This is an excerpt from chapter 6, which starts with introducing the Bicep language, a new and improved IaC language for a Azure. This snippet is the next section of the chapter, which lists the benefits…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/its-iaas-magic-quadrant-time-and-you-can-guess-who-s-in-the-top-right-corner-40a2dd52e918?source=search_post---------19,"This is a reprint (more or less) of Wednesday’s ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
Here’s an article from ZDNet sharing some highlights from the just-released Gartner Magic Quadrant for IaaS. Not surprisingly at all, Amazon Web Services is in the top-right corner, with Microsoft Azure trailing just a bit behind. Google is a distant third, but still separated quite a bit from the three-cloud cluster of IBM, Oracle and Alibaba.
People will take their shots at Gartner’s patented MQs, Hype Cycles and the like for numerous reasons, but in this case I would say the analyst firm pretty much nailed it. It’s easy enough to put AWS and Microsoft in a league of their own, but the description of Google as often a secondary choice seems accurate based on what I’ve heard. Google also seems to have a leg up on the competition at least in data workloads, and perhaps in cloud-native ones, but the big challenge there is winning enough of them now to actually execute a long game.
If AWS and Microsoft take everything today, Google will have a hard time wooing customers when they’re ready to jump head-first into AI and cloud-native architectures in the future. And, as should be clear if you listen to this week’s podcast interviews with Dan Kohn (CNCF) and Gabe Monroy (Microsoft), Microsoft is very serious about becoming a major player in the cloud-native space. It’s also very serious about open source, which is another area that Google views as a strategic advantage.
If you haven’t read it yet, I addressed a lot of this back in March, in a post titled “4 reasons why AWS can’t take its cloud dominance for granted.” There have been some conferences and announcements since then, and lots of activity in the AI space, but I think the arguments more or less hold up. If you’re feeling all cloudy after seeing the Magic Quadrant, you can check it out.
techcrunch.com
Developers, do you want a machine recommending code to you? Do you trust a machine recommending code to you? I think we’ve all been saved by auto-correct a time or two, but burned more frequently.
opensource.googleblog.com
Whatever your thoughts on how companies will ultimately monetize their AI research, it’s indisputable that having a large user community will be valuable. That’s exactly what Google is building with TensorFlow.
www.technologyreview.com
Social media as a data source for projects beyond advertising has always been an intriguing concept. One beauty of deep learning is that now we can actually churn through the sea of images generated globally.
It has been a while since I thought about AI startup Vicarious, which raised quite a bit of money and was making some noise a few years ago. However, the company just published two papers on arXiv on Thursday, and apparently has settled on a business model of building general intelligence for robots. Here are the two papers it put out, the first being about building systems that can learn causal relationships with limited data in new environments, and the second being about a better way to handle multiple objects in computer vision applications:
www.geekwire.com
It sounds like the company decides arbitrarily, or perhaps emotionally, when to enforce non-competes and when not to. Or at least when to sue over them. You do have to wonder how this might affect hiring going forward.
aws.amazon.com
Speaking of AWS, it also has a very popular cloud service, including its Rekognition computer-vision API. This is one of those use cases that everyone can applaud when it’s used wisely, but can be rife with risk, too.
cloudplatform.googleblog.com
It looks like American companies are going to have to step up their physical presence in Asia if they want to compete with Alibaba there. Having superior technology only offers so much, and will only last so long.
www.bloomberg.com
You just know that cloud CEOs are smiling in anticipation every time a story like this comes out. Whatever the cause, bad press for data centers is good press for bigger, multi-tenant data centers, er, clouds.
techcrunch.com
This is interesting for a couple of reasons, including that Ion Stoica of AMPLab/RISELab/Databricks was one of Conviva’s founders. Also, analytics are about to become a lot more important as more cord-cutting continues.
techcrunch.com
I don’t want to like anything about this particular space, but Entelo appears to be doing alright for itself. However, if it’s true that it’s not what you know but who you know, then LinkedIn has the data that really matters.
yahooeng.tumblr.com
As I’ve said before, it’s good to see Yahoo keep releasing open source tools, but it’s possible the company’s architecture is starting to show its age. I get the impression that Storm, which is part of the Bullet backend, is not the most popular project at the moment.
code.facebook.com
Would you be interested in talks by Facebook, Google, LinkedIn, Microsoft, Pinterest, Uber and Yandex, about topics ranging from massive-scale object recognition to globally distributed databases? If so, here you are.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
5 
5 claps
5 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@psostre/private-clouds-are-cheaper-than-public-clouds-no-im-not-kidding-c98abf9094c4?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pedro Sostre
Aug 22, 2017·5 min read
Recently, Matt Asay, a columnist at TechRepublic, wrote an article that was critical of a public v private cloud hosting study. His response piece was titled “Private cloud 3x cheaper than public cloud; you’re kidding, right?”
Disclaimer and biases right up front. Disclaimer: ServerPronto (the company who ran the study) is a client of mine. If they were wrong and Matt just called them out, that would be fine, but his aggressive and biased response seems to warrant a correction. Bias: I am not an IT guy. I am an entrepreneur, and I think like an entrepreneur. If I can save money and get a comparable service, I’m almost always going to say yes.
In his article, Matt calls the study’s findings silly, uses what can only be assumed to be a purposely misleading argument and title, and finally gives up and says that the findings were correct but it doesn’t matter because convenience.
Here’s why Matt’s article is a load of bull:
Matt’s article uses some strange and almost misleading (more on that later) logic to try and get the reader to believe that private cloud hosting isn’t cheaper than public cloud hosting.
It all starts in his title. The title makes it sound as though it’s ridiculous to think that private cloud hosting is cheaper than the public cloud. From there, he calls the findings of 3x cheaper servers silly in the subtitle.
However, in the article — there’s not much evidence to back up his claims that the findings were invalid.
He starts off by inferring that Dell’s founder Michael Dell was also incorrect when tweeting that EMC-Dell-based-VxRail was 2–4X cheaper than AWS. Which is a pretty bold thing to infer.
Now, not only is our study incorrect, but the founder of Dell also doesn’t know what he’s talking about?
Then without any evidence as to why the “claim” of cheaper pricing was wrong, he proceeds to quote Rambo, and call the “silly cloud price war” over.
His reason? It never began. According to Matt, cloud hosting was never a question of cost, but rather convenience…
Matt’s main point appears to be that the services and convenience offered by Microsoft, Google, Amazon, and other cloud hosts is worth the extra cost. He even goes so far to indicate that pricing is “irrelevant”.
To this, I can only respond: That’s not how business works.
Business owners don’t care about convenience. Stockholders don’t care about convenience. They care about profit margins. Choosing to spend 200–300% more on hosting because it’s ‘convenient’ for your IT staff is not a justification.
His main argument may not even carry weight as it’s assuming that AWS and other public cloud hosts are in fact more convenient. This isn’t always the case though.
AWS uses proprietary technology and software that requires one to invest a significant amount of time learning how to use it. In fact, there’s a whole training and certification course provided by amazon to help IT staff learn how to use AWS’ “convenient” services.
Private cloud offerings on the other hand only need common IT knowledge to be used. Any IT team should be capable of utilizing a private cloud.
This wasn’t even included in the ServerPronto study, but the fact that AWS and other cloud hosts require your IT staff to learn their proprietary tech will actually cost a businesses even more.
Salaries for individuals with AWS certifications can be significantly higher than a non AWS certified individual, and paying for current staff to learn the new platform can be equally pricey.
This cost must also factored when choosing to move to the cloud.
Another argument used by Matt is that buying expensive infrastructure puts a company in a tough spot because they are no longer able to be flexible and respond to changing data requirements.
But, this argument is completely invalid. ServerPronto doesn’t advise business owners to buy infrastructure, nor do they sell it! Private cloud hosting is leased in a month to month contract. The upfront costs for infrastructure are paid by your private cloud provider. Since the contract is month to month, it can be cancelled, upgraded, or downgraded at any time so you’re able to remain as flexible as needed.
So, to reiterate. Unless you know exactly what you need, Do NOT buy infrastructure. Lease it and let the server host take care of infrastructure. All you have to do is customize your plan and the host will turn on all hardware for you. No upfront costs, and plenty of flexibility.
Matt also claims that ServerPronto’s position as a dedicated server hosting company skews the findings of the study. But, they don’t just sell dedicated servers. They offer cloud hosting too and have made significant advancements in cloud hosting technology, such as their patented geo-redundant cloud hosting which boasts zero detectable downtime due to a proprietary IP sharing technology (and yes, it costs more than dedicated and private cloud hosting.)
If public cloud was the right call for everyone, they’d be saying so — after all, they have a stake in public cloud hosting too. But, increasingly many organizations are overpaying for hosting services and this is bring driven by bait and switch pricing and IT professionals that have a vested interest in their boss choosing AWS.
Matt, at the end of your article, you basically admit that the study was correct. It’s a negative article without basis and quite frankly it seems self-serving.
When it comes to the public cloud, it’s good to keep in mind the Gartner Hype Cycle. Public cloud is still relatively new tech that’s currently in the peak of inflated expectations, but soon enough it’ll be diving down into the trough of disillusionment and people will realize:
The cloud isn’t for everyone.
Accelerating startups through Navigate.capital - I help early-stage founders launch better, grow faster, and raise smarter. Published Author & Speaker.
See all (841)
10 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
10 claps
10 
Accelerating startups through Navigate.capital - I help early-stage founders launch better, grow faster, and raise smarter. Published Author & Speaker.
About
Write
Help
Legal
Get the Medium app
"
https://humanizing.tech/the-future-cloud-compute-storage-pricing-model-because-ai-b5cb285abf56?source=search_post---------74,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Sean M Everett
Jun 8, 2016·3 min read
If you take the Infrastructure-as-a-Service (IaaS) model all the way to the end game, what you realize is that the pricing model has to change.
It’s no secret that artificial intelligence is going to be the next mega trend (internet > mobile > video > AI). But in order to create the model (read: algorithm) that lets this machine “learn”, you need three things:
And so, most independent researchers are using their own computers because of one very real problem: cost of cloud services. Trying to train a large AI model in the AWS cloud when you’re working with back and forward prop, connectomics, etc means you’re going to run up a $10,000 bill in about 10 minutes.
The only one who can afford that is big companies.
Incidentally, that’s exactly why many researchers have left for big companies like Google and FAIR (Facebook): higher pay, lots more training data, unlimited budget to use the internal infrastrcuture for training.
So then, what’s the answer to how you get around that $10,000 bill for startups and independents? The only way to do it is to change the pricing model from GBs stored (storage) or transferred between regions (CDN bandwidth) or compute time (processing). You have to go down to the most basic component: energy.
So, where is AWS headed?
Cloud services will eventually price based on each unit of electricity that you use in the cloud to switch a bit between a 1 and a 0, and also cool it.
What I find completely ironic about this is that everything seems to come back to energy. It is the lifeblood of, well, life. Whether that’s plant, animal, or artificial.
NVIDIA’s brand new, high-end GPU aimed at AI, the Tesla GP100 has a TDP (essentially max power of cooling the chip but maybe also the total power draw) of 300 watts.
For one example, lets say you use this GPU for an entire month. An estimate of the cost would be:
300 watts * 730 hours per month ÷ 1000 * 15¢ per kWh = $32.85
That’s around the price of a home electricity rate. A data center could get that much cheaper. You need power to run the chip and cool it, so the electricity cost would become quite expensive at scale, but likely still less than what you’re paying for a per unit compute cost.
— Sean
20 years building planetary scale tech products, now building an internet army. Collector of magic, believer in invention. Learn more at everettadvisors.com
See all (1,771)
6 
1
A daily resource world-class companies use to invent new products, commercialize them, and stay current in the rapidly changing technology market. Stop by shop.humanizing.tech for more. Take a look.
6 claps
6 
1
Mechanics, insights, and tools to help you invent and commercialize new products.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dzimine/shane-siebken-you-are-spot-on-thank-you-for-the-excellent-summary-f1e6b08580cf?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dmitri Zimine
Sep 15, 2017·2 min read
Shane Siebken
@Shane Siebken you are spot-on, thank you for the excellent summary! I see the same: as the smoke settles, the choice crystalizes exactly as you described. The two camps have emerged:
One camp bets on well-architected kubernetes++ on top of IaaS — providers’ least common denominator. They avoid vendor lock-in and take control, but take on expense and time of a non-trivial effort of building and running their own “platform”. This is large camp as this approach is here for a while and has matured. DevOps is strongly associated with this camp as 1) DevOps mindset and toolset are essential to build micro-services on k8s++, and 2) subtly, this approach justifies their existence.
Another camp embraces vendor lockin, releasing control and gaining time to market and cost savings via 1) obviously pay-per-use model 2) less obviously over substantially less infrastructure to design, provide and operate. I touched on this here, and Paul Johnston expressed serverless’ community view to this choice here.
The jury is still out which camp prevails, or, rather, how they split the territory. Serverless won’t replace microservices. Like any medication it only cures where prescribed; as of now the sweet spot is the apps with highly uneven load, or rarely executed paths within microservice solutions.
To me, the perils of Serverless / PaaS lock-in is not as much the “price” hostage, but reactiveness on dependency change management. PaaS vendor can and will pull the rug off your solution by introducing a change. There’s more, a subj of a separate post.
I lean towards betting on serverless, not because I believe that it is better, rather because I find it provocatively unknown :)
Technical leader, devops enthusiast, software geek. Sharing my professional opinions on medium.
12 
12 
12 
Technical leader, devops enthusiast, software geek. Sharing my professional opinions on medium.
"
https://medium.com/sally-thinking/%E7%A8%8B%E5%BC%8F%E5%AD%B8%E7%BF%92%E4%B9%8B%E8%B7%AF-day36-%E9%9B%B2%E7%AB%AF%E6%9C%8D%E5%8B%99%E8%88%87%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99%E6%95%B4%E5%90%88%E6%87%89%E7%94%A8-microsoft-azure-aa4d9d973daf?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
今日重點：IaaS vs. PaaS vs. SaaS，練習發佈專案，Azure 資料庫，MVC5 + 資料庫 + Azure，虛擬機器
▸什麼是「雲端運算」？
▸TEDxTaipeiChange 2012 — Ben Jai (翟本喬) — 要上雲端，先換腦袋參考資料：專家觀點｜IaaS不只為省錢，而是創造過去達不到的價值
價值1：減少機會成本價值2：短時間內取得大量資源價值3：提高創新應用的概念驗證效率，減少創新成本
企業想要上雲端，首先要改變使用者的心態。例如，將過去編列預算、買東西的作法，改成編列使用費、租用東西的方式，就像編列水電費的預算，將原本的採購科目變成使用費，這只是企業付錢的方式改變，還有其他流程都等著企業調整，所以企業不能光要求廠商改變，自己也得換腦袋，才能上雲端。
▸在 Visual Studio 發佈專案時，一併建立 App Service
1.新增ASPNET專案(MVC) >在本地端localhost:2.主專案右鍵發行>發佈，建立新的App Service3.於發行介面中複製網址>用戶即可透過URL瀏覽網站
備註：1. 思考資源群組及主控方案的設定使用資源群組收納方案與應用程式方案如機房，機器的概念
2. 觀察Azure資源群組的情形
小結：直接使用VS內建發行功能，建立新的AppService,資源群組,服務方案，將應用程式發佈到Azure 。
▸先在 Azure Portal 建立 App Service，再將 Web 專案發佈出去
1.建立資源：Resource Group
資源群組可讓您統籌管理應用程式中的所有資源。資源群組功能由 Azure Resource Manager 提供。Resource Manager 可讓您將多項資源歸入同一個邏輯群組，為群組中所包含的各項資源界定其生命週期範圍。群組中所包含資源通常會與特定應用程式相關。例如群組中可以包含裝載您公用網站的網站資源、儲存網站所使用之關聯式資料的 SQL Database，以及儲存非關聯式資產的儲存體帳戶。
左上：建立資源 > 搜尋： resource group> 點選建立
建立資源群組：填寫基本＞評論及建立＞建立
建立完成
為甚麼要建立資源群組？
2. 建立資源：App Service Plan
App Service 方案讓您能將特定應用程式配置到指定資源集，並進一步最佳化 Azure 資源使用率。如此一來，若您想節省測試環境的成本，可以在多個應用程式間共用方案。
左上：建立資源 > 搜尋： App Service Plan> 點選建立
填寫方案內容
完成畫面
3. 建立資源：Web App應用程式
在數秒鐘內根據所需要的強度建立及部署網站運用現有的工具建立及部署應用程式，並且不會導致管理基礎結構的問題。Microsoft Azure 網站可為所有大小的 Web 應用程式提供安全且具彈性的開發、部署及縮放選項。使用架構與範本在數秒鐘內建立網站。從 TFS、GitHub 及 BitBucket 等來源控制選項中選擇。透過 .NET、PHP、Node.js 或 Python，使用任何工具或作業系統開發您的網站。
完成畫面
回到VS，新增一個ASPNET專案(MVC)> 將專案發行 > 挑選發佈目標：選取現有的
選擇資源群組＞確定建立
小結：先在Azure部署雲端環境，再從VS發行Web內容到雲端
目標：建立伺服器，內建資料庫，進行管理參考技術文件：依照步驟建立使用 Azure 入口網站在 Azure SQL Database 中建立單一資料庫
備註：建立新的伺服器
▸使用 Azure 入口網站為單一和集區資料庫建立伺服器層級防火牆規則
1. 查詢伺服器名稱
2. 開啟SQL Server ，填寫登入資訊(此處登入後，資料就同步到伺服器防火牆設定)
3. 伺服器設定防火牆設定
4. 將IP位址貼上並儲存
▸建立資料庫
2. SQL Server進行兩端連線：本機端 ＆ 遠端
3. 建立資料遠端作法：執行程式碼建立本端作法：匯出資料針對要匯出的資料庫＞使用匯入匯出精靈＞填寫資料來源＞設定完即完成
▸建立一個 MVC5 的網站 CRUD 本地端 directory 資料庫
1.建立ASPNET專案(空白+MVC)2. 加入ADONET
3. 加入Controller>具有檢視，使用Entuty Framework的MVC控制器
3. 修改 web.config 的連線字串，變更 Data Source 為步驟三的伺服器
4. 修改 web.config 的連線字串，變更為 SQL 認證，帳號密碼一併增訂data source=伺服器名稱;initial catalog=directory;integrated security=False;user id=帳號;password=密碼
5. 建立資源：Web App應用程式建立資源>Web App應用程式>填寫資料
思考：在Azure新增Web App的意義？ 可否直接透過VS發行建立？
5. 發佈網站到 Azure主專案進行發行
6. 測試發佈後的網站
完成畫面
點選連接>下載RDV檔案
成長
50 
50 claps
50 
Written by

成長
Written by

成長
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-cryptonation-part-2-updates-on-sne-token-launch-and-long-term-hodl-benefits-for-the-733716cb230f?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 12, 2021·6 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito dropped by the Crypto Nation community for AMA part 2 last 30 September 2021. Daniel discussed the company and our products and also talked about updates on the upcoming $SNE token public sale and IDO launch plans.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Catch up with some of the highlights from the Part 2 of StrongNode and CryptoNation AMA session below:
Ben (Not 10): Okay, let’s start this AMA session with some introducto9 questions. Shall we?
Daniel Saito: Yup! It was Sept. 20th since I was last here and we have some great strides moving towards the eventual date of our IDO.
Could you please introduce StrongNode to our community? (again)
Daniel: StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources.
What is the mission and vision of StrongNode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies. We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources.
When we first started the thought of StrongNode, we looked outside to see what we can do to help solve some of the world’s problems. Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. With that said, we had to come up with a solution or an alternative and looking inwards of what is available we noticed that we already had the blueprint already in front of us in the form of Open Source.
Bitcoin’s increasing energy consumption has triggered a passionate debate about the sustainability of the digital currency. We want to introduce the idea of GREEN MINING. As StrongNode tries to tackle Green Mining, we want to make “Computing for a Purpose” a reality. Due to the recent times with the pandemic people have been shuttered in their homes, this was no different for us as well. Here we take a stand and try to solve the world’s problem one problem at a time! Made with ❤️ for the NEW Normal!
Please tell us about the tokenomics of StrongNode token $SNE.
Daniel Saito: The $SNE token has several different features and value in our ecosystem that have been pulled together from years of experience from our team and our advisors. More news on this will come out as we get closer to launch, but we are incentivizing token holders to hold along with us. SEED and PRIVATE INVESTORS are locked up for 6 months. All employees and advisors of StrongNode are locked up for 4 years.
2% of the $SNE token supply will be sold for the $300K raise (200M tokens will go into circulation at $0.0015).
Talking about $SNE tokenomics, the total supply of $SNE is 10 billion & from that you have allocated 2.1 billion of $SNE tokens for sale. So What about the remaining 7.9 billion tokens? On Which purpose will the remaining 7.9 billion of $SNE tokens be used for?
Daniel Saito: The 7.9 Billion tokens are released in a linear fashion, to help finance developement and marketing activities. As for the team allocation and advisory they are locked up for 4 years vesting schedule.
Investors that came into the SEED/PRIVATE round all get vesting privilages at DAY 1 of IDO. But they only have access to 10% of liquidity
You say that StrongNode creates several economies of scale that benefit all types of users and platforms that access the StrongNodeEdge network, but really what are these different economies that can be created with StrongNode?
Daniel Saito: We we like the concept of democratizing your computer, commoditizing your computer resources. When you do that, we allow for people with just a smartphone or an outdated PC to still contribute something rather than nothing. Keeping the adoption curve low, for people to participate is key for us.
We believe in many people in different demographics to engage with this solution and helps them in some way.
We hope to crack the digital divide issue and ewaste issue at the same time.
I think those are problems worth solving in any economy of scale.
Do you have plans to burn your tokens in the future to reduce the supply of tokens and increase their investment attractiveness? Could you introduce yourself to the community and tell us about your background?
Daniel Saito: Well I slightly talked about this earlier. We want to see token distribution. We have 10 billion tokens that will eventually go into circulation. We want to achieve maximum adoption and coverage in the several wallets. To me a wallet address is a node. A wallet address is a user. We want to achive maximum scale and get the distribution necessary. It would be premature to say we are BURNING XX% without it getting the distribution it requires. I am not taking burning of tokens off the table, but its to early to think about it, since usually token manipulation like that is the race to the bottom, with a lot less tokens in circulation.. (which is not a good look to have).
Nearly 80% of investors only focus on short-term token price instead of understanding the real value of the project. Can you tell us the motivations and benefits for investors to hold your tokens in the long term?
Daniel Saito: For that long-term benefit, we want to be able to distribute $SNE token to as many wallets as possible. We want to utilize the idle and untapped compute resources. We want to make “Computing for a Purpose” a reality.
You get paid $SNE tokens when you lend your network and compute resources. And others will get to run workloads, processed in chunks, on your idle resources. It’s a symbiotic relationship within our ecosystem. The users are incentivized to HODL as long as they feel that they are comfortable with our product offering. We believe in the long term value of growth for our investors and our network. The longer that they keep HODL(ing) our coin in their wallet and use the AMM farms / LP pools we will have with QUICK, they can benefit.
A lot of projects disappear in these difficult market conditions. How can you ensure the sustainability of your token what is the receipt of success?
Daniel Saito: We have the best in planning and budgeting. Difficult markets are hard, we have gone to the extent to DOXX ourselves to prove our seriousness and take the extra steps to insure that the retail investor is comfortable with their investment.
Of course we recommend with all investments, Do your own research and invest responsibly.
Can you give me the 3 best characteristics to convince me and other investors to invest in your project for a long time
Daniel Saito:
Sustainability
Universal Base Income
Low barrier of Entry for Adoption.
Sustainability we want to re-enforce the idea of GREEN MINING and the sustainability behind it.
Universal base income, we want to take what you already have which are resources, and better spend those resources that can benefit the owner to offset daily cost in their lives.
Low Barrier of entry for adoption, we want it to be stupid easy to join. IF anything we want your compute resources, not crypto — this app is for people that want to compute for a purpose and passively make income.
For more information, visit us: https://strongnode.io/
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
320 
320 
320 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://towardsdatascience.com/types-of-cloud-computing-952ae75e07c9?source=search_post---------65,"Sign in
Giorgos Myrianthous
Nov 6, 2021·4 min read
In one of my recent articles I discussed about the deployment models in Cloud Computing, namely private, public and hybrid models. Now apart from deployment models, it’s equally important to know the three different types of cloud…
"
https://medium.com/@IBMDeveloper/3-factors-for-successful-open-source-contributions-19c4bd36d979?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
May 16, 2019·2 min read
Open source software is eating the world, with numerous projects that cover a breadth of technologies, from IaaS to PaaS to CaaS to Serverless, AI, and more.
With so many projects and so much technology, getting started in open source can be overwhelming. What project should you work on? How do you open source your own code? How do you make your project meaningful for customers? In this blog post, I share the three key aspects that you should pay attention to when contributing to open source.
Since 2012, I have worked in various open source communities and on many different open source projects, including OpenStack, Mesos, Kubernetes, Itsio, Kubernetes Federation V2, Knative, cluster-api, even becoming a maintainer for some of those projects.
The image above shows my journey in open source. It’s important to note that with every open source product that I worked on, IBM now has a product or clients using it. My open source contributions, product integrations, and clients moved me to the position of an open source maintainer.
Along my journey in open source, I have learned a few things along the way that I think will help if you are planning to contribute or already contributing to open source.
Read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
See all (6,456)
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-minted-lab-strongnode-io-eb1871628b7f?source=search_post---------319,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·6 min read
Infrastructure-as-a-Service (IaaS) tech company and innovation lab StrongNode.io’s CEO and Co-founder Daniel Saito talked with Minted Labs South Korea community for an AMA episode last 22 September 2021. Daniel replied to the questions about our company and shared facts about the upcoming plans for the IDO launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Check out the highlights from the StrongNode and Minted Labs AMA episode:
Manho | Minted Lab: I think it will be wonderful to get some basic information about StrongNode before the AMA. Can you tersely tell us what StrongNode is?
Daniel Saito: Hi. My name is Daniel Saito and I am the CEO and Co-founder of StrongNode. I started computing since age 8 and haven’t really stopped since. Active entrepreneur, started 2 ISPs in both USA and Japan. Launched 🚀 2 satellites🛰 into space for News Corp. I have been in technology for as long as I can remember.
The StrongNode core executive team consists of 12 (+13 backoffice) extremely passionate and dedicated people with 20+ years of diverse experience with some of the biggest names in various industries, establishing foundational technologies, and closing $1B+ exits.
StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. StrongNode was the name picked out by our amazing marketing team.
The mission and vision of Strongnode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies. We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources.
It is stated that with StrongNode, all data entering and leaving the computer is encrypted and decrypted and processed. If so, I wonder if this process doesn’t overload the system. It is stated that all users who interface on the StrongNode network can enter through a process such as user registration. If so, I wonder if it’s similar to logging in with Facebook or Google.
Daniel Saito: Good question, this is a two part answer as they are two different things..
Yes there is a level of encryption and hashing involved on the data entering and leaving. This is actually done inside of sandbox environment, so you don’t risk contamination/manipulation on the host or the datasets that are being provided. There is specific load but the data chunks that are coming in are very small. A good example would be if a ENTERPRISE NODE comes online with 1TB of data that needs to be processed, it can be onboarded (as they become a node themselves) and sets the amount of cost that they are willing to pay for the said data processing and the system partitions the data, shards/chunks the data into smaller sizes and deploys it onto the network of resources willing to process the jobs.
As for our USER REGISTRATION process (which will be out shortly) we will be building a SSO (single sign on) solution for crypto. As you would normally be used to “Login with kakao” or “Login with Naver” theses SSO solutions are prevalent in WEB 2.0 infrastructure. We want to do the same with crypto. We designed a system that has KYC enabled, and upon filling out your user specific identifiers (e.g, wallet address, email) after qualifying for KYC (management via 3rd party providers, we offset the responsibility to manage personal identification) the system sends you a WALLET/ID specific StrongNode NFT that is nonmovable from your wallet. This NFT acts as your passport/token for entry into StrongNode infrastructure. As the system will know you only by your wallet address, as it will be tethered to several devices you will pair StrongNode with. (similar on how you pair spotify onto your TV).
Manho | Minted Lab: SSO solution for crypto! What a service! Can’t wait to see that :)
Daniel Saito: So the requirement is that if you have any digital devices, then you just install and forget. Earn crypto passively.
We want users not your crypto.
What is your project’s plan to increase the number of active users and what motivates users to hold ‘SNE’ tokens for a long time?
Daniel Saito:
Additional note: Over the years we worked with the best of the best WEB 2.0 companies and enterprise clientele. When we were working at MySQL and MariaDB, we help architect several of consumer brands technology (Samsung, Kakao, Daum, Naver, KT, Korea Department of Defense, the list goes on.) what we noticed with these clientele — is the requirement for compute and low cost compute..
We want the hurdle for user adoption to be low. We are targeting non-crypto people to earn passive income, they don’t need to know that it is crypto running in the background.
We are going for mass consumer adoption, and to help people onboard them into this world of crypto.
With that said, we need the crypto aware audience to help jumpstart the networks in Korea. As for the folks that will want to hold the $SNE token for a long time (which we hope you do). We want to get the $SNE token to as many wallets as possible, the value of our token will be dependent on the size of our network and adoption, its the value of the overall network. I see a world later on in 4 years time, that if we can tackle at PETABYTE scale, wow. The valuation of our network would be very high.
Of course we will have other mechanisms in place to earn more $SNE with our AMM farms and yield optimizer algorithms
We will also have staking of $SNE and staking your uptime if you are a ENTERPISE NODE.. various hooks..
We will work with our 3rd liquidity providers, our partners at QUICKSWAP will provide Liquidity Pool management.
So there will be several DEFI hooks onto our platform where the users are incentivised to HODL as long as they feel that they are comfortable with our product offering.
We believe in the long term value of growth for our investors and our network. The longer that they keep HODL(ing) our coin in their wallet they can benefit.
What are the advantages of me participating in the node of StrongNode? And let me know how to participate.
Daniel Saito: The advantage is at no cost to you. It would probably take 5–8 minutes to install and get it configured. Axie Infinity taught us that you can play to earn, well what we want to teach is just install and earn. We will have more details available here on our socials as it comes available:
🗯 https://t.me/strongnodechat
🗣 https://t.me/strongnode
📚 https://twitter.com/StrongNodeEdge
🎟 https://discord.gg/Gk2ka3NCR6
📚 https://medium.com/@strongnode
🥮 http://strongnode.io
Daniel Saito: Additionally, the fact you will know that your spare processing / network / storage can be used to help solve a lot of today’s world problems.
Immediately we have BIG DATA from clienteles that need low cost computing to solve DATASET against COVID, Global Climate Change, Etc.
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
204 
204 
204 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@tejas-68965/iaas-security-for-it-enterprises-cloudcodes-best-casb-solutions-19961d686a26?source=search_post---------295,"Sign in
There are currently no responses for this story.
Be the first to respond.
CloudCodes Software
Oct 26, 2020·3 min read
IT companies are shifting their work from the local server to IaaS because of its speed and working flexibility. But, they forgot that it is compulsory for them to draw their attention towards data visibility and security of IaaS. The single cloud computing security measure is not enough to render comprehensive IaaS security. Cloud Computing business users need to adopt cloud security providers who are constantly focusing on their business content and preventing it from Cyber threats. In this blog, we are going to make readers familiar with one of the trusted solutions for IaaS Security.
The team always thinks holistically about data visibility and dealing with IaaS security issues in cloud computing. From day one, CloudCodes is committed to working with its client's trust and as a long-term partner throughout the journey. A comprehensive IaaS protection platform is offered by this solution with a unified online security assessment system and information monitoring capabilities. No kind of magic tricks or ideas are there that are adopted by CloudCodes, it is all about the defense-in-depth strategy. In the upcoming section, one is going to find basic requirements for the strategy of IaaS security and illustrate that how this recommended solution fits into that respective strategy.
A unified online security assessment system is a means to enforce the best cloud security practices for determining misconfigurations. Customers can utilize CloudCodes solutions when they decide to collaborate their work with cloud security and real-time visibility. It will deliver comprehensive IaaS security for the public cloud.
After going through this post, if you find the approach reliable for your business IaaS then, without going for a second thought, take trial of this solution. Only when you are satisfied with the demo then only, integrate with this IaaS security approach. This creates an imperative environment in which tools for continuous security monitoring are placed in their right place, for its proper utilization.
Originally published at https://www.cloudcodes.com on October 26, 2020.
CloudCodes is a cloud security solution provider founded in 2011. We focus on providing cloud security solutions to enterprise customers through its SSO.
CloudCodes is a cloud security solution provider founded in 2011. We focus on providing cloud security solutions to enterprise customers through its SSO.
"
https://medium.com/@ContinoHQ/3-common-mistakes-people-make-when-looking-at-multi-cloud-iaas-paas-in-the-enterprise-f5f2cf9d08e0?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Contino
Jan 28, 2019·2 min read
These quick thoughts come from the mind of Benjamin Wootton, Co-founder and CTO of Contino.
A few quick thoughts on some approaches that I see time and again in the enterprise that will stop you from getting the most out of your cloud programme.
1. Cloud Broker: Putting big enterprise-y bloatware between you and your cloud APIs
The multi-cloud sales pitches emphasise the consistent experience and controls across clouds, but the reality is that you end up stuck with outdated APIs, leaky abstractions, and tied into lowest common denominator services. Stick with native access to your cloud!
Be careful as data centre-centric software companies are trying to sell their rebadged old stuff as hybrid to remain relevant.
2. Portability Obsession: Avoiding tie in to higher-level services in the name of portability
This means you miss a lot of the time-to-value benefits of the cloud when the chance that you will actually want to move are tiny.
Keep an eye on portability, but don’t throw the baby out with the bathwater!
3. Split Investment: Reducing momentum by spreading yourself too thin
Building an enterprise cloud platform and getting momentum behind adoption is hard. Splitting your investment, hiring efforts and engineering programmes across multiple clouds means that you will struggle to achieve critical mass or demonstrate rapid ROI, so your cloud adoption programme will come under pressure.
Our advice is to come down off the fence and do some business valuable work with one cloud provider in a cloud-native manner.
So you’ve heard the Gospel of Cloud?
How do you make sure you get everything promised to you?
You need to evolve your operating model!
We’ve worked out the fundamentals through seeing it go well (and not so well) at countless clients and summarised it in our Cloud Operating Model white paper here. If you like clouds, operations or models, there might be something of interest.
Contino is a global DevOps and cloud transformation consultancy that enables highly-regulated organizations to accelerate innovation.
28 
28 
28 
Contino is a global DevOps and cloud transformation consultancy that enables highly-regulated organizations to accelerate innovation.
"
https://medium.com/@dviveiros/paas-iaas-hybrid-environments-ftw-f8904e51531?source=search_post---------270,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Viveiros
Nov 26, 2014·2 min read
I’m so happy right now! Google just announced on March 25th the “Managed VMs” (http://goo.gl/yv1w6F), confirming what I truly believe and have been saying for a while: PaaS is not dead and it should be a very important piece on your architectural strategy. The key aspect to consider here is that the platform will NOT address all your needs, but instead of discarding it because of that, you should design a hybrid cloud solution, using both PaaS and IaaS.
See: There is nothing wrong with PaaS (http://goo.gl/NWrBmf)
The general guidelines for this approach are:
1) Use PaaS as much as you can. Ideally, it should handle all the HTTP requests coming from end-users, keeping the IaaS usage for backend workloads. Remember: you definitely want to take advantage of the effortless scalability provided by the platform.
2) If possible, try to create a MVP (minimum viable product) that can be delivered using only the capabilities provided by the platform. Doing that, you will have a much better time-to-market, lower your risks and costs.
3) Keep in mind that you will need to use the flexibility of the infrastructure at some point. That’s fine! Design a mechanism to allow your servers to communicate with the platform. Task queues are a great choice! By the way, thank you Google for solving this problem for us.
4) Communicate asynchronously as much as you can.
5) Just one more thing: before jumping into IaaS to solve a specific problem, be sure that there are no other companies offering this capability as a service. For example: you don’t need to develop a video encoding feature on the top of your favorite Linux flavor just because the platform doesn’t provide such service to you. There are several great video encoding services out there you can use.
I look forward to testing this Managed VMs, really excited about it. Long live PaaS, IaaS and Hybrid environments!
Cheers, Daniel Viveiros
Like Loading…
Originally published at danielviveiros.com.br on March 27, 2014.
Apaixonado por tecnologia e curioso com o impacto dela nas relações humanas e profissionais. Aprendiz em uma série de áreas, incluindo escrita.
Apaixonado por tecnologia e curioso com o impacto dela nas relações humanas e profissionais. Aprendiz em uma série de áreas, incluindo escrita.
"
https://medium.com/nestegg/saving-the-world-with-iaas-infrastructure-as-a-service-f3a587ad6d32?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
These concepts below come from Jeremy Rifkin and his book “Zero Marginal Cost Society”.
Buy his book or look up his name on youtube. He introduces how approaching zero marginal cost in the real world will bring about new economic models that incentivize sustainability and access.
Ignoring maintenance and fueling costs, why pay several bitcoin for a vehicle that sits unused for 95% of the time you own it. Plus you’re mobile and not in the same place long enough where buying a car would make sense. Instead, you use your phone to hail a self-driving car that delivers you from point a-b, which works everywhere across the globe. You pay only for the marginal cost of each ride: fuel + maintenance + fee to car manufacturer for future developments.
Having a fancy car is no longer the symbol of power and influence, now it’s your Instagram feed with pictures of all your exotic and interesting experiences and achievements. (Note, this is not saying one is necessarily better than the other)
The communication revolution unlocked by the internet has new generations growing up without thinking about borders when they communicate. Without a cost to share, it’s unveiled a powerful human intrinsic motivation to collaborate. This motivation rivals traditional proprietary models that depend on profit motivations. Need proof? Google, Amazon, Facebook, and McDonalds all use linux servers to run the popular services you rely on everyday [link], not Microsoft or Mac.
Yet, this Open Source collaborative culture has largely been restricted to the web. Software has zero marginal cost, it can be distributed virally for free once created.
Not the case for objects in the physical world, which have material and distribution costs. Yet with the introduction of new technologies, namely IoT, 3D Printing, and public blockchains — there’s a path to bring this digital open source collaborative phenomenon into the physical world. Through what Rifkin calls, the Collaborative Common.
What happens when the physical world has the tools to share abundance efficiently and approach zero-marginal-cost?
Wealth for young people today is defined by access, not ownership. The companies that spurred these ideas, Uber, Airbnb and other sharing economy companies are onto something.
We’re already living in a post-scarcity world. The problem is distribution and access.
In other words, getting value out of unused physical assets by sharing it, whether that’s an empty seat in your car or apartment while on vacation. It’s unlocking current abundance to fill demand.
Yet the current sharing economies are rudimentary in that they are achieved through rather traditional middleman, with hefty 20% services fee to fund them. They also have unnatural incentive to grab monopoly on the concepts of ride- and accommodating-sharing itself, like google is now a defacto verb meaning to search something online. Uber itself is incredibly aggressive at raising massive war-chests from varied investors to rapidly build out its network through subsidizing ride costs, burning $2 billion in the first half of 2015. And what natural pressures exist to keep these fees from rising if a monopoly is reached?
Eliminating the middleman layer (uber & airbnb) from these taxi and accommodation services would unleash entirely new lifestyles and economic models. If current sharing economy middlemen can provide cost competitive options by unlocking abundance at ridiculous markups, what happens when you recreate these sharing economies into a public infrastructure, from which service providers directly provide access.
Back in 2015, KLM Airport in the Netherlands said no-more to buying light bulbs. Instead, they paid Philips to provide light “as a service”. KLM basically told Philips — look, we want light for the next few years, take care of it. [link]
All of a sudden, the economic incentive for Philips changed.
Philips kept ownership of all the lighting equipment and would be able to repurpose it after the duration contract. Instead of profiting off of the quantity of lightbulbs sold, they made money the less they needed to produce. This caused Philips to make more efficient lightbulbs that lasted longer and could be repaired and used again.
This model of “infrastructure as a service” creates the economic incentive to create longer-lasting, recyclable, sustainable products.
Let’s extend this thought experiment further.
Let’s say that instead of ordering your ride through Uber, it was through AutoNet — a decentralized network that pairs autonomous-cars with riders at the lowest possible transactional margin. The autos on the road would be produced by car manufacturers who act as service providers to this network. Because it’s a collaborative common, there’s no middleman and both the service provider and rider share that previous slice of the pie.
The common would rent Autos-as-a-Service for a specific time period. Since car manufacturers keep ownership of the car throughout the lifespan, they become incentivized to make their fleet live longer through cheap repairs and upgrades. These car manufacturers would also benefit from the dramatically reduced distribution costs of getting out their vehicles with no middlemen. Instead of dealing with a range of dealership networks, it simply installs the open source AutoNet, turns it on, and the car drives out the door to collect fares.
And this phenomenon extends beyond cars.
In fact if you think about most physical goods you own, for most of the time they go unused. Your relationship to those items changes when you can access them at the touch of a button.
We’ll look back and wonder that there ever was a time where everyone owned their own hammer.
— Astro Teller, Google X
The beauty of this model is that there is a separation of responsibilities allowing every party to specialize. In terms of sharing autonomous cars, the aggregate of all the party’s incentives aligned with AutoNet provides them all benefits while also incentivizing sustainability of resources.
By creating a diversity of services which are compatible with each other through AutoNet, all party’s benefit from maximum demand.
So how could this be made possible? Right now we’re exploring the future of pensions with APG, the largest pension provider in the Netherlands. Our project is to research trends of the not-so-distant future and discover ways that pension providers can play a role in this new world.
Join us at NestEgg.eu and follow us for future updates!
Invest in the future you wish to retire in.
59 
Thanks to Nathalie Drost. 
59 claps
59 
Written by
goofing is good, more hijinks at https://dean.lol
Invest in the future you wish to retire in.
Written by
goofing is good, more hijinks at https://dean.lol
Invest in the future you wish to retire in.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rcls/this-article-really-confuses-what-aws-is-iaas-as-to-mailchimp-saas-b972408478b2?source=search_post---------255,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rcls
·Jan 6, 2021
Jared A. Brock
This article really confuses what AWS is (IaaS) as to MailChimp (SaaS). Sure you can spend months building a tool where you keep your registered newsletter subscribers with their contact info, and create good looking email templates, or create an automated scheduler to send those 100,000 emails in bursts because even SES has a throttling limit.. But do you pay for the extra service or build it and maintain it yourself? You might need to pay people more money to do that than what you pay for MailChimp.
Maybe MailChimp uses AWS SES behind the scenes? Have not checked. SES is a service for email delivery. It is not an email marketing software.
Consultant, software architect and developer, freelance UI/UX designer, computer engineer, tech enthusiast, father.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technology-media-telecom/global-hyperscale-cloud-providers-dominate-iaas-growth-f89cb7c6bdc9?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
Leading public cloud service providers continue to gain market share across the globe. The cloud Infrastructure as a Service (IaaS) market grew 37.3 percent in 2019 to reach $44.5 billion — that’s up from $32.4 billion in 2018, according to the latest worldwide market study by Gartner.
Amazon (AWS) retained the number one position in the IaaS market in 2019, followed by Microsoft (Azure), Alibaba, Google and Tencent. The remaining cloud service providers are essentially niche players, by comparison.
“Cloud underpins the push to digital business, which remains at the top of CIOs’ agendas,” said Sid Nag, vice president at Gartner. “It enables technologies such as the edge, AI, machine learning and 5G, among others.”
Public Cloud IaaS Market Development
Each emerging technology requires a scalable, elastic and high-capacity infrastructure platform like public cloud IaaS. In 2019, the top five IaaS providers accounted for 80 percent of the market — that’s up from 77 percent in 2018. Three-quarters of all IaaS providers exhibited growth in 2018.
AWS continued to lead the worldwide IaaS market with an estimated $20 billion of revenue in 2019 and 45 percent of the total market. Amazon leveraged its top spot in 2018 to build out its capabilities beyond the IaaS layer in the public cloud stack and maintain its position in 2019.
Azure remained in the second position within the market, gaining more than half of its nearly $8 billion in revenue coming from North America. Microsoft’s IaaS offering grew 57.8 percent in 2019, as they applied enterprise sales co-selling with other Microsoft cloud products and services.
The dominant IaaS provider in China, Alibabanda Cloud, grew 62.4 percent in 2019 with revenue surpassing $4 billion — that’s up from $2.5 billion in 2018. Moreover, Alibaba will expand its cloud infrastructure and aim to offer cloud-based intelligent solutions to enable digital transformation.
China-based Tencent grew its IaaS offering by over 100 percent in 2019. It is the second-largest provider of cloud services in China, after Alibaba.
“As the cloud market matures, and its leaders experience natural market share erosion as a result, China-based providers such as Alibaba, Tencent and Huawei will start to gain more traction. It will also be hard for other providers to enter the China market given the country’s highly regulated environment,” said Mr. Nag.
Google’s IaaS revenue grew from $1.3 billion in 2018 to $2.4 billion in 2019, experiencing 80.1 percent growth. Google’s cloud services focused on providing organizations with industry-specific solutions on robust computing infrastructure. North America accounts for nearly half of Google’s IaaS revenue.
Moving forward, Gartner will be combining the IaaS and platform as a service (PaaS) segments into a single, complementary platform offering — entitled cloud infrastructure and platform services (CIPS).
The worldwide CIPS market grew 42.3 percent in 2019 to total $63.4 billion — that’s up from $44.6 billion in 2018. Amazon, Microsoft and Alibaba secured the top three positions in the CIPS market in 2019, while Tencent and Oracle were in a virtual tie for the number five position with 2.8 percent of the market.
Outlook for CIPS Market Growth Opportunities
“There will be a continued push of cloud spending as an outcome of the coronavirus pandemic,” said Mr. Nag. “When enterprises were compelled to move their applications to the public cloud as a result of the pandemic, they realized the true benefits of public cloud and it is unlikely that they will change course.”
According to the Gartner assessment, during the pandemic recovery and rebound phase, CIOs are recognizing that they don’t need to bring workloads back on-premises, which will further increase cloud spending and drive new applications around cloud-hosted collaboration that incorporate emerging technologies such as virtual reality and immersive video experiences.
Niche public cloud service providers will continue to seek out small pockets of large enterprise applications that they can pursue with some degree of success. That said, industry analysts must scrutinize niche cloud provider revenue reporting. Key growth trends must be reported accurately.
Originally published at https://blog.geoactivegroup.com.
GeoActive Group | David H. Deans
Written by
Technology, Media, Telecom analyst, consultant, columnist
GeoActive Group | David H. Deans
Written by
Technology, Media, Telecom analyst, consultant, columnist
GeoActive Group | David H. Deans
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/how-enterprise-it-got-boring-e1339735b24b?source=search_post---------68,"This is really a draft of a post that I am never going to finish, because I can’t figure out exactly what I want to say. But I believe it’s true at its core. Correct me where I’m wrong, or just ignore it :)
Enterprise IT can seem a little boring these days. Not boring like bad TV, but boring like local legislation. It’s important — more important than ever, in fact — and yet it can feel like you need to be a domain expert (or political wonk) to keep up. I attribute this shift to a change in focus from infrastructure (broadly defined) to application architecture, which, frankly, was a long time coming.
(Probably because I spent so many years covering this space for a living, I also feel extra sympathy for the reporters and bloggers who have to make sense of it all but aren’t knee-deep in the technology and customer discussions every day.)
Here’s my attempt as an explanation.
Developer tools and cloud-native components, for example, are now big business and critical to how many organizations build applications, but they kind of live in the weeds. Go ask a group of random non-cloud-native developers about their opinions on which service mesh is best or, frankly, about the vast majority of AWS services launched within the past couple years, and then count the blank stares. Or ask a group of CIOs about their major initiatives for this year, and count how many call out event streaming (despite the fact that Confluent last year raised $250 million at a valuation of $4.5 billion). They might mention something like digital transformation or application modernization, but the devils there are in the implementation details.
This isn’t a bad thing in the least. It just goes to show how far we’ve evolved from around 2003 through 2014, when we saw some pretty significant shifts begin to emerge, largely at the infrastructure layer: virtualization, cloud, open source, big data, containers/cloud native/Kubernetes, and NoSQL, to name a handful. Infrastructure was a major hurdle and cost center, the web was pushing databases to their limits, and it was exciting to watch this evolution while speculating about what this all meant for legacy vendors, startups, IT teams, and even the end-users who’d ultimately consume much of this innovation via applications. These were foundational shifts that touched everyone.
If I fast-forward to today and gloss over all that happened simultaneously and since then (some of which I covered the last time I really wrote anything here, in “What Happened to Hadoop”), I’d argue we’re at a point where infrastructure is all but solved. Not totally solved in the sense that we don’t need to think about it, or that there’s nothing to improve upon. But largely solved in the sense that compute, storage, networking, and data processing are vastly more automated, scalable, performant, and capable than they ever have been. For example:
Essentially, the tools now exist to build the types of applications we’ve been talking about for decades, and to manage operations in a way that aligns with the high-scale, real-time nature of the web in 2021. And it’s a good thing, because people have seen the light thanks to their experiences with the consumer web and we have smartphones from which many people expect to do almost everything — from ordering groceries to getting a new insurance policy.
So, naturally, the onus in enterprise IT has shifted to applications. More specifically, it has shifted to application architectures and the processes for continually improving upon existing applications and rolling out new ones. When I hear or talk about terms like digital transformation and application modernization, this is what I think about.
So, naturally, the onus in enterprise IT has shifted to applications. More specifically, it has shifted to application architectures and the processes for continually improving upon existing applications and rolling out new ones. The days of COTS are fading fast in areas where digital experiences are a point of differentiation. In retail, for example, large companies need more than a virtual storefront and some payment functionality in order to woo customers who might expect a seamless buy-online, pickup-curbside experience, or who are queuing up to buy a new pair of Yeezys and will see their dreams dashed if the site goes down.
When I hear about terms like digital transformation and application modernization, this is what I think about. I don’t think about how these companies had better move to the cloud or get themselves a more scalable database. Because while those things might very well help, they now exist and are very mature; if anybody really wants to use them, they can figure out a way. A decade or so ago, this wasn’t the case.
The name of the game today is implementing a platform that lets operations teams manage large numbers of distributed applications with relative ease, wherever they happen to be hosted, and empowering developers to take advantage of the glut of new tools, patterns, and processes — including all the goodness at the infrastructure and data layers — that optimize their productivity and make applications more secure and more reliable. It’s not always as exciting as the first time we heard “Ditch your servers” or “Say goodbye to your expensive database” — bastions of the first dotcom boom that dominated IT budgets— but it’s just as important.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
11 
11 claps
11 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@adamsimsy/what-is-the-difference-between-iaas-paas-and-saas-aaadae472cc7?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adam Simmonds
Jan 18, 2017·1 min read
Over recent years I've helped to architect solutions that use more and more cloud services and very often see the following terms used:
- IaaS or Infrastructure as a Service- PaaS or Platform as a Service- SaaS or Software as a Service
Just from looking at the names of each service types, most technical people can work out what is provided by each. But unfortunately sometimes people are unsure of the differences, particularly where IaaS ends and PaaS starts.
So i thought i’d put together a simple illustration which may help someone looking for a quick overview of the differences.
See all (9)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@manningbooks/secrets-management-with-terraform-6b73ba30d4e?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
·Jan 11, 2021
In case you missed this stream, here it is! Have a look.
Subscribe to our Twitch channel here: https://www.twitch.tv/manningpublications

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
51 claps
51 
Follow Manning Publications on Medium for free content and exclusive discounts.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ziimm/how-to-deploy-a-reactjs-web-application-on-digitalocean-11d6d8d57b68?source=search_post---------357,"Sign in
There are currently no responses for this story.
Be the first to respond.
Żimuzo Obiechina
Sep 15, 2021·6 min read
DigitalOcean is a cloud provider that offers services structured around different cloud service models — infrastructure-as-a-service (IaaS),platform-as-a-service(PaaS), software-as-a-service(SaaS) and so on.
This article is a walkthrough on how to deploy a ReactJS web application on DigitalOcean’s IaaS offering — DigitalOcean Droplets — with Nginx.
CAVEAT: Creating a React app is outside the scope of this article. However — after you are done writing code for your React application — you will need to run npm run build to bundle your React application into a /build directory — in order to optimize your application for best performance within a deployment environment.
For this article, the main highlights are:
Now, let’s get into it.
DigitalOcean offers an infrastructure-as-a-service (IaaS) option in the form of virtual machine instances called Droplets.
Provision a droplet with an Ubuntu 20.04 image, with the lowest payment plan ($5/month) — since we will be using the instance for demonstration purposes. Also, choose password-based authentication at this stage. This droplet is our host machine for the Nginx server.
Note that the default username is root@<droplet-external-IP> ; that is, root@xxx.xx.xx.xx . Your password is whatever value you pass in while creating the droplet.
Once the droplet is created, set up SSH keys for secure log in to your droplet.
After you set up SSH-based authentication, create a new user with administrative privileges.
Switch to the new user:
Install Nginx:
You need to set firewall rules using the firewall service ufw to allow web access to the Nginx server. View a list of firewall configurations with the following command:
You should get this output:
To allow HTTP traffic, use the actual profile name — rather than the value 80 for HTTP and 443 for HTTPS. Run the command:
Verify that the configuration is active:
This should be the output:
If your output says status:inactive, run the command sudo ufw enable
Run the following commands to restart the server (for the changes to take effect) and check the status of the Nginx server:
Nginx should be active and running as shown:
Enter your droplet’s IP address on your browser to confirm that Nginx is running properly. You can retrieve your IP address using the command:
If Nginx is running properly, the default Nginx start-up page should display on your browser.
Generally, Nginx serves web documents from the /var/www directory — the default start-up page originates from an index.html file in the /var/www/html directory.
Therefore, as a rule, place the web documents for your website in a named directory within the /var/www directory.
Let’s add the source files for the ReactJS application.
Clone your Git repository (for the React app) the /var/www directory:
This should create a directory /var/www/<your-directory-name> containing your source files/directories.
The /build directory for the React application — within /var/www/<your-directory-name> — is the most important here because it contains the index.html file to be served.
TIP: You may delete all other files and directories except the /build directory that contains the index.html file — since that is the only directory needed.
Change ownership of the /var/www/<your-directory-name> directory to ensure you have the permissions to modify files within the directory. Run the command:
This will assign ownership to the new user you created above.
Next, let’s create a server configuration for our React application.
All Nginx configuration files are within the /etc/nginx directory:
The configuration files in /etc/nginx/sites-available usually end with the .conf extension — however, you may choose to name them without the extension. These configuration files help Nginx identify and track the websites that it hosts.
Navigate to /etc/nginx/sites-available . It should contain a default or default.conf file. Create your own .conf file:
Open the newly created file in any text editor — we’ll be using vi :
The configuration for the React application will be placed in a server block. This block specifies how requests to the IP address or domain should be handled.
Paste the following configuration snippet:
Run the command :wq to save and exit the editor.
Create a symbolic link between your/etc/nginx/sites-available<filename>.conf file and /etc/nginx/sites-enabled directory:
Some information on symbolic links:
A symbolic link creates a special type of file that contains a pointer to a referenced file or directory.
Both the symbolic link and the file being referenced are indistinguishable from one another — whatever is written to the original file reflects in the symbolic link.
A common use case for symbolic links is in managing multiple versions of a single software. The symbolic link can point to the latest version of the software even though previous versions are available on the system — this way all files that depend on the software can just use the symbolic link instead of having to worry about versions.
Symbolic links to the configuration files are in the /etc/nginx/sites-enabled directory. The global Nginx configuration file nginx.conf reads from this directory — through the include directive — in order to ensure that only the current versions of configurations are applied.
Run the following command to confirm that there are no issues with your configuration:
The following output indicates a satisfactory configuration:
Now, restart the Nginx server. Run the command:
Refresh your browser. The React application should be successfully deployed.
Deploying an application on the cloud makes it possible to easily get an application up and running with minimal cost. You can take this process a step further by setting up a CI/CD pipeline to automate the build/deploy process.
You can also explore DigitalOcean’s App Platform — a PaaS offering — here.
I'm documenting my learning journey in tech, one article at a time
6 
6 
6 
I'm documenting my learning journey in tech, one article at a time
"
https://medium.com/swlh/understanding-cloud-models-for-adoption-strategy-37260bab92c5?source=search_post---------336,"There are currently no responses for this story.
Be the first to respond.
When moving your applications to Cloud is not only a matter of selecting the provider that most fit your needs, but also choosing between the multiple options of available solutions.
Before opening the panel of your preferred cloud provider, register and fill out your credit card to start “paying for what you use” is important to trace and define a solid strategy to move to the cloud.
"
https://medium.com/@storjproject/what-is-decentralized-cloud-storage-3a530f1552?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Oct 17, 2020·5 min read
Tardigrade is the world’s first enterprise-grade, decentralized cloud storage service. Through decentralization, Tardigrade is more secure, more performant, more affordable, and more private by default than centralized cloud providers.
What exactly is decentralized cloud storage? On the user’s end, it operates exactly the same as traditional cloud storage options like Amazon S3. But, instead of your files being stored in a big data center that’s vulnerable to outages and attacks, your information is stored on thousands of distributed Nodes all across the globe.
First off, we aren’t going to get super technical here. This is an overview of how it works, so if you really want to dig into the technical specifications of Tardigrade (the nuts and bolts stuff), you can check out our documentation. Here’s also a cool diagram to reference.
As previously mentioned, with traditional cloud storage, all of your data resides in one large data center. This often leads to downtime and outages when one of these facilities goes offline. With decentralized cloud storage, we have a large, distributed network comprised of thousands of Nodes across the globe that are independently owned and operated which store data on our behalf. A Node is simply a hard drive or a storage device someone owns privately. We pay all of our Node Operators to store files for our clients, and we compensate them for their bandwidth. Think of it like this: You have a 10 TB hard drive, and you’re only using 1 TB. You could sign up to be a Node Operator and we would store pieces of our clients’ files on your hard drive utilizing your unused space. Depending on how many files you store and how many times that data needed to be retrieved, we’d compensate you accordingly.
The real issue with centralized providers like Amazon S3 is all of your data resides in huge data centers. If a part of Amazon’s network goes down, you won’t be able to access your data at best, and at worst, your data could be permanently lost or damaged. Large data centers are also vulnerable to hackers, as we’ve seen time and time again. With decentralized cloud storage, end-to-end encryption is standard on every file-each file is encrypted on a user’s computer before it’s uploaded, broken into pieces, and then spread out to uncorrelated Nodes across our network. Only you have access to the encryption keys, making it virtually impossible for your data to be compromised or stolen.
Plus, centralized cloud storage costs a lot more than our decentralized network. Large data centers cost a ton of money and take a lot of resources to operate. The fact we don’t have to spend money operating a data center, but rather use individual, privately owned devices, means we pass those savings onto our customers.
We’ve never lost a single file on the Tardigrade network due to our decentralized architecture. Tardigrade has 99.99999999% file durability, and since we split each file into 80 pieces, and we only need 30 pieces to reconstitute a file, it would take 51 nodes going offline at the same time for your file to be lost. Complete files are fully retrieved at lightning speed by downloading the fastest 30 of 80 pieces. If you know how torrenting works, it’s the same concept. There’s no central point of failure, ensuring your data is always available. Since each file uploaded to Tardigrade is split into 80 pieces, encrypted, then stored on 80 different Nodes, one Node going offline won’t impact any files stored on any particular Node.
The real beauty of the decentralized architecture lies in the fact that a Node Operator doesn’t know what files are stored on their Node. Even if a Node operator wanted to access your files, they only have a small shard or piece of that file. They would have to track down at least 30 other Nodes to reconstitute a file, and all of those files are also encrypted. It’s virtually impossible to compromise a file.
Storj is what we like to call “trustless.” What does this mean? It means you don’t have to place your trust in any single organization, process, or system to keep the network running, and you don’t have to worry about your data because we couldn’t access it even if we wanted to. Tardigrade is private and secure by default, and files are encrypted end-to-end before they’re ever uploaded to our network, ensuring no one can access data without authorization.
A file on Tardigrade is almost impossible to access without the proper keys or permissions. Because everything is encrypted locally, your data is literally in your hands, and no one else’s. After files are encrypted, they get split into smaller fragments that are completely indistinguishable from each other. A typical file gets split into 80 pieces, of which any 30 can be accessed to reconstitute the file. Each of the 80 pieces is on a different drive, with different operators, power supplies, networks, geographies, etc. For example, there are currently 171 million files on our Tardigrade service. To compromise a single file, the hacker would first have to locate 30 of its pieces among the 171 million on the network, creating a needle in a haystack scenario. Then, they would have to decrypt the file, which is extremely difficult-if not impossible-without the encryption key. Then, the hacker would have to do all this again to access the next file.
So, there you have it, the decentralized cloud in a nutshell. If you’re interested in becoming a Node Operator, please visit www.storj.io. And, if you’re interested in trying Tardigrade for yourself, head on over to www.tardigrade.io.
Originally published at https://storj.io.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
See all (1,648)
119 
1
119 claps
119 
1
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cardmagic/5-ugly-devops-lies-a9e521b9dab3?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lucas Carlson
Oct 25, 2016·4 min read
If the snake oil salesman still exists in the 21st Century, it does so in the guise of the DevOps vendor. Like the secondhand car dealer, the solar panels sales guy, or the crooked cop, the DevOps vendor will tell you black is white — oh, no actually white is black.
Their lies travel like Japanese Knotweed.
Almost every week, a new technology solution is launched which promises to solve all your problems. One that will boil the ocean. Maybe some hot new trendy DevOps startup that just raised $10 million in seed funding and is ready to save the world. Why trust me? Because I used to run one of those trendy DevOps startups myself.
And if there’s one thing I learned, it’s to not believe a word of it. It’s all just a pack of lies. In fact, I’ve sat in so many presentations, watched so many launch demos from DevOps start-ups that I decided — for your delight and pleasure — to make a list of the lies I’ve seen spun recently.
“I will solve all your problems, all you have to do is just throw out all your old systems and replace them with our shiny new platform.”
DevOps vendors typically call on vulnerable customers to demolish their existing IT management practices and replace them with their solution. The name of their solution will vary from vendor to vendor. They might call it Platform-as-a-Service, or a Container Orchestration Platform, or a DevOps Management Platform. The names change but the truth doesn’t: These vendors simply want you to rebuild your systems on their architectures.
The Truth: New platforms that require major changes in IT processes, no matter how popular or trendy, carry major hidden risks around integration, production readiness, security and robustness.
“I will solve all your problems, all you have to do is just rebuild all your applications in our shiny new programming language.”
There’s a reason your most important code is 30 years old and still written in Cobol: It works. Almost every organization wants new, innovative code. But the insidious part of the lie is when vendors refuse to service your older apps. Refuse to help you add agility and modern tools to old code that still works. Not everything needs to be rewritten in Go or Clojure or Rust before it’s ready to go into a Continuous Delivery pipeline.
The Truth: Sometimes you can teach an old dog new tricks. There are solutions out there that can add agility to existing IT processes without rebuilding every application from scratch. But you have to look hard for them.
“Everything is moving to the cloud. Get on board before it’s too late.”
Vendors can’t stop themselves calling out the benefits of the cloud. Cheaper, more efficient, more reliable. It’s the future — on-premise is over. Soon they’ll argue the cloud will guess next week’s lottery numbers for you. It’s hype. Pure and simple. Yes, there are occasions when it makes sense to rent space (cloud), but sometimes it’s better to own (datacenter). Saying nobody’s going to own datacenters soon is like saying nobody’s going to buy a home in the future. The idea that somehow everyone’s going to magically realize one day that renting a house is better for everyone in all situations: It’s a lie. Datacenters will never go away. Sorry Amazon.
The Truth: Sometimes the total cost of ownership is lower than the total cost of renting. Sometimes renting is cheaper. There is no one-size-fits-all. Anyone who says otherwise has an agenda that’s probably not in your best interest.
“It’s open source, so it’s better.”
Open source has come a long way over the years, and the always-active open source communities are one of the primary reasons for its success. And there are many open source projects that are solid and battle-tested. But don’t let those DevOps vendors sell you snake-oil. Not all open source software works well at scale, and not all software that works well at scale is open source. While there are benefits to open source software, what you should be looking for in a DevOps solution is a long track record of stable and scalable execution.
The Truth: You would be surprised at how many open-source projects, even popular and trendy ones, have never been deployed in a system with more than 50 servers. Buyer beware, you might actually be getting exactly as much as you pay for it.
“Everybody’s talking about X, so you should have an X strategy.”
Don’t believe the hype. Especially if your job depends on it. New technology trends might end up becoming battle-tested eventually. But let other people forge those paths for you. The upside potential of new, disruptive technology is only matched by the unforeseen downsides of putting it into production. The reality is that if a system you’ve used has worked for the last 10 years, it’s likely to keep working for the next 10 years.
The Truth: Everyone wants to lead you in an IT Revolution. Nobody wants an IT Iteration. But what is lost in that discussion is the fact that there were good reasons to build things the way that they were built. For all its shortcomings, it works. Maybe what enterprise IT needs most today is iteration, not revolution.
So watch out. Many DevOps myths could end up murdering your business.
Want to learn more about the topics I cover in my blogs: DevOps, AgileOps, Continuous Delivery, Release Automation, Digital Transformation (and many others!), then feel free to register here to get regular updates from me.
Originally posted on DZone: https://dzone.com/articles/five-big-lies-devops-vendors-love-to-tell
Consultant, Entrepreneur & Novelist | CraftsmanFounder.com
2 
2 
2 
Consultant, Entrepreneur & Novelist | CraftsmanFounder.com
"
https://medium.com/@vunvulear/cloud-maturity-level-iaas-vs-paas-and-saas-a67735992bd4?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radu Vunvulea
Dec 9, 2019·6 min read
Nowadays cloud is just a commodity, and the number of cloud providers is high (including the private one also). Most of them have a vast number of services that can be consumed from their marketplace, from classical VMs to messaging systems and data warehouse. Once you start working with multiple cloud providers, the maturity level of the cloud providers is crucial. When you want to optimise the cost and improve the quality attributes of IT solutions that you build or manage the maturity level is important.
WhatWhen we look at the service list provided by a cloud important, it is essential to take into account the level of management that needs to be done by ourself. It is not the same if a cloud provider offers us just an image with Redis inside a VM or Docker in comparison with a full SaaS offering.Depending on where each service is in the IaaS-PaaS-SaaS (and even FaaS) scale, the cost of development, automation, integration is impacted. The most significant impact is at management and operation, where it is a big difference between managing an IaaS vs PaaS or SaaS.
An excellent example of this case is the database. It is a big difference when the cloud provider offers the database as IaaS. In the IaaS context, you need to manage the VM, the backup of the database and all other stuff. In comparison with a PaaS where all these things are part of the service offer. The only thing that you need to do is to specify for how long you want to keep the backup, what is the recovery policy and so on — at one click distance.It is so easy to offer any service as an IaaS (just the VM and the application). But taking a part of the management and operation responsibilities, offering the ability of the user to configure features from a configuration dashboard is a different thing. You need the processes, the mechanism and teams that can do the magic behind the scene.
WhyI have seen a significant cost difference from the management and operation perspective for different cloud providers. Also, the development teams were complaining that for some specific cloud providers, the development and automation are slow and complicated.It was a trigger to invest time and resources to understand what is the root cause of all of this.
Cloud Maturity LevelTaking this into account, one of the essential factors that define the maturity level of a cloud provider is the percent of the services offered in IaaS vs PaaS and SaaS. In general, you don’t realise the impact of it until you start to use 2 or 3 different cloud providers. When you try to do the same thing in another cloud provider and you realise that you need to manage a VM, not PaaS services, things become expensive and more complex.
For example, at this moment in time, AWS and Azure are offering Cassandra DB as PaaS. We need to spin up the service and copy/paste the connection string. Inside IBM Cloud, Cassandra DB it is available only as IaaS, so you need to spin-up the VM, configure the Cassandra DB and so on.
ComparisonI took the top 4 cloud providers used by enterprises — Azure, AWS, GCP and IBM. For each of them, I counted the number of services that are IaaS versus the ones that are PaaS and IaaS. There might be some mistakes in the way how I marked each service as IaaS or PaaS and SaaS. Overall the result should represent the reality that we have now — at the end of 2019.The initial result was exciting but it was not so shocking. AWS and Azure — that leaders of public cloud -have less than 12% of their service as IaaS. In comparison with IBM, where more than 27% of their services are IaaS.
The result was aligned with the feedback received from the market. The next question that I had was related to what are the categories of services where IaaS offer has a high impact on the cost.I wanted to find out what is the service category that has the highest impact on these costs (IaaS vs PaaS and SaaS)
ComputationThe first category is the computation that has many formats nowadays — VM, microservices, serverless and many more. From this point of view more than 85% of the services offered by IBM in this category as IaaS. It was shocking because Azure and AWS have less than 46% and GCP only 21%.It was pretty clear from this metric, why the operation and management cost of IBM cloud is so high. All the other cloud providers are offering many services inside PaaS, in comparison with IBM, where you have plenty of service as VMs.
Data and StorageFrom the database and storage services point of view, cloud providers are smaller. The average of IaaS offer in comparison with PaaS and SaaS is around 15%. It is interesting that from the primary services offered by AWS on data and storage, all of them are PaaS and SaaS.The highest number of IaaS services in this category is for Azure (22%). Even so, if we analyse the services from data and storage category, we notify that the cores ones are PaaS. Some additional services from different providers are offered as IaaS.
Other categoriesI also tried to extract information related to other categories like messaging and events or IoT. The results obtained were not very concluded, and it was not clear what is the impact of end customers.
Result overviewThere are two types of service categories offered as IaaS that have a high impact on the operation and management cost. Computation, together with data storage, have a significant impact on this landscape, especially for enterprise applications. Most of the applications from these categories are storing plenty of data in different formats that need to be processed and fetch to other systems. When we combine this with NFR requirements, we realise that if redundancy, backups or auto-scaling are offered out of the box in a PaaS or SaaS offers it is the best things that can happen for the IT teams.Less management, fewer operations that need to be done by the teams means fewer issues, low complexity and better NFR provided to the end-customer.
ConclusionThe lever of IaaS services in comparison with PaaS and SaaS is a useful maturity metric. The insights provided by this metric can offer us an overview related to complexity and cost. It seems that there is a direct connection between the cost of management and operation of IT projects and the number of cloud services offered in IaaS vs PaaS and SaaS. Additional to this during the implementation phase, there is a high impact on quality attributes and the complexity of the solution.
2 
2 
2 
"
https://medium.com/strongnode/ama-with-crypto-talkz-how-strongnode-leverages-blockchain-and-edge-technology-and-deals-with-6fd3031a1f2f?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito joined the Crypto Talkz AMA episode last 21 September 2021. Daniel answered questions from the community about our company, products, and shared news on the upcoming $SNE token public sale and IDO launch.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
No need to worry if you missed it. Here are some of the highlights from the StrongNode and CryptoTalkz AMA session:
Could you describe StrongNode and give us an introduction to the project?
Daniel Saito: StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. StrongNode was the name picked out by our amazing marketing team.
The mission and vision of Strongnode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies. We wanted to address the last mile, which is the EDGE and build a global edge network through access and monetization of idle resources.
Tell us, What are your main features that distinguish you from other projects and what competitive advantages do you have?
Daniel Saito: I just pointed out the exact use case of how it is easy to accumulate StrongNode $SNE tokens. So you probably want to know where it all comes from.
When we first started the thought of StrongNode we looked outside to see what we can do to help solve some of the world’s problems. Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it.
With that said, we had to come up with a solution or an alternative and looking inwards of what is available we noticed that we already had the blueprint already in front of us in the form of Open Source.
What differentiates us from the rest is that we introduce new ideas and concepts into peoples minds of ways to passively earn income. We are onboarding non-crypto people into the ecosystem through a simple installation of a binary on their Smart TV.
One of your advantages is that you will combine Edge technology with blockchain to create an edge networking. How will you combine Edge with blockchain together? How will this create a network effect to scale and grow the StrongNode? How will this increase your network value?
Daniel Saito: I would like to see 1 person = 1 node. That one person has a wallet in their control which is registered and tethered to several digital devices as they are paired upon installing a binary onto their PC, Mobile, TV, etc.. This node will join many nodes, now we have safety in many. As the node count increases, there will be more devices on the network that will contribute some sort of digital resource (cpu, gpu, bandwidth and storage).
At a certain point when you aggregate enough resources together you arrive at an inflection point where there is enough resources to get 99.999% uptime and there will be enough theoretical computing power to tap into large scale BIG DATA processing.
Regulation is very important. Several projects in many countries were closed due to a lack of necessary licencing. How does StrongNode deal with this problem? Are you working on a project that complies with regulations?
Daniel Saito: This is a great question actually, first time actually being asked this question. We were waiting for this question to be asked actually. Mainly we know that we need to be compliant and respectful to countries and jurisdictions. We have KYC’d every dollar in and will also KYC all users logging into our network. There needs to be control mechanisms in place where user registration means something and users are accountable if they access the network. We will have terms of services and privacy policy like a normal service provider offers.
With that said we have also architected StrongNode to have data sovereignty on our master nodes, so where we place our Nodes are in locations around the world (Dubai, Germany, etc..) StrongNode servers are kept private and encrypted in a deep bunker.
Daniel: Please come join our journey and see what is possible with the world’s computers at your fingertips. https://t.me/StrongNodeChat
For more details on StrongNode, visit us: http://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
214 
214 
214 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/lattice-research/iot-considerations-server-side-iaas-paas-saas-1f55afc03185?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
Next in the series on series of IoT considerations is the varied landscape of infrastructure in the cloud — Infrastructure as a Service, Platform as a Service and Software as a Service.
Regardless of what you’re doing, you’ll need a place to run your backend server code. This will generally do everything from allowing the hardware to connect, authentication and access control, security, provide the API and web services, serve frontend resources, run some logic, etc etc.
In the real world, these may not all come out of the same piece of server code. By using services or micro-services you can split up the code into more logical chunks. But for now, we’re looking at ways of running code in the cloud instead of how it is architectured, so let’s assume a single chunk of code and move on! We’ll talk micro-services (and containers) another time.
There’s, er, a lot of *aaS on the scene these days. No, i’m not intentionally being crude — the “aaS” stands for “as-a-service” and it can be prefixed by nearly anything.
Infrastructure-, Platform-, Software-, Whatever you’re having- as a service. You usually pay a monthly fee and get a service, be it a piece of software you can use, a platform or service on which to run an application, or a virtual server, with which you can do what you want.
And don’t forget — you could always just put some servers on a rack in a datacentre yourself! So, there are a lot of options, let’s take a look through them.
Cloud services can greatly reduce time to market, take the hassle out of managing infrastructure and generally make things easier, usually at a price.
Cloud services can broadly be broken down into three categories.
SaaS (Software) is something you simply sign up for and generally start using with little or no customisation. For example, Google Apps, Office 365, Basecamp, etc. You simply consume SaaS.
PaaS (Platform) is something which you use to help build or run your own applications more easily — e.g. a managed environment for running code or a DB as a service — examples are Google Compute Engine, Amazon RDS database server, Michrosoft SQL Azure, Amazon Elasticsearch, Heroku. You can build your applications on these platforms.
IaaS (Infrastructure) is core IT infrastructure — e.g either rent a virtual server and do with it as you will, or to provide solutions for file storage, load balancing or some other “network type” problem. Examples are Amazon EC2, Amazon S3, Google Compute Engine, Azure Virtual Machines. These things can replace existing IT infrastructure — e.g. migrate to it.
So, let’s start from the deepest level and work our way up.
Infrastructure as a Service is like the nuts and bolts. It doesn’t tell you how to do things, it just gives you the infrastructure to do it.
For example, Amazon AWS, Google Cloud and Microsoft Azure (among many others) all provide many of the traditional “IT” or operational functions as a service.
The most widely used case is that instead of building a new server from scratch and loading windows or linux, you can just click your mouse a few times and spin up a virtual machine, to your specs, pre-loaded with the OS of your choice.
There is no capital cost, instead you pay as you use it. If it is only running for one hour of the day this is what you pay for. Or, if you commit to running the server for multiple years and upfront you can get substantial discounts on running costs.
Other services, like load balancing, cloud file storage, administration, caching and security/firewall/ddos prevention are also available from many vendors.
IaaS is completely non-opinionated — that is, they provide a service to you to do whatever you wish with it.
It’s an evolution of what happened physically over the last 20 years of the web — but instead of deploying physical products you can now purchase a service in just a few clicks.
A Platform as a Service is a little higher level than IaaS. You leverage the platform to build your own applications — but by and large the reality of what is underlying the platform is abstracted away from you.
For running web applications in various languages, platforms such as Google Compute Engine, Heroku, Azure App Service or AppFog let you write and upload your code and just set it running. AppFog have a nice tagline which sums the concept up quite well - “Work on Code, Not Infrastructure Management”.
This is the key to a PaaS — purists might argue that you are losing control — and this is true to an extent — but the counter argument is that when building a new product or startup, surely you should focus 100% of your energy on your product, not building the infrastructure to run it?
But — and it’s a big one — there is one proviso. I’m not completely sold on using a DBaaS (DB as a service). Like the platforms for running code, DBaaS gives you a platform in which to put your data.
You can buy DBaaS in nearly any format (SQL, DocumentDB, BigTable, GraphDB, etc) which behave identically and are API and query equivalent to their product equivalents (e.g. Sql Server / PostgreSQL / MongoDB / Cassandra / Neo4J etc)
The downside of running your DB in a hosted environment alongside thousands or millions of other people’s DBs is one of threat — these services are prime targets for malicious attacks which could give others access to your account or data — and possibly lead to the comprimise or destruction of data. See an example from 2013 here.
Of course, this could happen on a code-hosting PaaS too — but if something happens there you just redirect the domain to an identical copy running on a different host, or spin up your own VM and self-host it. But when you have terabytes of user data, it’s more difficult to just “spin up” another DB.
That’s not to say that hosting it yourself or on a virtual server is much more secure — in fact, if mis-configured, it could be much worse. But at least you are the arbiter of your own data’s destiny.
Software as a service is an interesting one. You may use it for your core business functions — or you may use something which is a mix of SaaS/PaaS in one to build applications on too.
Straight SaaS soultions will help you run your day-to-day business (Office 365, Google Apps, Gmail, Basecamp, Zoho), manage your development (Github, Bitbucket, Jira, Bamboo, Slack), automate your processes or tasks (Zapier, Workato, etc), help you track application logs (Loggly) or application/server performance (New Relic)
Using these is pretty much a given for any startup on non-enterprise customer, unless you absolutely *must* run your own local software.
But there are more powerful SaaS offerings which blur the line between SaaS and PaaS. Anyone who has worked in a medium sized organisation will be familiar with CRM. Incumbent enterprise solutions are Salesforce and Microsoft Dynamics.
Salesforce is a good example — you pay your money, you use your CRM. But it actually goes far beyond that — allowing custom “objects” (like tables) in the database, fully customised UI layouts and custom logic. Even further, it lets you build custom html5 based pages, and even apps.
So utilising SaaS can go far beyond just productivity — you could even build your business on it.
Of course, the is the decades-old approach where you buy some servers, put them in a rack, build your own server environment, infrastructure and code, and put it live.
If you have certain requirements — extreme security, heavy processing, a need not to have multi-tenancy code running in your stack, then it’s worth considering.
If not, then this is generally not the favoured approach to getting up and running quickly today. It takes time and effort to set up, but you do fully “own” the infrastructure, and if this is essential to you, then go with it.
If you’re trying to rapidly push a product to market, it would be foolish not to at least evaluate IaaS/PaaS/SaaS to see if it can speed up your development time. Make sure you consider it carefully before you kick off your IoT project.
Next time, I’ll take a look at the world of Database and storage technology — which has advanced a long way form the days of simple tables and files — and how you can leverage new ways of storing your data.
Update — Part 7 is now available: IoT considerations — storage and database, SQL, NoSQL, historical data
—
Des Flynn is CTO of Lattice Research, who help companies to design, build, deploy, operate and service innovative and cost-effective IoT control systems to meet their customer’s needs. More information at www.lattice.ie
We help companies to design, build, deploy, operate and…
4 
4 claps
4 
Written by
Technologist, System Architect, Developer. Cofounder and CTO at @latticeresearch (#IoT platform for #business innovation)
We help companies to design, build, deploy, operate and service innovative and cost-effective IoT control systems to meet their customer’s needs. http://www.lattice.ie
Written by
Technologist, System Architect, Developer. Cofounder and CTO at @latticeresearch (#IoT platform for #business innovation)
We help companies to design, build, deploy, operate and service innovative and cost-effective IoT control systems to meet their customer’s needs. http://www.lattice.ie
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aheadcrm/google-and-sap-a-marriage-in-the-clouds-2e33526c8785?source=search_post---------141,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Mar 10, 2017·4 min read
On Mach 8, 2017, SAP and Google announced another marriage in the cloud during Google’s Cloud Next event: SAP HANA is certified on Google’s Cloud Platform GCP, and is generally available now. SAP Cloud Platform and more products and solutions are to follow.
The Google Cloud Launcher marketplace will be utilized to offer and deploy to and for customers and partners, starting with SAP HANA, express edition, which is already available, too.
Further topics that are covered by this partnership are
More on the still fuzzy side are end-to-end integrations and collaborations in the areas of AI and machine learning.
True to the SAP mantra of being an ecosystem player this is all about choice — choice for the customer to implement what is best for them.
Another interesting one!
SAP now covers all major cloud platforms. HANA is now certified on AWS, Azure, and GCS, apart from running in the SAP cloud. With this SAP now has the broadest footprint when it comes to running on an IaaS platform.
With the SAP Cloud Platform being available soon there also will be a very powerful PaaS solution on one of the strongest IaaS.
It is interesting that there is no mention of the legacy software (SAP Business Suite) at the moment, although with HANA running on GCS it should be possible to migrate a Business Suite installation to GCS — as long as it runs on HANA — in near future.
Another interesting aspect is that in the productivity arena there are a few overlaps between the G Suite and SAP solutions — think SAP Jam vs. Google Hangout. How easy will it be to use Hangout instead of (built in) Jam in future? Is that interesting for Google at all?
However, far more interesting are the allegations of future potential: Bernd Leukert explicitly mentions end-to-end business processes and machine learning with some next announcements to be expected at the next SAPPHIRE NOW. For SAP this is where the real juice is: Like Salesforce and Oracle their CLEA solutions predominantly rely on company internal data and lack the far reach of external data. This is where Google (and IBM Watson) step in by their ability to contribute relevant insight from the outside. So, this partnership essentially closes a gap between SAP and Microsoft — while giving an edge above Salesforce, who just announced an AI partnership with IBM Watson, which on top cannot be expected to be targeted towards CRM types of applications as well.
Lastly, GCP provides an ideal bed to run and scale IoT applications with their expected throughput- and scalability requirements.
Google gets an industry heavyweight to provide load on their infrastructure. Especially, if existing on-premise customers can get incentivized to migrate from their still predominantly Oracle-based instances to HANA based GCP instances; this can become a big one, as there are still tens of thousands of these instances available. Think the joint effort into containerization here …
And the availability of SAP HANA Express Edition and soon the SAP Cloud Platform should drive a good number of developers onto the Google cloud.
Additionally, it gives Google the opportunity to penetrate a Microsoft fortress: Microsoft Office is still very much a synonym for productivity apps in Enterprises.
Lastly, and probably most importantly, the AI angle. Business AI needs both: Insight from inside the company and from outside the company. Vendor owned and driven AIs have a hard time delivering this. With the notable exception of Microsoft. Companies like Google, Facebook, Apple, Baidu, Twitter,… and some specialized on business intelligence sit on an asset that enterprise software vendors desperately need. So, these might be the secret winners of the enterprise software clash of the titans.
So, overall there is a big gain for Google in Enterprises looming.
There is a fight for dominance going on in Enterprise software. With Microsoft, Oracle, Salesforce, and SAP here are four main tribes. In general terms of enterprise software this partnership gives SAP some more headway against the strong competition, especially if SAP also gets their ecosystem strategy implemented somewhat better — they still are fairly hard to play with.
On the CRM side this tack brings them closer to Microsoft and Salesforce.
The race goes on.
All around good. Especially as it seems that this was a customer driven (Colgate Palmolive) innovation. SAP offers most choice but also needs to offer some guidance when it comes to choosing. Given that SAP or their implementation partners deliver this guidance, there is considerable gain in this partnership: Additional competition in infrastructure, more possibilities in productivity, and so on.
For customers it all boils down to being enabled making the right choice.
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
See all (1,289)
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-highlights-with-gains-chat-the-executive-team-focuses-on-the-sne-token-and-the-strongnode-1cbb1550fa36?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 22, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, and our CEO and Co-founder Daniel Saito, CTO Colin Charles, and CPO Gil Bashan were hosted by the Gains Chat community in this informative AMA episode. The executives answered questions regarding our upcoming IDO launch and the $SNE token utility and the ecosystem.
Our IDO is happening with our partners Starter.xyz and Bull Perks. Learn how to participate in our IDO process here: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
In case you missed it, take a look at some of the highlights from the StrongNode and Polygon Mania AMA session posted below:
Did you raise funds so far? If so, how did you handle them? Are you planning to do any future raises?
So far the token sale is going well! We have had unanimous success in our fundraising from our customers. Yes, we sold our tokens who would want to use it in the overall workflow in reducing the costs of their computational spend.
When will you have the public sale, and what role will the token play in the StrongNode ecosystem?
We will first start by announcing USER REGISTRATION of all users that will participate in the node network and will require to REGISTER for a StrongNodeID. This will not only get you some free StrongNode tokens, but you will manage your devices.
Colin: ​​The tokens can be earned by nodes when they run CPU-intensive jobs or even be a network endpoint. The token can be spent when you send jobs. The token can also be traded, but I’m sure Daniel will tell you more about that.
Daniel: ​​Yes, we foresee that our token will have A LOT of transactions. With that, we are adding a minuscule “reflect” tax on the token. This tax will be spent on financing a SAFU fund and conduct token buybacks for redistribution to the token holders.
Gil: I would like to add that like I’m sure many of you have seen, we’ve seen so many reward tokens follow a common death spiral because their tokens don’t have utility.
The StrongNode token has so many use cases within the StrongNode ecosystem which includes our innovation lab projects focused on gaming, social impact, entertainment, lifestyle platforms building on the StrongNode Edge technology.
Companies will mostly be paying for services via traditional methods, via the $SNE token, or a combination of both (with a lot of incentive to use $SNE). If traditional FIAT methods are chosen, a portion of that would be used to buy back StrongNode token and distribute a portion back to edge resource providers.
There will be many incentives to token holders baked in, especially for HODLers like most of us are (more to be announced). Soon after launch, farming will launch, and then other AMM strategies.
Do I need to complete any KYC with StrongNode Edge to participate in any IDO or anything related to this?
Daniel: Yes, you need KYC enabled to use StrongNode, this is similar to logging into the computer. So we need to know what *wallet* is doing what and or contributing what. ALL private information that reveals your IDENTITY is with our 3rd party KYC provider.
Can a person without knowledge of blockchain and technology take advantage of the products in StrongNode? Or is the platform only for professionals?
Colin: 100% we want to ensure everyone even non-crypto (“normies”) can get onboard. This is why we make a simple tray table app to ensure that end-users get on with a StrongNode ID. We also have UX folk ensuring that it isn’t too crypto-oriented. As for the big data stuff, sure, we have to work on making that even more user-friendly; but the VPN endpoint? That will be the same as logging onto any VPN you’re used to now.
Some products you’re working on at the moment include StrongNodeID (SSO, open ID, KYC, etc.). What is the utility of StronNodeID? When can we access it? How will it be beneficial to your users and how can they earn from it?
Colin: StrongNodeID is there to ensure you can access the platform, either as a node, or to use it as a consumer or dispatcher of data. It is core/central to how we operate, naturally, since it is your access ID. It is also how you utilize everything coming out of our innovation lab (SSO). Part of the onboarding launch will ensure you see the first use of StrongNodeID. Earn from it?… well… no ID… no play.
Daniel: Comes as an NFT that pairs with your wallet making it REALLY UNIQUE and non-movable, where if the system discovers that it’s NFT it will be your passport and will allow you entry frictionless.
Gil: There will be different levels of access. For instance, on our gaming platform OGLife (Original Gamer Life or OGL.tv), depending on the level of KYC, you could have almost full access to everything, but if you are in let’s say North Korea, might not get the full benefit of the portions built on DeFi.
To learn more about our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
Please follow us and join us:
💻 http://strongnode.io
🐤 https://twitter.com/StrongNodeEdge
📚 https://www.reddit.com/r/strongnode/
📞 https://t.me/strongnodechat — Main TG Chat
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
1.5K 
1.5K 
1.5K 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/geekculture/anything-as-a-service-xaas-523f55aa510c?source=search_post---------343,"There are currently no responses for this story.
Be the first to respond.
Certainly, you have come across a service that is named “XaaS”, either SaaS, PaaS, etc… But what are these XaaS? Today I’ll attempt to explain to you a bit more about these customer services, give you some examples and broaden your XaaS knowledge.
This term refers to solutions that are presented ‘as a service’ to a customer. This can be any type of service that…
"
https://medium.com/@shawn.ho/%E7%95%B6%E5%AE%B9%E5%99%A8%E9%81%87%E5%88%B0vra-iaas-%E6%BA%AB%E6%95%85%E7%AF%87-7364032f8e3f?source=search_post---------236,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shawn Ho
Jan 1, 2019·3 min read
好久好久以前我介紹過一篇VMware Admiral的使用方式，之後由於個人的頹廢，沒有再繼續更新後續。其實在一年前，Admiral已經悄悄的融合進vRealize Automation 7.3版 （2019/01/01的vRA已經是7.5版的）。
在過去的一年中，我其實也發現很多捧油不知道該如何使用vRA 7.3裡的Container (也就是Admiral的變形），在2019年的第一篇，我們就來看一下這個Tab該怎麼玩。
溫故：我們先回憶一下，之前Admiral裡面的服務該如何佈建出來？對了！就是透過Docker Host (如前篇1.中所述），不過最近跟一些同好切磋，發現很多人不會設定Docker Host，所以我們來敘述一下:
4. 進行ubuntu更新，並安裝docker
5. 將本身帳號加入docker使用群組，並記得要重新登入，讓權限生效
Docker安裝完成後，我們開啟Docker Host的遠端API服務，參照Docker標準遠端API文件，原始的Docker Service設定在/lib/systemd/system/docker.socket。透過新增一個設定檔在 /etc/systemd/system/docker.srvice/startup_options.conf，我們可以在不影響更改原有文件下，來覆寫原有設定
原始文件中，是指定
但筆者使用fd://在Ubuntu 16.04中，執行會有問題。-H 是指定Dockerd要綁定的socket port，unix:// or fd:// 都是指本機I/O端口，tcp://0.0.0.0:2376是指綁定該台機器上的2376 port，這個是Admiral 遠端 Host的必要步驟。
重新Load並重開dockerd服務
這樣就大功告成。可以透過在其他台有安裝Docker CLI的機器上，設定環境變數DOCKER_HOST（舉例：export DOCKER_HOST=[docker-host-ip]:2376)，就可以測試我們的Remote Docker API是否開啟成功囉！
一個心態年輕的中年大叔。年輕時不學好~ 台大電機畢業後，去美國取得了博士學位，念完博士後，不想教書。當過補習班老師，碼農，產品總監，ISO稽核顧問，技術銷售，目前在Google Cloud跟客戶一起玩Kubernetes，也跟喜歡的客戶在金融, 政府, 電信, 高科技業內共同成長學習是斜槓人生的好案例。
一個心態年輕的中年大叔。年輕時不學好~ 台大電機畢業後，去美國取得了博士學位，念完博士後，不想教書。當過補習班老師，碼農，產品總監，ISO稽核顧問，技術銷售，目前在Google Cloud跟客戶一起玩Kubernetes，也跟喜歡的客戶在金融, 政府, 電信, 高科技業內共同成長學習是斜槓人生的好案例。
"
https://medium.com/@renebuest/the-way-to-the-holy-iaas-grail-96f04536dfc5?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·9 min read
Before using IaaS there is the fundamental question how and for which purpose the cloud infrastructure will be used. In this context the capacity planning plays a decisive role. In most cases companies know their applications and workloads and thus can estimate how scalable the infrastructure regarding performance and availability must be. However, scalability must also be considered from a global point of view. If the company focuses mainly on the German or DACH market a local provider with a data center in Germany is enough to serve the customers. If the company wants to expand in global markets in the midterm, a provider with a global footprint is to recommend who also operates data centers in the target markets. Following questions are:
Talking about scalability the term “hyper scaler” is often used. These are provider whose cloud infrastructure theoretically is capable to scale endlessly. To these belong Amazon Web Services, Microsoft Azure and Google. The term endlessly should be treat with caution. Even the Big Boys hit the wall. Finally the virtual infrastructure based on physical systems and hardware doesn’t scale.
Companies who have a global strategy to grow into their target markets in the midterm should concentrate on an international operating provider. Among the above named Amazon AWS, Google, and Microsoft also HP, IBM (Softlayer) or Rackspace come into play which are operating a public or managed cloud offering. Who sets on a “global scaler” from the beginning gets an advantage later on. The virtual infrastructure and the running applications and workloads on top of it can be deployed easier to accelerate the time to market.
Cloud connectivity (low latency, high throughput and availability) should also not be underestimated. Is it enough that the provider and its data centers are only able to serve the German market or exists a worldwide-distributed infrastructure of data centers, which are linked to each other?
Two more parameters are the cloud model and the related type of service. Furthermore, hybrid and multi cloud scenarios should be considered. Following questions are:
Current offerings distinguish public, hosted and managed private clouds. Public clouds are built on a shared infrastructure and are mainly used by service providers. Customers share the same physical infrastructure and are logically separated based on a virtualized security infrastructure. Web applications are an ideal use case for public clouds since standardized infrastructure and services are sufficient. A hosted cloud model transfers the ideas of the public cloud into a hosted version administered by a local provider. All customers are located on the same physical infrastructure and are virtual separated from each other. In most cases the cloud provider operates a local data center. A managed private cloud is an advanced version of a hosted cloud. It is especially attractive to those companies who want to avoid the public cloud model (shared infrastructure, multi-tenancy) but do not have the financial resources and the knowledge to run a cloud in the own IT infrastructure. In this case, the provider operates an exclusive and dedicated area on its physical infrastructure for the customers. The customer is able to use the managed private cloud exactly like a public cloud but on a non-shared infrastructure, which is located in a provider’s data center. In addition, the provider offers consultancy services to help the customer to transfer its applications and systems in the cloud or to develop them from scratch.
These hyper scaler respectively global scalers named above are mainly public cloud provider. Offering a self-service model the customers are responsible for building and operating the virtual infrastructure respectively the applications running on top of the infrastructure. In particular cloud player like Amazon AWS, Microsoft Azure and Google GCE are offering their infrastructure services based on a public cloud model and a self-service. Partner networks are helping the customers to build and run the virtual infrastructure, applications and workloads. Public cloud IaaS offerings with a self-service are very limited in Germany. The only providers are ProfitBricks and JiffyBox by domainFactory. However, JiffyBox’s focus is on webhosting and not enterprise workloads. CloudSigma from Switzerland should be named as a native provider in DACH. This German reality also reflects the provider’s strategies. The very first German public IaaS provider ScaleUp Technologies (2009) completely renewed its business model by focusing on managed hosting plus consultancy services.
Consultancy is the keyword in Germany. This is the biggest differentiator to the international markets. German companies prefer hosted and managed cloud environments including extensive service and value-added services. In this area providers like T-Systems, Dimension Data, Cancom, Pironet NDH or Claranet are present. HP has also recognized this trend and serves consultancy services in addition to its OpenStack-based HP Helion Cloud offering.
Hybrid- und multi-cloud environments shouldn’t be neglected in the future. A hybrid cloud connects a private cloud with the resources of a public cloud. In this case, a company operates an own cloud and uses the scalability and economies of scale from a public cloud provider to get further resources like compute, storage or other services on demand. A multi-cloud concept extends the hybrid cloud idea with the number of clouds that are connected. More precisely, it is about n-clouds that are connected, integrated or used in any form. For example, cloud infrastructures are connected so that the applications can use several infrastructure or services in parallel, depending on the capacity utilization or according to the current prices. Even the distributed or parallel storage of data is possible in order to ensure the availability of the data. It is not necessary that a company connect each cloud that is used to run a multi-cloud scenario. If more than two SaaS applications are part of the cloud environment it is basically already a multi-cloud setup.
At application level Amazon AWS doesn’t offer extensive hybrid cloud functionalities at present but is ever expanding. Google doesn’t offer any hybrid cloud capabilities. Because of public and private cloud solutions Microsoft and HP are able to offer hybrid cloud scenarios on a global scale. In addition, Microsoft has the Cloud OS Partner Network, which enables companies to build Microsoft based hybrid clouds together with a hosting provider. As a German provider T-Systems has the capabilities to build hybrid clouds on a local level as well as on a global scale. Local providers like Pironet NDH are offering hybrid capabilities on German ground.
Since Edward Snowden and the NSA scandal happened many legends have been created around data privacy and data security. Providers, especially from Germany, advertise with a higher security and protection against espionage and other attacks when the data is stored in a German data center. The confusion. When it comes to security, two different terms are frequently being mixed: data security and data privacy.
Data security means the implementation of all technical and organizational procedures in order to ensure confidentially, availability and integrity for all IT systems. Public cloud providers by far offer better security than a small business is able to achieve. This is due to the investments that cloud providers are making to build and maintain their cloud infrastructures. In addition, they employ staff with the right mix of skills and have created appropriate organizational structures. For this reason, they are annually investing billions of US dollars. There are only few companies outside of the IT industry that are able to achieve the same level of IT security.
Data privacy is about the protection of personal rights and privacy during the data processing. This topic leads to the biggest headaches for most companies, due to the fact that the legislative authority can’t take it easy. This means that a customer has to audit the cloud provider in compliance with the local federal data protection act. In this case, it is advisable to use the expert report of a public auditor since it is time and resource consuming for a public cloud provider to be audited by each of its customers. Data privacy is a very important topic; after all, it is about a sensitive dataset. However, it is essentially a topic of legal interest that must be ensured by data security procedures.
A German data center as a protection against the espionage of friendly countries is and will stay a myth. When there’s a will, there’s a way. When an attacker wants to get the data it is only about the criminal energy he is willing to undertake and the funds he is able to invest. If the technical challenges are too high, there is still the human factor as an option — and a human is generally “purchasable”.
However, US American cloud players have recognized the concerns of German companies and have announced or started to offer their services from German data centers. Among other Salesforce (partnership with T-Systems), VMware, Oracle and Amazon Web Services. Nevertheless, a German data center has nothing to do with a higher data security. It just fulfills
During the general technical assessment of an IaaS provider the following characteristics should be considered:
Scalability is the characteristic to increase the overall performance of a system by adding more resources like complete computing units or granular units like CPU or RAM. Using this approach the system performance is capable to grow linear with the increasing demand. So, unexpected load peaks can be absorbed and the system doesn’t break down. Scalability differs scale-up and scale-out. Scale-out (horizontal scalability) increases the system performance by adding complete compute units (virtual machines) to the overall system. In contrast, scale-up (vertical scalability) increases the system performance by adding further granular resources to the overall system. These resources can be storage, CPU or RAM. Taking a closer look on the top cloud applications, these are mainly developed by startups, uncritical workloads or developments from scratch. Attention should be paid to the scale-out concept, which makes it complicated for enterprises to move their applications and systems into the cloud. At the end of the day, the customer has to develop everything from scratch since a non-distributed developed system doesn’t work, as it should run on a distributed scale-out cloud infrastructure.
IT decision makers should consider that their IT architects are detaching from the underlying infrastructure in the future to move applications and workloads over different providers without borders. Container technologies like Docker make this possible. From the IT decision makers point of view thus the selection of a provider that supports e.g. Docker is a strategic tool to optimize modern application deployments. Docker helps to ensure the portability of an application to increase the availability and decrease the overall risk.
Hybrid and multi cloud scenarios are not only a trend but reflects the reality. Cloud provider should act in terms of their customers and instead of using a proprietary technology also set on open source technologies respectively a de-facto standard like OpenStack. Thus they enable the interoperability between cloud service provider and creating the requirements for a comprehensive ecosystem, in which users are getting a better comparability as well as the capabilities to build and manage truly multi cloud environments. This is the groundwork to empower IT buyer to benefit from the strength of individual provider and the best offerings on the market. Open approaches like OpenStack are fostering the prospective ability to act of IT buyer across provider and data center borders. This makes OpenStack to an important cloud-sourcing driver.
Depending on the requirements the way to the holy IaaS grail can become very rocky. In particular, enterprise workloads are more difficult to handle as novel web applications. Regardless of this, it must be considered that applications, which are running on IaaS must be developed from scratch. This depends on the particular provider. But in most cases this is necessary in order to use the specific provider occurrences. Mastering the individual path the following point of views can help:
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@glenprobinson/the-matrix-will-run-on-custom-hardware-3839741645f9?source=search_post---------342,"Sign in
There are currently no responses for this story.
Be the first to respond.
Glen Robinson
Jun 9, 2017·3 min read
In 2006, Amazon EC2 came along and changed the way we do things. Some of the most celebrated and embraced features of IaaS such as elasticity, pay as you go, t-shirt size infrastructure, automation, API interfaces and the like are well understood. I don’t see those core tenets of IaaS changing anytime soon.
But there was another feature which is less broadly discussed. The shift to x86 architectures. Suddenly we didn’t have a choice when it came to exploiting specific hardware capability. It was x86 or not at all. This has created inertia around some application types as they are hard coupled to specific chips and device requirements. Network devices are an obvious example. In the old world, they would run on hardware that had all manner of ASICs to support specific functionality. SDN and NFV that isn’t provided by the cloud provider directly, but via the ISV ecosystem, is now dependent on virtual appliances running on x86 based virtual machines.
To be fair, we’ve seen cloud providers offer up GPU technology, and we’ve seen some of them offer a hardware virtualization model which does expose some of the underlying chip capabilities, but for the most part, we’re still running on Intel based x86 architectures.
But is this about to change? In a previous post covering punctuated equilibrium in cloud, I wrote about the TPU processor that Google has brought us via its cloud platform. It’s already on V2 of the design, and it is showing great promise for running specialist workload types.
Recently, ARM announced its new A75 chip which will allow AI to run on devices at greater performance whilst being very power efficient.
The Internet of Things, and Machine Intelligence are two disruptive technology areas which have the ability to disrupt the current status quo — that being, the standardization onto x86. I feel that for many years, at least from an IaaS point of view, organizations have been seeing this as a positive move, and looking for investment and opportunity to shift their workloads away from custom hardware and software — for example, Itanium, HP UX — and onto an x86 architecture. In my experience, this is normally coupled with a move to open source software initiative, the most obvious being the swing to open source Linux.
The benefits of IoT running on custom hardware that offers the benefits of the A75 ARM chip, and MI running on TPU processors, are compelling enough to start a swing for multiple workload types back to destinations that support custom hardware. The big cloud providers have been customizing hardware for some time and offering it back to us abstracted behind cloud services. Today I’d suggest this is mostly manifested in the network layer, and we get all sorts of cool cloud network functions that are supported behind the scenes by custom hardware designs. But will we start to see some of that hardware exposed to allow us to drop our applications on top, and get direct access to the chip specific benefits? This would be a huge market for ISVs to build improved virtualized appliances that are no longer constrained to x86 VMs. The benefit to the end consumer of the cloud service would be huge, and I feel innovation in this space has been sorely lacking. As we see the growth of edge computing, I feel the design will be very much predicated on custom hardware designs, and the ARM A75 chip is a great example of a design that will be prevalent in the hardware architectures, at least, I’m sure that’s what ARM are hoping.
As we look forward and see all this coming together, the Matrix (cloud, IoT, MI, and so on) will demand more specialist hardware as running it all centralized, on x86 architectures, will constrain it too much. I believe we are about to see the battle for application run-time venue be dominated by big cloud vendors differentiating on custom hardware.
Watch this space, I certainly am.
Adventurer, traveler, surfer, loving life, growing through constant experiences.
3 
3 
3 
Adventurer, traveler, surfer, loving life, growing through constant experiences.
"
https://medium.com/@soniacomp/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-iaas-%EB%A5%BC-%EC%A7%80%ED%83%B1%ED%95%98%EB%8A%94-%EA%B8%B0%EC%88%A0-1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-95a8e77faeda?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
SoniaComp
Apr 14, 2021·36 min read
큰 회사의 개발자들과 대화를 할 기회가 생겼다. 물론 그 자리가 나의 기술적인 기본기를 입증하고, 내가 기록한 포트폴리오를 검증하는 자리이기 때문에 긴장도 된다. 하지만, 내가 일하고 싶었던 분야에서 치열하게 길을 개척하고 있는 개발자들과 무려 1시간이나 함께 기술적인 이야기를 나누게 되다니!! 기대된당~..~
작년에 회사에서 1인 개발자로 지내면서, 다른 개발자를 찾기가 쉽지 않았다. 그래서 알고리즘 스터디를 찾아서 하기도 하고, 회사를 통해 퇴사한 개발자를 찾아가 조언을 구하기도 했다. 그때마다 내가 느낀 건, 개발자에게 가장 큰 benefit 중 하나가 성장이라는 것이다. 그래서 나는 내가 더 많이 배우는 입장이긴 하지만, 상대방에게도 뭔가 도움을 주고 싶어, 멘토들과 커뮤니케이션 할 때, 더 많은 자료 조사를 하려고 노력했다.
다음주에 있을 선배 개발자들의 대화가, 기본적인 CS 지식을 다시 한번 짚고 넘어가는 데 급급한 시간들이 아닌, 기술적인 피드백을 주고 받는 시간이 되었으면 좋겠다. 그래서 나에게도, 현직 개발자들에게도 즐거운 대화를 나눌 수 있는 시간이 되었으면 좋겠다!! 그래서 이 프로젝트를 진행하고자 한다. 스따뜨~
인터넷: 네크워크들의 비호환성 문제 해결 기술 → 한가지 LAN 기술로는 전 세계를 연결할 수 없다.
비호환성 문제: 공통 기술 개발 → IP → 비호환 기술이 만나는 곳: 라우터
특정 목적을 달성하기 위해 협조하는 둘 이상의 당사자 사이에 사용되는 엄밀히 약속된 ‘언어’
프로토콜 spec(규약)
인터넷 프로토콜 스위트(영어: Internet Protocol Suite)는 인터넷에서 컴퓨터들이 서로 정보를 주고받는 데 쓰이는 통신규약(프로토콜)의 모음이다. 인터넷 프로토콜 슈트 중 TCP와 IP가 가장 많이 쓰이기 때문에 TCP/IP 프로토콜 슈트라고도 불린다.
계층화 원칙 → 모듈화
010101 → 아날로그 신호
물리적 주소: MAC주소
[ 프레임 ]
IP가 데이터 링크 계층에 요구하는 것
게이트 웨이: IP는 각 라우터에서 목적지 IP 주소를 가기 위해 다음에 거쳐야 할 라우터(‘홉')을 알아낼 수 있다.
[ 타입 ]
[ IP 에게 인터페이스는 커널 내의 자료구조 → 인터페이스가 가진 정보 ]
[ 인터페이스: 하드웨어 NIC을 커널 내에서 대표하는 자료 구조 — 예외: 로컬 룹백(Loopback) 인터페이스네트워크 계층 ]
“넷”들의 호환성 문제를 극복하여 연결한다
IP: 배달 [ 최종 목적지 — 헤더 / Next HOP 주소 ]
라우팅 프로토콜: 경로계산
프로세스간 통신(IPC: interprocess communication)
프로토콜 데이터 단위(Protocol Data Unit): 상위 계층이 전달한 데이터에 붙는 제어 정보
그 외 용어
[ 브라우저에서 일어나는 일 ]
[ 프로토콜 스택, LAN 어댑터에서 일어나는 일 ]
[ 허브, 스위치,라우터에서 일어나는 일 ]
만약, 사내 메신저 같이 LAN 안에서 일어나는 일이라면, 사내 스위치와 라우터 만으로도 충분히 통신이 가능할 것이다. → 스위치를 통해 라우터에 도착하고, 라우터가 다른 사내 라우터로 전달하고, 라우터에서 다시 스위치로 전달
만약, 외부 인터넷 망에 접속해야 한다면, 인터넷 접속용 라우터를 통해 패킷을 프로바이더(통신사)에게 전달하여 인터넷에 접속
[ 액세스 회선, 프로바이더에서 일어나는 일 ]
POP(통신사용 라우터)를 거쳐 수많은 고속 라우터들을 통해 패킷이 목적지 서버까지 도착한다.
[ 방화벽, 캐시 서버에서 일어나는 일 ]
[ 웹 서버에서 일어나는 일 ]
API는 기능을 제어할 수 있게 만든 인터페이스
역할
REST라는 아키텍처를 구현하는 웹 서비스
일정 시간 동안 같은 브라우저로부터 들어오는 요청을 하나의 상태로 보고, 그 상태를 유지하기 위한 기술
자원을 적절한/유효한 사용자에게 전달/공개하기 위한 방법
임의의 바이트들의 연속을, 더 길지만 헤더 필드 값으로 사용할 수 있는 일반적인 문자들으로 이루어진 문자열로 변환한다. [ HTTP 파서를 망가뜨릴 수 있는 값들 제외 ]
key/value 데이터를 안전하게 저장할 수 있는 매커니즘
장점
HTTP 의 무상태성과 인증/인가의 상태 유지라는 패러다임(인식의 체계)의 충돌
I/O 작업이 진행되는 동안 User Process의 작업을 중단하지 않음.
좀 더 쉽게 얘기하면,
→ 캐시로 사용
→ Persistence Data Storage로 사용
I will be a software architect.
28 
1
28 
28 
1
I will be a software architect.
"
https://medium.com/@shawn.ho/%E7%95%B6%E5%AE%B9%E5%99%A8%E9%81%87%E5%88%B0vra-iaas-%E7%9F%A5%E6%96%B0%E7%AF%87-5ac23abb0ab5?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shawn Ho
Jan 1, 2019·4 min read
這裏我們來驗證在vRealize Automation中，如何使用Docker Host。
透過software component，直接將docker host製作於Ubuntu 16.04的範本上：本機制需要在Ubuntu範本中，安裝vRA-Agent，安裝方法請參照本文件，提醒vRA 7.1之後的版本，Agent下載點在：https://vraApplianceFQDN/software/download/prepare_vra_template.sh
加入成功之後，我們就可以來試著透過UI，對容器進行佈建。點入預設的Repository頁面，我們可以快速瀏覽vRA預設的容器範本
上圖中，透過按下『Provision』，就可以看到佈建過程在右邊完成
點入佈建出的容器，即可看到所耗用的資源，並得到可以直接連線的Port，查看佈建出來的容器服務。
進階：（本功能目前僅支援Docker-Compose v2前的語法，v3尚未支援）
vRA裡的Container Tab目前也支援客製化的Docker-Compose，我找了一個Wordpress的範本給大家練習，如下圖。
按下『Import』後，我們可以在Templates裡面，看到我們指定的Wordpress客制範本。
雖然目前比較流行的是v.3，但尚未被vRA支援。下方為標準的docker-compose v2的語法，底下的服務意義為：
之後，就勇敢的把『Provision』給他按下去。就會看到底下自動生成兩個容器：Wordpress和Mysql，大功告成！
一個心態年輕的中年大叔。年輕時不學好~ 台大電機畢業後，去美國取得了博士學位，念完博士後，不想教書。當過補習班老師，碼農，產品總監，ISO稽核顧問，技術銷售，目前在Google Cloud跟客戶一起玩Kubernetes，也跟喜歡的客戶在金融, 政府, 電信, 高科技業內共同成長學習是斜槓人生的好案例。
See all (31)
一個心態年輕的中年大叔。年輕時不學好~ 台大電機畢業後，去美國取得了博士學位，念完博士後，不想教書。當過補習班老師，碼農，產品總監，ISO稽核顧問，技術銷售，目前在Google Cloud跟客戶一起玩Kubernetes，也跟喜歡的客戶在金融, 政府, 電信, 高科技業內共同成長學習是斜槓人生的好案例。
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dispatchframework/dispatch-a-multi-tenant-faas-faf8f485ffe1?source=search_post---------370,"There are currently no responses for this story.
Be the first to respond.
We are in a world where it’s hard to imagine any enterprise service not being a multi-tenant system. From your IaaS layer to container registries, companies like to easily onboard users and teams, enforce access policies and quotas all while operating as few instances of a service as possible. Why should your FaaS (Function as a Service) offering be any different?
With the latest release of Dispatch — an open source serverless framework, we are closer than ever to help you build a multi-tenant FaaS. In this blog post, we will explore how to create different tenants in Dispatch, add users to it and run functions. We will also discuss some shortcomings and how we are planning to address them in the future. To keep it simple, we will assume Dispatch is configured with OpenFaaS as the FaaS engine and uses Google Identity Platform as the OpenID Connect provider.
But before we talk about multi-tenancy, it’s important to understand about authentication & authorization in Dispatch, as they form the core principles upon which multi-tenancy can be designed.
Dispatch does not manage your user accounts
Dispatch does not manage user accounts on its own, in other words, there is no notion of adding accounts with a specific username/password to Dispatch using an API call. This is because:
Hence, Dispatch does not manage user accounts or store credentials but integrates well with OpenID Connect providers like Google Identity Platform, vIDM, Auth0, or Dex that provides session authentication over a well-established standard.
But wait, what about automation or integration accounts running in a non-browser environment? Dispatch has the concept of service accounts for such needs. These are locally managed accounts secured with RSA keys.
Policy Driven
Authorization in Dispatch is enforced by policies that describe the access rights of a user/group. Before a user can login to Dispatch, we need to create policies that grant privileges on one or more resources like functions, images etc. Within a single tenant, users can have different privileges based on their functional role. Typically, an administrator will have access to add base-images and images — these are secure container images with the runtime & dependencies needed for executing a function. A Developer, on the contrary, can build functions off of an image provided by an administrator. On the other hand, a service-account used for monitoring resource usage could just have read-only privileges. You get the point!
A Sample policy in Dispatch described in JSON format:
Okay, let’s get to the point of this blog. Tenants in Dispatch are called Organizations. You can create organizations and setup policies under each to onboard users/teams. In this example, we will create organizations representing two fictitious frontend & backend teams of a typical product. We will then add admin policies, user policies and then let the users create and run functions.
After installing dispatch based on the steps detailed here, you are now logged in to Dispatch under the default organization ‘dispatch’ with a user that has god-privileges or in other words a super-admin user. You can use this account to setup other organizations and policies.
Ensure you have the required privileges by listing existing organizations:
Now, create two new organizations frontend-team & backend-team with:
We now add two policies granting admin privileges to specific users of the teams. Since we configured Google Identity Platform as the OpenID connect provider, we use the email address associated with the admin users while setting up the policy.
Note the usage of the--organization flag to denote the tenant under which the policy must be created. This works since we are logged in as the super-admin user that has privileges across all organizations.
Let’s logout as the super-admin user and login as the admin of the frontend team.
Since we configured Google Identity Platform as the OpenID Connect provider, dispatch login command will open up the default browser and prompt the user to sign-in to their google account. In this case, since we are logging in as the frontend admin, let’s choose the corresponding account.
Upon entering the credentials for the frontend admin account, you will now notice some redirections related to OAuth2 Authorization Code Grant and then the following message on the browser appears:
At this point, your CLI has received a cookie for the session and you are logged-in as the frontend admin to the frontend team organization.
Create base-images & images for use by the team members. As mentioned above, these images are secure container images that provide the runtime and dependencies for your function code.
The following commands adds policies for the developers of the frontend-team restricting their access to creating & executing functions. Since images are fundamental to creating functions, we provide the users with read-access to images.
Dispatch has an in-built API Gateway (powered by Kong) that supports the creation of API’s that then point to your function.
Once the API is created, you can access it through the Dispatch’s API Gateway. For this blog, the API Gateway has been setup to use the host orgdemo.dispatchframework.io. Hence, all the API paths can be accessed at https://orgdemo.dispatchframework.io/<org_name/<path>. Thus, the API created above can be accessed at the following URL:
Let’s execute similar steps as before for the backend team. For brevity, let’s not discuss each step in detail.
We are at the whiteboard jotting designs (and doodling..) to enhance the multi-tenancy experience in many ways. Some of the tasks that are in the pipeline are:
With the latest version of Dispatch, it’s now possible to build a Multi-Tenant FaaS service in your enterprise and onboard your teams and developers. Isn’t that FaaScinating?
Dispatch framework is a batteries-included open source…
1 
Thanks to Berndt Jung and Karol Stępniewski. 
1 clap
1 
Written by
Serverless Enthusiast | Amateur Photographer | New Dad
Dispatch framework is a batteries-included open source function service
Written by
Serverless Enthusiast | Amateur Photographer | New Dad
Dispatch framework is a batteries-included open source function service
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@beth-kindig/best-bet-for-tech-stocks-in-2019-secular-iaas-73a22cae58cb?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Beth Kindig
Feb 8, 2019·4 min read
If ever there was a growth story in the next 2–3 years, especially during potential economic uncertainty, then infrastructure-as-a-service (IaaS) is it. This past week, Amazon’s IaaS offering, AWS, reported sales growth of 45% from $5.11 billion to $7.43 billion, with operating income increasing 61% to $2.18 billion up from $1.35 billion. Microsoft’s IaaS offering, Azure, was up 76 percent (same as last quarter) reaching $4 billion in revenue. Microsoft’s overall commercial cloud computing revenue which includes software grew 48 percent to $9 billion. If both companies continue on this trajectory in 2019, then Microsoft will narrow its gap from 3:1 to 2:1 with Amazon.
2018 CLOUD IAAS REVENUES $26 billion AWS
$10 billion MS
UPDATED PROJECTED 2019 if growth continues at current rate $16 billion MS
$30 billion AWS
Note: I’ve written quite a bit of analysis over the last few months about the duopoly between Microsoft and Amazon. To quickly summarize, my first analysis discussed the strategic acquisition of Github. My second analysis discussed the great efforts Microsoft has put into become a serious bidder for the Pentagon contract.
Truly, there is plenty of green field for both players. The investment window for the IaaS market is far from over as it took twelve years for the IaaS market to reach $40 billion and it will take only three years to double to $80 million — and this figure is on the low end of estimates.
My newsletter subscribers get this information first. Sign up here.
Here are a few of the projections for this space from various analysts:
On a micro-level, the tech industry is in a state of transition. Mobile is hitting saturation, social media faces privacy regulations, chip makers are getting hurt in the trade war, and meanwhile, 5G, artificial intelligence, and autonomous vehicles are too nascent to see returns in the near term. This is one reason I continue to hammer on IaaS as a safe, secular bet. Companies are going through a major transition right now by transferring work loads into the cloud.
As these transitions take place, IaaS will be as essential to companies as food, gas and cigarettes are to consumers. The company that has transferred to the cloud cannot exist without budgeting for this operating expense. Meanwhile, the companies who have not transferred to the cloud risk losing on competitive advantages such as artificial intelligence, machine learning, and scaling quickly through server virtualization.
As it currently stands, IaaS is Amazon’s largest revenue segment and Microsoft’s fastest growing revenue segment — although there is plenty of addressable market left for both players. Amazon’s capex spending (which includes all capex; not AWS specific) was at $14 billion in 2018 while Microsoft reported capex of $12 billion. One major drawback is that these are not pure play IaaS stocks which introduces risk from other revenue segments. You can read my follow up analysis on 6 pure play cloud stocks here.
I consult for financial firms. Inquire here.
I’m an industry insider who writes free in-depth analysis on public tech companies. This year, I predicted Facebook’s Q2 crash, Roku’s meteoric rise, Oracle’s slow decline, and more. Be industry-specific. Know more than the broader markets. Sign Up Now. I look forward to staying connected.
Originally published at beth.technology on February 8, 2019.
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
See all (1,477)
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/cloud-computing-working-model-2ab1b6887b4c?source=search_post---------330,"There are currently no responses for this story.
Be the first to respond.
Following are the risk involved is using IaaS:
Platform as a service provides the platform to programming developers a runtime environment to develop, build, test and run their application. It offers development and deployment tools required to develop the application. PaaS has a feature of point-and-click tools that enables non-developers to create web applications such as App Engine of Google and Force.com are examples of PaaS offering vendors where a developer may log on to these websites and use the built-in API to create web-based applications. PaaS removes the hassle for managing virtual machines or infrastructure to develop any application, the cloud provider will be responsible for creating infrastructure for application and managing the same.
Following are the benefits of PaaS:
Following are the risk involved is using PaaS:
SaaS model in which software\application is hosted at Cloud Service and made available to the end user over the internet. It is also known as the “on-demand software” or “pay-as-you use”. The software & the applications associated with it are centrally located on the cloud server and users can access them via a thin client connecting application i.e. using a web browser. Some are the examples where SaaS provides application such as CRM Application, HR Solution, Billing Invoice systems etc. Some of the SaaS application are difficult to customize but since it is following the SOA architecture, the customer can develop the customized application using API’s exposed by SaaS application.
Following are the benefits of SaaS:
Following are the risk involved is using SaaS:
There are two main problems which any organization is facing during the Employee access right:
To resolve these above-mentioned issues, the organization is more inclined toward using IDaaS which provides management of employee or user’s identity information as the digital entity. It is an authentication infrastructure that is built and hosted by third-party servers and can contain a range of services, but typically includes single sign-on (SSO), multi-factor authentication (MFA), directory services, profile management, registration information and directory services that provide organizations with simple and cost-effective identity and access management (IAM) capabilities
Network-as-a-Service allows us access to network infrastructure directly and securely as NaaS makes it possible to deploy custom routing protocols. NaaS can include flexible and extended Virtual Private Network (VPN), bandwidth on demand, custom routing, multi-cast protocols, security firewall, instructions detection and prevention, Wide Area Network (WAN), content monitoring and filtering, and anti-virus. The NaaS providers maintain & manage network resources which decrease the workload of customers/users. Moreover, NaaS offers network as a utility. NaaS is also based on the pay-per-use model.
empowerment through data, knowledge, and expertise.
10 
10 claps
10 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Technology Lead | Lifelong Developer | Avid Reader | Technical Blogger | Enthusiast Learner
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@min.rim/databank-will-start-offering-noia-sdn-to-their-clients-ad99d521f72?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mindaugas Rimavičius
Jun 13, 2019·2 min read
We’re glad to present you our new Proof-of-Concept(PoC) client — Databank — a company that offers cloud Infrastructure-as-a-Service(IaaS), rents bare metal servers and provides virtual desktop infrastructure for businesses. As the company is based in Lithuania the majority of their clients are also from there. Yet, almost all of their clients depend on their relationships with foreign markets and the best possible connection to their users and clients abroad.
NOIA SDN — a perfect Network-as-a-Service solution
NOIA SDN is a great fit for DataBank clients as they want to achieve high-quality, undisrupted and healthy connection with their end users. From now on, DataBank clients will have an opportunity to choose NOIA SDN as their Network-as-a-Service solution. We’ll achieve that by building an additional Point-of-Presence (PoP) in Kaunas, Lithuania, and connecting it to our existing PoP’s all around the world. We’ll ensure the best quality connection for all the Databank’s clients working with foreign markets by providing them with many possible pathways that are constantly “thinking” and adapting to the situation — algorithms avoid routes that are congested, thus always searching for the most efficient route.
We’re excited to take on this challenge. Not only will it help us improve our technology but also help spread the word about how NOIA SDN benefits businesses.
114 
114 claps
114 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mcraddock/drupal-standard-for-uk-public-sector-57d4dfaa3a29?source=search_post---------78,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mark Craddock
Oct 16, 2015·1 min read
I can find many Drupal suppliers on G-Cloud that can build me a website, but is this site portable between IaaS providers. Yes, you all cry, but when we look into the details, it’s not.
There’s a grey line between IaaS and PaaS, many IaaS providers will supply an operating system, some even preconfigured builds. This needs tuning, hardening and optimising to support Drupal. There is no standard way of doing this and each supplier has their own favorite configuration.
Some will patch the operating system, some will look after PHP, some will tune, some wont. Some will provide a separate managed MySQL database, some wont.
Will supplier A, take over a Drupal installation from Supplier B? May be, but they will need to do a full audit of the configuration.
What I need is a standard configuration that IaaS providers can say they will support and that Drupal suppliers will understand and take over from another supplier.
This should include;
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
1 
1
1 
1 
1
Building marketplace for #IoT. Ex UK Gov #CloudStore Lead. Built UK #CloudStore, 13,000+ Cloud services / 1,200 suppliers. Public Sector Startup guru (3 so far)
"
https://medium.com/@laiyuanyuan-sg/cloud-computings-most-basic-concepts-service-models-3a39123409b?source=search_post---------337,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stefanie Lai
Jan 12, 2021·7 min read
With the containerization of Docker, Kubernetes, CNCF, and the development of open-source communities, related frameworks and technology is booming and iterating rapidly. Cloud…
"
https://betterprogramming.pub/how-can-cloud-services-help-improve-your-businessess-efficiency-ea3fb038948e?source=search_post---------62,"Sign in
SeattleDataGuy
Mar 28, 2020·5 min read
In the 20th century, companies relied on servers and computers that were on the premises.
This meant when new servers had to be spun up, it could take weeks or even months to get everything set up. From getting the budget approved, to putting out orders, to having…
"
https://medium.com/hackernoon/how-to-choose-cloud-hosting-for-ecommerce-saas-vs-paas-vs-iaas-f710c6295b8d?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
Cloud hosting for ecommerce is on the hype now for several reasons: faster website speed and performance, high uptime and availability, very easy to scale server resources, redundant server environment, flexible pricing system.
Every ecommerce owner is thinking about moving to the cloud as more than half of the companies are already there. But there also are few abbreviations that can create confusion. Let’s discover what SaaS, PaaS, IaaS is and how ecommerce can benefit each of these cloud models.
In a nutshell, any of the above services is designed to remove a certain part of the time and financial costs of deploying and supporting your ecommerce website. The whole difference is what part of the concerns you will keep for yourself, and which part you will give to the management of the cloud hosting provider.
In the context of website hosting, SaaS is a completely ready-to-go platform. You don’t have to deploy and support the code of your website. The only thing you are responsible for — the content and visual look of your online store.
The best example of the SaaS in ecommerce is Shopify. With Shopify, the only thing you have to do to have your website online is to pay a subscription. Everything else is on the Shopify team.
You have access to the products, analytics, additional apps, themes, and other management features, but don’t bother yourself with deployment and maintenance of the code and servers.
Bigcommerce is another SaaS platform that handles coding, hosting, and updating parts for the monthly fee.
You should consider using SaaS if:
PaaS is something between typical shared-hosting and SaaS.
As part of this ecommerce cloud hosting model, you do not need to administer the operating system and system software. You are provided with a platform, on which you can set up your own ecommerce website. With PaaS, you are able to upload the content, custom code and manage databases. But you will be separated from the server administration: both its hardware and software.
The best example of PaaS in ecommerce is Magento Commerce Cloud or Bluehost. With that solution, you are provided with hosting, but still can edit the source code of the store and develop your custom applications for Magento.
Apart from that, you can use other popular PaaS providers: Heroku, AWS Elastic Beanstalk, Windows Azure, Google App Engine.
You can manage them easily with a managed cloud hosting provider.
PaaS is beneficial or even necessary if:
When renting a virtual infrastructure from an IaaS provider for your ecommerce website, you can use IaaS services of various sizes: a virtual server (VPS / VDS) and a virtual network.
In the first case, you rent a single virtual server, in the second — a pool of virtual servers with the possibility of connecting them into a virtual network.
You get full administrative rights inside leased virtual servers. All the operating system settings of these servers you need to do on your own: install the software, configure the firewall, etc. Of course, the support service of the IaaS provider can answer your questions if you’ll face difficulties. Some vendors can even do some set up work for you, for a fee.
Initially, the IaaS provider only ensures that your server will be accessible over the network in accordance with the service level agreement (SLA).
The main tasks of the IaaS provider are installation and maintenance of equipment and basic infrastructure software. The hardware on which the virtual infrastructure is built is located in specialized data processing centers (DPCs). These centers provide redundancy of communication channels, protection from power outages and much more. As a result, everything that is directly related to the uptime and availability of equipment will no longer bother you.
Just as with SaaS and PaaS, there are cases when you better use IaaS:
Most of the hosting companies provide VPS nowadays, but there also are specialized cloud hosting providers.
We at Elligense host our clients’ ecommerce solutions in the cloud using DigitalOcean as we find it the best choice according to the convenience of usage, features, and price.
Get $100 free credit, following this link.
The great user-friendly provider is Bluehost.
However, you can also take a look at Linode, Amazon Web Services (AWS), Rackspace, Microsoft Azure, Google Cloud.
Each of the cloud models mentioned above has it’s own features and characteristics. To sum up and help you choose, let’s remind for which cases each of the models is most suitable for:
Whether you choose SaaS, PaaS or IaaS, it’s a good choice to migrate to the cloud as this is the future of the business. I hope this material has helped you understand the variety of “as a service” models and choose the right cloud hosting for your ecommerce website.
If you need help with moving your business to the cloud, fill free to contact Elligense, we can help you.
#BlackLivesMatter
53 
Thanks to Natasha from Hacker Noon. 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
53 claps
53 
Written by
Looking for a reliable IT vendor? Check out Relevant Software → https://relevant.software/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Looking for a reliable IT vendor? Check out Relevant Software → https://relevant.software/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@svetlanasharapova011/%D0%BE%D1%82%D0%BA%D0%B0%D0%B7%D0%BE%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D0%B0%D1%8F-%D0%B8%D0%BD%D1%84%D1%80%D0%B0%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D0%B0-%D0%B4%D0%BB%D1%8F-%D0%B2%D1%82%D0%B1-8a48dd9342ee?source=search_post---------386,"Sign in
There are currently no responses for this story.
Be the first to respond.
Светлана Шарапова
Apr 26, 2018·4 min read
Обеспечивать катастрофоустойчивость системы не так просто. Если ты пользуешься услугами IaaS-провайдера, то можешь не беспокоится об этом. Но для таких крупных банковских систем, как ВТБ логично иметь собственное физическое оборудование и администрировать его самостоятельно.
Задача: Построить инфраструктуру для поддержки 150 систем обслуживания и обеспечить их защиту, избежав простоя систем.
Решение №1: Реализовать решение из двух одинаковых ЦОД (основной и резервный) с «ручным» переключением на резервную площадку.
От реализации решения №1 отказались, потому что в таком виде “резервная площадка простаивала бы без дела, хотя требовала бы такого же обслуживания, как и основная.”
Решение №2: Реализовать схему active-active. Оба ЦОД должны быть разнесены географически (в данном случае на 40 км) и обслуживать бизнес-системы одновременно в штатном режиме. Данное решение позволит удвоить общую ёмкость и производительность. В данном случае это особо актуально, потому что пиковые нагрузки в банковских системах далеко не редкость. К тому же можно проводить обслуживание без ущерба для бизнес-процессов.
Зарезервировали 50 особо важных бизнес-систем, для которых не должно быть ни секунды простоя. Среди них АБС, система противодействия мошенничеству, процессинг, CRM, система дистанционного банкинга. Именно из-за разнообразия систем возникла сложность в разработке эскизных решений для инфраструктуры резервирования. Применение типовых решений, к сожалению, оказалось не применимо к реальным бизнес-системам. Компоненты просто не легли на типовые решения. Например типовое решение для Oracle и БД Microsoft SQL не соответствовали требованию полного отсутствия потери данных. В числе критических были и внутренние информационные шины, через которые обмениваются данными остальные системы. В частности УСБС-front и УСБС-back.
Создан третий ЦОД, в котором собраны устройства, выполняющие роль координаторов. Это сделано для того, чтобы в случае разрыва связи между двумя основными площадками не возникла ситуация split-brain. Сеть из двух основных ЦОД плоская, без маршрутизации, построена на оборудовании Cisco, использует туннель L2 в L3 через OTV, а сами площадки соединены сетью MPLS по оптоволокну (идущему двумя разными путями). Для сети передачи данных используется канал 160 Гбит/с, а для сети хранения данных — 256 Гбит/с. В сети хранения данных обе площадки связаны по оптике.
Приобретено 40% оборудования. Остальные 60% были уже в наличии. На обеих площадках СХД объединены, а для универсального доступа извне к серверу приложений сделан кластер балансировщиков F5 BIG-IP. Для виртуальных машин построен растянутый кластер VMware, использованы виртуализаторы EMC VPLEX и дисковые массивы EMC Vmax и Hitachi VSP, подключенные на площадках в кластер виртуализаторов. Файловый сервис растянут между двумя дата-центрами и построен на технологиях Hitachi: для синхронизации данных между площадками используется Hitachi GAD, а для предоставления файлового сервиса — кластеризованные устройства HNAS, расположенные в обоих дата-центрах.
Репликация БД произведена встроенными средствами: Oracle Data Guard для Oracle и Always On для серверов Microsoft SQL. Чтобы избежать потери данных, Always On работает в синхронном режиме, а у Oracle идет одновременная запись redo на другую площадку, это позволит восстановиться по состоянию на последний момент. Методика проработана, отлажена и документирована.
Для баз данных многих систем используются серверы IBM Power, 1700 blade-серверов x86 Hewlett Packard разных поколений, в основном двухпроцессорных. Сеть построена на оборудовании Cisco Nexus 7000, SAN — на Brocade DCX разных поколений. Также по площадкам распределены инженерные системы Oracle: Exadata, SuperCluster, Exalogic.
Полезная емкость зарезервированных систем в каждом из двух основных ЦОД составляет примерно по 2 петабайта. Средствами оборудования зарезервированы только хранилище, системы виртуальных машин и файловые сервисы. Все остальные базы и прикладные системы резервируются средствами ПО. Синхронизация между массивами производится в файловом сервисе по технологии Hitachi GAD. Во всех остальных случаях данные реплицируются средствами самих баз данных или приложений.
После завершения первого этапа — резервирования порядка 50 самых критичных бизнес-систем —произведена проверка работы всех элементов: сети, дисковых массивов, виртуализации СХД и прочего. Протестирована работа каждой бизнес-системы при одновременном использовании ЦОД и при переключении между ними: система сначала ставилась в катастрофоустойчивую среду, а затем полностью переключалась на другой ЦОД. Там проверялась работа системы и возвращалась обратно в продакшн-среду. Во время тестирования измерялась производительность и оценивалась динамина. При любых схемах работы и переключений производительность не снизилась, доступность не пострадала. В итоге обеспечено бесшовное соединение между дата-центрами на уровне физических серверов (кластерная конфигурация), виртуальной инфраструктуры (распределенный кластер), систем хранения данных (зеркалирование) и сети передачи данных (резервирование ядра сети).
Информация взята из блога ВТБ на Хабрахабре. Ссылка на их статью — https://habr.com/company/vtb/blog/353842/
Делитесь статьёй и следите нами — https://t.me/cloudinside
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/zenduty/azure-service-health-alerts-and-escalation-with-zenduty-306bab2494f1?source=search_post---------352,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is a cloud computing service providing infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) supporting multiple Microsoft Specific and third-party services and systems with 90+ compliance offerings and trusted by 95% of Fortune 500 companies to base their business on.
What is a system downtime and how does it affect me or my business?
According to Webopedia, “Downtime refers to periods of time during which a computer system, server, or network is shut off or unavailable for use. Systems go through periods of downtime for a variety of reasons, including power or hardware failure, system crashes, hacker attacks, system reboots, operating system and/or software updates, lack of network connectivity, and more”. While downtimes may be planned for maintenance, in most cases they are unplanned and unexpected.
The cost and impact of system downtime are rising rapidly due to the business’s increasing dependence on data and technology. System downtime can cause loss of opportunities, productivity loss, confidence erosion, potential employee overtime costs to recover work lost and catching up, service level agreement penalties, supply chain ripple effects, and most importantly, a damaged reputation. Effective communication with staff, customers, and service providers are very important to manage the impact of downtime.
A key feature of Azure is its alerting service which enables you to set up alerts to monitor the metrics and log data for the entire stack across your infrastructure. Azure dispatches these alerts via Email. However, for critical metrics indicative of degraded user experience, email may not be good enough to elicit a prompt response and resolution from your reliability and NOC teams. For high availability services with SLA timeframes in minutes, you need to be able to promptly alert the right engineers and teams about a critical issue, and also gather responders, subject matter experts and communicate to the relevant internal and external stakeholders while at the same time, keeping a watchful eye on the SLAs. This is where Zenduty can help you.
Zenduty acts as a dispatcher for the alerts generated by Azure. Zenduty determines the right engineers to notify based on on-call schedules and escalations and notifies via using email, text messages (SMS), phone calls, Slack, Microsoft Teams, and Android & iOS push notifications.
Zenduty provides you with everything you need you to minimize your mean time to recovery with advanced routing rules, flexible scheduling, analytics and reporting, integrated ChatOps (with Slack and Teams), stakeholder communications and SLA alerts.
Below is a step-by-step guide to integrate Azure alerts with Zenduty.
In Zenduty:
In Microsoft Azure:
2. Create a new alert rule.
3. Select the resource which you want to monitor. Here we shall monitor a Virtual Machine.
4. Select the condition based on which the alert is to be triggered. Here we create an alert that is triggered when the CPU utilization of a Virtual Machine crosses 50%.
5. Create an Action Group (or use a previously created action group)
6. Enter the Action group name and short name:
7. Enter an action name and select Webhook under Action Type. A dialog box appears in the side.
8. Enter the webhook URL copied earlier from the Zenduty integration page.
9. Enable the common alert schema by toggling the button to yes and then click on OK.
10. Click on OK to add the action group.
The action group has now been created. It should visible under the section of Action Groups
11. Fill in the alert details appropriately. The alert rule name along with the severity will appear as the alert message and the alert description will appear as the summary in Zenduty
12. Click on create alert rule. You have now successfully created the alert.
Using the combination of Azure’s Application Insights along with webhook integration to Zenduty is a simple way to provide 24/7 monitoring and notification. Many aspects of application performance can be monitored with Azure alerts, including custom events detected within the application code. Now all essential Azure alerts and incidents will appear on the Zenduty dashboard enabling you to make use of all the power and capabilities of Zenduty to resolve incidents quickly and efficiently.
An end-to-end incident management platform.
2 
2 claps
2 
An end-to-end incident management platform. Making a dent in the SRE/DevOps/Support/ITOps universe and helping companies institutionalize reliability.
Written by
Building YellowAnt - http://www.yellowant.com — an enterprise-grade ChatOps framework
An end-to-end incident management platform. Making a dent in the SRE/DevOps/Support/ITOps universe and helping companies institutionalize reliability.
"
https://medium.com/%E7%A1%AC%E5%B8%81%E6%98%9F%E7%90%83/%E6%B0%B8%E6%81%92%E5%8D%B3%E6%9C%8D%E5%8A%A1-iaas-3fc82c39a7?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
翻译：李林 校对：杜江南
英文原文: Immutability as a Service Volumne 1, Monday, Febuary 25, 2019
整个世界只有很少的应用需要永恒不变这种特性，因为这种特性太贵了。
假如我们可以把永恒不变视为一种服务，世界上的任何应用都能锚定或者链接到这个服务上，那么我们就可以换一个角度来观察比特币：一个交易清算网络，或者有价值状态的清算网络。
举一个例子：
现在几乎所有的公司都不需要自己购买服务器，以及搭建和维护自己的服务器设施。因为这么做不仅仅代价巨大，而且和公司的核心业务没什么关系。所以绝大多数公司都会使用类似亚马逊的云服务。
规模效应可以解释为什么现在只有三个靠谱的供应商
你一定要问为什么？因为他们入场早，而且投入了数十亿美金，并持续不断的增加投入
实现永恒不变也是类似的，当然也有不同。
类似的地方：是在数字世界实现永恒不变非常贵（把谷歌，亚马逊和微软的云服务投入加起来都不够），虽然投入这么大，但是只在人们需要的时候才显得有价值。
不同的地方：是只靠一个或者几个实体搞不出这种东西。必须有非常多的参与者才能实现永恒不变这种特性。换个说法，越是分布式，越是去中心化架构，并且拥有者越多，见证人越多，节点越多，这个系统才越鲁棒，才能实现永恒不变的特性。试图用一个或者几个实体来管理这一切是无法实现永恒不变的。
与互联网类似，比特币的永恒不变服务将吸引更多的经济活动加入比特币网络。互联网最开始只是把远程计算机连接起来，随着时间流逝（更多的人用起来，并且信任互联网），它逐渐演进成为数据提供路由服务的网络。互联网真是一个非凡的创新，以互联网架构为基础，我们创造了很多东西。
下一步就是将货币属性融合到一个全民所有的互联网协议，而这个协议的核心就是存在于数字世界的永恒不变中。一个时间无法倒流的数字世界（就像真实世界）。
所有需要永恒不变特性的经济活动，以及要求发送，存储，接收这三个核心功能绝对靠谱的经济活动（货币/银行/资本/财务）都会出现在这个网络上。
就如同我之前所强调的，基于比特币的经济活动越多，比特币就越永恒，越安全。它已经在复合增长，已经在自我强化，并且已经达到了临界点，它现在已然是失控的火车，越跑越快。
现在已经有很多共识机制，而且还有会有很多新的共识机制。其中一小部分靠谱，剩下大多数都是垃圾。
这些共识机制可能用在私有和专用网络上，或者用在那些不需要绝对永恒不变和安全的领域。
我觉得如果一个与钱有关的应用只能运行在专有网络上，那是绝对不靠谱的。因为网络，特别是很多人参与的网络，通常会融合为一个整体，而不是成为专用和私有网络。
这种融合现象可以解释很多东西，比如我们只有一个互联网，我们只有一套电子邮件协议，我们都用交流电。语言的网络会融合为一个，金钱也类似（在美国，只有一种美金），也可以用来解释全球化这个词。
英语的发展达到了临界点，网络效应使得在全世界都可以使用英语。
即使我们忽略基于效率和实用原因产生的融合效应，这个世界也极有可能只存在一个绝对的，永恒不变的，无审查，安全的POW链，因为这东西太贵了。现在看来只有比特币。
如果我们把POW用于所有的领域，我们将毁灭这个地球(因为pow假设我们不能信任任何人，这个问题更大），而且
1.如果有人把pow视为一个服务，那么他们必将选择POW里面保证担保能力最强的那个，这会导致强者恒强。
2. 而且如果你确实搞出一个比较轻快的共识机制，你可以在需要证实一个声明或者做一个最终判断的时候将这个机制锚定到类似比特币这样的东西上。
正是这个逻辑使我相信：长期看来绝大多数经济活动都会被比特币网络吞噬，而不是那些酷炫新概念。
– – – – – – – – – – – – – – – –
这篇文章的 PRESS.one 签名:
https://press.one/file/v?s=bd0c8e03c485581e0553ab2a5a95f4e7d33ceb9d40a6d99e40f74b3c8992f41af0501fcee2c796056503f872142e13e5c0c836efab74f07e30c711217145a9a30&h=e86c34234cb04a8181b5deebb7bf3134ee28693b9c2464443bd32b257a51d232&a=ed72c8b2600502386467d4be1584e2f00a7db74c&f=P1&v=3
在改变世界和赚钱里，中本聪选择了改变钱！Long Bitcoin, Short Everything ！
1 
1 clap
1 
在改变世界和赚钱里，中本聪选择了改变钱！Long Bitcoin, Short Everything ！
Written by

在改变世界和赚钱里，中本聪选择了改变钱！Long Bitcoin, Short Everything ！
"
https://medium.com/microsoftazure/azure-health-checks-via-csharp-35ce1d5b7c6f?source=search_post---------302,"There are currently no responses for this story.
Be the first to respond.
There were two reasons to write this article, the first one is that basic health checks usually not added in the early stages of projects and not deployed with infrastructure. The second one is the awesome Microsoft Extensions.Diagnostics.HealthChecks is not supported by Azure Functions for a good reason, but there is a native health monitor configuration and walkaround from the community on GitHub.
So I decided to share several approaches that can be used together or independently, but all approaches are super easy to repeat for any project(Azure Portal setup is not an easily repeatable solution). This article is a part of the #AzureSpringClean #AzureFamily community event.
TL;DR; I will cover Azure Monitor health checks that can be deployed along the main infrastructure written via C#(Pulumi), then provide anoverview of Azure Functions health monitor configuration which I`m using and provide usage example of Health checks package for WebApps and link to workaround for Azure Functions
Let’s start with a simple approach that is not related to changes in a solution code, but still needs to be written with C# :).
For this purpose, we can use the Pulumi framework, which I started to adopt recently to increase the adoption of the IaaS approach within developers teams. Writing JSON and YAML file is not fun and usually meet a lot of backpressure from developer teams g the classic response is “we will ask DevOps engineer to do a task”. And the ability to do this task with strongly typed language is usually met with curiosity.
Short introduction Pulumi/C# video below, if you prefer video format.
The first step is to install Pulumi, generate a new project, apply your C# infrastructure code and deploy it to Azure.
Now let's have a look at our project, Program.cs is a classic entry point for application start, and Stack.cs will contain our infrastructure.
Let's write some infrastructure, I cannot express how cool it is to write a C# code for my Azure infrastructure. Let's examine basic infrastructure with my example.
There a few things to note, at the moment Pulumi has two providers for Azure, the old one based on Terraform and the new one based on Azure Resource Manager API. I will use the new one - Pulumi.AzureNative v0.7 and version 1.0 are due at the end of April.
And another important thing is that Pulumi is not free for teams, but the great news that it is free for personal usage and only 50$ per month for small teams. Which is cheaper than dedicated DevOps engineer hours :). I will also not do the deep dive into why “desired state configuration” is great.
The infrastructure code below contains Azure Functions, Azure SQL.
The only thing is needed to open cmd and run command pulumi up.
There is an option to avoid the command line and make deployments with a C# code, also Pulumi can be integrated with Azure DevOps or GitHub, the command line in this article used for the sake of brevity.
And after selection of the yes option, my resources are online.
And now to the fun part, let's add a basic availability test. While most of the infrastructure is covered by Pulumi, some exotic cases still require XML stuff to work, such as fields in this availability test. And moreover, current documentation sometimes doesn't reflect all needed changes for exotic cases like this alert. But they will improve WebTestPropertiesConfigurationArgs class to handle all configuration fields.
Let's add this test to our infrastructure.
But before I will share the successful deployment screen, let me share what will happen if you misspell something in the XML string above. Not very informative.
But in other cases, linking internal fields of Pulumi objects inside concatenated string can produce meaningful errors :).
So after a few fixes, I deployed the alert and then updated it.
One of many benefits of writing infrastructure via Pulumi/C# is
In this section, I will cover the magic of the Azure Functions host.json file. If there is a need to configure ingest, scalability of monitoring is a place to go. So configuring the application this way is also a repeatable option, though it can be changed at any time.
I will not repeat official documentation in this article but provide my configuration and few important takeaways from usage. You need to monitor additional parameters of function app host like connection limits and track these errors in connected Application Insights Instance. The Azure Functions runtime in Azure is not aware of any additional logic like HealthCheck middleware outside of functions execution context.
So in order to have proper health checks, you should create a special alert rule(with IaaS code) in Application Insights so it can be triggered with a specific type of event, that contains “Host thresholds exceeded - message”
This solution is only for applications deployed to Consumption Plan.
Host Health Monitor · Azure/azure-functions-host Wiki (github.com)
An alternative workaround(or if you use Premium functions plan) is to use a guide and code from Keith Smith, his approach will use classic NuGet packages from Microsoft along with a custom code wrapper and Azure Function to expose health checkpoint of application.
Adding Health Checks to your Azure Functions (keithmsmith.com)
.NET Core is providing middleware for health checks and reporting of the health of a variety of standard components such as Entity Core Framework DB context, underlying databases, and its super easy to customize health checks for any of your components.
Let's cover this functionality briefly, everything starts with the standard Startup class and health monitoring of standard components. Take for example package:
Microsoft.Extensions.Diagnostics.HealthChecks.EntityFrameworkCore
All you need is to add a health check to services and configure the monitoring endpoint, so you can visit it via the application.
One important notice, this health endpoint can be also configured from the App Service via Azure Portal since August 2020.
Sometimes basic scenarios are enough and if you can create them via easily configurable code and repeating of this code for all you project can be a first major milestone.
But there is more, you might need to monitor the health of your database in Azure, receive additional alerts via different Azure Monitor alerts. In order to write less code you can create a set of alerts for different types in Azure Portal, then export the ARM template and convert them to C# Pulumi via an online converter https://www.pulumi.com/arm2pulumi/.
Or you can embrace the native .NET Core approach via code provided by Microsoft. My take is to go with the infrastructure approach for cloud component and try to delegate this work to developers(not DevOps engineers)
That’s it, thanks for reading. Cheers!
Any language.
60 
3
60 claps
60 
3
Written by
Azure MVP | MCT | Software/Cloud Architect | Dev | https://github.com/staslebedenko | Odesa MS .NET/Azure group | Serverless fan 🙃| IT2School/AtomSpace
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Azure MVP | MCT | Software/Cloud Architect | Dev | https://github.com/staslebedenko | Odesa MS .NET/Azure group | Serverless fan 🙃| IT2School/AtomSpace
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/gowthamlabs/openstack-products-distros-at-a-glance-55582dbf0524?source=search_post---------353,"There are currently no responses for this story.
Be the first to respond.
OpenStack is a collection of open source software projects that provides capabilities necessary to build a typical IaaS based solution for enterprises, that includes, but not limited to VM’s, Containers, self-service provisioning.
Technology Writings
2 
2 claps
2 
Technology Writings
Written by
Astrologer | Enterprise Architect
Technology Writings
"
https://medium.com/@nicholas.sartor/on-iaas-innovation-as-a-science-ced33b68cc72?source=search_post---------209,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nicholas Sartor — The Unprofessional Innovator
Apr 8, 2019·7 min read
Innovation is believed to be unpredictable, uncontrollable, unmanageable. Science is known to be methodical, precise, real, to yield concrete results.
Is there any way we can make Innovation look more like Science?
If you ask to the average manager in the average company to give you a definition of Innovation, you would probably receive a confused mix of technology, new things, ideas and creativity. And it’s not his fault, Innovation is too often approached as an esoteric ritual rather than a scientific discipline.
In fact, one of the biggest issues with people trying to deal with Innovation, is explaining what they do, to other people.
And…if you are not able to define and explain it, your Innovation is unlikely to be very good. Therefore how can you think to actually make use of it…?
1 The first thing that you should do if you were to treat Innovation as a science is know what you are talking about. In any different scenario, it is just going to be another business buzzword, and more gibberish. We absolutely do not want that.
When I tell people at conferences and fairs I work with Innovation Management, the average reaction is ”ah nice, so crazy what technology can do” or “We have an amazing product using (insert-random-technology) which can solve all your problems”. Indeed, innovation has a lot to do with technology, but it is not limited to that.
To me, Innovation is the science that studies how we can transform ideas into novel solutions. Therefore, Innovation Management is a process, that starts with something imaginary and powerless such as an idea, and outputs something generating value, which is new to a certain ecosystem.
PROCESS.NOVELTY.VALUE.
This process is mainly focused in understanding what the value is you can deliver, and how to deliver it in a way that satisfies all the different stakeholders involved.
For me it takes two main shapes:
And all the different shades of grey between the two. (10 types of Innovation is a great framework!)
The added value could be intrinsic in a new technology, sure. But it could even just lay in a different business model, or value propositions.
Uber, just to make and example, is enabled by technology that is easily replicable (Lyft, Grab, Curb, DidiChuxing, Ola Cabs, etc..). The innovation lays in new elements delivering value (rating, prepayment, etc..), delivered in a novel way.
To put it very clear: Doing basic research is not innovation. Having a genius idea is not innovation. Discovering a new technology is not Innovation. Seeing an uncovered customer need is not Innovation. Coming up with a business model is not innovation. Developing a product/service is not innovation. Selling a product/service is not innovation.
Not until you put EVERY SINGLE of these steps together.
2 This inevitably leads us to the second point. For Innovation to be a Science, it need to follow a structured method.
Science, is mainly useful to humanity for helping us understand how things behave — natural phenomena, and therefore man-made objects. It’s thanks to scientists that engineers can predict the behaviour of a steel beam, and build bridges. Or understand gravity, fluid-dynamics and fly spaceships.
A Science-like Innovation should help us creating knowledge to understand what value is needed and how it is created for some users, in order to make informed decisions about how to build our products, services, and what business model should be around them (to whom this value appeals, how to deliver the value, how to make money).
How do scientists achieve that? How can they be so confident their theory is true? How can they transform intuitions into engineering formulas?
Tadaaaa! They conduct EXPERIMENTS! :)
Thanks to Galileo Galilei, we have access to a formalised method to create knowledge:
When the experiments shows results which are different from the hypothesis, they try again with different hypotheses, or kill the project.
In my time as a consultant I once heard something sounding like:
“Executives won’t like the word experiments, it sounds too risky, we don’t want that”.
Unfortunately, IMHO, taking risks, unbalancing the status-quo, avoid the business as usual, is the best way to get anywhere. Experiments, if anything, will help you reducing those risks.
Why?
Unfortunately, successful innovation (as we defined it) has a lot to do with people’s behaviour, needs, perceptions, trends, culture, etc. They are complex and mutable, much more than scientific phenomena — the mutability part at least. No matter how many experts you will consult, no one will be able to predict beforehand, and with accuracy how a specific new product/service will do in the market, or which business model will work best. (I still have the hope AIs might do better than us, but for now we have to rely on other methods.)
In other words, how do we decide which ideas to promote, and when to kill them?
A scientist would not suggest you to indefinitely persevere in a direction that you cannot prove (or guesstimate) to be right. As an example, no one is 100% sure we will ever be able to extract energy from nuclear fusion, but there are good reasons to believe further iterations will get us closer and closer to a working prototype and then to a more an more performing reactor. Iterative experiments that proved different hypotheses to be correct (or almost) lead them to the current point.
Same as people, some ideas are born already mature, and do not have to go through endless iterations in order to boom. Others will only yield results after much “running in circles”. Is this invalidating the entire point?
I believe not. But it calls for defining different degrees of innovation. These are commonly known as Incremental and Radical innovation. But we are not talking about this now.
100% of them (being radical or incremental), though, will come with some degree of uncertainty in the beginning. Either because the idea has never been tried before, a new technology is used, the conditions are changed (geography, customer base), or some competitors are already there. So it’s just the amount and scope of experimentation that changes.
The secret is going step by step. Most risky assumptions first, fine tuning later on.
Usually the first question you want to ask yourself is “Who really wants this?” the second “Can we do this?” and the third “Shall we do this?” and then many others. If you do not have empirical proofs that answer each of the questions, you are dealing with assumptions, which means uncertainties, which means high risk!
Once it is clear to the decision makers that risks are high and not easily estimated, they will also understand that any assumption that is not proven by experiments can be thrown in the garbage, together with the detailed business model you wrote without leaving your desk.
In the beginning of a completely new solution, progress will be damn slow, and several pivots might be necessary to go further. These will be seen as failures from some peoples, as lessons learned from the Innovators. The further you go, the more proven-knowledge you have and the more expensive your experiments can be, exactly because your risk is lower. Usually this happens when you experience less pivots, and the core assumptions of your ideas are transformed into certainties.
Does all of this sound familiar? Lean startup, Design Thinking, Agile, they are all based on the Scientific method from Galileo. Their contribution is placing the focus of early experiments on customers needs and introducing several iterations, to deal with increased complexity, or chaos.
Here experiments to try to answer those questions are absolutely necessary, and they should be aimed specifically to answer those questions.
“Do they want it?” “No” … then try to understand if the problem you are trying to solve is actually real, or just something you imagined.
“Fail fast and cheap” (if you need to fail) is the only way you won’t be caught in the “sunk cost fallacy” — the “ I spent sooooo much money on this, I don’t want to pull out now” syndrome.
The objective is not to fail on purpose. The concept is that failure is an acceptable outcome of an experiment. Same as it happens in science, when an experiment bring bad results, in business we can either pivot, or kill the project.
Perseverance is key to success… only if you’re pushing in the right direction!
To sum up, I really believe Innovation is a concrete, manageable and powerful scientific tool, which can yield great results if used in the correct way, if Managed.
Clear definitions have been given by bunch of scholars, but for some reason these are still not well understood in many companies, independently of the size.
Methods do exist, and they are all based on scientific thinking: gather knowledge step by step, reduce uncertainty and focus on the key questions, to bring new value to your customers.
It’s our job to spread this powerful tool and use it effectively!
Have fun!
Just a person, with the passion of making the world better. I love to make things work, fix them. Innovation is one of those, and it shouldn’t be wearing formal
2 
2 claps
2 
Just a person, with the passion of making the world better. I love to make things work, fix them. Innovation is one of those, and it shouldn’t be wearing formal
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@totalcloudio/top-3-areas-to-automate-in-aws-to-avoid-overpaying-cloud-bills-399f036ca3ac?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
Totalcloud.io
Jul 8, 2019·5 min read
AWS is one of the most used cloud services across the world. Gartner Magic Quadrant ranks AWS as the largest IaaS providers. AWS services are used by enterprises from every possible domain. Around 1000,000 companies worldwide are using AWS as their IaaS provider.
From Netflix to Unilever to Met Office everyone has moved to cloud infrastructure on AWS. Since you are reading this blog, chances are you are using AWS too, and overpaying too! !
A report suggests enterprises around the globe overpay an amount of 6.4 Billion Dollars for AWS cloud services. An amount that would make at least 6 startups a Unicorn.
Amount overpaid for AWS solutions = $6.4 Billion
It is dangerously easy to get your organization to move to cloud infrastructure and more easier to get it wrong, thus overpaying in hundreds of dollars on AWS deployments. Here are a few pointers:
There are resources that are required only on case-to-case basis. However, enterprises end up paying for them round the clock. Areas, where such resources can be found, are development, testing, QA, staging, etc. Similarly, there are resources that are not required anymore. However, they continue to run. Some examples to elucidate would be:
With the amount of data enterprises require, irrespective of the industry/vertical, you have to be prudent in how you store and manage the data. Failing to do so can result in your enterprise compute costs flying through the roof. Some of the places where you need to efficiently manage data are S3 usage, archiving data, caching data, etc.
In the early days of cloud adoption, one of the major reasons for enterprises’ reluctance to shift to a cloud was security concerns. Even today, one of the major challenges enterprises face is ensuring a secure AWS infrastructure 24/7.
Despite all measures, enterprises have been victims of security breaches. For instance, in 2017, Accenture accidentally configured four of their AWS S3 buckets as accessible to the public. This meant anyone who could access and figure out one of the bucket’s URL would be able to download the bucket’s content. In the same year, hackers got access to Uber’s GitHub data and were able to extract the AWS credentials of the company.
With the augmentation of AWS in an enterprise setup, several factors, such as instance options, management, selection, and deployment of the right solutions become more and more complex to handle.
Despite these limitations, an enterprise can scale on AWS without overpaying! How? Using AUTOMATION!
Efficient use of S3, proper data retrieval, archiving and information caching methods can help save hundreds of dollars for enterprises. So, automate the following:
In an enterprise setup, managing hundreds of AWS EC2 instances poses a huge challenge. Optimizing their usage and avoiding cost wastage are of paramount importance. Autoscaling definitely helps in utilizing EC2s optimally, however, there are other areas that can be automated, to avoid overpaying, such as:
While AWS security solutions emphasize security protocols, organizations still face challenges while ensuring a robust security infrastructure, and sometimes face security breaches like the ones mentioned earlier in the article. There are two ways in which security issues may arise — inefficient user management and unauthorized access.
In such cases, automation can help. Here’s how:
AWS is a boon for enterprises for scaling their infrastructure into a secure platform. However, not optimizing the usage can lead to wastage of precious dollars for your enterprise. Automating the critical elements can help enterprises achieve this. And this is where cloud management automation platform like TotalCloud can help you make the most out of your AWS deployments and help you avoid overpaying your AWS bills. Know more …
TotalCloud automates actions on AWS resources and services using its unique cloud graph engine. Sign up for a free trial.
Originally published at https://blog.totalcloud.io on July 8, 2019.
TotalCloud helps cloud engineers indulge in no-code AWS automation. We enable engineers to go script-less, saving more than 95% of engineering time.
See all (347)
2 
2 claps
2 
TotalCloud helps cloud engineers indulge in no-code AWS automation. We enable engineers to go script-less, saving more than 95% of engineering time.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intergrupo/para-donde-va-la-nube-ef5682a2ff45?source=search_post---------362,"There are currently no responses for this story.
Be the first to respond.
Los modelos de nube más que un modelo de computo (IaaS, PaaS y SaaS) se han convertido en un modelo de negocio denominado “as a Services” donde muchas organizaciones han creado sus nuevos modelos basados en estas características, sin embargo, es importante entender que la nube sigue avanzando tecnológicamente y por ello en este pequeño texto quiero explicar brevemente a donde va a parar este gran fenómeno que transformó el mundo y las organizaciones como uno de los principales habilitadores de la transformación digital.
Hoy las empresas habla con fluidez de IaaS, PaaS y SaaS, dado que para muchas de ellas es un servicio ofrecido e incluso contratado en sus diferentes sabores como es el caso de servidores virtuales en Amazon, productividad en Office 365 o Gestion comercial en Saleforce, sin embargo, se han preguntado ¿si está la organización preparada para soportar multiples nubes?
Hoy existe un gran reto en las organizaciones que iniciaron el proceso de adopción de la nube bajo un modelo Ad-hoc, es decir, organizaciones que compraron un servicio sin pensar en integraciones, seguridad, gestión etc… Razón por la cual hoy se están viendo abocadas a adquirir servicios o productos que les permitan resolver esta problematica, es decir, una solución que integre todas las marcas y servicios de las nubes contratadas, que automatice los despliegues, que brinde autoservicio, que permita hacer seguimiento y control de costos, que permita publicar un catálogo entre otras actividades, en otras palabras, volverse un Cloud Brokerage, es decir, un agregador de valor que administre de forma simple, rápida y ágil todas las nubes que van a conformar los servicios en una organización.
Imaginen los servicios que un departamento de TI le ofrece al área comercial de una empresa, donde el CRM puede estar en Dynamics 365 (primer nube), luego le ofrecen servicios de correo y colaboración en Office 365 (Segunda nube), luego le ofrecen el ERP de SAP en la nube (Tercera nube), le ofrecen la capacidad de autenticación en Azure Active directory (cuarta nube), le ofrecen una solución de gestión de ventas en un proveedor especializado (quinta nube), le ofrecen un escritorio virtual en Amazon (sexta nube) y así sucesivamente. Nos hemos preguntado entonces ¿cómo vamos a garantizar los niveles de servicio esperados por el negocio? ¿Qué tan fácil será la gestión? ¿Cómo es el aprovisionamiento? Si vemos esto extrapolado a las demás áreas del negocio nos vamos a encontrar con multiples nubes por lo cual debemos pensar un un Broker de Cloud.
La nube en su capacidad de computo está llegando a niveles de comoditizacion muy rápido (entiéndase commodity a aquellos servicios cuya diferenciación es solo el precio) por esta razón y con el apoyo de los Contenedores, los Broker van a tener la capacidad de procesar la información en un datacenter donde sea mucho más barato, por ejemplo: mientras Europa duerme, Latinoamérica esta despierta y usará esas capacidades. Otro ejemplo: puede ser más barato el computo en la nube de Microsoft en la tarde, mientras en las noches es más barato en Amazon, por esta razón y a través de estas nuevas técnicas de virtualización llamadas Contenedores (Docker o Kubernetes) el Broker podrá moverlos rápidamente en cualquiera de estas nubes sin afectar la disponibilidad de la aplicación o servicio.
El mundo viene avanzando a pasos agigantados con tecnologías y mecanismos de optimización llamados Serverless o Functions, que consisten en la creación de un algoritmo maestro donde esta la aplicación, pero cada una de las funciones, es decir invocaciones que requieran traer un dato o procesar algo, es almacenada en la nube la cual se cobra únicamente por llamado, eliminando entonces los altos consumos de computo y las tareas de gestión.
La respuesta es sí, pero ¿qué negocio se mueve hoy en el mundo sin un canal de comunicaciones o una red de datos? Sin embargo, la nube viene explorando a escala empresarial una tecnología que surgio de los carros autónomos llamada FOG Computing (Nube de borde) que permitirá realizar tareas mínimas de forma local en aquellos casos en los que haya ocurrido un problema con la conexión a la nube, una vez establecido dicho canal, se procederá a realizarse la sincronización.
La verdad es que las organizaciones no están preparadas para adoptar la nube, hay que entender que adoptarla es más allá que compra un producto en la nube, hay muchos temas que resolver desde el ámbito de procesos, cultura, skills, tecnología entre otros; lo anterior, sin hablar de lo que pasa con las aplicaciones actuales (existentes) legadas o no, con las que cuenta la organización, la pregunta es entonces: ¿Cómo me preparo para la nube? ¿Puedo adoptar la nube en mi negocio? ¿Cómo compro? ¿A quién le puedo comprar? ¿Qué hago con lo que tengo en mi centro de datos? ¿Tiene que ser nube pública o privada?
Son muchas las preguntas a resolver, por eso, antes de adoptar la nube es importante definir un mapa de ruta, una arquitectura de referencia, un análisis de la situación actual y futura, y con ello iniciar un proceso de adopción de forma controlada que les permita ser exitosos.
Your Future Our Passion
2 
2 claps
2 
Written by

Your Future Our Passion
Written by

Your Future Our Passion
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/experimenting-with-google-cloud-platform-441da66154ba?source=search_post---------109,"There are currently no responses for this story.
Be the first to respond.
Google offers a $300 credit to get started with GCP for free. It’s time to experiment.
What’s needed to sign up for the trial:
Once set up,
Deploying a serverless env in minutes with GCP
Google Cloud community articles and blogs
8 
8 claps
8 
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-cryptopanda-strongnode-io-highlights-nft-and-defi-applications-and-sne-token-utility-e38ce422c20?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 8, 2021·3 min read
Infrastructure-as-a-Service (IaaS) tech company and innovation lab StrongNode.io’s CEO and Co-founder Daniel Saito joined the AMA with CryptoPanda last 19 September 2021. CEO Saito replied to questions regarding our company, products, and shared facts about the $SNE token and the upcoming IDO launch.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Discover some of the highlights from the StrongNode and Crypto Panda AMA episode below:
Can you please tell us StrongNode token in detail?
Daniel Saito: All transactions whether you BUY, SELL, transfer your token the transaction is taxed. That tax goes back to buying back token and rewards distributed to its token HODLr. Additionally the % of the tax goes into a SAFU fund in event of a catastrophic financial attack on our DEFI AMM pools. But all smart contracts and apps will be audited and certified by 3rd party auditors.
Please tell us the biggest milestone you have ever achieved and also your future milestone?
Daniel Saito: There are many milestones that made up this venture to this date. But I would have to say the biggest milestone to dates is getting everyone together as a team and be unified in a vision of creating a vast network that reaches and helps millions. Of course getting oversubscribed in our financing as a team was a great milestone, although we knew it would get financed. We took a different approach in getting this financed. Instead of going with traditional VCs and raising capital for the venture. We have done that with the traditional VC (Benchmark, Sequioa, A16Z, Intel Capital.) in our past ventures we started. We wanted to take a different approach to get StrongNode financed. We financed our project from our enterprise end customers and sold them our token. We are seasoned entrepreneurs that knows that in a startup you need revenue. If the business cannot self sustain itself, it would not be a viable business. This also applies to cryptocurrency based projects. The (problem / solution) that the product is serving needs to account for its own for its own revenue base and not rely on the price of the token to pay for the ongoing of the business (NOTE: it can to a certain extent).
What do you think about the boom of the NFT + DeFi association that is causing so much interest in the world? How does Your project take advantage of and exploit this reality?
Daniel Saito: We take a lot of cues from the industry, we can choose to take the best practices (which we did) but we wanted to re:invent how it can be done. For example, User Registration is a product called StrongNodeID. Once you registered through the process you are delivered a [nonmovable] NFT that is unique to your wallet and identity in the system. This StrongNode NFT will act like a passport to enter the StrongNode infrastructure and will be the equivalent to [notable] SSO solutions like “Login with Google” or “Login with Facebook” but for crypto. This NFT is proof that the user has been KYC’d and is allowed entry into the network.
As for the DEFI components we we will open up staking and AMM farming. all developed on custom code and all code will be audited. We will have liquidity pooling with the folks at QUICKSWAP. Basically we took all the best of breed implementation and put it under one platform.
For more information, visit: https://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
300 
300 
300 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@spencer.cox/i-wonder-if-this-would-also-be-true-written-from-the-perspective-of-paas-not-iaas-1a058bf0994c?source=search_post---------213,"Sign in
There are currently no responses for this story.
Be the first to respond.
Spencer Cox
Jul 25, 2017·1 min read
Andrew Walker
I wonder if this would also be true written from the perspective of PaaS not IaaS? App Engine maps pretty literally to EC2. Might even be closer to the Elastic Beanstalk to manage EC2 and create fully automated environments?
However within AWS even that is old school in many senses… Using the serverless framework to string together true pay per use services like Lambda, SQS, S3 and API Gateway seems like the ultimate way to lower costs and minimize interaction with the stack. I think this is where AWS might even have a leg up on Google.
I guess it depends on the use-case… development or deployment maybe?
VP of Engineering at SensorUp. Data geek, geospatial thinker, and backcountry adventurer.
See all (179)
1 
1
1 clap
1 
1
VP of Engineering at SensorUp. Data geek, geospatial thinker, and backcountry adventurer.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@unvired/is-the-iaas-cloud-really-low-cost-by-srini-subramanian-614f55c310c?source=search_post---------289,"Sign in
There are currently no responses for this story.
Be the first to respond.
Unvired Inc
Dec 30, 2015·4 min read
Host on the cloud, get a dedicated server or build your own? This is a common question that most entrepreneurs who are launching a service ask themselves. Seeking answers via the web probably leaves more questions than answers.
The reasons are simple, there are a plethora of articles that sing the praises (for many right reasons) of Infrastructure As A Service (IAAS) such as AWS, Google and other similar services. The ease with which you can launch servers on these clouds make it even more appealing. The fact that Netflix and other companies are using it gives the added assurance. Now head over to the wonderful calculator that all these services provide and crunch some numbers. You may either buy into it so completely that you are ready to launch your service or taken aback by actually how much it can cost. Lets face it, its not as cheap as its made out to be.
To better understand this problem, lets crunch some numbers with an example server. Suppose the server we need is equivalent to 4 Cores (or more), 32 GB RAM and about 200 GB hard disk space (Note that in most cases RAM is the overarching choice and CPU cores are not really configurable, notable new exception is Google). For simplicity I’am also not considering prepayment as no startup probaly wants to commit for more than a few months.
Cloud Servers (Instances):
AWS — Consider the m4.2xlarge at 32GB RAM and 8 cores. Linux instance is at $0.504 per hour or ~375$ per month. Add charges for storage and data transfer and approximate it to 400$ per month. Google — Consider the n1-standard-8 at 30GB RAM and 8 cores. Linux instance is at $0.280 per hour lowest price with 100% usage or ~208$. Add charges for storage and data transfer and approximate to 230$ per month.
As you can notice there is already a significant difference between the two services. If you check other providers the price range will be similar.
Dedicated Servers:
Typically dedicated servers have always set you back by a significant sum and hence may not have been a choice. But a host of new providers has meant that hybrid offerings are available which has already reduced prices. Significantly Managed dedicated servers are way more expensive than unmanaged servers. The significant difference being in unmanged only the hardware is supported by the provider, every other responsibility us yours (software, backup etc).
Rackspace — A 24GB prepackaged dedicated server with 6 cores will set you back by ~ 674$ per month. This is backed by Rackspace Fanatical Support of course.
Packet.net — Packet offers dedicated baremetal hardware in a cloud like fashion. Type 1 server with 4 Cores and 32GB RAM (with 2x120GB SSD drives) is at $0.4 per hour or 297.6$ per month. There are no other charges as everything is included in this.
OVH — A major European provider with a NA presence in Canada and data center. A 32GB / 4 Core dedicated server (unmanaged) costs 79$ a month (no setup fee)
Hetzner.de — German data center, 32GB 4 Cores is priced at 39 Euros or ~43$ per month with a 79 Euro / ~87$ setup fee (one time)
As can be seen the range is again wide not considering a huge number of smaller providers. Depending on whether some of the administrative tasks can be managed in house or not, choice of provider can be made. Point to note though is that dedicated servers can actually be cheaper than cloud.
Conclusion: So before deciding, its important to decide based on these (There could be many more significant ones I’am missing, add to the comments)
1. Elastic Scaling — Are your users going to grow that dramatically that you need elastic capabiloty of that nature? Less than 1% of all web apps need this kind of scaling, rest are happy with more deterministic scaling. (Cloud v/s dedicated) 2. Redundancy — Sometimes cost of 2 dedicated servers is still cheaper than 1 cloud instance. So even HA is not an issue with dedicated, but multi region availability etc can also determine the choice. 3. Time — The longer you are willing to commit the cheaper some of the cloud services will be. 4. Legal or Security related — If customers dont prefer shared multi tenant instances, then you may have to go dedicated.
Net-net: Cloud services such as AWS are not the only choice. Even dedicated servers can be bought month on month with significant cost and performance gain. You actually have more choice than what is sometimes made out to be!
Note: This post originally appeared on LinkedIN: https://www.linkedin.com/pulse/iaas-cloud-really-low-cost-srinivasan-subramanian
Blog Technology Viewpoint
Originally published at unvired.com on December 30, 2015.
Unvired Enables Digital Transformation resulting in Enhanced Competitive Advantage for Enterprises both large and small globally.
Unvired Enables Digital Transformation resulting in Enhanced Competitive Advantage for Enterprises both large and small globally.
"
https://medium.com/part-1-of-4-oracle-iaas-and-seven-pillars-of/part-2-of-4-oracle-iaas-and-seven-pillars-of-trusted-enterprise-cloud-platform-f99d214f1718?source=search_post---------250,"There are currently no responses for this story.
Be the first to respond.
This is the second part of our blog series where we do a deep dive into the Oracle Cloud Infrastructure security approach. As a recap, we design our security architecture and build security solutions based on seven core pillars. And under each of these pillars, we focus on delivering solutions and capabilities to help ensure our customers can improve the security posture of their overall cloud infrastructure. In the first post, we discussed how we enable customers to achieve isolation and encrypt their data. In this post, we dig into our 3rd and 4th pillars, and discuss how you can obtain the security controls and visibility needed for your cloud environment.
Security controls offer customers effective and easy-to-use security management. The solutions that we offer allow you to control access to your services and segregate operational responsibilities to reduce the risk associated with potentially malicious and accidental user actions.
User authentication and authorization-based security controls:
Each user has one or more of the following credentials to authenticate themselves to Oracle Cloud Infrastructure. Users can generate and rotate their own credentials. In addition, a tenancy security administrator can reset credentials for any user within their tenancy.
Console password: Used to authenticate a user to the Oracle Cloud Infrastructure Console. API key: All API calls are signed using a user-specific 2048-bit RSA private key. The user creates a public key pair and uploads the public key in the Console. A user can access an instance via SSH. This requires that the user has an SSH key pair. Swift password: Used by Recovery Manager (RMAN) to access the Object Storage service for database backups. To ensure sufficient complexity, the IAM services create the password and the customer cannot provide it.Customer secret key: Used by Amazon S3 clients to access the Object Storage service’s S3-compatible API. To ensure sufficient complexity, the IAM services create the password and the customer cannot provide it.
Instances:
Instances are a new principal type in IAM. Customers no longer need to configure user credentials on the services running on their compute instances or rotate those credentials. Each compute instance has its own identity, and it authenticates using the certificates that get added to the instances by instance principals. Because these certificates are automatically created, assigned to instances, and rotated, customers do not need to distribute credentials to their hosts nor rotate them. You can group instances in logical groups called dynamic groups and you can define IAM policies for these groups. ynamic groups allow you to group Oracle Cloud Infrastructure instances as principal actors, similar to user groups. You can then create policies to permit instances in these groups to make API calls against Oracle Cloud Infrastructure services. Membership in the group is determined by a set of matching rules.
Federated Users:
Federated users who attempt to authenticate to the Oracle Cloud Infrastructure graphical administration console are redirected to the configured identity provider, after which they can manage Oracle Cloud Infrastructure resources in the console just like a native IAM user. Currently Oracle Cloud Infrastructure supports the Oracle Identity Cloud Service and Microsoft Active Directory Federation Service (ADFS) as identity providers. Federated groups can be mapped to native IAM groups to define what policy applies to a federated user.
Security Lists:
Oracle IaaS also provides a native firewall-as-a-service in the form of security lists that are applied at the subnet level. The security list rules for the database subnet restrict it to connecting from and to the web server’s subnet. The security list for the web server subnet allows all outgoing connections while restricting incoming connections.
A security list provides a virtual firewall for an instance, with ingress and egress rules that specify the types of traffic allowed in and out. Each security list is enforced at the instance level. However, you configure your security lists at the subnet level, which means that all instances in a given subnet are subject to the same set of rules. The security lists apply to a given instance whether it’s talking to another instance in the VCN or a host outside the VCN.
When you create a security list rule, you choose whether it’s stateful or stateless.
Stateful: If you add a stateful rule to a security list, that indicates that you want to use connection tracking for any traffic that matches that rule (for instances in the subnet the security list is associated with). This means that when an instance receives traffic matching the stateful ingress rule, the response is tracked and automatically allowed back to the originating host, regardless of any egress rules applicable to the instance. When an instance sends traffic that matches a stateful egress rule, the incoming response is automatically allowed, regardless of any ingress rules.
Stateless: If you add a stateless rule to a security list, that indicates that you do not want to use connection tracking for any traffic that matches that rule (for instances in the subnet that the security list is associated with). This means that response traffic is not automatically allowed. To allow the response traffic for a stateless ingress rule, you must create a corresponding stateless egress rule.
Containers:
For containers, the Kubernetes RBAC Authorizer can enforce more fine-grained access control for users on specific clusters via Kubernetes RBAC roles and clusterroles. A Kubernetes RBAC role is a collection of permissions. For example, a role might include read permission on pods and list permission for pods. A Kubernetes RBAC clusterrole is just like a role, but can be used anywhere in the cluster. A Kubernetes RBAC rolebinding maps a role to a user or set of users, granting that role’s permissions to those users for resources in that namespace. Similarly, a Kubernetes RBAC clusterrole binding maps a clusterrole to a user or set of users, granting that clusterrole’s permissions to those users across the entire cluster.
IAM and the Kubernetes RBAC Authorizer work together to enable users who have been successfully authorized by at least one of them to complete the requested Kubernetes operation. When a user attempts to perform any operation on a cluster (except for create role and create clusterrole operations), IAM first determines whether the group that the user belongs to has the appropriate permissions. If so, the operation succeeds. If the attempted operation also requires additional permissions granted through a Kubernetes RBAC role or clusterrole, the Kubernetes RBAC Authorizer then determines whether the user has been granted the appropriate Kubernetes role or clusterrole. By default, users are not assigned any Kubernetes RBAC roles (or clusterroles). So before attempting to create a new role (or clusterrole), users must be assigned an appropriately privileged role (or clusterrole).
You can connect to worker nodes using SSH. If you provided a public SSH key when creating the node pool in a cluster, the public key is installed on all worker nodes in the cluster. On UNIX and UNIX-like platforms (including Solaris and Linux), you can then connect through SSH to the worker nodes using the SSH utility (an SSH client) to perform administrative tasks. Before you can connect to a worker node using SSH, you must define a security ingress rule in the security list for the worker node subnet to allow SSH access.
In order to give you the visibility you need over your cloud infrastructure, Oracle offers comprehensive log data and security analytics that you can use to audit and monitor actions on your resources. This allows you to meet your audit requirements and reduce security and operational risk.
The Oracle Cloud Infrastructure Audit service records all API calls to resources in a customer’s tenancy as well as login activity from the graphical management console. Using the Audit service, customers can achieve their own security and compliance goals by monitoring all user activity within their tenancy. Because all Console, SDK, and command line (CLI) calls go through our APIs, all activities from those sources are included.
Audit records are available through an authenticated, filterable query API or can be retrieved as batched files from Oracle Cloud Infrastructure Object Storage. You can also search for API calls via the Console. Audit log contents include what activity occurred, the user that initiated it, the date and time of the request, as well as source IP, user agent, and HTTP headers of the request. New activities are usually appended to the audit logs within 15 minutes of occurrence.
By default, audit logs are retained for 90 days, but you can configure it to retain logs for up to 365 days.
In addition to the audit services, Oracle CASB-based security monitoring performs Oracle Cloud Infrastructure resource activity configuration checks, IAM user behavior analysis, and IP reputation analysis.
Examples of CASB Oracle Cloud Infrastructure security checks:
<> Publicly accessible object store buckets<> Open VCN Security Lists → 0.0.0.0/0<> VCN accessible to the internet<> IAM user password not rotated for more than 90 days<> IAM user API keys not rotated for more than 90 days<>IAM user password complexity checks<> MFA not enabled on admin account
In my next blog post, I will cover the next 2 pillars — secure hybrid cloud and high availability.
Oracle Cloud IaaS Security Services for the Enterprises
Written by

Oracle Cloud IaaS Security Services for the Enterprises
Written by

Oracle Cloud IaaS Security Services for the Enterprises
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@fastup/iaas-2-0-b1f1edb8b086?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sachin Dole
Jan 3, 2018·5 min read
Cloud technologies have come a long way since Salesforce presented it’s SaaS (Software as a Service) offerings and Amazon and Microsoft offered the API driven IaaS (Infrastructure as a Service). Along with these large players, there are many PaaS (Platform as a Service) providers such as Heroku, Google App Engine. Between IaaS and PaaS offerings, measured by market size in 2016 as a proxy to customer value, IaaS provided 80% of the value to customers out of a total size of $22B. In other words, customers spent a bunch of resources on designing, implementing and operating public cloud infrastructure. This is projected to increase every year with great growth rates — all good for customers.
In this post, I want to dig deeper into this trend and make a prediction of a new avatar for the IaaS market. Consider a forward thinking IT organization inside any corporation in the US marketplace. Let’s say they want to improve their cost structure (e.g. re-allocate operational expenses) and business metrics (e.g. time to market). There are many options that the modern CIO can use to achieve this improvement. This post being about IaaS, let’s say that one of the chosen options is to migrate to public cloud and derive all those benefits promised to the CIO by consultants and by Cloud Providers. At this point, the IT organization, led by this CIO goes through a potentially long and arduous business transformation. Every time I went to a talk at Re:Invent where a technology executive came on stage to talk about AWS migration, I invariably heard of a process that failed at first and then eventually succeeded. If you look at the process itself, AWS presents it’s 6 perspective framework (CAF), that comprehensively leads customers towards the public cloud. Eventually, the IT organization in question achieves some or all of it’s goals by reducing staff costs, discontinuing contracts with incumbent vendors, closing data centers and what have you. All this is great.
My intention today is to present an alternative way to doing all these great things. The question I ask is, can this organization shorten all that journey and simply arrive at the destination in a simpler and faster way?
Let’s dig deeper. Almost all the blue ovals in Fig. 1. are things that any IT organization must deal with in any migration. All organizations must re-train employees, they all must evaluate a few different cloud providers to decide which one to pick. They all must re-negotiate contracts with current vendors to run vendor software on cloud or to cancel data center contracts and such. The proof of this long and arduous process is in the variety of ways in which a plethora of consulting companies sell cloud migration services to their clients. There are 100s of articles and books written on “how to do cloud migration right” or a variation of that. I am part of a consulting company doing that kind of migration work, I can see that most of my customers ask for the same thing over and over.
The other option for customers is to skip most of the blue ovals and pick a PaaS offering for their solutions. I mean, if one customer deploys a 8 node Sitecore CMS cluster for a global web site, how different will it be from a different a US oriented installation? The only thing that may change drastically is the size of it. Majority of the value of Sitecore is in it’s software. So, a customer may gain plenty of value by going to Sitecore’s own cloud offering — a PaaS offering where the entire sitecore platform is available by signing up for it. However, there are limits to how much control and capability will an organization cede to it’s vendors. Further, contracting with specialized vendors for specific PaaS capabilities brings back the enormous variability in operational abilities and complexities that one was attempting to replace in the first place.
On one hand, “Migration to the cloud” can be long, arduous and on the other hand, “PaaS” can lead to loss of control and abilities. There has to be a better way. There needs to be a new class of vendors who will take away all those blue ovals from Fig.1. and allow IT organizations to move from the “old world” to the “new world” on their own terms and the speed of a pre-built technical solution.
This is not an impossible task. All cloud public cloud providers provide technical tools to design infrastructure in code and implement in variety of operational settings. Why shouldn’t there be a vendor such as FastUp who can offer all the blue ovals of Fig.1. as a service. In this new proposal, the vendor should provide a user interface where system designers from the IT organization can browse effectively through thousands of pre-build IT solutions, customize these slightly and then have those implemented and operated in a secure way. Amazon Web Services already offers a product called CodeStar that does some of this. CodeStar implements a solution in the customer’s AWS Account. This still leaves the customer to deal with all blue ovals of Fig.1. except for “Consultants”. There needs to be something better than CodeStar where the vendor takes on all security on a Service Level Agreement backed by “cash back” for intrusions, hacks and such. The vendor should be able to provide low cost guarantees — once again backed by “cash back” for lower costs found elsewhere in the market place.
Over the next few posts, I am going to attempt to structure this new idea that I am calling IaaS 2.0. I will try to provide specific standards and principles that FastUp would adhere to when FastUp addresses this idea.
I am a dad, husband, motorcycle and mountain enthusiast.
36 
36 
36 
I am a dad, husband, motorcycle and mountain enthusiast.
"
https://medium.com/@Connected_Dots/interoperability-as-a-service-iaas-the-ai-enabled-blockchain-3dafe776050c?source=search_post---------15,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edward Bukstel
Apr 18, 2017·6 min read
(Part 2/2)
It’s time for patients to come to terms with the fact that there is no financial incentive for healthcare providers to consolidate and normalize data from disparate providers.
Patients must be cautious maintaining their patient records on a blockchain or another platform that cannot be used by other institutions, providers, or entities. Without portability, blockchains will add little value to advance patient medical record mobility. Healthcare providers may discover some indirect benefits from the consolidation of medical records — even records not immediately accessible by patients. Furthermore, billions of dollars have been wasted by healthcare systems (payers and providers) developing patient portals, that have attained less than a 2% market adoption, according to Accenture.
Brian Eastwood, a researcher at Chilmark Research, presented an excellent overview of patient portals in the YouTube video below. His research focuses on the role that provider, payer and personal technology can play in advancing consumer driven health and empowering consumers to make healthy decisions.
Maximizing patient value: the real catalyst for change
Imagine a world where blockchains are woven into the fabric of the next generation patient portal architecture.
To maximize healthcare data, located on a blockchain, it is time to seriously consider a Patient Interface Engine (aka Personal Interface Engine, aka PIE). This PIE could deliver normalized data on a patient blockchain. Twenty-five years ago, I coined the term “Clinical Data Exchange.” This term has since grown in meaning, and has been applied to multiple facets within the healthcare ecosystem.
Many platforms, offer software as a service (SaaS) capabilities. For example, IBM offers IBM Blockchain on Bluemix and Microsoft offers Microsoft Ethereum Blockchain as a Service on Azure (ETH BaaS). This field is evolving quickly, and Forbes recently proposed data as a service (DaaS) as a big emerging opportunity for businesses. Changes are coming to a healthcare provider near you.
AI working for the patient
Brandon Allgood also described the commoditization of services offered by cloud service providers and introduced a future where artificial intelligence (AI) is provided as a service, e.g., AIaaS. It looks like this AIaaS may soon be the new “thing” everyone talks about.
In healthcare, before we can transform data into information and apply AI, the data must be interoperable. The Patient Interface Engine is a simple version of interoperability as a service which will be foundational for the advancement of healthcare treatment and disease diagnosis.
If you believe that artificial intelligence, machine learning, precision medicine, and telehealth, are the future of healthcare, then you must also believe that the blockchain will empower patient experience, patient mobility, and data interoperability.
Hospitals used to advertise and highlight differentiation by hiring high caliber surgeons, physicians, or employing advanced technological innovations. These approaches were soon followed by promotions of new treatments, proton therapy, cancer service lines, MRI centers, and precision medicine. In the not to distance future hospitals and health systems will advertise their superior artificial intelligence capabilities. Every hospital will tout their unique machine learning algorithm as the next wave of healthcare transformation to extend the comfort and the longevity of their patients’ lives — a true differentiator.
Provider or Patient owned AI?
These differentiations also come with cognitive bias; inherent in the design of the AI service, bias will be manually machined into the workflow. In all sincerity, how many times will the Mayo Clinic AI service inform a patient that they should better transfer to the University of Pennsylvania for enhanced care? In the very near future patients will be provided customized AI messages from HCPs across the country and the world. These custom AI enabled analyses will be followed up with detailed telehealth visits, discussing specific treatments, prior to a patient ever meeting a doctor “in real life.”
Who owns the duty to care? Hospitals have habitually gamed the system by changing the rules. For instance, in 1998 HCIA (now part of Zacks), performed analytics for US News and World Reports Best Hospital Ratings. While working with HCIA, I observed a New York hospital that quickly shed their badge as the worst rated Cardiac Care to facility, only to rise to the first spot within a year. How did they do this? The simply chose to “dump” patients with more serious conditions to neighboring hospitals.
Blockchain has the potentiation to prevent these types of egregious advertising and practices of outcome manipulations. This change will finally force healthcare organizations to stop gaming the system and offer services that patients may access on a blockchain — a practice that will tailor patients treatment plan before the physician sees the patient. This new practice of AIaaS will disrupt how traditional break-and-fix patient care is managed. Blockchain will create an immediate level of transparency and trends that AIAAS powered by “Interop As A Service” would immediately identify. Transparency will be a fact of life and healthcare in a world with blockchain enabled interoperability and patient powered artificial intelligence.
Joe Guagliardo, Prashant Natarajan, and Sharon Klien are correct to point out concerns of administering incentives and interoperability from Blockchain. Health systems marketing services must be transparent in the incentives they offer and services they provide. They have a duty to ethically care for patients and respect patient portability by supporting interoperability approaches such as blockchain enabled medical records. In fact, Cancer Centers and hospitals tripled their advertising spending from $54 million in 2005 to $174 million in FY 2014. Amazingly, the Cancer Treatment Centers of America individually invested over $100 million in ad spending during FY 2014. It is a fact that health systems will fight for patient mindshare as blockchain opens a global diversity of patient choice and services. In fact, it may be possible for a patient to fly to India from Philadelphia, in less than an hour in 2030.
Unmasking the future of Patient AI
Initially, blockchain will be adopted by healthcare providers as an add-on service to attract patients in search of additional options for mobility of their medical records. This will enable patients to anonymously and instantaneously submit an immutable chronological health event listing. The aggregation of health events will have interoperability, and will allow AI interpretations from global healthcare providers. It may be the case, where some providers are actually pharmaceutical companies looking for patients for highly specialized precision medicine randomized clinical trials. This access will be provided enabling access to potentially thousands of providers worldwide that may provide insight into a patient’s health via an anonymous and interoperable blockchain.
Patient’s will choose destinations for their healthcare based upon the specific AI knowledge, precision medicine plan, and telehealth followup that will extend patient care globally in ways that we truly cannot predict. The responses will include information about how many patients were treated with similar genomic characteristics, as well as specificity on the non-genomic treatments available for a patient. Patients are already opting for Medical Tourism destinations and will spend in excess of $32 billion in 2019. The market has been growing at an 18% growth rate, and the commoditization of AI will insure that more global destinations will try and reach US patients via analysis of medical records referenced on a Blockchain.
Healthcare plans will directly generate custom treatment plans, forecast disease diagnosis, and assemble discrete background information based on AI and machine learning algorithms for the benefit of improved patient care. The AI output will include information about patients treated with similar genomic abnormalities, in addition to greater clarity on available non-genomic patient treatments.
Are patients prepared to have their medical information shared on an anonymous blockchain platform? The recent success of 23andMe demonstrates that 2 million people are prepared and willing to participate on a medical platform for DNA analysis. For the informed, blockchain does offer a near-guarantee of anonymity.
The healthcare reimbursement system is evolving from a system of volume based transactions to value-based care — a new system where the final decision will be the patients.
This isn’t the end, but rather just the beginning. The start of AI enabled patient treatment on a globally accessible blockchain.
Edward Bukstel
CEO, Clinical Blockchain
Father of 2 beautiful daughters, CEO, Clinical #Blockchain, #Pharmaceutical, #Healthcare Technology, #Ethereum www.clinicalblockchain.com
58 
1
58 
58 
1
Father of 2 beautiful daughters, CEO, Clinical #Blockchain, #Pharmaceutical, #Healthcare Technology, #Ethereum www.clinicalblockchain.com
"
https://medium.com/@ory_51733/serverless-and-the-evolution-in-cloud-security-how-faas-differs-from-iaas-4ea8bc7b1dff?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ory Segal
Jan 29, 2019·5 min read
Security is a shared responsibility between the cloud provider and the customer. This shared model can help relieve customer’s operational burden as cloud providers operate, manage and control the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates.
Up until recently, when deploying applications on IaaS platforms such as AWS EC2 instances, the customer assumed responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the network firewalls in the cloud. With virtual instances, customers needed to carefully consider the services they choose as their responsibilities varied depending on the services used, the integration of those services into their IT environment, and applicable laws and regulations.
With the introduction of serverless computing (FaaS — Function as a Service), security responsibility shifted even more towards cloud providers and many tasks are now offloaded from customers, leaving customers to concentrate on their core business.
From a security point of view — serverless computing provides a dramatic boost in the level of security that can be achieved by relying on the expertise of cloud providers to secure the environment, and coupling that with a best of breed serverless security platform such as PureSec, which was developed from the ground up for securing serverless applications in a cloud-native manner.
It’s quite common to hear people state that when adopting serverless architectures, organizations don’t have to deal with the security of the underlying platform. But how much benefit really hides under that blanket statement?
Lets briefly enumerate the core security requirements and tasks that organizations need to handle in order to build and maintain secure applications. The items are listed bottom-up, starting with physical security and all the way up to the application layer.
Now lets see how these requirements and tasks were divided between the cloud provider and the customer, when using IaaS instances as the underlying platform for application development:
CLOUD PROVIDER RESPONSIBILITY:
CUSTOMER RESPONSIBILITY:
Now, let’s see how these responsibilities are divided when developing applications on serverless architectures:
CLOUD PROVIDER RESPONSIBILITY
CUSTOMER RESPONSIBILITY
ROUGH COMPARISON
Even though not all tasks and requirements were created equal, and some of these are obviously much more resource/budget intensive, we can still see the huge benefits to security posture for organizations that adopt serverless.
When developing applications on IaaS, the security responsibilities were roughly divided as following:
For serverless, things are quite different:
Bottom line — here is yet another very important reason to go serverless, Let someone else be responsible for the majority of mundane security tasks, and stay focused on developing and securing your core business logic.
Originally published at www.puresec.io.
Application Security Overlord, Hacker/Innovator/Researcher. CTO @ PureSec, Serverless Architectures Security, former Sr. Director of Threat Research at Akamai.
2 
2 
2 
Application Security Overlord, Hacker/Innovator/Researcher. CTO @ PureSec, Serverless Architectures Security, former Sr. Director of Threat Research at Akamai.
"
https://stacksense.io/serverless-and-sla-do-you-care-966540f97dcb?source=search_post---------93,"Sign in
Krish
Jan 9, 2018·3 min read
The cloud industry has debated for a long time on SLAs and they have standardized on service credits as the compensation for IaaS when the service provider fails to meet the promised SLA. For example, AWS EC2, EBS, Fargate, and ECS offer 10% service credit less than 99.99% but equal to or greater than 99.0% and 30% credit for less than 99.0%. Other providers also offer similar SLA. The rationale for this kind of SLA was based on the fact that end users are responsible for OS and above in the case of infrastructure services. It is the responsibility of the end-users to ensure that their applications are resilient to failures and meet their uptime needs. Such a rationale worked well because end-users had control over the uptime through application architecture and operational efficiencies.
Recent Meltdown and Spectre news brought into focus once again the responsibility of end-users. In this case, the users are responsible for patching the operating system for infrastructure services whereas cloud providers are responsible for higher-order abstractions like PaaS and Serverless. This leads to a lot of hype around how Serverless is helping end customers when something as dramatic as Meltdown and Spectre vulnerability hits users.
When AWS made the blog post about Meltdown and Spectre vulnerability, the discussions about user responsibility came up on Twitter and the industry veteran Tal Klein clearly pointed out that in the case of serverless, even though the responsibility to patch is with the cloud service provider, there is no accountability for any data loss or downtime. His tweet got me thinking again about a topic I was focused on for quite some time as I advocated higher-order abstractions like Serverless as the path forward. It is about SLAs for Cloud Functions offered by major public cloud providers.
It is about the lack of SLA for Serverless functions. Neither AWS Lambda nor Google Cloud Functions offer any SLA. Microsoft Azure also offers no SLA for Azure Functions when used under the consumption model but is backed by their 99.95% uptime SLA when used with App Service Plan (an equivalent of Reserved Instances in AWS). In other words, if you are using Functions as a Service from any of the cloud providers, you are on your own.
Let us take a look one level below the cloud functions in terms of abstraction, PaaS, or CaaS layer. At this level of abstraction, AWS offers SLAs (service credit like their infrastructure services) for both Amazon ECS and Fargate. Google offers SLA for Google App Engine in both standard environment and flexible environment. Microsoft doesn’t offer any SLA for Azure Container Instances because it is a free service but it is driven by the SLA of underlying Virtual Machines. This is not surprising to me because all these services are billed using instance types (even though Google App Engine's flexible environment is slightly different in their billing compared to their standard environment or other cloud providers, they do require you to pick the underlying VM type). In other words, SLAs are available for infrastructure-like services.
Even though SLAs are not available today for Cloud Functions, it will be available in the future as more and more enterprises start using these services. However, we can take advantage of the lack of SLAs to have a discussion about the nature of SLAs for Serverless. Should we stick to the service credit idea of infrastructure services or do we keep cloud service providers more accountable? It is important to keep in mind that the end users had more control with IaaS but lose control over both the environment and, to a certain extent, the applications themselves. Does the idea of service credits even make sense in the case of Serverless? Or, are we going to give a lifelong free “get out of jail” card? As end-users of such higher-order services, it is your responsibility to demand the right insurance policy for your business. What do you think?
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
This blog helps enterprise decision-makers understand the emerging technologies and it is part of Rishidot Research publications
"
https://medium.com/@daneshzaki/iaas-vs-paas-confusion-1364d38fe4b5?source=search_post---------278,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dânesh Hussain Zaki
Mar 22, 2019·1 min read
I was recently involved in an opportunity where I had to assess integration platforms for a customer. The requirements were broad and typical of most customers. I included the popular iPaaS (Integration Platform as a Service) platforms and learnt that the customer was interested in looking at PaaS (Platform as a Service) options too. But some of the products that I was evaluating were being positioned as IaaS (Infrastructure as a Service).
Now, as I understand, IaaS is concerned about providing Virtual Machines and Cloud resources, where we install and manage our own platform , and PaaS means availability of a platform that is managed by the Cloud provider. The use of the term IaaS for integration platforms baffled me and I was not able to get a satisfactory response from the respective SME on why it was termed so.
Have you come across such usage? Or is my understanding of what an IaaS or PaaS is incorrect? Please let me know your thoughts.
Cross posted at https://www.linkedin.com/pulse/iaas-vs-paas-confusion-danesh-hussain-zaki/
Centrist, Software Consultant
See all (44)
Centrist, Software Consultant
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-tag-ventures-vietnam-importance-of-user-protection-and-security-and-sne-token-e066cf41778d?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·5 min read
Infrastructure-as-a-Service (IaaS) tech company and innovation lab StrongNode.io’s CEO and Co-founder Daniel Saito talked with the TAG Ventures Vietnam community for a new AMA episode last 22 September 2021. Daniel replied to the questions about our company and shared facts about the upcoming IDO launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Don’t despair if you missed it. Check out some of the highlights from the AMA session between StrongNode and TAG Ventures Vietnam:
What do you consider to be the best features of “StrongNode” ? Are there any features that “StrongNode” has that we can’t get on any other platform? Where can we stay informed of all the news that “StrongNode” has to offer us?
Daniel Saito: Thank you for the question, you can follow us here for a lot of follow up questions and answers. As we get closer to launch, you probably want to keep informed of all product and platform updates
🥮 http://strongnode.io
🗯 https://t.me/strongnodechat
🗣 https://t.me/strongnode
📚 https://twitter.com/StrongNodeEdge
🎟 https://discord.gg/Gk2ka3NCR6
📚 https://medium.com/@strongnode
When we first started the thought of StrongNode we looked outside to see what we can do to help solve some of the world’s problems. Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. With that said, we had to come up with a solution or an alternative and looking inwards of what is available we noticed that we already had the blueprint already in front of us in the form of Open Source.
We wanted to address the last mile, which is the EDGE. It’s the consumers’ latent resources from your PC/MAC/LINUX [environment] (CPU, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with a token economic incentive to participate both as a broadcaster and a receiver. The solution was designed with security in mind and made with ❤️ for the NEW NORMAL.
We wanted to grow the digital footprint of our decentralized distributed network and to be able to get token distribution to as many wallets as possible which will, in the long term, help the value of the $SNE token.
The StrongNode $SNE token has so many use cases within the StrongNode ecosystem which includes our innovation lab projects focused on gaming, social impact, entertainment, lifestyle platforms building on the StrongNode Edge technology. You get paid for your idle resources and others get to run workloads, processed in chunks, on your idle resources. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. There are more mechanics yet to be announced that will enhance the appeal of token adoption.
User Protection and Privacy has become an issue that many projects face in their development. Can you explain the security of the StrongNode and how the StrongNode protects its users?
Daniel Saito: As a former security analyst, researcher and developer; I was originally tasked at designing the framework for the trusted mobile platform using a TPM or otherwise known in today’s term as “secure enclave” in 4G devices with NTT DoCoMo R&D, Intel and IBM. I want to take the same principle but also apply it to the anonymous / cryptographic model of crypto and create the ultimate solution in network and privacy where the user add value and in return they receive a monetary reward.
As for privacy, we only know you by your wallet address at any time. All personal KYCd data will be stored offsite with our KYC authentication partners. You will need to be properly KYCd to join the StrongNode Network. We plan to not only have Audit certificates but we also plan to use 3rd party services to attack our network, have bug bounties, and most of all it will be OPEN SOURCE (even that will be all audited).
In order to help this process, we reinvented OpenID. This is our SSO (Single Sign On) approach like “Login with Google” or “Login with Facebook” to validate you as a user to enter the network. Upon completing the KYC component at User Registration, not only do we airdrop you $SNE tokens, but you also get this nifty NFT dropped into your wallet. This NFT is specific to your wallet address and your identity, which cannot be moved out of your wallet. This NFT acts as an access token (a key) to login to various crypto properties that we are working on in our innovation labs. We want to use best of breed open source projects and develop StrongNode with the tech.
Security and anonymity are always prioritized by BlockChain projects in the development of project platforms and technologies. So, do you have any technological solutions or plans to enhance user trust in these issues?
Daniel Saito: Yes, we took extra constraints to ensure data sovereignty distribute our MASTER nodes in locations where GOV’T have strict regulatory on user privacy and encryption.
This is the next era of where and how data is stored. Cloud makes things easy, but if the provider is a USA-based corporation. The US government will have access to that data.
Daniel Saito: Please join us and grow Vietnam’s digital footprint on StrongNode.
For more information, visit us: http://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
213 
213 
213 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/chenjd-xyz/azure-fundamental-iaas-paas-saas-973e0c406de7?source=search_post---------151,"There are currently no responses for this story.
Be the first to respond.
When talking about the topic of cloud computing services, you must have heard of these concepts, Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). They are the three major service categories provided by cloud providers. It is important to understand what they are and what is the difference between them because they will appear on various occasions where cloud computing services are discussed.
"
https://medium.com/@sw_samuraj/oci-monitoring-via-prometheus-service-discovery-851f022f0cdb?source=search_post---------324,"Sign in
There are currently no responses for this story.
Be the first to respond.
SoftWare Samuraj
Nov 27, 2018·5 min read
OCI (Oracle Cloud Infrastructure) is one of the clouds out there. It isn’t anything you wouldn’t expect from a mature IaaS platform, so if you already know other providers like AWS, Azure, or GCP, you should be familiar what to expect. OCI consists from many products, but what I’m going to focus on is Compute.
Basically, Compute provides you with computing resources — you can start VM instances with desired parameters (like number of CPUs and RAM), those VMs run in specific VCN (Virtual Cloud Network) and subnet(s), and so on. Of course, you can configure some load-balancers, or restrict network traffic with security lists.
Alright, we have couple of running VM instances and we would like to monitor them — what approach we should take? One of the well established monitoring solution in the cloud environment is a Prometheus suite. So, how do we harness Prometheus to monitor our OCI instances?
You can define Prometheus monitoring targets via scrape configuration. And although there is couple of out-of-the-box service discovery providers for some of established clouds, the OCI service discovery is not there.
Unfortunately, Prometheus developers put a moratorium on adding of new service discovery providers and recommended alternative is to use file based service discovery.
This kind of discovery means that Prometheus periodically check a specific file (or set of files) which contains a list of monitored targets. The content of this file is a JSON structure in following format:
The question is, how to populate this file with the list of OCI instances, ideally with some helpful labels?
Checking out the internet, I found out that there is nothing ready yet, so I wrote such discovery by myself: oci-sd. It works in following way:
As you can see, OCI-SD is periodically querying OCI API to obtain a list of instances (in a specific compartment). Then it processes instances into targets with some meta-data and finally, it writes the targets in a file. This file is then periodically consumed by Prometheus, which re-labels meta-data and finally/optionally, drops those targets which are not supposed to be monitored.
OCI-SD can be used in two manners — either as a standalone application (this approach we follow further in this article), or as a Golang package, which can be imported and use inside your own application.
Currently, we are living in the “GitHub Era” — so, to have good documentation and practical examples is crucial. As a part of the oci-sd project, I prepared an example/tutorial where you can:
Of course, you can create needed testing instances manually via OCI admin console, but Infrastructure as Code (IaC) is hype today, so let’s ride that wave: there is couple of Terraform scripts in the example/terraform directory. To be able to run them, you need two things: Terraform binary and an OCI tenancy with a policy that allows you to manage resources.
I expect that you are already familiar with OCI CLI Configuration to be able to access OCI API. You need to configure couple of equivalent environment variables for running of the Terraform scripts. The most easy way is to edit env-vars file and store it with your specific values:
Then you can export these variables and run Terraform:
In few minutes, you should be able to see following (abridged) Terraform output as well as to see three running instances in your OCI console:
Every one of these instances should have running node_exporter on the 9100 port. This port is already open in the configured security list:
You can check that everything is fine by curl:
If you see something like following (abridged) output, you are ready for service discovery.
If you run oci-sd as a standalone application, you need to provide a configuration file. By default, it’s called oci-sd.toml and it should contain all those OCI access values you have already used for the Terraform scripts:
Once you run oci-sd application, e.g.
you should see (roughly after 1 minute) a similar content in a newly created oci-sd.json file:
Voilà, we are almost there!
It’s easy-peasy to configure Prometheus to grab the oci-sd.json file to obtain monitoring targets. A little bit challenging could be re-labeling, so here is a short example:
Such configuration tells Prometheus to:
When you run your Prometheus with this configuration, you should see new targets in the Prometheus GUI:
Going back to Prometheus re-labeling, you can find a complete set of available meta-labels in the oci-sd documentation: see Metadata labels.
You are now able to monitor your OCI instances in the given compartment. Now it’s your turn to find out that the discovery is really dynamic — try to stop, or terminate some instance, create a new one either manually, or with Terraform and so on.
Just mind two things. First, it takes some time for a target(s) to show up/display a change. The change propagation is result of time consequences — mixture of three(!) refresh intervals in oci-sd and Prometheus.
And second, especially for newly created instances we define a target dropping via freeform tags in the configuration, so just you are not surprised that instances without such tag don’t show up as targets.
So, that’s all, folks. Happy monitoring a let me know about your thoughts, problems and ideas!
SoftWare Samurai & Golang ninja. Avid reader & marathon runner.
14 
14 
14 
SoftWare Samurai & Golang ninja. Avid reader & marathon runner.
"
https://medium.com/nerdofcode/getting-started-with-linode-97ac155e3ccf?source=search_post---------383,"There are currently no responses for this story.
Be the first to respond.
Having troubles deciding on an IaaS( Infrastructure as a Service) provider in 2019? Well, check this problem off your list as by the end of this post, I will have solved it!
First off, Linode is by far, one of the big brothers in the Iaas game. Linode started off in mid 2003, with only three data centers to its…
"
https://betterprogramming.pub/how-to-build-an-asp-net-web-api-with-entity-framework-and-retrieve-data-from-an-sql-server-43b57677a14a?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Bogan
Jul 14, 2019·4 min read
Choosing between PaaS and IaaS is critical. The wrong choice can not only slow down your team from the start, but it can cause a spiral into long-term costs as you release more code, build more features, and become more and more embedded in your decision. However…
"
https://medium.com/db-one/oracle-cloud-infrastructure-sdk-6a35983037f8?source=search_post---------375,"There are currently no responses for this story.
Be the first to respond.
Oracle second generation public IaaS named Oracle Cloud Infrastructure (or OCI) has an open source mindset. As such, delivery is really dynamic and new capabilities are added at a weekly if not daily pace.
In order to test rapidly these new capabilities OCI, proposes different SDKs (in Java, Python, Ruby and Go) for developers.
These open source SDKs are available on github such as the one for Java: https://github.com/oracle/oci-java-sdk/releases
Enjoy the numerous examples!
Data stories…
1 
1
1 clap
1 
1
Written by

Data stories…
Written by

Data stories…
"
https://medium.com/@cloud66/managed-kubernetes-on-more-public-clouds-good-news-but-also-distracting-15dd5deca2af?source=search_post---------77,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Jul 10, 2018·4 min read
Joining the fray One of the most predictable surprises in the public IaaS market recently has been DigitalOcean’s public launch of their own managed Kubernetes service, at KubeCon Europe last May. In my opinion, it was predictable for at least three reasons. First, customers. DigitalOcean is one of the largest and most prominent public-cloud providers, and an extremely user-focused company. Its users were asking for this capability, so that they could try out Kubernetes on their existing and favorite provider.
Second, competition. Since the announcement of Amazon’s EKS and Fargate in late 2017, we’ve seen a flurry of reaction from what I call “tier 1.5” and “tier-2” providers (compared to the top-3 of AWS, Azure and Google Cloud), such as IBM, Rackspace, Alibaba, T-Systems and literally dozens of others. Not playing this game would be detrimental to any provider’s competitive position in this rapidly-consolidating market.
Third, it just makes sense, if you’re following this market. IaaS companies build services that help users spin up more machines, and Kubernetes is no different. With an impressive pedigree from Google, a vibrant and strong community in the CNCF, and tailwind from the top-3 clouds, Kubernetes is the dominant container orchestration technology. If your users are writing microservices apps, they’ll be using Kubernetes much more often than they will any alternatives.
Cloud 66 ♥ DigitalOcean (as well as all our other cloud provider partners) Cloud 66 has had a long and fruitful partnership with DigitalOcean. On average, our joint users spin up hundreds of new “Droplets” a month to deploy their Rails, Node, or microservices apps — with many thousands more still running. Many of these joint customers even use Maestro, our own multi-cloud lifecycle management tool, itself backed by Kubernetes.
Is the abundance of managed Kubernetes services good news? Kind of. As we’ve said in a previous post, if you’re a cloud provider who is not enabling users to spin up Kubernetes clusters in 10 minutes through a friendly UI, then most likely another provider will.
More options is a good thing, but in our experience, some of these services are better (more robust, feature-rich, performant) than others — which will be important to production users. Also, a wealth of options is valuable if it helps you get the most out of the Kubernetes promise of portability. In this case, you will need to make sure that you use managed Kubernetes services that don’t lock you out of a multi-cloud or hybrid architecture (again, that is where tools like Maestro come in handy).
So yes, it can definitely be good news for the savvy user, but at the same time, it’s not really interesting news, because this is ultimately a solved problem, and a distraction from the greater operational challenges embedded into Kubernetes, which we’ve discussed in this post.
Dev friendly can also be Ops-friendly While it’s great for everyone to have a Kubernetes cluster on their cloud of choice with a few clicks, ultimately it is in an ops-friendly environment (IaaS), while the challenges are how to add a dev-friendly experience to that layer, that operators can trust.
So if managed-public-Kubernetes is a given, what are the next problems? From our experience running thousands of customer workloads on containers, they are mostly around security (container and code vulnerabilities, runtime access, secrets management etc.); lifecycle management (multi-cloud deployment, leveraging stateful workloads, network management etc.); and container pipeline (delivery and deployment).
We’ve had to solve most of these issues ourselves over the years by developing tools, and have been offering some of them in our container toolchain and our open source products. With regards to the pipeline, your challenges might revolve around things like:
In the end, an operator’s job about thinking what will happen when scale comes. What won’t scale is bespoke manual processes, reliance on custom tech in a commoditizing market, un-portability, processes that make people wait, rusty knobs that are not fit for the new purpose, or wobbly ones that don’t work first-time, every-time.
What will scale is tested infrastructure, repeatable and reliable automation, self-service for developers that operators can trust, abstracted workload portability, and above all, tools that facilitate the shared world view that Kubernetes mandates.
The latter list has driven our product development since we started Cloud 66, and is embedded into our tooling. Check out our container toolchain and our open source products as the best complement to whatever Kubernetes, hosted or on-prem, you are using.
Originally published at blog.cloud66.com on July 10, 2018.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
27 
27 claps
27 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/datio-big-data/iaas-provider-or-bare-metal-directly-95e90fda4de9?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
One common question when deploying a cloud architecture is why using virtual servers (probably an IaaS) instead of using a bare metal server directly. The right choice depends on the concrete use case of the system and is determined by different factors, but the main one is the performance/cost ratio (using the available resources with efficiency). This post aims to give an opinion about why choosing one of the options, giving advantages and disadvantages for each one.
If money is no object and performance is the only thing that matters, this is a perfectly rational choice. Virtualization adds an abstraction layer over the resources available, but it’s not free of charge, and the performance is reduced compared to a bare metal server with the same resources, including the “noisy neighbor” effect. This effect is given by having multiple virtual machines running on the same physical server, so these VMs are not physically isolated and one of them can affect the performance of the rest by fighting for shared hardware resources (like IO devices or networks). Even security can be involved if the virtual instances are not properly isolated. Actual state of the art allows avoiding some of these problems, but there’s still an impact in the performance. Major Hayden performed a benchmark to compare running on bare metal against KVM (linux kernel hypervisor, used for virtualization) and he showed that there is a 2% performance impact. When looking at IO performance (disk, memory and network) the difference is even bigger. For those who need a probe of these results, there is a very interesting study performed by IBM comparing native servers against Docker containers and KVM virtualization.
Imagine that there is a house with a perimeter of eighty meters. The aim is to put a wood fence around the house, which should have a height of two meters to avoid thieves entering the house. For doing that, there are eight wood boards from a previous work. Each board has a width of one meter and a height of twenty meters. One way to go is to place these boards together in front of the house, but, even when the fence has a height of twenty meters, it only covers eight meters of the eighty meters perimeter, so the thieves can still enter the house easily. This means that the available resources don’t fit the use case and another solution is needed. The easier solution is to go to a DIY store and buy eighty boards, each one with a width of one meter and a height of two meters, and then build the fence putting all the new boards together. However, other solution is to get a compass saw and start splitting each of the eight original boards into ten smaller boards with the desired size, getting the same resources in the previous solution. The first option was faster (has a better performance), but also was more expensive, because there is a need to buy the required boards. The second one was slower (splitting the boards is not an easy job) but there is no additional cost, having a more efficient use of the available resources by adapting them to fit the use case.
The compass saw is like the IaaS provider (Openstack) and the original wood boards are like bare metal legacy servers. As the example shows, the IaaS allows making a more efficient use of the resources, reducing costs and giving more flexibility (very different use cases can be covered by the same resources) and scalability (adding and removing virtual machines is easier than physical ones when there are enough resources). The final result is a cost reduction, but with a performance impact. So, if there are legacy resources or there are many different use cases that need to be covered by the same resources without taking care of performance, the IaaS provider is the right choice.
For choosing between using bare metal or an IaaS provider the two main attributes to consider should be performance against cost. This ratio is the key for choosing one of the options, but there can be some preferences for one of the attributes.
Performance is always better in bare metal servers than when using an IaaS provider due to the abstraction layer added by virtualization, but it allows improving resource utilization so it has lower costs.
Finally, it’s important to mention that some IaaS providers can work directly with bare metal servers instead of virtual instances, taking the advantages of the two options.
Originally published at www.datio.com on November 2, 2016.
Stories about #data engineering and the people who build…
Stories about #data engineering and the people who build @datiobd
Written by
Developer and Software Engineer. Vocational Computer Scientist. Always learning.
Stories about #data engineering and the people who build @datiobd
"
https://saasholic.com/moving-to-the-cloud-how-to-choose-between-iaas-paas-saas-solutions-39cf535580cc?source=search_post---------198,"The owners of both small and mid-sized businesses and large enterprises should by now be aware of the disadvantages of building on-premise servers.
For example, did you know that physical servers require continuous updates every 3 to 5 years? To keep them running like a charm, you will need to hire a team of skilled IT professionals, which can result in substantial costs.
The next problem you may face is scalability. To adapt your on-premises infrastructure to your growing business’ needs, you will need to constantly invest in new hardware and software items.
Finally, in case of data breaches, thefts, or physical disasters like floods or fire, physical infrastructures may be harmed. This means companies may lose sensitive data and would not be able to retrieve them, which may hinder the workflow and workplace productivity.
Precisely because of that, the number of businesses investing in the cloud is constantly growing. Surveys say that the public cloud computing market is expected to exceed $330 billion by the end of 2020, while 90% of companies have already been using cloud services.
These stats show that the cloud has become mainstream. Therefore, you should never ask yourself whether you should migrate to the cloud. The answer to this question is clear — yes. The question you should ask is “What kind of the cloud should I choose?”
According to the technology model you will use, there are the three most typical types of cloud infrastructures to invest in — IaaS, PaaS, and SaaS.
In this brief guide, you will learn more about them.
Software as a Service, or SaaS, models are a preferred option for both small businesses and large enterprises. SaaS services are delivered over the internet. They eliminate the need for having your IT teams download and install software on individual devices. Instead, SaaS is managed from a central location and hosted on a remote server. This way, every member of your team can access their data, irrespective of their location. This is particularly important to the companies investing in remote teams.
Using SaaS can boost your business performance on multiple levels. For example, it could work for you if you need certain tools when working on short-term projects or when using apps that are not needed that often. It is also perfect for the everyday apps your employees need to access both from their mobile and desktop devices.
The major benefit of SaaS is that companies investing in it do not need to care about software and hardware maintenance. The SaaS vendor manages everything, from applications, middleware, data, and O/S to virtualization, servers, storage, and networking. This way, SaaS will streamline your maintenance and support efforts and simplify many aspects of your IT teams’ jobs. Instead of wasting time installing apps on other employees’ computers, they can finally focus on more complex activities and issues within the organization.
The major disadvantage of investing in SaaS is greater security risks. When investing in a SaaS platform, you will not have any control of the infrastructure it runs on. For example, in case of cyber breaches, natural disasters, or network problems, your business’ performance may also be harmed. Another major issue you may face is the lack of customization, as you will need to use the platform’s predefined integrations and features.
The most common examples of SaaS are:
IaaS lets you build your own, customized environment online. Standing for Infrastructure as a Service, IaaS lets you shift major IT aspects to the cloud vendor, including networking, servers, storage, and virtualization. You are basically rending the space and equipment needed to enable an uninterrupted workflow online.
This is an amazing option for companies developing the devops mindset and wanting to launch their software faster by bridging the gap between their development and operations teams. Task automation, the maximum utilization of resources, and continuous integration and deployment will help you build a more secure infrastructure, solve problems faster, and boost productivity and innovation.
With IaaS, you will also keep complete control of your applications, data, and infrastructure. Yet, you will pay only for the services you are using at the moment. Above all, you are given the flexibility to scale your cloud infrastructure up or down, without spending more money.
On the other hand, you will need to manage and upgrade software, including operating systems, data, runtime, applications, and middleware.
Given that your IT staff maintains software components, you will have greater control of your data in the cloud. Still, you will need to hire an experienced IT team, as well as in the tools they can use to get the most out of this process.
The most common example of IaaS service providers are:
PaaS, or Platform as a Service, is primarily designed for developers and companies building their own software. This way, they can easier develop, run, and manage software without needing to worry about infrastructure management.
In other words, developers do not need to start from scratch when developing software. Instead of investing in expensive hardware and software, they can save both and money when writing on the code that already exists. Precisely because of that, PaaS is perfect for any company wanting to develop their own app and yet save up.
This way, developers can focus more on the creative aspects of the app development process, such as app testing or deployment. With PaaS, companies need to manage only applications and data. The PaaS provider handles the operating system, runtime, middleware, virtualization, servers, storage, and networking. As it is built on virtualization technologies, PaaS is also highly scalable and affordable.
Like with any other cloud service, when investing in PaaS, many companies worry about their data security. There are also many performance issues to consider. If the platform breaks, you will not be able to access your software.
The most common examples of PaaS service providers are:
Each cloud option has certain advantages and disadvantages. You need to consider the specific features and functionalities they offer. Still, no matter which type of cloud infrastructure you choose, it will benefit your business in multiple ways. Instead of building a pricey on-premises infrastructure that is difficult to adapt to your business’ growing needs, with the cloud, you will reduce costs, enhance data security, and enable greater scalability and flexibility.
I hope these tips will help you!
© 2022 SaaSholic. All rights reserved.
"
https://medium.com/@jaychapel/5-things-to-look-for-in-an-iaas-cost-management-tool-aff7be5a9447?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 24, 2019·3 min read
With $39.5 billion projected to be spent on Infrastructure as a Service (IaaS) this year, many cloud users will find it’s time to optimize spend with an IaaS cost management tool. With so many different options to choose from — picking the right one can be overwhelming. While evaluating your options, you should have an idea of what would be most compatible for you and your organization. In order to cut cloud costs and waste, make sure you look for these 5 things while picking an IaaS cost management tool.
When adopting a new piece of software, you should not be stressed out trying to figure out how it works. It should be designed around the end user in order to give them an easy user experience so they can accomplish tasks quickly. Many native tools required by the cloud providers require specialized coding knowledge that the IaaS users in your organization may not have. Whether it is useful or not depends on how simple and easy to follow it is so that every cloud user can contribute to the task of managing IaaS cost.
It is essential that you have all of your information available to you in one place — this helps make sure you didn’t overlook anything. Seeing all your resources on one screen, all at once, will allow you to pinpoint strengths/weaknesses you need to focus on to that will help manage your IaaS cost. Of course, cost management includes more than visibility, which leads to the next points.
You want your organization to be well informed, so it is important that any IaaS cost management tool you adopt includes the ability to generate cost and savings reports. You can’t change something if you don’t know what it means, the data gathered — past and present — will help you understand the past and make a forecast for the future. These reports will give you the information you need to make quick, informed decisions. Preferably, they contain automated recommendations as well based on your resource utilization history and patterns. Additionally, it’s important for any cost optimization tool to report on the amount of money you have saved using it, so you can justify the cost of the tool as needed to your management or Finance department.
After gathering the data and making suggestions, the next step in cost optimization is to actually make these changes. Using the reports and data gathered, the tool should be able to manage your resources and implement any necessary changes without you having to do anything.
Even though it goes on in the background, APIs are necessary because they allow your tool to work in conjunction with other operations. With the support of inbound actions and outbound notifications, this automated process allows you to streamline all of your data. This will make things faster and more efficient — allowing you to cut down on time and IaaS cost. Highlights to look for include Single Sign-On, ChatOps integrations, and a well-documented API.
These are just a few of the things you should be looking for when searching for IaaS cost optimization — but you have to find the platform that works best for you!
ParkMyCloud automatically optimizes your IaaS costs with these principles in mind — try it out with a 14-day free trial and see if it’s the right fit for you.
Originally published at www.parkmycloud.com on January 22, 2019.
CEO of ParkMyCloud
See all (317)
8 
8 claps
8 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/sometimes-aws-is-smartest-when-it-targets-applications-over-infrastructure-5228482bacdb?source=search_post---------75,"This is a reprint (more or less) of Monday’s ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
If you’re reading this, I assume you know that Amazon Web Services’ annual re:Invent conference is taking this week and probably have read at least one story outlining what the industry expects will be announced (and possibly also realize the event now spans 4 separate hotels on the Las Vegas strip, which really are nowhere near one another). But in case you haven’t done the latter, here are couple to check out:
The TL;DR version is that, among other things, people think AWS will roll out some new AI capabilities (obviously), as well a bare metal instance type and enhanced Kubernetes support, or just a straight-up Kubernetes offering.
AWS has actually been getting the AI ball rolling since last week, when it announced the Amazon ML Solutions Lab to help customers with their AI journeys. Today, AWS announced a an award program for machine learning researchers that will give them access to funding and AWS credits.
As much attention as these — and whatever else is announced starting with the first re:Invent keynote on Wednesday — will get, we’d be remiss to ignore the unimpeachable logic behind two new services AWS rolled out (somewhat under the radar) today: Amazon Sumerian and AWS Media services. The former is a tool for easily creating AR and VR experiences, and the latter is a service for processing and storing video content.
I think these services are very smart in similar, but slightly different ways. Sumerian is smart because if it catches on, AWS is getting ahead of the curve on new types of applications that will require cloud computing resources in order to run. AWS made a killing powering the first big wave of mobile apps several years ago, and willing early AR/VR developers would set it up nicely to repeat that success with a new type of application. It did something similar in theory, but in a much different field, with the Connect call center service it announced in March.
AWS Media Services, on the other hand, looks like an attempt to take a bite out of YouTube’s dominance as online video content becomes even bigger. Obviously, the service isn’t targeting everyday consumers who just want to share their vacation videos, or probably even hobbyists posting passion projects or wannabe internet celebrities desperately seeking virality. Rather, AWS’s message seems to directly target companies who either (1) already create and publish a lot of video or (2) know they’re about to start doing so.
If the options are to process and host those videos on YouTube as their primary home (which I suspect an awful lot of companies and individuals currently do because it’s free and easy to embed them) or to process and host them using AWS Media Services, the latter might look like a better option. Yes, the AWS option costs more money, but it also offers a broader set of capabilities around processing, editing, security and output formats. And you can always upload files to YouTube later, too.
This application strategy is the same one AWS has employed before in its competition with Google and Microsoft, around email, docs and meetings, and one it definitely needs to employ more often if its goal is total cloud domination. Because while AWS built up a huge early lead in the cloud infrastructure race, its two biggest competitors own applications you might have heard of such as Office, YouTube, Gmail, YouTube and more.
It’s easy to forget that users have personal and business allegiances that might run deeper than which cloud provider has the better database option or the cheapest instances. There are package deals, email/calendar/collaboration integrations, and just plain, old familiarity that might result in choosing one provider over another. The goal for everyone is making signing onto their platform the most important thing users need to do each day.
Finally, speaking of cloud competition, I feel obliged to share this story about how Microsoft’s new VMware-on-Azure service (which I linked to last week) apparently was developed independent of VMware and is not supported by VMware. AWS’s similar service, on the other hand, was developed with VMware and is supported by VMware. Conventional wisdom typically gives Microsoft the edge when it comes to winning really large enterprise accounts, so you have to wonder whether a VMware connection is really necessary and, if so, how much of an edge that gives AWS.
medium.com
techcrunch.com
www.technologyreview.com
www.bloomberg.com
www.nextplatform.com
www.zdnet.com
www.reuters.com
architecht.io
www.wired.com
www.technologyreview.com
www.wired.com
deepmind.com
arxiv.org
blog.acolyer.org
www.redhat.com
techcrunch.com
techcrunch.com
cloudplatform.googleblog.com
hbr.org  Define the outcomes you want, and the structure will follow.
www.geekwire.com
cloudplatform.googleblog.com
arxiv.org
architecht.io
jaxenter.com
engineering.linkedin.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
3 
3 claps
3 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@estroid/it-makes-sense-information-architecture-fe7bafa1098b?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dangerous Thinking
May 12, 2017·4 min read
The non-profit organization, Information Architecture Institute, offers a handy definition and describes “information architecture” (IA) as “helping people understand their surroundings and find what they’re looking for, in the real world as well as online.”
A common example of information architecture is evident when viewing a map, which helps us to understand where we are in relation to what is around us, what to expect when we take a turn or head west, and how to best navigate to get somewhere.
Good information architecture does the heavy lifting of prioritizing what information should be emphasized according to the user’s context and what may be most useful to them.
According to UX Booth, information architecture is “a task often shared by designers, developers, and content strategists. But regardless of who takes on the task, IA is a field of its own, with influences, tools, and resources that are worth investigation.”
Put simply, information architecture is an important practice within multiple disciplines (such as user experience, interaction design, and content strategy, to name a few).
It’s no wonder then that the founder of modern information architecture, Richard Saul Wurman, was both a graphic designer and an architect (and it should be mentioned too — the creator of TED conference!).
According to UX Booth, in the 1970s, Wurman felt information should be organized similarly to how a building is structured —that is, “with a solid foundation.” The author Gary Wolf also described Wurman’s work as fostering the idea that “the presentation of information can be more important than the information itself.”
Usability.gov emphasizes that information architecture should organize, structure, and label content in such as way as to make content effective and sustainable for users. In other words, users should be able to find information and complete tasks.
In 1998, Louis Rosenfield and Peter Morville published Information Architecture and the World Wide Web, a text about applying the principles of architecture and library science to web site design.
Morville and Rosenfeld describe the relationship between users, context, and content as “information ecology” and visualized it in a venn diagram:
Examples of “context” include business goals, funding, politics, culture, technology, resources, and constraints. Considerations for “content” could include content objectives, document and data types, volume, existing structure, governance and ownership while “users” may involve an audience, tasks, needs, information-seeking behavior, and experience.
As far as information architecture goes, these two quotes by Wurman seem to perfectly capture the possibilities of information architecture:
And:
Allow the information to tell you how it wants to be displayed. As architecture is ‘frozen music’, information architecture is ‘frozen conversation’. Any good conversation is based on understanding.
In 2014, Morville offered his thoughts about the dangers and ethics of information architecture during the 15th Annual IA Summit Keynote:
…our biggest barriers aren’t design and technology but governance and culture. We can’t create good products and services without well-defined goals, roles, relationships, processes, and metrics; but all too often we’re seduced by what’s simple and superficial. Plan and build get split, and we fail to learn. Us and them are divided, and we fall apart. Classification shapes collaboration in nearly imperceptible ways….All maps are traps. To see and share this truth is dangerous, but not bad. The first step is seeing where our categories come from. Our bodily experience is embodied in language and quietly shapes how we think. This is clear in our use of binary oppositions: in-out, up-down, male-female, self-other. Dichotomies help us make sense. We only understand hot in relation to cold. But, the first term tends to be primary….So, it’s better to be in than out, up than down, true not false, good not evil, us not them….This is where we get into trouble. Dualism works because it’s simple, but that’s also precisely why it fails….We can do better by being more creative and courageous in describing and organizing ourselves….But we can and will do better. It starts with awareness. There’s more than one way to classify a cat. Once that door is open, we can nudge ourselves and our colleagues towards celebrating both differences and similarities.
Ultimately, how can information architecture — and those practitioners who apply it — help to thaw, so to speak, the “frozen” aspect of the conversation that Wurman mentions (thereby fostering the dynamic, continual fluidity of exchange) and, at the same time, resist the easy binaries of categorizations and our fears of complexity that Morville warns us about?
Writer, Educator, & Designer for Social Change Living on a 35' Sailboat (WayfindersNow.com)
See all (533)
4 
4 claps
4 
Writer, Educator, & Designer for Social Change Living on a 35' Sailboat (WayfindersNow.com)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intuit-engineering/dse-pronto-ea336bc00ee0?source=search_post---------323,"There are currently no responses for this story.
Be the first to respond.
Intuit engineers Ben Covi and Nancy Li are excited to launch DSE Pronto. DSE Pronto is an “IaaS” automation suite for deploying and managing DataStax Cassandra clusters in AWS.
DSE Pronto is a framework of automation tools used to deploy and manage DataStax Cassandra clusters in AWS. Intuit engineers Ben Covi and Nancy Li developed the repository after finding no third-party suite of similar tools existed to support self-managed clusters. There are fully managed solutions such as Amazon Keyspaces and DataStax Astra, but DSE Pronto represents a collection of configurable tooling for users who want to own their infrastructure.
Scaling and restacking a Cassandra cluster can be challenging. DSE Pronto collects years of cluster management experience, and solves many problems with automation. It uses Hashicorp’s Packer and Terraform (both open source) for building images and deploying resources in AWS; both of these tools support other cloud providers as well, which should allow the framework to be extended into Azure or GCP. Red Hat’s Ansible (also open source) is used to conduct runtime configuration management over SSH connections.
All of the above tools are built into a Docker image, which can then be used to run every phase of deployment from the orchestration platform of your choice (Jenkins, CodeBuild/CodeDeploy, Bamboo, GitLab, Spinnaker).
These elements are all well understood, given the open source nature, maturity, and wide distribution of the tools involved. But a framework tying everything together using best practices is something new, and hopefully useful. Ben started by releasing DSE Pronto internally at Intuit via InnerSource, and the user community has grown quickly. He hopes open sourcing will help it to grow even further.
Thoughts from Intuit on tech, data, and the culture…
9 
9 claps
9 
Written by
Aliza Carpio is Technology Evangelist at Autodesk| Inventor http://www.linkedin.com/in/alizacarpio/ ; https://creativelybrave.me/
Thoughts from Intuit on tech, data, and the culture that powers today's innovation.
Written by
Aliza Carpio is Technology Evangelist at Autodesk| Inventor http://www.linkedin.com/in/alizacarpio/ ; https://creativelybrave.me/
Thoughts from Intuit on tech, data, and the culture that powers today's innovation.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/iaas-ci-cd-with-kubernetes-on-gcp-694905653382?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
We recently finished a project at a client where we revamped its entire devops using “Google Deployment Manager”, “Google Cloud Build” and “Spinnaker”. While this was centered on Kubernetes engine (which is the way to go if you want to leverage the full benefits of the cloud for your application), this framework could be used for deploying monolith applications on VM, deploying cloud functions and apps on app engine. There is a multitude of tools outside for carrying out devops; however we are convinced that the combo of GCP and open source tools described in this article is one of the most efficient and user-friendly.
Let’s start by having a high overview of the architecture used and an architecture drawing is worth a thousand words.
It all starts with the codebase and having a thorough git strategy is one of the prerequisite. GCP has now its own version control module called source repository and it can mirror instantaneously your code from Bitbuckets and Github. In our case, the client had its code on Bitbucket and we did mirror to source repository. Having your codebase on GCP makes it easier for integration with Cloud Build as well for debugging on Kubernetes. On top of this, it has a decent Free Tier.
As mentioned, you need to have a git strategy in place before implementing a CI/CD pipeline. There is ton of literature outside and for this project we opted for the successful git branching model with use of the git-flow extension. Then there is the debate on how your team should collaborate (do they need to rebase, merge fast-forward, squash, etc.). This really depends on the size of your team and how they have been used to work in the past. Our approach has been:
Having your codebase on source repository and your git strategy well defined, you can now start your CI/CD pipeline to let your developers focus only coding and being able to test and deploy their applications in a matter of a few minutes. The tools used also facilitates the life of your devops team in the sense that their work moves from a mix of manual steps, debugging and hot fixes into a job of monitoring and insuring smooth working of the different tools.
This framework frees your developers from non-added value tasks and empowers your devops team with more interesting and strategic tasks.
Before deep diving into the CI and CD tools, let’s talk about the project architecture as this has been something of great added value for the customer. We have opted for a separation of the different environment into different projects with one additional project for all the shared services. This latter project is concerned with all the devops and will host the container images, the cloud build triggers, K8s manifests, the cluster with spinnaker on it, etc. This separation of projects and one dedicated project for devops permits to have very granular access policy and gives clarity to the purpose of each project. Nothing is mixed and the idea behind it is “one project, one purpose”.
Let’s now focus on the Continuous Integration part with Cloud Build. Its only objective is either
Both these actions happen once a certain push has been done on the codebase that includes respectively change in the application or in the K8s manifest files (Cloud Build will listen to these changes through different triggers). Cloud Build works with a list of steps and each of these steps define an action to be done. The main steps that are encountered are fetching the codebase, running unit tests and building the image. Basically, any actions can be performed through Cloud Build as each step is a container application on its own. Once cloud build has finished all its steps, the new image on GCR or the new K8s files on GCS will send a notification to Pub/Sub that will have Spinnaker as a subscriber.
Pub/Sub, GCP message-oriented middleware, is the communication pillar between the CI and the CD part.
2Let’s now move on to the deployment part. This is done through to the open-source software Spinnaker that is deployed on a cluster on the shared services project. Once deployed, you can easily configure the UI to be accessible on a public domain with oauth2.0 access (see tutorial). Spinnaker user-friendly UI make it very simple to create pipelines and deploy on your different GCP projects once you have passed some configuration setups. Two pipelines have been developed as follows:
Wait, where is IaaS in all this and why should I need it ?
IaaS, through Google Deployment Manager, is the practice of making the configuration of your infrastructure reproducible, scalable, and easy to review by describing it using code. Infrastructure as code comes from the realization that infrastructure is also “software”, which is particularly true for the public cloud.
Basically, it is translating any UI actions that you do in the GCP UI (create a cluster, set IAM policy, create a bucket, etc.) into code.
Once your infrastructure changes, you just change it in your code and you then update it via a gcloud command.
Various tools have been described through this document and things can get messy if we don’t have one notification tool that integrates it altogether. This is where Slack comes to the rescues thanks to static and dynamic integrations with all the tools mentioned in this document. From git integration where it informs you about pull request that you can accept dynamically to Spinnaker manual judgement, you can really develop any kind of notifications using slack apps and Google cloud functions.
Want assistance developing this kind of evolved CI/CD pipeline with your K8s application ? Our experienced developers will guide you through all the different steps and the consulting can go from a simple architecture design with tips to a full customized implementation. Feel free to reach us at niels@fourcast.io or charles.verleyen@fourcast.io. Niels and Charles are the leading developers at Fourcast on K8s and devops tech on GCP.
Visit us on www.fourcast.io
Google Cloud community articles and blogs
10 
10 claps
10 
Written by
Google Cloud Lead Engineer at Fourcast
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Lead Engineer at Fourcast
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@fastup/re-mapping-iaas-cost-structures-on-amazon-web-services-f48532b4d7ca?source=search_post---------171,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sachin Dole
Feb 4, 2018·4 min read
At all customers of FastUp, AWS EC2 and On-Prem hardware is the majority of the costs of Infrastructure. Each customer is different, so averaging such costs over a broad range of customers does not always make sense. Despite that caveat, I’d say that 80% of a customer’s AWS bill is made of EC2 instances that run a plethora of services ranging from DNS, firewall to functional web services.
At some of the more progressive customers, this is already changing in two ways. Firstly, many customers are re-architecting applications to go serverless (cloud-native) using API gateway, Lambda and DynamoDB. Depending on volumes, this is relatively cheaper to operate than a classical server/vm based model. Secondly, many customers are automating a majority of infrastructure operations at a exceedingly well. Once the initial knowledge gap is crossed, customers are able to leverage automation much faster and better. This is causing the infrastructure vendor bill (AWS bill) to shrink and at the same time, internal labor costs to shrink.
This is really good. Serverless is the way to go! In theory, any application can be moved to a serverless architecture where routine server maintenance tasks are performed by the cloud vendor and customers only write functional and infrastructure code. In practice, this is a new technology and hard to get right. It may take a few years for customers to get it right. It will surely be a few years before Java Developers, Dot Net Developers can also write deftly write Python & NodeJS to move their apps around. Many customers have written millions of lines of code which will simply take too long or will prohibit the movement to cloud native architectures due to the sheer cost of re-write. Despite this, customers will still want to move to this new cost structure where Infrastructure costs and Labor costs are lower.
This is the main business driver why there needs to be a re-think of how IaaS is adopted by customers. The question is, “how to change the cost structure of the IT organization if serverless is not an option?”. Answering this question needs a re-think of how cloud virtual machines are consumed. Answering this question needs a re-think of how AWS partners design and deliver solutions to clients. I mean, everyone in town has a “Cloud Migration Service”. I say that is a disservice at best. Cloud Migration is good for many reasons. However, the real value of a good AWS partner is derived only when the customer can cut infrastructure and labor costs drastically. This drastic cost cutting is what I refer to as IaaS 2.0.
For example, consider Architecture of applications. Since software started eating the world, designing a great software solution has been paramount to getting ahead. What if the greatest functional idea of a business did not run well because of the underlying architecture? Or worse, what if the cost of the architecture and operations ate a big chunk of the profit margin of that idea? That would be bad! The companies that get ahead in this world are the ones who are able to create great architecture and also run it at low costs. This is where AWS (and other cloud providers) and partners (such as FastUp) come into play. What if partners offered a technology through which customers can obtain the best architectures without any effort on their end? What if customers were able just summon up systems at will without having to know or manage what is in the system?
Consider a simple website that a company wants to run in order to promote their products. To run a website, the company may need a Web CMS, a couple of environments (to test and operate the site), a few VMs, databases and firewalls. In order to do this, the company may commission an architect and a business analyst just to evaluate a variety of software, run it on their systems, set up a bunch of infrastructure and then come up with an elaborate plan to operate the site. What if all that was already done and available to pick and choose on an app? Think AWS Marketplace, but, without any need for sophistication needed to install and setup. Thing AWS Codestar, but, no need to monitor and react. That is the level of automation that needs to be created. That’s what I would like to call IaaS 2.0
Over the next few weeks and months, I want to share this idea with you and hear your point of view. I think Cloud Architecture and Operations as an industry is just getting started. There are many interesting things to come.
I am a dad, husband, motorcycle and mountain enthusiast.
See all (168)
5 
5 claps
5 
I am a dad, husband, motorcycle and mountain enthusiast.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/infrastructure-as-a-service-meets-blockchain-614d1590b494?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
Infrastructure as a service (IaaS) is one of three main categories of cloud computing services (the other two are software as a service (SaaS) and platform as a service (PaaS)). Cloud computing lets organizations outsource their infrastructure needs to an external provider. The provider of IaaS houses and maintains computer infrastructure that clients can access remotely. Clients usually pay on a per-use or utility computing basis for this service.
Currently, a few big companies dominate the IaaS sector. Some of the most well-known companies providing IaaS include Amazon AWS, Microsoft Azure, and IBM.
Blockchain technology provides new ways of handling infrastructure as a service (IaaS). As with many industries, it could disrupt how big tech companies operate today.
A blockchain involves many machines working together in a decentralized network, thus, it is a shared computer system infrastructure. It is possible for a cloud computing service to take the idle capacity of CPU’s and GPU’s working in the network and make it available to clients on a per-use basis, similar to how a centralized datacenter leases out capacity in today’s cloud computing models.
A blockchain consists of a decentralized network of computers. At any given moment, some computers will be working on specific tasks, taking computing power and storage from others on the network. This arrangement can be monetized so those computers using more power than their own capacity can pay in digital tokens to others to lease their idle capacity.
This model can apply to different infrastructure components. It can leverage decentralized computing power itself. Memory and data storage could also be decentralized in a similar way.
There are already some blockchain projects operating in this space. Although none of them are currently marketing themselves as a full infrastructure as a service provider, both Golem and DeepBrainChain have aspects of IaaS in that both provide computing power for AI.
Titanium Blockchain Infrastructure Services is running IaaS on a dedicated Ethereum Blockchain that uses Raiden technology to offer a quick and safe transaction service. Storj provides a similar service for hard disk memory space. MaidSAFE plans to offer both decentralized computing power and data storage.
Blockmason’s Link is a new blockchain IaaS that would let developers use smart contracts and programmable blockchains without any blockchain experience. Using Link, programmers will be able to create classic, conventional web-based APIs for any smart contract written on a programmable blockchain.
Blockmason describes Link as a “missing component” in blockchain-based app development. There are no other cryptocurrencies needed, browser plugins, or other barriers. A developer just adds the smart contracts they want to use to Link. Once added, they can read from and write to these smart contracts just like any other API.
Link runs on the BLINK utility token. BLINK allows access to the service and API transaction capacity. BLINK is consumed when a developer uses Link to build an API for their smart contract and the API is used.
For example, Blockmason has a partnership with GoChain so developers that interact with smart contracts on GoChain-compatible blockchains can use Link to speed up development and reduce overhead. Blockmason has also partnered with MagnaChain, a public blockchain platform for game developers.
Blockchain technology is naturally-suited to IaaS use cases. Fundamentally, it is simply a shared computer system infrastructure. Various companies are monetizing this arrangement to offer different blockchain infrastructure component to clients. Because these companies aim to make it as easy as possible to use blockchain technology — even for those who have no previous experience — the growth of the blockchain IaaS sector could facilitate mainstream adoption of blockchain.
Subscribe to my Medium and Twitter channels if you like my articles and would like to know more about blockchain and cryptocurrency projects.  If you have any questions about this article, please comment in the section below. Thank you!
#BlackLivesMatter
4 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
4 claps
4 
Written by
Writer interested in blockchain projects that will add to the social good
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Writer interested in blockchain projects that will add to the social good
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@joelgsamuel/putting-the-security-in-saas-209342b51820?source=search_post---------356,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joel Samuel
Jul 3, 2018·7 min read
Michael (a freelance cybersecurity consultant) wrote about an article about ‘Security concerns with platforms and services in the cloud’ which I summarily agree with but have thoughts on.
This isn’t a rebuttal of his post but perhaps of an expansion but also my views (which aren’t a million miles away). Hat-tip for him reviewing this article before publication.
Michael runs a weekly cyber roundup newsletter (great for those — like me — who lack to always stay up to date with recent events within a reasonable period of time) — https://tinyletter.com/CyberWeekly/.
Lets go alphabetically, we’ll get onto order of preference in a wee while.
For simplicity, I will focus on the XaaSes that get a lot of attention in the cybersecurity space as opposed to Data-as-a-Service (DaaS) such as GOV.UK Registers or a UK postcode / address lookup service or Desktop-as-a-Service (DaaS) such as AWS WorkSpaces or Microsoft Azure Remote Desktop Services.
In-line I will also discuss where the shared responsibility (the delineation between your responsibilities for security, configuration et al versus the supplier’s). The responsibility lines move quite a lot so to save character count the common denominators are:
FaaS is serverless computing. FaaS allows you to develop code and have someone else runs it when you want them to — entirely mitigating the need to run a container, operating system etc.
Lambda from Amazon Web Services (AWS) is a nice example.
Shared responsibility line: you have to write the code, debug it etc and ensure it is appropriate to be run by the provider and think about requirements like temporary or persistent data stores given your application stops running and underlying resources are recycled once your application ends. The provider usually takes care of routing and secrets management for you (or provides mechanisms for you to easily consume that within your FaaS code).
SaaS is effectively renting of virtual compute resources (CPU, RAM, storage) without having to house (power, cool, network) them yourself, worry about cooling or pay the upfront capital expenditure to get the kit in the first place.
AWS EC2, Azure Virtual Machines, Alibaba Elastic Compute Service (and so on) are all prime examples.
Shared responsibility line: you have to pick the operating system, your resources (and ensure they are sufficient for your needs) and manage the entire operating system and any consequences — patching, upgrades, configurations, services you run, applications you run etc.
‘Immutable infrastructure’ is on the rise (replace not patch-in-place) but either way you have to solve that problem.
Various IaaS providers provide tooling to help with some of the commodity things such as via AWS’ EC2 Systems Manager and maintained base images.
‘PaaS’ can mean slightly difference things but in essence it is above the IaaS layer (so you generally don’t need to worry about patching etc just your application code/artefacts) but not SaaS as you’re running your own applications.
CloudFoundry is a good PaaS example as you provide it with application code and configuration, it will create the relevant application and IaaS stack and simply run your application for you — potentially managing databases, backups, HTTPS and load balancing for you.
Kubernetes is another PaaS, as it can help you with secrets management (etc) in addition to the traditional containerised compute problem.
Shared responsibility line: you have to provide suitable application code and minimal viable configuration that the corresponding PaaS platform needs to understand your application and provide the surrounding services, as applicable.
Unless you’re running the PaaS yourself: you’re only responsible for the security of the application not the security of the platform/infrastructure — the responsiblity for the Confidentiality, Integrity and Availability triad is heavily skewed towards the PaaS provider.
The SaaS model is consuming an application itself from the vendor (typically via a web browser or thin application) and you’re responsible for very little unless the SaaS is complex and lets you change a bunch of things about how you use it.
Shared responsibility line: just the common denominators — subject to the complexity of the SaaS as then you’re responsible for configuring to your requirements — that could range from interconnectivity through to security.
IaaS, PaaS and SaaS (et al) by themselves are things most people in this space understand but can often become buckets of “I suppose its this” or “it isn’t those so its probably this”.
“If it isn’t IaaS or PaaS its probably SaaS” — maybe?
SaaS and FaaS aren’t actually comparable because they are entirely different paradigms (a venn diagram would show little overlap) — the vendor does pretty much all it versus you developing your own code.
As per Michael’s article: SaaS can be large productivity suites such as Microsoft’s Office 365 or Google’s G-Suite versus smaller SaaS like MailChimp, Trello.com, Slack or TrackSeries.TV and in my opinion that is too wide of a definition to be useful but heyho.
This order of preference is based on only two things: simplicity and security.
Simplicity: in general control is great but you have to do stuff which may be cost/time/effort — we’re discussing commodity SaaS here so you should be leveraging that and not customising when you don’t need to.
Security: increased options and complexity offers room for benefit or error — you can tighten or loosen security controls based on your risk appetite but you have to pay attention and not inadvertently decrease security by misunderstanding how the options work or forgetting to set an option.
1st preference (joint): FaaS
1st preference (joint): SaaS
2nd preference: PaaS
3rd preference: IaaS
As described above, FaaS and SaaS are so very different but aren’t distant cousins when only discussing simplicity and security.
(I will likely reflect later — probably after taking some flack — that putting SaaS and FaaS next to each other was not a great idea.)
Not a line by line of Michael’s post but picking out some things that made me think.
“Security concerns don’t change very much based on whether you are using a platform, or infrastructure or just services. They change based on the maturity of the solution and the maturity of the organisation you are using.”
As highlighted above, I think the change in shared responsibility means you probably have more to do which likely means you have more security-related things to do — but security functionality or capability (and the utilisation of the same) is not the same as vendor-based supply chain risks.
I would argue the supply chain risks broadly speaking remain the same between IaaS, PaaS and SaaS/FaaS as the biggest burden is always personal/sensitive data security and in all XaaS scenarios discussed here the vendor can see your data if they really wanted to — you have to trust the vendor to some degree and this is where security shouldn’t say “no” it should say “we should take some proportional, understood and manageable risks in order to achieve overall benefit via functionality and time/cost savings”.
Security-related topics can come with a heavy dose of “It Depends” given context but you need to consider whether your ‘security requirements’ are over-egged and if you’re being special for the sake of it.
“To use an example, Amazon is a multi billion dollar company, and has a set of robust and audited security processes in place. Honest Bob’s Cloud is not, and probably does not.”
I concur.
I agree with Michael’s views here (also weighed in my 2c during his drafting) — overall, the probability of such occurrences are also very low.
While perceived law enforcement over-extension is often an important issue it is key to weigh proportional risk and probability when making XaaS decisions as encryption data is good (do it) but attempting to encrypt data to keep it away from law enforcement acting under a warrant is cause for serious reflection.
Michael’s article links out to guidance from the UK National Cyber Security Centre (NCSC) on assessing vendor SaaS security.
I was asked to review the guidance many moons ago while it was early draft and my thoughts haven’t changed much since:
I agree that questions about data-centre locations etc do not always need to be asked but this is where context comes in — if you’re worried about personal data or intellectual property because those are things are important in the thing you’re trying to do then you should ask many more questions than the SaaS security guidance suggests and are likely to be asking questions which may be considered ‘old’ — where are your data-centres, where are your staff, what levels of access do your staff have to my data etc.
One day soon I will write about “What I mean when I say cybersecurity” and cover some of the inter-related and equally valuable personas that organisations need for security in cyber space.
… but for now: thats all folks.
You might find other exciting posts in my Medium profile. I’m on Twitter as @JoelGSamuel.
The thin blue line between technology and everything else. joelgsamuel.com
3 
3 
3 
The thin blue line between technology and everything else. joelgsamuel.com
"
https://medium.com/@belougatech/managed-services-or-not-iaas-paas-saas-how-to-choose-74272a6f4876?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Monirath Ngor
Feb 24, 2020·6 min read
Not only a comparison.
Your company or your client decides to Move to Cloud and you’re in charge of a wonderful project to migrate legacy applications or to create a new environment on a CSP (Cloud Solution Provider).
Rapidly, one of the questions that will come is to choose between IaaS, PaaS, SaaS (sometimes more) for some services.
We talk here about moving from a self-managed database installed on a bare metal server to a managed CosmoDB in Azure for example. Or, you have stars in the eyes with GCP pub/sub instead of your old unmaintained Rabbitmq instance. Or else, does it worth to use Dropbox compared to a cheap S3 AWS bucket?
Starting from a blank page, here are some considerations to take into account when designing the solution.
First of all, here is a short reminder between IaaS, PaaS, and SaaS. This will help to know what are the key points to take into consideration for the choice.
IaaS (Infrastructure as a Service) is a distribution model where the provider hosts the infrastructure (servers, CPU, GPU, RAM, Storage, network) and manages it.
The customer has full control of the OS, the middlewares and the applications.
Example: Amazon EC2, Google Compute Engine (GCE), Rackspace
In PaaS (Platform as a Service), the Cloud hosts the infrastructure and provides features and tools (OS, development tools, database management system, etc.), delivered over the Internet.
The customer can focus on the application development without worrying about infrastructure, OS or middleware management, and maintenance.
Example: Amazon Elastic Beanstalk, Google AppEngine (GAE), Heroku
SaaS (Software as a Service) is a distribution model of an application through the Cloud. The provider fully manages the application and makes them available through the Internet, most of the cases with a classical web browser.
The software is fully externalized and accessible by paying a monthly or annual fee.
Example: Dropbox, Gmail, Microsoft O365
Each of the solutions has its pros and cons:
This is said, let’s move on now to the interesting part.
Here are some topics that can drive the choice. Depending on the situation, some of them may require more attention.
It’s essential to know how the team is composed. Because they will be the users and the ones to build and operate on.
Those users can be already in place, or new people may arrive in the organization. HR and managers need to be involved.
Users can be more or less technical, have more or less developer background. Users can be internal, but also external resources, especially when expertise is required (using a specific framework for example).
Moving to a managed service may have a different impact. Some codes or the environment may require to be adapted.
IaaS won’t fit for a small team unless it has strong experience in the new infrastructure. Managed services may be expensive if an infrastructure is already in place because processes can be capitalized.
Cost strategy is, of course, important to know because it can be a strong driver.
With SaaS solutions, everything comes with it. The price will be more expensive, but users won’t care about upgrades, hotfixes, incidents, administration, security, etc. So no technical staff or tools is required.
A lot of tools are provided with PaaS solution to ease users’ life, this is what you pay. They need to care only about the application.
With IaaS solution, you’ll get VMs. And the rest has to be built, administrated and operated (security, redundancy, network, backup, permissions, configuration, etc.).
Here, the calculators provided by the CSP will be your friend (you can build your own). Just check you’re in the right region, because the price may differ.
Example: GCP calculator, AWS calculator, Azure calculator
It can be used compare for example the cost of a MySQL instance installed on a VM versus Cloud SQL, which is the MySQL managed service on GCP. If the requirement is a simple database with few users and replication is not critical, going to a managed service may not be relevant.
Another case is to compare a Cloud Storage service like Dropbox with a solution you can implement on a CSP. Rolling out your solution can be more expensive at the end. The storage itself is cheap, but you need to set up the frontend, the security, the replication and you need people to maintain the solution afterward.
And don’t forget to add other fees like licenses (OS, middlewares, usage licenses, etc.) or egress traffic to your project quotation.
Analyzing the on-prem design may help to understand some constraints or justify a transformation opportunity.
You may have heard something like “it’s like that because it’s historical, it’s always been there”. 2 possibilities there. Either, there were some strong constraints and no-one was able to change it (a customized development or a dependency to a 3rd party). Or, the years have gone by and no-one cared about it.
Here, IaaS is very flexible and design can be customized to the constraints. Some adaptation may not be possible with managed services.
Read carefully all the documentation about limits, quotas, and SLAs of the products or solutions you intend to use. This will prevent one round trip for nothing.
First, get inputs about how the business or usage grows. It’s about capacity and dimensioning. And compare figures with the limits and quotas like bandwidth, storage volume, IOPS, etc.
It’s even more true if a PaaS or SaaS solution is on the map. Packaged solutions can offer a rapid move to production with less management. But have in mind, the solution is built to answer to most of the needs, and may not be adapted to your situation.
And limitations are not only about numbers. But also about designing (network hops, tenants, peering, versions, configurations, etc.).
For example, you can’t customize every parameter field in a managed-database. Some types of managed-storage can be limited in the bandwidth. Or managed-firewall is limited in the number of rules.
SLA’s story is the way around. Managed services are pretty good. With IaaS, service recovery, incident management, and so on have to be prepared.
The beauty of Cloud providers is that they are quite dynamic and propose more and more solutions.
When moving an on-prem solution to a managed service on a CSP, check carefully version compatibility. The managed services may not support the same version.
Another thing is that brand new services are sexy but often still in alpha or beta. And the GA date may not match with the delivery time of your project.
Note that it’s not recommended to go to production with a beta version of the service.
Consider the following topics when deciding to go IaaS or managed services :
I hope it will help you to weigh the pros and cons!
Engineer | Entrepreneur | Cloud
See all (9)
Engineer | Entrepreneur | Cloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cryptosense-tech/how-to-deploy-ocaml-on-heroku-9903548aafa5?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
Heroku is a PaaS cloud offering (Platform as a Service), where you pay for hosting an application. Compared to a IaaS (Infrastructure as a Service) where you pay for “just” a virtual machine, this means that you get some nice extras, such as easier deployment, system updates, etc. This post describes how we use it to deploy Cryptosense’s backend webservices written in OCaml.
Heroku originally supported only Ruby, but then added support for other languages, such as Python or Node.js. It’s actually flexible and able to run OCaml without any problem.
Let’s try it with a toy service that listens for HTTP requests and counts the number of words in it. That’s what the Cryptosense Analyzer does except it counts problematic usages of cryptography instead of words.
The following OCaml program uses the cohttp library to perform this task:
The interface between this code and Heroku is fairly minimal:
We can try it locally:
Let’s create an Heroku application for this service. If no name is passed, a random one is generated.
By default, Heroku will try to detect what kind of project this is. For example, if there is a requirements.txt file, it is a Python project if a Gemfile is present, it is a Ruby project, etc. It then invokes the correct “buildpack”, a script that installs the dependencies and can take extra steps, like preprocess some code or precompile assets.
It is not possible to set no buildpack at all, so we will use the “null buildpack”, a buildpack that does nothing. It just receives an executable file and runs it. Heroku is just Linux on a 64-bit system, so we can generate a binary and deploy it.
Now we just have to send the application to Heroku. There are several ways to do that. The simplest one is to push to a special git remote that will compile and deploy the code. But since we’re sending a binary, it is not a good fit (it would require to check in the binary in the repository). Instead; we will use the heroku-builds plugin:
Once the deployment has been done, we can see it start in the logs:
And here it is, serving requests in the cloud using OCaml:
That’s it, we wrote a small server and deployed it to Heroku. There are some limitations on this approach. For example, it is impossible to depend on native libraries. A way to fix that could be to write a OCaml buildpack that understands opam dependencies and builds the binary on Heroku, installing required dependencies.
Interested in webservices and functional programming? Join us and let’s fix the world’s broken cryptography together.
Cryptosense tech blog
32 
32 claps
32 
Cryptosense tech blog
Written by
Senior Engineer at Cryptosense
Cryptosense tech blog
"
https://medium.com/appscale/platform-as-a-service-versus-infrastructure-as-a-service-paas-vs-iaas-d63ce6830c6?source=search_post---------232,"There are currently no responses for this story.
Be the first to respond.
Cloud computing comes in many flavors of providing compute power to users.
A person can sign up for a service, provide a credit card, and have access to practically unlimited computational resources. In this post we discuss the difference between what makes a platform-as-a-service (PaaS) compared to infrastructure-as-a-service (IaaS).
IaaS allows for a user to get a virtual machine on demand. The virtual machine can be a bare bones system running an operating system, or one with a preloaded software stack (LAMP stack). The user is in charge of managing the resources on that machine. Memory and CPU usage are left to the user to administer. The most popular public IaaS provider is Amazon, with EC2 (Elastic Compute Cloud). Other competitors include Google Compute Engine, RackSpace, DigitalOcean, Azure, and Linode.
PaaS takes the abstraction higher. A platform hosts an application without the user having to worry about CPU or memory usage, or even the number of machines needed to satisfy their user base. PaaS users can worry just about their application and let the service worry about the rest. This is great for businesses who want to focus on their core competence and just work on their business application. The most popular public PaaS is Google App Engine, where Google’s staff worries about the infrastructure, security, and scalability of your application. Other PaaS offerings include Heroku and EngineYard.
There is an inflection point in costs, where having your application or service hosted on a public provider becomes more costly than to just bring it in-house. For those businesses they have open source options to run the same sort of public services (IaaS and PaaS) on self hosted resources. For EC2 compatibility there is Eucalyptus, and for Google App Engine, there is AppScale. A nice feature about AppScale is that it too can run on public clouds as well as private clouds, giving your applications the ultimate portability.
You can get started on a private deployment of AppScale on your laptop right now with 5 minutes of setup time.
www.appscale.com
Originally published at blog.appscale.com on November 5, 2013.
Blog posts on AppScale, serverless platforms, and beyond…
Blog posts on AppScale, serverless platforms, and beyond…
Written by
AWS-compatible hybrid cloud software that can be deployed in many environments (on-premises, in a co-lo, at a service provider, at a BMaaS provider).
Blog posts on AppScale, serverless platforms, and beyond…
"
https://medium.com/@eslamhelmy523/iaas-paas-saas-78a78dc74586?source=search_post---------259,"Sign in
There are currently no responses for this story.
Be the first to respond.
eslam helmy
Apr 15, 2021·2 min read
Cloud services are categorized in these three types, we are going to discuss the differences between them.
Cloud provides us with the infrastructure such as compute, storage and network as a service.
you have only the underlying platform and it’s your role to manage it.
Virtual machines
the cloud provides the host machine, networking and disks.
Then the client use this Infrastructure to create a virtual machine, install its software and maintain it. So it is the responsibility of the client to make sure the machine is up and running.
Cloud provides us with the platform, in a different way we have the infrastructure services and the runtime environment, so we don’t care about running or maintaining the machine that has our Application, it’s cloud responsibility now.
The client responsibility is just to bring the code to run.
The client cannot access the underlying machine.
Web Apps
The cloud provides the runtime for running web apps.
The client just upload the code to run.
Cloud provides us with a running software,and client only uses it without any access to its runtime or machine itself.
Clients have no idea what is the infrastructure or the runtime used by these services.
Office365
The cloud provides us with different products.
The client just uses it.
Finally to summarize the three types of cloud services, let’s look at this image.
On-Premise, Client have to handle everything and this was the traditional way before the cloud.
In IaaS, Client controls everything starting from creating our virtual machine, we have the underlying platform to help us create VM.
In PaaS, Cloud controls hosting machine and runtime. Client need to just upload the code.
In SaaS, Cloud controls everything and client just use the service.
References
https://www.bmc.com/blogs/saas-vs-paas-vs-iaas-whats-the-difference-and-how-to-choose/
"
https://medium.com/@conanmoon/%EC%83%88%EB%A1%9C%EC%9A%B4-%EB%B0%B0%EC%9B%80-iaas-paas-saas-a9a02dc52974?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
배우는 자(Learner Of Life)
Apr 29, 2021·11 min read
이제는 인프라, 플랫폼, 소프트웨어도 서비스 받을 수 있는 시대
# 인프라, #플랫폼, #서비스
오늘 한일:
이번 글 부터는 더 이상 “데이터과학 유망주의 매일 글쓰기""는 없을 것이다. 나는 데이터 사이언스 과정을 나왔고, 이제는 Technical Writer로의 길을 가기로 결정했기 때문이다. Technical Writer 역시 개발자나 데이터 사이언티스트 만큼은 아니지만, 새로운 기술을 빠르게 이해하고 그것을 다른 사람에게 쉽게 설명할 수 있어야하는 만큼, IT분야에서 계속 변하는 기술의 변화를 따라잡을 수 있어야한다.
IaaS(Infrastructure as a Service), PaaS(Platform as a Service), SaaS(Software as a Service)는 최근 IT 분야에서 크게 화두가 되고 있다. 서버와 클라이언트로 구성되는 인프라를 직접 구축해야했고, 컴퓨터와 운영체제로 대표되는 플랫폼을 모두 갖추어야 했으며, 필요한 소프트웨어는 직접 깔아서 설치 및 유지보수해야 하는 시대는 지났다. 이 모든 것을 간편하게 제공하는 Amazon, Google, Microsoft같은 회사들이 생겨났기 때문이다.
그렇다면 인프라, 플랫폼, 소프트웨어를 서비스한다는 것은 정확히 어떤 의미일까? 오늘 블로그에서는 이 주제에 대해 다루어 보고자한다.
보통 IT에서 인프라라함은, 단순히 컴퓨터 하드웨어 및 소프트웨어를 넘어서서, 컴퓨터간 인터넷으로 연결되어, 특정한 서버에서 데이터를 처리해주고 클라이언트에 전달하는 네트워크를 말한다. IaaS는 말 그대로 이러한 인프라를 제공해주는 서비스라고 할 수 있다. 가장 넓은 범위의 서비스이며, 클라우드 기반 서비스로써, 저장(storage), 네트워킹(Networking), 가상화(Virtualization)등 필요한 만큼 지불하는 “Pay-as-you-go” 서비스를 말한다.
이 서비스의 장점은 유저에게 사내 인프라(On-Premise Infrastructure)에 대한 클라우드 기반 대안을 제시하며, 비싼 현장 리소스에 대한 비용을 지불하는 것을 막을 수 있다는 것이다. 알다시피, 사내인프라를 유지 보수하는 것은 시간적, 물리적으로 매우 많은 비용을 소모할 수 있다. 특히, 초기에 하드웨를 셋팅하는 과정에서 많은 비용이 들어갈 수 있는데, 이때 외부 IT IaaS 업체를 활용하면, 하드웨어를 잘 유지하면서 모든 것을 업데이트된 상태로 다룰 수 있다.
이 서비스는 필요한 만큼 구매하고, 추가적인 필요가 발생하면 추후에 더 구매할 수 있다. 즉, 인프라를 관리하는데 있어 확장성과 유연성이 높다. 또한, 다수의 유저들이 쉽게 접근할 수 있으며, 비용을 절감할 수 있다. 다루는 사람이 IaaS를 잘 몰라도, 플랫폼을 쉽게 접근하고 모니터링할 수 있도록 되어있다. 모든 규모 및 형태의 비즈니스에서 활용할 수있으며, 전체 인프라를 쉽게 관리할 수 있도록 해주고, 필요한 만큼 구매할 수 있기 때문에 비용면에서도 효율적이다.
IT업계에서는 직접 하드웨어나 인프라에 투자해야할 필요가 없어지는 추세이기 때문에, IaaS는 매우 안전하고 신뢰성 높은 대안이라고 할 수 있다. 또한, IaaS를 제공하는 업체는 대부분 실시간으로 빠른 고객 지원 서비스를 제공하고, 필요한 만큼 확장할 수 있게 해준다. 대부분의 IaaS 제공 회사들은 어플리케이션, 데이터, 런타임, 미들웨어, 운영체제 등을 직접관리해 준다. 대표적인 IaaS에는 AWS EC2, Google Compute Engine(GCE) 등이 있다. 특히, AWS EC2는 클라우드 기반 어플리케이션을 호스팅하려하는 회사들에게 매우 확장성이 높고 경제적인 솔루션을 제공한다.
일반적으로 플랫폼이라 하면 소프트웨어를 돌릴 수 있는 운영체제(O/S)와 이를 실행할 수 있는 물리적인 하드웨어(컴퓨터)를 포함해 지칭한다. PaaS는 가상화(Virtualization) 기법을 통해 이러한 소프트웨어와 하드웨어를 인터넷 기반으로 제공하는 서비스다. IaaS와는 다르게 어플리케이션 및 데이터를 제외한 부분만 관리해준다. PaaS 공급사들은 인터넷기반 하드웨어 및 소프트웨어 툴을 제공하며, 이용자는 이를 활용하여 원하는 어플리케이션을 개발할 수 있다. 주로 개발자들이 활용하며, 인터넷을 통해 서비스된다.
이 서비스의 장점은 개발자들에게 유니크하며 커스터마이징이 용이한 소프트웨어를 만들 수 있는 플랫폼을 제공한다는 것이다. 개발자들은 어플리케이션 개발을 위해 맨땅에 헤딩을 해야할 필요가 없으며, 시간과 돈을 매우 아낄 수 있다. PaaS는 어플리케이션의 모든 기본적 기능들을 처음부터 만들지 않고도, 원하는 어플리케이션을 개발할 수 있는 모든 툴을 제공한다. 마치 예를 들자면, 현실에서 Venue를 활용해 이벤트를 기회과는 것과 Venue를 만들어 이벤트를 기획하는 차이이며, 웹 개발에서는 Django/Flask등으로 웹 프레임워크를 활용하여 웹 어프리케이션을 만드는 것과 처음부터 프레임워크를 만들어 시작하는 것의 차이라고 볼 수 있다.
또한, 다수의 유저가 접근 가능하며 확장성이 높아, 비즈니스 규모에 적절한 리소스를 제공하는 회사를 고를 수 있다. 가상화 기반 서비스이며, 시스템 관리 지식이 없어도 쉽게 실행을 할 수 있다. 개발자들에게는 유니크한 어플리케이션을 만들 수 있는 비용 및 시간면에서 가장 효율적인 대안이라 할 수 있다. 무엇보다도, 개발자는 소프트웨어 업데이트 또는 보안 패치 관리 등의 잡무를 피할 수 있으며, 앱 개발에서 중요한 부분에만 집중할 수 있다. 즉, 개발자가 앱을 생성하고, 테스트하고, Deploy하는데 집중할 수 있게 해준다는 것이다.
대표적으로는 AWS Elastic Beanstalk, Heroku, Windows Azure 등의 서비스가 있다. 예를 들어, AWS Elastic Beanstalk은 AWS가 제공하는 EC2, RDS, S3등 약 100여 개의 클라우드 컴퓨팅 서비스 중 하나다. 대부분의 서비스는 IaaS로 활용될 수 있으나, 대부분의 고객은 자신이 원하는 서비스만 골라 구입한다. 하지만, 여러개의 다른 서비스를 관리하는 것은 굉장히 복잡하고 어려워지기 쉬운 작업이 될 수 있다. 이를 위해 AWS Elastic Beanstalk이 IaaS의 맨 위에서 하나의 레이어로 동작할 수 있으며, load balancing, scaling, 어플리케이션 모니터링을 자동으로 수행할 수 있다. 개발자는 자신의 어플리케이션을 업로드하고 유지하는 것만 신경쓰면된다.
소프트웨어는 Windows, MacOS 같은 운영체제(O/S) 개념으로써, MS Office, Adobe Photoshop등의 어플리케이션을 돌릴 수 있는 환경을 말한다. 바로 어플리케이션을 돌릴 수 있는 소프트웨어 환경을 제공받을 수 있는 서비스라 할 수 있다. 역시 클라우드 기반으로 인터넷 상에서 제공되는 서비스이며, 주로 월 정액제로 판매된다.
직접 자신의 컴퓨터에 소프트웨어 어플리케이션을 설치하여 실행할 필요가 없고, 인터넷 상에서 로그인만하면 사용이 가능하다는 장점이 있다. 또한, 그 어떤 디바이스에서도 언제 어디서든 인터넷만 있다면 소프트웨어에 접근할 수 있다.
모든 사람이 자신의 권한에 따른 접근을 로그인을 통해 할 수 있으며, 소프트웨어의 설치 및 업데이트는 클라우드 상에서 이루어지기 때문에 걱정할 필요가 없다. 주로 월 정액제로 운영되기 때문에 확실하게 비용을 알 수 있으며, 이에 따라 예산내에서 어느 정도를 소비할지 계획할 수 있다. 대부분 구입한 플랜에서 소프트웨어의 유지보수 및 보안 관리 등이 포함되어 있기 때문에 사내 소프트웨어를 유지 및 보수하면서 발생할 수 있는 시간적, 비용적 소모를 줄일 수 있다. 고객의 수요에 따라 여러 다른 복잡도의 소프트웨어 서비스 플랜을 제공받을 수 있으며, 기반 소프트웨어를 빠르게 설치하고 실행할 수 있다. SaaS 제공 회사는 대부분 실시간으로 빠른 고객 지원 서비스를 제공한다. 즉, 소프트웨어의 유지보수 및 보안 관리 등을 걱정하지 않아도 된다. 모든 것이 서비스 회사를 통해 관리되기 때문에, 소프트웨어에 대한 여러가지 관리 문제를 신경쓰고 싶지 않고, 오직 잘 동작하게 끔 하는 것 만이 목적이라면 가장 좋은 솔루션이 될 수 있다.
SaaS의 개념을 좀 더 쉽게 설명하자면 이메일 서버를 예로 들 수 있다. 만약, 이메일 어플리케이션의 업데이트와 복잡한 설정을 직접한다면, 이는 굉장히 성가신 작업이 될 수 있다. 혹여나, 이메일 서버가 업데이트 안되어서 동작하지 않는다면 정말 끔찍한 일이 될 것이다. SaaS 서비스를 활용하면 이러한 문제를 걱정하지 않고 이메일 서비스를 활용할 수 있으며, 이메일 서버가 다운 될 가능성을 크게 낮춘다. 문제가 생기면, SaaS 제공자에게 바로 문의하면된다. 단순히 서비스에 대한 비용을 지불하는 것을 넘어서, 불필요한 작업 수행을 걱정할 필요없게 끔 심리적인 평안을 가져다 주는 솔루션이라고 볼 수 있다. SaaS의 대표적인 예로는 SalesForce, Dropbox, Slack 등이 있다.
앞으로 할일:
오늘은 IaaS/PaaS/SaaS에 대해 정리해 보았다. 인프라, 플랫폼, 소프트웨어 환경을 걱정하지 않아도 된다니 정말 편리한 시대에 살고 있다는 것을 실감하게 되었다. 결론적으로 모두 사내에서 자체적으로 운영하는(On-Premise) 그 어떤 것보다, 필요에 따라 훨씬 효율적인 솔루션이 될 수 있다는 것을 배웠다.
대부분의 비즈니스는 SaaS와 IaaS를 혼용하는 클라우드 컴퓨팅 서비스 모델을 활용하며, 개발자로 하여금 PaaS를 활용해 어플리케이션을 만들도록 권유한다고 한다. 경우에 따라서 컴퓨팅 서버의 모델이 복잡해 질 수는 있어도, 전체적으로 IaaS, PaaS, SaaS 모드 사내 호스팅 서비스가 제공할 수 없는 유연성 및 편의를 제공한다. 시스템 관리의 복잡도는 사내 > IaaS > PaaS > SaaS 순으로, 사내에서 관리하는 것이 그 어떤 형태의 서비스와 비교해보아도 더 어렵다고 한다. 이렇게 보니 규모 좀 되는 회사라면 셋 중 하나를 쓰지 않을 이유가 없어 보인다.
정리하면, IaaS는 커스텀 개발 어플리케이션을 호스팅하는데 있어 최대의 유연성을 제공하며 데이터 저장을 할 수 있도록 데이터 센터를 활용할 수 있게한다. PaaS는 IaaS의 맨 위에 만들어져, 시스템 관리의 필요성을 줄인다. 인프라 관리보다도 어플리케이션을 개발하는데 집중할 수 있게한다. SaaS는 특정한 비즈니스 필요를 충족하는 소프트웨어 서비스(웹사이트 및 이메일 등)를 빠르게 제공한다. 대부분의 SaaS 플랫폼은 IaaS 및 PaaS 플랫폼 위에 만들어진다. 회사의 필요에 따라 한 가지 클라우드 컴퓨팅 서비스 모델 만을 사용하거나, 두 가지 이상을 혼합하여 활용할 수 있다.
이제 나는 “데이터 과학 유망주”가 아닌 “배우는 자""라는 이름으로 새롭게 콘텐츠를 만들어 나갈 계획이다. 언제까지나 한 분야의 유망주로 남을 수는 없다. 한 때 데이터 과학 유망주였지만, 그 때의 배움에 대한 자세와 태도는 잃지 않았다. 내가 얼마를 배우든 부족할 것이다. 마치 알면 알수록 더 많은 것을 모르는 양자역학처럼, 나는 배우면 배울수록 내가 더 부족하다는 사실을 더 크게 느낄 것이다. 하지만, 그것은 오히려 더 넓은 세상을 보고 싶어하는 나의 욕구를 자극해 더 열정적으로 즐겁게 배울 수 있게한다.
앞으로도 내가 배우는 것들을 계속 기록해 나갈 것이다. 지금까지 약 200개가 넘는 글을 써 왔으니, 다음은 500개가 목표다. 물론, 단순히 양 뿐만 아니라, 글의 질도 높이면서 써나갈 것이다. 어쨌든 글쓰기는 나의 가장 좋은 학습 도구다. 내가 배운 것을 나만의 언어로 정리하면, 다른 사람에게도 설명하기 쉬워진다. 또한, 내가 나중에 스스로 복습할 수 있는 좋은 자료가 되며, 다른이들에게 정보를 전달하는 훌륭한 선생님이 된다. 글쓰기는 또한 나의 가장 큰 취미다. 글을 통해 나의 생각과 감정을 표현하면, 그것은 내 삶의 기록이며 역사가된다. 내가 써온 것들을 가끔씩 돌아보며 좀 더 나은 표현이 떠오르면 수정하기도 하고, “그때 그랬었지”하며 추억을 회상하면 나도 모르게 미소가 지어진다. 거기에서 얻는 소소하지만 확실한 즐거움은 매우 중독성이 강하다. 내 글쓰기의 흔적을 따라가다보면, 내가 어떤 것을 배우고 어떻게 성장해 왔는지 알 수 있다. 거기에서 내가 잊어버린 것을 다시 복습하거나, 전에는 발견하지 못한 새로운 것을 배우기도 한다. 그동안 내가 무엇을 배웠는지 되돌아불 수 있다는 것은, 글쓰기를 통해 얻을 수 있는 가장 큰 선물 중 하나다.
내 배움의 역사를 앞으로도 꾸준히 기록해 나갈 것이다. 나는 죽을 때까지 글쓰는 “배우는 자""로 남을 테니까.
참조:
(1) https://www.bigcommerce.com/blog/saas-vs-paas-vs-iaas/#executive-summary-summing-up-saas-vs-paas-vs-iaas
새로운 것을 배우길 좋아하는 사람
See all (69)
새로운 것을 배우길 좋아하는 사람
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/aws-and-microsoft-get-some-company-in-gartners-iaas-magic-quadrant-49f396e6b18e?source=search_post---------9,"This is a reprint (more or less) of the ARCHITECHT newsletter from May 24, 2018. Sign up here to get new issues delivered to your inbox.
Gartner recently released its latest Magic Quadrant for IaaS and — surprise! — Amazon Web Services is in the top, right corner, followed closely by Microsoft Azure. And while there could be some reasons to quibble about their exact placement, that feels about right.
But the really interesting part is seeing Google Cloud make its entry into the vaunted “Leaders” category along with those two peers. Say what you will about Gartner’s methods (and people have said plenty), but the firm really does have cachet among enterprise buyers and enterprise IT vendors. If, as Gartner claims, Google has been able to work its way into that category based largely on the strength of its data-analysis and artificial intelligence tools (a claim that seems perfectly reasonable based on all I’ve seen and heard), that’s saying something.
What it says to me is that as (or if) GCP’s partner ecosystem and enterprise pricing models mature, Google could be in position to be taken very seriously for large-enterprise deals. The big if is how intensely Google decides to go after the enterprise market in the traditional manner versus going after the market in a Google-y (or even early AWS-y) manner.
Google’s creation of and leadership in the Kubernetes space is a definite strength among developers, as is its general willingness to embrace and develop other open source technologies in the software-development arena — including projects originally built by Netflix to run on AWS. (By the way, Netflix just open sourced Zuul 2, which is its gateway for routing internet traffic across its cloud infrastructure.) There’s a longer path to the top, wherein Google attracts developers interested in things like open source, portability and composability, and it eats away at AWS’s market share from the bottom up — much like AWS did to everything else a decade ago.
It’s a tall order considering the pace of innovation at AWS, but I’d also argue that the parallels to AWS’s ascent in the face of incumbent technologies aren’t that hard to spot if you look for them.
Interestingly, the rest of the Magic Quadrant only includes three other IaaS providers — Alibaba Cloud, Oracle and IBM — all hanging out in the “Niche Players” section. I find it odd that Oracle is ranked above IBM, and I’ve been writing for a while now not to overlook Alibaba in the international market. If what Gartner means by “niche” is that Alibaba serves markets not in North America, that’s a pretty big and nice niche to have.
On a related note, Facebook made some announcements around its network infrastructure this week:
There are perpetual rumblings about how Facebook could/should get into the cloud or software provider spaces (including from me), and stuff like this is the reason why. There are so few companies with Facebook’s scale and knowledge, and the rest seem to be very successfully supporting their core businesses with cloud services. Facebook might have some steep hurdles to overcome (namely its reputation for abusing user trust, but also the very specific use cases for which it’s IT efforts are optimized), but it’s always fun to speculate even if we all know, in our heart of hearts, that it’s not going to happen.
MongoDB Atlas is a fully-managed, database as a service that runs on AWS, Azure and GCP. Trusted by thousands of customers — from the most bleeding edge startups to some of the world’s largest enterprises — MongoDB Atlas is globally distributed, self-healing, and fully elastic. Atlas leaves the database management to the experts and lets users make changes on the fly, painlessly upgrade their clusters, easily back up and restore data, and access the latest MongoDB features.
Visit MongoDB to get started with a free cluster running on MongoDB Atlas today.
In this episode of the ARCHITECHT Show, Timescale founders (and TimescaleDB creators) Ajay Kulkarni and Mike Freedman discuss their company, which began life as an internet of things platform, and the popular time-series database that serves as its foundation. And while time-series data certainly has a place in IoT, Kulkarni and Freedman explain how developers and companies across many industries are using TimescaleDB for everything from monitoring to security. Other topics include data privacy, GDPR and succeeding commercially with an open source database.
Here is Intel’s official blog post on the new Nervana processors. When they finally arrive, we’ll see what chance Intel has to expand its data center dominance into the AI world.
theregister.co.uk
It doesn’t use LIDAR and claims it doesn’t need millions of training miles on the road. People are skeptical.
arstechnica.com
Good luck.
techcrunch.com
It’s supposed to make Cortana sound more realistic, and folks are sounding the alarms because this comes on the heels of Google Duplex.
fortune.com
architecht.io
There has been a lot of change in the AI department at Baidu recently. FWIW, Baidu has long noted that ads were an early and profitable application of deep learning within the company.
techcrunch.com
To be clear, though: research labs do not necessarily equate to economic boons, especially if the company is based somewhere else. Samsung is also opening a lab in Toronto. The clear message here is that it knows it needs to step up its game, or Google, Amazon and Apple will eat its lunch in the smarthome space.
thenextweb.com
Alibaba being the company building it, alongside SenseTime, a very well-funded AI startup (if you can call it that).
scmp.com
They’re talking about pattern discovery, not recognition. Unless I’m way off base, this seems similar in nature to what companies like Ayasdi promise. So it’s a valuable technology, but perhaps not revolutionary.
geekwire.com
architecht.io
Another investment in deep learning for drug development. This is absolutely a killer application, especially if it can help drive costs down.
venturebeat.com
As competition heats up in the chip space, I think we’ll start seeing Nvidia invest more in research to seed adoption of its products.
nvidia.com
Anyone know the answer to this situation, where folks are upset that police departments and other agencies are using cloud AI tools to build facial recognition systems? It’s an interesting intersection of capitalism and constitutionality.
fortune.com
Probably a more important issue than people realize. There are so many nefarious uses of this technology that it pays to be prepared to spot them.
technologyreview.com
From Pinterest, which knows a lot about recommendations.
acolyer.org
architecht.io
An interesting approach from an accomplished group of researchers. Basically, they’re thinking about the things that might get left out of simulated environments (e.g., an ambulance in a driving simulation) that could cause bad results.
arxiv.org
This is one of those papers that makes me question how widely we’ll actually see AI adopted at any significant scale outside of huge companies. Unless, of course, we can drive down performance and data requirements.
arxiv.org
Yes, please. Obviously, I understand the benefits of the ad-driven business model, but there are different levels of obtrusiveness/danger/performance, and many sites have gotten out of hand.
arxiv.org
Google wants reinforcement learning systems to reason better without requiring massive compute power.
arxiv.org
Interesting note: One of the authors is Pat Hanrahan, a founder of Tableau. Also, this the second group from Stanford I’ve seen tackle this issue lately, with the other being Matei Zaharia’s DAWN group.
arxiv.org
Replicated gives SaaS and software vendors a cloud-native platform for easily and securely deploying their applications inside customers’ data centers or VPC environments. Replicated provides tooling for automatic updates, license management, support, audit logs, LDAP integration and more. Sign up for a free trial here.
This is very smart and, dare I say, a look into how a lot of companies would probably prefer to manage resources at some point.
googleblog.com
$34 million is a lot of cash, and this post also reads like a press release. The latter point aside, abstracting the complexity of CI/CD and porting web apps to the cloud still seems like a good opportunity.
techcrunch.com
If you don’t recall, this is based on work from Intel and is supported by the OpenStack Foundation, but is not an OpenStack project. There’s something here in terms of the security of a micro-VM inside a container, but the speed of Kubernetes development suggests that project will tackle the issue in due time and Kata will fight for relevance.
theregister.co.uk
Mostly around blockchain, IoT, edge computing and other buzzwords. But truth be told, IBM and all companies with large European businesses would be wise to hire young, creative employees there to thread and innovate its changing privacy landscape.
reuters.com
architecht.io
Packet is making some interesting partnerships as it tries to grow its bare-metal IaaS business. There’s promise here as a niche business, but I predict it becomes the bare metal/hybrid arm of a larger provider.
techcrunch.com
This is a good analysis of the pros and cons of Red Hat getting into the database business at all. It would be a bit of an odd fit, but it could definitely help create new revenue and possibly add another piece to its OpenShift play.
redmonk.com
Speaking of Red Hat … This is fun, but are we really still beefing over OpenStack?
zdnet.com
architecht.io
This is a vague way of saying that Confluent’s cloud service is now available on Google Cloud, where it can more easily integrate with BigQuery, TensorFlow, etc. And that there’s a new, more-managed version called Confluent Cloud Professional (which doesn’t exactly say “developer” …).
confluent.io
A rundown of Cloudera’s latest batch of announcements aimed at transitioning its business into the cloud, including its Altus service on Azure. That’s a tough row to hoe, but a necessary one.
venturebeat.com
This is a billion-dollar opportunity that, from what I can tell, no one has cracked open yet. Still lots of silos and minimal intelligence. Maybe (but probably not) Okera will be the one.
datanami.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
93 
93 claps
93 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@narenltk/automating-iaas-using-terraform-open-stack-cloud-in-linux-5a490e76bf6e?source=search_post---------152,"Sign in
There are currently no responses for this story.
Be the first to respond.
Narendiran Krishnan
Feb 25, 2020·6 min read
Terraform is an Open Source Infrastructure as a software tool which is designed by HashiCorp. With Terraform we can easily build, change as well as version our infrastructure safely and efficiently. We can use Terraform in any cloud service provider.
It’s easy to deploy Infrastructure as code quickly and develop quickly.
OpenStack another free as well as open standard cloud computing platform which is widely deployed as an Infrastructure-as-a-Service in both Public and Private Clouds. Users can easily manage access through a web-based dashboard, Command Line Tools. It receives a lot of development support from many Tech Giants.
I do have access to a Private Cloud. I will show you how to automate Infrastructure using Terraform.
Here we will be creating our first Terraform Script., i.e. we are going to automate our Infrastructure. For this you need to have some basic knowledge about cloud computing and the rest are all scripting based on your requirements.
Which looks like this
You can download Terraform from the link given below and you need follow the guide to install Terraform on your specific system. Here in this example I am going to execute a Terraform script from a Virtual Machine (VM) i.e. we will execute a script from a cloud environment.
So I suggest you guys download a Linux Based Version either 32-bit or 64-bit version.
Terraform Download Link
Terraform Guide
Once you have continued installation of the Terraform in your VM, follow the remaining steps.
There are only 4 main commands that we should remember in Terraform, which are as follows,
All the Terraform files should be saved as “ .tf ”.
A Command used here is “ vi test.tf ”, then you will save the file with “:wq“.
You should also have the following output as displayed,
Now we are going to write the script which is needed for automation.
www.terraform.io
The following is the code, i.e. what the “ test.tf ” file contains,
Let me explain what I have done here and will also show you where you need to access the files in order to get the details.
First part of the code → provider “openstack”
tenant_name → this is also the same as the username, i.e. in which user / project you are going to spin your new instance it will be given here.
In-order to get other details of auth_url and tenant_id and second part of the code we need to access the following in our dashboard to gather this information.
Under the Project Tab → Access & Security → Download Openstack RC file v2.0
Once you download that file it will be a “ .sh “ file where you need to open it with Notepad and gather the information for the following and you can also download v3 file too, just focus on the details which we need for automation alone,
This confidential information can be accessed from the “ .sh “ file which we’ve downloaded.
Now second part of script → resource “openstack_compute_instance_v2” “my_instance”
To get the information of image_id, flavour_id and key_pair we need to access the following to get the information.
Under the Project Tab / Admin → Instance → select any instance name (you will find the details shown in fig)
You can get the details of the following,
Once all the scripting is done, now we need to run the 4 main commands of Terraform as shown in the table above.
First → terraform init
Once we execute that some of you may get an error if you didn’t exactly follow the above mentioned steps,
This is nothing to worry, it is just a syntax error if you have not typed the syntax properly.
Note:- you need to stay in the same folder where you have written your terraform script.
Once you’re done that you may now execute the terraform init command once again and it will display you the following output,
Now the appropriate plugins has been downloaded we can proceed to the next command,
Second → terraform plan
Now the final step, we need to create instance, execute the command
Third → terraform apply
Note → Here it will prompt you to type “ YES “ to proceed, just type “YES“
Once you type “ YES ” the instance will be created, as shown below
In-order to verify it open your Open stack dashboard and you can verify, as shown below,
Yuppieeeeeee……..!!!! We have created our own Terraform based infrastructure automation in Open stack.
Well the final command if you wish or you’re asked to destroy the instance which you have created you may give terraform destroy as shown below,
This will destroy what you have created finally.
Reference
If you wish to stay connected,
www.linkedin.com
you can just google “ narenltk / narendiran krishnan ” or just drop a mail to → narenltk@gmail.com → Happy to help..!!!
AI blogger. Inspiring & working towards a better future through technology & Artificial Intelligence. Join me in the quest ..!!
See all (27)
10 
10 claps
10 
AI blogger. Inspiring & working towards a better future through technology & Artificial Intelligence. Join me in the quest ..!!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/azure-na-pr%C3%A1tica-gratuito-5-infraestrutura-na-nuvem-saiba-como-foi-conte%C3%BAdos-gratuitos-633f123df5ec?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
Neste último sábado (12/09/2020) o Azure na Prática promoveu seu quinto minicurso online e gratuito, focando desta vez em Infraestrutura na Nuvem. Além de uma introdução englobando conceitos como IaaS (Infrastructure as a Service), PaaS (Platform as a Service) e Orientação a Eventos, ao longo deste treinamento foi coberto também o uso de serviços como Azure Storage, Azure Virtual Network, Máquinas Virtuais no Azure (Windows e Linux), Azure Bastion e Azure Resource Manager.
Caso você queira ter acesso ao conteúdo dos 4 primeiros minicursos (promovidos em Maio, Junho, Julho e Agosto/2020, respectivamente) de forma gratuita ou, até mesmo, deseje revê-los, acesse os links a seguir:
Azure na Prática Gratuito #1 — Desenvolvimento Web: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #2 — Docker: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #3 — Azure DevOps: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #4 — Azure Functions: saiba como foi + conteúdos gratuitos
Fui um dos organizadores desta iniciativa, juntamente com meus amigos Milton Câmara (Microsoft MVP, MTAC) e Diego Matos (Microsoft MVP). O Diego Matos (Microsoft MVP) foi desta vez o instrutor, apresentando um conteúdo de altíssimo nível e respondendo pacientemente a dezenas de dúvidas ao longo do evento. O resultado geral desta iniciativa superou mais uma vez nossas expectativas , com 3015 inscrições via Sympla:
Tendo um pico de 630 pessoas nos assistindo ao longo da live no YouTube, com mais de 3,5 mil visualizações até o momento da publicação deste post:
A gravação já está disponível no canal Azure na Prática no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar você que está lendo esse post para que se inscreva no mesmo):
Os slides utilizados pelo Diego foram disponibilizados no SlideShare:
Aproveitamos para agradecer:
Recebemos inúmeros agradecimentos em redes sociais (Facebook, LinkedIn) por este esforço, algo que sempre nos motiva para seguir em frente com esse tipo de iniciativa! Eis alguns prints que atestam todo este apoio:
Nas próximas seções estão avisos incluindo conteúdos gratuitos sobre diversos serviços do Microsoft Azure, eventos online gratuitos nos próximos dias cobrindo esta plataforma e descontos para os próximos cursos pagos do Azure na Prática.
No blog Azure na Prática temos postagens semanais, cobrindo o uso de diversos serviços do Microsoft Azure. Deixamos o convite para que você se inscreva aqui, recebendo assim notificações de nossos conteúdos gratuitos:
medium.com
O blog do Diego (OnTheCloud) é uma excelente fonte para conteúdos voltados a Infraestrutura e DevOps no Azure:
Blog voltado a Tecnologias de Nuvem, DevOps e tecnologias Microsoft em geral.onthecloud.com.br
Também venho produzido conteúdos sobre Azure para o meu blog pessoal:
medium.com
O Diego Matos também possui um canal no YouTube:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que serviços do Microsoft Azure são abordados com frequência:
www.youtube.com
www.youtube.com
Uma iniciativa promovida anualmente pelo Canal .NET é o Azure Tech Nights, evento online e gratuito cobrindo diferentes tecnologias que integram a nuvem Microsoft. A edição 2020 aconteceu recentemente (Fevereiro a Abril), com os links da gravação de cada palestra podendo ser encontrados no seguinte post:
Azure Tech Nights 2020: saiba como foi — Vídeos Gratuitos
A seguir estão também diversos artigos abordando o uso de Azure Functions deserviços que integram o Microsoft Azure (há vídeos sendo referenciados em alguns destes posts, lembrando que todas estão referências são gratuitas):
Clonando uma VM no Azure
Utilizando o Azure Domain Services
Copiando dados no Azure Storage
Acessando VMs Windows via console
Acessando VMs Linux via console
Aprendendo Cloud Computing na faixa: artigos, vídeos, canais, comunidades…
Aprendendo DevOps na faixa: artigos, vídeos, canais, comunidades…
Docker — Guia de Referência Gratuito
Kubernetes — Guia de Referência Gratuito
GitHub Actions — Guia de Referência Gratuito
Azure DevOps — Guia de Referência Gratuito
10 Serviços do Azure que você precisa conhecer na prática | Conteúdos Gratuitos
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 2
Blog do Azure na Prática
38 
38 claps
38 
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@raultroyo/day-29-setting-a-launch-date-choosing-an-iaas-provider-seeing-our-illustrations-1bf874b1ea4f?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
Raul Troyo
Jun 18, 2016·2 min read
We relaxed a bit today. Tasks were marked as done in Asana, but at a slower rate. We did make a couple of important decisions.
We will launch July 1st 2016. Why? Because, we are 3 days behind on our coding efforts. Version .1 should be fully coded by Wednesday (07/22). We will have 2 days for Quality Assurance and a whole week to deploy.
We currently host our website and testing environment on Digital Ocean. We use Digital Ocean because it is affordable and it runs smoothly. GRVTYlabs website, customer demos and internal tools will remain in Digital Ocean.
We decided to host Facturabot on AWS. The main selling point “Free Layer”+Elastic Beanstalk. AWS won’t be free, since we have to pay for an EC2 instance with 6GB Ram. Our deploy will be cheaper on AWS than on DO, and Beanstalk will help us achieve our weekly release goal.
We read blogs and analyzed social media properties for similar companies in the US, Canada and Mexico. We are getting closer to understand how to speak to our customers and how to reach them.
Teff sent us her proposal for our first composition. This is still a work in progress. The drawing is raw, the composition will change a bit, traces will be softened and digitalized.
Spent So Far: $2,531.08 USD
I write about product management, business and SaaS. PdM at Cordage.io
See all (165)
I write about product management, business and SaaS. PdM at Cordage.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pavithra_38952/building-infrastructure-with-microsoft-azure-and-ansible-e5245e5b33a8?source=search_post---------305,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pavithra GB
Apr 28, 2018·13 min read
Microsoft Azure, formerly known as Windows Azure, is a cloud service from Microsoft that offers Infrastructure as a Service, (IaaS), Platform as a Service (PaaS), and Software as a Service(SaaS) through its more than 600 services.
Ansible interacts with the Azure resource manager’s REST APIs to manage infrastructure components using Python SDK provided by Microsoft Azure, which requires credentials of an authorized user to interact with Azure REST APIs.
Ansible packages cloud modules to interact with Azure resource manager. These modules require Azure Python SDK to interact with the Azure resource manager’s APIs. We will now prepare our host to run Ansible using Azure cloud modules and put it together so that it can create and manage Azure resources:
2. We need to set up credentials for the Azure resource manager to interact with Azure APIs. Azure provides two ways to authenticate with Azure:
Let’s ensure that we have the correct permissions to create our application for Ansible:
2. Select User settings, as shown in the following screenshot:
3. Check whether the App Registrations setting is set to Yes. If it’s set to Yes, then anyone other than the admin can register an application and we can proceed. If it’s set to No, either we need to be the admin in order to register an application or we should get it enabled by the admin of the Azure account:
Once we have the required permissions, we will register an application.
2. Select New Application Registrations and we will see a dialog box, as shown in the following screenshot:
3. Enter a name and URL for the application.
4. We will need the application ID and authorization key for our registered application. Navigate to App Registrations and select our application:
5. We can copy the Application ID:
6. Let’s generate an authorization key. Navigate to Keys:
7. Enter the key’s name, select the expiration for this key, and note down the auto-generated authentication key:
While accessing the Azure resource manager, it also requires the tenant ID. Let’s look for the tenant ID.
2. The Application ID here is our tenant ID:
Now our application is ready, but it needs access to create new resources. We will now assign a role to our application using Access Control (IAM).
2. Add a new IAM control, using the role from a predefined list and by selecting Application under Resources. We can select roles or create one depending on what we want to achieve through Ansible automation. To keep this step simple, we are using the Owner role.
3. Once we have all the required credentials, we can create a credential file inside .azure in the home directory:
Before we jump into creating a Linux VM, we should know the following terms with respect to Azure:
3. Storage account: The Azure Storage Account is a Microsoft-managed cloud service which provides scalable, highly available, and redundant storage. Azure Storage offers three kinds of storage:
4. Location: This is a region where we can deploy our resources in Azure Cloud. All of these will be deploying resources in the westus region. We will define this location as azure_region in group_vars for the playbook:
To do this, follow the steps below:
2. Create a storage account for our VM disk:
3. Let’s create our first VM in Azure Cloud:
An Azure virtual machine can be attached to multiple network interface cards. With a network interface card, the virtual machine can access the internet and communicate with other resources both inside and outside Azure Cloud. In the, Creating an Azure virtual machine, while creating a virtual machine, it creates a default NIC card for the VM with default configurations. In this, we will see how to create, delete, and manage a NIC with custom settings.
Before we move ahead and create a NIC, we should be aware of the following term:
To do this, follow the steps below:
2. Create a subnet:
3. Create a Network Interface Card:
4. Access the private IP address:
In this, we will create a public IP address and associate it with the network interface.
Azure allocates the public IP address using one of two methods, static or dynamic. An IP address allocated with the static method will not change, irrespective of the power cycle of the virtual machine; whereas, an IP address allocated with the dynamic method is subject to change. In this, we will create a public IP address allocated with the Static method.
To do this, follow the steps below:
2. Display a public IP:
In this, we will create a virtual machine and network interface using the public IP we created. After creating a virtual machine with the public network interface, we will log into it using SSH.
2. Create a virtual machine with the existing network interface:
2. Log into the VM using the public IP from the recipe, Working with public IP addresses:
In the first two steps, we created a NIC with an existing public IP, created in the recipe Working with public IP addresses, and a virtual machine using that NIC.
In step 3, we logged into the virtual machine created with the public NIC.
In Azure, a network security group is an access control list (ACL), which allows and denies network traffic to subnets or an individual NIC. In this recipe, we will create a network security group with some basic rules for allowing web and SSH traffic and denying the rest of the traffic.
Since a network security group is the property of the network and not the virtual machine, we can use subnets to group our virtual machines and keep them in the same network security group for the same ACL.
2. Attach the subnet to the security group:
3. Attach the NIC card to the security group:
The Azure Storage Account offers Blob storage, which operates in the same way as files stored on our local systems, such as pictures, videos, PDFs, and so on. In this recipe, we will learn how to use the Azure Storage Account for storing blobs.
2. Add a blob to the container:
3. Download the uploaded file:
In this, we will use dynamic inventory to target hosts and build an Ansible inventory using the Azure Resource Manager API. Dynamic inventory enables us to leverage its dynamic nature by not hardcoding the host’s addresses in a static inventory file.
Ansible can target the hosts present in a group, and by default, the Azure inventory creates the following groups:
We can define a few other parameters during configuration to restrict hosts in the inventory,. We could define specific groups and filters, such as resource groups, using the following inventory configuration file:
2. Use ansible-playbook with dynamic inventory:
3. Use Azure tags to target hosts in Ansible:
We will now deploy a simple phonebook application in Azure Cloud. In this application, we will create a resource group, a virtual network (phonebook-vnet01), a subnet (phonebook-net01), a security group (phonebook) allowing HTTP and SSH traffic, a network interface with a public IP, and finally a virtual machine.
We create and save our playbook as phonebook.yml:
We should note that the hosts in this playbook are set as tag_Name_first_vm, which will create an inventory for our phonebook application at runtime. We can execute our playbook using the following command:
We used a command-line inventory argument for our playbook, since we are creating our resources in the playbook itself. After successful completion of this playbook, we will be able to access our application on port 8000 and the host IP of the virtual machine created in this playbook, which will be displayed in the last task using the debug module.
17 
5
17 
17 
5
"
https://medium.com/@mikethebbop/convincing-enterprise-business-looking-for-iaas-saas-paas-to-think-google-aad068d78474?source=search_post---------32,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ira Michael Blonder
Mar 24, 2016·2 min read
Recent Google news reports speak to an interest in building Google Cloud business with enterprise business customers. Just today (March 23, 2016), in an article titled “Google Showcases Its Cloud Efforts, Determined to Catch Up to Rivals” Steve Lohr of the New York Times opined on the topic.
A lot of copy has been written on the addition of Diane B. Greene, one of the founders of VMware, as a step forward, by Google, towards a more attractive presentation for enterprise business prospects. Steve Lohr’s story shares this topic. On Ms. Greene he notes “She knows the enterprise computing business, which has not been Google’s strength”.
Leaving aside the unhealthy sprinkling of opacity throughout the article (for example, what does “She Knows” mean in the sentence I quoted above? Or how did Mr. Lohr reach the conclusion enterprise business has, “traditionally”, bought “expensive software”?), the main issue I have with Mr. Lohr’s story, as someone who has collaborated and partnered with enterprise business customers for over 20 years on software purchases, is twofold:
What my customers are looking for are hybrid solutions with a bridge between their on-premises data centers and IaaS/SaaS/PaaS offers. Any potentially attractive hybrid cloud offer will have to include a convincing argument for data security.
Hybrid cloud and access controls built on Microsoft’s Active Directory may not be flashy. But I’m confident enterprise business prospects attending the conference will be looking for them as they stroll the exhibit floor. If Oracle can learn to speak the right language, how come Google is struggling?
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
1 
1 
1 
35 years non stop experience marketing & selling IT products to very large organizations. MA in English. Technical Writer. MARCOM & PR writer. Product Mktng.
"
https://medium.com/@strongnode/ama-highlights-with-cryptomuni-strongnode-io-8c22be0b8ba0?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 13, 2021·9 min read
The first of the three AMA sessions featuring StrongNode.io, the Infrastructure-as-a-Service (IaaS) technology company and innovation lab, on 10th September 2021 was at CryptoMuni. Our StrongNode.io CEO and Co-founder Daniel Saito and CTO Colin Charles were hosted by CryptoMuni as they answered the questions from the community regarding our company, technology, our upcoming IDO, $SNE token, and future endeavors.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on the 6th of October 2021, 15:00 p.m. UTC/11:00 a.m. EST, on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
Check out some of the key moments in last Friday’s AMA session below:
INTRODUCTIONS:
Daniel Saito: Hi everyone wow, it’s a pleasure to be here. It’s always cool to address the UK/EU crypto scene. I also have with me, Colin Charles, CTO of StrongNode.
Colin Charles: Hi! I’m Colin the CTO and basically, we are doing an ID platform for all the innovations we roll out but we also do edge node processing for big data jobs that we batch reduce. This is the premise, using idle resources and getting paid for it, too.
Daniel: So, @bytebot (Colin) and I are serial entrepreneurs, we are recently known for our $1B dollar exit to Sun Microsystems, and rebooting another database company called MariaDB. Colin is the co-founder of the MariaDB server.
Can you explain where nodes come into the project?
Colin: Nodes are edge — they can be your computer. They can be your phone (while plugged in). They can be your smart tv. As @StrongNode_CEO alluded to, we were at Sun once post-acquisition and they always said, “the network is the computer”. It’s kind of like what we’re after.
Daniel: We are firm believers in [ubiquitous] computing, anything that has silicon and has network connectivity we believe we can make it a node.
Upon our research, you and your team predominately come from strong gaming and development backgrounds. How will your previous experience benefit this project and any incubating projects you are working on?
Colin: I come from an open source background and it still goes thru my veins. My experience is to add a payments layer to open source. Like SaaS but more equitable.
Daniel: I am known in gaming history that developed the first network and network playable game (https://www.youtube.com/watch?v=fP1FxPtUMeM)on a console. I had to create an ISP from the ground up and recode the TCP stack on the SEGA Dreamcast console. A gaming focus low latency, gaming network (in a world of dial-up internet), and a network playable game.
Che$ter: I remember playing SEGA games as a kid! Am I correct in saying you are incubating a gaming project to come under the StrongNode ecosystem?
Daniel: I remember coming to the SEGA UK offices to work with the Sonic Dev team out this way to make sure it works on PAL (yes, remember those days?) We also have ties with the British eSport Leagues.. so that’s going to be interesting.
Larry Hoover: We love all things open-source :)
Colin: From Linux in 1996 to Fedora, OpenOffice.org, gnome, MySQL, MariaDB, ProxySQL, Galera, etc, I’ve touched a lot of open source and don’t plan to stop :)
(I don’t play games that’s why I only focus on technology or scaling games companies :)
Che$ter: Why do you think StrongNode is a better product than other Nodes-as-a-Service providers?
Colin: We are using open source, say Hadoop, to just spread processing down to edge nodes for batch jobs. We won’t be reinventing the network layer either for an endpoint. And so on. This is us taking open source and going forward.
Che$ter: This sounds great. What benefits do you get running an edgeware node, compared to running a validator node?
Colin: I think it’s important that we aren’t using validator/crypto here. Your machine is actually going to do processing of big data chunks to solve Covid, climate, ad clickstream data, or even find aliens like the now-defunct seti@home
Daniel: Yes we have to separate the notion of using these valuable compute cycle just for the purpose of crypto, there are real-world problems that can utilize this precious resource. Like tackling COVID genome 🧬 sequencing data, so it becomes faster and cheaper to process, that helps mankind.
+ 44: Very unique I must say. Could you give us an insight into what is required from a user to run a node? In terms of costs and equipment?
Colin: Your run-of-the-mill windows; Linux or macOS pc to start with. The only resource with a tray table app. If you’re currently at 20% CPU utilization, we’ll take it up to 80% and you get paid for that 60% processing and bandwidth use.
Of course, we’ll be mindful to not run while you’re on a battery for a laptop unless you choose to. And if you’re on a mobile data plan, we’ll turn it off too, ie if you’re tethered to an iPhone or iPad
Colin: (You can override, but best we have sensible defaults from the start)
Daniel: I see a future after we get the footprint of x86 64 bit architecture we will go after chipsets like ARM and get the raspberry pi community
Colin: Pi is marginal. Think arm on iPhone, Android, smart tv :)
Che$ter: How much can users expect to earn in rewards for running an edgeware node on a simple device like a computer or phone?
Daniel: Well it’s a marketplace, so che$ter has a project that requires the use of high computational resources to be made available on the fly. He segments the data to a folder and loads up the node software and specifies the dataset to be processed. It’s not critical data but would like it to be processed by the morning. che$ter would set up his stake for processing the said data and broadcast the contract out to the world. The world gets Che$ter broadcast and swarms into the process of the extensive dataset and gets paid to do the job.
This as a result makes our solution very transaction-heavy by nature. All transactions are taxed minimally as a form of “reflect” and conducts token buyback and helps grow the token price for just HODLing it, along with other features at a token level.
Che$ter: that sounds great, I also saw on your website that you plan to be blockchain agnostic, what benefits does this bring for StrongNode users?
Colin: Mainly fees and we aren’t here to pick a winner.
Daniel: Exactly you wouldn’t see our solution on $ETH anytime soon.
TheRealMover: Can you explain to those who don’t know what IOT is and how you plan to use the tech in your project?
Daniel: IoT is Internet of Things. Your lightbulb likely has some silicon in it that allows for you to get granular control of the lightbulb 💡!
Strangely my toilet in Tokyo is connected to my wifi, don’t know the purpose for that except flushing down 💩 coins.
+ 44: So any device that has basic processing ability and can connect to WIFI can generate rewards? A great concept.
Daniel: Yes most of those devices stay idle specifically TVs.
Colin: Ideally something with processing power and of course network connectivity. So maybe not your smart fridge just yet.
+ 44: Do you have an accurate breakdown of rewards so we have an idea how much each device could earn daily etc?
Daniel: Well don’t know yet depends on the network demand for compute resources
Daniel: Our IDO will be on https://starter.xyz
LIVE AMA SEGMENT:
Larry Hoover | Will Never Ask For Keys or Money: Great AMA guys! So to clear this up for those who may not know already, how will the rewards model remain fair? (in the sense that someone may be mining with a $6000 rig and someone may be mining with their smart TV or another low powered device)
Daniel: Of course someone putting a beast of compute on the network, as it ensures proper uptime and connectivity it would get paid out more as more jobs will be given to it as it’s a more reliable contributor to the network.
Kai: Hello guys thanks for ur time today, can I ask will the device have to be connected to wifi or will Ethernet hard-wired connection be suitable? Also how many devices can we run on the same network & finally will these be free to acquire? To add, will rewards be paid in [$SNE] tokens?
Daniel: Yes we pay in our native token ($SNE token). [As for the number of devices that we can run], as many as you can register with your wallet
Yahya 🇵🇸: Thank you for giving us the chance to ask, my first question is
When will you begin marketing and what scale of reach are you going for with your first campaign?
Daniel: We have started to market (we are here, right?). We are doing an average of two AMAs a day on various platforms and channels.
Kai: I’d also like to know if the node is capable of running on Mac software? Ie a MacBook while plugged into power
Daniel: Yup Mac/windows / Linux for first versions
TMC: Does that mean it is down to the number of devices we have? So if we wanted to create several nodes we need several devices?
Colin: Yes. One node per device. But an account / ID can have multiple of course
Yahya 🇵🇸: I would also like to ask — How do you plan to remain relevant & competitive in the future considering the amount of new projects that will be coming out?
Daniel: It’s about token distribution that’s how we keep relevant.
Kai: Thank you for your responses guys much appreciated, if I am not mistaken the hardware is free of cost?
Daniel: Well that phone that is sitting in that desk drawer because you recently upgraded can be used eventually
Desty: Use-case to maintain the demand. Any burning mechanism? As people will be minting rewards there needs to be a counteractive measure in place right?
Daniel: Use case is various. It is the currency that will be used to stake your uptime, at the same time it can also be used to enter other projects that are currently incubated in our incubation labs. We will also have AMM farming and all that DeFi jazz.
Desty: Do we have anything planned to keep a balance with rewards as many projects are failing to maintain it with the use case of their token compared to how much they reward. Simply put minting compared to burning, I guess
Daniel: There is a limited amount or tokens burning isn’t something that we go off at start because that also tends to be more of the race to the bottom if not properly thought out yet. We need to understand how users will require on the platform at any given point in time. Basically we aren’t planning on burning 50% of supply for s***s and giggles because we think that is best for the project. no we don’t do that.
Che$ter: This sounds great. Thank you everyone for taking time out of your day to attend, and a big thank you to @StrongNode_CEO @bytebot for answering questions. A final question from me- How and where can people buy your token on launch?
Daniel: We will be on the POLYGON (MATIC) chain. As our IDO will be listed at starter.xyz . Some of you guys have already have received an allocation, for that we thank you for your early contributions. For those who want to get in on this, we can work out something, please extend your interest to che$ter or @stillw0rking
September 23rd.
Visit https://strongnode.io/ for more information.
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
See all (8)
452 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
452 claps
452 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-vm-is-a-vm-is-a-vm-or-is-it-3f9a611c017a?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
A VM is a VM is a VM (or is it?)
When I hear that all IaaS clouds were created equal I feel like a decade or so ago arguing that Solaris 10 was actually bringing lots of innovation to the operating system market (Dtrace, ZFS, Zones). Google’s Compute Engine is adding unique features one at a time, to the point where flexibility is really the reason the cloud makes more sense than ever before.
It all starts with the ability to create your VM using custom machine types rather than having to chose from a pre-defined set of configurations. You can also change the characteristics of an existing machine — more memory, more CPU to adjust to different requirements or when moving from development to production. Then, the boot time has been repeatedly measured as being extremely fast which really relates to how flexible a cloud environment should be.
When running your VM, you can rely on live migration voodoo to keep your services running (while, for instance, Google upgrades the underlying infrastructure) as well as wonderful local SSD performance. Performance, reliability, flexibility. Choose three.
Price is always important and since you don’t have time to become a billing expert, Compute Engine is priced by the minute (so you truly pay for what you use), offers automatic discounts on sustained use and a crazy-cheap option with preemptible VMs. Customers of course benefit from the pricing competition and Google is committed to providing the best value.
Of course there are many other things that make Compute Engine attractive including Google’s networking infrastructure, the new UX-friendly Console with great in-browser SSH support, and of course all the higher-level services such as Container Engine (hosted Kubernetes). But at the end of the day what matters is that innovation and competition are alive and well.
Google works hard to make sure not all Cloud VMs are created equal but you should check out for yourself — you’re just minutes away from SSH’ing into your own Compute Engine VM.
Google Cloud community articles and blogs
8 
1
8 claps
8 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/the-future-of-indie-gaming-is-multi-platform-and-bright-43267bac038f?source=search_post---------61,"There are currently no responses for this story.
Be the first to respond.
Disclosure: PubNub, the global DSN and realtime IAAS company, has previously sponsored Hacker Noon.
Today we’re going to catch up with Jordan Schuetz, Developer Relations at PubNub. He started Ninja Pig Studios 7 years ago, and to date, they’ve developed and released over 15 products which have accumulated over a half a million downloads, such as IQ Test, Meme Run, and Tiny Birds. Today we discuss the direction of the indie gaming industry, the technology powering game performance, and the intricacies of developer relations.
In terms of making it easier to build a great game on your own or with a small team, what technologies are leading the way?
Indie game developers should always choose technologies that fit their skill level in development while also allowing them to target the platforms they want to release their content on. Developers with limited programming experience should go with engines where they feel more comfortable. I started off with Corona SDK since the programming language Lua was easy to pickup. However, when I needed to target additional platforms, Unity became my number one pick. Unity is killing the competition right now since it empowers developers to write one code base and publish to almost every platform. This is great for indies since they don’t have to worry about making sure their application works on the thousands of different devices out there; Unity does all the heavy lifting for them.
You recently gave talk: Standing Out in the Competitive, Crowded Indie Game Market. How does PubNub fit into gaming space? What is the PubNub’s current usage in the gaming industry? And what aspects of building games do you identify as a future high growth/usage for PubNub?
PubNub excels in two distinct areas of gaming. The first is powering the multiplayer functionality — in other words, making players move and syncing game state. We supply the infrastructure and APIs that with a few lines of code, you can get PubNub up and running in your game.
The other thing we’re great at is the social functionality of a multiplayer game — in simpler terms, making players talk. We make it easy to build and power in-game realtime chat, leaderboards, and notifications. In addition, PubNub can be used to update players positions, input in realtime and can show the current number of players connected.
We have a ton of large customers in the gaming space that use us for a wide variety of use cases. Pocket Gems, creator of the popular mobile game War Dragons, uses PubNub to power the realtime battles in their game, as well as Storage & Playback to allow players to join battles even if the player isn’t online for the very beginning of the battle. In addition, PubNub is globally replicated which allowed Pocket Gems to reach regions they normally didn’t have access too due to reliability constraints.
Gaming is a massive growth industry for us, since every game state change such as an input event, chat messages or a player movement is a message that needs to be sent through our network. Developers are needing ways to transmit this realtime information in a fast and reliable manner, and using the PubNub network we are able to provide that service to developers.
The Indie Game market is estimated to be over $1 billion just on Steam alone. Could you walk me through the evolution of indie game market?
Before the mid-1990’s, game distribution was controlled completely by publishers and retailers. As an indie developer, you would have to go out and pitch publishers to see if they were willing to distribute your title. Publishers would then offer their insight, request changes, and most of the times flat out deny your title from release. If your product wasn’t approved to be published, your only option as an indie would be to release your product as shareware and hope that you could receive enough donations to pay the bills. What changed the industry was in the 2000’s, Valve released their digital distribution platform called Steam which opened up the doors for indies to self-publish their titles. Indies now had the financial independence and independence of thought to create the games they wanted through their own budget constraints.
A lot of the challenge facing Indie Developers is distribution. The larger gaming companies have a lot of built-in distribution channels. As more developers are becoming Indie game developers, how have the distribution channels for the gaming industry evolved?
Companies that own digital distribution channels such as Steam really paved the way to how people consume and own digital content. Apple followed in Steam’s footsteps by creating the App Store, and then the Google Play store followed shortly after. XBOX, Playstation, the Nintendo Wii and 3DS all started coming out with their own digital distribution platforms after seeing the customer demand for purchasing content instantly. Consumers went from wanting to own hard disk copies of their games to wanting a digital library without needing to take up physical space in their house, or worry about their titles being stolen. This also improved the indie experience since now indies don’t have to worry about their games being resold in secondary markets.
Do you have any insight into how game usage is changing by platform/hardware? And what platform or hardware are especially excited about for future gamer adoption?
Gamers are caring more about multiplayer experiences where they can socially interact with other gamers and work together towards a common goal. The multiplayer sector is taking off, which makes it ever more important for game developers to implement multiplayer features in their titles. Devices such as the Oculus, HTC Vive, and Hololens are exciting new technologies that the gaming community is rapidly adopting due to the more immersive experience they offer. The VR/AR space is going to be rapidly improving over the next 5–10 years and will become the future of multiplayer gaming. As an indie, trying to be first to market on emerging platforms is a great monetization strategy and is where indies have the most room for growth.
How do you evaluate where/how/when a game should dedicate their marketing resources? What marketing trends do you see in the gaming industry for outlets like Facebook ads, Youtube Videos, Twitch Community Building, App Store Optimization, etc.?
The most effective way to get marketing exposure is to get other influencers in the community excited about your title. I never spent a penny on marketing my titles, however I did spend a large amount of time developing relationships with YouTubers and offering them free download codes so they could try out my titles. Twitch and YouTube are the two largest platforms right now for game discovery, so they need to be your focus, rather than just spending money on blanket advertisements.
Could you walk us through a week in your work life? What are your core metrics? How do you measure progress?
Half of the week I’m in the office writing content and the other half I’m either at local technology meetup groups or working at a conference. My goal for working in DevRel at PubNub is to get developers excited about our product and show them how PubNub saves them time which allows them to focus more on marketing their title rather than developing it. I’m measured on overall engagement on my content I produce and how many people I can get in front of at meetups and conferences. DevRel can be a hard role to quantify since many times your marketing efforts and impact on the community can’t be measured to a tee. However, creating fun and engaging demos that developers are searching for online is the most measurable way to evaluate performance.
What makes a good developer relations professional? What are the biggest mistakes developer relations professionals are currently making?
Working in the DevRel role requires the individual to have charisma and passion. You should always be thinking about how you can improve the developer experience and make content that makes developers excited to try out and play with your product. I think the biggest challenge developer relations professionals face is they can become disconnected from what it’s like to be a developer in the first place. DevRel professionals need to give developers what they didn’t even know they wanted. Working in this position takes a lot of creativity and brainstorming, but is rewarding when you pull it off right.
At an early age, what was your favorite computer game? And why/how did it blow your mind?
My favorite game at an Early age was a game called Dark Reign: The Future of War. It was a real-time strategy game that I used to play on my dad’s Windows 98 computer. I loved the idea of controlling troops in a futuristic world. I used to play Dark Reign for hours, and my parents actually had to take the computer away since they realized I was a little too addicted to it.
What makes a game addicting? What are your favorite games of all time?
I think competitive realtime games are the most addicting since it facilitates competition with your peers and incentives improvement in the game. I think my favorite game of all time has to be League of Legends since it’s very challenging and always changing depending on the difficulty of players you are paired against.
When Meme Run really started to take off (a Wii U game you made as a 20 year old), it received a fair amount of negative press.
How did the negative press increase/decrease usage of the game? And on a personal level, how did you handle/channel this internet negativity?
My core audience who were younger kids absolutely loved Meme Run and thought the humor was outrageous and funny. However, most of the negative press came from older gamers who thought the game was a “disgrace” to the Nintendo Wii U platform. All the negative press is what made the game so controversial, and actually allowed the game to be picked up by Twitch Streamers and YouTubers who literally only wanted to criticize it. This drew millions of combined views on different channels which overall increased the sales. People wanted to buy the game just to show their friends how crazy, outrageous and controversial the game was. I even had pizzas ordered to my house and was DOXXed by internet trolls who disliked the game. What kept me going through all the hate was the knowledge that my core audience loved the game. I even instigated the negativity by shilling the game on 4chan by creating negative posts about Meme Run on /v/. I understood why so many people disliked the game since me and my brother created it as a joke, however I gave people something they didn’t know they wanted. What made it all worth it was seeing how many people did enjoy the game, seeing fans comments on Miiverse, and watching YouTubers react to the outrageous humor.
A lot of digital games that are hits lack longevity. Like viral hits often have a novelty and a shelf life. What digital games do you as having longevity? I.e. like what game is to the industry as Craiglist is to the internet?
I believe all games have a limited half life. Competitive games with eSports teams seem to have the most longevity. In addition, games that are constantly updated with new patches and support from the developers seem to be the most successful. Games such as Counter Strike, League of Legends and Overwatch seem to be doing well due since every game the player joins presents a new challenge which keeps bringing them back.
Why should a developer consider using PubNub for multi-player gaming versus building a solution themselves or building atop some open source software?
As an indie developer, PubNub allowed me to make my multiplayer game quickly which allowed me more time to handle marketing my title. Developers usually get too caught up with developing their game and trying to make it perfect rather than releasing it to see how it goes. PubNub allows you to rapidly develop a multiplayer game and release it globally without any hiccups. As an indie, you simply don’t have the time to build out a globally distributed network. You should be focused on creating a fun game rather than worrying about the backend infrastructure.
To the game developer who is working for a larger company, but has their own game idea and is considering going out on their own, what advice would you give him or her?
Indie game development is an extremely competitive space and that only provides short term revenue. It is possible to hit it rich, however in most cases you have to constantly be putting out new games in order to make a living. You should go into indie game dev with the mindset that you are creating something that you are passionate about, and if you do it right the money will follow. I think the best strategy is to go at it when you have free time since I’ve seen many give up years of their life to making games but ultimately failing. Most developers spend too much time on development, and not enough time on marketing which is what leads to their failure. Knowing your audience is the key to being successful in the indie game space. Try to remember back to when you were a kid and the things you thought were funny back then, and emulating that through a game will resonate with your players.
#BlackLivesMatter
191 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
191 claps
191 
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@JacobsEdo/scalable-stochastic-model-for-iaas-cloud-computing-platforms-29e159ec7cc3?source=search_post---------280,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jacobs Edo
May 29, 2019·5 min read
Gartner defines cloud computing as a style of computing where scalable and elastic information technology-enabled capabilities are provided as a service to external or internal customers using internet technologies(Gartner, 2019). The above definition also articulates some of the critical characteristics of cloud computing — standardization and automation, virtualization, rapid elasticity, flexibility and a pricing model that supports “pay as you go” billing. In the recent past, several cloud service models have emerged, prominent amongst which are: —…
"
https://medium.com/strongnode/ama-with-cryptofury-on-the-strongnode-sne-token-and-upcoming-ido-launch-on-starter-xyz-f72fbdec85d3?source=search_post---------312,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 30, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) technology company and innovation lab, and our CEO and Co-founder Daniel Saito joined the Telegram AMA episode with CryptoFury last 18 September 2021. Daniel answered questions from the community regarding our company, project, products, and talked about our upcoming initial decentralized exchange offering (IDO) launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 2 launchpads. The IDO is happening on Starter.xyz and BullPerks.com.
For the IDO on BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70 . The KYC process for the StrongNode IDO on Starter starts 30 Sept via Blockpass, please click here.
The highlights from the AMA session between StrongNode and CryptoFury are posted below:
Hosted by: Mirzin🥇CMO🥇:
Can you briefly introduce yourself as well as StrongNode? What is your mission and vision to build this project? What do you want to achieve through your project in the future?
Daniel Saito: Hello, I am Daniel Saito. I am the CEO and Co-founder of StrongNode. We are an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle computers like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources. StrongNode was the name picked out by our amazing marketing team.
The mission and vision of Strongnode? We are on a mission to power companies globally through the next generation of edge networking by uniquely harvesting a trifecta of idle compute resources and leveraging new blockchain technologies.
We wanted to address the last mile, which is the EDGE and build a global edge network. Right now, we are planning to have a successful IDO on starter.xyz and Bull Perks [on October 6]. We have several partnerships planned in the pipeline. Ultimately, we are building a global edge network through access and monetization of idle resources.
Do you have any Coin Burn / BuyBack systems or any Token Burn plans to increase the value of Token & attract Investors to invest?
Daniel Saito: We do have a TOKEN Buyback for our coin, in a form of a reflect (we tax all transactions). This tax contributes to TOKEN buyback. We have no plans on burning since our objective is to get into as many wallets as possible. So burning immediately after IDO doesn’t make more sense other than TOKEN price manipulation.
Users often care less about technology, but rather the value of the token. How do you manage to strike a balance between developing the technology and also improving the value of the token?
Daniel Saito: I see the product as two different things, I see our technology as a product that we need to push and also the TOKEN as a product.
Technology will progress further and that is overseen by the CTO office, the TOKEN price is overseen by the executive team and works with our best of breed market makers in the industry to grow the value of the coin.
Can you list the outstanding features of StrongNode that you think are the strengths that will help StrongNode succeed? In the future, will the StrongNode token be developed with more utilities?
Daniel Saito: Yes, We’re currently working on StrongNodeID (SSO, open ID, KYC, etc.). Once that is done, we’re working on the dispatcher to send jobs to edge nodes and receive them back processed; all while having economics for you to pay for jobs to get done, and to pay out to those (machines) that get the jobs done. We also have other incubated projects that will ride on the network, hence the SSO needs. The platforms we are launching are social impact, gaming, entertainment, lifestyle B2C companies that will be symbiotically powering and being powered by our global Edge network. Some of the projects that we have incubating are Original Gamer Life or OGLife, which is an esports and gaming community that will take advantage of the StrongNode infrastructure by providing game servers at the edge.
We have a lot of announcements in the pipeline as we are just getting started. We have everything in the pipeline to address, MUSIC, GAMING, NFTs, and DEFI. In our future project offering, solutions that you have not seen before. We will launch on POLYGON first and eventually branch out to other chains. User registration starts NEXT WEEK! Following with our IDO on starter.xyz and Bull Parks in a few weeks’ time! … So register now to get in on the PRESALE!
Daniel: Join us here at https://t.me/strongnodechat as our IDO is weeks away and get whitelisted to participate. Our IDO will be available on starter.xyz and BullPerks.com
Please join our community and keep on top of all the fun stuff we are building! Visit our website: https://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
452 
452 
452 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@nutanix/the-rise-of-anything-as-a-service-xaas-the-new-hulk-of-cloud-computing-5eca37c2ff02?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Apr 26, 2017·3 min read
Cloud Computing, as we see it today, has seen a tremendous evolution in the service segments — right from the dawn of Software-as-a-Service (SaaS) to Infrastructure-as-a-Service (IaaS) and Platform-as-a-Service (PaaS), and now Anything-as-a-Service (XaaS).
Analysts forecasted that the global XaaS market will grow at a CAGR of 38.22% between 2016–2020. Besides the typical SaaS, IaaS, and PaaS offerings discussed, there are other ‘As-a-Service(aaS)’ offerings too. For instance, Database-as-a-Service, Storage-as-a-Service, Windows-as-a-Service, and even Malware-as-a-Service.
No doubt the ‘Cloud-driven aaS’ era is clearly upon us and cloud computing remains the top catalyst for all these services’ growth. The converse holds true too.
In the words of Amarkant Singh, Head of Product, Botmetric, “The persuasive wave of cloud computing is affecting every industry and every vertical we can think of. Thanks to all of its fundamental models — IaaS, PaaS, and SaaS plus the latest XaaS, cloud has brought in democratization of infrastructure for businesses. Talking about XaaS. It is the new hulk of the cloud computing and is ushering in more of ready-made, do-it-yourself components and drag-and-drop development.”
The XaaS model was born as a result of the elasticity that the cloud offers. More so, the XaaS provides an ever-increasing range of solutions that ultimately gives businesses the extreme flexibility to choose exactly what they want tailored for their business, irrespective of size/vertical.
Recently, Stratoscale asked 32 IT experts to share their insights on the differences between IaaS, PaaS and SaaS and compiled an exhaustive Op-Ed report IaaS/PaaS/SaaS: The Good, the Bad and the Ugly[1]. Among these experts, Amarkant too has penned few lines for the report.
Here are the excerpts from the article:
“More companies across the spectrum have gained trust in cloud infrastructure services, pioneered by AWS. While IaaS provides a high degree of control over the cloud infrastructure, it is very-capital intensive and has geographic limitations. On the other hand, PaaS comes with decreased costs but offers limited scalability.
With its roots strongly tied to virtualization, SOA and utility/grid computing, SaaS is gaining more popularity. More so, it is gaining traction due to its scalability, resilience, and cost-effectiveness.
According to a recent survey by IDC, 45% of the budget organizations allocate for IT cloud computing is spent on SaaS.
As organizations move more of their IT infrastructure and operations to the cloud, they are willing to embrace a serverless/NoOps model. This marks the gradual move towards the XaaS model (Anything as a Service), which cannot be ignored.
XaaS is the new hulk of the cloud computing. Born due to elasticity offered by the cloud, XaaS can provide an ever-increasing range of solutions, allowing businesses to choose exactly the solution they want, tailored for their business, irrespective of size/vertical. Additionally, since these services are delivered through either hybrid clouds or one or more of the IaaS/PaaS/SaaS models, XaaS has tremendous potential to lower costs. It can also offer low-risk infrastructure for building a new product or focusing on further innovation. XaaS embracement has already gained traction, so the day is not far when XaaS will be the new norm. But at the end of the day, it all matters on how cloud-ready a company is for XaaS adoption.”
Each expert has an idiosyncratic perspective to what, where, when, and why XaaS. For few, it stands for everything-as-a-service and refers to the increasing number of services delivered through cloud over the Internet. For few it is anything-as-a-service. Techopedia quotes it as a broad category of services related to cloud computing and remote access where businesses can cut costs and get specific kinds of personal resources. Different perspective, different views, but one goal: Putting cloud in perspective.
Read what other experts are deliberating on XaaS on Stratoscale’s Op-Ed article ‘IaaS/PaaS/SaaS — the Good, the Bad and the Ugly.’[1]
Share your thoughts in the comment section below or give us a shout out on either Facebook, Twitter, or LinkedIn. We would love to hear what’s your take on XaaS.
[1] Stratoscale, 2017, “IaaS/PaaS/SaaS — the Good, the Bad and the Ugly.”
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
21 
1
21 
21 
1
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@mvs_org/metaverse-foundation-acute-angle-cloud-form-strategic-partnership-631fed8f4fea?source=search_post---------64,"Sign in
There are currently no responses for this story.
Be the first to respond.
Metaverse Foundation
Dec 19, 2017·2 min read
The Metaverse Foundation and global distributed IaaS platform Acute Angle Cloud have formally announced a strategic partnership to jointly build an interconnected digital asset platform.
Acute Angle Cloud is a global, distributed IaaS platform built on the foundation of the Acute Angle Chain, IPFS protocol and Acute Angle PC. The Acute Angle Cloud will be iteratively improved upon through the Acute Angle Chain, Acute Angle Cloud 1.0 and Acute Angle Cloud 2.0.
The Acute Angle Cloud will position itself as a “Distributed Cloud Ecosystem” and aims to develop distributed cloud storage, public blockchains and hardware systems. Within this ecosystem, the Acute Angle PC is a hardware device based on blockchain technology and will serve as the PC device. The Acute Angle PC is a general-purpose computer based on the IPFS peer-to-peer hypermedia distribution protocol, using the Acute Angle Chain public blockchain to manage digital assets and smart contracts to define user reward systems. The Acute Angle PC was announced on the Acute Angle Cloud on 12 December, 2017 with much attention from the press and tech community. The Metaverse Foundation was very impressed by Acute Angle’s product and expressed that it would have a positive impact on the development of blockchain ecosystems.
The Metaverse Foundation aims to invest in and provide guidance to blockchain applications based on the Metaverse blockchain that form part of the industry value chain.
The Metaverse Foundation and Acute Angle Cloud will collaborate on technical and market development to integrate advantageous resources, cultivate the blockchain industry and expand the pool of decentralized applications. Technical innovation will resolve issues with user-to-user trust, and can provide a new network of production relations as well as enhance the merger of community consensus, individual interests and the exchange of value.
To learn more about the Metaverse foundation, please visit https://mvs.org/foundation.html . Details about the Acute Angle Cloud may be found at http://acuteangle.com/.
Metaverse Foundation aims to maintain the operations of Metaverse Blockchain and build a complete affluent Metaverse ecosystem through grants & education.
See all (15)
181 
1
181 claps
181 
1
Metaverse Foundation aims to maintain the operations of Metaverse Blockchain and build a complete affluent Metaverse ecosystem through grants & education.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@PicardParis/discovering-google-cloud-platform-e621ea3200d7?source=search_post---------84,"Sign in
There are currently no responses for this story.
Be the first to respond.
Laurent Picard
Mar 17, 2017·2 min read
Google Cloud Next 2017 event was huge. Over 200 sessions (about 45 min each) are available, covering many subjects: IaaS, PaaS, Containers, Web Apps, APIs, Machine Learning, Databases, G Suite, Chrome, Android… The momentum around Google Cloud Platform (GCP) is just impressive.
Over the 3 days, they had 100 announcements.
blog.google
Last but not least, Google is currently offering a $300 credit to get started with GCP for free.
cloud.google.com
Experimenting with Google Cloud Platform
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
"
https://medium.com/strongnode/ama-with-cryptoscreen-getting-ready-for-strongnodeid-global-token-distribution-and-innovation-ae38c63845c?source=search_post---------310,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 8, 2021·4 min read
Infrastructure-as-a-Service (IaaS) tech company and innovation lab StrongNode.io and its CEO and Co-founder Daniel Saito joined the Cryptoscreen AMA episode. CEO Saito replied to questions regarding our company, project, products, and shared facts about the upcoming IDO and $SNE token launch and public sale.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Here are some of the AMA highlights from the episode between StrongNode and CryptoScreen below:
Syaiful: Can you explain how StrongNode and what kind of services it offers to users?
Daniel Saito: With StrongNode, it’s about computing for a purpose. So we made an application that caters to the new normal. Yes, you can start with Windows, Linux, or Mac OS PC. If you’re currently at 20% CPU utilization, we’ll take it up to 80% and you get paid for that 60% processing and bandwidth use — earn rewards through your idle compute resources. As we use and implement the best of breed open source projects we started with a Hadoop approach by spreading processing down to edge nodes for batch jobs.
We are rolling a suite of products that run on top of StrongNode; like new products like StrongNodeID, that handles users and access to access edge nodes which can help get paid for the utilisation of idle resources (keeping in mind, we have plenty of idle CPU and bandwidth at most situations and a lot more devices now, with the IoT revolution). The platforms we are launching are social impact, gaming, entertainment, lifestyle B2C companies that will be symbiotically powering and being powered by our global Edge network. So some of our projects that we have incubated is OGLife, which is an esports and gaming community for a 40+ audience. There will be a lot to get excited about this year!
How do you solve liquidity issues and how to ensure user asset security?
Daniel Saito: That’s a new question I have never been asked in an AMA. Love it. We have long term plans on being around so that means to be in the wallets of many around the world. Global token distribution is key to democratizing the digital divide between nations. This is the world’s project that we hope to leave behind running perpetually. We are working with the best of breed market makers as we are coordinating with them to get CEX listing, where we can shape our liquidity goals and qualify for the several pending applications with various TIER 1 ~ TIER 2 exchanges.
We are living in the advent of YIELD FARMING and Liquidity Mining, can you share your personal opinion on Yield Farming and also Briefly explain your Liquidity Mining Program??
Daniel Saito: We have announced our partnership with QUICKSWAP and liquidity mining will be made possible with $QUICK. We will also be releasing our own Yield Optimizer on StrongNode. Don’t forget we will also have reflected code (as all transactions will be taxed) in our token which will do Token buy backs.
Where can I currently buy Token?
Daniel Saito: You can’t, we haven’t had our IDO yet. It will be available on BullPerks.com and Starter.xyz in a few weeks.
The IDO is just around the corner, you can prepare for it as our launching partner https://starter.xyz/ you will need to go through our user registration process to qualify.
2% of the supply will be sold $300K raise (200M tokens will go into circulation at $0.0015) find out more at the following link:
https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
Daniel Saito: Please come and visit us on our telegram channel for the latest alpha for StrongNode. https://t.me/strongnodechat
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
307 
307 
307 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/@jrodthoughts/five-reasons-why-aws-is-going-after-office365-and-g-suite-4c374e4ac2b?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jesus Rodriguez
Mar 4, 2017·3 min read
AWS has been steadily expanding beyond its traditional IaaS-PaaS capabilities into business applications and not it seems to be ready to take those efforts to the next level. A few days ago, there were some reports that indicated that AWS has been working g on a new line of services for traditional information worker tasks such as word processing, presentations or spreadsheets. The initiative is a clear indication of AWS’ intentions to compete with cloud rivals such as Microsoft and Google for the supremacy in the cloud productivity tools space.
AWS’s new line of services are far from being the only effort of the cloud gain in the business apps space. Last week, AWS announced Chime, a video-collaboration tool that rivals offers such as Skype for Business or Google Hangouts. WorkMail is another business centric service included in the AWS cloud.
Whether AWS’ move was anticipated or not it still results fascinating. Here we have a company with over 60% domination in the cloud platform market expanding into a complementarily market vastly dominated by its two closes competitors in the cloud space. It doesn’t get any better than that. The move can be compared with Facebook’s audacious entrance in the messaging space or Google release of Home to compete with Amazon Alexa.
What could be triggering AWS’s decisions to go into productivity tools? How feasible are those plans? Here is part of my initial analysis:
1 — Preventive Measures
Office365 and G-Suite are two of the biggest assets that Microsoft and Google can use to disrupt AWS’ domination in the cloud platform space. Both platforms are indirect channels for the commercialization of Azure and Google Cloud and AWS, obviously, intends to build that market.
2 — Building New Distribution Channels
Building on the previous point, Office365 and G-Suite count with millions of business as customers which provides an easier transition point to Azure or the Google Cloud platform. That channel is particularly important in large enterprise environments.
3 — Becoming Competitive via M&A
One aspect that I haven’t heard analysts consider is AWS’s capability to grow its productivity apps portfolio via acquisitions. At the end, a large percentage of the popular cloud business apps in the market are built on AWS. That position contrasts with Office365 and G-Suite’s grow that has been mostly based on in-house IP.
4 — The Battle of Emerging Markets
I think is safe to assume that Office365 and G-Suite are going to remain the dominant cloud productivity suites in first-world markets such as North America, Europe or Australia. However, AWS is rapidly trying to consolidate its leadership position in emerging markets such as China, India, Middle East or Brazil in which a new business apps suite can be a great asset.
5 — Non-Developers Tools
Microsoft and Google have done a remarkable job expanding the capabilities of Office365 and G-Suite to non-developers. Tools such as Office365's Flow or PowerApps are great examples of this trend. Until now, AWS have remained exclusively an infrastructure and developer-centric platform but the new productivity apps and service can set the foundation for non-developer tools and solutions.
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
1 
1 
1 
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
"
https://medium.com/strongnode/ama-highlights-with-polygon-defi-chat-strongnode-ceo-daniel-saito-and-cto-colin-charles-explains-e06a19c9b019?source=search_post---------51,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Sep 22, 2021·4 min read
StrongNode.io, the Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito and CTO Colin Charles were hosted by the Polygon DeFi Chat AMA episode. The two executives responded to queries about our company, project, products, and shared facts about our upcoming initial decentralized exchange offering (IDO) launch and $SNE token public sale.
Our IDO is happening with our partners Starter.xyz and Bull Perks. Learn how to participate in our IDO process here: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
Some of the highlights from the StrongNode and Polygon DeFi Chat AMA session are listed below:
Colin Charles: KYC will happen for IDO, yes. We, unfortunately, can’t sell tokens to folks on the “negative list.”
Daniel Saito: Yes, regardless if you want to take part in the StrongNode network you need to register. We can’t let anyone on the network run arbitrary commands. This isn’t why we are making it. This is part of something large but needs to be administered as well, as we will always know you by your wallet address, nothing more.
My experience in crypto was after reading the Bitcoin whitepaper and had my hand in mining early. Additionally bought later in 2009, lost a lot on Mt. Gox.
Does StrongNode Security work with the best security experts in the world?
Colin Charles:
We aim to and, of course, we will Open Source pretty much everything so many eyes make shallow bugs.
Daniel Saito:
Yes, a great use case behind this is the MySQL repository that was curated by Colin and his team at MySQL. Because of the code transparency, security scrutiny is out in the open.
But of course, security overall will be pen tested and audited by various 3rd Party companies, certifying the source code and pen testing the network.
Colin Charles:
To use the edge nodes, we have 2 initial users. One is enterprise data sets that are ideal for map/reduce from AI, Covid analysis to ad clickstream data. The 2nd is end-users who want VPN endpoints/exit nodes on residential IPs.
Daniel Saito:
There are two moving target markets that we are always keeping an eye on. 1) Broadcasters, People that have the requirement for low-cost compute cycles that is readily available. As Colin mentioned, “enterprise data sets that are ideal for map/reduce from AI, Covid analysis to ad clickstream data.” 2) Receivers, Users that want to benefit as a benefactor for participation or end users who want VPN endpoints/exit nodes on residential IPs.
Many blockchain projects exist to sell their tokens, but they don’t solve any real problems. What can StrongNode do to solve the problems we face today in the blockchain ecosystem? And what solution does StrongNode provide?
Colin Charles:
We aren’t afraid to take Open Source and build a crypto payments layer on it. Have you seen the success behind cloud/SaaS? We’re taking that and decentralizing it to some extent.
Daniel Saito:
As mentioned in the previous question, we are taking the successful open source project *Hadoop* and tokenizing the operations. This will allow us to harness computers from various devices, to help reduce compute cycle costs for genome sequencing lab for covid testing.
Think that we will do the same with OpenVPN and other solutions.
I noticed that you very often follow some AMAs. I think this is very good for increasing project adoption. But I want to know, besides the AMA, where else do you promote your project? How do you make your project more popular and widely known by the public?
Daniel Saito:
Well, I do public speaking in person when possible abiding by COVID protocols. We have a publicist and she keeps me busy with press interviews and meetings on messaging. I have a marketing team that keeps my calendar full with activities and talking points to address. Basically, we address marketing as any other consumer focus product would require.
To know more about our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70
Please follow us and join us:
💻 http://strongnode.io
🐤 https://twitter.com/StrongNodeEdge
📚 https://www.reddit.com/r/strongnode/
📞 https://t.me/strongnodechat — Main TG Chat
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
1.5K 
1
1.5K 
1.5K 
1
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/beanfield-metroconnect/mythbusting-cloud-service-fud-6bb4024af047?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
Summer 2018 has arrived and the outlook for cloud service providers has never been better; Q1 IaaS (Infrastructure-as-a-Service) revenue for the big five cloud service providers (AWS, Microsoft, Google, IBM, Oracle) topped $20 billion. Q1 IaaS cloud revenue has outgrown the global server market, even after a surge in server sales in Q4 of last year. Great Managed IT Service Providers (MSPs) have moved with the market, fully embracing change and launching cloud integration practices. But maybe your MSP is still telling you the cloud isn’t secure, and that the best decision for your business is to buy a new server? Your trusted advisor certainly knows your business better than I do, but if your company already has access to metro ethernet fibre, there is a strong possibility you’re being sold FUD (Fear, Uncertainty, & Doubt). Let’s talk about some of the biggest myths and the FUD keeping your business out of the cloud, so that you can have an honest conversation with your MSP about streamlining your IT budget and maximizing performance, security, and efficiency by moving workloads to the cloud.
If your MSP claims Azure, AWS, GCP, or Oracle Cloud are more expensive than colocation or hardware for your office, have them prove it. It’s true that cloud service pricing can seem confusing or convoluted, but the big five vendors have already made TCO (Total Cost of Ownership) calculators and other helpful tools available. Bear in mind that if an advisor is recommending you evaluate cloud services by comparing the cost of new hardware against the cost of utility services using public Pay-as-you-Go cloud pricing, you have a problem. When was the last time you paid the sticker price for a new car? Ask your MSP for quotes from each vendor, and request committed spend discounted pricing for use as a high watermark. The best MSPs have the capability to provide cloud optimization services, detailed reporting on monthly cloud spend, and certifications with at least two or more cloud service vendors. These MSPs are leading conversations about #multicloud and these advisors are vendor agnostic. If your MSP doesn’t fit that description, you might have the wrong trusted advisor. Microsoft, Amazon, and Google are waging a price war and it is a direct benefit to your business. Each provider mentioned above has options for enterprise agreements, workload commit, and preemptible or spot rate discounts. For big data projects and analytics services, your business has the ability to arbitrage workloads between providers to achieve the highest possible cost efficiency. I guarantee your business already has a named account rep with each provider, so introduce yourself — they’re waiting to hear from you!
“… [Y]our business has the ability to arbitrage workloads between providers to achieve the highest possible cost efficiency.”
Vendor lock-in is a myth. Plain and simple. It is one of the biggest concerns we hear from customers evaluating cloud services for the first time or from customers with large committed annual spends with a single cloud vendor. Moving your workloads, data, and users between cloud service providers is seamless and can be done at any time with minimal cost. What’s more, Microsoft, Amazon, and Google have been growing their product catalogues at the speed of light. If your sales reps at these companies tell you their platform can do anything and everything that the competitors can, they’re right. Still, the legacy software specializations of these vendors are relevant, so don’t be afraid to leverage multiple providers and move your data and workloads to different vendors for each application. Microsoft Office 365 Exchange Online will always dominate the hosted Exchange service marketplace, Google’s Google Container Engine (GKE) rules the Kubernetes managed service market, and Amazon, as the first mover, has the deepest product and service catalogue. Of course Oracle Cloud and SAP Cloud have service platforms that offer unparalleled performance for their respective flagship products. Your business isn’t locked in to one vendor, and by leveraging that freedom of selection, your business can save money.
“Your business isn’t locked in to one vendor, and by leveraging that freedom of selection, your business can save money.”
It’s true, cloud services aren’t immune to downtime. Outages happen and can carry serious consequences to businesses. NPR reported that an outage on Amazon Web Services in 2017 cost publicly traded companies $150 million. These high profile events can be misleading and perpetuate the myth that cloud services are less reliable than dedicated colocation or high availability managed services. The very best defence available for unplanned downtime is a solid multi-cloud strategy. By leveraging multiple service providers when implementing your cloud service based disaster recovery plan, you can achieve the lowest possible recovery time objectives (RTO) and recovery point objectives (RPO). The reality is your business cannot achieve comparable RTO and RPO benchmarks in a data centre on its own or with your managed service provider because you simply can’t match or outspend existing investments by Amazon, Microsoft, or Google. Take advantage of their investments and limitless scalability. 100.00% up-time is an achievable service level, but nearly impossible outside of the cloud.
“The reality is your business cannot achieve comparable RTO and RPO benchmarks in a data centre on its own or with your managed service provider because you simply can’t match or outspend existing investments by Amazon, Microsoft, or Google.”
High-profile cloud security breaches of personal credit information, credit card data, and user login credentials are still fresh in the minds of the public. These events may have impacted you personally, but the response to these incidents is driving change and investment. Recently, governments around the world have introduced or enacted legislation requiring greater oversight and protection from service providers, and security protocols to provide users with more control over their sensitive information. Microsoft, Amazon, and Google have continued in 2018 to invest billions more in securing their cloud data centres, but most businesses are unaware of their own responsibilities and best practices for protecting their cloud workloads. What most people don’t understand is that their users are the biggest security threat to their organizations. The best way to protect sensitive data from bad actors is to limit access to applications and SaaS (Software-as-a-Service) services containing sensitive PHIPA information, financial data, personal credit information, user login credentials, and other confidential information to private line direct connectivity services. Private line connectivity can eliminate user vulnerability to phishing or man-in-the-middle attacks. SaaS products containing this sensitive data such as Workday, Peoplesoft, and Salesforce can be secured by limiting platform access to private line direct connections and forcing users outside of the office to access those services over an encrypted VPN connection to your primary network.
“The best way to protect sensitive data from bad actors is to limit access to applications and SaaS sites containing sensitive PHIPA information, financial data, personal credit information, user login credentials, and other confidential information to private line direct connectivity services.”
Finding a trusted advisor who will be forthright and willing to work against their own self-interest on behalf of their customers is a challenge. Some business owners might say impossible. As we have discussed, the FUD holding your business back is either unfounded or exaggerated. The best way for your business to thrive is by embracing cloud services, a multi-cloud strategy, and becoming as capital efficient as possible. Find a provider who isn’t a box pusher, and isn’t afraid to enable digital transformation projects that free capital, provide better performance, security, and resiliency.
The best advisors know that their value to their customer can evolve and change with technology. During my time providing vCIO services to financial service and tech companies in Toronto, I took great pride in solving people’s problems first and worrying about my quota or commission second. I still hold myself to that principle in my new role at Beanfield and work hard to help companies unburden their organizations of server rooms and colocation facilities. vCIOs live and die by their professional track records and reputations, and people remember the moments when they demonstrate that they sit on the same side of the table as their clients. MSPs have a challenge ahead to prove that their business can evolve and support their customers as managed service revenue evaporates. The great MSPs will be great CSIPs (Cloud Service Integration Providers) and continue to deliver valuable services to your business.
The retail data centre is dead, the server room is dead. Long live the cloud!
About the Author:
Daniel Simmons is the Director of Cloud Strategy at Beanfield Metroconnect, a cloud product manager, cloud evangelist, and a solution architect for cloud and data centre services. He lives in Toronto with his partner and dog.
If you think beanfield Metroconnect is like other…
1 
1 clap
1 
Written by
Is the Chief Product & Strategy Officer at Shared Tower.
If you think beanfield Metroconnect is like other telecommunications companies, think again. With a 100% fibre-optic network — that we own, build and operate — we’re solely in control of our network. That allows us to set our sights high. And give your business more power.
Written by
Is the Chief Product & Strategy Officer at Shared Tower.
If you think beanfield Metroconnect is like other telecommunications companies, think again. With a 100% fibre-optic network — that we own, build and operate — we’re solely in control of our network. That allows us to set our sights high. And give your business more power.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/strongnode/ama-with-altcoin-buzz-sne-token-utility-and-user-registration-in-the-strongnode-io-10ea3664aed5?source=search_post---------301,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·5 min read
Infrastructure-as-a-Service (IaaS) tech company StrongNode.io CEO and Co-founder Daniel Saito joined the Altcoin Buzz AMA episode on 23 September 2021. Daniel answered all the questions from the community regarding our project and products. He also shared updates on the upcoming plans for the $SNE token public sale and IDO launch.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
In case you missed it, check out some of the highlights from the StrongNode and Altcoin Buzz AMA episode:
Ani: So in a very simple way can you please explain to me, what are the various components and features of the Edge Node product? How can a Business or a Retail Customer use it? Is it available right now, or do we need to wait?
Daniel Saito: You need to wait.
We are still in development in both the commercial side of things and the community side of things and the product side of things. It is all a coordinated effort.
But the technology fortunately is the easiest part of the whole equation.
We see StrongNode as two products there is StrongNode as an APP and StrongNode as a token $SNE.
We found a problem and created a solution behind it. When COVID broke out genetic sequencing labs were backed up in sequencing and processing lab results for various strains. They were also backed up in computing workflows. We worked with them to create a solution around this and henceforth here we are weeks away from an IDO.
They can use our $SNE token off load their compute workloads off of centralized solutions and process on the EDGE. The folks that want to harvest and mine these datablocks and receive $SNE just need to participate and give their resources, and passively make income.
Our project is open to the novice all the way to über developer as our project strives to be open source. We hope to simply work by installing and onboarding our User Registration to StrongNodeID and we get you started.
We utilize the consumers’ latent resources from your PC / MAC / LINUX [environment] (cpu, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver. The solution was designed with security in mind. We will be bringing all these idle resources [CPU and GPU cycles, Bandwidth, and Data Storage] available on our network. But don’t expect to be available to be online on DAY 1. Building up these resources will take time and there will be a moment on the network where we will have enough resources to be able to have 99.999% uptime. But this will take time for us to achieve — underpromise, overdeliver.
Next Question. How does blockchain come into picture? Which blockchain do you plan to use and for what cause?
Daniel Saito: At StrongNode, we are working to be blockchain agnostic.
A few things to share: We are releasing on POLYGON due to the recent usage and projects that are popping up in the ecosystem (still early, but not as bad). Yes, speed, versatility, scalability and most of all gas fees are critical considerations of choosing the right chain. POLYGON is kinda like that. We considered BSC, but we found that the ecosystem is rather tainted with al ot of noise and most of all really bad projects. I would say the versatility is great because the portability of the contract (with minor adjustments of course), documentation is pretty straight forward… But this is our launching pad, we will then move on to other chains like SOLANA. Like new chains come with new challenges, like Solana uses RUST (luckily, I love RUST). The implementation of how they handle nodes is great and we can see a future there more long term as long as they get their shit together (e.g. last week network reset doesn’t sit well with me).
But we will eventually try different chains, but with each adoption of a chain brings new security vectors that open up. So we will put into a lot of those data points into consideration when scaling out to other chains.
Wonderful! What have been the key achievements of the company till date?
Daniel Saito: There are many milestones that made up this venture to this date. I think first and foremost — attracting the talent behind such endeavor and be unified in a vision of creating a vast network that reaches and helps millions.
Build a working business around it and have the employees reap the rewards. We choose to do the same as that seems to be a recipe for success. The creation of the innovation lab was a genius model, for our plan to get scaled out where we will incubate future projects with great leaders / entrepreneurs behind each team, as they will use the StrongNode infrastructure in a forced adoption model and to help us grow.
Of course getting oversubscribed in our financing as a team was a great milestone, although we knew it would get financed. We took a different approach in getting this financed. We wanted to take a different approach to get StrongNode financed. We financed our project from our enterprise end customers and sold them our token. We are seasoned entrepreneurs that knows that in a startup you need revenue. If the business cannot self sustain itself, it would not be a viable business. This also applies to cryptocurrency based projects. The (problem / solution) that the product is serving needs to account for its own for its own revenue base and not rely on the price of the token to pay for the ongoing of the business (NOTE: it can to a certain extent).
For more updates, visit our website: http://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
222 
222 
222 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://camilorojas.co/video-un-recorrido-por-ibm-cloud-iaas-2-2-dd7409f25281?source=search_post---------241,"Sign in
Camilo
Jul 19, 2018·1 min read
En este segundo video de la serie continuamos la navegación por la consola de administración de IBM Cloud IAAS (antes llamado Softlayer). Revisamos las opciones de storage, data migration, backup, networking, bandwidth pools, seguridad y soporte. Via David Martinez
Interested in AI, ML, Analytics, Startups and a bunch of other stuff
Ideas on tech transformation, #analytics, #ai, #ml, #cognitive enthusiast, techie, #startup mentor, #algotrader, #python, author
"
https://medium.com/@heliossolutionsno/microsoft-azure-markedsandel-er-nummer-to-etter-amazons-aws-i-cloud-iaas-c37d9898ccd4?source=search_post---------286,"Sign in
There are currently no responses for this story.
Be the first to respond.
Helios Solutions
Mar 9, 2017·2 min read
Estimated Read Time : 3 Min
Siden lanseringen i 2010, har Microsoft Azure vokst sikkert og visst i offentlig skyvirksomhet, og er rangert som en nær nummer to til industrileder Amazon Web Services. Selv om Amazon er skyens leder, vokser Azure i et raskere tempo, som får eksperter i industrien til å spekulere i om Microsoft til slutt ta igjen Amazon.Om Microsoft tar igjen Amazon eller ikke, er det av største betydning for bedriften din å ha en fordel i skyen. Samarbeid med en tjenesteleverandør av cloud computing og ta din bedrift til et høyere nivå.
Microsoft Azure ble utgitt i februar 2010 som Windows Azure etter sin kunngjøring i oktober 2008, og ble i mars 2014 omdøpt til Microsoft Azure. Amazon Web Services (AWS), på den annen side, ble lansert i 2006, og denne første-mover-fordelen har betalt seg pent for Amazon. AWS har sikret seg en dominerende plass i skyen med omtrent 30 prosent markedsandel og Azure holder andre plass med om lag 11 prosent. Den totale lagringskapasiteten til AWS sies å være større enn den samlede lagringskapasitet til alle de andre infrastrukturene i skyen, somsier noe om hvor Amazon står i forhold til sine konkurrenter.
Siden Microsoft lanserte Azure, har de nektet å avsløre sine virkelige inntekter. Men Wall Street Journal rapporterer at en analytiker ved JP Morgan har tatt en gjetning på at Azure har trukket inn 2,7 milliarder dollar (ca.) i inntekter i 2016. Dessuten snakket Microsoft om at inntektene i Azure hadde vokst med 93 prosent i løpet av årene i fjerde kvartal, 2016. Sammenliknet var AWS i den aktuelle perioden bare opp 47 prosent. Microsoft har også bekreftet at omsetningen i forrige kvartal skøt opp til 116 prosent.
Som per diskusjonen over ser det ut til at Microsoft og andre utfordrere kommer til å ha vansker med å avsette dagens regjerende leder av skyen, Amazon. Microsoft vil måtte avhenge av strategisk planlegging fremforkortsiktig fortjeneste for å kjempe bort Amazons lederskap, med deres betydelige forskjeller i kundebase, størrelse og inntekter tatt i betraktning. Amazon setter nå trenden i prisreduksjoner og Microsoft har bekreftet at de vil matche disse kostnads rabattene. Og for å forsterke sin skyposisjon taper Microsoft marginer i andre forretningsområder.
Se etter mer ekspert informasjon om Software Utvikling Specialist, Cloud Computing Tjenesteleverandør og Sharepoint Web App Utvikling.
Originally published at https://www.linkedin.com on March 9, 2017.
"
https://medium.com/@joachim8675309/divergence-change-management-118e66c4d508?source=search_post---------131,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joaquín Menchaca (智裕)
Feb 11, 2018·2 min read
This is the least desirable change management method, but likely the most popular in practice.
History of Combating Divergence
Before the arrival of change configuration tools (CFEngine, Chef, Puppet, Ansible, and others), there were a few methods to combat divergence:
In the 1990s workstations were expensive, around $90,000 in today’s dollars, so thin clients (diskless workstations) that used netboot were popular and cut costs. The added bonus from this only one central computer needed to be maintained and configured.
When workstations came down in price, thin computing was less popular. Management of configurations starting becoming important, especially account management. Maintaining consistent accounts across workstations was expensive — 100 users on 100 workstations are 10,000 configurations. This gave rise to login services like NIS, LDAP, or RADIUS. Now if you wanted to use the same credentials across different services, let’s say four, costs are more prohibitive as well, e.g. 100 users for 4 services are 400 configurations, so SSO (single-sign-on) solutions, often involving Kerberos, were all the rave back then.
Early efforts to manage the consistent configuration across systems came in the form of golden master image, where ops maintains a library of images with latest patches, packages, and configurations. But this system is inflexible, which creates headaches managing image libraries for different configurations and versions.
Early stage provisioners like Kickstart or Debian Preseed allowed some flexibility to tailor systems with a simple script, e.g. configurations for developer, email, web systems. In this method netboot is used and the client using PXE boot process installs the operating system. This method can be combined with golden image pattern to automate configuration on an image itself.
Today you can see golden image pattern used for IaaS platforms like AWS, Google Cloud or Azure, local virtualization solutions like Xen, KVM, QEMU, VMWare, Virtualbox, and with tools like HashiCorp Packer.
Divergence: Preferred Choice
This is crazy, yeah, I know, but today, many organizations still practice divergence method, especially when using IaaS solutions like AWS, Google Cloud, or Azure. They start creating resources using the graphical web interface. As the organization grows, it becomes hard to track what was created and why it was created. Costs can get out of control, as well as security, such as controlling access to the infrastructure.
For this reason, it is vital to practice IaC (Infrastructure As Code), where the IT infrastructure is managed through code, rather than using a manual process. The two most popular tools that can manage this are Ansible and Terraform.
Reference Notes
Linux NinjaPants Automation Engineering Mutant — exploring DevOps, o11y, k8s, progressive deployment (ci/cd), cloud native infra, infra as code
1
1
Linux NinjaPants Automation Engineering Mutant — exploring DevOps, o11y, k8s, progressive deployment (ci/cd), cloud native infra, infra as code
"
https://medium.com/sitewards/docs-in-jira-eh-github-mm-git-histories-fuck-yeah-1576cc5a6c39?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
The software stack required to build and deploy a web application in the last few years is … complex. It requires an IAAS provider, Linux kernel, operating system, 3–4 applications specialising in data management, template rendering or business logic and caching of various kinds. In addition, the “template and business logic” components have become enormous. My current project has a codebase as follows:
That’s 4 million lines of code. All of this complexity needs to be maintained, and much of it for many years at a time by large teams. In addition, new features are consistently developed, tested and released and portions of the codebase deprecated and replaced with newer constructs.
This requires pooling of knowledge and coordination among large numbers of people — documentation. That documentation needs to be maintained and accessible by the team of people working on the software for the entire time of the project.
At Sitewards, we have three (or more) different tools for managing projects, each that serve different purposes or audiences.
Jira is our canonical project management tool. It is designed to coordinate the creation of user stories, assignment of stories (or story components) to a developer, testing and merging of those stories into a release and the release of software to production.
This is an inherently complex task involving the project management team, customers, developers, QA team and perhaps others, and is the central place where decisions are made around a project.
Well, at Sitewards we use BitBucket — but the process is the same. Bitbucket is used as the version control management tool, and is the primary way we coordinate the actual code changes made to projects. It’s used for creating branches, running pipelines but most importantly (in terms of documentation) submitting pull requests.
There is invariably much discussion on a pull request. Different developers have different thoughts on how each thing should be implemented, and what to them is important. Pull requests are usually approved by someone, and merged into the mainline.
At Sitewards, we use git for version control. Git stores the history of all changes made to a repository as far as the codebase was maintained by our, or ideally the previous team.
Perhaps most critically in terms of documentation, it requires that each change is documented as it is applied to the code base. Termed a “commit”, the change is record with the date, time, author and some notes about what the changes were.
Given that we have three different systems (at least) in which we might record project information, it begs the question — where do we go to look it up?
In my mind, the answer is and always should be: git
Git has a number of advantages over the other tools which make it superior as a mechanism to track information:
Version control is a fundamental part of our development workflow here at Sitewards. Additionally, if GitHub is any proxy it is fundamental to vast chunk of all software development. Indeed, it’s rare that you will find an open source project not available by git, or at least some version control.
This means that the documentation of git is additionally attached to the code. Further, it is even attached directly to the line of code that changed with that commit. Developers can look up documentation right next to the code that they are attempting to understand.
Git commits are, as mentioned, attached to the code. This means that wherever the code is checked out with version control, the documentation is additionally checked out. There are no dependencies on third party tools, nothing locked behind organisational knowledge bases or ticket systems or simply defunct internet servers — at worst, developers can run git log or git blame on systems to read the relevant documentation.
No matter what other changes happen organisationally, or even as a project gets transferred between organisations the documentation attached to git commits will remain.
Because git has become the defacto standard version control management tool and writing this documentation is a requirement as part of using the tool, it has become extremely well supported by third party tooling. GitHub specialises in the easy management of the sometimes opaque git (pictured above) but it’s also accessible by almost all IDEs, text editors, browsers etc.
Further, some platforms have standardise overloading it’s plaintext format such that, like Markdown, it still is pleasant to read when the overloading is not present however the overloading provides useful context when viewing in certain platforms.
Lastly, there exists a large amount of tooling that provides additional context around git (such as viewing the “subway” of the merge history) which are made even more useful by this documentation.
Embedding documentation in a git commit is a requirement of git. Git is usually configured to simply reject saving work without including some form of message — even if that message is a somewhat unhelpful fixed bug.
There are few other systems that are as consistent as git. Jira, for example, does not require appending a comment with each change of ticket status. Bitbucket doesn’t enforce replying to comments. But git requires documenting with each addition to the codebase. Developers are required to express something, and wedging additional information there is not super difficult.
Accordingly, git will usually be the most up to date documentation.
Given that it’s a requirement of the essentially defacto way to manage software, it’s also become reasonably well understood. Developers are commonly used to operations like git log to read a linear history of the project, or git blame to look for context given a particular file change.
Unfortunately the power of commit messages isn’t entirely well understood. It took me quite some years of experience and finding unhelpful commit messages while debugging issues to become as evangelistic as I have about providing context in git commits. However, hopefully this post helps!
Unfortunately there are some things this form of documentation is not suitable for. General project steps such as how to deploy, what the architecture and constraints are, what the intent is over time — this is hard to express in git’s linear format.
Luckily, git allows us to save that documentation as files. Simply write it in docs/${FILE}.txt and commit it. This pattern is well established with the README.md or CONTRIBUBTING.mdpattern, as well as in the Linux kernel with documentation the same directory as the code.
There are many different tools that we must keep up to date as we are coordinating a project with all stakeholders. But in my mind, there is only one place which must reflect an accurate summary of all the tools — the same place that manages the actual things being built.
git.
The agency for valuable and sustainable digital commerce projects.
2 
2 claps
2 
Hello, we are Y1. The agency for valuable and sustainable digital commerce projects. https://www.y1.de/
Written by
See https://www.andrewhowden.com/
Hello, we are Y1. The agency for valuable and sustainable digital commerce projects. https://www.y1.de/
"
https://saasholic.com/saas-iaas-paas-haas-as-diferentes-faces-do-as-a-service-infogr%C3%A1fico-c3cd842c5b34?source=search_post---------194,"Originalmente publicado no blog da Positivo.
SaaS’s estão cada vez mais presentes no nosso dia a dia. A nuvem facilita muita coisa e vários outros serviços podem ser disponibilizados através dela.
O que significa, claro, que não pararíamos em softwares, né?
Nesse post bem legal feito pelo pessoal da Positivo, eles explicam outras ramificações do “as-a-service” e como usamos no nosso dia a dia.
Recomendo fortemente a leitura do post com tudo o que você precisa saber sobre SaaS que acompanha esse infográfico!
© 2022 SaaSholic. All rights reserved.
"
https://medium.com/@JacobsEdo/scalable-stochastic-model-for-iaas-cloud-computing-platforms-4e13f4448046?source=search_post---------290,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jacobs Edo
May 15, 2019·5 min read
Gartner defines cloud computing as a style of computing where scalable and elastic information technology-enabled capabilities are provided as a service to external or internal customers using internet technologies(Gartner, 2019). The above definition also articulates some of the critical characteristics of cloud computing — standardization and automation, virtualization, rapid…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kelvinkok2000/the-rise-of-function-as-a-service-2265bcf051b2?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kelvin Kok
May 11, 2017·2 min read
We started from Infrastructure As A service (IAAS), where you are charged for how long does your VM run to Platform as A service (PAAS), where you can forget about the infrastructure and focus on building your logic for your business. Now we are talking about serverless architecture for Function As A Service (FAAS), where you can run your application like a function. And you are only charged when your function is executed.
Cool… right ? Imagine you run a website, today regardless of whether you are running PAAS or IAAS you have to pay your cloud provider even if you do not have customers that are using your application. In this case the serverless it means you don’t have to own any VM or instance to run your application.
With serverless architecture, the days of spending thousands of dollars for servers are over. Regardless of whether you are running AWS Lambda or Azure Functions. You are only charged when your customers access your application, because you are charged by how long the application runs when it is triggered.
Serverless architecture removes the need of ‘always on’ system for your application and can reduce your costs significantly
The next benefit is about scaling. Building a scaleable application is key in this digital world. For IAAS and PAAS you have to worry about scaling, you need to configure rules to scale your application according to the traffic.
Scaling is transparent in serverless architecture
This also means that you focus on your business logic and let cloud provider to worry about infrastructure and scaling problem.
In addition, this enables you to build event driven functions. For example, if you want to create image thumbnail for any image that you uploaded. Last time user has to wait for image uploading and thumbnail creation. And it is not hard to imagine this piece of code might be repeated countless time in an organization for multiple applications.
With serverless architecture you can write a small piece of code that create thumbnail. For AWS context, you will deploy the code on Lambda and when image is uploaded to S3, this Lambda code will be triggered automatically to create thumbnail. The same concept applied to Azure Functions and Blob storage.
This is all about creating function driven code and event trigger code
The real excitment come when you can wrap this function as a API endpoint in your application. If you are using Azure Function, you can use Function Proxy to do it. For AWS Lambda you can do it using API gateway.
With the same function now you can use it internally and expose it as a API endpoint for your application to create a microservices environment.
These lead the discussion to microservices, instead of building big and monolithic. We should build application that is loosely coupled, small modular and independently deployable.
This simple means with microservices you can ship product with higher quality in shorter time to market.
Let’s embrace the application paradigm shift and build your application using serverless architecture.
Cloud architect with passion in emerging technologies and digital transformation. The posts are my personal opinion on technology.
14 
14 
14 
Cloud architect with passion in emerging technologies and digital transformation. The posts are my personal opinion on technology.
"
https://medium.com/@shubhaat/define-the-cloud-computing-saas-paas-and-iaas-4322b82cc2e2?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shubha A T
Aug 18, 2017·1 min read
I went looking for answers, sometimes its about putting a precise definition on things … and answers I got. So for those of you who often use the word Cloud computing, SaaS, PaaS or IaaS in your work but can’t define it, these resources might help.
The thoroughness monster in me feels happy now!
Foodie | Bookworm | Traveller | Mobile and Internet Enthusiast | Passionate about Education | Not your usual girl
See all (436)
55 
55 claps
55 
Foodie | Bookworm | Traveller | Mobile and Internet Enthusiast | Passionate about Education | Not your usual girl
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/as-cloud-usage-keeps-rising-can-iaas-continue-to-be-a-zero-sum-game-95a26936a68f?source=search_post---------17,"This is a reprint (more or less) of the ARCHITECHT newsletter from April 15, 2018. Sign up here to get new issues delivered to your inbox.
Analyst firm Gartner released a report this week predicting that spending on cloud computing services will increase 21.8 percent in 2018, to $186.4 billion. The infrastructure as a service portion of that will grow at an even faster clip, up to $40.8 billion from $30 billion in 2017. In fact, Gartner’s estimates have IaaS as the fastest-growing of cloud services, eventually reaching $83.5 billion by 2021.
It’s not surprising that cloud usage and revenue are growing, but just how fast they continue to grow is notable. I find this particularly interesting in the context of the public fight happening right now among cloud providers vying for a multi-billion-dollar Pentagon contract — a contract that Amazon Web Services’ competitors say heavily favors that company winning the entire contract. (A collection of links to new stories on this is below.)
Even taking into account the bias inherent in comments from AWS competitors, I do think there’s some logic in the United States not wanting a single point of failure for something as sensitive at defense data and workloads. Nor would I blame a cloud provider for not wanting to take on that much risk all by itself, despite the size of the deal. (But, I guess I also wouldn’t blame a cloud provider for wanting that money, being confident it could handle the job, and making the argument that trying to integrate systems across multiple clouds could add unnecessary complexity.)
Here’s the latest in that controversy:
On a broader scale, though, this whole situation begs the question of whether cloud computing contracts — especially IaaS contracts — should be viewed as a zero-sum game, where there’s only room for one provider. At least from the customer perspective, there are plenty of good arguments that they shouldn’t be, including an avoidance of lock-in and the benefits around cost, flexibility, latency, performance and resiliency that can come from decentralization.
Furthermore, with the ever-shifting currents of corporate muck-ups and public outrage — and cloud-provider entries into new industries — pressure to cut ties with a company like Amazon, Google or Microsoft could come from anywhere, and some companies might actually want to be in a position to do it.
(Speaking of Facebook, I thought these two stories — “AI is an excuse for Facebook to keep messing up” and “Facebook uses artificial intelligence to predict your future actions for advertisers, says confidential document” do a good job capturing its awkward relationship with artificial intelligence and user data.)
I don’t suspect large cloud providers will jump on the cooperation bandwagon anytime soon (although some smaller providers are) because there’s just too much money at stake to go for the kill with every deal. But I do think we’re going to see (well, more than we’re already seeing) savvy buyers start forcing the issue themselves through clever architectures and tooling, like container orchestration platforms, that can abstract infrastructure and open the door for better third-party applications.
Because if there’s one thing the Pentagon-contract controversy is underscoring, it’s that cloud computing is no longer a boutique business or the alternative to cumbersome legacy IT processes. Cloud computing is IT, and the vendors leading that industry are larger, more powerful and more all-encompassing than anything we’ve seen in decades of enterprise computing. The pros and cons of this situation haven’t been fully realized yet, but a little freedom of cloud choice might go a long way as they become more clear.
Derrick
Datadog integrates seamlessly to gather metrics and events from more than 200 technologies, including AWS, Chef, Docker, and Redis. With built-in dashboards, algorithmic alerts, and end-to-end request tracing, Datadog helps teams monitor every layer of their stack in one place. But don’t take our word for it — start a free trial today & Datadog will send you a free T-shirt! Visit here to get started.
datadog.com
(1) It’s a good week for Israeli AI startups, as Nike just acquired one, too. (2) It’s telling that tech companies have stopped acquiring/investing in all the AI startups and it’s now large traditional companies doing it. (3) If there really is going to be an AI talent crunch (which I’m not sure will be true in the long run), this trend will only exacerbate it for companies without investment cash to spare.
thetower.org
This post addresses the AI talent shortage I mentioned above. I’m less sure there’ll actually be one, though, because it’s so easy to get started learning and experimenting with these technologies right now. Even as the cutting edge keeps moving, there’s still a lot of mileage in tried, true and well-understood techniques that most companies haven’t even begun to implement.
zdnet.com
A great proof point for the power of the technology, but probably also quite worrisome if you’re into privacy and/or civil rights. Either sci-fi predictions are going to come true, or there will be a major reset of our our relationship with technology.
thenextweb.com
This is definitely where the chip world is headed, but the big challenge for startups will be competing with larger hardware competitors, and also the move to implement AI on existing edge chips.
graimatterlabs.ai
Google wants to make it easier to search through books by asking natural language questions about their content, and surfacing the right passages.
googleblog.com
This is interesting work from Microsoft, and worth considering in the context of general intelligence or even how smart we can actually make our homes, offices, etc. The systems involved here are distinct and complex, so proper integration is key.
microsoft.com
architecht.io
I am really curious to see where Cloudflare ends up fitting in the grand scheme of cloud computing and IT over the next few years. With Spectrum, it’s protecting and encrypting corporate systems beyond just website or web apps. Coupled with its new edge JavaScript service, there’s a lot of interesting opportunities.
wired.com
RISC-V again. This could make for a fascinating future in terms building highly optimized systems and devices, but hardware can be a trickier fix than software when something goes wrong or a company goes kaput.
wired.com
This is a cool case study in combining IoT with serverless computing to help power smarter cities.
googleblog.com
Yes, monitoring of cloud services. Someone has to do it, and while most companies are still transitioning to the cloud, there’s still a lot of opportunity in the monitoring space.
geekwire.com
About halfway down, this topic is discussed in some detail: “[W]e have to face a core reality in front of us that we are growing out of our existing data centers and need to move functions into the cloud. We refer to this strategy as a “tripod” because it consists of two clouds and one data center on our premises.”
uber.com
architecht.io
Seriously, GDPR is wider- and deeper-reaching than we all might realize. It’s definitely worth thinking about how it will affect data systems and pipelines, which on the surface might seem to have little to do with privacy but play a major role in how data is stored and moved.
danlebrero.com
This is a good, if not prescriptive, analysis of the challenges and opportunities facing Cloudera, but also its big data peers. Between cloud and AI/ML, there’s a lot of business to be had, but also a lot of other folks competing for it.
redmonk.com
I haven’t followed Couchbase too closely over the past few years, but this seems like a continuation of something it’s been working toward for a while. Not sure how business is going there, but this seems like a good point of distinction.
adtmag.com
FYI … Actian has pretty broad range of products, now owned by an Indian outsourcing provider HCL.
theregister.co.uk
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
57 
57 claps
57 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@arjun.bahree/a-z-of-patch-management-on-microsoft-azure-iaas-998179b13d21?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arjun Bahree
Aug 31, 2018·11 min read
Exploring Patch Management Strategies for Cloud IaaS, specifically Microsoft Azure.
For all Cloud IaaS deployments, having a Patch Management process is essential. It is as Important as Patch Management process at your on-premises DC.
Why do we need Patch management?
There are many compelling reasons, like:
In this post, we will look at Patch Management for Cloud IaaS deployments, specifically on Microsoft Azure, and for Windows Server-based Azure VMs. We will not specifically cover Linux based Azure VMs here, but same base guidance would apply to them equally.
However, what we discuss here would equally apply to any other Cloud IaaS platform like AWS or GCP. Though we will occasionally reference the traditional on-premises Patch management process, wherever required.
Fundamentally, Cloud IaaS model is a virtualized abstraction of physical Infrastructure. It is built on underlying clusters of physical host servers of various capacities/capabilities. The responsibility of patching these underlying physical host servers rests with the Cloud provider. In the case of Azure, Microsoft holds this responsibility.
However, for VMs provisioned on the Cloud IaaS layer, VM maintenance is the sole responsibility of the customers. This model of shared responsibility is the same across all Cloud providers, like Azure, AWS, GCP etc.
Now let’s focus on Patch Management from Microsoft Azure perspective.
Microsoft does regularly update VM Images they have published in the Azure Marketplace, with latest patches. These Images are thoroughly tested for stability, before being published in the Marketplace. However, Microsoft does not make the frequency/schedule public for updation of these VM Images. Hence, whenever you create a new VM based on an Image from the Azure Marketplace, you would be lucky if you get one which has been just updated with latest patches. That will save you from applying any additional updates (rare chance). In nearly most cases, depending on how far did Microsoft update the Image, you will have to download a larger/smaller delta of the applicable patches.
Given that both Windows Server OS and Azure Platform are Microsoft products, would have been Ideal if Microsoft had a native automated patch management service in Azure.
However, Microsoft does not currently have any full-featured standard Azure based native service offering for Patching/Update management. At best what they offer is a revised “Update Management” solution through Azure Automation, which is linked to another external Microsoft service called Microsoft Operations Management Suite (OMS). This new Update Management solution collects Updates related data from all the VMs (Windows/Linux) deployed in Azure and/or On-premises (Hybrid setup) through Microsoft OMS agents installed on those VMs and pushes that data to OMS. Thereafter, you can use OMS to monitor the Update status of the monitored VMs to see which ones are missing any updates and push Installation of those missing Updates unilaterally. However, OMS based Update management solution currently misses many critical features/capabilities essential for a good Update/Patch management solution and is a no-go option for any production IaaS deployments.
With the latest change of Integrating Microsoft OMS into the Microsoft Azure portal, nothing major has changed whatsoever in terms of Patch Management functionality.
Microsoft still expects customers to either manually do the patch management themselves (using native tools like WSUS, MBSA, PowerShell etc.), or use commercial patch management systems. This strategy does not make things any easier for customers. However, it does Indirectly benefit from promoting an ecosystem of ISVs, who build such products to be sold commercially.
Organizations considering either migrating their existing on-premises workloads to Azure, or building net new Cloud Infrastructure will necessarily need to consider having a Cloud Patch management process.
Let’s look at the following step-wise approach an Organization should consider, for establishing a patch management process on Azure (or any Cloud IaaS for that matter):
Stage 1: Prepare Patch Inventory
You should first create a Patch Inventory, which should capture the following information for your IaaS deployment:
Additionally, you should also prepare another related Inventory for production VM’s in your environment, which should capture the following information:
These Inventory Items should be regularly updated on a predefined frequency, which will depend on the patching cycle you may want to follow. Inputs for this Inventory will also come from later stages in the Patch management process, like from Patch Testing stage.
The above-listed Inventory data points are not absolutely exhaustive but should give you a fair Idea on what level of Inventory you must have, before embarking on Incorporating a patch management process on Azure.
Stage 2 — Perform VM Baselining
Baselining VMs refers to building an initial stable configuration of the VMs, established at a specific point-in-time. This means that the VM Server OS, Application Software(s) Installed within, and any Initial configurations done on either of these, are thoroughly tested, found stable, and standardized for being used as a base VM configuration. Baselining VMs enables us to reliably restore them from any future state to a previously stable state, and helps probing/rectifying any potential problems with a later version. It also helps to minimize amount of patches/updates we need to deploy on the VMs as well as gives us an ability to monitor compliance at a granular level.
For baselining Azure VMs, you should consider following the high-level process:
Stage 3 — Discover Patch Notifications and Repository Channels
Next, you would need to discover and setup channels for getting regularly notified on new patches for the VM Server OS/version and Application Software(s)/version installed within.
You will also need a remote repository source/mechanism to download these patches on an Update server (where they will be tested first against VM Asset categories), through an automated mechanism preferably.
For the Windows Server OS running on Azure VM, and any other Microsoft Application Software(s) Installed within, you can get regular notifications through Microsoft Security Bulletin Service from Microsoft Security Response Center (MSRC). You can then automatically trigger a download of these patches/updates through existing native services/tools (like WSUS, MBSA etc.)
However, for non-Microsoft Application Software(s) installed in the Azure VM, this will vary greatly, and will depend on existing update notification channel for those Software vendors (if they exist, what frequency they operate on, and in what form) as well as downloading mechanism.
Stage 4 — Setup Patch Management System
After you have discovered and setup patch notification and repository channels, next step would be to look at setting up a patch management system.
Before you move forward on selecting a patch management system, you should:
There are a number of tools/solutions available for Patch management, few from Microsoft, and several from commercial vendors. Some of these tools/solutions support only Windows Server OS, and others also support Linux Server OS. You could use either of these tools/solutions for your Azure IaaS environment. However, your choice will depend on factors like Implementation efforts, time, cost of deployment, licensing, support options etc.
Few such popular tools/solutions are listed below:
Some of these tools offer limited support for a few stages detailed in this post, but none of them supports the whole defined process end-to-end.
Stage 5 — Patch Testing & Authorization
You need to establish a mandatory Patch Testing process as part of the overall Patch Management process. Let us look at why.
Imagine a scenario, where you apply a new patch on one or more VMs in your Azure IaaS environment. You then discover that suddenly one or many things stopped working. Maybe you are unable to RDP into the VMs, or Installed Application starts misbehaving, or a host of other problems surface. These are some of the many common Issues, which frequently occur when you don’t test patches before applying them in production VMs.
Testing any patches, before applying on production Azure VMs is always deemed a mandatory step you will need to rigorously follow. Not doing so may lead to very serious Implications for your deployment.
However, misses do happen in real life, and few untested patches may very well make their way to production Azure VMs. Also, If the testing process is not thorough, problematic patches can easily escape undetected to production, causing Issues.
When untested patches make their way to the production environment, they may fail and also break the current configuration/operations of the VMs. Your patch management process should have the ability to rollback and restore those Azure VMs to an earlier restore point. Not being able to do so can seriously compromise the intended functioning of the concerned VMs.
For VM rollbacks to be possible, you need to be already performing regular backups of your Azure VMs. Couple options for taking backup are through Azure Recovery Services Vault and through System Center DPM.
All patch testing activity should be recorded in a separate testing repository, and should reference/record against the existing Patch Inventory from Stage 1.
Depending upon if a patch passed or failed during testing, an authorization status should be assigned to it in the Patch Inventory. This authorization status will determine if a patch is ready to be applied to the target VMs (or VM Asset categories), or needs to be deferred for future testing, or rejected.
After successful authorization of each patch, you also need to assess and record the Impact it will have when applied to an Individual VM or a VM Asset category in your deployment. The possible impacts could be like forced downtime, dependency on other patches/components, the order of applying etc.
As a final step, each patch will need to undergo an approval process, based on justification you give on why is it Important to be applied to the production servers. This Information will also get captured in the Patch inventory.
Stage 6 — Patch Monitoring
Once you have the Patch testing process setup, you will then need to set up a Patch monitoring process. Here you will need to regularly probe all your Azure VMs to identify the following:
Once you are able to get above Information, you will need to compare that against the list of authorized/approved patches in Patch Inventory. This way you will be able to find out which patches need to be applied/reapplied, where, when, and in what order. Thereafter, you can schedule for their manual/automated deployment accordingly.
Additionally, you should consider performing the following activities on a schedule as a part of patch monitoring:
Before concluding, I would like to address one of the common questions asked by many about Patch Management on Azure (or any Cloud Whatsoever):
Why don’t we enable auto-update on all Cloud/Azure VMs, and let them update themselves whenever the need be? Windows already has this mechanism of Auto Updates, and same can be scheduled similarly on Linux too. If any updates fail, we can always restore from the Backups, isn’t it?
The answer to this questions is that never should we allow auto-updates to happen on Windows or Linux servers in production, whether on-premises or on Cloud. If we do, we expose our production deployment to a Huge Risk as anytime an update related failure may occur, rendering our production environment unusable. This practice of disallowing auto-updates is mandatorily followed by most Orgs across the world, for both their on-premises and Cloud deployments. You could maybe enable auto-updates in a dev/test environment because there is minimal Impact there.
Furthermore, all good Infrastructure deployments in the Cloud or on-premises will either never give VMs direct access to the Internet, or only give restricted access secured behind proxies/bastions/WAF’s. So enabling auto-updates over the Internet would anyways be not available.
Hope you found this post useful/helpful. In case of any questions/suggestions/thoughts, please feel free to comment.
Technology Architect::Developer::Advisor | Infrastructure::Applications | Cloud::Containers::Automation | PowerShell::C#::Python::Node::Go | Windows::Linux
Technology Architect::Developer::Advisor | Infrastructure::Applications | Cloud::Containers::Automation | PowerShell::C#::Python::Node::Go | Windows::Linux
"
https://medium.com/@ashayasharma/pre-production-the-5-paas-vs-iaas-battle-274e4c8078b6?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ashaya Sharma
Apr 25, 2017·3 min read
This past November, Amazon Web Services orchestrated a fundamental shift in how consumers think about the development of early pre-production applications and school projects with the announcement of Amazon Lightsail — a low cost virtual private server starting at 5$ [1]. This triggers two important questions: Firstly, why not use Virtual Private Servers as IaaS (Infrastructure as a Service) as opposed to PaaS (Platform as a Service) solutions? Secondly, why not seriously consider LightSail as an alternative to the infamous Digital Ocean in this space? I am often put in the position where I should decide what technologies to pursue both immediate and long term.
It is no surprise that almost everyone thinks cloud first when thinking about development and that debate has already been finished. No one hosts their own servers anymore. PaaS solutions such as Heroku have flourished in recent years for their simplicity as a platform that can be deployed to, hassle free, with very limited overhead for a reasonable cost (7$ for a Hobby Dyno) [2]. With additional add-ons such as database packages, this cost could bubble up to close to 20$ — the price of a third tier LightSail instance or Digital Ocean droplet offering much higher performance capabilities.
One of the major reasons that PaaS is so enticing to developers as an immediate build is its ease of deployment. However, with LightSail and Digital Ocean so easy to spin up and have a VPS running in ~2 minutes, this argument is slowly losing fruit. To get a service working on Heroku, you must nitpick parts of your design to comply specifically with their requirements. Researching up their requirements takes about as much time as learning how to prepare and install the necessary tools to get up and running on Ubuntu on a VPS. Certain system utilities (network modules for example) do not even work once deployed on Heroku. Recently I planned to deploy a project on Heroku and after deploying had to switch to a VPS because I was not getting the desired behavior.
I was initially a skeptic of LightSail at first too and Digital Ocean offered the same services for a long period. Their feature lists are virtually identical. This article is not meant to be a benchmark gladiator fight (see Josh Sherman’s article for that) however, I spun up two instances of each of them and compared some benchmarks repeatedly and had the following averages on the same benchmark tests relative to a base. Depending on the specific goal of your application, you might pick one over the other but in a head to head battle, LightSail was impressive.
My great grandfather told my father to “always search for a shade under the biggest tree.” Say your business grows and you must scale up. Fast. Your service is enveloped by the rest of AWS, and can seamlessly be integrated into much more complicated clusters. Other services can not hold the bragging right of being under the umbrella of the leading corporate cloud service platform.
I am not an employee of any of the companies mentioned above and neither but as a consumer who has considered these alternatives, why not let our applications sail with Lightsail? It is seriously worth a look.
1 
1 
1 
"
https://medium.com/@manningbooks/provisioning-the-autoscaling-group-370a547985a9?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Apr 30, 2020·8 min read
From Terraform in Action by Scott Winkler
This article discusses provisioning the autoscaling group and other, associated services.
__________________________________________________________________
Take 37% off Terraform in Action by entering fccwinkler into the discount code box at checkout at manning.com.__________________________________________________________________
This article is about provisioning the autoscaling group, load balancer, IAM instance role, and everything else that the web server needs to run and serve up a healthy application. The inputs and outputs of the autoscaling module are illustrated by figure 1.
Like the networking module, the autoscaling module provisions a lot of resources. These are depicted by figure 2.
From figure 3, it’s clear we need to inject three additional variable values: vpc, sg and db_config. The first two come from the networking module but the last comes from the database module. The way data bubbles up from the networking module and trickles down into the VPC module’s shown in figure 3. I’m only going to show this one, but the other data values are passed similarly.
The revised code for main.tf in the root module is shown in Listing 1.
Listing 1. main.tf in root module
#A input arguments for the autoscaling module, set by other module’s outputs
The input variables of the module are used to set the variables in variables.tf. First, create an autoscaling directory under ./modules, then put in four new files: main.tf, variables.tf , outputs.tf and cloud_config.yaml. The last one is a template file. Template files don’t need to end in “.txt”, and I generally use an extension that makes what the template file is clearer. Listing 2 presents the code for variables.tf, in the autoscaling module.
Listing 2. variables.tf
#A Enforcing a strict type schema for the db_config object. The value set for the variable must implement the type schema
The infrastructure in this article is trickier than other two modules. We’re going to be using an autoscaling group behind a load balancer, with a launch template for startup configuration. I like to draw a rough diagram of the dependencies between resources and modules before I start writing any code. An initial dependency diagram I came up with is depicted in figure 4.
I find these sorts of sketches to be useful when planning out the code of a Terraform module. Even after my code is written, I find them to be much more helpful than the diagrams generated by terraform graph at visualizing my infrastructure. Whenever I plan out the code for a Terraform module, I always consider inter-resource dependencies (i.e. what depends on what) because it helps me predict potential race conditions that require an explicit depends_on. These sketches are often incomplete or outright wrong, but they provide a starting point from which to work from. After we’re finished writing the code in this section, we’ll compare what my initial dependency diagram looks like versus what the final dependency diagram looks like.
Another thing I like to do is write the Terraform code from top-to-bottom, in a way that matches how my dependency diagram look like; i.e. resource having fewer dependencies are put at the top of the file, and resources having more dependencies are put at the bottom. You can reason that resources at the top of the file are created before the resources at the bottom of the file, kind of like “normal” procedural code. Technically you don’t need to do this, because Terraform optimizes and organizes resources for you when it generates an execution plan, but I find it helpful anyways. In particular, it helps me understand what my code’s doing and anticipate how it will behave at runtime. Many other Terraform developers in the community l do this without even realizing it, because it’s such a natural way of thinking. Now that we’re finished with the detailed planning, let’s start writing the code.
Remember how I said that templates are useful for init scripts? Here is a case in point. Listing 3 showcases the code for creating a launch template based on some hardcoded input configuration, as well as an IAM instance role and a cloud init configuration.
Listing 3. main.tf
#A A module I published for creating iam_instance_profiles based on a list of permissions
#B rds:* is too open, you don’t want to do this in production
#C Content for the cloud init configuration comes from a template file
#D Specifying implicit dependencies between 1) iam_instance_profile, aws_ami, and template_cloudinit_config and 2) aws_launch_template
Notice that the cloud init configuration is templated using the templatefile function. This function accepts two arguments, a path to a template file named cloud_config.yaml, and a variables object referenced from var.db_config. We use the special interpolation variable path.module to get a reference to the relative filesystem path. The result of this function is the configuration which the web server needs to be able to connect with the database when it starts up. The code for cloud_config.yaml is shown in Listing 4.
Listing 4. cloud_config.yaml
#A the content of this file’s templated by the db_config object
#B downloading the web application code and starting the server
This is a pretty normal cloud init file. All it does is install some packages, create a configuration file (/etc/server.conf), fetch the application code (deployment.zip) and start the server.
Finally, we’re ready to add the code for the launch template, autoscaling group and load balancer to main.tf. The code in Listing 5 shows how to do this. Don’t worry too much about what the values mean, these can all be found in the AWS provider documentation, or the module README.md. Most are default or “safe” values chosen for the purposes of this exercise. Instead, pay attention to how data flows from other modules into the resources and modules declared here. This is what I mean by “trickling down”.
Listing 5. main.tf
#A Using the namespace variable to prevent resource name collisions
#B The autoscaling group always uses the latest launch template version
#C Security group gets set here after traveling from the networking module
WARNING: Exposing port 80 over HTTP for a publicly facing load balancer is unacceptable security for production level applications. Always use port 443 over HTTPS with an SSL/TLS certificate!
Now that we’re done, we can draw a new dependency diagram based on what exists Our new dependency diagram looks more like figure 5.
The discrepancy between how you plan a module versus what you end up with is a common occurrence when working with Terraform. Does it matter whether the autoscaling group registers itself with the load balancer or the load balancer does the registering? I’ say no, it doesn’t matter because its Terraforms’ job to manage dependencies, not yours. As long as the code does what it’s supposed to and it’s organized in a way which is easy to understand, you’ve done a great job.
TIP: besides organizing code top-to-bottom based from least to most dependencies, you can also organize code by grouping related resources together in the same file with a multi-line comment block header describing the groups purpose.
Lastly, there’s an output of the module, lb_dns_name, which we need to include. This output’s used to make it easier to find the DNS name after deploying and it’s bubbled up to the output of the root module. Only outputs from the root module show up in the command line after applying. Listing 6 has the code for outputs.tf.
Listing 6. outputs.tf
We can make this output available from the root module by adding another output value to passthrough the data.
Listing 7. outputs.tf in root module
That’s all for this article. If you want to learn more about the book, you can check it out on our browser-based liveBook reader here and in this slide deck.
Follow Manning Publications on Medium for free content and exclusive discounts.
See all (32)
1 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
Follow Manning Publications on Medium for free content and exclusive discounts.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@marknca/keeping-your-sanity-securing-iaas-paas-and-saas-cloud-services-4b7467f24631?source=search_post---------37,"Sign in
There are currently no responses for this story.
Be the first to respond.
marknca
Mar 2, 2016·7 min read
In most organizations today, cloud services are a fact of life. Whether you’re deploying and managing servers in the cloud, building on top of a globally distributed platform, or consuming constantly updated services, the cloud is a fundamental part of your IT service delivery…whether you know it or not.
And why wouldn’t you move to the cloud? The business advantages are clear. You can great reduce the time to deploy new services, reduce your operational burden and costs, and rapidly iterate on new ideas.
There are security advantages as well.
It may require a cultural shift in your organization to accept extending trust to your cloud service providers (CSP), that trust is well place. Top tier CSPs understand that they live and die on their reputation. It’s in their best interests to deliver a secure service to you.
But that’s not to say that you don’t have responsibilities for security when using cloud service. All cloud services (regardless of SPI model; IaaS, PaaS, or SaaS) use this simple model.
Of the main areas of security, the CSP is always responsible for;
Depending on the service, you may be responsible for securing the;
And you are always responsible for;
Put these areas together across all three SPI methods and you get figure 1, “Shared Responsibility Model”.
Looking at cloud security in this manner brings clarity. You can take each type of service (IaaS, PaaS, SaaS) and apply reasonable security controls in order to fulfill your day-to-day responsibilities
It’s important to note that we’re talking about day-to-day responsibilities here. You’re always responsible for the security of your deployments. However you delegate some of the day-to-day work to your CSP. In these cases, you have to trust but verify the work your CSP is doing.
When dealing with IaaS, most of the controls you are used to from the datacenter are still applicable. They’re just delivered in a different manner in order to optimize for the attributes of a cloud environment.
You see this with controls like intrusion prevent and filtering. Traditionally gateway controls, it is now much more effective to deploy them directly on an instance or virtual machine. This maintains the scalability and flexibility of the cloud without sacrificing security.
Platform deployments can be tricky to secure because of how intertwined your application is with the platform itself. This is a service type where secure design, a strong understanding of the CSP’s role, and programmable security controls are critical to a successful, secure deployment.
Securing software delivered as a service is typically accomplished using a combination of a CASB (cloud access service broker) and configuring the native service controls in order to meet your security needs.
While the plan for securing each service type is clear, the pace of change in this space is a major challenge.
Cloud services (of all types) are readily available. It’s never been easier to stand up a new application or service.
This rapid pace of innovation is a huge boon to business. IT is finally a consistent enabler within the organizations.
The challenge is for security to keep pace.
Innovation in at an all time high in the security space but even with current levels of investment and effort, it’s difficult for security controls to keep pace with the new services being developed.
This rapid pace of change is leading to more and more security solutions being required to properly secure the vast number of services that each organization is using.
The average organization uses a lot of services. Ok, I’m sure there’s an actual number but it’s hard to nail down. Depending on the source, the average is somewhere between 5 and 700. So let’s settle on “lots”.
Solid guidance exists on how best to secure each of these services according to your needs. The challenge is stitching the security of each of these services together into a cohesive whole.
The industry (lead by organizations like the Cloud Security Alliance, of which Trend Micro is a member) is working towards a common goal to help address this challenge.
The goal is to be able to provide tools that can organizations can get to easily work together (regardless of vendor) in order to provide a comprehensive security solution around cloud services.
The strategic vision and guidance is already in place with the Cloud Control Matrix (the CCM, a living document currently at version 3.0.1). This document lays out the types of controls that should be applied to various cloud services.
In addition to the CCM, there are a number of efforts in place to help organizations combine the right tools for their security needs. The Cloud Security Open API shows a lot of promise in helping make this a reality.
Separate from these efforts are the individual roadmaps for each cloud security tool. This is a very active and innovate space (yes, I realize I have a bit of bias here but just look around at the number of cybersecurity startups and established companies efforts and I think you’ll agree).
But each of these efforts are a medium term solution at best. What are organizations supposed to do now to address this problem?
When attempting to address this problem today there are 3 main areas where you should focus your efforts.
First you want to try and reduce the organizations overall exposure when it comes to using cloud services.
At solid first action is to attempt to inventory the number and type of services currently in use. To do this you should enlist a combination of technology and old fashioned methods (a/k/a asking teams what they are using).
With a better idea of what you’re attempting to secure, you can then start working with the teams throughout the organization to ensure that they are aware of the risks and security challenges associated with the services they use.
An ongoing discussion and education campaign is a pillar of the good security practice and critical to address the issue of multi-service use.
These discussions will also help inform your internal security policies. A strong, realistic policy will help establish a baseline for all stakeholders. It lays out what the norms for your organization and acts as a standard to compare against for any new business initiatives.
Above all, the responsiveness of your internal IT services is instrumental in reducing your overall exposure. Many teams don’t want to go against policy or organizational standards but don’t have a choice when internal service delivery is unresponsive.
As exposure is inventoried and scaled back (hopefully), your next step should be to implement a robust monitoring practice.
This will require a lot of initial work with an ongoing effort.
The variety of services and security controls applied to those services creates a unique challenge for each organization.
In general, you want to start with the lowest common denominator for monitoring (access logs, basic API access, network traffic, etc.). Where possible these should be tied to business metrics and risk.
For example, knowing that a business unit’s use of a cloud storage service is increasing week over week is a good monitoring metric (GB used) tied to a business risk (the exposure of that data on a 3rd party service).
Due to the nature of the problem, you best approach is a lot of spit, glue, and hope. This step requires a lot of manual effort but is crucial to being able to answer the deceptively simple questions, “where is the organization’s data stored and what’s it exposure?”.
With time, your monitoring practice will mature and you’ll grow to have a better understanding of your business requirements.
The lessons you learn should be applied to selecting cloud services that align with your business needs as well as your security strategy and tactics.
The organization should select services that allow you to easily get data in and out, provide support for standard APIs (or at least logical and well supported APIS), and have a strong reputation for services and security.
Choosing a provider based on these attributes will go a long way to ensuring that you have a consistent approach to onboarding new cloud services.
Building a coherent security practice for organizations using multiple cloud services is a challenge today and will continue to be a challenge for the foreseeable future.
The most efficient way to address this challenge is to focus on;
These three areas create a solid foundation for your security practice.
This will allow you to adapt and grow as the strategy for cloud security evolves, as more and more services support standard APIs, and as security technologies continue to provide innovate solutions that better address the new reality of modern IT service delivery.
This essay was built on a talk I presented at the CSA Summit during RSA 2016, “Defending The Whole. Iaas, PaaS, and SaaS”. It was originally posted in 2 parts on the Trend Micro blog (part 1, part 2). The slides are available on SlideShare.
For some additional thoughts and perspective on my talk, check out this piece by Rob Wright for TechTarget’s SearchCloudSecurity site.
Mark is a seasoned infromation security professional currently focused on researching & teaching cloud security and usable security systems at scale. Catch him on Twitter or at his site, markn.ca.
☁️🔬 Cloud Strategist @LaceworkInc . @awscloud Community Hero. Builder. Working to make security easier for everyone. Opinionated but always looking to learn
See all (2,068)
☁️🔬 Cloud Strategist @LaceworkInc . @awscloud Community Hero. Builder. Working to make security easier for everyone. Opinionated but always looking to learn
About
Write
Help
Legal
Get the Medium app
"
https://read.acloud.guru/iaas-paas-serverless-the-next-big-deal-in-cloud-computing-34b8198c98a2?source=search_post---------56,"Cloud migration. What does it mean? Cloud migration is the process of moving digital assets — like data, workloads, IT resources, or applications — to cloud infrastructure. Cloud migration commonly refers to moving tools and data from old, legacy infrastructure or an on-premises* data center to the cloud.
*On-premises is sometimes shortened to “on-prem” and commonly — though incorrectly (from a grammar point-of-view) — called “on-premise.” 
Though “cloud migration” typically refers to moving things from on-premises to the cloud, it can also refer to moving from one cloud to another cloud. A migration may involve moving all or just some assets. It also involves a whole lot of other things. That’s why we’ve complied this guide to cover all things cloud migration.
Looking to learn more about cloud migration? This beginner’s guide to cloud migration covers everything you need to go from knowing nothing about cloud migration to knowing something about cloud migration. We’ll start with an explain-it-like-I’m-five intro to cloud basics and take you all the way to cloud migration strategies and migration tools and services in this high-level look at successfully accomplishing a cloud migration. Plus, you’ll get handy resources to take you from getting the gist to getting it down pat.

The cloud — often used as shorthand for “cloud computing” — refers to a pool of computer services accessed over the internet. This pool is accessible on-demand and self-service, giving instant access to services without setup. With the cloud, computers that businesses once had to have on-premises can be stored in massive data centers* around the world.
*“Data center” is a term used to describe the space dedicated to housing computer systems and their necessary accouterments. 
Unlike a server* room or server closet, the computer-systems-storing units you might find behind a (hopefully) locked door in any given office, data centers are dedicated primarily to housing computer equipment. They’re normally huge and designed specifically for the express purpose of keeping a ton of tech running in optimal conditions.
*“Tap the brakes. What are servers?” you may be asking. Servers are basically heavy-duty computers designed to run all the time and typically dedicated to a single task, like storing, sending, or processing data.
 Learn server security in a non-technical environment!
These data centers are managed by companies focused on running data centers, reducing the need for local IT resources — computers, that is, not people. (You’ll still probably want some people, particularly people who know how to use cloud.)
Interested in upscaling or beginning your journey with Cloud Data? A Cloud Guru’s AWS Data Learning Paths offers custom courses fit for beginners and advanced gurus!
These computer services don’t actually exist in the sky, of course. (You probably knew that, but just in case . . . ) The name “cloud” actually comes from the symbol used in conceptual network diagrams from the 1970s. 
The cloud is sometimes used interchangeably with the term “internet,” but, technically, the cloud isn’t the internet. Rather, the cloud uses the internet to deliver resources. But when people are doing things on the internet, they’re probably using cloud services — from checking Gmail to uploading a dog pics to Instagram to streaming Adam Sandler movies on Netflix.
So, what can you do with the cloud? Almost anything you can do with a computer or a server is available as a service in the cloud.
Services vary in complexity, from storing the digital pieces that compose a webpage to employing beefy hardware to perform resource-intensive tasks, like using artificial intelligence (A.I.) and machine learning (M.L.) to analyze business data for insights.
The reasons businesses migrate to the cloud are plentiful and varied. But one big reason is that working in the cloud gives you access to virtually limitless computer resources. 
An easy thought exercise is to think of this nearly bottomless pool of computing power and storage a bit like electricity. Sure, you could produce electricity on your own by running a generator, but the upfront cost for the hardware is expensive. Then, you’ve got to keep it running, which requires a level of expertise and time spent on ongoing maintenance. And if it goes down, you’re out of luck — unless you’ve really splurged and have a second generator just sitting around ready to go at a moment’s notice.
But like the way you (probably) consume electricity, if you and others around you all need electricity, it makes more sense to outsource the setup and day-to-day management of electricity production. Then, you and all your neighbors who need electricity can tap into electricity with the flip of a switch. You’re free to use what you need and pay for only what you use. Got a bunch of people coming to visit for a holiday? You can have all the electricity they need, no problem. The electricity company has you covered. They focus on delivering you electricity quickly and efficiently, so you’re free to focus on what you need to do. (If analogies aren’t your thing, just re-read the last two paragraphs and replace “electricity” with “cloud.”) 
The plain and simple of it is this: Cloud abstracts and serves up IT on tap. You get servers and computer services without the need to buy, maintain, and manage hardware or dedicate employees to the low-level admin work that comes with that.
If you think about cloud like a utility (like electricity), you’re basically thinking about public cloud, which is just one type of cloud deployment model. But what’s the difference between public and private cloud? And what’s hybrid cloud? Funny you ask.
What’s the difference between SaaS, PaaS, and IaaS? They’re three different kinds of cloud service models, cloud service categories, or the types of cloud computing — all just different terms for the same three funny-looking acronyms. Whatever you want to call them, understanding these is a solid step to wrapping your head around what cloud can do.
“aaS” stands for “as a service.” In its simplest form, this means moving that piece of the tech stack to the cloud. For example, software as a service means moving software that would typically reside on a computer to the cloud. 
Are you an IT Manager? Admin? IT Team Leadership? A Cloud Guru’s AWS Executive Learning Paths offers enterprise continuous learning with custom courses fit for beginners and advanced gurus!
At a basic level, the benefits of cloud are often around efficiency, achieving the maximum results with minimal expense. For organizations that pull off a successful cloud migration, the rewards reaped can include increased scalability, lower costs, and security. 
For simplicity’s sake, we’ll focus specifically on public cloud benefits, though some of these benefits apply to other cloud deployment models, too. Here are seven big benefits of migrating to the cloud.
Cloud allows for improved scalability, giving organizations the ability to almost instantaneously add or take away resources on an as-needed basis or to match demand. Elasticity goes hand in hand with scalability and refers to the ability to quickly expand or decrease computer processing, memory, and storage resources (data storage optimization) to meet changing demands without concerning yourself with cloud capacity planning. Elasticity enables scalability.
Think about it this way: An influx of users can eat up a server’s resources. With traditional self-hosted hardware, requests would then be denied. But the cloud has a practically infinite set of resources. That means servers can scale up to handle the rush without a hitch.
The process of scaling can be done automatically (called autoscaling), based on, for example, the time of day or the amount of processor resources being used.
Cloud gives you the ability to only pay for the resources you use. (Think: taking an Uber instead of buying a car.) This gives you access to resources that would cost way too much time and money to keep up yourself, which ties back into scalability. 
A traditional IT approach to scaling up is costly. It takes planning; it can take months, requires hardware at a big upfront cost, electricity to keep it all operating and cool, and skilled IT staff capable of getting (and keeping) it all up and running. With cloud, that’s all done nearly instantly by your cloud provider.
But where cloud scalability really leaves on-prem scalability in the dust is around decreasing resources. Traditional IT infrastructure can require having enough resources to handle peak demand. (For example, a retailer having to pay for the data center resources to be ready for a rush of Black Friday shoppers, even though they’re not half as busy the other 364 days of the year.) Cloud infrastructure can scale up and down with the peaks and valleys of a year, month, day, or hour. 
Keep in mind, an organization’s cloud spend can get out of control without the proper planning and execution, and cloud isn’t inherently a money-saving move if done improperly. It does however undoubtedly allow an organization to . . . 
Cloud shifts tech systems from a capital expenditure (CapEx) to an operating expenditure (OpEx) — or from an investment in something you hold onto for a few years that will depreciate in value (like a bunch of expensive hardware gathering cobwebs) to a regular, ongoing cost for running the business (like paying for internet). That’s great news for businesses that like to hang onto as much cash as possible. (You can talk to your finance department to confirm, but that’s probably anywhere anyone works.)
Agility can mean a couple of different things around the cloud. “Cloud agility” is often used to describe the ability to quickly develop, test, and launch business applications. But cloud also gives you the agility to respond quickly as needs change. 
Even small businesses get access to the same powerful tools that the biggest enterprises use. New services can be accessed with a few mouse clicks, so when a new need, challenge, or opportunity arises, it’s possible to respond immediately.
And cloud can also make it easier for organizations with multiple offices — eliminating the need to set up infrastructure in each location. Just turn on the power and the internet and your people are ready to go. It also makes it possible for your workforce to suddenly switch to, say, working from home (looking at you, 2020).
The big cloud service providers run a worldwide, world-class network of facilities packed with cutting-edge tech. This ensures everything from keeping network latency low to delivering near unparalleled data backup and disaster recovery. No matter where the people who need access to your tools are, the cloud typically brings them closer.
Most cloud providers are big companies with big companies relying on them. That’s why they go out of the way to consider security and compliance, which includes staying on top of updates and trends that will ensure your sensitive data is safe in the cloud.
Public cloud providers typically bring to the table policies, tech, and controls that are a huge step up from the average organization’s security practices. This is paired with considerations for almost any industry-specific compliance needs.
Plus, keeping data in the cloud rather than on your hard drive can also keep data from getting compromised if a device is stolen or misplaced.
Maintaining computer hardware and software is a full-time job. Literally. With public cloud, you don’t have to have employees spending time dedicated to the tedious maintenance of equipment that doesn’t directly contribute to business objectives. Your cloud service provider ensures the infrastructure is in place, so your tech wizards can focus on driving business outcomes.
Create a culture of cloud innovation with accelerate cloud success with hands-on learning at scale. Upskill or reskill 10 or 10,000 with the most comprehensive and up-to-date learning library, assessments and sandbox software.
Without the right strategy, you can hurt your chances of achieving your desired cloud benefits. There are many common mistakes when companies go cloud, and a botched migration can mean hampered performance and increased costs. It’s regularly reported that more than half of companies don’t see the cloud benefits they expected. This is commonly due to faults in migration strategy and a lack of the necessary cloud talent.
If you’re looking for a guide to cloud migration, here are the three fundamentals to consider no matter what type of cloud and what cloud service models you’ll use:
At a more detailed level, it’s all about figuring out the migration process and giving plenty of thought to planning.
Need an accurate read of cloud competency across your organization? Start with a cloud training needs analysis to identify skills gaps. Try our Skills Assessment to position your team for further cloud success.
Ideally, you won’t be winging it when migrating to the cloud. Keep tabs on what’s done and what’s next to ensure all the moving pieces end up where they should. Need a cloud migration template? Here some steps to include on your cloud migration checklist.
Why are you moving to the cloud? This calls for a formal business case! It’s important for leadership to be clear on the purpose of the migration — and set aggressive goals to drive the organization forward. This includes creating a baseline of your current IT infrastructure and forming some cloud migration key performance indicators (KPIs). These will make it possible to measure your cloud migration success.
Take stock of what’s in your environment, noting any interdependencies, then figure out what you’ll migrate first and how you’ll migrate it. Look at which applications can be moved as they are, which ones will need some (or A LOT of) reworking, and what tools are out there to simplify migration of those trickier workloads. Pick your cloud deployment models and the various tools and services out there (see below). You’ll ideally want to figure out ROI (return on investment) for things you’ll be migrating and how long that might take to achieve. 
Time to roll those sleeves up and get your hands cloudy. When you’re ready to start migrating, it’s typically best to start with something not overly complex or business-critical. With any luck, that quick win will boost excitement and teach you some things along the way. 
Applications should be designed, migrated, and validated using one of the migration strategies covered below. After making the move, you’ll want to test everything out and decommission your old systems. This can mean running two environments for a time, but you can keep this time in limbo from lasting too long by ensuring your cloud leaders are ready to confirm all systems are go and are able to measure your cloud success.
As you migrate applications, keep hammering away to figure out your new operating model, turn off those legacy systems, and keep pushing forward.
There are three basic types or patterns of cloud migration. In the order below, they run from easiest and fastest (with some drawbacks) to more difficult (with bigger benefits). 
Let’s dig in a bit more into the types of cloud migration strategies and the pros and cons of each.
What are the advantages of a lift and shift (also called “rehosting) approach? It’s quick and requires minimal refactoring. But (and this is a big but) the downsides of lift-and-shift include the fact that you may miss out on benefits you’d see if going cloud native because you’re performing the bare minimum changes needed. This means you could end up paying for the speed and the ease of your migration in the long run — at least when compared to a more thorough approach.
Lift-and-shift can be used for simple, low-impact workloads, particularly by organizations that are still far from cloud maturity. If your current setup includes loads of virtual machines, it’s relatively straightforward. There are even products from vendors that claim to help you automate your migration (but doing the process manually can be a great learning exercise). The good news is even if you’ve hastily made a lift-and-shift, applications are easier to rework and optimize once they’re running in cloud.
The move-and-improve (or replatforming) approach to migration includes making some modern updates to your application — like, say, introducing scaling or automation — without throwing the whole thing out. This happy-medium approach can seem like the superior option at a glance, but it can result in migrations where you keep all your technical debt and get none of the cloud-native benefits.
This approach (also called refactoring or re-architecting) means rebuilding your workload from scratch to be “cloud-native.” It takes an investment in time and skills development (particularly reskilling and upskilling your existing talent), but it pays out with the maximum benefits available in the cloud. 
While every organization and workload is unique, if the reason for moving to the cloud is to take advantage of its awesome benefits and capabilities, embracing cloud-native design principles should be your approach. This means taking the time to plan and do things right: ensuring your people have the necessary skills to make the move and refactoring your code.
Big public cloud providers like AWS, GCP, and Azure want you to move to their slice of the cloud (go figure), so they throw plenty of tools your way to make migrations as pain-free as possible.
Of course, they’re also happy to take your money if you want to just throw it their way. Cost savings are a potential benefit of moving to cloud, but cloud costs can also easily get out of control. That’s why it’s important to use all the tools at your disposal to plan for what you need and adjust processes. Things that worked fine on-premises can be costly mistakes in the cloud.
Cloud cost calculators can help you sneak a peek at the cost of your setup before making the move. Check out the one your public cloud provider offers. Examples include the AWS Pricing Calculator, the Azure Pricing Calculator, and the (equally surprisingly named) Google Cloud Pricing Calculator.
Other tools to check out early on include (for AWS) AWS Trusted Advisor and (for Azure) Microsoft Azure Advisor. These give you real-time guidance around cloud best practices and can also help with cost optimization, as well as security and performance.
What cloud migration tools does AWS offer? If you’re looking for Amazon cloud migration services, the cloud juggernaut has a range of solutions — including plenty of free ones — to help you kick off your migration.
There are also several tools for migrating data and files, including AWS Snowball, AWS Snowball Edge, AWS Snowmobile, AWS DataSync, and AWS Transfer for SFTP. You can get more details on these and other services from AWS.
Need Azure cloud migration tools? Azure offers plenty of resources, including tools, tutorials, and videos. (We also have a few tips for those looking to migrate servers to Azure.)
GCP has two different paths for simplifying cloud migration planning. The first (and newer option) is called Google Cloud Rapid Assessment & Migration Program (RAMP), which the company describes as a “holistic, end-to-end migration program.” The second option is The Google Cloud Adoption Framework (a whitepaper you can download) and the 15-minute Cloud Maturity Assessment.
Ready to start migrating to the cloud that Google built? The company has its own laundry list of migration services.
Other Google Cloud migration tools can be viewed on Google’s migration center site.
Anticipating potential issues is a key part of your cloud migration strategy. What are some common cloud migration challenges?
Hungry for more cloud knowledge? Need self paced practice labs for a true contextual learning experience? A Cloud Guru makes it easy to get up to speed in the cloud — whether you’re an individual looking to learn more or part of an organization looking to upskill thousands.
Start a free trial for yourself, view plans for your organization, or peruse our rotating lineup of free cloud courses and start learning cloud by doing.

Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://medium.com/fsmk-engineering/think-roles-not-secrets-fcbcb698dbfa?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Amarnath
Aug 21, 2019·3 min read
Secrets are inevitable in modern-day application development. These may include database credentials, IaaS platform key-pairs, API keys of third-party web services or encryption keys for generating access tokens for your own application.
Here are some common problems often associated with managing secrets:
Together, these pose a serious security risk.
A new class of tools has now emerged to deal with these problems: Secrets Management Systems. One such tool is Vault from Hashicorp.
A secrets-management system nicely complements a configuration management system that is often already employed in a distributed applications/micro-services setup. Here are some of the key features that Vault provides:
The last feature is the most interesting one and is the main topic of this post. We intuitively think about protecting our secrets and rotating them periodically and that makes sense in cases where it is not easy to automate their regeneration — eg., third-party API keys. However, a different way to look at the same problem is to assign roles to your services instead of configuring them with pre-generated, static credentials.
In this model, as a service author, you think about the permissions that your service needs. You can then formally define these permissions in a say, AWS IAM policy document or a set of PostgreSQL GRANT queries. This privilege manifest can then safely be co-located with your source code and checked in to your SCM server (think GitHub, Gitlab or Bitbucket). (Note: Pushing secrets to your SCM server, on the other hand, is generally considered an anti-pattern)
Vault’s Dynamic Secrets feature reads this privilege manifest and when asked to, generates credentials with the given privileges and a reasonably short time-to-live (expiration). These credentials can then be used by the application to authenticate with a target service (AWS or PostgreSQL or RabbitMQ, say) as it would normally do.
Here is how Dynamic Secrets can be set up with PostgreSQL
In summary, Vault’s Dynamic Secrets offers a novel solution to some of the problems that I listed earlier: Secrets no longer need to be long-lived or shared as they can be generated on-demand.
One topic that we have not talked about here is authenticating with Vault itself i.e. How can a service identify itself with Vault and securely only read the secrets that it has access to?. You can find the different authentication methods supported by Vault here.
another socially awkward engineer
29 
29 claps
29 
Funding Societies | Modalku is a licensed digital lending platform that connects small and medium-sized businesses with retail and institutional lenders.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@manningbooks/serverless-made-effortless-bca6bc1df98c?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manning Publications
Dec 23, 2019·6 min read
From Terraform in Action by Scott Winkler
This article discusses how Terraform can make serverless infrastructure easy and painless to use.
_________________________________________________________________
Take 37% off Terraform in Action by entering fccwinkler into the discount code box at checkout at manning.com. _________________________________________________________________
This scenario is something I like to call “the two-penny website”, because this is how much I estimate it costs to run on a monthly basis. If you can scrounge some coins from between the seat cushions, you’ll be good for at least a year or two of web hosting. Not to mention (I’m probably overestimating a bit); for most low traffic applications, the true cost is likely to be a fraction of one penny. Because serverless is based on the premise of pay-as-you-go, it’s difficult to estimate ahead of time exactly how much it will cost. Perhaps, if you have a popular website, you might be charged five cents rather than two cents. Nevertheless, I’m sure you agree that serverless has the ability to be comically affordable, and by the end of this article, you won’t have any excuse not to have some sort of web presence. You can even justify making silly, or single purpose websites, because it’s practically free anyways.
The website we’re going to deploy is a ballroom dancing forum, called “Ballroom Dancers Anonymous”. Unauthenticated users are able to leave comments which are stored in a database and are viewable by other users on the site. The design is fairly simple, but the beauty is that this is generalized to work with a wide variety of different web applications. A sneak peak of the final product is shown in figure 1.
We’ll be using Azure to deploy the serverless website. A basic deployment strategy is shown in figure 2.
Although this website costs only pennies to run, by no means is it a toy. Because it’s deployed on Azure Functions, it’s able to rapidly scale up to handle tremendous spikes in traffic and do this with low latency. It also uses HTTPS, a NoSQL database, and serves both static content (HTML/CSS/JS) as well as a fully-fledged RESTful API. An architecture diagram is shown in figure 3.
Because the code we’ll write’s relatively short and cohesive, I’ve decided to put it all in a single main.tf file, instead of using nested modules.
Tip As a rule of thumb, I suggest having no more than two hundred lines of code per file. Any more, and it becomes more difficult for a reader to build a mental map of how the code works.
The big question is, how should the code be organized to be both easy to read and easy to understand? Organizing code based on the number of dependencies is a tried and true approach. Typically, this means organizing code such that resources having fewer number of dependencies are located toward the top, but resources having greater number of dependencies are located toward the bottom. This leaves a lot of room for ambiguity, like when resources have the same number of dependencies, but don’t feel like they belong together.
Instead of strictly organizing your code by the number of dependencies each resource has, I recommend grouping related resources together and sorting them by group (i.e. sorting first by group, then by resource). This can be visualized in figure 4.
In the same way that it’s quicker to search for a word in a dictionary than in a crossword puzzle, it’s faster to find what you’re looking for when your code’s organized in a sensible manner (such as the pattern shown above). Although Terraform won’t necessarily operate on the code synchronously — due to the complex way that Terraform walks the dependency graph — the goal is that someone reading your code from top-to-bottom should be able to completely understand how it works without having to think too hard.
For this project, I’ve divided it into four main groups of resources, each one serving a particular purpose. These groups are:
This can be pictured in figure 5.
Note Because we’ll be working through these groups one at a time, I’ll use the word “step” interchangeably with the word “group”
The last part of planning we need to do is consider the overall inputs and outputs of the root module. From a black box perspective, there are three inputs and one output. Two of the inputs are used for configuring the provider/base resources (client_secret and region) and the third is for affording a consistent naming scheme (namespace). The sole output value we’ll have is website_url, which is a live link to the final deployed website. This can all be visualized in figure 6.
If you’re anything like me, you might be thinking to yourself “it’s great you’re showing me how to do all this, but there’s no way I could possibly have come up with this on my own.” When writing this book, I didn’t want it to be another cookbook of copy-paste snippets. Terraform can be used in many different ways and there’s no possible way I could cover every potential use case. Instead, I wanted to give you a set of tools and thought processes to use to solve your own problems. Here’s a list of steps that I take when tackling a new problem with Terraform:
For this particular scenario, I knew my goal was that I wanted to deploy a serverless website on Azure, but I haven’t used Azure much before (I’m more of an AWS guy). I had to do a lot of research to figure out what services were available and how they could best be used. Then I selected Azure Functions and Azure Storage because those both seemed to fit my requirements. Afterwards, I built a quick and dirty prototype using the UI editor, before finally sitting down to translate this all into clean and working Terraform configuration code. It looks neat and tidy only if you ignore all the dead-ends I went through before settling on the final design.
Tip problem solving is an art, and the only way to get better is with practice.
That’s all for this article. If you want to learn more about the book, you can check it out on our browser-based liveBook reader here and in this slide deck.
Check out more great free content from our titles on our Free Content Center.
Follow Manning Publications on Medium for free content and exclusive discounts.
See all (32)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Follow Manning Publications on Medium for free content and exclusive discounts.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/altoros-blog/tips-and-tricks-to-monitor-a-cloud-foundry-deployment-across-all-levels-fa3c996b7bd?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
A real-life Cloud Foundry deployment involves several layers, starting with IaaS all the way to individual apps.The blog post shares best practices on what metrics it is essential to monitor within each layer, as well as overviews the tools to simplify the task. In addition, you will learn some general recommendations on how to keep your deployment up to date and under control.
Read the full article on our blog:
paas.ly
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@s489791150/unity-2020%E5%B9%B4%E9%81%8A%E6%88%B2%E5%B8%82%E5%A0%B4%E5%9B%9E%E9%A1%A7%E5%88%86%E6%9E%90-%E4%B8%A6%E4%B8%8D%E6%98%AF%E6%89%80%E6%9C%89%E9%81%8A%E6%88%B2%E9%83%BD%E9%81%A9%E5%90%88%E6%8F%92%E5%85%A5%E5%BB%A3%E5%91%8A-d2dbbb374c88?source=search_post---------107,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sam Liang
·Feb 28, 2021
Unity近期公佈了2020年的遊戲市場回顧，很難得有廣告變現相關的報告，其中有些細節值得分享給大家。
根據GamesIndustry的資料，2020整理的手遊市場份額成長約9.7%，但在這份Unity的報告中，除了博弈、運動和策略遊戲以外，幾乎所有遊戲類型的廣告收入都成長超過30%以上，可以說廣告變現應該是個發展中的趨勢。
以上三個類型中，博弈遊戲強調玩家拉霸的沈浸感，使用廣告只會破壞用戶體驗；運動遊戲去年受到封城影響，有IP的遊戲如NBA相關等都下滑慘烈；而策略遊戲偏重度，每單位用戶內購收入高，要在不影響玩家的情形下插入廣告的難度也高。(不過Art of War倒是個反例)
雖然Unity很明顯的，想要推廣開發者使用廣告變現(畢竟其收入來源大多來自於廣告)，報告中也說明了有使用 IAA廣告的短長留存皆略為“大於”沒使用內建廣告的遊戲。但若我們仔細研究各類型的狀況，有些類型如角色扮演、博弈遊戲，內建廣告的平均長線留存”反而比較差”，策略遊戲更有趣的凸顯了廣告僅對“短期留存有幫助，長線留存反下滑“的趨勢。
而使用廣告對於留存最有幫助的，看起來是文字遊戲，30日留存可以提升接近50%。
我認為，由於廣告會造成跳出干擾，越強調塑造用戶上癮與沈浸的遊戲，譬如博弈、色情、二次元等，也許較不適合使用廣告變現機制。
廣告變現以外，內購(IAP)有個趨勢也很有趣。2020年大部分遊戲的每次交易金額和2019年差不多，平均大概在6~7美金上下；只有博弈遊戲異軍突起，從9美金上升到11.92美金。這也許代表很多習慣實體場域的賭客轉往到了線上？
除了以上，這份報告也凸顯了遊戲在接近年底時廣告收入eCPM會飆高、手機遊戲玩家越來越傾向首日付費、以及週間和週末行為界線模糊的趨勢。
有趣的是，有61%的受訪者使用Unity開發，但只有34%使用其廣告變現系統，這對Unity究竟是劣勢還是潛力呢？
完整報告建議大家有空可以一觀：https://reurl.cc/OXGQz9
希望以上對你有所啟發。
90 claps
90 
遊戲業人士，又名飛鳥涼不涼，喜歡藉由寫作鍛鍊自己與交朋友，討厭任何形式的道德綁架，對一直突破巔峰的肚子感到哀傷，最開心的事情….有人說從自己的文章中獲得啟發。
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/spoon-university-cloud-66-creating-an-online-food-resource-for-gen-z-b480db991793?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Sep 6, 2016·3 min read
06 September 2016
Spoon University was founded in 2013 as this generation’s trusted online resource for all things food and lifestyle related. Anyone interested in learning more about food (whether they’re in college or not) can check out the site, and engage with a wide range of food-related content made by students on campuses around the world. Students can join teams on their campuses or in their cities, get trained in digital media and marketing, and start producing content that’s seen by millions of people.
I had the opportunity to catch up with Matt Lovan, Spoon’s director of engineering, who told me more about what it takes to build an online resource for Generation Z food novices worldwide:
Hi Matt, what can you tell us about Spoon and the food movement?  At a time when food is becoming part of young people’s lifestyles and identities, we help people all over the world navigate the educational and practical aspects of food and food preparation a little more easily. We have simple recipes on the site that you can make in a dorm room or a tiny first apartment, and tons of hacks and how-tos that are only obvious once you know them. Plus, we share all the food news everyone’s going to be talking about, like the best local restaurants and the craziest Instagram food trends. All of the content on Spoon University is created by and for this generation, so it’s naturally way more authentic and relatable than everything else that’s out there.
Can you talk us through your current infrastructure?  Spoon University is built on a distributed architecture using several different technologies. User management, user training, and content analytics is managed in a Ruby-on-Rails application. User content is created in WordPress, hosted by a third-party, but we’re currently in the process of creating a custom CMS in our Rails application. The frontend is powered by a Docker cluster of node servers that pulls content from MongoDB and Elasticsearch.
Prior to using Cloud 66, what sort of problems were you trying to resolve, and how were we able to offer support?  We were a little challenged due to having to manage container deployments to multiple IaaS providers. Our back-up systems were also spread across several IaaS providers and regions for redundancy purposes. I was looking for a solution to automate the deployment process and help manage backing-up our database. I also wanted to find a way to better utilize a ton of free credit we had with various different cloud vendors.
Thanks to you guys, I’ve practically reduced my infrastructure cost to zero, as I was able to mix and match my cloud providers based on available credits. My team of developers can easily deploy new code without needing to interact with various APIs to manage infrastructure, which has been great.
That’s awesome to hear. What are some of your favourite Cloud 66 features?  Cloud 66 has been a great stepping-stone for us as we move to control more of our infrastructure. I like how we’ve been able to standardize our workflow across multiple IaaS providers thanks to the multi-cloud deployment capability. The ease of cloning stacks and import/backup data saves a huge amount of time, as does the ability to pull code from Github and automatically build Docker containers with BuildGrid. The easy SSL install with LetsEncrypt has also been a great addition.
Being able to quickly create a real stacks with defaults provided by Cloud 66 has allowed us to learn as we grow. A big thank you to your Support team, who’ve been a great resource and I love the Cloud 66-branded Tile on my keychain!
Originally published at blog.cloud66.com on September 6, 2016.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
See all (2,795)
2 
2 claps
2 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
About
Write
Help
Legal
Get the Medium app
"
https://blog.devgenius.io/iaas-paas-saas-difference-fbed005dd196?source=search_post---------156,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Hi developers 🥳
You probably started learning Cloud(Azure, AWS or GCP) and came across concepts such as: IaaS, Paas, SaaS.
And you just can’t understand what is the difference? Not a problem, in this article I will try to explain everything in simple words :)
Before starting, I will leave a picture so that you can refer to it and I hope it will give you more understanding.
Imagine that you or your team have written an application and it’s time to deploy it on the Internet, now you need server and networking hardware, room for its placement (data center or server room), specialists for configuration and maintenance.
It sounds very scary, out of date, and most importantly expensive! And here a modern solution comes to our help — IaaS.
We can just buy virtual servers with a certain processing power and memory size (of course all this can be increased in the future on demand, simply by clicking the mouse).
All issues of administration of server and network equipment are decided by the provider, but the settings at the level of operating systems and applications in them are carried out by the client himself (usually the system administrator).
The main difference between IaaS and traditional hosting is the ability to quickly scale and charge only for consumed resources.
IaaS examples: IBM Softlayer, Hetzner Cloud, Microsoft Azure, Amazon EC2, GigaCloud.
Well, what if we don’t want to deal with setting up the OS, but want us to already have a ready-made platform — development environments, deployment tools, databases, machine learning libraries, etc.
Then PaaS is what we need. The provider’s area of responsibility includes all physical infrastructure, as well as administration at the operating system level. We manage applications deployed on the basis of this infrastructure.
The main advantage of PaaS is the ability to quickly launch applications, including for small teams. In addition, using cloud services, developers can collect statistics on the operation of their software, analyze and make the best decisions for the business.
PaaS examples: Google App Engine, IBM Bluemix, Microsoft Azure, VMWare Cloud Foundry.
Finally, then we got to the point that each of you daily uses in your work (or maybe not 😕): Google Doc, Microsoft Office 365, Flickr, Dropbox, Facebook.
As you might have guessed, SaaS is when programs and services are developed and maintained by a provider, placed in the cloud and offered to the end-user through a browser or an application on his PC. The client only pays a monthly fee (or uses the service for free), the provider is responsible for updating and technical support of the programs.
The main customer of SaaS services is a regular user.
From an end-user perspective, SaaS is the most intuitive and user-friendly cloud model. It is often easier and more efficient to use a ready-made SaaS service that already meets certain requirements. But ready-made solutions do not always exist, and in this case, the PaaS and IaaS models are irreplaceable.
If you like this article, you can buy me a cup of coffee and I will drink it when I write the next article :)
www.buymeacoffee.com
Coding, Tutorials, News, UX, UI and much more related to development
42 
1
Get the latest news and update from DevGenius publication Take a look.
42 claps
42 
1
Written by
Full Stack Developer who is inspired by new technologies
Coding, Tutorials, News, UX, UI and much more related to development
Written by
Full Stack Developer who is inspired by new technologies
Coding, Tutorials, News, UX, UI and much more related to development
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devobs/le-iaas-et-le-paas-sont-morts-vive-lipaas-e187c552ec7b?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
TL;DR :Kubernetes réconcilie les ops et les dev, le IaaS et le PaaS, la polyvalence et la simplicité.
Remontons un peu dans le temps.Le IaaS et le PaaS ont été pendant la fin des années 2000 et le début des années 2010 les deux technologies qui se sont affrontés pour la domination de la production.Elles ont une histoire et une origine pourtant bien différentes.
Premier apparu, le IaaS est une évolution des techniques d’Ops.Elle repose sur une automatisation extrême des opérations pour pouvoir proposer des machines à un rythme effréné.Cette automatisation s’est également conjuguée avec la mise en place d’API pour pouvoir piloter la consommation de ressources.
Le principal représentant du IaaS est bien évidemment EC2, troisième service proposé par AWS en 2006.
Le IaaS a l’avantage de réutiliser toutes les technologies pré-existantes et permet de se concentrer sur la problématique de la mise à l’échelle.Il n’y a pas (beaucoup) besoin d’adaptation du code des applications ou du système.
Les avantages sont en revanche assez limités : on doit toujours s’occuper des mises à jour d’OS, de leur compatibilité hardware (même si simplifié par une interface virtuelle), de la sécurité, du déploiement du logiciel, de sa configuration…
À la philosophie diamétralement opposée, le PaaS se concentre sur les outils de développement.Grâce à une intégration très forte dans le code et le flux de travail des développeurs, le PaaS doit fournir simplement une mise à l’échelle “automagique”.
Lancé en 2007, Heroku est sans doute le PaaS le plus connu. Mais il ne faut surtout pas oublier Google App Engine premier service fourni par GCP !
Pour pouvoir utiliser un PaaS, le développeur va souvent avoir besoin d’un SDK fourni par la plateforme, développer son code autour (accès à la base de données, métrologie, log, etc) puis pousser son code directement chez le fournisseur.Le PaaS va alors se charger de tester, créer et déployer le logiciel sans intervention d’un “ops”.
Mais bien que très alléchant sur le papier, les inconvénients sont extrêmement forts :
hackernoon.com
Mais le principal défaut des PaaS est sa faible capacité à gérer les applications “legacy” ou externes (open source ou d’un éditeur tiers).En effet l’adaptation peut être difficile (fork complexe) voir impossible (technologie ou contrat de support).Il est donc souvent nécessaire d’avoir en parallèle une infrastructure IaaS pour toutes ces applications… limitant l’économie réalisée (obligation de conserver une équipe “Ops”).
La notion de “turing-complet” est connu dans le développement. C’est la capacité d’un langage à pouvoir implémenter n’importe quel algorithme.Pour l’infrastructure une telle notion n’existe pas… encore. J'appellerai donc ce concept “Frazelle-complet” : la capacité d’une infrastructure à pouvoir exécuter n’importe quel logiciel.
Bien que très efficace, la structure non Frazelle-complet d’un PaaS va limiter de manière très importante les usages… ce qui sera presque fatal à GCP.
www.quora.com
À l’opposé, le IaaS est totalement Frazelle-complet… mais limite l’efficacité opérationnelle.
Après un (relatif) échec sur le PaaS et une arrivée tardive sur le IaaS (2013, 6 ans après AWS !), GCP se devait de proposer une solution innovante et adaptée.C’est en se reposant sur l'expérience interne acquise chez Google que GCP annonce en juin 2014 Kubernetes, très vite suivie en novembre de la même année d’une première offre hébergée.
Kubernetes est une révolution© dans le secteur.Totalement déclaratif, il permet via une simple description de pouvoir déployer n’importe quelle architecture logicielle, même la plus complexe !Reposant sur la technologie des conteneurs, il est totalement Frazelle-complet !
Ce double aspect propose une caractéristique unique et créer même une nouvelle couche logique : l’iPaaS pour infrastructure Platform as a Service.
Comme je l'évoquais dans un précédent post, Kubernetes a tout d’une abstraction d’infrastructure. Il est possible de consommer des ressources matérielles et systèmes de façon transparente.Mais il reste simple et générique à utiliser.
Kubernetes ne remplace pas votre besoin d’un IaaS, il se repose dessus pour se déployer et monter à l'échelle.Il ne remplace pas non plus un PaaS pour des applications internes adaptées, à l’inverse un PaaS peut se reposer sur Kubernetes (comme OpenShift).
medium.com
Il est donc important d’appréhender Kubernetes comme ce premier exemple d’un iPaaS, une nouvelle couche logique pour votre infrastructure :Aussi puissant qu’un IaaS, aussi compréhensible qu’un PaaS.
Le magazine et observatoire du DevOps
21 
Some rights reserved

21 claps
21 
Le magazine et observatoire du DevOps
Written by
SRE Indépendant / Partisan du DevOps / Kube Primo Adoptant. Fondateur de barpilot.io
Le magazine et observatoire du DevOps
"
https://medium.com/t-t-software-solution/%E0%B8%95%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87-microsoft-sql-server-2017-express-%E0%B9%81%E0%B8%A5%E0%B8%B0-windows-server-2019-%E0%B8%9A%E0%B8%99-azure-virtual-machine-d3bc79062a09?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
สวัสดีผู้อ่านทุกท่านน่ะครับ ^^
บทความก่อนหน้านี้ผมเคยแนะนำการสร้าง Ubuntu บน Azure Virtual Machine
medium.com
ซึ่งในบทความนี้ก็จะคล้ายกันครับ แต่เปลี่ยน OS เป็น Microsoft Server แทนครับผม โดยจะแนะนำในรูปแบบของบันทึกการสร้าง Service ประเภท IaaS (Infrastructure-as-a-service) น่ะครับ
เราจะทำการสร้าง Windows Server บน Azure และเน้นรับผิดชอบในเฉพาะในส่วนของ Software เช่นการดูแล OS, ติดตั้ง Application ต่างๆ
ส่วนของ Hardware นั้นให้ทาง Microsoft ช่วยดูแลให้ครับ ทำให้เราลดเวลาที่ต้องดูแล Server ได้เช่นการเพิ่มลด Hard Disk
ถ้าใครสนใจเนื้อหาประเภทของ Cloud Services ต่างๆ ผมขอแนะนำบทความจากสลัดผักน่ะครับ
www.saladpuk.com
Azure มีบริการ SQL Server หลายหลายรูปแบบเลยครับ แต่ในที่นี้จะขอยกหลักๆมา 3 บริการน่ะครับ
แนะนำบทความจาก Microsoft ในการศึกษาเทคนิคการ Migrate to Cloud น่ะครับ
docs.microsoft.com
เหตุผลที่ผมต้องเลือก SQL Virtual Machine มาจากเรื่องของระยะเวลาและค่าใช้จ่ายครับ
Resource Group จะเกี่ยวกับการจัดหมวดหมู่ของ Azure Resource เข้าด้วยกันเพื่อง่ายต่อการจัดการน่ะครับ เช่น สร้าง Group Dev สำหรับ Resource ทั้งหมดที่เกี่ยวกับ Development ซึ่ง เวลาลบเราสามารถลบได้ทั้ง Group เลยในครั้งเดียว
Region
Availability Options
Azure มี Availability สำหรับ VM เพื่อทำให้ระบบ High Availability ครับ โดยเราสามารถเลือกได้ตาม Options นี้
Images
docs.microsoft.com
Size
Inbound Ports
ใครสนใจเรื่อง Azure Network เพิ่มเติม ผมแนะนำบทความข้างล่างนี้น่ะครับ
medium.com
www.mvpskill.com
www.mvpskill.com
azure.microsoft.com
medium.com
ขอบคุณมากๆครับผมนายป้องกัน
https://www.tt-ss.net/
12 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
12 claps
12 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aheadcrm/customer-experience-is-a-platform-play-always-was-7f98ba2e1768?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Feb 7, 2018·8 min read
The most important tool that enterprise software vendors have in their respective arsenals is their platform.
While Vinnie Mirchandani rightfully states that Enterprise Software Platforms have so far underwhelmed, Denis Pombriant proclaims them the new battleground. In my opinion it is not that new a battleground but as part of the Clash of Titans it is becoming more evident as a battleground. An enterprise software platform was always part of the battle for dominance in the customer engagement — or putting it into (marketing) industry lingo — customer experience market. It is actually an integral part of it. This is largely because of the ongoing commoditization of transactional business applications.
But it was sexier to talk about shiny topics like engagement and experience than to talk about the grease and the machinery behind that drives and enables the technical delivery of engagements — note, that there are systems of engagement, but there is nothing like a system of experience.
And now topics like chatbots, machine learning, AI, ambient computing, IoT, to name a few, made the machinery — the platform — the new black.
When looking at the broad topic of CRM, customer engagement or customer experience, we have seen a lot of change happening since the early days of Sales Force Automation, SFA.
Back in the early 90s one of the first topics has been SFA, with a focus on making a distributed sales force more effective and efficient. Contact management came even earlier, call center software and field service quickly followed.
The emerging industry was dominated by little players that mostly got acquired by bigger enterprise software companies (anyone still knows a company called Kiefer & Veittinger?). Essentially we lived in a best-of-breed world. Nobody really talked about user- or customer experience. Times were all about efficiency. But what happened was that these point solutions brought an improved customer engagement, and with it an improved experience. Sales cycles shortened, got more predictable, which led to increased customer satisfaction overall.
One of the players, Siebel, stepped up the game by coining the term Customer Relationship Management, CRM, and with it came the solution suite that increasingly covered all areas of CRM: Sales, Service, and Marketing. The advantage was clear: Having a highly integrated system not only allows for more efficient internal collaboration but also enabled one consistent face to the customer — better engagement possibilities and therefore potential for improved experiences again.
The first real protagonist of this was SAP, but let’s not forget Oracle.
In the mid 2000s Cloud Computing fully arrived, or rather took off. One could say, that this was also propelled by Salesforce with its then disruptive Software as a Service model. With this model we arrived at best-of-breed software again, as nimble SaaS players could offer quick solutions for departmental problems, something the big software houses could not do — nor did they want to.
SaaS was born, and with it came the possibility to nimbly react to increasing customer demands, which in times of social media and the communications revolution that mobile devices caused, came very handy. The big incumbents realized this, too; some faster than the others.
All of them, SaaS players or not, had their own development platforms, with different strengths, with or without own databases. This is important as it allowed them as well as their partners to efficiently enhance their software to offer more specialized functionality.
Again, without talking about it, one result was better user- and customer experience through improved engagement.
At the same time players like Google started to offer their environments as development platforms, too.
PaaS — platform as a service was born.
Add a vast number of little players and great software that got built using available open source technologies only.
Shortly after, with the arrival of AWS, Infrastructure as a Service became a hot topic.
However, a problem arose of all this, too. The many SaaS applications that were built using the different technology and development platforms had a hard time integrating, which ate up a lot of the advantages the SaaS applications themselves offered. Consistent engagement, and with it customer experience suffered.
To address this, middleware services became part of PaaS offerings. Additionally there was also an increasing trend of consolidation on a smaller number of offered platforms, increasing the economies of scale for all involved parties.
As a side effect the suite reappeared on stage, but with a twist. Instead of being a kind of supermarket it had converted to something more like a mall. Specialized software was integrated via a (fairly) common data layer, based upon one platform. While this is not entirely true you get the picture.
This happened towards the end of the first half of the 2010s, and, along with increasing commoditization of enterprise software, gave birth to the ‘platform war’. Just that, sticking to military terminology, CEM, CEX, Martech, etc., have been the proxy wars.
In came topics like chatbots, AI, machine learning, IoT, ambient computing.
And this brought the platform topic into the foreground.
A well-architected and comprehensive platform is the foundation for integrated business processes. This, in turn is the precondition for being able to deliver great engagements and therefore for the end customer to perceive a great experience. There is no experience without engagement.
The platform is the customer experience platform.
Now, this is a good question!
There are probably as many definitions as there are enterprise software and infrastructure vendors. And these definitions are largely based upon the vendors’ legacy and core business.
The industry largely distinguishes between two layers of platform:
While IaaS is fairly simple (no disrespect meant) as it mainly deals with the many hardware aspects of a distributed cloud platform, it is challenging on its own.
And, supplying the compute, storage, and networking abilities, IaaS is literally fundamental. That makes it a part of the platform to reckon with. IaaS is a core element of Oracle’s strategy, and increasingly gets Microsoft’s attention. And then there are players like AWS, Google, Ali Baba, IBM, Rackspace, etc. that are providing cloud infrastructure to businesses as (a part of) their main business.
The importance of IaaS is evidenced by all major enterprise software vendors heavily investing into their own datacenters. Although infrastructure is a commodity it is a commodity that the big software vendors are depending upon — at a scale that makes them vulnerable.
PaaS is more complex than IaaS. It is the foundation for building thriving application ecosystems. It therefore needs to supply all services that are needed to efficiently build, deploy, sell, and manage business applications and business application services (aka micro services). It also needs to provide the foundational services that enable business application developers to concentrate on solving business problems. That’s why analytics, IoT services, machine learning infrastructures, AI services, middleware, database engines, and much more, are part of it, too.
It is the software platform that lays the foundation for being able to consistently engage in a way that can deliver positive experiences. The IaaS part of the platform makes sure that the software platform itself can deliver with low latency and the performance that is necessary at any time.
The ability to consistently engage in a way that can deliver positive experiences is why especially the PaaS platform is that important.
And then there is the challenge of on premise software and/or “private clouds” as well as increasing regulatory pressure that places demands on the storage location of data with the latter becoming a non-issue soon, as every significant IaaS vendor will be able to support upcoming regional legislation — doing so is a matter of survival for them.
The other matter of survival for them is whether and how they can attract business workloads to their infrastructures. And this means partnerships. No big enterprise software vendor — who all provided PaaS — will make itself dependent on one single, or only two IaaS providers.
Which means that the software platform will remain the decisive factor.
Of which we have largely four: Microsoft, Oracle, Salesforce, and SAP, discounting for specialists and ambitious startups like Freshworks or Zoho, amongst others.
As bland as it sounds, this is an individual decision. There is no one size fits it all.
The main criteria are:
The four vendors have very different strategies. All of them have their own infrastructure, but Microsoft and Oracle strongly prefer its own, while Salesforce and SAP are partnering, which also helps drive cost down.
Microsoft, Oracle, and SAP are full suite vendors, with offerings covering the full value chain, while Salesforce concentrates on the customer side of processes. Microsoft on top of this has a strong productivity suite and essentially ‘owns’ the office.
Microsoft, Salesforce and SAP are running thriving ecosystems, with Microsoft and Salesforce clearly having an edge over SAP. Oracle is less strong.
All four are investing heavily into important technologies like machine learning, blockchain, VR and AR. Oracle’s investments into AI based database security is something unique in this area; until Microsoft goes that route, too — followed by SAP, which has least data on database attacks — but likely lots on attacks on the application server.
Microsoft, followed by SAP have best access to data, Microsoft via LinkedIn and its stake in InsideSales, SAP via the Ariba network and now Gigya.
In brief: The race is on.
My personal view is that Microsoft, Oracle, and SAP have an edge over Salesforce, as these companies offer more of the value chain, in breadth and depth. Salesforce, on top of it, is not overly profitable. The company surely provides high value, but also at a high price. What if other vendors provide (or are perceived to provide) similar value at a lower price point? After all one major driver for a move into the cloud is lowering cost.
On the other hand at least Oracle and SAP, probably even Microsoft, can learn from Salesforce about eco systems.
Look out for what Microsoft is offering, especially if you are not (yet) a large enterprise but consider yourself upper end of mid market. If your concern is the whole value chain and you want to grow, look for SAP. Look for Oracle if you are an Oracle shop.
If your main, or only, concern is customer engagement, have a good look at Salesforce.
But always keep your existing infrastructure in mind.
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
See all (1,289)
6 
6 claps
6 
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/heroku-iaas-as-apples-apple-seeds-a1f9df03dcf9?source=search_post---------154,"There are currently no responses for this story.
Be the first to respond.
Executive Summary: This began as an email to a customer who wrote in asking Heroku to lower its prices to counter an offer of free credits from AWS. Over time, this email has morphed from a retention email into an detailed examination of the ways in which Heroku allows companies to laser-focus on innovation and business value — the main things that make and retain customers — instead of on keeping the lights on. Heroku’s best-in-class Developer Experience — and the freedom that comes from abstracting away a massive amount of operational complexity — is exactly why we do not compete directly on price with IaaS providers. Please read on through this open letter, and feel free to reach out with any questions or (constructive) criticism.
Hi Developer/Engineer/CTO,  I’ve put together what I hope is a helpful — if perhaps overly thorough — rubric by which you can compare Heroku against IaaS. It seems there can be a perception that Heroku is expensive when compared with IaaS. However, my thoughts below have been collected from > two years of witnessing businesses grapple with this deceptively challenging question: “Which IaaS/PaaS should I trust to host and scale my core service?” My answer to that question here primarily references AWS, however, the comparison is also still quite apt to Azure, GCP, and others. For the sake of laying out my case, I’ve structured my case into four sections, which are as follows:
I. Comparing Heroku to AWS
II. How do you address additional DevOps burden/complexity if you decide to leave Heroku?
III. How have things gone for customers who’ve left Heroku for AWS?
IV. So, what to DO?
Before I continue, I should admit that I obviously have a bias towards the platform that is my employer; however, I hope you’ll trust that I’ve strived to base my opinion here on unbiased 3rd party sources to help show why a move to an ostensibly “cheaper” IaaS provider is almost never as simple as it seems. For that reason, TL;DR: Heroku unit prices for customers paying on our monthly credit card billing model are fixed; however, I outline some ways you can take advantage of 3rd party services while still benefitting from Heroku’s market-leading DevEx in 3rd and 4th sections below. Finally, while this piece will read most cohesively from top to bottom, if you think of yourself as clearly within the target demographic and are short on time, feel free to skip to section IV.
As you may know, Heroku is built as a layer on top of AWS, so there has never been a time in Heroku’s history when we have been—or would want to be—in direct competition with them. Due to the substantial value that Heroku adds for developers and businesses — by abstracting away the true complexity of a direct deploy to AWS, comparing us (and our prices) to AWS is less like comparing apples to oranges and more like comparing apples to apple seeds. To put it mildly, the work that it would take simply to get an app or database—let alone an entire CI/CD pipeline—on any of the major IaaS providers to reach parity with what Heroku provides out of the box is massive, and is perhaps best illustrated by the maroon part of the graph in this image.
Comparing Heroku to AWS in terms of raw, surface-level costs completely ignores the value that we have built into our battle-hardened, reliable, durable, and secure platform. Over the last several years, we’ve been very public about the work we do to improve our platform’s compliance with International Standards and to address critical security gaps. Here are some posts from the Heroku blog covering ways in which we’ve handled security and compliance issues that you will need to address yourself on IaaS:
As you can hopefully see from the above, when you decide to run directly on IaaS, dealing with vulnerabilities and bringing your apps into compliance with internationally accepted standards falls to you. As a scary reminder of how frequently businesses forget this fact, Palo Alto Networks’ Unit42 recently found over 34 MILLION (!) preventable vulnerabilities in apps deployed across AWS, GCP, and Azure. Their key takeaway is “the threats are not the result of cloud providers themselves but the applications customers deploy on cloud infrastructure.” Read SiliconAngle’s summary here.
Moreover, the simplicity of continuous delivery and implementation (CI/CD) on Heroku is hard to overstate. I work with many developers who routinely tell me that Heroku pipelines — and especially Review Apps—are one of their favorite parts of our platform because they allow them to easily share their PRs with non-technical stakeholders to solicit feedback prior to merging into master. When they’re ready to merge, click a button to promote to staging or prod, and on to the next. While a variety of IaaS providers have been gaining ground, none of them can boast the same “Apple of App Dev” DX (<—real quote from a customer) that we offer. For more about why that is and why that’s important, check out Heroku Master Technical Architect Greg Nokes’ terrific overview video from earlier this month here.
For further unbiased reading, check out these 3rd party assessments of AWS vs. Heroku.
As a wide variety of blog posts have illustrated over the years, Heroku’s main value add is that we abstract away the complexity of managing your infrastructure yourself. Put another way, while some larger Heroku customers use us even with full-time DevOps on staff, we significantly reduce or delay the need to hire DevOps, particularly for small companies. The flip side to that, of course, is if you decide to go straight to IaaS, operational complexity increases significantly, and usually, you’ll need to hire at full-time DevOps engineer.
Let’s say you do hire a DevOps Engineer. The average DevOps Engineer Salary in the US is $131k per year according to Neuvoo. Glassdoor says it’s $99k. Either way — even if you are spending that much on Heroku each year, do you anticipate that a single DevOps engineer could do everything for your org that I’ve described above, with a nearly flawless track record?
Perhaps you have a family friend in Turkey — as one of my earliest customers did — who is pro DevOps and works for pennies on the dollar. Even then, quickly Googling “Average DevOps Salary [[insert your geography here]]” will not address a crucial factor: the cost to actually recruit, train, and retain good DevOps talent. While I don’t have specific $ values for that, other folks have already written extensively about the costs associated. Take a look at these two excellent blog posts discussing the costs to find and keep top-tier DevOps engineers:
“Well, we have a pretty lean startup. We’re just going to do DevOps ourselves!” I’ve heard many developers say this over the years, and of course, it’s an option. Yes, technically, you can skip hiring DevOps altogether and assign the added workload to your existing team. However, even after assuming that someone on your team has the requisite knowledge to effectively take on pro-level DevOps, many companies have small technical teams, and as such, each team member’s time is at a premium. So with that in mind, being lean and agile is exactly why you need to ask yourself the following question:
“What is the opportunity cost of taking time away from our existing day-to-day activities to focus on operational tasks that don’t contribute towards our core differentiators to our customers?”
Put another way, will your customers benefit more from an operational change that requires you to spend more of your time “wearing the pager”, so to speak? Or will they benefit more—and hopefully, stick around to keep paying you longer—if you instead take that time to iterate on and improve your core product or service?
Candidly, this is far from the first time I’ve had to make this case. In August 2018, I had a company reach out, asking me to discount Heroku based on an offer of $40k in credits from AWS. In response, I wrote to him almost exactly what I’ve written here. Experience being the best teacher, he decided to take the credits and migrate to AWS anyway. In February 2019 (about 6 months later), I received this response from him:
Just an update on this. We attempted the migration to AWS and… we failed. It turns out that Elastic Beanstalk isn’t right at the same level as Heroku in terms of PaaS features. Congrats on you guys [sic], as it shows that you’ve built a really good product over the years.
So I just wanted to let you know that we’ll be with you guys for the time being. It turns out you were right.
- Anonymous Heroku Customer, Feb 2019
This customer is now back and is happily running on Heroku today. You can also read about one of our other customers, Connect Space, who went on the record and wrote a full blog post about exactly this topic a year or so ago. Check out their story here.  Here’s another example from Bryan Woods, CTO of Rhino:
AWS gave us a ton of free credits to use, like tons of money. And I thought as a cost saving measure, maybe it would make a lot of sense to move our databases out to AWS just to save like 100 bucks a month on hosting. That ended up being a fool’s errand because once we got established, we lost those startup credits and then we had to do a project to move those databases back into Heroku…
So [now, on Heroku] we have all these tools, we’ve built a data warehouse, we can now roll back to any arbitrary point in time. We have these automated backups, we have these great metrics dashboards… All this stuff that we would have otherwise had to invest time and resources, if not building our own, then tying all these third party services together, it would just be silly. We’re not an infrastructure business.
You can also listen to Bryan tell this story on episode 67 of our podcast Code[ish]: “Launching a Startup in a Regulated Industry.”
This only covers three customers, so obviously I’m not presenting a scientific study. However, the issues that these businesses wrangled on IaaS are extremely common complaints amongst Heroku customers past and present. Most IaaS platforms are (on the surface, at least) endlessly customizable and as unfathomably vast in their power and capabilities as they are affordable in their prices. In reality, however, while you could certainly optimize your unit cost on apple pie if you grew your own apple trees from seeds, for most people, it’s just a whole lot simpler to go buy some apples.
On a related note, my team doesn’t only deal with customers jumping from Heroku -> IaaS; we’ve also seen plenty come from IaaS -> Heroku because they’ve spent years dealing with the ever-increasing operational complexity of managing a fully IaaS-based architecture and simply don’t want to deal with it anymore. For a deeper dive into why, check out John Vester’s 4 part series:
Ultimately, the marginal utility of spending a ton of money and time doing SRE rarely becomes a competitive advantage in 2020, or at least not until you are routinely signing uptime SLAs with your customers. And well, let’s just say, Azure doesn’t guarantee better than 99.9% uptime and they just won the $10B JEDI contract… meanwhile, Heroku is showing 99.9985% (US) and 99.9999% (EU) uptime over the last 60 days.
So unless you can get in a time machine to 2002 and beat Benjamin Treynor Sloss to the punch by a year, chances are high that taking on this particular type of complexity in 2020 will neither create a core differentiator for your business, nor will it fill your garage with McLarens like it did his.
A lot of cost bloat on Heroku can be solved with one word: Optimize! Relatively few customers seem to take advantage of seemingly basic cost optimization tools like auto-scaling. For more information, check out these add-ons.
For further questions about case-by-case cost optimization strategies, feel free free to reach out to our team here or check out the reams of info on StackOverflow.
For what it’s worth, there are absolutely still ways that you can benefit from using cutting edge technologies from IaaS providers while simultaneously keeping your mission-critical app pipeline on Heroku. And if you have credits, use them! Instead of thinking that this has to be some sort of Heroku OR AWS zero-sum game, do what Heroku customers have been doing for years now and leverage hybrid- or multi-cloud architectures. AWS offers a wide variety of services for which Heroku does not provide a corollary: Redshift, Cassandra, Spark, S3, Cloudfront, etc. And if you don’t have credits kicking around, check out the wide array of data store and data utility add-ons in Heroku’s Elements Add-on Marketplace—which allow you to provision S3 buckets and a wide array of other 3rd party services—directly from the dashboard or CLI of your Heroku account. For further research, check out the following resources:
Finally and critically, many Heroku customers find that they’re able to unlock more attractive unit pricing via a Heroku Enterprise annual contract. Enterprise isn’t a panacea by any means. However, it is the primary vehicle that growing businesses on Heroku use to access crucially important features and benefits, such as:
While Heroku has gradually been making Enterprise-grade services available to credit card customers (on the monthly model), Enterprise is often the best way for companies of all sizes to access the innovations that our product and engineering teams have put their blood, sweat, and tears into building and hardening over the last half-decade or so. To be abundantly clear, this is not a paean to the virtues of Heroku Enterprise. This section is instead a call to action to any CTOs, Lead Developers, Engineers, Product Managers, or other technical stakeholders, especially in the SMB to Mid-Market space.
If you feel as though you’re bumping up against some perceived limitations in your Heroku credit card account, you owe it to yourself and to your team to take a good, hard look at Heroku Enterprise (read: contact us!) before deciding that you need to lift and shift to another platform or provider.  — Me, Just Now
If you made it this far, thank you for reading! I hope this perspective has (a) helped clarify the value Heroku provides and (b) provided some ideas as to how you might proceed going forward.
I welcome any and all questions thoughtful feedback.  Best, David
Get smarter at building your thing. Join The Startup’s +750K followers.
95 
95 claps
95 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
Unlocking Business Value & Social Impact through Developer Success @ Heroku || Moonlighting as a solo Musician, Cook, & Gamer
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/@soniacomp/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-iaas-%EB%A5%BC-%EC%A7%80%ED%83%B1%ED%95%98%EB%8A%94-%EA%B8%B0%EC%88%A0-1-%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-3087c1bd0c38?source=search_post---------207,"Sign in
There are currently no responses for this story.
Be the first to respond.
SoniaComp
Apr 15, 2021·47 min read
운영체제에서 작동하는 응용 프로그램을 관리하는 기능이다.
어떤 의미에서는 프로세서(CPU) 관리하는 것이라고 볼 수도 있다.  → 현재 CPU를 점유해야 할 프로세스를 결정 → 이 프로세스 간 공유 자원 접근과 통신 등을 관리
프로세스는 각각 별도의 메모리 주소 공간을 할당한다. (독립적) → Code: 코드 자체를 구성하는 메모리 영역(프로그램 명령) → Data: 전역 변수, 정적 변수, 배열 등(초기화된 데이터) → Heap: 동적 할당 시 사용(new(), malloc() 등) → Stack: 지역 변수, 매개 변수, 리턴 값(임시 메모리 영역)
공간을 나눈 이유최대한 데이터를 공유하여 메모리 사용량을 줄여야 한다. → 전역변수와 지역변수(LIFO)의 특징 때문에 메모리를 분리했다.
기본적으로 프로세스마다 최소 1개의 스레드 소유(메인 스레드 포함)하나의 프로세스가 생성될 때, 기본적으로 하나의 스레드 같이 생성
장점: 안전성 (메모리 침범 문제를 OS 차원에서 해결)
단점: 각각 독립된 메모리 영역을 갖고 있어, 작업량 많을 수록 오버헤드 발생. Context Switching으로 인한 성능 저하
Context Switching: 프로세스의 상태 정보를 저장하고 복원하는 일련의 과정 → 동작 중인 프로세스가 대기하면서 해당 프로세스의 상태를 보관(PCB)하고, 대기하고 있던 다음 순번의 프로세스가 동작하면서 이전에 보관했던 프로세스 상태를 복구하는 과정을 말함→ 프로세스는 각 독립된 메모리 영역을 할당받아 사용되므로, 캐시 메모리 초기화와 같은 무거운 작업이 진행되었을 때 오버헤드가 발생할 문제가 존재함
장점: 독립적인 프로세스에 비해 공유 메모리만큼의 시간, 자원 손실이 적음. 전역변수와 정적 변수에 대한 자료를 다른 스레드와 공유
단점: 안정성 문제. 하나의 스레드가 데이터 공간을 망가뜨리면, 모든 스레드가 작동 불능 상태 → Critical Section: 하나의 스레드가 공유 데이터 값을 변경하는 시점에 다른 스레드가 그 값을 읽으려 할 때 발생하는 문제를 해결하기 위한 동기화 과정(상호 배제, 진행, 한정된 대기를 충족해야함)
현재 공유자원에 접근할 수 있는 쓰레드, 프로세스의 수를 나타내는 값을 두어 상호배제를 달성하는 기법
임계 영역을 가진 쓰레드들의 Running Time이 서로 겹치지 않게 각각 단독으로 실행하게 하는 기술이다. 다중 프로세스들의 공유 리소스에 대한 접근을 조율하기 위해 locking과 unlocking을 사용한다.
프로그램을 실행하는 도중에 예기치 않은 상황이 발생할 경우 현재 실행중인 작업을 즉시 중단하고, 발생된 상황을 우선 처리한 후 실행중이던 작업으로 복귀하며 계속 처리하는 것
fork새로운 Process를 생성할 때 사용
waitchild 프로세스가 종료될 때까지 기다리는 작업
exec단순 fork는 동일한 프로세스의 내용을 여러 번 동작할 때 사용함.child에서는 parent와 다른 동작을 하고 싶을 때는 exec를 사용할 수 있음.execvp( 실행 파일, 전달 인자 ) 함수는, code segment 영역에 실행 파일의 코드를 읽어와서 덮어 씌운다. 씌운 이후에는, heap, stack, 다른 메모리 영역이 초기화되고, OS는 그냥 실행한다. 즉, 새로운 Process를 생성하지 않고, 현재 프로그램에 wc라는 파일을 실행한다. 그로인해서, execvp() 이후의 부분은 실행되지 않는다.
CPU가 프로세스가 여러개일 때, CPU 스케줄링을 통해 관리하는 것을 말함
PCB(Process Control Block): 프로세스 메타데이터들을 저장해 놓는 곳, 한 PCB 안에는 한 프로세스의 정보가 담김 → Process Meta data(Process ID, Process State, Process Priority, CPU Registers, Owner, CPU Usage, Memory Usage)
PCB 관리 방법: Linked List 방식으로 관리PCB List Head에 PCB들이 생성될 때마다 붙게 된다. 주소 값으로 연결이 이루어져 있는 연결리스트이기 때문에 삽입 삭제가 용이하다. → 프로세스가 생성되면 해당 PCB가 생성되고 프로세스 완료 시 제거된다
Context Switching수행중인 프로세스를 변경할 때, CPU 레지스터 정보가 변경되는 것CPU가 이전의 프로세스 상태를 PCB에 보관하고, 또 다른 프로세스의 정보를 PCB에 읽어 레지스터에 적재하는 과정보통 인터럽트가 발생하거나, 실행중인 CPU 사용 허가 시간을 모두 소모하거나, 입출력을 위해 대기해야 하는 경우 Context Switching이 발생한다.즉, 프로세스가 Ready → Running, Running → Ready, Running → Waiting처럼 상태 변경 시 발생!
Context Switching Overhead프로세스 작업 중에 Overhead를 감수해야 하는 상황이 있다.프로세스를 수행하다가 입출력 이벤트가 발생해서 대기 상태로 전환시킴.이때, CPU를 그냥 놀게 놔두는 것보다 다른 프로세스를 수행시키는 것이 효율적이다. 따라서, CPU에 계속 프로세스를 수행시키도록 하기 위해서 다른 프로세스를 실행시키고 Context Switching을 하는 것이다. CPU가 놀지 않도록 만들고, 사용자에게 빠르게 일처리를 제공해주기 위한 것이다.
프로세스는 독립적으로 실행된다. 즉, 다른 프로세스에게 영향을 받지 않는다. 프로세스는 커널이 제공하는 IPC 설비를 이용해 프로세스간 통신을 할 수 있게 된다.1) 익명 PIPE파이프는 두 개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고, 다른 하나는 데이터를 읽기만 할 수 있다.한쪽 방향으로만 통신이 가능한 반이중 통신이라고도 부른다.따라서 양쪽으로 모두 송/수신을 하고 싶으면 2개의 파이프를 만들어야 한다.매우 간단하게 사용할 수 있는 장점이 있고, 단순한 데이터 흐름을 가질 땐 파이프를 사용하는 것이 효율적이다. 단점으로는 전이중 통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우에 사용한다. (부모-자식 프로세스 간 통신처럼)2) Named PIPE(FIFO)Named 파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다.즉, 익명 파이프의 확장된 상태로 부모 프로세스와 무관한 다른 프로세스도 통신이 가능한 것 (통신을 위해 이름있는 파일을 사용)하지만, Named 파이프 역시 읽기/쓰기 동시에 불가능함. 따라서 전이중 통신을 위해서는 익명 파이프처럼 2개를 만들어야 가능3) Message Queue입출력 방식은 Named 파이프와 동일함다른점은 메시지 큐는 파이프처럼 데이터의 흐름이 아니라 메모리 공간이다.사용할 데이터에 번호를 붙이면서 여러 프로세스가 동시에 데이터를 쉽게 다룰 수 있다.메시지를 queue 데이타 구조 형태로 관리 → 메시지큐는 커널에서 전역적으로 관리되며(이를테면 커널 전역변수형태로), 모든 프로세스에서 접근가능하도록 되어있으므로, 하나의 메시지큐 서버가 커널에 요청해서 메시지큐를 작성하게 되면, 메시지큐의 접근자(식별자)를 아는 모든 프로세서는 동일한 메시지큐에 접근함으로써, 데이타를 공유할수 있게 된다.메시지큐의 IPC로써의 특징은 다른 공유방식에 비해서 사용방법이 매우 직관적이고 간단하다라는데 있다. 다른 코드의 수정없이 단지 몇줄만의 코드를 추가시킴으로써 간단하게 메시지큐에 접근할수 있다.또한 뒤에서 자세히 다루겠지만 메시지의 type 에 의해서 여러종류의 메시지를 효과적으로 다룰수 있는 장점을 가지고 있다. 여러개의 프로세스가 하나의 메시지큐를 엑세스 할때, 각 메시지에 type 를 줌으로써 각 프로세스에게 필요로하는 메시지만을 가져가게 할수 있는 상당히 편리한 기능을 제공한다.4) 공유 메모리파이프, 메시지 큐가 통신을 이용한 설비라면, 공유 메모리는 데이터 자체를 공유하도록 지원하는 설비다.공유 메모리는 프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용해준다. 프로세스가 공유 메모리 할당을 커널에 요청하면, 커널은 해당 프로세스에 메모리 공간을 할당해주고 이후 모든 프로세스는 해당 메모리 영역에 접근할 수 있게 된다.중개자 없이 곧바로 메모리에 접근할 수 있어서 IPC 중에 가장 빠르게 작동함5) 메모리 맵공유 메모리처럼 메모리를 공유해준다. 메모리 맵은 열린 파일을 메모리에 맵핑시켜서 공유하는 방식이다. (즉 공유 매개체가 파일+메모리)주로 파일로 대용량 데이터를 공유해야 할 때 사용한다.6) 소켓네트워크 소켓 통신을 통해 데이터를 공유한다.클라이언트와 서버가 소켓을 통해서 통신하는 구조로, 원격에서 프로세스 간 데이터를 공유할 때 사용한다. → 서버(bind, listen, accept), 클라이언트(connect)
IPC 통신에서 프로세스 간 데이터를 동기화하고 보호하기위해 사용(공유된 자원에 한번에 하나의 프로세스만 접근할 수 있도록 제한)
임계구역(Critical Section)여러 프로세스가 데이터를 공유하며 수행될 때, 각 프로세스에서 공유 데이터를 접근하는 프로그램 코드 부분공유 데이터를 여러 프로세스가 동시에 접근할 때, 잘못된 결과를 만들 수 있기 때문에, 한 프로세스가 임계 구역을 수행할 때, 다른 프로세스가 접근하지 못하도록 해야 함
뮤텍스 : 한 스레드, 프로세스에 의해 소유될 수 있는 Key를 기반으로 한 상호배제 기법
세마포어 : 현재 공유자원에 접근할 수 있는 스레드, 프로세스의 수를 나타내는 값을 두어 상호배제를 달성하는 기법
뮤텍스와 세마포어의 목적은 특정 동기화 대상이 이미 특정 스레드나 프로세스에 의해 사용중일 경우, 다른 스레드가 해당 동기화 대상에 접근하는 것을 제한하는 것으로 동일하지만, 관리하는 동기화 대상이 몇개인가에 따라 차이가 생긴다.
두 기법 모두 완벽하지는 않다. 이 기법들을 사용하더라도 데이터 무결성을 보장할 수 없으며 데드락이 발생할 수도 있다. 하지만 상호배제를 위한 기본적인 기법이며 여기에 좀 더 복잡한 매커니즘을 적용해 꽤나 우아하게 동작하는 프로그램을 짤 수 있다고 한다.
유저모드와 커널모드: 유저 어플리케이션이 함부로 운영체제의 치명적인 데이터를 수정하거나 삭제하기 못하게 하기 위해 분리되어 존재. 커널모드는 모든 시스템과 메모리에 접근이 허가된 프로세스 실행모드이다. 유저모드보다 커널 모드에 더 높은 권한을 줌으로써, 유저모드에서 에러가 발생했을 때 시스템 전체의 안전성을 보장해준다.
사용자가 직접적으로 하드웨어 장치를 제어한다면, 큰 문제가 발생할 수 있기때문에, 사용자 애플리케이션은 System Call을 통해 직접적인 하드웨어 요청이나 중요한 시스템 요청을 한다. 요청을 하면, 유저 애플리케이션은 유저 모드에서 커널 모드로 잠시 전환 되었다가 커널 모드에서 작업을 실행한 뒤 응답을 유저 애플리케이션에 반환하면서 다시 유저 모드로 되돌아가게 된다. → 이러한 구조를 갖춤으로써, 사용자 프로세스가 운영체제와 데이터를 함부로 들여다보거나 변경하지 못하게 보호한다.
유저모드: 사용자 애플리케이션의 코드가 실행됨, 시스템 데이터에 제한된 접근만이 허용. 하드웨어 직접 접근 불가. 시스템 서비스 호출시 유저 모드에서 커널 모드로 잠시 전환됨. 스레드는 자신만의 유저모드 스택을 가짐커널모드: 모든 시스템 메모리에 접근할 수 있고, 모든 CPU 명령 실행 가능. 운영체제 코드나 디바이스 드라이버 같은 커널 모드 코드를 실행한다.
[ 유저 모드의 동기화 ]커널의 힘을 빌리지 않는 동기화 기법(커널의 코드가 실행되지 않음.)성능상 이점이 있으나 기능상의 제한점이 존재.임계 구역 기반의 동기화, 인터락 함수 기반의 동기화.
[ 커널 모드의 동기화 ]커널에서 제공하는 동기화 기능을 이용하는 방법.커널 모드로의 변경이 필요하고 이는 성능 저하로 이어진다. 그러나 다양한 기능을 활용할 수 있다.세마포어, 뮤텍스, 모니터 등등.
멀티프로그래밍 환경에서 공유 자원에 대한 접근을 제한하는 방법
P : 임계 구역 들어가기 전에 수행 ( 프로세스 진입 여부를 자원의 개수(S)를 통해 결정)
V : 임계 구역에서 나올 때 수행 ( 자원 반납 알림, 대기 중인 프로세스를 깨우는 신호 )
한 프로세스가 P 혹은 V를 수행하고 있는 동안 프로세스가 인터럽트 당하지 않게 된다. P와 V를 사용하여 임계 구역에 대한 상호배제 구현이 가능하게 되었다.
임계 구역을 가진 스레드들의 실행시간이 서로 겹치지 않고 각각 단독으로 실행되게 하는 기술(상호 배제(Mutual Exclusion)의 약자임)
커널: 운영체제의 핵심적인 부분으로, 다른 모든 부분에 여러 기본적인 서비스를 제공해줌
프로세스의 상태 전이
선점 / 비선점 스케줄링선점 (preemptive) : OS가 CPU의 사용권을 선점할 수 있는 경우, 강제 회수하는 경우 [ I/O or Event Wait ]비선점 (nonpreemptive) : 프로세스 종료 or I/O 등의 이벤트가 있을 때까지 실행 보장 (처리시간 예측 어려움) [ Interrupt, Scheduler Dispatch ]
스케줄링 척도Response Time: 작업이 처음 실행되기까지 걸린 시간Turnaround Time: 실행 시간과 대기 시간을 모두 합한 시간으로 작업이 완료될 때 까지 걸린 시간
공유 자원에 대해 여러 프로세스가 동시에 접근할 때, 결과값에 영향을 줄 수 있는 상태 (동시 접근 시 자료의 일관성을 해치는 결과가 나타남)
프로세스가 자원을 얻지 못해서 다음 처리를 하지 못하는 상태‘교착 상태’라고도 부름시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생
데드락이 발생하는 경우멀티 프로그래밍 환경에서 한정된 자원을 얻기 위해 서로 경쟁하는 상황 발생한 프로세스가 자원을 요청했을 때, 동시에 그 자원을 사용할 수 없는 상황이 발생할 수 있음. 이때 프로세스는 대기 상태로 들어감대기 상태로 들어간 프로세스들이 실행 상태로 변경될 수 없을 때 ‘교착 상태’ 발생
1차 저장장치에 해당하는 메인 메모리와 2차 저장장치에 해당하는 하드디스크, NAND(플래시 메모리) 등을 관리하는 기능이다.
메인 메모리는 CPU가 직접 접근할 수 있는 접근 장치프로세스가 실행되려면 프로그램이 메모리에 올라와야 함메인 메모리는 주소가 할당된 일련의 바이트들로 구성되어 있다.
CPU는 레지스터가 지시하는대로 메모리에 접근하여 다음에 수행할 명령어를 가져온다. 명령어 수행 시 메모리에 필요한 데이터가 없으면 해당 데이터를 우선 가져와야 한다. 이 역할을 하는 것이 바로 MMU다.
메모리 관리장치(MMU)는 논리주소를 물리주소로 변환해준다. 뿐만 아니라 메모리 보호나 캐시 관리 등 CPU가 메모리에 접근하는 것을 총 관리해주는 하드웨어다. 메모리의 공간이 한정적이기 때문에, 사용자에게 더 많은 메모리를 제공하기 위해 ‘가상 주소’라는 개념이 등장했다. (가상 주소는 프로그램 상에서 사용자가 보는 주소 공간이라고 보면 됨) 이 가상 주소에서 실제 데이터가 담겨 있는 곳에 접근하기 위해선 빠른 주소 변환이 필요한데, 이를 MMU가 도와주는 것이다. 또한 메인 메모리의 직접 접근은 비효율적이므로, CPU와 메인 메모리 속도를 맞추기 위해 캐시가 존재한다.
MMU의 메모리 보호프로세스는 독립적인 메모리 공간을 가져야 되고, 자신의 공간만 접근해야 한다. 따라서 한 프로세스에게 합법적인 주소 영역을 설정하고, 잘못된 접근이 오면 trap을 발생시키며 보호한다.
base와 limit: 레지스터를 활용한 메모리 보호 기법base: 레지스터는 메모리상의 프로세스 시작주소를 물리 주소로 저장limit: 레지스터는 프로세스의 사이즈를 저장이로써 프로세스의 접근 가능한 합법적인 메모리 영역(x)은 base ≤ x < base+limit이 영역 밖에서 접근을 요구하면 trap을 발생시킨다.안전성을 위해 base와 limit 레지스터는 커널 모드에서만 수정 가능하도록 설계한다. (사용자 모드에서는 직접 변경할 수 없도록)
메모리 과할당(over allocating)실제 메모리의 사이즈보다 더 큰 사이즈의 메모리를 프로세스에 할당한 상황페이지 기법과 같은 메모리 관리 기법은 사용자가 눈치 채지 못하도록 눈속임을 통해 메모리를 할당해준다. (가상 메모리를 이용해서)
이러한 과할당을 해결하기 위해선, 빈 프레임을 확보할 수 있어야 한다.
페이징 기법은 사용자 모르게 시스템 능률을 높이기 위해 선택한 일이므로 들키지 않게 처리해야함
페이지 교체메모리 과할당이 발생했을 때, 프로세스 하나를 swap out해서 빈 프레임을 확보하는 것
페이지 교체가 이루어지면 아무일이 없던것 처럼 프로세스를 계속 수행시켜주면서 사용자가 알지 못하도록 해야 한다. 이때, 아무일도 일어나지 않은 것처럼 하려면, 페이지 교체 당시 오버헤드를 최대한 줄여야 한다
페이지 교체 오버헤드이처럼 빈 프레임이 없는 상황에서 victim 프레임을 비울 때와 원하는 페이지를 프레임으로 올릴 때 두 번의 디스크 접근이 이루어진다.페이지 교체가 많이 이루어지면, 이처럼 입출력 연산이 많이 발생하게 되면서 오버헤드 문제가 발생한다.
페이지 교체 오버헤드 해결방법
주기억장치에 저장된 내용의 일부를 임시로 저장해두는 기억장치CPU와 주기억장치의 속도 차이로 성능 저하를 방지하기 위한 방법
CPU가 이미 봤던걸 다시 재접근할 때, 메모리 참조 및 인출 과정에 대한 비용을 줄이기 위해 캐시에 저장해둔 데이터를 활용한다캐시는 플리플롭 소자로 구성되어 SRAM으로 되어있어서 DRAM보다 빠른 장점을 지녔다.
CPU와 기억장치의 상호작용CPU에서 주소를 전달 → 캐시 기억장치에 명령이 존재하는지 확인(존재) Hit: 해당 명령어를 CPU로 전송 → 완료(비존재) Miss: 명령어를 갖고 주기억장치로 접근 → 해당 명령어를 가진 데이터 인출 → 해당 명령어 데이터를 캐시에 저장 → 해당 명령어를 CPU로 전송 → 완료
지역성의 원리: 캐시 적중률을 극대화시키기 위한 방법기억 장치 내의 정보를 균일하게 액세스 하는 것이 아니라 한 순간에 특정부분을 집중적으로 참조하는 특성시간 지역성 : 최근에 참조된 주소의 내용은 곧 다음에도 참조되는 특성공간 지역성 : 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성
캐싱 라인빈번하게 사용되는 데이터들을 캐시에 저장했더라도, 내가 필요한 데이터를 캐시에서 찾을 때 모든 데이터를 순회하는 것은 시간 낭비다.즉, 캐시에 목적 데이터가 저장되어있을 때 바로 접근하여 출력할 수 있어야 캐시 활용이 의미있어진다.따라서 캐시에 데이터를 저장할 시, 자료구조를 활용해 묶어서 저장하는데 이를 캐싱 라인이라고 부른다.즉, 캐시에 저장하는 데이터에 데이터의 메모리 주소를 함께 저장하면서 빠르게 원하는 정보를 찾을 수 있다. (set이나 map 등을 활용)
CPU 칩에는 여러 개의 캐시가 들어가며, 각각의 캐시는 각자의 목적과 역할을 가지고 있다.
컴퓨터에서 파일이나 자료를 쉽게 발견할 수 있도록, 유지 및 관리하는 방법이다. 저장매체에는 수많은 파일이 있기 때문에, 이런 파일들을 관리하는 방법을 말한다
특징커널 영역에서 동작파일 CRUD 기능을 원활히 수행하기 위한 목적계층적 디렉터리 구조를 가짐디스크 파티션 별로 하나씩 둘 수 있음
역할파일 관리보조 저장소 관리파일 무결성 메커니즘접근 방법 제공
개발 목적하드디스크와 메인 메모리 속도차를 줄이기 위함파일 관리하드디스크 용량 효율적 이용
구조메타 영역 : 데이터 영역에 기록된 파일의 이름, 위치, 크기, 시간정보, 삭제유무 등의 파일 정보데이터 영역 : 파일의 데이터
[외부 단편화(external fragmentation)]프로그램의 크기보다 분할의 크기가 작은 경우, 해당 분할이 비어있음에도 불구하고 프로그램을 적재하지 못하기 때문에 발생하는 메모리 공간을 말한다.어떤 프로그램에도 배당되지 않은 빈 공간임에도 현재 상태에서 사용될 수 없는 작은 분할이다.
[내부 단편화(internal fragmentation)]프로그램의 크기보다 분할의 크기가 큰 경우, 해당 분할에 프로그램을 적재하고 남는 메모리 공간을 말한다.즉, 하나의 분할 내부에서 발생하는 사용되지 않는 메모리 조각이다.ex) 메모리 자유 분할 공간이 10,000B가 있고 프로세스 A가 9,999B를 사용하게 되면 2B가 남게 된다. 이러한 현상을 내부 단편화라 한다.
[압축(Compaction)]외부 단편화를 해소하기 위해 프로세스가 사용하는 공간들을 한쪽으로 몰아, 자유 공간을 확보하는 방법론이다.단점 : 작업 효율이 좋지 않다.
[Swapping]메모리에 올라온 프로세스의 주소 공간 전체를 디스크의 스왑 영역으로 일시적으로 내려놓는 것이다. 메모리 공간을 확보하면 이후에 다른 프로세스의 메모리를 불러 들일 수 있다.주의할 점은 프로세스가 종료되어 주소 공간을 디스크로 내쫓는 것이 아니라 특정한 이유로 수행중인 프로세스의 주소공간을 일시적으로 메모리에서 디스크로 내려놓은 것을 의미한다.스왑인 : 디스크 -> 메모리스왑 아웃 : 메모리 -> 디스크
[연속 할당 방식]각각의 프로세스를 물리적 메모리의 연속적인 공간에 올리는 방식
[불연속 할당 방식]하나의 프로세스를 물리적 메모리의 여러 영역에 분산하여 적재하는 방식
[ 연속 메모리 관리 ]프로그램 전체가 하나의 커다란 공간에 연속적으로 할당되어야 함고정 분할 기법 : 주기억장치가 고정된 파티션으로 분할 (내부 단편화 발생)동적 분할 기법 : 파티션들이 동적 생성되며 자신의 크기와 같은 파티션에 적재 (외부 단편화 발생)
[ 불연속 메모리 관리 ]프로그램의 일부가 서로 다른 주소 공간에 할당될 수 있는 기법페이지 : 고정 사이즈의 작은 프로세스 조각프레임 : 페이지 크기와 같은 주기억장치 메모리 조각단편화 : 기억 장치의 빈 공간 or 자료가 여러 조각으로 나뉘는 현상세그먼트 : 서로 다른 크기를 가진 논리적 블록이 연속적 공간에 배치되는 것
[ 단순 페이징 ]각 프로세스는 프레임들과 같은 길이를 가진 균등 페이지로 나뉨외부 단편화 X소량의 내부 단편화 존재
[ 단순 세그먼테이션 ]각 프로세스는 여러 세그먼트들로 나뉨내부 단편화 X, 메모리 사용 효율 개선, 동적 분할을 통한 오버헤드 감소외부 단편화 존재
[ 가상 메모리 페이징 ]단순 페이징과 비교해 프로세스 페이지 전부를 로드시킬 필요X필요한 페이지가 있으면 나중에 자동으로 불러들어짐외부 단편화 X복잡한 메모리 관리로 오버헤드 발생
[ 가상 메모리 세그먼테이션 ]필요하지 않은 세그먼트들은 로드되지 않음필요한 세그먼트 있을때 나중에 자동으로 불러들어짐내부 단편화X복잡한 메모리 관리로 오버헤드 발생
페이지 부재 발생 → 새로운 페이지를 할당해야 함 → 현재 할당된 페이지 중 어떤 것 교체할 지 결정하는 방법(Why? : 만약 수정되면 메인 메모리에서 내보낼 때, 하드디스크에서 또 수정을 진행해야 하므로 시간이 오래 걸림. 이와 같은 상황에서 상황에 맞는 페이지 교체를 진행하기 위해 페이지 교체 알고리즘이 존재하는 것!)
Page Reference StringCPU는 논리 주소를 통해 특정 주소를 요구함메인 메모리에 올라와 있는 주소들은 페이지의 단위로 가져오기 때문에 페이지 번호가 연속되어 나타나게 되면 페이지 결함 발생 X따라서 CPU의 주소 요구에 따라 페이지 결함이 일어나지 않는 부분은 생략하여 표시하는 방법이 바로 Page Reference String
교체방식Global 교체: 메모리 상의 모든 프로세스 페이지에 대해 교체하는 방식Local 교체: 메모리 상의 자기 프로세스 페이지에서만 교체하는 방식다중 프로그래밍의 경우, 메인 메모리에 다양한 프로세스가 동시에 올라올 수 있음. 따라서, 다양한 프로세스의 페이지가 메모리에 존재. 페이지 교체 시, 다양한 페이지 교체 알고리즘을 활용해 victim page를 선정하는데, 선정 기준을 Global로 하느냐, Local로 하느냐에 대한 차→ 실제로는 전체를 기준으로 페이지를 교체하는 것이 더 효율적이라고 함. 자기 프로세스 페이지에서만 교체를 하면, 교체를 해야할 때 각각 모두 교체를 진행해야 하므로 비효율적
TCP/IP 기반의 인터넷에 연결하거나, 응용 프로그램이 네트워크를 사용하려면 운영체제에서 네트워크 프로토콜을 지원해야 한다.
운영체제에서 분산 처리는 컴퓨터 사용자 간에 서로 데이터를 교환하여 처리할 수 있도록 네트워크로 상호 연결한 것 → 네트워크로 연결한 시스템은 사용자의 액세를 제어하여 편리하게 자원을 공유할 수 있게 한다.
네트워크로 연결한 시스템은 분산 시스템과 다중 시스템으로 나눈다. 분산 시스템은 메모리와 클록을 공유하지 않고 지역 메모리를 유지하는 프로세서로 구성되는데, 서로 독자적으로 동작한다.다중 처리 시스템은 병렬 처리 시스템이라고도 하는데, 하나 이상의 프로세스로 구성되며, 프로세스들이 메모리와 출력을 공유한다. 이 절에서는 분산 시스템을 지원하는 네트워크의 구성, 분산 시스템의 목적과 특징을 알아본다.
네트워크: 서로 독립된 시스템 몇개가 적절한 영역 안에서 속도가 빠른 통신 채널을 이용하여 상호 통신할 수 있도록 지원하는 데이터 통신 시스템
[ 강결합 시스템 ] 프로세서들이 메모리를 공유하는 다중 처리 시스템이다. 프로세서 간에 공유 메모리를 이용하여 통신하므로 공유 메모리를 차지하려는 프로세서 간의 경쟁을 최소화해야 한다. 프로세서 간의 경쟁은 결합 교환 방법으로 해결할 수 있다. 이는 하나의 공유 메모리를 차지하려는 프로세서 여러개 중 오직 하나의 프로세서만 액세스를 허용하는 것이다.
[ 약결합 시스템 ]둘 이상의 독립된 시스템을 통신선으로 연결한다. 각 시스템은 자신만의 운영체제, 메모리, 프로세서, 입출력 장치 등이 있으며, 독립적으로 운영한다. 그리고 필요할 때, 통신선을 이용하여 메시지 전달이나 원격 프로시저 호출로 통신한다. 통신선으로 다른 시스템의 파일을 참조할 수 있으며, 각 시스템의 부하를 조절하려고 부하가 적은 프로세서에 작업을 보낼 수도 있다. 하나의 시스템에서 장애가 발생해도 다른 시스템의 프로세서를 독립적으로 수행할 수 있어 치명적인 시스템 장애는 발생하지 않는다.
분산 시스템과 단일 시스템의 가장 큰 차이는 프로세스 간 통신이다. 단일 시스템에서 프로세스들은 공유 메모리로 입출력하여 서로 통신할 수 있으나, 분산 시스템은 공유 메모리가 없다. 분산 시스템에서 프로세스 간 통신은 클라이언트와 서버의 요청, 응답 형태로 구현할 수 있는데, 대표적인 예가 원격 프로시저 호출이다.
전송 데이터를 준비하고 수신 데이터를 변환해서 올바르게 해석할 수 있도록 지원하여 처리 결과를 교환하는 모듈을 스터브(stub)라고 한다. 스터브는 RPC나 지역 호출이 같도록 만드는데, 클라이언트 스터브와 서버 스터브로 분류할 수 있다. 이런 스터브 프로그램은 데이터를 전송하려고 다양한 형태의 데이터로 서로 변환하는 기능을 수행한다.
공유 메모리와 공유 클록이 없는 프로세서의 집합으로, 각 프로세서는 자신들의 메모리와 통신 회선을 이용하여 서로 정보를 교환한다. 또 여러 사용자가 자원을 공유하여 대규모 작업을 지원하기 때문에, 다양한 사용자에게 서비스 할 수 있다.
분산 시스템 사용자들이 서로 다른 장소에 위치한 컴퓨터를 하나의 통합된 컴퓨팅 장비로 인식하여 시스템 자원의 위치에 관계없이 사용할 수 있는 환경을 제공한다. 즉, 분리된 상태에서 컴퓨터 통신 네트워크로 약결합을 하는 정보 시스템이다. → 분산 시스템의 목적은 사용자가 하나의 컴퓨터 시스템으로 인식할 수 있도록 완전한 투명성을 제공하는 것이다.
단일 사용자 환경인 과거 운영체제에 네트워크 기능을 추가통신을 제어하고, 분산된 자원을 공유하면서 독립된 시스템들을 서로 연결하려고 개발되었다.
[ 피투피 peer-to-peer 모델 ]장점) 클라이언트/서버 기능을 한 컴퓨터에서 구현함으로 적은 초기 비용, 자원의 활용 극대화와 자원을 각각 균일하게 공유단점) 파일과 응용 프로그램에 중앙 저장소 없이 분산되므로 관리하기가 어렵고 서버와 클라이언트에 보안을 제공하지 못한다.
[ 클라이언트 / 서버 모델 ]장점) 서버에서 자원과 데이터 보안을 제어하고, 새로운 기술을 시스템에 쉽게 통합 할 수 있다. 서버를 여러 플랫폼이나 원격으로 액세스할 수 있는 접근성이 좋다.단점) 전용 서버에 초기 투자가 필요하고, 네트워크 운영체제 소프트웨어가 필요하며, 서버에 의존성이 높다. 대형 네트워크 작업을 효율적으로 동작할 수 있는 관리자가 필요하다.
사용자가 시스템에 연결된 모든 원격 컴퓨터의 원격 자원을 지역 자원으로 공유. 네트워크 운영체제의 지역적인 자원 관리와 지역제어의 제한을 벗어나 시스템 자원의 전역 제어 및 관리의 필요성에 따라 발전되었다. 공동 운영체제로 사용자에게 시스템이 제공하는 여러 자원에 액세스가 가능한 참조 투명성을 제공한다. 분산된 컴퓨터 간의 자원을 쉽게 공유하고 액세스 할 수 있는 운영체제이다.
연산: 데이터 이동, 연산 이동, 프로세스 이동
[ 분산 시스템의 교착 상태 ]자원 할당 교착 상태, 메시지 전송 교착 상태, 통신 교착 상태
[ 3계층의 클라이언트 / 서버 시스템 ] 분산 시스템 환경이 한 대의 메인 컴퓨터가 모든 일을 처리하는 중앙 집중화된 환경에서 여러 대의 컴퓨터가 작업을 기능별로 분담하여 처리하는 분산 처리 컴퓨팅 환경 → 특히 파일 캐시의 일관성을 유지할 수 있음[3계층]미들웨어: 클라이언트/서버 컴퓨팅 기술의 이점을 살리려면 서로 다른 환경에서 실행되는 응용 프로그램이 서로 원만하게 통신할 수 있고, 개발자가 모든 플랫폼으로 시스템 자원에 액세스 할 수 있는 도구가 필요하다. 이처럼 교량 역할을 하며, 서비스를 제공하는 소프트웨어를 미들웨어 라고 한다.
[ 3계층의 장점]1. 클라이언트의 작업 처리 부담을 덜고, 응용 프로그램 서버가 작업 처리를 맡으므로, 중간 계층은 프로세스 대기, 응용 프로그램 실행 등 많은 기능을 실행할 수 있어 시스템의 성능과 융통성이 많이 향상된다.2. 응용 프로그램 서버에 작업 로직을 배치하여 프로그래밍 언어와 관계없이 개발할 수 있고, 응용 프로그램 로직을 모듈화하는 방법으로 유지관리하기가 쉽다.3. 확장성이 크게 향상된다. 데이터 베이스 서버는 응용 프로그램 서버 기능을 하지 않고, 데이터 작업만 수행하여 데이터 베이스 서버의 처리 용량이 증가한다.
다중처리(multiprocessing)다중처리는 병렬 처리라고도 하며, 다수의 프로세서를 동시 수행함으로써, 시스템 성능을 향상 시키는 방법이다. 연결하는 방법에 따라 공동 버스 시스템, 크로스바 교환 행렬 시스템, 다중 포트 메모리 시스템, 하이퍼큐브 시스템으로 구분할 수 있다.
다중처리 시스템의 운영체제
클러스터네트워크로 여러대의 컴퓨터를 연결하여 하나의 단일 컴퓨터처럼 동작하도록 구현한 시스템이다. 대칭형 다중 처리 대안으로 높은 성능과 확장성을 제공한다.
하나의 PC로도 여러 사람이 사용하는 경우가 많다. 그래서 운영체제는 한 컴퓨터를 여러 사람이 사용하는 환경도 지원해야 한다.
따라서, 운영체제는 각 계정을 관리할 수 있는 기능이 필요하다. 사용자 별로 프라이버시와 보안을 위해 개인 파일에 대해선 다른 사용자가 접근할 수 없도록 해야 한다. 이 밖에도 파일이나 시스템 자원에 접근 권한을 지정할 수 있도록 지원하는 것이 사용자 관리 기능이다.
리눅스의 모든 파일과 디렉토리는 퍼미션들의 집합으로 구성되어있다.이러한 Permission은 시스템에 대한 읽기, 쓰기, 실행에 대한 접근 여부를 결정한다. (ls -l로 확인 가능)퍼미션은, 다중 사용자 환경을 제공하는 리눅스에서는 가장 기초적인 보안 방법이다.
적절한 권한을 가진 인가자만 특정 시스템이나 정보에 접근할 수 있도록 통제하는 것
각종 해킹에서 보호하려고 보안 기능이 통합된 보안 커널을 추가로 이식한 운영체제다. 시스템 사용자 식별, 인증, 강제적이고 임의적인 액세스 제어, 감사추적(audit & trail), 침입 탐지 등 강력한 보안 기능을 포함한다.
운영체제에서 하드웨어를 인식하고 관리하게 만드는 계층
운영체제 안에 하드웨어를 추상화 해주는 계층이 필요하다. 이 계층이 바로 디바이스 드라이버라고 불린다. 하드웨어의 종류가 많은 만큼, 운영체제 내부의 디바이스 드라이버도 많이 존재한다.
www.yes24.com
gyoogle.dev
github.com
I will be a software architect.
I will be a software architect.
"
https://medium.com/@lopezlucas/introducci%C3%B3n-a-cloud-saas-iaas-paas-772c92c29bb1?source=search_post---------18,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lucas Lopez
May 8, 2018·5 min read
Hace un tiempo entramos en una época donde los servicios cloud y las plataformas digitales construidas en ellos dominan casi todos los aspectos de la vida empresaria, clientes, consumidores y personas por igual. La idea de este articulo es ofrecer una corta introducción sobre algunos de los modelos más importantes de los servicios cloud.
Aunque actualmente las oferta de servicios cloud ya no tiene los limites tan estrictamente definidos como antes, seguimos hablando de tres modelos principales.
Uno de los primeros modelos cloud del mercado que tuvo un éxito rotundo fue el modelo de software como servicio. Compañías empezaron a ofrecer servicios basados en Internet, en la nube, donde los clientes y los usuarios no tenían que preocuparse de ningún aspecto relacionado con donde se ejecutaba la solución.
Todo comenzó en el momento que parecía menos probable, cuanto la burbuja de Internet estaba reventando. Un claro ejemplo de esto es SalesForce.com. Fundada en 1999, sale a cotizar en bolsa en 2004, juntando 110 Millones de dólares. Hoy tiene una capitalización de mercado de más de 90 mil millones de dólares, siendo una de las empresas más valiosas de este modelo.
Muchas más la siguieron y hoy la oferta es casi ilimitada. Este modelo aceleró la adopción de herramientas de IT, permitiendo que no solos los departamentos de sistemas tuvieran el control sobre estas herramientas.
Este modelo tiene dos ventajas fundamentales:
Pero estos aspectos tienen un lado negativo:
En muchos aspectos, la contra-cara de software como servicio es la infraestructura como servicio. Por? Imaginemos que somos una empresa que ofrece software como servicio. Donde vamos a ejecutar nuestra aplicación que usan muchísimos clientes y usuarios? Obviamente podemos ofrecer nuestro servicio desde nuestro propio datacenter pero si hacemos eso, realmente estamos cumpliendo el manifiesto cloud?
Obviamente muchas empresas, de las más grandes, siguen ese modelo pero la revolución del software como servicio está soportada por el modelo de Infraestructura como Servicio. En vez de ofrecer la solución desde un datacenter propio es más fácil usar un proveedor de infraestructura en la nube, que gestiona todos los aspectos relacionados con el hardware, energía y conectividad, permitiendo a los clientes poner foco en sus negocios. Además se reduce la inversión necesaria para comenzar a operar.
Este modelo también tiene ventajas y desventajas:
Pero estos aspectos impactan en:
El modelo de plataforma como servicio nace de dos necesidades distintas, la de las empresas digitales que no querían tener que administrar la infraestructura en la nube y la de los proveedores que empezaron a detectar que la utilización de su infraestructura no era la más eficiente cuando el escala de clientes iba creciendo. Por que pagar y/o mantener por hardware que realmente no se estaba utilizando?
Entonces los proveedores de servicios en la nube comenzaron a ofrecer el modelo de plataforma como servicio, donde el foco de los clientes es el desarrollo de las soluciones, pudiendo delegar el resto de las operaciones relacionadas al hardware. Las operaciones se trasforman en DevOps. En los últimos años, las plataformas como servicio alcanzaron una especialización muy importante, hay servicios de plataforma para cada necesidad, desde base de dato y tiempo de ejecución a servicios orientados a inteligencia artificial e Internet of Things. La verdad es que el modelo PaaS engloba todo un conjunto de sub categorías con sus propias características y diferencias
Este nuevo modelo ofrece como ventajas:
Aunque el lograr estos beneficios puede comprometerse por
No hay modelo perfecto ni tampoco un modelo malo. Todo depende de la arquitectura de nuestra solución y como combinamos cada uno de los componentes de los modelos. No hay respuesta correcta, sino alternativas… con la excepción de creer que un solo modelo es la respuesta a todos nuestros problemas.El tratar de migrar o adoptar modelos en la nube, como servicio, sin evaluar nuestra arquitectura y la necesidad de re-ingeniería puede impactar nuestras soluciones, incluso elevando los costó de ejecutar las operaciones.
La presentación completa está disponible en en Slideshare
Todas las opiniones expresadas son mías y no representan opiniones de ninguna entidad con la que he estado, estoy o estaré afiliado.
All views expressed are my own and do not represent opinions of any entity whatsoever with which I have been, am now, or will be affiliated
Avid Technologist at heart, a lifetime of projects, experience in software development and project management areas.
9 
Some rights reserved

9 
9 
Avid Technologist at heart, a lifetime of projects, experience in software development and project management areas.
"
https://medium.com/strongnode/ama-with-polygon-on-computing-with-a-purpose-and-the-benefits-of-launching-on-polygon-4723afc8552e?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·6 min read
StrongNode.io, an Infrastructure-as-a-Service (IaaS) tech company, CEO and Co-founder Daniel Saito and Chief Product Officer Gil Bashan joined a brand new AMA at the Polygon Official channel last 29 September 2021. Daniel and Gil discussed our company and products and talked about the upcoming $SNE token public sale and IDO launch plans.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Be sure to check out some of the highlights from the StrongNode and Polygon AMA session below:
Chinmay | Polygon: To dive into StrongNode, how did that came into being, what was the plan behind launching it and how will it make the difference?
Daniel Saito: Well the idea came over the years, over the span of the decade of myself and Colin working together during our years at MySQL.
We eventually took action earlier this year to make it an organization and hire people around the project to get started.
When we first started the thought of StrongNode, we looked outside to see what we can do to help solve some of the world’s problems. Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it.
When myself and Colin worked at MySQL we were consulting with [numerous] Web 2.0 companies optimizing the performance of their MySQL queries and tweaking their implementation architecture so it scales with effective transactions per second, we started to notice a need out in large scale enterprises all the way to small-medium businesses that they had a requirement to scale out their data and processing capabilities, hence the *cloud infrastructure* race (AWS, GCP, MSFT).
StrongNode is an Infrastructure-as-a-Service technology company and innovation lab. We harness the power of latent and idle compute like the CPU/GPU cycles, network, and storage of the everyday common man/woman. In return, you get paid in StrongNode tokens [$SNE] when you lend your network and compute resources.
Daniel Saito: @IndCryptoGeek Quick question, how many digital devices do you have around the house?
Chinmay | Polygon: But got your point too many Idle devices
Daniel Saito: yeah right, look around you. How many devices do you have just plugged in and idle?
Chinmay | Polygon: 2–3 ease
Daniel Saito: yeah if they are just sucking the power out of the wall and not doing anything productive for you while charging. That is a waste of idle resources.
GB: They say that by 2030, every person will have around 15 devices
Daniel Saito: Damn. That’s not good for ewaste.
Chinmay | Polygon: Exactly, couldn’t agree more
Daniel Saito: So computing for a purpose as our technology is made for the new normal.
Chinmay | Polygon: This is a great concept! Please tell us more about the process, how it works what are its features 🙌
Daniel Saito: Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. With that said, we had to come up with a solution or an alternative and looking inwards of what is available we noticed that we already had the blueprint already in front of us in the form of Open Source (https://www.youtube.com/watch?v=SpeDK1TPbew).
Bitcoin’s increasing energy consumption has triggered a passionate debate about the sustainability of the digital currency. We want to introduce the idea of GREEN MINING. As StrongNode tries to tackle Green Mining, we want to make “Computing for a Purpose” a reality. Due to the recent times with the pandemic people have been shuttered in their homes, this was no different for us as well. Here we take a stand and try to solve the world’s problem one problem at a time! Made with ❤️ for the NEW Normal!
I think the most important question would be of security, for the whole ecosystem that StrongNode has created, how do the users stay secure and what are all the measures taken by StrongNode
Daniel Saito: As a former security analyst, researcher and developer; I was originally tasked at designing the framework for the trusted mobile platform using a TPM or otherwise known in today’s term as “secure enclave” in 4G devices with NTT DoCoMo R&D, Intel and IBM. I want to take the same principal but also apply it to the anonymous / cryptographic model of crypto and create the ultimate solution in network and privacy where the user add value and in return they receive a monetary reward.
As for privacy, we only know you by your wallet address at any time. All personal KYCd data will be stored offsite with our KYC authentication partners. You will need to be properly KYCd to join the StrongNode Network. We plan to not only have Audit certificates but we also plan to use 3rd party services to attack our network, have bug bounties, and most of all it will be OPEN SOURCE (even that will be all audited).
In order to help this process, we reinvented OpenID this is our SSO (Single Sign On) approach like “Login with Google” or “Login with Facebook” to validate you as a user to enter the network. Upon completing the KYC component at User Registration, not only do we airdrop you $SNE tokens, but you also get this nifty NFT dropped into your wallet. This NFT is specific to your wallet address and your identity, which cannot be moved out of your wallet. This NFT acts as an access token (a key) to login to various crypto properties that we are working on in our innovation labs. We want to use best of breed open source projects and develop StrongNode with the tech.
Gil: Security is very important to us as you can see
PARTNERSHIP WITH POLYGON
How was your experience building on Polygon, how does it affects StrongNode and what are the benefits of using Polygon, for StrongNode users
Daniel Saito: Well we enjoyed working on Polygon the learning curve was negligible.
We can take some of the best ideas implemented on BSC or ETH and be able to do it cheaper and faster..
But of course finding talent for smart contract devs is always hard. Always looking for the best of the best, and if you think you have what it takes we are hiring! DM me.
How can a NEWBIE like me understand the STRONGNODE . ONE platform so easily? Can beginners with little cryptographic knowledge be able to successfully Use, trust & invest in your platform?
Gil: Another big part of our ethos is creating product that is sleek, sexy, and easy for people across all walks of life access and get benefit from.
We are a technology ecosystem that leverages blockchain, NOT a blockchain company doing x, y, and z.
This is a big distinction. Too many companies build way too complicated technology and market it the wrong way, immediately cutting out a huge user base needed for product and mission success. Not StrongNode though, we want everyone to onboard, no matter what your tech background!
GB: Be sure to join our socials — we have a lot of fun announcements and developments incoming.
For more information, visit: http://strongnode.io
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
370 
370 
370 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/strongnode/ama-with-bang-pateng-on-sne-token-long-term-benefits-and-harnessing-the-trifecta-of-idle-compute-e5438e04fd49?source=search_post---------328,"Sign in
There are currently no responses for this story.
Be the first to respond.
StrongNode
Oct 10, 2021·5 min read
Infrastructure-as-a-Service (IaaS) tech company StrongNode.io’s CEO and Co-founder Daniel Saito joined the Bang Pateng Group AMA episode last 23 September 2021. Daniel answered queries about our company, project, and shared facts about the future plas for the IDO launch and $SNE token public sale.
Our $SNE Token Sale will roll out on 3 launchpads. The IDO is happening on TrustPad.io, Starter.xyz, and BullPerks.com.
For the IDO on TrustPad and BullPerks, details on how to join will be available later on their website. For Starter, here’s how to participate in our IDO: https://starterxyz.medium.com/guide-how-to-take-part-in-the-strongnode-edge-ido-on-starter-c09233aa3d70. The KYC process for the StrongNode IDO on Starter started 30 Sept via Blockpass, please click here.
Some of the highlights from the AMA session between StrongNode and Bang Pateng Group are posted below:
Can you introduce STRONG NODE to our community?
Daniel Saito: When we first started the thought of StrongNode, we looked outside to see what we can do to help solve some of the world’s problems. Last year wasn’t any joke and now we have a pandemic on our hands. We went to our clientele and saw what keeps them up at night. We noticed that the cost of computing has gone up, these cloud providers have a foothold on us and they know it. With that said, we had to come up with a solution or an alternative and looking inwards of what is available we noticed that we already had the blueprint already in front of us in the form of Open Source.
We wanted to address the last mile, which is the EDGE. It’s the consumers’ latent resources from your PC/MAC/LINUX [environment] (CPU, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with a token economic incentive to participate both as a broadcaster and a receiver. This computer is then harnessed to process some of the world’s challenging compute problems such as offering cost effective compute resources for genome sequencing analysis for COVID.
We are working on “Computing with a Purpose.” The solution was designed with security in mind and made with ❤️ for the NEW NORMAL.
We see StrongNode as two products, there is StrongNode as an APP and StrongNode as a token $SNE. We found a problem and created a solution behind it. When COVID broke out genetic sequencing labs were backed up in sequencing and processing lab results for various strains. They were also backed up in computing workflows. We worked with them to create a solution around this and henceforth here we are weeks away from an IDO. They can use our $SNE token off load their compute workloads off of centralized solutions and process on the EDGE. The folks that want to harvest and mine these data blocks and receive $SNE just need to participate and give their resources, and passively make income.
What are the core features of STRONG NODE ? How are you different and unique from all other projects out there?
Daniel Saito: Well, we are targeting the masses with our $SNE Token.
Sir, How does a project increase the $token’s value and utility? So that it can lead to an increase in $token price? What is your plan to make your project stable and provide the highest return for investors in the long term?
Daniel Saito: The $SNE token has several different features and value in our ecosystem that have been pulled together from years of experience from our team and our advisors. More news on this will come out as we get closer to launch, but we are incentivizing token holders to hold along with us. SEED and PRIVATE INVESTORS are locked up for 6 months. All employees and advisors of StrongNode are locked up for 4 years.
We wanted to grow the digital footprint of our decentralized distributed network and to be able to get token distribution to as many wallets as possible which will, in the long term, help the value of the $SNE token.
We utilize the consumers’ latent resources from your PC / MAC / LINUX [environment] (cpu, bandwidth, storage) and use it for a batch processing cluster using the map/reduce method of sorting jobs and processing it with the $SNE token economic incentive to participate both as a broadcaster and a receiver. We have many other use cases planned as we scale out our node network with launching our innovation lab and all the fun platforms we have planned out to use the StrongNode technology. We will place mechanisms in place and people can engage by offering these services getting $SNE tokens. By ensuring your uptime and availability of your computer on the node network will allow for you to earn more tokens passively while browsing the web or watching Netflix.
StrongNode is collecting a trifecta of idle and untapped computing resources on the network:
1) CPU and GPU cycles
2) Bandwidth
3) Data storage
How will this help them achieve their goal of developing a one-of-a-kind global last mile mesh network?
Daniel Saito: We will be bringing all these idle resources available on our network. But don’t expect to be available to be online on DAY 1. Building up these resources will take time and there will be a moment on the network where we will have enough resources to be able to have 99.999% uptime. But this will take time for us to achieve, under promise, over deliver..
For more information, visit our website: http://strongnode.io
Join us on Telegram: https://t.me/strongnodechat
#####
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
120 
120 
120 
StrongNode.io is an Infrastructure-as-a-Service technology company and innovation lab. We are creating the future of digital connectivity.
"
https://medium.com/joe-gardiner/moving-to-cloud-iaas-and-paas-is-it-a-silver-bullet-13d027afcfce?source=search_post---------275,"There are currently no responses for this story.
Be the first to respond.
I was recently at a networking event where I got chatting to a founder of an Internet of Things consultancy company. We had very similar views about the confusion around cloud services — at the moment IaaS is the default choice (it doesn’t even have to be explained), but is this the best option?
This got me thinking, what are senior decision makers, or buyers, looking for with a move to a cloud platform. If buying decisions are being driven by a CIO then it’s probably going to be cost savings. This makes perfect sense; if you have ephemeral computing requirements then of course you should benefit from the race to the bottom between the big public IaaS providers.
The problem is that cost savings don’t naturally deliver business value. Value is one of those intangible things that means different things to different businesses. Moving a load of business services into a public cloud just to save money will almost certainly not deliver value and may in fact result in higher costs. Migration plans never go wrong, right?
Moving to the cloud is far more of a cultural challenge than technological one.
I think that to make sure that value (however a business measures this) is derived from what public IaaS can offer all the stake holders need to be on-board. This means engaging with the people carrying out the migration, those running and orchestrating the infrastructure and those making the purchase.
I hear a lot of confusion around infrastructure being the silver bullet for a business who has a lot of legacy IT, but I think you need to abstract away from the infrastructure to actually get value in the business and that means engaging with your developers and business executives, and making sure that the conversation about cloud is happening across the board.
Home of Joe Gardiner’s stories.
Home of Joe Gardiner’s stories. Here you’ll find stories about DevOps, IT transformation, tooling, containers and the evolution of business culture.
Written by
Automater of things, whipping up awesome, regular conference speaker. Keeps all the moving parts working together. I help businesses transform their IT
Home of Joe Gardiner’s stories. Here you’ll find stories about DevOps, IT transformation, tooling, containers and the evolution of business culture.
"
https://medium.com/lightspeedindia/this-fourth-wave-of-indian-enterprise-software-startups-is-world-class-5c16a654acc2?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
[Published on Yourstory.com]
India’s enterprise software industry has been slowly bubbling since the 1980s but has generally failed to deliver a large number of high impact, high value companies. We do have some companies that everybody talks about — iFlex, Tally, Zoho — but these are far and few between. I believe that we are seeing a new scalable wave of enterprise software companies coming out of India and there is a potential to deliver several high impact companies over the next decade. Here at Lightspeed Venture Partners, leveraging our global strength in enterprise technologies, we see opportunities to partner with companies that are cloud-native and have cracked a global market — examples of current active categories in India are CRM, analytics/big data, marketing automation and infrastructure.
India’s enterprise software industry has to be looked at separately from the outsourcing/BPO firms like Genpact, Cognizant, Tata Consulting Services and Infosys. Starting in the 1980s and early 1990s, this services industry is now mature and at scale.
Separate from the outsourcing/BPO industry, India’s enterprise software industry (or “products” as it is called by many here in India) has evolved from the 1980s to now in what I think can be divided into four waves, coinciding somewhat with three trends: 1) enterprise software moving from desktop to client-server to cloud; 2) evolution of Indian industry post 1991 liberalization; and 3) increased experience of Indians at successful US product companies.
WAVE 1
The first wave of software products came along in the late 1980s/early 1990s — the focus was desktop products for business accounting. Companies in this wave include Tally Solutions (still the undisputed leader in SME accounting software in India), Instaplan, Muneemji and Easy Accounting.
WAVE 2
This generation of software products emerged in the 1990s as projects within outsourcing firms or from internal services arms of larger corporates. Infosys launched Finacle. Ramco Systems launched its ERP. And Citibank launched CITIL which became i-Flex. Other notable companies included 3i Infotech, Cranes Software, Kale Consultants, Newgen Software, Polaris Financial Technologies, Srishti Software and Subex.
I remember attending CEBIT in Hanover in 1989 when many of these Indian software and consulting companies were first introduced to Europe.
The late 1990s saw a wavelet of ASP (application service provider) startups in India, most of which got crushed after the dotcom bust.
WAVE 3
The 2000s saw on-premise India-first companies such as Drishti-Soft, Eka Software, Employwise, iCreate Software, iViz, Manthan Systems, Quick Heal Technologies, Talisma (for which I did some initial product management work while at Aditi Technologies) and Zycus get started. This was the era of 8–10% GDP growth in India which lasted till about 2010. Many of these companies had a direct sales model. After India, they generally expanded into the global South (Africa, Middle East, SE Asia, Latin America) where they found similar customer requirements and little competition from Western software companies. Bootstrapped in their earlier years, some of these companies grew over several years and have broken through to $25 million+ in annual revenue. Key verticals have traditionally been BFSI (banking, financial services and insurance), telecom, retail/FMCG (fast-moving consumer goods aka CPG in the US) and outsourcing/BPO.
Having been around for over a decade, some of these companies generally face the challenge of migrating to the cloud, upgrading user experience to modern Web 2.0 levels, and expanding addressable markets beyond the global South to the US and Europe. We have seen some of these companies get venture funded, typically at much later stages in their go-to-market relative to US-based software companies. Several of these companies have received funding in the past couple of years, ostensibly to “go international” and “go cloud,” not an easy task, especially when done together.
WAVE 4
Starting in around 2010, a new wave of cloud-native companies were launched, perhaps following the slowdown in India’s economy and the growth/acceptance of SaaS as a delivery model and as a sales model in the US. These companies have grown and now could power beyond the $10M/year revenue glass ceiling. The reason for the scale potential being higher for this cloud-native wave is the cracking of efficient online sales channels to reach markets globally.
Why this decade? Because there is an increased willingness of companies around the world to search for and buy software products online. There is now a large pool of founders who have worked at global enterprise product companies (e.g. Indian offshore development centers or in Silicon Valley itself with companies like SAP, Oracle, Google, Microsoft, Adobe) and have experience in product management, marketing and sales. And finally, there has been a dramatic reduction in the capital required to bootstrap enterprise software companies. Everybody uses AWS and software from other startups to get started. It’s quite meta.
Wave 4 companies have the opportunity to break through the barriers that previously relegated Indian enterprise software companies to selling to the global South. We have seen Atlassian (Australia), Zendesk (Denmark) and Outbrain (Israel) do this move to Western or global markets. Zoho is an Indian company that is rumored to be at $100 million per year revenue scale — they have been part of many of the waves I have described.
This cloud-native wave, I believe, can be divided into two dimensions. One dimension is the platform/tools companies versus workflow automation (applications) companies. The other dimension is India-first companies versus the global-first companies. We see opportunities in all four quadrants, each having its own challenges. We are interested in looking at companies in all these segments, with a bias toward companies which have reached some scale ($1M ARR) and are going after large addressable markets with aggressive sales & marketing execution.
[Please note this is not a comprehensive list of companies nor a view on which companies we admire or not]
Global-first companies coming out of India have started to crack or have cracked the online sales model, using SEO, SEM, content marketing and telesales. They are typically going after mature segments where buyers are typing keywords into Google at a high rate. This online selling model results in an SMB and mid-market customer base. In many cases, founders may have to move to the US to pursue direct enterprise sales. It’s worth noting that scale markets are not necessarily all in the US — companies could get built with a general global diffusion of customers, perhaps with help from resellers.
I see India-first companies typically going after newer high-growth companies in India (e.g. ecommerce, retail) and startups. Some go after Indian arms of multinationals (MNCs). This is a reasonable early adopter market to cut a product’s teeth on, but has limited ability to scale. Of the newer crop of India-first companies, very few go after large enterprises in India — there are exceptions like Peelworks and Wooqer. The model here generally is SaaS as a delivery model but not SaaS as a sales model (ie direct sales, not self-service). Many software companies are essentially verticalized.
We continue to see a few high-ticket, high touch direct sales enterprise software companies which are global-first, including companies like Cloudbyte, Druva, Indix, Sirion Labs and Vaultize. Many of these start out with teams in both Silicon Valley and India or transplant themselves to the Valley over time. I think this will continue to happen but we will not see the explosion here that we are seeing in the number of companies utilizing low touch online sales models. I see several high-impact companies coming out of these direct sales enterprise software startups as well.
I think this dichotomy between India-first and global-first companies is interesting and makes India a distinctly different type of investment geography, different from Israel (which has very small domestic market where tech companies move to the US very quickly), different from China (which mostly has domestic market focused startups and very little enterprise software) and different from the US (which is primarily domestic-focused in $500B enterprise tech industry in the early years of most startups). In terms of investor and founder interest, the pendulum may also swing back and forth between these two models as the Indian economy grows, sometimes at high speed, sometimes at a snails pace.
[With input from the team at iSPIRT and several of the companies mentioned above].
Tomorrow, built today.
5 
1
5 claps
5 
1
Tomorrow, built today.
Written by
Venture capital investor at Lightspeed India Partners.
Tomorrow, built today.
"
https://medium.com/@stevefan1999/is-saas-dying-62199ff5d135?source=search_post---------392,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steve “Stefan” Fan
Jun 2, 2017·1 min read
Noah Jessop
Is SaaS dying? I guess not. You see, all that Office 365, Creative Cloud and Salesforce is still vibing.
Is SaaS decaying into something else? I guess so. They are now more IaaS-ish, that means more freedom to the user, but the software itself is more generic. This could be either good or bad.
The ‘Docker-heat’ used to resurrect SaaS(along with PaaS, which SaaS relied on), but instead it was re-purposed for cloud deployment, scalability and testing.
Seems like there’s no way out for SaaS. But it’s not yet dead. It’s turning into another form in order to survive.
A mad lad
See all (10)
A mad lad
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aheadcrm/great-cx-from-an-oracle-point-of-view-8a2c1b4a95aa?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Wieberneit
Jan 15, 2021·2 min read
The CRMKonvos team started into the new year talking with Daniel Renggli, Director Field Marketing North at Oracle. As we all know, Oracle has a vast range of solutions across the whole value chain and is one of the few that can (almost) support a whole enterprise with its solution. Oracle was also known as a cloud laggard — a notion that got vastly changed in the last few years. And Oracle governs the complete hardware and software stack to support businesses, which again is rare in the enterprise applications world. The company’s IaaS infrastructure even can extend into the customer data center, which is a very interesting offering.
Last, but not least, Oracle has a strong set of CRM- and CX applications, on premise and in the cloud, with the cloud software being established in the market in the past years.
Oracle is one of the contenders in the #ClashOfTitans.
All the more reason to talk with an Oracle representative about what is great CX and how to get there.
Being confronted with the complete CRMKonvos gang, Daniel took his stand.
And he did well.
We learned a lot. You can, too. Here is the recording of the conversation. It is worthwhile the time. Trust me.
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
Helping businesses to improve in Digital Transformation, Customer Engagement, Customer Experience, CRM, Innovation
"
